{"custom_id": "keras#85c9a58892d4626d0a37a6fdc13113a6dbb301d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -79,11 +79,11 @@ def to_categorical(x, num_classes=None):\n     ...               .04, .01, .94, .05,\n     ...               .12, .21, .5, .17],\n     ...               shape=[4, 4])\n-    >>> loss = keras.backend.categorical_crossentropy(a, b)\n+    >>> loss = keras.ops.categorical_crossentropy(a, b)\n     >>> print(np.around(loss, 5))\n     [0.10536 0.82807 0.1011  1.77196]\n \n-    >>> loss = keras.backend.categorical_crossentropy(a, a)\n+    >>> loss = keras.ops.categorical_crossentropy(a, a)\n     >>> print(np.around(loss, 5))\n     [0. 0. 0. 0.]\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4993d49fd683d46e441e1847ebe5a31c3281d56b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 62 | Churn Cumulative: 4596 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -238,7 +238,7 @@ class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n         self.assertAllClose(v.numpy(), np.array([1, 2, 3]))\n \n     @pytest.mark.skipif(\n-        backend.backend() != \"tf\",\n+        backend.backend() != \"tensorflow\",\n         reason=\"Tests for MirroredVariable under tf backend\",\n     )\n     def test_variable_numpy_scalar(self):\n\n@@ -235,6 +235,8 @@ class Discretization(TFDataLayer):\n             dtype=self.compute_dtype,\n             backend_module=self.backend,\n         )\n+        if self.sparse:\n+            return tf.sparse.from_dense(outputs)\n         return outputs\n \n     def get_config(self):\n\n@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n@@ -11,7 +12,7 @@ from keras import testing\n from keras.saving import saving_api\n \n \n-class DicretizationTest(testing.TestCase, parameterized.TestCase):\n+class DiscretizationTest(testing.TestCase, parameterized.TestCase):\n     def test_discretization_basics(self):\n         self.run_layer_test(\n             layers.Discretization,\n@@ -125,6 +126,55 @@ class DicretizationTest(testing.TestCase, parameterized.TestCase):\n         model = saving_api.load_model(fpath)\n         self.assertAllClose(layer(ref_input), ref_output)\n \n-    def test_sparse_inputs(self):\n-        # TODO\n-        pass\n+    @parameterized.parameters(\n+        [\n+            (\n+                \"one_hot\",\n+                [[-1.0, 0.2, 0.7, 1.2]],\n+                [\n+                    [\n+                        [1.0, 0.0, 0.0, 0.0],\n+                        [0.0, 1.0, 0.0, 0.0],\n+                        [0.0, 0.0, 1.0, 0.0],\n+                        [0.0, 0.0, 0.0, 1.0],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"multi_hot\",\n+                [[[-1.0], [0.2], [0.7], [1.2]]],\n+                [\n+                    [\n+                        [1.0, 0.0, 0.0, 0.0],\n+                        [0.0, 1.0, 0.0, 0.0],\n+                        [0.0, 0.0, 1.0, 0.0],\n+                        [0.0, 0.0, 0.0, 1.0],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"count\",\n+                [[-1.0], [0.2], [0.7], [1.2]],\n+                [\n+                    [1.0, 0.0, 0.0, 0.0],\n+                    [0.0, 1.0, 0.0, 0.0],\n+                    [0.0, 0.0, 1.0, 0.0],\n+                    [0.0, 0.0, 0.0, 1.0],\n+                ],\n+            ),\n+        ]\n+    )\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"Sparse tensor only works in TensorFlow\",\n+    )\n+    def test_sparse_output(self, output_mode, input_array, expected_output):\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        x = np.array(input_array)\n+        layer = layers.Discretization(\n+            bin_boundaries=[0.0, 0.5, 1.0], sparse=True, output_mode=output_mode\n+        )\n+        output = layer(x)\n+        self.assertTrue(isinstance(output, tf.SparseTensor))\n+        self.assertAllClose(output, np.array(expected_output))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2588a15440a10de8d0cc10b50c21ede333a2bd01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 2 | Churn Cumulative: 1770 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -357,7 +357,6 @@ class MultiHeadAttention(Layer):\n             )\n         )\n         self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n-        if self.dropout:\n         self._dropout_layer = Dropout(\n             rate=self._dropout, dtype=self.dtype_policy\n         )\n\n@@ -49,6 +49,7 @@ class Dropout(Layer):\n         self.rate = rate\n         self.seed = seed\n         self.noise_shape = noise_shape\n+        if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n         self.built = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5dad3905969e62a85f6da6aefead2790a8aa8f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 39084 | Files Changed: 98 | Hunks: 98 | Methods Changed: 773 | Complexity Δ (Sum/Max): -1274/0 | Churn Δ: 39084 | Churn Cumulative: 91739 | Contributors (this commit): 26 | Commits (past 90d): 281 | Contributors (cumulative): 283 | DMM Complexity: 0.0642915642915643\n\nDIFF:\n@@ -1,370 +0,0 @@\n-\"\"\"\n-Title: GPT2 Text Generation with KerasNLP\n-Author: Chen Qian\n-Date created: 04/17/2023\n-Last modified: 04/17/2023\n-Description: Use KerasNLP GPT2 model and `samplers` to do text generation.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-In this tutorial, you will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a\n-pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models)\n-(originally invented by OpenAI), finetune it to a specific text style, and\n-generate text based on users' input (also known as prompt). You will also learn\n-how GPT2 adapts quickly to non-English languages, such as Chinese.\n-\"\"\"\n-\n-\"\"\"\n-##  Before we begin\n-\n-Colab offers different kinds of runtimes. Make sure to go to **Runtime ->\n-Change runtime type** and choose the GPU Hardware Accelerator runtime\n-(which should have >12G host RAM and ~15G GPU RAM) since you will finetune the\n-GPT-2 model. Running this tutorial on CPU runtime will take hours.\n-\"\"\"\n-\n-\"\"\"\n-## Install KerasNLP, Choose Backend and Import Dependencies\n-\n-This examples uses [Keras 3.0](https://keras.io/keras_core/) to work in any of\n-`\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras Core is baked into\n-KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select\n-the backend of your choice. We select the JAX backend below.\n-\"\"\"\n-\n-\"\"\"shell\n-pip install git+https://github.com/keras-team/keras-nlp.git -q\n-\"\"\"\n-\n-import keras_nlp\n-import tensorflow as tf\n-import json\n-import keras\n-import os\n-import tensorflow_datasets as tfds\n-import time\n-\n-\n-\"\"\"\n-## Introduction to Generative Large Language Models (LLMs)\n-\n-Large language models (LLMs) are a type of machine learning models that are\n-trained on a large corpus of text data to generate outputs for various natural\n-language processing (NLP) tasks, such as text generation, question answering,\n-and machine translation.\n-\n-Generative LLMs are typically based on deep learning neural networks, such as\n-the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by\n-Google researchers in 2017, and are trained on massive amounts of text data,\n-often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/)\n-and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),\n-are trained with a large dataset from various data sources which allows them to\n-generate output for many tasks. The core of Generative LLMs is predicting the\n-next word in a sentence, often referred as **Causal LM Pretraining**. In this\n-way LLMs can generate coherent text based on user prompts. For a more\n-pedagogical discussion on language models, you can refer to the\n-[Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/).\n-\"\"\"\n-\n-\"\"\"\n-## Introduction to KerasNLP\n-\n-Large Language Models are complex to build and expensive to train from scratch.\n-Luckily there are pretrained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/)\n-provides a large number of pre-trained checkpoints that allow you to experiment\n-with SOTA models without needing to train them yourself.\n-\n-KerasNLP is a natural language processing library that supports users through\n-their entire development cycle. KerasNLP offers both pretrained models and\n-modularized building blocks, so developers could easily reuse pretrained models\n-or stack their own LLM.\n-\n-In a nutshell, for generative LLM, KerasNLP offers:\n-\n-- Pretrained models with `generate()` method, e.g.,\n-    `keras_nlp.models.GPT2CausalLM` and `keras_nlp.models.OPTCausalLM`.\n-- Sampler class that implements generation algorithms such as Top-K, Beam and\n-    contrastive search. These samplers can be used to generate text with\n-    custom models.\n-\"\"\"\n-\n-\"\"\"\n-## Load a pre-trained GPT-2 model and generate some text\n-\n-KerasNLP provides a number of pre-trained models, such as [Google\n-Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n-and [GPT-2](https://openai.com/research/better-language-models). You can see\n-the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n-\n-It's very easy to load the GPT-2 model as you can see below:\n-\"\"\"\n-\n-# To speed up training and generation, we use preprocessor of length 128\n-# instead of full length 1024.\n-preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n-    \"gpt2_base_en\",\n-    sequence_length=128,\n-)\n-gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n-    \"gpt2_base_en\", preprocessor=preprocessor\n-)\n-\n-\"\"\"\n-Once the model is loaded, you can use it to generate some text right away. Run\n-the cells below to give it a try. It's as simple as calling a single function\n-*generate()*:\n-\"\"\"\n-\n-start = time.time()\n-\n-output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n-print(\"\\nGPT-2 output:\")\n-print(output)\n-\n-end = time.time()\n-print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")\n-\n-\"\"\"\n-Try another one:\n-\"\"\"\n-\n-start = time.time()\n-\n-output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n-print(\"\\nGPT-2 output:\")\n-print(output)\n-\n-end = time.time()\n-print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")\n-\n-\"\"\"\n-Notice how much faster the second call is. This is because the computational\n-graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n-re-used in the 2nd behind the scenes.\n-\n-The quality of the generated text looks OK, but we can improve it via\n-fine-tuning.\n-\"\"\"\n-\n-\"\"\"\n-## More on the GPT-2 model from KerasNLP\n-\n-Next up, we will actually fine-tune the model to update its parameters, but\n-before we do, let's take a look at the full set of tools we have to for working\n-with for GPT2.\n-\n-The code of GPT2 can be found\n-[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n-Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n-modules in KerasNLP, all of which have a *from_preset()* function that loads a\n-pretrained model:\n-\n-- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n-    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n-- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n-    causal LM training. It does the tokenization along with other preprocessing\n-    works such as creating the label and appending the end token.\n-- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n-    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n-    `GPT2`.\n-- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n-    output of `GPT2Backbone` by embedding matrix to generate logits over\n-    vocab tokens.\n-\"\"\"\n-\n-\"\"\"\n-## Finetune on Reddit dataset\n-\n-Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one\n-step further to finetune the model so that it generates text in a specific\n-style, short or long, strict or casual. In this tutorial, we will use reddit\n-dataset for example.\n-\"\"\"\n-\n-reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)\n-\n-\"\"\"\n-Let's take a look inside sample data from the reddit TensorFlow Dataset. There\n-are two features:\n-\n-- **__document__**: text of the post.\n-- **__title__**: the title.\n-\n-\"\"\"\n-\n-for document, title in reddit_ds:\n-    print(document.numpy())\n-    print(title.numpy())\n-    break\n-\n-\"\"\"\n-In our case, we are performing next word prediction in a language model, so we\n-only need the 'document' feature.\n-\"\"\"\n-\n-train_ds = (\n-    reddit_ds.map(lambda document, _: document)\n-    .batch(32)\n-    .cache()\n-    .prefetch(tf.data.AUTOTUNE)\n-)\n-\n-\"\"\"\n-Now you can finetune the model using the familiar *fit()* function. Note that\n-`preprocessor` will be automatically called inside `fit` method since\n-`GPT2CausalLM` is a `keras_nlp.models.Task` instance.\n-\n-This step takes quite a bit of GPU memory and a long time if we were to train\n-it all the way to a fully trained state. Here we just use part of the dataset\n-for demo purposes.\n-\"\"\"\n-\n-train_ds = train_ds.take(500)\n-num_epochs = 1\n-\n-# Linearly decaying learning rate.\n-learning_rate = keras.optimizers.schedules.PolynomialDecay(\n-    5e-5,\n-    decay_steps=train_ds.cardinality() * num_epochs,\n-    end_learning_rate=0.0,\n-)\n-loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-gpt2_lm.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate),\n-    loss=loss,\n-    weighted_metrics=[\"accuracy\"],\n-)\n-\n-gpt2_lm.fit(train_ds, epochs=num_epochs)\n-\n-\"\"\"\n-After fine-tuning is finished, you can again generate text using the same\n-*generate()* function. This time, the text will be closer to Reddit writing\n-style, and the generated length will be close to our preset length in the\n-training set.\n-\"\"\"\n-\n-start = time.time()\n-\n-output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n-print(\"\\nGPT-2 output:\")\n-print(output)\n-\n-end = time.time()\n-print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")\n-\n-\"\"\"\n-## Into the Sampling Method\n-\n-In KerasNLP, we offer a few sampling methods, e.g., contrastive search,\n-Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but\n-you can choose your own sampling method.\n-\n-Much like optimizer and activations, there are two ways to specify your custom\n-sampler:\n-\n-- Use a string identifier, such as \"greedy\", you are using the default\n-configuration via this way.\n-- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration\n-via this way.\n-\"\"\"\n-\n-# Use a string identifier.\n-gpt2_lm.compile(sampler=\"top_k\")\n-output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n-print(\"\\nGPT-2 output:\")\n-print(output)\n-\n-# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n-greedy_sampler = keras_nlp.samplers.GreedySampler()\n-gpt2_lm.compile(sampler=greedy_sampler)\n-\n-output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n-print(\"\\nGPT-2 output:\")\n-print(output)\n-\n-\"\"\"\n-For more details on KerasNLP `Sampler` class, you can check the code\n-[here](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers).\n-\"\"\"\n-\n-\"\"\"\n-## Finetune on Chinese Poem Dataset\n-\n-We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n-this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n-model to become a poet!\n-\n-Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n-contains some Chinese characters, we can use the original vocab to finetune on\n-Chinese dataset.\n-\"\"\"\n-\n-\"\"\"shell\n-# Load chinese poetry dataset.\n-git clone https://github.com/chinese-poetry/chinese-poetry.git\n-\"\"\"\n-\n-\"\"\"\n-Load text from the json file. We only use《全唐诗》for demo purposes.\n-\"\"\"\n-\n-poem_collection = []\n-for file in os.listdir(\"chinese-poetry/全唐诗\"):\n-    if \".json\" not in file or \"poet\" not in file:\n-        continue\n-    full_filename = f\"{'chinese-poetry/全唐诗'}/{file}\"\n-    with open(full_filename, \"r\") as f:\n-        content = json.load(f)\n-        poem_collection.extend(content)\n-\n-paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]\n-\n-\"\"\"\n-Let's take a look at sample data.\n-\"\"\"\n-\n-print(paragraphs[0])\n-\n-\"\"\"\n-Similar as Reddit example, we convert to TF dataset, and only use partial data\n-to train.\n-\"\"\"\n-\n-train_ds = (\n-    tf.data.Dataset.from_tensor_slices(paragraphs)\n-    .batch(16)\n-    .cache()\n-    .prefetch(tf.data.AUTOTUNE)\n-)\n-\n-# Running through the whole dataset takes long, only take `500` and run 1\n-# epochs for demo purposes.\n-train_ds = train_ds.take(500)\n-num_epochs = 1\n-\n-learning_rate = keras.optimizers.schedules.PolynomialDecay(\n-    5e-4,\n-    decay_steps=train_ds.cardinality() * num_epochs,\n-    end_learning_rate=0.0,\n-)\n-loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-gpt2_lm.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate),\n-    loss=loss,\n-    weighted_metrics=[\"accuracy\"],\n-)\n-\n-gpt2_lm.fit(train_ds, epochs=num_epochs)\n-\n-\"\"\"\n-Let's check the result!\n-\"\"\"\n-\n-output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200)\n-print(output)\n-\n-\"\"\"\n-Not bad 😀\n-\"\"\"\n\n@@ -1,187 +0,0 @@\n-\"\"\"\n-Title: PixelCNN\n-Author: [ADMoreau](https://github.com/ADMoreau)\n-Date created: 2020/05/17\n-Last modified: 2020/05/23\n-Description: PixelCNN implemented in Keras.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-PixelCNN is a generative model proposed in 2016 by van den Oord et al.\n-(reference: [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)).\n-It is designed to generate images (or other data types) iteratively\n-from an input vector where the probability distribution of prior elements dictates the\n-probability distribution of later elements. In the following example, images are generated\n-in this fashion, pixel-by-pixel, via a masked convolution kernel that only looks at data\n-from previously generated pixels (origin at the top left) to generate later pixels.\n-During inference, the output of the network is used as a probability ditribution\n-from which new pixel values are sampled to generate a new image\n-(here, with MNIST, the pixels values are either black or white).\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-from keras import ops\n-from tqdm import tqdm\n-\n-\"\"\"\n-## Getting the Data\n-\"\"\"\n-\n-# Model / data parameters\n-num_classes = 10\n-input_shape = (28, 28, 1)\n-n_residual_blocks = 5\n-# The data, split between train and test sets\n-(x, _), (y, _) = keras.datasets.mnist.load_data()\n-# Concatenate all the images together\n-data = np.concatenate((x, y), axis=0)\n-# Round all pixel values less than 33% of the max 256 value to 0\n-# anything above this value gets rounded up to 1 so that all values are either\n-# 0 or 1\n-data = np.where(data < (0.33 * 256), 0, 1)\n-data = data.astype(np.float32)\n-\n-\"\"\"\n-## Create two classes for the requisite Layers for the model\n-\"\"\"\n-\n-\n-# The first layer is the PixelCNN layer. This layer simply\n-# builds on the 2D convolutional layer, but includes masking.\n-class PixelConvLayer(layers.Layer):\n-    def __init__(self, mask_type, **kwargs):\n-        super().__init__()\n-        self.mask_type = mask_type\n-        self.conv = layers.Conv2D(**kwargs)\n-\n-    def build(self, input_shape):\n-        # Build the conv2d layer to initialize kernel variables\n-        self.conv.build(input_shape)\n-        # Use the initialized kernel to create the mask\n-        kernel_shape = ops.shape(self.conv.kernel)\n-        self.mask = np.zeros(shape=kernel_shape)\n-        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n-        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n-        if self.mask_type == \"B\":\n-            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n-\n-    def call(self, inputs):\n-        self.conv.kernel.assign(self.conv.kernel * self.mask)\n-        return self.conv(inputs)\n-\n-\n-# Next, we build our residual block layer.\n-# This is just a normal residual block, but based on the PixelConvLayer.\n-class ResidualBlock(keras.layers.Layer):\n-    def __init__(self, filters, **kwargs):\n-        super().__init__(**kwargs)\n-        self.conv1 = keras.layers.Conv2D(\n-            filters=filters, kernel_size=1, activation=\"relu\"\n-        )\n-        self.pixel_conv = PixelConvLayer(\n-            mask_type=\"B\",\n-            filters=filters // 2,\n-            kernel_size=3,\n-            activation=\"relu\",\n-            padding=\"same\",\n-        )\n-        self.conv2 = keras.layers.Conv2D(\n-            filters=filters, kernel_size=1, activation=\"relu\"\n-        )\n-\n-    def call(self, inputs):\n-        x = self.conv1(inputs)\n-        x = self.pixel_conv(x)\n-        x = self.conv2(x)\n-        return keras.layers.add([inputs, x])\n-\n-\n-\"\"\"\n-## Build the model based on the original paper\n-\"\"\"\n-\n-inputs = keras.Input(shape=input_shape)\n-x = PixelConvLayer(\n-    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n-)(inputs)\n-\n-for _ in range(n_residual_blocks):\n-    x = ResidualBlock(filters=128)(x)\n-\n-for _ in range(2):\n-    x = PixelConvLayer(\n-        mask_type=\"B\",\n-        filters=128,\n-        kernel_size=1,\n-        strides=1,\n-        activation=\"relu\",\n-        padding=\"valid\",\n-    )(x)\n-\n-out = keras.layers.Conv2D(\n-    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n-)(x)\n-\n-pixel_cnn = keras.Model(inputs, out)\n-adam = keras.optimizers.Adam(learning_rate=0.0005)\n-pixel_cnn.compile(optimizer=adam, loss=\"binary_crossentropy\")\n-\n-pixel_cnn.summary()\n-pixel_cnn.fit(\n-    x=data, y=data, batch_size=128, epochs=50, validation_split=0.1, verbose=2\n-)\n-\n-\"\"\"\n-## Demonstration\n-\n-The PixelCNN cannot generate the full image at once. Instead, it must generate each pixel in\n-order, append the last generated pixel to the current image, and feed the image back into the\n-model to repeat the process.\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-# Create an empty array of pixels.\n-batch = 4\n-pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n-batch, rows, cols, channels = pixels.shape\n-\n-# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n-for row in tqdm(range(rows)):\n-    for col in range(cols):\n-        for channel in range(channels):\n-            # Feed the whole array and retrieving the pixel value probabilities for the next\n-            # pixel.\n-            probs = pixel_cnn.predict(pixels)[:, row, col, channel]\n-            # Use the probabilities to pick pixel values and append the values to the image\n-            # frame.\n-            pixels[:, row, col, channel] = ops.ceil(\n-                probs - keras.random.uniform(probs.shape)\n-            )\n-\n-\n-def deprocess_image(x):\n-    # Stack the single channeled black and white image to rgb values.\n-    x = np.stack((x, x, x), 2)\n-    # Undo preprocessing\n-    x *= 255.0\n-    # Convert to uint8 and clip to the valid range [0, 255]\n-    x = np.clip(x, 0, 255).astype(\"uint8\")\n-    return x\n-\n-\n-# Iterate over the generated images and plot them with matplotlib.\n-for i, pic in enumerate(pixels):\n-    keras.utils.save_img(\n-        \"generated_image_{}.png\".format(i), deprocess_image(np.squeeze(pic, -1))\n-    )\n-\n-display(Image(\"generated_image_0.png\"))\n-display(Image(\"generated_image_1.png\"))\n-display(Image(\"generated_image_2.png\"))\n-display(Image(\"generated_image_3.png\"))\n\n@@ -1,419 +0,0 @@\n-\"\"\"\n-Title: GPT text generation from scratch with KerasNLP\n-Author: [Jesse Chan](https://github.com/jessechancy)\n-Date created: 2022/07/25\n-Last modified: 2022/07/25\n-Description: Using KerasNLP to train a mini-GPT model for text generation.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-In this example, we will use KerasNLP to build a scaled down Generative\n-Pre-Trained (GPT) model. GPT is a Transformer-based model that allows you to generate\n-sophisticated text from a prompt.\n-\n-We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,\n-which is a dataset made from several novels. It is a good dataset for this example since\n-it has a small vocabulary and high word frequency, which is beneficial when training a\n-model with few parameters.\n-\n-This example combines concepts from\n-[Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n-with KerasNLP abstractions. We will demonstrate how KerasNLP tokenization, layers and\n-metrics simplify the training\n-process, and then show how to generate output text using the KerasNLP sampling utilities.\n-\n-Note: If you are running this example on a Colab,\n-make sure to enable GPU runtime for faster training.\n-\n-This example requires KerasNLP. You can install it via the following command:\n-`pip install keras-nlp`\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import keras_nlp\n-import keras\n-\n-import tensorflow.data as tf_data\n-import tensorflow.strings as tf_strings\n-\n-\"\"\"\n-## Settings & hyperparameters\n-\"\"\"\n-\n-# Data\n-BATCH_SIZE = 64\n-MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n-SEQ_LEN = 128  # Length of training sequences, in tokens\n-\n-# Model\n-EMBED_DIM = 256\n-FEED_FORWARD_DIM = 128\n-NUM_HEADS = 3\n-NUM_LAYERS = 2\n-VOCAB_SIZE = 5000  # Limits parameters in model.\n-\n-# Training\n-EPOCHS = 5\n-\n-# Inference\n-NUM_TOKENS_TO_GENERATE = 80\n-\n-\"\"\"\n-## Load the data\n-\n-Now, let's download the dataset! The SimpleBooks dataset consists of 1,573 Gutenberg books, and has\n-one of the smallest vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k,\n-a third of WikiText-103's, with around the same number of tokens (~100M). This makes it easy to fit a small model.\n-\"\"\"\n-\n-keras.utils.get_file(\n-    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n-    extract=True,\n-)\n-dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n-\n-# Load simplebooks-92 train set and filter out short lines.\n-raw_train_ds = (\n-    tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n-    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n-    .batch(BATCH_SIZE)\n-    .shuffle(buffer_size=256)\n-)\n-\n-# Load simplebooks-92 validation set and filter out short lines.\n-raw_val_ds = (\n-    tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n-    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n-    .batch(BATCH_SIZE)\n-)\n-\n-\"\"\"\n-## Train the tokenizer\n-\n-We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`,\n-which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as\n-we will see later on\n-that it has a large effect on the number of model parameters. We also don't want to include\n-*too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In\n-addition, three tokens are reserved in the vocabulary:\n-\n-- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both\n-`reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider\n-`0`/`vocab[0]` as the default padding.\n-- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n-`WordPieceTokenizer`.\n-- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token\n-representing the beginning of each line of training data.\n-\"\"\"\n-\n-# Train tokenizer vocabulary\n-vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n-    raw_train_ds,\n-    vocabulary_size=VOCAB_SIZE,\n-    lowercase=True,\n-    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n-)\n-\n-\"\"\"\n-## Load tokenizer\n-\n-We use the vocabulary data to initialize\n-`keras_nlp.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient\n-implementation of the WordPiece algorithm used by BERT and other models. It will strip,\n-lower-case and do other irreversible preprocessing operations.\n-\"\"\"\n-\n-tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n-    vocabulary=vocab,\n-    sequence_length=SEQ_LEN,\n-    lowercase=True,\n-)\n-\n-\"\"\"\n-## Tokenize data\n-\n-We preprocess the dataset by tokenizing and splitting it into `features` and `labels`.\n-\"\"\"\n-\n-# packer adds a start token\n-start_packer = keras_nlp.layers.StartEndPacker(\n-    sequence_length=SEQ_LEN,\n-    start_value=tokenizer.token_to_id(\"[BOS]\"),\n-)\n-\n-\n-def preprocess(inputs):\n-    outputs = tokenizer(inputs)\n-    features = start_packer(outputs)\n-    labels = outputs\n-    return features, labels\n-\n-\n-# Tokenize and split into train and label sequences.\n-train_ds = raw_train_ds.map(\n-    preprocess, num_parallel_calls=tf_data.AUTOTUNE\n-).prefetch(tf_data.AUTOTUNE)\n-val_ds = raw_val_ds.map(\n-    preprocess, num_parallel_calls=tf_data.AUTOTUNE\n-).prefetch(tf_data.AUTOTUNE)\n-\n-\"\"\"\n-## Build the model\n-\n-We create our scaled down GPT model with the following layers:\n-\n-- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n-for the token and its position.\n-- Multiple `keras_nlp.layers.TransformerDecoder` layers, with the default causal masking.\n-The layer has no cross-attention when run with decoder sequence only.\n-- One final dense linear layer\n-\"\"\"\n-\n-inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n-# Embedding.\n-embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n-    vocabulary_size=VOCAB_SIZE,\n-    sequence_length=SEQ_LEN,\n-    embedding_dim=EMBED_DIM,\n-    mask_zero=True,\n-)\n-x = embedding_layer(inputs)\n-# Transformer decoders.\n-for _ in range(NUM_LAYERS):\n-    decoder_layer = keras_nlp.layers.TransformerDecoder(\n-        num_heads=NUM_HEADS,\n-        intermediate_dim=FEED_FORWARD_DIM,\n-    )\n-    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n-# Output.\n-outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n-model = keras.Model(inputs=inputs, outputs=outputs)\n-loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n-model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n-\n-\"\"\"\n-Let's take a look at our model summary - a large majority of the\n-parameters are in the `token_and_position_embedding` and the output `dense` layer!\n-This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n-while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much.\n-\"\"\"\n-\n-model.summary()\n-\n-\"\"\"\n-## Training\n-\n-Now that we have our model, let's train it with the `fit()` method.\n-\"\"\"\n-\n-model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n-\n-\"\"\"\n-## Inference\n-\n-With our trained model, we can test it out to gauge its performance. To do this\n-we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n-and progressively sample the model by making predictions for each subsequent\n-token in a loop.\n-\n-To start lets build a prompt with the same shape as our model inputs, containing\n-only the `\"[BOS]\"` token.\n-\"\"\"\n-\n-# The \"packer\" layers adds the [BOS] token for us.\n-prompt_tokens = start_packer(tokenizer([\"\"]))\n-prompt_tokens\n-\n-\"\"\"\n-We will use the `keras_nlp.samplers` module for inference, which requires a\n-callback function wrapping the model we just trained. This wrapper calls\n-the model and returns the logit predictions for the current token we are\n-generating.\n-\n-Note: There are two pieces of more advanced functionality available when\n-defining your callback. The first is the ability to take in a `cache` of states\n-computed in previous generation steps, which can be used to speed up generation.\n-The second is the ability to output the final dense \"hidden state\" of each\n-generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which\n-avoids repetition by penalizing repeated hidden states. Both are optional, and\n-we will ignore them for now.\n-\"\"\"\n-\n-\n-def next(prompt, cache, index):\n-    logits = model(prompt)[:, index - 1, :]\n-    # Ignore hidden states for now; only needed for contrastive search.\n-    hidden_states = None\n-    return logits, hidden_states, cache\n-\n-\n-\"\"\"\n-Creating the wrapper function is the most complex part of using these functions. Now that\n-it's done, let's test out the different utilities, starting with greedy search.\n-\"\"\"\n-\n-\"\"\"\n-### Greedy search\n-\n-We greedily pick the most probable token at each timestep. In other words, we get the\n-argmax of the model output.\n-\"\"\"\n-\n-sampler = keras_nlp.samplers.GreedySampler()\n-output_tokens = sampler(\n-    next=next,\n-    prompt=prompt_tokens,\n-    index=1,  # Start sampling immediately after the [BOS] token.\n-)\n-txt = tokenizer.detokenize(output_tokens)\n-print(f\"Greedy search generated text: \\n{txt}\\n\")\n-\n-\"\"\"\n-As you can see, greedy search starts out making some sense, but quickly starts repeating\n-itself. This is a common problem with text generation that can be fixed by some of the\n-probabilistic text generation utilities shown later on!\n-\"\"\"\n-\n-\"\"\"\n-### Beam search\n-\n-At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n-each timestep, and predicts the best next token from all sequences. It is an improvement\n-over greedy search since it stores more possibilities. However, it is less efficient than\n-greedy search since it has to compute and store multiple potential sequences.\n-\n-**Note:** beam search with `num_beams=1` is identical to greedy search.\n-\"\"\"\n-\n-sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n-output_tokens = sampler(\n-    next=next,\n-    prompt=prompt_tokens,\n-    index=1,\n-)\n-txt = tokenizer.detokenize(output_tokens)\n-print(f\"Beam search generated text: \\n{txt}\\n\")\n-\n-\"\"\"\n-Similar to greedy search, beam search quickly starts repeating itself, since it is still\n-a deterministic method.\n-\"\"\"\n-\n-\"\"\"\n-### Random search\n-\n-Random search is our first probabilistic method. At each time step, it samples the next\n-token using the softmax probabilities provided by the model.\n-\"\"\"\n-\n-sampler = keras_nlp.samplers.RandomSampler()\n-output_tokens = sampler(\n-    next=next,\n-    prompt=prompt_tokens,\n-    index=1,\n-)\n-txt = tokenizer.detokenize(output_tokens)\n-print(f\"Random search generated text: \\n{txt}\\n\")\n-\n-\"\"\"\n-Voilà, no repetitions! However, with random search, we may see some nonsensical words\n-appearing since any word in the vocabulary has a chance of appearing with this sampling\n-method. This is fixed by our next search utility, top-k search.\n-\"\"\"\n-\n-\"\"\"\n-### Top-K search\n-\n-Similar to random search, we sample the next token from the probability distribution\n-provided by the model. The only difference is that here, we select out the top `k` most\n-probable tokens, and distribute the probability mass over them before sampling. This way,\n-we won't be sampling from low probability tokens, and hence we would have less\n-nonsensical words!\n-\"\"\"\n-\n-sampler = keras_nlp.samplers.TopKSampler(k=10)\n-output_tokens = sampler(\n-    next=next,\n-    prompt=prompt_tokens,\n-    index=1,\n-)\n-txt = tokenizer.detokenize(output_tokens)\n-print(f\"Top-K search generated text: \\n{txt}\\n\")\n-\n-\"\"\"\n-### Top-P search\n-\n-Even with the top-k search, there is something to improve upon. With top-k search, the\n-number `k` is fixed, which means it selects the same number of tokens for any probability\n-distribution. Consider two scenarios, one where the probability mass is concentrated over\n-2 words and another where the probability mass is evenly concentrated across 10. Should\n-we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n-\n-This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n-`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n-dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n-90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n-top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n-similarly filter out the top 10 tokens to sample from.\n-\"\"\"\n-\n-sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n-output_tokens = sampler(\n-    next=next,\n-    prompt=prompt_tokens,\n-    index=1,\n-)\n-txt = tokenizer.detokenize(output_tokens)\n-print(f\"Top-P search generated text: \\n{txt}\\n\")\n-\n-\"\"\"\n-### Using callbacks for text generation\n-\n-We can also wrap the utilities in a callback, which allows you to print out a prediction\n-sequence for every epoch of the model! Here is an example of a callback for top-k search:\n-\"\"\"\n-\n-\n-class TopKTextGenerator(keras.callbacks.Callback):\n-    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n-\n-    def __init__(self, k):\n-        self.sampler = keras_nlp.samplers.TopKSampler(k)\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        output_tokens = self.sampler(\n-            next=next,\n-            prompt=prompt_tokens,\n-            index=1,\n-        )\n-        txt = tokenizer.detokenize(output_tokens)\n-        print(f\"Top-K search generated text: \\n{txt}\\n\")\n-\n-\n-text_generation_callback = TopKTextGenerator(k=10)\n-# Dummy training loop to demonstrate callback.\n-model.fit(\n-    train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback]\n-)\n-\n-\"\"\"\n-## Conclusion\n-\n-To recap, in this example, we use KerasNLP layers to train a sub-word vocabulary,\n-tokenize training data, create a miniature GPT model, and perform inference with the\n-text generation library.\n-\n-If you would like to understand how Transformers work, or learn more about training the\n-full GPT model, here are some further readings:\n-\n-- Attention Is All You Need [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n-- GPT-3 Paper [Brown et al., 2020](https://arxiv.org/abs/2005.14165)\n-\"\"\"\n\n@@ -1,314 +0,0 @@\n-\"\"\"\n-Title: Text generation with a miniature GPT\n-Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n-Date created: 2020/05/29\n-Last modified: 2020/05/29\n-Description: Implement a miniature version of GPT and train it to generate text.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to implement an autoregressive language model\n-using a miniature version of the GPT model.\n-The model consists of a single Transformer block with causal masking\n-in its attention layer.\n-We use the text from the IMDB sentiment classification dataset for training\n-and generate new movie reviews for a given prompt.\n-When using this script with your own dataset, make sure it has at least\n-1 million words.\n-\n-This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n-with TensorFlow 2.3 or higher.\n-\n-**References:**\n-\n-- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n-- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n-- [GPT-3](https://arxiv.org/abs/2005.14165)\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-# We set the backend to TensorFlow. The code works with\n-# both `tensorflow` and `torch`. It does not work with JAX\n-# due to the behavior of `jax.numpy.tile` in a jit scope\n-# (used in `causal_attention_mask()`: `tile` in JAX does\n-# not support a dynamic `reps` argument.\n-# You can make the code work in JAX by wrapping the\n-# inside of the `causal_attention_mask` function in\n-# a decorator to prevent jit compilation:\n-# `with jax.ensure_compile_time_eval():`.\n-import os\n-os.environ['KERAS_BACKEND'] = 'tensorflow'\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-from keras.layers import TextVectorization\n-import numpy as np\n-import os\n-import string\n-import random\n-import tensorflow\n-import tensorflow.data as tf_data\n-import tensorflow.strings as tf_strings\n-\n-\n-\"\"\"\n-## Implement a Transformer block as a layer\n-\"\"\"\n-\n-\n-def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n-    \"\"\"\n-    Mask the upper half of the dot product matrix in self attention.\n-    This prevents flow of information from future tokens to current token.\n-    1's in the lower triangle, counting from the lower right corner.\n-    \"\"\"\n-    i = ops.arange(n_dest)[:, None]\n-    j = ops.arange(n_src)\n-    m = i >= j - n_src + n_dest\n-    mask = ops.cast(m, dtype)\n-    mask = ops.reshape(mask, [1, n_dest, n_src])\n-    mult = ops.concatenate(\n-        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n-    )\n-    return ops.tile(mask, mult)\n-\n-\n-class TransformerBlock(layers.Layer):\n-    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n-        super().__init__()\n-        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n-        self.ffn = keras.Sequential(\n-            [\n-                layers.Dense(ff_dim, activation=\"relu\"),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n-        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n-        self.dropout1 = layers.Dropout(rate)\n-        self.dropout2 = layers.Dropout(rate)\n-\n-    def call(self, inputs):\n-        input_shape = ops.shape(inputs)\n-        batch_size = input_shape[0]\n-        seq_len = input_shape[1]\n-        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n-        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n-        attention_output = self.dropout1(attention_output)\n-        out1 = self.layernorm1(inputs + attention_output)\n-        ffn_output = self.ffn(out1)\n-        ffn_output = self.dropout2(ffn_output)\n-        return self.layernorm2(out1 + ffn_output)\n-\n-\n-\"\"\"\n-## Implement an embedding layer\n-\n-Create two separate embedding layers: one for tokens and one for token index\n-(positions).\n-\"\"\"\n-\n-\n-class TokenAndPositionEmbedding(layers.Layer):\n-    def __init__(self, maxlen, vocab_size, embed_dim):\n-        super().__init__()\n-        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n-        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n-\n-    def call(self, x):\n-        maxlen = ops.shape(x)[-1]\n-        positions = ops.arange(0, maxlen, 1)\n-        positions = self.pos_emb(positions)\n-        x = self.token_emb(x)\n-        return x + positions\n-\n-\n-\"\"\"\n-## Implement the miniature GPT model\n-\"\"\"\n-vocab_size = 20000  # Only consider the top 20k words\n-maxlen = 80  # Max sequence size\n-embed_dim = 256  # Embedding size for each token\n-num_heads = 2  # Number of attention heads\n-feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n-\n-\n-def create_model():\n-    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n-    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n-    x = embedding_layer(inputs)\n-    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n-    x = transformer_block(x)\n-    outputs = layers.Dense(vocab_size)(x)\n-    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n-    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-    model.compile(\n-        \"adam\",\n-        loss=[loss_fn, None],\n-    )  # No loss and optimization based on word embeddings from transformer block\n-    return model\n-\n-\n-\"\"\"\n-## Prepare the data for word-level language modelling\n-\n-Download the IMDB dataset and combine training and validation sets for a text\n-generation task.\n-\"\"\"\n-\n-\"\"\"shell\n-curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n-tar -xf aclImdb_v1.tar.gz\n-\"\"\"\n-\n-\n-batch_size = 128\n-\n-# The dataset contains each review in a separate text file\n-# The text files are present in four different folders\n-# Create a list all files\n-filenames = []\n-directories = [\n-    \"aclImdb/train/pos\",\n-    \"aclImdb/train/neg\",\n-    \"aclImdb/test/pos\",\n-    \"aclImdb/test/neg\",\n-]\n-for dir in directories:\n-    for f in os.listdir(dir):\n-        filenames.append(os.path.join(dir, f))\n-\n-print(f\"{len(filenames)} files\")\n-\n-# Create a dataset from text files\n-random.shuffle(filenames)\n-text_ds = tf_data.TextLineDataset(filenames)\n-text_ds = text_ds.shuffle(buffer_size=256)\n-text_ds = text_ds.batch(batch_size)\n-\n-\n-def custom_standardization(input_string):\n-    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n-    lowercased = tf_strings.lower(input_string)\n-    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n-    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n-\n-\n-# Create a vectorization layer and adapt it to the text\n-vectorize_layer = TextVectorization(\n-    standardize=custom_standardization,\n-    max_tokens=vocab_size - 1,\n-    output_mode=\"int\",\n-    output_sequence_length=maxlen + 1,\n-)\n-vectorize_layer.adapt(text_ds)\n-vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n-\n-\n-def prepare_lm_inputs_labels(text):\n-    \"\"\"\n-    Shift word sequences by 1 position so that the target for position (i) is\n-    word at position (i+1). The model will use all words up till position (i)\n-    to predict the next word.\n-    \"\"\"\n-    text = tensorflow.expand_dims(text, -1)\n-    tokenized_sentences = vectorize_layer(text)\n-    x = tokenized_sentences[:, :-1]\n-    y = tokenized_sentences[:, 1:]\n-    return x, y\n-\n-\n-text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n-text_ds = text_ds.prefetch(tf_data.AUTOTUNE)\n-\n-\n-\"\"\"\n-## Implement a Keras callback for generating text\n-\"\"\"\n-\n-\n-class TextGenerator(keras.callbacks.Callback):\n-    \"\"\"A callback to generate text from a trained model.\n-    1. Feed some starting prompt to the model\n-    2. Predict probabilities for the next token\n-    3. Sample the next token and add it to the next input\n-\n-    Arguments:\n-        max_tokens: Integer, the number of tokens to be generated after prompt.\n-        start_tokens: List of integers, the token indices for the starting prompt.\n-        index_to_word: List of strings, obtained from the TextVectorization layer.\n-        top_k: Integer, sample from the `top_k` token predictions.\n-        print_every: Integer, print after this many epochs.\n-    \"\"\"\n-\n-    def __init__(\n-        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n-    ):\n-        self.max_tokens = max_tokens\n-        self.start_tokens = start_tokens\n-        self.index_to_word = index_to_word\n-        self.print_every = print_every\n-        self.k = top_k\n-\n-    def sample_from(self, logits):\n-        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n-        indices = np.asarray(indices).astype(\"int32\")\n-        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n-        preds = np.asarray(preds).astype(\"float32\")\n-        return np.random.choice(indices, p=preds)\n-\n-    def detokenize(self, number):\n-        return self.index_to_word[number]\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        start_tokens = [_ for _ in self.start_tokens]\n-        if (epoch + 1) % self.print_every != 0:\n-            return\n-        num_tokens_generated = 0\n-        tokens_generated = []\n-        while num_tokens_generated <= self.max_tokens:\n-            pad_len = maxlen - len(start_tokens)\n-            sample_index = len(start_tokens) - 1\n-            if pad_len < 0:\n-                x = start_tokens[:maxlen]\n-                sample_index = maxlen - 1\n-            elif pad_len > 0:\n-                x = start_tokens + [0] * pad_len\n-            else:\n-                x = start_tokens\n-            x = np.array([x])\n-            y, _ = self.model.predict(x, verbose=0)\n-            sample_token = self.sample_from(y[0][sample_index])\n-            tokens_generated.append(sample_token)\n-            start_tokens.append(sample_token)\n-            num_tokens_generated = len(tokens_generated)\n-        txt = \" \".join(\n-            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n-        )\n-        print(f\"generated text:\\n{txt}\\n\")\n-\n-\n-# Tokenize starting prompt\n-word_to_index = {}\n-for index, word in enumerate(vocab):\n-    word_to_index[word] = index\n-\n-start_prompt = \"this movie is\"\n-start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n-num_tokens_generated = 40\n-text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n-\n-\n-\"\"\"\n-## Train the model\n-\n-Note: This code should preferably be run on GPU.\n-\"\"\"\n-\n-model = create_model()\n-\n-model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])\n\n@@ -1,253 +0,0 @@\n-\"\"\"\n-Title: Sequence to sequence learning for performing number addition\n-Author: [Smerity](https://twitter.com/Smerity) and others\n-Date created: 2015/08/17\n-Last modified: 2020/04/17\n-Description: A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\".\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In this example, we train a model to learn to add two numbers, provided as strings.\n-\n-**Example:**\n-\n-- Input: \"535+61\"\n-- Output: \"596\"\n-\n-Input may optionally be reversed, which was shown to increase performance in many tasks\n- in: [Learning to Execute](http://arxiv.org/abs/1410.4615) and\n-[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n-\n-Theoretically, sequence order inversion introduces shorter term dependencies between\n- source and target for this problem.\n-\n-**Results:**\n-\n-For two digits (reversed):\n-\n-+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n-\n-Three digits (reversed):\n-\n-+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n-\n-Four digits (reversed):\n-\n-+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n-\n-Five digits (reversed):\n-\n-+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-import numpy as np\n-\n-# Parameters for the model and dataset.\n-TRAINING_SIZE = 50000\n-DIGITS = 3\n-REVERSE = True\n-\n-# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n-# int is DIGITS.\n-MAXLEN = DIGITS + 1 + DIGITS\n-\n-\"\"\"\n-## Generate the data\n-\"\"\"\n-\n-\n-class CharacterTable:\n-    \"\"\"Given a set of characters:\n-    + Encode them to a one-hot integer representation\n-    + Decode the one-hot or integer representation to their character output\n-    + Decode a vector of probabilities to their character output\n-    \"\"\"\n-\n-    def __init__(self, chars):\n-        \"\"\"Initialize character table.\n-        # Arguments\n-            chars: Characters that can appear in the input.\n-        \"\"\"\n-        self.chars = sorted(set(chars))\n-        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n-        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n-\n-    def encode(self, C, num_rows):\n-        \"\"\"One-hot encode given string C.\n-        # Arguments\n-            C: string, to be encoded.\n-            num_rows: Number of rows in the returned one-hot encoding. This is\n-                used to keep the # of rows for each data the same.\n-        \"\"\"\n-        x = np.zeros((num_rows, len(self.chars)))\n-        for i, c in enumerate(C):\n-            x[i, self.char_indices[c]] = 1\n-        return x\n-\n-    def decode(self, x, calc_argmax=True):\n-        \"\"\"Decode the given vector or 2D array to their character output.\n-        # Arguments\n-            x: A vector or a 2D array of probabilities or one-hot representations;\n-                or a vector of character indices (used with `calc_argmax=False`).\n-            calc_argmax: Whether to find the character index with maximum\n-                probability, defaults to `True`.\n-        \"\"\"\n-        if calc_argmax:\n-            x = x.argmax(axis=-1)\n-        return \"\".join(self.indices_char[x] for x in x)\n-\n-\n-# All the numbers, plus sign and space for padding.\n-chars = \"0123456789+ \"\n-ctable = CharacterTable(chars)\n-\n-questions = []\n-expected = []\n-seen = set()\n-print(\"Generating data...\")\n-while len(questions) < TRAINING_SIZE:\n-    f = lambda: int(\n-        \"\".join(\n-            np.random.choice(list(\"0123456789\"))\n-            for i in range(np.random.randint(1, DIGITS + 1))\n-        )\n-    )\n-    a, b = f(), f()\n-    # Skip any addition questions we've already seen\n-    # Also skip any such that x+Y == Y+x (hence the sorting).\n-    key = tuple(sorted((a, b)))\n-    if key in seen:\n-        continue\n-    seen.add(key)\n-    # Pad the data with spaces such that it is always MAXLEN.\n-    q = \"{}+{}\".format(a, b)\n-    query = q + \" \" * (MAXLEN - len(q))\n-    ans = str(a + b)\n-    # Answers can be of maximum size DIGITS + 1.\n-    ans += \" \" * (DIGITS + 1 - len(ans))\n-    if REVERSE:\n-        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n-        # space used for padding.)\n-        query = query[::-1]\n-    questions.append(query)\n-    expected.append(ans)\n-print(\"Total questions:\", len(questions))\n-\n-\"\"\"\n-## Vectorize the data\n-\"\"\"\n-\n-print(\"Vectorization...\")\n-x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n-y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n-for i, sentence in enumerate(questions):\n-    x[i] = ctable.encode(sentence, MAXLEN)\n-for i, sentence in enumerate(expected):\n-    y[i] = ctable.encode(sentence, DIGITS + 1)\n-\n-# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n-# digits.\n-indices = np.arange(len(y))\n-np.random.shuffle(indices)\n-x = x[indices]\n-y = y[indices]\n-\n-# Explicitly set apart 10% for validation data that we never train over.\n-split_at = len(x) - len(x) // 10\n-(x_train, x_val) = x[:split_at], x[split_at:]\n-(y_train, y_val) = y[:split_at], y[split_at:]\n-\n-print(\"Training Data:\")\n-print(x_train.shape)\n-print(y_train.shape)\n-\n-print(\"Validation Data:\")\n-print(x_val.shape)\n-print(y_val.shape)\n-\n-\"\"\"\n-## Build the model\n-\"\"\"\n-\n-print(\"Build model...\")\n-num_layers = 1  # Try to add more LSTM layers!\n-\n-model = keras.Sequential()\n-# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n-# Note: In a situation where your input sequences have a variable length,\n-# use input_shape=(None, num_feature).\n-model.add(layers.Input((MAXLEN, len(chars))))\n-model.add(layers.LSTM(128))\n-# As the decoder RNN's input, repeatedly provide with the last output of\n-# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n-# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n-model.add(layers.RepeatVector(DIGITS + 1))\n-# The decoder RNN could be multiple layers stacked or a single layer.\n-for _ in range(num_layers):\n-    # By setting return_sequences to True, return not only the last output but\n-    # all the outputs so far in the form of (num_samples, timesteps,\n-    # output_dim). This is necessary as TimeDistributed in the below expects\n-    # the first dimension to be the timesteps.\n-    model.add(layers.LSTM(128, return_sequences=True))\n-\n-# Apply a dense layer to the every temporal slice of an input. For each of step\n-# of the output sequence, decide which character should be chosen.\n-model.add(layers.Dense(len(chars), activation=\"softmax\"))\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-model.summary()\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 30\n-batch_size = 32\n-\n-\n-# Train the model each generation and show predictions against the validation\n-# dataset.\n-for epoch in range(1, epochs):\n-    print()\n-    print(\"Iteration\", epoch)\n-    model.fit(\n-        x_train,\n-        y_train,\n-        batch_size=batch_size,\n-        epochs=1,\n-        validation_data=(x_val, y_val),\n-    )\n-    # Select 10 samples from the validation set at random so we can visualize\n-    # errors.\n-    for i in range(10):\n-        ind = np.random.randint(0, len(x_val))\n-        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n-        preds = np.argmax(model.predict(rowx), axis=-1)\n-        q = ctable.decode(rowx[0])\n-        correct = ctable.decode(rowy[0])\n-        guess = ctable.decode(preds[0], calc_argmax=False)\n-        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n-        print(\"T\", correct, end=\" \")\n-        if correct == guess:\n-            print(\"☑ \" + guess)\n-        else:\n-            print(\"☒ \" + guess)\n-\n-\"\"\"\n-You'll get to 99+% validation accuracy after ~30 epochs.\n-\n-Example available on HuggingFace.\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/🤗%20Model-Addition%20LSTM-black.svg)](https://huggingface.co/keras-io/addition-lstm) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-Addition%20LSTM-black.svg)](https://huggingface.co/spaces/keras-io/addition-lstm) |\n-\"\"\"\n\n@@ -1,62 +0,0 @@\n-\"\"\"\n-Title: Bidirectional LSTM on IMDB\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/05/03\n-Last modified: 2020/05/03\n-Description: Train a 2-layer bidirectional LSTM on the IMDB movie review sentiment classification dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-\n-max_features = 20000  # Only consider the top 20k words\n-maxlen = 200  # Only consider the first 200 words of each movie review\n-\n-\"\"\"\n-## Build the model\n-\"\"\"\n-\n-# Input for variable-length sequences of integers\n-inputs = keras.Input(shape=(None,), dtype=\"int32\")\n-# Embed each integer in a 128-dimensional vector\n-x = layers.Embedding(max_features, 128)(inputs)\n-# Add 2 bidirectional LSTMs\n-x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n-x = layers.Bidirectional(layers.LSTM(64))(x)\n-# Add a classifier\n-outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n-model = keras.Model(inputs, outputs)\n-model.summary()\n-\n-\"\"\"\n-## Load the IMDB movie review sentiment data\n-\"\"\"\n-\n-(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n-    num_words=max_features\n-)\n-print(len(x_train), \"Training sequences\")\n-print(len(x_val), \"Validation sequences\")\n-# Use pad_sequence to standardize sequence length:\n-# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n-x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n-x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n-\n-\"\"\"\n-## Train and evaluate the model\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/bidirectional-lstm-imdb)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/bidirectional_lstm_imdb).\n-\"\"\"\n-\n-model.compile(\n-    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n-)\n-model.fit(\n-    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n-)\n\n@@ -1,288 +0,0 @@\n-\"\"\"\n-Title: Character-level recurrent sequence-to-sequence model\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2017/09/29\n-Last modified: 2023/11/22\n-Description: Character-level recurrent sequence-to-sequence model.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to implement a basic character-level\n-recurrent sequence-to-sequence model. We apply it to translating\n-short English sentences into short French sentences,\n-character-by-character. Note that it is fairly unusual to\n-do character-level machine translation, as word-level\n-models are more common in this domain.\n-\n-**Summary of the algorithm**\n-\n-- We start with input sequences from a domain (e.g. English sentences)\n-    and corresponding target sequences from another domain\n-    (e.g. French sentences).\n-- An encoder LSTM turns input sequences to 2 state vectors\n-    (we keep the last LSTM state and discard the outputs).\n-- A decoder LSTM is trained to turn the target sequences into\n-    the same sequence but offset by one timestep in the future,\n-    a training process called \"teacher forcing\" in this context.\n-    It uses as initial state the state vectors from the encoder.\n-    Effectively, the decoder learns to generate `targets[t+1...]`\n-    given `targets[...t]`, conditioned on the input sequence.\n-- In inference mode, when we want to decode unknown input sequences, we:\n-    - Encode the input sequence into state vectors\n-    - Start with a target sequence of size 1\n-        (just the start-of-sequence character)\n-    - Feed the state vectors and 1-char target sequence\n-        to the decoder to produce predictions for the next character\n-    - Sample the next character using these predictions\n-        (we simply use argmax).\n-    - Append the sampled character to the target sequence\n-    - Repeat until we generate the end-of-sequence character or we\n-        hit the character limit.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-import os\n-from pathlib import Path\n-\n-\"\"\"\n-## Download the data\n-\"\"\"\n-\n-fpath = keras.utils.get_file(\n-    origin=\"http://www.manythings.org/anki/fra-eng.zip\"\n-)\n-dirpath = Path(fpath).parent.absolute()\n-os.system(f\"unzip -q {fpath} -d {dirpath}\")\n-\n-\"\"\"\n-## Configuration\n-\"\"\"\n-\n-batch_size = 64  # Batch size for training.\n-epochs = 100  # Number of epochs to train for.\n-latent_dim = 256  # Latent dimensionality of the encoding space.\n-num_samples = 10000  # Number of samples to train on.\n-# Path to the data txt file on disk.\n-data_path = os.path.join(dirpath, \"fra.txt\")\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-# Vectorize the data.\n-input_texts = []\n-target_texts = []\n-input_characters = set()\n-target_characters = set()\n-with open(data_path, \"r\", encoding=\"utf-8\") as f:\n-    lines = f.read().split(\"\\n\")\n-for line in lines[: min(num_samples, len(lines) - 1)]:\n-    input_text, target_text, _ = line.split(\"\\t\")\n-    # We use \"tab\" as the \"start sequence\" character\n-    # for the targets, and \"\\n\" as \"end sequence\" character.\n-    target_text = \"\\t\" + target_text + \"\\n\"\n-    input_texts.append(input_text)\n-    target_texts.append(target_text)\n-    for char in input_text:\n-        if char not in input_characters:\n-            input_characters.add(char)\n-    for char in target_text:\n-        if char not in target_characters:\n-            target_characters.add(char)\n-\n-input_characters = sorted(list(input_characters))\n-target_characters = sorted(list(target_characters))\n-num_encoder_tokens = len(input_characters)\n-num_decoder_tokens = len(target_characters)\n-max_encoder_seq_length = max([len(txt) for txt in input_texts])\n-max_decoder_seq_length = max([len(txt) for txt in target_texts])\n-\n-print(\"Number of samples:\", len(input_texts))\n-print(\"Number of unique input tokens:\", num_encoder_tokens)\n-print(\"Number of unique output tokens:\", num_decoder_tokens)\n-print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n-print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n-\n-input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n-target_token_index = dict(\n-    [(char, i) for i, char in enumerate(target_characters)]\n-)\n-\n-encoder_input_data = np.zeros(\n-    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n-    dtype=\"float32\",\n-)\n-decoder_input_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype=\"float32\",\n-)\n-decoder_target_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype=\"float32\",\n-)\n-\n-for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n-    for t, char in enumerate(input_text):\n-        encoder_input_data[i, t, input_token_index[char]] = 1.0\n-    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n-    for t, char in enumerate(target_text):\n-        # decoder_target_data is ahead of decoder_input_data by one timestep\n-        decoder_input_data[i, t, target_token_index[char]] = 1.0\n-        if t > 0:\n-            # decoder_target_data will be ahead by one timestep\n-            # and will not include the start character.\n-            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n-    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n-    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n-\n-\"\"\"\n-## Build the model\n-\"\"\"\n-\n-# Define an input sequence and process it.\n-encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n-encoder = keras.layers.LSTM(latent_dim, return_state=True)\n-encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n-\n-# We discard `encoder_outputs` and only keep the states.\n-encoder_states = [state_h, state_c]\n-\n-# Set up the decoder, using `encoder_states` as initial state.\n-decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n-\n-# We set up our decoder to return full output sequences,\n-# and to return internal states as well. We don't use the\n-# return states in the training model, but we will use them in inference.\n-decoder_lstm = keras.layers.LSTM(\n-    latent_dim, return_sequences=True, return_state=True\n-)\n-decoder_outputs, _, _ = decoder_lstm(\n-    decoder_inputs, initial_state=encoder_states\n-)\n-decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n-decoder_outputs = decoder_dense(decoder_outputs)\n-\n-# Define the model that will turn\n-# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n-model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-model.compile(\n-    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n-)\n-model.fit(\n-    [encoder_input_data, decoder_input_data],\n-    decoder_target_data,\n-    batch_size=batch_size,\n-    epochs=epochs,\n-    validation_split=0.2,\n-)\n-# Save model\n-model.save(\"s2s_model.keras\")\n-\n-\"\"\"\n-## Run inference (sampling)\n-\n-1. encode input and retrieve initial decoder state\n-2. run one step of decoder with this initial state\n-and a \"start of sequence\" token as target.\n-Output will be the next target token.\n-3. Repeat with the current target token and current states\n-\"\"\"\n-\n-# Define sampling models\n-# Restore the model and construct the encoder and decoder.\n-model = keras.models.load_model(\"s2s_model.keras\")\n-\n-encoder_inputs = model.input[0]  # input_1\n-encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n-encoder_states = [state_h_enc, state_c_enc]\n-encoder_model = keras.Model(encoder_inputs, encoder_states)\n-\n-decoder_inputs = model.input[1]  # input_2\n-decoder_state_input_h = keras.Input(shape=(latent_dim,))\n-decoder_state_input_c = keras.Input(shape=(latent_dim,))\n-decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n-decoder_lstm = model.layers[3]\n-decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n-    decoder_inputs, initial_state=decoder_states_inputs\n-)\n-decoder_states = [state_h_dec, state_c_dec]\n-decoder_dense = model.layers[4]\n-decoder_outputs = decoder_dense(decoder_outputs)\n-decoder_model = keras.Model(\n-    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n-)\n-\n-# Reverse-lookup token index to decode sequences back to\n-# something readable.\n-reverse_input_char_index = dict(\n-    (i, char) for char, i in input_token_index.items()\n-)\n-reverse_target_char_index = dict(\n-    (i, char) for char, i in target_token_index.items()\n-)\n-\n-\n-def decode_sequence(input_seq):\n-    # Encode the input as state vectors.\n-    states_value = encoder_model.predict(input_seq, verbose=0)\n-\n-    # Generate empty target sequence of length 1.\n-    target_seq = np.zeros((1, 1, num_decoder_tokens))\n-    # Populate the first character of target sequence with the start character.\n-    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n-\n-    # Sampling loop for a batch of sequences\n-    # (to simplify, here we assume a batch of size 1).\n-    stop_condition = False\n-    decoded_sentence = \"\"\n-    while not stop_condition:\n-        output_tokens, h, c = decoder_model.predict(\n-            [target_seq] + states_value, verbose=0\n-        )\n-\n-        # Sample a token\n-        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n-        sampled_char = reverse_target_char_index[sampled_token_index]\n-        decoded_sentence += sampled_char\n-\n-        # Exit condition: either hit max length\n-        # or find stop character.\n-        if (\n-            sampled_char == \"\\n\"\n-            or len(decoded_sentence) > max_decoder_seq_length\n-        ):\n-            stop_condition = True\n-\n-        # Update the target sequence (of length 1).\n-        target_seq = np.zeros((1, 1, num_decoder_tokens))\n-        target_seq[0, 0, sampled_token_index] = 1.0\n-\n-        # Update states\n-        states_value = [h, c]\n-    return decoded_sentence\n-\n-\n-\"\"\"\n-You can now generate decoded sentences as such:\n-\"\"\"\n-\n-for seq_index in range(20):\n-    # Take one sequence (part of the training set)\n-    # for trying out decoding.\n-    input_seq = encoder_input_data[seq_index : seq_index + 1]\n-    decoded_sentence = decode_sequence(input_seq)\n-    print(\"-\")\n-    print(\"Input sentence:\", input_texts[seq_index])\n-    print(\"Decoded sentence:\", decoded_sentence)\n\n@@ -1,571 +0,0 @@\n-\"\"\"\n-Title: MultipleChoice Task with Transfer Learning\n-Author: Md Awsafur Rahman\n-Date created: 2023/09/14\n-Last modified: 2023/09/14\n-Description: Use pre-trained nlp models for multiplechoice task.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-In this example, we will demonstrate how to perform the **MultipleChoice** task by\n-finetuning pre-trained DebertaV3 model. In this task, several candidate answers are\n-provided along with a context and the model is trained to select the correct answer\n-unlike question answering. We will use SWAG dataset to demonstrate this example.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -q keras --upgrade\n-pip install -q keras-nlp --upgrade\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n-\n-import keras_nlp\n-import keras\n-import tensorflow as tf\n-\n-import numpy as np\n-import pandas as pd\n-\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Dataset\n-In this example we'll use **SWAG** dataset for multiplechoice task.\n-\"\"\"\n-\n-\"\"\"shell\n-wget \"https://github.com/rowanz/swagaf/archive/refs/heads/master.zip\" -O swag.zip\n-unzip -q /content/swag.zip\n-\"\"\"\n-\n-\"\"\"shell\n-ls /content/swagaf-master/data\n-\"\"\"\n-\n-\"\"\"\n-## Configuration\n-\"\"\"\n-\n-\n-class CFG:\n-    preset = \"deberta_v3_extra_small_en\"  # Name of pretrained models\n-    sequence_length = 200  # Input sequence length\n-    seed = 42  # Random seed\n-    epochs = 5  # Training epochs\n-    batch_size = 8  # Batch size\n-    augment = True  # Augmentation (Shuffle Options)\n-\n-\n-\"\"\"\n-## Reproducibility\n-Sets value for random seed to produce similar result in each run.\n-\"\"\"\n-\n-keras.utils.set_random_seed(CFG.seed)\n-\n-\n-\"\"\"\n-## Meta Data\n-* **train.csv** - will be used for training.\n-* `sent1` and `sent2`: these fields show how a sentence starts, and if you put the two\n-together, you get the `startphrase` field.\n-* `ending_<i>`: suggests a possible ending for how a sentence can end, but only one of\n-them is correct.\n-    * `label`: identifies the correct sentence ending.\n-\n-* **val.csv** - similar to `train.csv` but will be used for validation.\n-\"\"\"\n-\n-# Train data\n-train_df = pd.read_csv(\n-    \"/content/swagaf-master/data/train.csv\", index_col=0\n-)  # Read CSV file into a DataFrame\n-train_df = train_df.sample(frac=0.02)\n-print(\"# Train Data: {:,}\".format(len(train_df)))\n-\n-# Valid data\n-valid_df = pd.read_csv(\n-    \"/content/swagaf-master/data/val.csv\", index_col=0\n-)  # Read CSV file into a DataFrame\n-valid_df = valid_df.sample(frac=0.02)\n-print(\"# Valid Data: {:,}\".format(len(valid_df)))\n-\n-\"\"\"\n-## Contextualize Options\n-\n-Our approach entails furnishing the model with question and answer pairs, as opposed to\n-employing a single question for all five options. In practice, this signifies that for\n-the five options, we will supply the model with the same set of five questions combined\n-with each respective answer choice (e.g., `(Q + A)`, `(Q + B)`, and so on). This analogy\n-draws parallels to the practice of revisiting a question multiple times during an exam to\n-promote a deeper understanding of the problem at hand.\n-\n-> Notably, in the context of SWAG dataset, question is the start of a sentence and\n-options are possible ending of that sentence.\n-\"\"\"\n-\n-# Define a function to create options based on the prompt and choices\n-def make_options(row):\n-    row[\"options\"] = [\n-        f\"{row.startphrase}\\n{row.ending0}\",  # Option 0\n-        f\"{row.startphrase}\\n{row.ending1}\",  # Option 1\n-        f\"{row.startphrase}\\n{row.ending2}\",  # Option 2\n-        f\"{row.startphrase}\\n{row.ending3}\",\n-    ]  # Option 3\n-    return row\n-\n-\n-\"\"\"\n-Apply the `make_options` function to each row of the dataframe\n-\"\"\"\n-\n-train_df = train_df.apply(make_options, axis=1)\n-valid_df = valid_df.apply(make_options, axis=1)\n-\n-\"\"\"\n-## Preprocessing\n-\n-**What it does:** The preprocessor takes input strings and transforms them into a\n-dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process\n-starts with tokenization, where input strings are converted into sequences of token IDs.\n-\n-**Why it's important:** Initially, raw text data is complex and challenging for modeling\n-due to its high dimensionality. By converting text into a compact set of tokens, such as\n-transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`,\n-we simplify the data. Many models rely on special tokens and additional tensors to\n-understand input. These tokens help divide input and identify padding, among other tasks.\n-Making all sequences the same length through padding boosts computational efficiency,\n-making subsequent steps smoother.\n-\n-Explore the following pages to access the available preprocessing and tokenizer layers in\n-**KerasNLP**:\n-- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n-- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)\n-\"\"\"\n-\n-preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n-    preset=CFG.preset,  # Name of the model\n-    sequence_length=CFG.sequence_length,  # Max sequence length, will be padded if shorter\n-)\n-\n-\"\"\"\n-Now, let's examine what the output shape of the preprocessing layer looks like. The\n-output shape of the layer can be represented as $(num\\_choices, sequence\\_length)$.\n-\"\"\"\n-\n-outs = preprocessor(\n-    train_df.options.iloc[0]\n-)  # Process options for the first row\n-\n-# Display the shape of each processed output\n-for k, v in outs.items():\n-    print(k, \":\", v.shape)\n-\n-\"\"\"\n-We'll use the `preprocessing_fn` function to transform each text option using the\n-`dataset.map(preprocessing_fn)` method.\n-\"\"\"\n-\n-\n-def preprocess_fn(text, label=None):\n-    text = preprocessor(text)  # Preprocess text\n-    return (\n-        (text, label) if label is not None else text\n-    )  # Return processed text and label if available\n-\n-\n-\"\"\"\n-## Augmentation\n-\n-In this notebook, we'll experiment with an interesting augmentation technique,\n-`option_shuffle`. Since we're providing the model with one option at a time, we can\n-introduce a shuffle to the order of options. For instance, options `[A, C, E, D, B]`\n-would be rearranged as `[D, B, A, E, C]`. This practice will help the model focus on the\n-content of the options themselves, rather than being influenced by their positions.\n-\n-**Note:** Even though `option_shuffle` function is written in pure\n-tensorflow, it can be used with any backend (e.g. JAX, PyTorch) as it is only used\n-in `tf.data.Dataset` pipeline which is compatible with Keras 3 routines.\n-\"\"\"\n-\n-\n-def option_shuffle(options, labels, prob=0.50, seed=None):\n-    if tf.random.uniform([]) > prob:  # Shuffle probability check\n-        return options, labels\n-    # Shuffle indices of options and labels in the same order\n-    indices = tf.random.shuffle(tf.range(tf.shape(options)[0]), seed=seed)\n-    # Shuffle options and labels\n-    options = tf.gather(options, indices)\n-    labels = tf.gather(labels, indices)\n-    return options, labels\n-\n-\n-\"\"\"\n-In the following function, we'll merge all augmentation functions to apply to the text.\n-These augmentations will be applied to the data using the `dataset.map(augment_fn)`\n-approach.\n-\"\"\"\n-\n-\n-def augment_fn(text, label=None):\n-    text, label = option_shuffle(text, label, prob=0.5  # Shuffle the options\n-    return (text, label) if label is not None else text\n-\n-\n-\"\"\"\n-## DataLoader\n-\n-The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data\n-processing. Notable aspects of `tf.data` include its ability to simplify pipeline\n-construction and represent components in sequences.\n-\n-To learn more about `tf.data`, refer to this\n-[documentation](https://www.tensorflow.org/guide/data).\n-\"\"\"\n-\n-\n-def build_dataset(\n-    texts,\n-    labels=None,\n-    batch_size=32,\n-    cache=False,\n-    augment=False,\n-    repeat=False,\n-    shuffle=1024,\n-):\n-    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n-    slices = (\n-        (texts,)\n-        if labels is None\n-        else (texts, keras.utils.to_categorical(labels, num_classes=4))\n-    )  # Create slices\n-    ds = tf.data.Dataset.from_tensor_slices(\n-        slices\n-    )  # Create dataset from slices\n-    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n-    if augment:  # Apply augmentation if enabled\n-        ds = ds.map(augment_fn, num_parallel_calls=AUTO)\n-    ds = ds.map(\n-        preprocess_fn, num_parallel_calls=AUTO\n-    )  # Map preprocessing function\n-    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n-    opt = tf.data.Options()  # Create dataset options\n-    if shuffle:\n-        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n-        opt.experimental_deterministic = False\n-    ds = ds.with_options(opt)  # Set dataset options\n-    ds = ds.batch(batch_size, drop_remainder=True)  # Batch dataset\n-    ds = ds.prefetch(AUTO)  # Prefetch next batch\n-    return ds  # Return the built dataset\n-\n-\n-\"\"\"\n-Now let's create train and valid dataloader using above funciton.\n-\"\"\"\n-\n-# Build train dataloader\n-train_texts = train_df.options.tolist()  # Extract training texts\n-train_labels = train_df.label.tolist()  # Extract training labels\n-train_ds = build_dataset(\n-    train_texts,\n-    train_labels,\n-    batch_size=CFG.batch_size,\n-    cache=True,\n-    shuffle=True,\n-    repeat=True,\n-    augment=CFG.augment,\n-)\n-\n-# Build valid dataloader\n-valid_texts = valid_df.options.tolist()  # Extract validation texts\n-valid_labels = valid_df.label.tolist()  # Extract validation labels\n-valid_ds = build_dataset(\n-    valid_texts,\n-    valid_labels,\n-    batch_size=CFG.batch_size,\n-    cache=True,\n-    shuffle=False,\n-    repeat=False,\n-    augment=False,\n-)\n-\n-\n-\"\"\"\n-## LR Schedule\n-\n-Implementing a learning rate scheduler is crucial for transfer learning. The learning\n-rate initiates at `lr_start` and gradually tapers down to `lr_min` using **cosine**\n-curve.\n-\n-**Importance:** A well-structured learning rate schedule is essential for efficient model\n-training, ensuring optimal convergence and avoiding issues such as overshooting or\n-stagnation.\n-\"\"\"\n-\n-import math\n-\n-\n-def get_lr_callback(batch_size=8, mode=\"cos\", epochs=10, plot=False):\n-    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n-    lr_ramp_ep, lr_sus_ep = 2, 0\n-\n-    def lrfn(epoch):  # Learning rate update function\n-        if epoch < lr_ramp_ep:\n-            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n-        elif epoch < lr_ramp_ep + lr_sus_ep:\n-            lr = lr_max\n-        else:\n-            decay_total_epochs, decay_epoch_index = (\n-                epochs - lr_ramp_ep - lr_sus_ep + 3,\n-                epoch - lr_ramp_ep - lr_sus_ep,\n-            )\n-            phase = math.pi * decay_epoch_index / decay_total_epochs\n-            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n-        return lr\n-\n-    if plot:  # Plot lr curve if plot is True\n-        plt.figure(figsize=(10, 5))\n-        plt.plot(\n-            np.arange(epochs),\n-            [lrfn(epoch) for epoch in np.arange(epochs)],\n-            marker=\"o\",\n-        )\n-        plt.xlabel(\"epoch\")\n-        plt.ylabel(\"lr\")\n-        plt.title(\"LR Scheduler\")\n-        plt.show()\n-\n-    return keras.callbacks.LearningRateScheduler(\n-        lrfn, verbose=False\n-    )  # Create lr callback\n-\n-\n-_ = get_lr_callback(CFG.batch_size, plot=True)\n-\n-\"\"\"\n-## Callbacks\n-\n-The function below will gather all the training callbacks, such as `lr_scheduler`,\n-`model_checkpoint`.\n-\"\"\"\n-\n-\n-def get_callbacks():\n-    callbacks = []\n-    lr_cb = get_lr_callback(CFG.batch_size)  # Get lr callback\n-    ckpt_cb = keras.callbacks.ModelCheckpoint(\n-        f\"best.keras\",\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=False,\n-        mode=\"max\",\n-    )  # Get Model checkpoint callback\n-    callbacks.extend([lr_cb, ckpt_cb])  # Add lr and checkpoint callbacks\n-    return callbacks  # Return the list of callbacks\n-\n-\n-callbacks = get_callbacks()\n-\n-\"\"\"\n-## MultipleChoice Model\n-\n-\n-\n-\n-\n-\"\"\"\n-\n-\"\"\"\n-\n-### Pre-trained Models\n-\n-The `KerasNLP` library provides comprehensive, ready-to-use implementations of popular\n-NLP model architectures. It features a variety of pre-trained models including `Bert`,\n-`Roberta`, `DebertaV3`, and more. In this notebook, we'll showcase the usage of\n-`DistillBert`. However, feel free to explore all available models in the [KerasNLP\n-documentation](https://keras.io/api/keras_nlp/models/). Also for a deeper understanding\n-of `KerasNLP`, refer to the informative [getting started\n-guide](https://keras.io/guides/keras_nlp/getting_started/).\n-\n-Our approach involves using `keras_nlp.models.XXClassifier` to process each question and\n-option pari (e.g. (Q+A), (Q+B), etc.), generating logits. These logits are then combined\n-and passed through a softmax function to produce the final output.\n-\"\"\"\n-\n-\"\"\"\n-\n-### Classifier for Multiple-Choice Tasks\n-\n-When dealing with multiple-choice questions, instead of giving the model the question and\n-all options together `(Q + A + B + C ...)`, we provide the model with one option at a\n-time along with the question. For instance, `(Q + A)`, `(Q + B)`, and so on. Once we have\n-the prediction scores (logits) for all options, we combine them using the `Softmax`\n-function to get the ultimate result. If we had given all options at once to the model,\n-the text's length would increase, making it harder for the model to handle. The picture\n-below illustrates this idea:\n-\n-![Model Diagram](https://pbs.twimg.com/media/F3NUju_a8AAS8Fq?format=png&name=large)\n-\n-<div align=\"center\"><b> Picture Credict: </b> <a\n-href=\"https://twitter.com/johnowhitaker\"> @johnowhitaker </a> </div></div><br>\n-\n-From a coding perspective, remember that we use the same model for all five options, with\n-shared weights. Despite the figure suggesting five separate models, they are, in fact,\n-one model with shared weights. Another point to consider is the the input shapes of\n-Classifier and MultipleChoice.\n-\n-* Input shape for **Multiple Choice**: $(batch\\_size, num\\_choices, seq\\_length)$\n-* Input shape for **Classifier**: $(batch\\_size, seq\\_length)$\n-\n-Certainly, it's clear that we can't directly give the data for the multiple-choice task\n-to the model because the input shapes don't match. To handle this, we'll use **slicing**.\n-This means we'll separate the features of each option, like $feature_{(Q + A)}$ and\n-$feature_{(Q + B)}$, and give them one by one to the NLP classifier. After we get the\n-prediction scores $logits_{(Q + A)}$ and $logits_{(Q + B)}$ for all the options, we'll\n-use the Softmax function, like $\\operatorname{Softmax}([logits_{(Q + A)}, logits_{(Q +\n-B)}])$, to combine them. This final step helps us make the ultimate decision or choice.\n-\n-> Note that in the classifier, we set `num_classes=1` instead of `5`. This is because the\n-classifier produces a single output for each option. When dealing with five options,\n-these individual outputs are joined together and then processed through a softmax\n-function to generate the final result, which has a dimension of `5`.\n-\"\"\"\n-\n-# Selects one option from five\n-class SelectOption(keras.layers.Layer):\n-    def __init__(self, index, **kwargs):\n-        super().__init__(**kwargs)\n-        self.index = index\n-\n-    def call(self, inputs):\n-        # Selects a specific slice from the inputs tensor\n-        return inputs[:, self.index, :]\n-\n-    def get_config(self):\n-        # For serialize the model\n-        base_config = super().get_config()\n-        config = {\n-            \"index\": self.index,\n-        }\n-        return {**base_config, **config}\n-\n-\n-def build_model():\n-    # Define input layers\n-    inputs = {\n-        \"token_ids\": keras.Input(\n-            shape=(4, None), dtype=t\"int32\", name=\"token_ids\"\n-        ),\n-        \"padding_mask\": keras.Input(\n-            shape=(4, None), dtype=\"int32\", name=\"padding_mask\"\n-        ),\n-    }\n-    # Create a DebertaV3Classifier model\n-    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n-        CFG.preset,\n-        preprocessor=None,\n-        num_classes=1,  # one output per one option, for five options total 5 outputs\n-    )\n-    logits = []\n-    # Loop through each option (Q+A), (Q+B) etc and compute associted logits\n-    for option_idx in range(4):\n-        option = {\n-            k: SelectOption(option_idx, name=f\"{k}_{option_idx}\")(v)\n-            for k, v in inputs.items()\n-        }\n-        logit = classifier(option)\n-        logits.append(logit)\n-\n-    # Compute final output\n-    logits = keras.layers.Concatenate(axis=-1)(logits)\n-    outputs = keras.layers.Softmax(axis=-1)(logits)\n-    model = keras.Model(inputs, outputs)\n-\n-    # Compile the model with optimizer, loss, and metrics\n-    model.compile(\n-        optimizer=keras.optimizers.AdamW(5e-6),\n-        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n-        metrics=[\n-            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-        ],\n-        jit_compile=True,\n-    )\n-    return model\n-\n-\n-# Build the Build\n-model = build_model()\n-\n-\"\"\"\n-Let's checkout the model summary to have a better insight on the model.\n-\"\"\"\n-\n-model.summary()\n-\n-\"\"\"\n-Finally, let's check the model structure visually if everything is in place.\n-\"\"\"\n-\n-keras.utils.plot_model(model, show_shapes=True)\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-# Start training the model\n-history = model.fit(\n-    train_ds,\n-    epochs=CFG.epochs,\n-    validation_data=valid_ds,\n-    callbacks=callbacks,\n-    steps_per_epoch=int(len(train_df) / CFG.batch_size),\n-    verbose=1,\n-)\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-# Make predictions using the trained model on last validation data\n-predictions = model.predict(\n-    valid_ds,\n-    batch_size=CFG.batch_size,  # max batch size = valid size\n-    verbose=1,\n-)\n-\n-# Format predictions and true answers\n-pred_answers = np.arange(4)[np.argsort(-predictions)][:, 0]\n-true_answers = valid_df.label.values\n-\n-# Check 5 Predictions\n-print(\"# Predictions\\n\")\n-for i in range(0, 50, 10):\n-    row = valid_df.iloc[i]\n-    question = row.startphrase\n-    pred_answer = f\"ending{pred_answers[i]}\"\n-    true_answer = f\"ending{true_answers[i]}\"\n-    print(f\"❓ Sentence {i+1}:\\n{question}\\n\")\n-    print(f\"✅ True Ending: {true_answer}\\n   >> {row[true_answer]}\\n\")\n-    print(f\"🤖 Predicted Ending: {pred_answer}\\n   >> {row[pred_answer]}\\n\")\n-    print(\"-\" * 90, \"\\n\")\n-\n-\"\"\"\n-## Reference\n-* [Multiple Choice with\n-HF](https://twitter.com/johnowhitaker/status/1689790373454041089?s=20)\n-* [Keras NLP](https://keras.io/api/keras_nlp/)\n-* [BirdCLEF23: Pretraining is All you Need\n-[Train]](https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train)\n-[Train]](https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train)\n-* [Triple Stratified KFold with\n-TFRecords](https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords)\n-\"\"\"\n\n@@ -1,515 +0,0 @@\n-\"\"\"\n-Title: English-to-Spanish translation with KerasNLP\n-Author: [Abheesht Sharma](https://github.com/abheesht17/)\n-Date created: 2022/05/26\n-Last modified: 2022/12/21\n-Description: Use KerasNLP to train a sequence-to-sequence Transformer model on the machine translation task.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-KerasNLP provides building blocks for NLP (model layers, tokenizers, metrics, etc.) and\n-makes it convenient to construct NLP pipelines.\n-\n-In this example, we'll use KerasNLP layers to build an encoder-decoder Transformer\n-model, and train it on the English-to-Spanish machine translation task.\n-\n-This example is based on the\n-[English-to-Spanish NMT\n-example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n-by [fchollet](https://twitter.com/fchollet). The original example is more low-level\n-and implements layers from scratch, whereas this example uses KerasNLP to show\n-some more advanced approaches, such as subword tokenization and using metrics\n-to compute the quality of generated translations.\n-\n-You'll learn how to:\n-\n-- Tokenize text using `keras_nlp.tokenizers.WordPieceTokenizer`.\n-- Implement a sequence-to-sequence Transformer model using KerasNLP's\n-`keras_nlp.layers.TransformerEncoder`, `keras_nlp.layers.TransformerDecoder` and\n-`keras_nlp.layers.TokenAndPositionEmbedding` layers, and train it.\n-- Use `keras_nlp.samplers` to generate translations of unseen input sentences\n- using the top-p decoding strategy!\n-\n-Don't worry if you aren't familiar with KerasNLP. This tutorial will start with\n-the basics. Let's dive right in!\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\n-Before we start implementing the pipeline, let's import all the libraries we need.\n-\"\"\"\n-\n-\"\"\"shell\n-!pip install -q rouge-score\n-!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade\n-\"\"\"\n-\n-import keras_nlp\n-import pathlib\n-import random\n-\n-import keras\n-from keras import ops\n-\n-import tensorflow.data as tf_data\n-from tensorflow_text.tools.wordpiece_vocab import (\n-    bert_vocab_from_dataset as bert_vocab,\n-)\n-\n-\"\"\"\n-Let's also define our parameters/hyperparameters.\n-\"\"\"\n-\n-BATCH_SIZE = 64\n-EPOCHS = 1  # This should be at least 10 for convergence\n-MAX_SEQUENCE_LENGTH = 40\n-ENG_VOCAB_SIZE = 15000\n-SPA_VOCAB_SIZE = 15000\n-\n-EMBED_DIM = 256\n-INTERMEDIATE_DIM = 2048\n-NUM_HEADS = 8\n-\n-\"\"\"\n-## Downloading the data\n-\n-We'll be working with an English-to-Spanish translation dataset\n-provided by [Anki](https://www.manythings.org/anki/). Let's download it:\n-\"\"\"\n-\n-text_file = keras.utils.get_file(\n-    fname=\"spa-eng.zip\",\n-    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n-    extract=True,\n-)\n-text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n-\n-\"\"\"\n-## Parsing the data\n-\n-Each line contains an English sentence and its corresponding Spanish sentence.\n-The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n-Before adding the text to a list, we convert it to lowercase.\n-\"\"\"\n-\n-with open(text_file) as f:\n-    lines = f.read().split(\"\\n\")[:-1]\n-text_pairs = []\n-for line in lines:\n-    eng, spa = line.split(\"\\t\")\n-    eng = eng.lower()\n-    spa = spa.lower()\n-    text_pairs.append((eng, spa))\n-\n-\"\"\"\n-Here's what our sentence pairs look like:\n-\"\"\"\n-\n-for _ in range(5):\n-    print(random.choice(text_pairs))\n-\n-\"\"\"\n-Now, let's split the sentence pairs into a training set, a validation set,\n-and a test set.\n-\"\"\"\n-\n-random.shuffle(text_pairs)\n-num_val_samples = int(0.15 * len(text_pairs))\n-num_train_samples = len(text_pairs) - 2 * num_val_samples\n-train_pairs = text_pairs[:num_train_samples]\n-val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n-test_pairs = text_pairs[num_train_samples + num_val_samples :]\n-\n-print(f\"{len(text_pairs)} total pairs\")\n-print(f\"{len(train_pairs)} training pairs\")\n-print(f\"{len(val_pairs)} validation pairs\")\n-print(f\"{len(test_pairs)} test pairs\")\n-\n-\n-\"\"\"\n-## Tokenizing the data\n-\n-We'll define two tokenizers - one for the source language (English), and the other\n-for the target language (Spanish). We'll be using\n-`keras_nlp.tokenizers.WordPieceTokenizer` to tokenize the text.\n-`keras_nlp.tokenizers.WordPieceTokenizer` takes a WordPiece vocabulary\n-and has functions for tokenizing the text, and detokenizing sequences of tokens.\n-\n-Before we define the two tokenizers, we first need to train them on the dataset\n-we have. The WordPiece tokenization algorithm is a subword tokenization algorithm;\n-training it on a corpus gives us a vocabulary of subwords. A subword tokenizer\n-is a compromise between word tokenizers (word tokenizers need very large\n-vocabularies for good coverage of input words), and character tokenizers\n-(characters don't really encode meaning like words do). Luckily, KerasNLP\n-makes it very simple to train WordPiece on a corpus with the\n-`keras_nlp.tokenizers.compute_word_piece_vocabulary` utility.\n-\"\"\"\n-\n-\n-def train_word_piece(text_samples, vocab_size, reserved_tokens):\n-    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n-    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n-        word_piece_ds.batch(1000).prefetch(2),\n-        vocabulary_size=vocab_size,\n-        reserved_tokens=reserved_tokens,\n-    )\n-    return vocab\n-\n-\n-\"\"\"\n-Every vocabulary has a few special, reserved tokens. We have four such tokens:\n-\n-- `\"[PAD]\"` - Padding token. Padding tokens are appended to the input sequence\n-length when the input sequence length is shorter than the maximum sequence length.\n-- `\"[UNK]\"` - Unknown token.\n-- `\"[START]\"` - Token that marks the start of the input sequence.\n-- `\"[END]\"` - Token that marks the end of the input sequence.\n-\"\"\"\n-\n-reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n-\n-eng_samples = [text_pair[0] for text_pair in train_pairs]\n-eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n-\n-spa_samples = [text_pair[1] for text_pair in train_pairs]\n-spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)\n-\n-\"\"\"\n-Let's see some tokens!\n-\"\"\"\n-\n-print(\"English Tokens: \", eng_vocab[100:110])\n-print(\"Spanish Tokens: \", spa_vocab[100:110])\n-\n-\"\"\"\n-Now, let's define the tokenizers. We will configure the tokenizers with the\n-the vocabularies trained above.\n-\"\"\"\n-\n-eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n-    vocabulary=eng_vocab, lowercase=False\n-)\n-spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n-    vocabulary=spa_vocab, lowercase=False\n-)\n-\n-\"\"\"\n-Let's try and tokenize a sample from our dataset! To verify whether the text has\n-been tokenized correctly, we can also detokenize the list of tokens back to the\n-original text.\n-\"\"\"\n-\n-eng_input_ex = text_pairs[0][0]\n-eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n-print(\"English sentence: \", eng_input_ex)\n-print(\"Tokens: \", eng_tokens_ex)\n-print(\n-    \"Recovered text after detokenizing: \",\n-    eng_tokenizer.detokenize(eng_tokens_ex),\n-)\n-\n-print()\n-\n-spa_input_ex = text_pairs[0][1]\n-spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n-print(\"Spanish sentence: \", spa_input_ex)\n-print(\"Tokens: \", spa_tokens_ex)\n-print(\n-    \"Recovered text after detokenizing: \",\n-    spa_tokenizer.detokenize(spa_tokens_ex),\n-)\n-\n-\"\"\"\n-## Format datasets\n-\n-Next, we'll format our datasets.\n-\n-At each training step, the model will seek to predict target words N+1 (and beyond)\n-using the source sentence and the target words 0 to N.\n-\n-As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n-\n-- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n-`encoder_inputs` is the tokenized source sentence and `decoder_inputs` is the target\n-sentence \"so far\",\n-that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target\n-sentence.\n-- `target` is the target sentence offset by one step:\n-it provides the next words in the target sentence -- what the model will try to predict.\n-\n-We will add special tokens, `\"[START]\"` and `\"[END]\"`, to the input Spanish\n-sentence after tokenizing the text. We will also pad the input to a fixed length.\n-This can be easily done using `keras_nlp.layers.StartEndPacker`.\n-\"\"\"\n-\n-\n-def preprocess_batch(eng, spa):\n-    batch_size = ops.shape(spa)[0]\n-\n-    eng = eng_tokenizer(eng)\n-    spa = spa_tokenizer(spa)\n-\n-    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n-    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n-        sequence_length=MAX_SEQUENCE_LENGTH,\n-        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n-    )\n-    eng = eng_start_end_packer(eng)\n-\n-    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n-    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n-        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n-        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n-        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n-        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n-    )\n-    spa = spa_start_end_packer(spa)\n-\n-    return (\n-        {\n-            \"encoder_inputs\": eng,\n-            \"decoder_inputs\": spa[:, :-1],\n-        },\n-        spa[:, 1:],\n-    )\n-\n-\n-def make_dataset(pairs):\n-    eng_texts, spa_texts = zip(*pairs)\n-    eng_texts = list(eng_texts)\n-    spa_texts = list(spa_texts)\n-    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n-    dataset = dataset.batch(BATCH_SIZE)\n-    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n-    return dataset.shuffle(2048).prefetch(16).cache()\n-\n-\n-train_ds = make_dataset(train_pairs)\n-val_ds = make_dataset(val_pairs)\n-\n-\"\"\"\n-Let's take a quick look at the sequence shapes\n-(we have batches of 64 pairs, and all sequences are 40 steps long):\n-\"\"\"\n-\n-for inputs, targets in train_ds.take(1):\n-    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n-    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n-    print(f\"targets.shape: {targets.shape}\")\n-\n-\n-\"\"\"\n-## Building the model\n-\n-Now, let's move on to the exciting part - defining our model!\n-We first need an embedding layer, i.e., a vector for every token in our input sequence.\n-This embedding layer can be initialised randomly. We also need a positional\n-embedding layer which encodes the word order in the sequence. The convention is\n-to add these two embeddings. KerasNLP has a `keras_nlp.layers.TokenAndPositionEmbedding `\n-layer which does all of the above steps for us.\n-\n-Our sequence-to-sequence Transformer consists of a `keras_nlp.layers.TransformerEncoder`\n-layer and a `keras_nlp.layers.TransformerDecoder` layer chained together.\n-\n-The source sequence will be passed to `keras_nlp.layers.TransformerEncoder`, which\n-will produce a new representation of it. This new representation will then be passed\n-to the `keras_nlp.layers.TransformerDecoder`, together with the target sequence\n-so far (target words 0 to N). The `keras_nlp.layers.TransformerDecoder` will\n-then seek to predict the next words in the target sequence (N+1 and beyond).\n-\n-A key detail that makes this possible is causal masking.\n-The `keras_nlp.layers.TransformerDecoder` sees the entire sequence at once, and\n-thus we must make sure that it only uses information from target tokens 0 to N\n-when predicting token N+1 (otherwise, it could use information from the future,\n-which would result in a model that cannot be used at inference time). Causal masking\n-is enabled by default in `keras_nlp.layers.TransformerDecoder`.\n-\n-We also need to mask the padding tokens (`\"[PAD]\"`). For this, we can set the\n-`mask_zero` argument of the `keras_nlp.layers.TokenAndPositionEmbedding` layer\n-to True. This will then be propagated to all subsequent layers.\n-\"\"\"\n-\n-# Encoder\n-encoder_inputs = keras.Input(\n-    shape=(None,), dtype=\"int64\", name=\"encoder_inputs\"\n-)\n-\n-x = keras_nlp.layers.TokenAndPositionEmbedding(\n-    vocabulary_size=ENG_VOCAB_SIZE,\n-    sequence_length=MAX_SEQUENCE_LENGTH,\n-    embedding_dim=EMBED_DIM,\n-)(encoder_inputs)\n-\n-encoder_outputs = keras_nlp.layers.TransformerEncoder(\n-    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n-)(inputs=x)\n-encoder = keras.Model(encoder_inputs, encoder_outputs)\n-\n-\n-# Decoder\n-decoder_inputs = keras.Input(\n-    shape=(None,), dtype=\"int64\", name=\"decoder_inputs\"\n-)\n-encoded_seq_inputs = keras.Input(\n-    shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n-)\n-\n-x = keras_nlp.layers.TokenAndPositionEmbedding(\n-    vocabulary_size=SPA_VOCAB_SIZE,\n-    sequence_length=MAX_SEQUENCE_LENGTH,\n-    embedding_dim=EMBED_DIM,\n-)(decoder_inputs)\n-\n-x = keras_nlp.layers.TransformerDecoder(\n-    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n-)(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n-x = keras.layers.Dropout(0.5)(x)\n-decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n-decoder = keras.Model(\n-    [\n-        decoder_inputs,\n-        encoded_seq_inputs,\n-    ],\n-    decoder_outputs,\n-)\n-decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n-\n-transformer = keras.Model(\n-    [encoder_inputs, decoder_inputs],\n-    decoder_outputs,\n-    name=\"transformer\",\n-)\n-\n-\"\"\"\n-## Training our model\n-\n-We'll use accuracy as a quick way to monitor training progress on the validation data.\n-Note that machine translation typically uses BLEU scores as well as other metrics,\n-rather than accuracy. However, in order to use metrics like ROUGE, BLEU, etc. we\n-will have decode the probabilities and generate the text. Text generation is\n-computationally expensive, and performing this during training is not recommended.\n-\n-Here we only train for 1 epoch, but to get the model to actually converge\n-you should train for at least 10 epochs.\n-\"\"\"\n-\n-transformer.summary()\n-transformer.compile(\n-    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n-)\n-transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n-\n-\"\"\"\n-## Decoding test sentences (qualitative analysis)\n-\n-Finally, let's demonstrate how to translate brand new English sentences.\n-We simply feed into the model the tokenized English sentence\n-as well as the target token `\"[START]\"`. The model outputs probabilities of the\n-next token. We then we repeatedly generated the next token conditioned on the\n-tokens generated so far, until we hit the token `\"[END]\"`.\n-\n-For decoding, we will use the `keras_nlp.samplers` module from\n-KerasNLP. Greedy Decoding is a text decoding method which outputs the most\n-likely next token at each time step, i.e., the token with the highest probability.\n-\"\"\"\n-\n-\n-def decode_sequences(input_sentences):\n-    batch_size = 1\n-\n-    # Tokenize the encoder input.\n-    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n-    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n-        pads = ops.full(\n-            (1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0\n-        )\n-        encoder_input_tokens = ops.concatenate([encoder_input_tokens, pads], 1)\n-\n-    # Define a function that outputs the next token's probability given the\n-    # input sequence.\n-    def next(prompt, cache, index):\n-        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n-        # Ignore hidden states for now; only needed for contrastive search.\n-        hidden_states = None\n-        return logits, hidden_states, cache\n-\n-    # Build a prompt of length 40 with a start token and padding tokens.\n-    length = 40\n-    start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n-    pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n-    prompt = ops.concatenate((start, pad), axis=-1)\n-\n-    generated_tokens = keras_nlp.samplers.GreedySampler()(\n-        next,\n-        prompt,\n-        end_token_id=spa_tokenizer.token_to_id(\"[END]\"),\n-        index=1,  # Start sampling after start token.\n-    )\n-    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n-    return generated_sentences\n-\n-\n-test_eng_texts = [pair[0] for pair in test_pairs]\n-for i in range(2):\n-    input_sentence = random.choice(test_eng_texts)\n-    translated = decode_sequences([input_sentence])\n-    translated = translated.numpy()[0].decode(\"utf-8\")\n-    translated = (\n-        translated.replace(\"[PAD]\", \"\")\n-        .replace(\"[START]\", \"\")\n-        .replace(\"[END]\", \"\")\n-        .strip()\n-    )\n-    print(f\"** Example {i} **\")\n-    print(input_sentence)\n-    print(translated)\n-    print()\n-\n-\"\"\"\n-## Evaluating our model (quantitative analysis)\n-\n-There are many metrics which are used for text generation tasks. Here, to\n-evaluate translations generated by our model, let's compute the ROUGE-1 and\n-ROUGE-2 scores. Essentially, ROUGE-N is a score based on the number of common\n-n-grams between the reference text and the generated text. ROUGE-1 and ROUGE-2\n-use the number of common unigrams and bigrams, respectively.\n-\n-We will calculate the score over 30 test samples (since decoding is an\n-expensive process).\n-\"\"\"\n-\n-rouge_1 = keras_nlp.metrics.RougeN(order=1)\n-rouge_2 = keras_nlp.metrics.RougeN(order=2)\n-\n-for test_pair in test_pairs[:30]:\n-    input_sentence = test_pair[0]\n-    reference_sentence = test_pair[1]\n-\n-    translated_sentence = decode_sequences([input_sentence])\n-    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n-    translated_sentence = (\n-        translated_sentence.replace(\"[PAD]\", \"\")\n-        .replace(\"[START]\", \"\")\n-        .replace(\"[END]\", \"\")\n-        .strip()\n-    )\n-\n-    rouge_1(reference_sentence, translated_sentence)\n-    rouge_2(reference_sentence, translated_sentence)\n-\n-print(\"ROUGE-1 Score: \", rouge_1.result())\n-print(\"ROUGE-2 Score: \", rouge_2.result())\n-\n-\"\"\"\n-After 10 epochs, the scores are as follows:\n-\n-|               | **ROUGE-1** | **ROUGE-2** |\n-|:-------------:|:-----------:|:-----------:|\n-| **Precision** |    0.568    |    0.374    |\n-|   **Recall**  |    0.615    |    0.394    |\n-|  **F1 Score** |    0.579    |    0.381    |\n-\"\"\"\n\n@@ -1,490 +0,0 @@\n-\"\"\"\n-Title: English-to-Spanish translation with a sequence-to-sequence Transformer\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2021/05/26\n-Last modified: 2023/02/25\n-Description: Implementing a sequence-to-sequence Transformer and training it on a machine translation task.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In this example, we'll build a sequence-to-sequence Transformer model, which\n-we'll train on an English-to-Spanish machine translation task.\n-\n-You'll learn how to:\n-\n-- Vectorize text using the Keras `TextVectorization` layer.\n-- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n-and a `PositionalEmbedding` layer.\n-- Prepare data for training a sequence-to-sequence model.\n-- Use the trained model to generate translations of never-seen-before\n-input sentences (sequence-to-sequence inference).\n-\n-The code featured here is adapted from the book\n-[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n-(chapter 11: Deep learning for text).\n-The present example is fairly barebones, so for detailed explanations of\n-how each building block works, as well as the theory behind Transformers,\n-I recommend reading the book.\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-# We set the backend to TensorFlow. The code works with\n-# both `tensorflow` and `torch`. It does not work with JAX\n-# due to the behavior of `jax.numpy.tile` in a jit scope\n-# (used in `TransformerDecoder.get_causal_attention_mask()`:\n-# `tile` in JAX does not support a dynamic `reps` argument.\n-# You can make the code work in JAX by wrapping the\n-# inside of the `get_causal_attention_mask` method in\n-# a decorator to prevent jit compilation:\n-# `with jax.ensure_compile_time_eval():`.\n-import os\n-os[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import pathlib\n-import random\n-import string\n-import re\n-import numpy as np\n-\n-import tensorflow.data as tf_data\n-import tensorflow.strings as tf_strings\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-from keras.layers import TextVectorization\n-\n-\"\"\"\n-## Downloading the data\n-\n-We'll be working with an English-to-Spanish translation dataset\n-provided by [Anki](https://www.manythings.org/anki/). Let's download it:\n-\"\"\"\n-\n-text_file = keras.utils.get_file(\n-    fname=\"spa-eng.zip\",\n-    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n-    extract=True,\n-)\n-text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n-\n-\"\"\"\n-## Parsing the data\n-\n-Each line contains an English sentence and its corresponding Spanish sentence.\n-The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n-We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence.\n-\"\"\"\n-\n-with open(text_file) as f:\n-    lines = f.read().split(\"\\n\")[:-1]\n-text_pairs = []\n-for line in lines:\n-    eng, spa = line.split(\"\\t\")\n-    spa = \"[start] \" + spa + \" [end]\"\n-    text_pairs.append((eng, spa))\n-\n-\"\"\"\n-Here's what our sentence pairs look like:\n-\"\"\"\n-\n-for _ in range(5):\n-    print(random.choice(text_pairs))\n-\n-\"\"\"\n-Now, let's split the sentence pairs into a training set, a validation set,\n-and a test set.\n-\"\"\"\n-\n-random.shuffle(text_pairs)\n-num_val_samples = int(0.15 * len(text_pairs))\n-num_train_samples = len(text_pairs) - 2 * num_val_samples\n-train_pairs = text_pairs[:num_train_samples]\n-val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n-test_pairs = text_pairs[num_train_samples + num_val_samples :]\n-\n-print(f\"{len(text_pairs)} total pairs\")\n-print(f\"{len(train_pairs)} training pairs\")\n-print(f\"{len(val_pairs)} validation pairs\")\n-print(f\"{len(test_pairs)} test pairs\")\n-\n-\"\"\"\n-## Vectorizing the text data\n-\n-We'll use two instances of the `TextVectorization` layer to vectorize the text\n-data (one for English and one for Spanish),\n-that is to say, to turn the original strings into integer sequences\n-where each integer represents the index of a word in a vocabulary.\n-\n-The English layer will use the default string standardization (strip punctuation characters)\n-and splitting scheme (split on whitespace), while\n-the Spanish layer will use a custom standardization, where we add the character\n-`\"¿\"` to the set of punctuation characters to be stripped.\n-\n-Note: in a production-grade machine translation model, I would not recommend\n-stripping the punctuation characters in either language. Instead, I would recommend turning\n-each punctuation character into its own token,\n-which you could achieve by providing a custom `split` function to the `TextVectorization` layer.\n-\"\"\"\n-\n-strip_chars = string.punctuation + \"¿\"\n-strip_chars = strip_chars.replace(\"[\", \"\")\n-strip_chars = strip_chars.replace(\"]\", \"\")\n-\n-vocab_size = 15000\n-sequence_length = 20\n-batch_size = 64\n-\n-\n-def custom_standardization(input_string):\n-    lowercase = tf_strings.lower(input_string)\n-    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n-\n-\n-eng_vectorization = TextVectorization(\n-    max_tokens=vocab_size,\n-    output_mode=\"int\",\n-    output_sequence_length=sequence_length,\n-)\n-spa_vectorization = TextVectorization(\n-    max_tokens=vocab_size,\n-    output_mode=\"int\",\n-    output_sequence_length=sequence_length + 1,\n-    standardize=custom_standardization,\n-)\n-train_eng_texts = [pair[0] for pair in train_pairs]\n-train_spa_texts = [pair[1] for pair in train_pairs]\n-eng_vectorization.adapt(train_eng_texts)\n-spa_vectorization.adapt(train_spa_texts)\n-\n-\"\"\"\n-Next, we'll format our datasets.\n-\n-At each training step, the model will seek to predict target words N+1 (and beyond)\n-using the source sentence and the target words 0 to N.\n-\n-As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n-\n-- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n-`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n-that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n-- `target` is the target sentence offset by one step:\n-it provides the next words in the target sentence -- what the model will try to predict.\n-\"\"\"\n-\n-def format_dataset(eng, spa):\n-    eng = eng_vectorization(eng)\n-    spa = spa_vectorization(spa)\n-    return (\n-        {\n-            \"encoder_inputs\": eng,\n-            \"decoder_inputs\": spa[:, :-1],\n-        },\n-        spa[:, 1:],\n-    )\n-\n-\n-def make_dataset(pairs):\n-    eng_texts, spa_texts = zip(*pairs)\n-    eng_texts = list(eng_texts)\n-    spa_texts = list(spa_texts)\n-    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n-    dataset = dataset.batch(batch_size)\n-    dataset = dataset.map(format_dataset)\n-    return dataset.shuffle(2048).prefetch(16).cache()\n-\n-\n-train_ds = make_dataset(train_pairs)\n-val_ds = make_dataset(val_pairs)\n-\n-\"\"\"\n-Let's take a quick look at the sequence shapes\n-(we have batches of 64 pairs, and all sequences are 20 steps long):\n-\"\"\"\n-\n-for inputs, targets in train_ds.take(1):\n-    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n-    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n-    print(f\"targets.shape: {targets.shape}\")\n-\n-\"\"\"\n-## Building the model\n-\n-Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n-and a `TransformerDecoder` chained together. To make the model aware of word order,\n-we also use a `PositionalEmbedding` layer.\n-\n-The source sequence will be pass to the `TransformerEncoder`,\n-which will produce a new representation of it.\n-This new representation will then be passed\n-to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n-The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n-\n-A key detail that makes this possible is causal masking\n-(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n-The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n-sure that it only uses information from target tokens 0 to N when predicting token N+1\n-(otherwise, it could use information from the future, which would\n-result in a model that cannot be used at inference time).\n-\"\"\"\n-import keras.ops as ops\n-\n-class TransformerEncoder(layers.Layer):\n-    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n-        super().__init__(**kwargs)\n-        self.embed_dim = embed_dim\n-        self.dense_dim = dense_dim\n-        self.num_heads = num_heads\n-        self.attention = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim\n-        )\n-        self.dense_proj = keras.Sequential(\n-            [\n-                layers.Dense(dense_dim, activation=\"relu\"),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm_1 = layers.LayerNormalization()\n-        self.layernorm_2 = layers.LayerNormalization()\n-        self.supports_masking = True\n-\n-    def call(self, inputs, mask=None):\n-        if mask is not None:\n-            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n-        else:\n-            padding_mask = None\n-\n-        attention_output = self.attention(\n-            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n-        )\n-        proj_input = self.layernorm_1(inputs + attention_output)\n-        proj_output = self.dense_proj(proj_input)\n-        return self.layernorm_2(proj_input + proj_output)\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"embed_dim\": self.embed_dim,\n-                \"dense_dim\": self.dense_dim,\n-                \"num_heads\": self.num_heads,\n-            }\n-        )\n-        return config\n-\n-\n-class PositionalEmbedding(layers.Layer):\n-    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.token_embeddings = layers.Embedding(\n-            input_dim=vocab_size, output_dim=embed_dim\n-        )\n-        self.position_embeddings = layers.Embedding(\n-            input_dim=sequence_length, output_dim=embed_dim\n-        )\n-        self.sequence_length = sequence_length\n-        self.vocab_size = vocab_size\n-        self.embed_dim = embed_dim\n-\n-    def call(self, inputs):\n-        length = ops.shape(inputs)[-1]\n-        positions = ops.arange(0, length, 1)\n-        embedded_tokens = self.token_embeddings(inputs)\n-        embedded_positions = self.position_embeddings(positions)\n-        return embedded_tokens + embedded_positions\n-\n-    def compute_mask(self, inputs, mask=None):\n-        if mask is None:\n-            return None\n-        else:\n-            return ops.not_equal(inputs, 0)\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"sequence_length\": self.sequence_length,\n-                \"vocab_size\": self.vocab_size,\n-                \"embed_dim\": self.embed_dim,\n-            }\n-        )\n-        return config\n-\n-\n-class TransformerDecoder(layers.Layer):\n-    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n-        super().__init__(**kwargs)\n-        self.embed_dim = embed_dim\n-        self.latent_dim = latent_dim\n-        self.num_heads = num_heads\n-        self.attention_1 = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim\n-        )\n-        self.attention_2 = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim\n-        )\n-        self.dense_proj = keras.Sequential(\n-            [\n-                layers.Dense(latent_dim, activation=\"relu\"),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm_1 = layers.LayerNormalization()\n-        self.layernorm_2 = layers.LayerNormalization()\n-        self.layernorm_3 = layers.LayerNormalization()\n-        self.supports_masking = True\n-\n-    def call(self, inputs, encoder_outputs, mask=None):\n-        causal_mask = self.get_causal_attention_mask(inputs)\n-        if mask is not None:\n-            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n-            padding_mask = ops.minimum(padding_mask, causal_mask)\n-        else:\n-            padding_mask = None\n-\n-        attention_output_1 = self.attention_1(\n-            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n-        )\n-        out_1 = self.layernorm_1(inputs + attention_output_1)\n-\n-        attention_output_2 = self.attention_2(\n-            query=out_1,\n-            value=encoder_outputs,\n-            key=encoder_outputs,\n-            attention_mask=padding_mask,\n-        )\n-        out_2 = self.layernorm_2(out_1 + attention_output_2)\n-\n-        proj_output = self.dense_proj(out_2)\n-        return self.layernorm_3(out_2 + proj_output)\n-\n-    def get_causal_attention_mask(self, inputs):\n-        input_shape = ops.shape(inputs)\n-        batch_size, sequence_length = input_shape[0], input_shape[1]\n-        i = ops.arange(sequence_length)[:, None]\n-        j = ops.arange(sequence_length)\n-        mask = ops.cast(i >= j, dtype=\"int32\")\n-        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n-        mult = ops.concatenate(\n-            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n-            axis=0,\n-        )\n-        return ops.tile(mask, mult)\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"embed_dim\": self.embed_dim,\n-                \"latent_dim\": self.latent_dim,\n-                \"num_heads\": self.num_heads,\n-            }\n-        )\n-        return config\n-\n-\n-\"\"\"\n-Next, we assemble the end-to-end model.\n-\"\"\"\n-\n-embed_dim = 256\n-latent_dim = 2048\n-num_heads = 8\n-\n-encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n-x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n-encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n-encoder = keras.Model(encoder_inputs, encoder_outputs)\n-\n-decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n-encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n-x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n-x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n-x = layers.Dropout(0.5)(x)\n-decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n-decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n-\n-decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n-transformer = keras.Model(\n-    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n-)\n-\n-\"\"\"\n-## Training our model\n-\n-We'll use accuracy as a quick way to monitor training progress on the validation data.\n-Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n-\n-Here we only train for 1 epoch, but to get the model to actually converge\n-you should train for at least 30 epochs.\n-\"\"\"\n-\n-epochs = 1  # This should be at least 30 for convergence\n-\n-transformer.summary()\n-transformer.compile(\n-    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n-)\n-transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n-\n-\"\"\"\n-## Decoding test sentences\n-\n-Finally, let's demonstrate how to translate brand new English sentences.\n-We simply feed into the model the vectorized English sentence\n-as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n-we hit the token `\"[end]\"`.\n-\"\"\"\n-\n-spa_vocab = spa_vectorization.get_vocabulary()\n-spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n-max_decoded_sentence_length = 20\n-\n-\n-def decode_sequence(input_sentence):\n-    tokenized_input_sentence = eng_vectorization([input_sentence])\n-    decoded_sentence = \"[start]\"\n-    for i in range(max_decoded_sentence_length):\n-        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n-        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n-\n-        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n-        sampled_token_index = ops.convert_to_numpy(ops.argmax(predictions[0, i, :])).item(0)\n-        sampled_token = spa_index_lookup[sampled_token_index]\n-        decoded_sentence += \" \" + sampled_token\n-\n-        if sampled_token == \"[end]\":\n-            break\n-    return decoded_sentence\n-\n-\n-test_eng_texts = [pair[0] for pair in test_pairs]\n-for _ in range(30):\n-    input_sentence = random.choice(test_eng_texts)\n-    translated = decode_sequence(input_sentence)\n-\n-\"\"\"\n-After 30 epochs, we get results such as:\n-\n-> She handed him the money.\n-> [start] ella le pasó el dinero [end]\n-\n-> Tom has never heard Mary sing.\n-> [start] tom nunca ha oído cantar a mary [end]\n-\n-> Perhaps she will come tomorrow.\n-> [start] tal vez ella vendrá mañana [end]\n-\n-> I love to write.\n-> [start] me encanta escribir [end]\n-\n-> His French is improving little by little.\n-> [start] su francés va a [UNK] sólo un poco [end]\n-\n-> My hotel told me to call you.\n-> [start] mi hotel me dijo que te [UNK] [end]\n-\"\"\"\n\n@@ -1,507 +0,0 @@\n-\"\"\"\n-Title: Pretraining BERT with Hugging Face Transformers\n-Author: Sreyan Ghosh\n-Date created: 2022/07/01\n-Last modified: 2022/08/27\n-Description: Pretraining BERT using Hugging Face Transformers on NSP and MLM.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\"\"\"\n-\n-\"\"\"\n-### BERT (Bidirectional Encoder Representations from Transformers)\n-\n-In the field of computer vision, researchers have repeatedly shown the value of\n-transfer learning — pretraining a neural network model on a known task/dataset, for\n-instance ImageNet classification, and then performing fine-tuning — using the trained neural\n-network as the basis of a new specific-purpose model. In recent years, researchers\n-have shown that a similar technique can be useful in many natural language tasks.\n-\n-BERT makes use of Transformer, an attention mechanism that learns contextual relations\n-between words (or subwords) in a text. In its vanilla form, Transformer includes two\n-separate mechanisms — an encoder that reads the text input and a decoder that produces\n-a prediction for the task. Since BERT’s goal is to generate a language model, only the\n-encoder mechanism is necessary. The detailed workings of Transformer are described in\n-a paper by Google.\n-\n-As opposed to directional models, which read the text input sequentially\n-(left-to-right or right-to-left), the Transformer encoder reads the entire\n-sequence of words at once. Therefore it is considered bidirectional, though\n-it would be more accurate to say that it’s non-directional. This characteristic\n-allows the model to learn the context of a word based on all of its surroundings\n-(left and right of the word).\n-\n-When training language models, a challenge is defining a prediction goal.\n-Many models predict the next word in a sequence (e.g. `\"The child came home from _\"`),\n-a directional approach which inherently limits context learning. To overcome this\n-challenge, BERT uses two training strategies:\n-\n-### Masked Language Modeling (MLM)\n-\n-Before feeding word sequences into BERT, 15% of the words in each sequence are replaced\n-with a `[MASK]` token. The model then attempts to predict the original value of the masked\n-words, based on the context provided by the other, non-masked, words in the sequence.\n-\n-### Next Sentence Prediction (NSP)\n-\n-In the BERT training process, the model receives pairs of sentences as input and learns to\n-predict if the second sentence in the pair is the subsequent sentence in the original\n-document. During training, 50% of the inputs are a pair in which the second sentence is the\n-subsequent sentence in the original document, while in the other 50% a random sentence\n-from the corpus is chosen as the second sentence. The assumption is that the random sentence\n-will represent a disconnect from the first sentence.\n-\n-Though Google provides a pretrained BERT checkpoint for English, you may often need\n-to either pretrain the model from scratch for a different language, or do a\n-continued-pretraining to fit the model to a new domain. In this notebook, we pretrain\n-BERT from scratch optimizing both MLM and NSP objectves using 🤗 Transformers on the `WikiText`\n-English dataset loaded from 🤗 Datasets.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-\"\"\"\n-### Installing the requirements\n-\"\"\"\n-\n-\"\"\"shell\n-pip install git+https://github.com/huggingface/transformers.git\n-pip install datasets\n-pip install huggingface-hub\n-pip install nltk\n-\"\"\"\n-\n-\"\"\"\n-### Importing the necessary libraries\n-\"\"\"\n-\n-import nltk\n-import random\n-import logging\n-\n-import keras\n-\n-nltk.download(\"punkt\")\n-# Set random seed\n-keras.utils.set_random_seed(42)\n-\n-\"\"\"\n-### Define certain variables\n-\"\"\"\n-\n-TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n-TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n-\n-BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n-NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n-SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n-MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n-\n-MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n-\n-TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n-MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n-LEARNING_RATE = 1e-4  # Learning rate for training the model\n-\n-MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from 🤗 Model Hub\n-\n-\"\"\"\n-## Load the WikiText dataset\n-\"\"\"\n-\n-\"\"\"\n-We now download the `WikiText` language modeling dataset. It is a collection of over\n-100 million tokens extracted from the set of verified \"Good\" and \"Featured\" articles on\n-Wikipedia.\n-\n-We load the dataset from [🤗 Datasets](https://github.com/huggingface/datasets).\n-For the purpose of demonstration in this notebook, we work with only the `train`\n-split of the dataset. This can be easily done with the `load_dataset` function.\n-\"\"\"\n-\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n-\n-\"\"\"\n-The dataset just has one column which is the raw text, and this is all we need for\n-pretraining BERT!\n-\"\"\"\n-\n-print(dataset)\n-\n-\"\"\"\n-## Training a new Tokenizer\n-\"\"\"\n-\n-\"\"\"\n-First we train our own tokenizer from scratch on our corpus, so that can we\n-can use it to train our language model from scratch.\n-\n-But why would you need to train a tokenizer? That's because Transformer models very\n-often use subword tokenization algorithms, and they need to be trained to identify the\n-parts of words that are often present in the corpus you are using.\n-\n-The 🤗 Transformers `Tokenizer` (as the name indicates) will tokenize the inputs\n-(including converting the tokens to their corresponding IDs in the pretrained vocabulary)\n-and put it in a format the model expects, as well as generate the other inputs that model\n-requires.\n-\n-First we make a list of all the raw documents from the `WikiText` corpus:\n-\"\"\"\n-\n-all_texts = [\n-    doc for doc in dataset[\"train\"][\"text\"] if len(doc) > 0 and not doc.startswith(\" =\")\n-]\n-\n-\"\"\"\n-Next we make a `batch_iterator` function that will aid us to train our tokenizer.\n-\"\"\"\n-\n-\n-def batch_iterator():\n-    for i in range(0, len(all_texts), TOKENIZER_BATCH_SIZE):\n-        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]\n-\n-\n-\"\"\"\n-In this notebook, we train a tokenizer with the exact same algorithms and\n-parameters as an existing one. For instance, we train a new version of the\n-`BERT-CASED` tokenzier on `Wikitext-2` using the same tokenization algorithm.\n-\n-First we need to load the tokenizer we want to use as a model:\n-\"\"\"\n-\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n-\n-\"\"\"\n-Now we train our tokenizer using the entire `train` split of the `Wikitext-2`\n-dataset.\n-\"\"\"\n-\n-tokenizer = tokenizer.train_new_from_iterator(\n-    batch_iterator(), vocab_size=TOKENIZER_VOCABULARY\n-)\n-\n-\"\"\"\n-So now we our done training our new tokenizer! Next we move on to the data\n-pre-processing steps.\n-\"\"\"\n-\n-\"\"\"\n-## Data Pre-processing\n-\"\"\"\n-\n-\"\"\"\n-For the sake of demonstrating the workflow, in this notebook we only take\n-small subsets of the entire WikiText `train` and `test` splits.\n-\"\"\"\n-\n-dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n-dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(1000)])\n-\n-\"\"\"\n-Before we can feed those texts to our model, we need to pre-process them and get them\n-ready for the task. As mentioned earlier, the BERT pretraining task includes two tasks\n-in total, the `NSP` task and the `MLM` task. 🤗 Transformers have an easy to implement\n-`collator` called the `DataCollatorForLanguageModeling`. However, we need to get the\n-data ready for `NSP` manually.\n-\n-Next we write a simple function called the `prepare_train_features` that helps us in\n-the pre-processing and is compatible with 🤗 Datasets. To summarize, our pre-processing\n-function should:\n-\n-- Get the dataset ready for the NSP task by creating pairs of sentences (A,B), where B\n-either actually follows A, or B is randomly sampled from somewhere else in the corpus.\n-It should also generate a corresponding label for each pair, which is 1 if B actually\n-follows A and 0 if not.\n-- Tokenize the text dataset into it's corresponding token ids that will be used for\n-embedding look-up in BERT\n-- Create additional inputs for the model like `token_type_ids`, `attention_mask`, etc.\n-\"\"\"\n-\n-# We define the maximum number of tokens after tokenization that each training sample\n-# will have\n-max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n-\n-\n-def prepare_train_features(examples):\n-    \"\"\"Function to prepare features for NSP task\n-\n-    Arguments:\n-      examples: A dictionary with 1 key (\"text\")\n-        text: List of raw documents (str)\n-    Returns:\n-      examples:  A dictionary with 4 keys\n-        input_ids: List of tokenized, concatnated, and batched\n-          sentences from the individual raw documents (int)\n-        token_type_ids: List of integers (0 or 1) corresponding\n-          to: 0 for senetence no. 1 and padding, 1 for sentence\n-          no. 2\n-        attention_mask: List of integers (0 or 1) corresponding\n-          to: 1 for non-padded tokens, 0 for padded\n-        next_sentence_label: List of integers (0 or 1) corresponding\n-          to: 1 if the second sentence actually follows the first,\n-          0 if the senetence is sampled from somewhere else in the corpus\n-    \"\"\"\n-\n-    # Remove un-wanted samples from the training set\n-    examples[\"document\"] = [\n-        d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")\n-    ]\n-    # Split the documents from the dataset into it's individual sentences\n-    examples[\"sentences\"] = [\n-        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n-    ]\n-    # Convert the tokens into ids using the trained tokenizer\n-    examples[\"tokenized_sentences\"] = [\n-        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n-        for doc in examples[\"sentences\"]\n-    ]\n-\n-    # Define the outputs\n-    examples[\"input_ids\"] = []\n-    examples[\"token_type_ids\"] = []\n-    examples[\"attention_mask\"] = []\n-    examples[\"next_sentence_label\"] = []\n-\n-    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n-        current_chunk = []  # a buffer stored current working segments\n-        current_length = 0\n-        i = 0\n-\n-        # We *usually* want to fill up the entire sequence since we are padding\n-        # to `block_size` anyways, so short sequences are generally wasted\n-        # computation. However, we *sometimes*\n-        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n-        # sequences to minimize the mismatch between pretraining and fine-tuning.\n-        # The `target_seq_length` is just a rough target however, whereas\n-        # `block_size` is a hard limit.\n-        target_seq_length = max_num_tokens\n-\n-        if random.random() < SHORT_SEQ_PROB:\n-            target_seq_length = random.randint(2, max_num_tokens)\n-\n-        while i < len(document):\n-            segment = document[i]\n-            current_chunk.append(segment)\n-            current_length += len(segment)\n-            if i == len(document) - 1 or current_length >= target_seq_length:\n-                if current_chunk:\n-                    # `a_end` is how many segments from `current_chunk` go into the `A`\n-                    # (first) sentence.\n-                    a_end = 1\n-                    if len(current_chunk) >= 2:\n-                        a_end = random.randint(1, len(current_chunk) - 1)\n-\n-                    tokens_a = []\n-                    for j in range(a_end):\n-                        tokens_a.extend(current_chunk[j])\n-\n-                    tokens_b = []\n-\n-                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n-                        is_random_next = True\n-                        target_b_length = target_seq_length - len(tokens_a)\n-\n-                        # This should rarely go for more than one iteration for large\n-                        # corpora. However, just to be careful, we try to make sure that\n-                        # the random document is not the same as the document\n-                        # we're processing.\n-                        for _ in range(10):\n-                            random_document_index = random.randint(\n-                                0, len(examples[\"tokenized_sentences\"]) - 1\n-                            )\n-                            if random_document_index != doc_index:\n-                                break\n-\n-                        random_document = examples[\"tokenized_sentences\"][\n-                            random_document_index\n-                        ]\n-                        random_start = random.randint(0, len(random_document) - 1)\n-                        for j in range(random_start, len(random_document)):\n-                            tokens_b.extend(random_document[j])\n-                            if len(tokens_b) >= target_b_length:\n-                                break\n-                        # We didn't actually use these segments so we \"put them back\" so\n-                        # they don't go to waste.\n-                        num_unused_segments = len(current_chunk) - a_end\n-                        i -= num_unused_segments\n-                    else:\n-                        is_random_next = False\n-                        for j in range(a_end, len(current_chunk)):\n-                            tokens_b.extend(current_chunk[j])\n-\n-                    input_ids = tokenizer.build_inputs_with_special_tokens(\n-                        tokens_a, tokens_b\n-                    )\n-                    # add token type ids, 0 for sentence a, 1 for sentence b\n-                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n-                        tokens_a, tokens_b\n-                    )\n-\n-                    padded = tokenizer.pad(\n-                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n-                        padding=\"max_length\",\n-                        max_length=MAX_LENGTH,\n-                    )\n-\n-                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n-                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n-                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n-                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n-                    current_chunk = []\n-                    current_length = 0\n-            i += 1\n-\n-    # We delete all the un-necessary columns from our dataset\n-    del examples[\"document\"]\n-    del examples[\"sentences\"]\n-    del examples[\"text\"]\n-    del examples[\"tokenized_sentences\"]\n-\n-    return examples\n-\n-\n-tokenized_dataset = dataset.map(\n-    prepare_train_features,\n-    batched=True,\n-    remove_columns=[\"text\"],\n-    num_proc=1,\n-)\n-\n-\"\"\"\n-For MLM we are going to use the same preprocessing as before for our dataset with\n-one additional step: we randomly mask some tokens (by replacing them by [MASK])\n-and the labels will be adjusted to only include the masked tokens\n-(we don't have to predict the non-masked tokens). If you use a tokenizer you trained\n-yourself, make sure the [MASK] token is among the special tokens you passed during training!\n-\n-To get the data ready for MLM, we simply use the `collator` called the\n-`DataCollatorForLanguageModeling` provided by the 🤗 Transformers library on our dataset\n-that is already ready for the NSP task. The `collator` expects certain parameters.\n-We use the default ones from the original BERT paper in this notebook. The\n-`return_tensors='tf'` ensures that we get `tf.Tensor` objects back.\n-\"\"\"\n-\n-from transformers import DataCollatorForLanguageModeling\n-\n-collater = DataCollatorForLanguageModeling(\n-    tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB, return_tensors=\"tf\"\n-)\n-\n-\"\"\"\n-Next we define our training set with which we train our model. Again, 🤗 Datasets\n-provides us with the `to_tf_dataset` method which will help us integrate our dataset with\n-the `collator` defined above. The method expects certain parameters:\n-\n-- **columns**: the columns which will serve as our independant variables\n-- **label_cols**: the columns which will serve as our labels or dependant variables\n-- **batch_size**: our batch size for training\n-- **shuffle**: whether we want to shuffle our training dataset\n-- **collate_fn**: our collator function\n-\"\"\"\n-\n-train = tokenized_dataset[\"train\"].to_tf_dataset(\n-    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n-    label_cols=[\"labels\", \"next_sentence_label\"],\n-    batch_size=TRAIN_BATCH_SIZE,\n-    shuffle=True,\n-    collate_fn=collater,\n-)\n-\n-validation = tokenized_dataset[\"validation\"].to_tf_dataset(\n-    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n-    label_cols=[\"labels\", \"next_sentence_label\"],\n-    batch_size=TRAIN_BATCH_SIZE,\n-    shuffle=True,\n-    collate_fn=collater,\n-)\n-\n-\"\"\"\n-## Defining the model\n-\"\"\"\n-\n-\"\"\"\n-To define our model, first we need to define a config which will help us define certain\n-parameters of our model architecture. This includes parameters like number of transformer\n-layers, number of attention heads, hidden dimension, etc. For this notebook, we try\n-to define the exact config defined in the original BERT paper.\n-\n-We can easily achieve this using the `BertConfig` class from the 🤗 Transformers library.\n-The `from_pretrained()` method expects the name of a model. Here we define the simplest\n-model with which we also trained our model, i.e., `bert-base-cased`.\n-\"\"\"\n-\n-from transformers import BertConfig\n-\n-config = BertConfig.from_pretrained(MODEL_CHECKPOINT)\n-\n-\"\"\"\n-For defining our model we use the `TFBertForPreTraining` class from the 🤗 Transformers\n-library. This class internally handles everything starting from defining our model, to\n-unpacking our inputs and calculating the loss. So we need not do anything ourselves except\n-defining the model with the correct `config` we want!\n-\"\"\"\n-\n-from transformers import TFBertForPreTraining\n-\n-model = TFBertForPreTraining(config)\n-\n-\"\"\"\n-Now we define our optimizer and compile the model. The loss calculation is handled\n-internally and so we need not worry about that!\n-\"\"\"\n-\n-from keras.optimizers import Adam\n-\n-model.compile(optimizer=Adam(learning_rate=LEARNING_RATE))\n-\n-\"\"\"\n-Finally all steps are done and now we can start training our model!\n-\"\"\"\n-\n-model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)\n-\n-\"\"\"\n-Our model has now been trained! We suggest to please train the model on the complete\n-dataset for atleast 50 epochs for decent performance. The pretrained model now acts as\n-a language model and is meant to be fine-tuned on a downstream task. Thus it can now be\n-fine-tuned on any downstream task like Question Answering, Text Classification\n-etc.!\n-\"\"\"\n-\n-\"\"\"\n-Now you can push this model to 🤗 Model Hub and also share it with with all your friends,\n-family, favorite pets: they can all load it with the identifier\n-`\"your-username/the-name-you-picked\"` so for instance:\n-\n-```python\n-model.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n-tokenizer.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n-```\n-And after you push your model this is how you can load it in the future!\n-\n-```python\n-from transformers import TFBertForPreTraining\n-\n-model = TFBertForPreTraining.from_pretrained(\"your-username/my-awesome-model\")\n-```\n-or, since it's a pretrained model and you would generally use it for fine-tuning\n-on a downstream task, you can also load it for some other task like:\n-\n-```python\n-from transformers import TFBertForSequenceClassification\n-\n-model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")\n-```\n-In this case, the pretraining head will be dropped and the model will just be initialized\n-with the transformer layers. A new task-specific head will be added with random weights.\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,285 +0,0 @@\n-\"\"\"\n-Title: Fine-tuning a pre-trained TorchVision Model\n-Author: [Ayush Thakur](https://twitter.com/ayushthakur0), [Soumik Rakshit](https://twitter.com/soumikRakshit96)\n-Date created: 2023/09/18\n-Last modified: 2023/09/18\n-Description: Fine-tuning a pre-trained Torch model from TorchVision for image\n-classification using Keras.\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-[TorchVision](https://pytorch.org/vision/stable/index.html) is a library part of the\n-[PyTorch](http://pytorch.org/) project that consists of popular datasets, model\n-architectures, and common image transformations for computer vision. This example\n-demonstrates how we can perform transfer learning for image classification using a\n-pre-trained backbone model from TorchVision on the [Imagenette\n-dataset](https://github.com/fastai/imagenette) using KerasCore. We will also demonstrate\n-the compatibility of KerasCore with an input system consisting of [Torch Datasets and\n-Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n-\n-### References:\n-\n-- [Customizing what happens in `fit()` with\n-PyTorch](https://keras.io/keras/guides/custom_train_step_in_torch/)\n-- [PyTorch Datasets and\n-Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n-- [Transfer learning for Computer Vision using\n-PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n-- [Fine-tuning a TorchVision Model using Keras\n-](https://wandb.ai/ml-colabs/keras-torch/reports/Fine-tuning-a-TorchVision-Model-using-Keras--Vmlldzo1NDE5NDE1)\n-\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"torch\"\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-import torch\n-import torch.nn as nn\n-import torch.nn.functional as F\n-\n-import torchvision\n-from torchvision import datasets, models, transforms\n-\n-import keras\n-from keras.layers import TorchModuleWrapper\n-\n-\"\"\"\n-## Define the Hyperparameters\n-\"\"\"\n-\n-batch_size = 32\n-image_size = 224\n-initial_learning_rate = 1e-3\n-num_epochs = 5\n-\n-\"\"\"\n-## Creating the Torch Datasets and Dataloaders\n-\n-In this example, we would train an image classification model on the [Imagenette\n-dataset](https://github.com/fastai/imagenette). Imagenette is a subset of 10 easily\n-classified classes from [Imagenet](https://www.image-net.org/) (tench, English springer,\n-cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball,\n-parachute).\n-\"\"\"\n-\n-# Fetch the imagenette dataset\n-data_dir = keras.utils.get_file(\n-    fname=\"imagenette2-320.tgz\",\n-    origin=\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\",\n-    extract=True,\n-)\n-data_dir = data_dir.replace(\".tgz\", \"\")\n-\n-\"\"\"\n-Next, we define pre-processing and augmentation transforms from TorchVision for the train\n-and validation sets.\n-\"\"\"\n-\n-data_transforms = {\n-    \"train\": transforms.Compose(\n-        [\n-            transforms.RandomResizedCrop(image_size),\n-            transforms.RandomHorizontalFlip(),\n-            transforms.ToTensor(),\n-            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n-        ]\n-    ),\n-    \"val\": transforms.Compose(\n-        [\n-            transforms.Resize(256),\n-            transforms.CenterCrop(image_size),\n-            transforms.ToTensor(),\n-            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n-        ]\n-    ),\n-}\n-\n-\"\"\"\n-Finally, we will use TorchVision and the\n-[`torch.utils.data`](https://pytorch.org/docs/stable/data.html) packages for creating the\n-dataloaders for trainig and validation.\n-\"\"\"\n-\n-# Define the train and validation datasets\n-image_datasets = {\n-    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n-    for x in [\"train\", \"val\"]\n-}\n-\n-# Define the torch dataloaders corresponding to the train and validation dataset\n-dataloaders = {\n-    x: torch.utils.data.DataLoader(\n-        image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4\n-    )\n-    for x in [\"train\", \"val\"]\n-}\n-dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\n-class_names = image_datasets[\"train\"].classes\n-\n-# Specify the global device\n-device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-\n-\"\"\"\n-Let us visualize a few samples from the training dataloader.\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-sample_images, sample_labels = next(iter(dataloaders[\"train\"]))\n-sample_images = sample_images.numpy()\n-sample_labels = sample_labels.numpy()\n-for idx in range(9):\n-    ax = plt.subplot(3, 3, idx + 1)\n-    image = sample_images[idx].transpose((1, 2, 0))\n-    mean = np.array([0.485, 0.456, 0.406])\n-    std = np.array([0.229, 0.224, 0.225])\n-    image = std * image + mean\n-    image = np.clip(image, 0, 1)\n-    plt.imshow(image)\n-    plt.title(\"Ground Truth Label: \" + class_names[int(sample_labels[idx])])\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## The Image Classification Model\n-\"\"\"\n-\n-\"\"\"\n-We typically define a model in PyTorch using\n-[`torch.nn.Module`s](https://pytorch.org/docs/stable/notes/modules.html) which act as the\n-building blocks of stateful computation. Let us define the ResNet18 model from the\n-TorchVision package as a `torch.nn.Module` pre-trained on the [Imagenet1K\n-dataset](https://huggingface.co/datasets/imagenet-1k).\n-\"\"\"\n-\n-# Define the pre-trained resnet18 module from TorchVision\n-resnet_18 = models.resnet18(weights=\"IMAGENET1K_V1\")\n-\n-# We set the classification head of the pre-trained ResNet18\n-# module to an identity module\n-resnet_18.fc = nn.Identity()\n-\n-\"\"\"\n-Even though Keras supports PyTorch as a backend, it does not mean that we can nest torch\n-modules inside a [`keras.Model`](https://keras.io/keras/api/models/), because\n-trainable variables inside a Keras Model is tracked exclusively via [Keras\n-Layers](https://keras.io/keras/api/layers/).\n-\n-KerasCore provides us with a feature called `TorchModuleWrapper` which enables us to do\n-exactly this. The `TorchModuleWrapper` is a Keras Layer that accepts a torch module and\n-tracks its trainable variables, essentially converting the torch module into a Keras\n-Layer. This enables us to put any torch modules inside a Keras Model and train them with\n-a single `model.fit()`!\n-\"\"\"\n-\n-# We set the trainable ResNet18 backbone to be a Keras Layer\n-# using `TorchModuleWrapper`\n-backbone = TorchModuleWrapper(resnet_18)\n-\n-# We set this to `False` if you want to freeze the backbone\n-backbone.trainable = True\n-\n-\"\"\"\n-Now, we will build a Keras functional model with the backbone layer.\n-\"\"\"\n-\n-inputs = keras.Input(shape=(3, image_size, image_size))\n-x = backbone(inputs)\n-x = keras.layers.Dropout(0.5)(x)\n-x = keras.layers.Dense(len(class_names))(x)\n-outputs = keras.activations.softmax(x, axis=1)\n-model = keras.Model(inputs, outputs, name=\"ResNet18_Classifier\")\n-\n-model.summary()\n-\n-# Create exponential decay learning rate scheduler\n-decay_steps = num_epochs * len(dataloaders[\"train\"]) // batch_size\n-lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n-    initial_learning_rate=initial_learning_rate,\n-    decay_steps=decay_steps,\n-    decay_rate=0.1,\n-)\n-\n-# Compile the model\n-model.compile(\n-    loss=\"sparse_categorical_crossentropy\",\n-    optimizer=keras.optimizers.Adam(lr_scheduler),\n-    metrics=[\"accuracy\"],\n-)\n-\n-# Define the backend-agnostic WandB callbacks for KerasCore\n-callbacks = [\n-    # Save best model checkpoints\n-    keras.callbacks.ModelCheckpoint(\n-        filepath=\"model.weights.h5\",\n-        monitor=\"val_loss\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-]\n-\n-# Train the model by calling model.fit\n-history = model.fit(\n-    dataloaders[\"train\"],\n-    validation_data=dataloaders[\"val\"],\n-    epochs=num_epochs,\n-    callbacks=callbacks,\n-)\n-\n-\n-def plot_history(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_history(\"loss\")\n-plot_history(\"accuracy\")\n-\n-\"\"\"\n-## Evaluation and Inference\n-\n-Now, we let us load the best model weights checkpoint and evaluate the model.\n-\"\"\"\n-\n-model.load_weights(\"model.weights.h5\")\n-_, val_accuracy = model.evaluate(dataloaders[\"val\"])\n-print(\"Best Validation Accuracy:\", val_accuracy)\n-\n-\"\"\"\n-Finally, let us visualize the some predictions of the model\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-sample_images, sample_labels = next(iter(dataloaders[\"train\"]))\n-\n-# We perform inference and detach the predicted probabilities from the Torch\n-# computation graph with a tensor that does not require gradient computation.\n-sample_pred_probas = model(sample_images.to(\"cuda\")).detach()\n-sample_pred_logits = keras.ops.argmax(sample_pred_probas, axis=1)\n-sample_pred_logits = sample_pred_logits.to(\"cpu\").numpy()\n-\n-sample_images = sample_images.numpy()\n-sample_labels = sample_labels.numpy()\n-\n-for idx in range(9):\n-    ax = plt.subplot(3, 3, idx + 1)\n-    image = sample_images[idx].transpose((1, 2, 0))\n-    mean = np.array([0.485, 0.456, 0.406])\n-    std = np.array([0.229, 0.224, 0.225])\n-    image = std * image + mean\n-    image = np.clip(image, 0, 1)\n-    plt.imshow(image)\n-    title = \"Ground Truth Label: \" + class_names[int(sample_labels[idx])]\n-    title += \"\\nPredicted Label: \" + class_names[int(sample_pred_logits[idx])]\n-    plt.title(title)\n-    plt.axis(\"off\")\n\n@@ -1,242 +0,0 @@\n-\"\"\"\n-Title: Collaborative Filtering for Movie Recommendations\n-Author: [Siddhartha Banerjee](https://twitter.com/sidd2006)\n-Date created: 2020/05/24\n-Last modified: 2020/05/24\n-Description: Recommending movies using a model trained on Movielens dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates\n-[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n-using the [Movielens dataset](https://www.kaggle.com/c/movielens-100k)\n-to recommend movies to users.\n-The MovieLens ratings dataset lists the ratings given by a set of users to a set of movies.\n-Our goal is to be able to predict ratings for movies a user has not yet watched.\n-The movies with the highest predicted ratings can then be recommended to the user.\n-\n-The steps in the model are as follows:\n-\n-1. Map user ID to a \"user vector\" via an embedding matrix\n-2. Map movie ID to a \"movie vector\" via an embedding matrix\n-3. Compute the dot product between the user vector and movie vector, to obtain\n-the a match score between the user and the movie (predicted rating).\n-4. Train the embeddings via gradient descent using all known user-movie pairs.\n-\n-**References:**\n-\n-- [Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/371920.372071)\n-- [Neural Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/3038912.3052569)\n-\"\"\"\n-\n-import pandas as pd\n-from pathlib import Path\n-import matplotlib.pyplot as plt\n-import numpy as np\n-from zipfile import ZipFile\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-\"\"\"\n-## First, load the data and apply preprocessing\n-\"\"\"\n-\n-# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n-# Use the ratings.csv file\n-movielens_data_file_url = (\n-    \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n-)\n-movielens_zipped_file = keras.utils.get_file(\n-    \"ml-latest-small.zip\", movielens_data_file_url, extract=False\n-)\n-keras_datasets_path = Path(movielens_zipped_file).parents[0]\n-movielens_dir = keras_datasets_path / \"ml-latest-small\"\n-\n-# Only extract the data the first time the script is run.\n-if not movielens_dir.exists():\n-    with ZipFile(movielens_zipped_file, \"r\") as zip:\n-        # Extract files\n-        print(\"Extracting all the files now...\")\n-        zip.extractall(path=keras_datasets_path)\n-        print(\"Done!\")\n-\n-ratings_file = movielens_dir / \"ratings.csv\"\n-df = pd.read_csv(ratings_file)\n-\n-\"\"\"\n-First, need to perform some preprocessing to encode users and movies as integer indices.\n-\"\"\"\n-user_ids = df[\"userId\"].unique().tolist()\n-user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n-userencoded2user = {i: x for i, x in enumerate(user_ids)}\n-movie_ids = df[\"movieId\"].unique().tolist()\n-movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n-movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n-df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n-df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n-\n-num_users = len(user2user_encoded)\n-num_movies = len(movie_encoded2movie)\n-df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n-# min and max ratings will be used to normalize the ratings later\n-min_rating = min(df[\"rating\"])\n-max_rating = max(df[\"rating\"])\n-\n-print(\n-    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n-        num_users, num_movies, min_rating, max_rating\n-    )\n-)\n-\n-\"\"\"\n-## Prepare training and validation data\n-\"\"\"\n-df = df.sample(frac=1, random_state=42)\n-x = df[[\"user\", \"movie\"]].values\n-# Normalize the targets between 0 and 1. Makes it easy to train.\n-y = (\n-    df[\"rating\"]\n-    .apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n-    .values\n-)\n-# Assuming training on 90% of the data and validating on 10%.\n-train_indices = int(0.9 * df.shape[0])\n-x_train, x_val, y_train, y_val = (\n-    x[:train_indices],\n-    x[train_indices:],\n-    y[:train_indices],\n-    y[train_indices:],\n-)\n-\n-\"\"\"\n-## Create the model\n-\n-We embed both users and movies in to 50-dimensional vectors.\n-\n-The model computes a match score between user and movie embeddings via a dot product,\n-and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n-interval via a sigmoid (since our ratings are normalized to this range).\n-\"\"\"\n-EMBEDDING_SIZE = 50\n-\n-\n-class RecommenderNet(keras.Model):\n-    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_users = num_users\n-        self.num_movies = num_movies\n-        self.embedding_size = embedding_size\n-        self.user_embedding = layers.Embedding(\n-            num_users,\n-            embedding_size,\n-            embeddings_initializer=\"he_normal\",\n-            embeddings_regularizer=keras.regularizers.l2(1e-6),\n-        )\n-        self.user_bias = layers.Embedding(num_users, 1)\n-        self.movie_embedding = layers.Embedding(\n-            num_movies,\n-            embedding_size,\n-            embeddings_initializer=\"he_normal\",\n-            embeddings_regularizer=keras.regularizers.l2(1e-6),\n-        )\n-        self.movie_bias = layers.Embedding(num_movies, 1)\n-\n-    def call(self, inputs):\n-        user_vector = self.user_embedding(inputs[:, 0])\n-        user_bias = self.user_bias(inputs[:, 0])\n-        movie_vector = self.movie_embedding(inputs[:, 1])\n-        movie_bias = self.movie_bias(inputs[:, 1])\n-        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)\n-        # Add all the components (including bias)\n-        x = dot_user_movie + user_bias + movie_bias\n-        # The sigmoid activation forces the rating to between 0 and 1\n-        return ops.nn.sigmoid(x)\n-\n-\n-model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n-model.compile(\n-    loss=keras.losses.BinaryCrossentropy(),\n-    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n-)\n-\n-\"\"\"\n-## Train the model based on the data split\n-\"\"\"\n-history = model.fit(\n-    x=x_train,\n-    y=y_train,\n-    batch_size=64,\n-    epochs=5,\n-    verbose=1,\n-    validation_data=(x_val, y_val),\n-)\n-\n-\"\"\"\n-## Plot training and validation loss\n-\"\"\"\n-plt.plot(history.history[\"loss\"])\n-plt.plot(history.history[\"val_loss\"])\n-plt.title(\"model loss\")\n-plt.ylabel(\"loss\")\n-plt.xlabel(\"epoch\")\n-plt.legend([\"train\", \"test\"], loc=\"upper left\")\n-plt.show()\n-\n-\"\"\"\n-## Show top 10 movie recommendations to a user\n-\"\"\"\n-\n-movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n-\n-# Let us get a user and see the top recommendations.\n-user_id = df.userId.sample(1).iloc[0]\n-movies_watched_by_user = df[df.userId == user_id]\n-movies_not_watched = movie_df[\n-    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n-][\"movieId\"]\n-movies_not_watched = list(\n-    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n-)\n-movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n-user_encoder = user2user_encoded.get(user_id)\n-user_movie_array = np.hstack(\n-    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n-)\n-ratings = model.predict(user_movie_array).flatten()\n-top_ratings_indices = ratings.argsort()[-10:][::-1]\n-recommended_movie_ids = [\n-    movie_encoded2movie.get(movies_not_watched[x][0])\n-    for x in top_ratings_indices\n-]\n-\n-print(\"Showing recommendations for user: {}\".format(user_id))\n-print(\"====\" * 9)\n-print(\"Movies with high ratings from user\")\n-print(\"----\" * 8)\n-top_movies_user = (\n-    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n-    .head(5)\n-    .movieId.values\n-)\n-movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\n-for row in movie_df_rows.itertuples():\n-    print(row.title, \":\", row.genres)\n-\n-print(\"----\" * 8)\n-print(\"Top 10 movie recommendations\")\n-print(\"----\" * 8)\n-recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n-for row in recommended_movies.itertuples():\n-    print(row.title, \":\", row.genres)\n-\n-\"\"\"\n-**Example available on HuggingFace**\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Collaborative%20Filtering-black.svg)](https://huggingface.co/keras-io/collaborative-filtering-movielens) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Collaborative%20Filtering-black.svg)](https://huggingface.co/spaces/keras-io/collaborative-filtering-movielens) |\n-\"\"\"\n\n@@ -1,476 +0,0 @@\n-\"\"\"\n-Title: Classification with Neural Decision Forests\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2021/01/15\n-Last modified: 2021/01/15\n-Description: How to train differentiable decision trees for end-to-end learning in deep neural networks.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example provides an implementation of the\n-[Deep Neural Decision Forest](https://ieeexplore.ieee.org/document/7410529)\n-model introduced by P. Kontschieder et al. for structured data classification.\n-It demonstrates how to build a stochastic and differentiable decision tree model,\n-train it end-to-end, and unify decision trees with deep representation learning.\n-\n-## The dataset\n-\n-This example uses the\n-[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n-provided by the\n-[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n-The task is binary classification\n-to predict whether a person is likely to be making over USD 50,000 a year.\n-\n-The dataset includes 48,842 instances with 14 input features (such as age, work class, education, occupation, and so on): 5 numerical features\n-and 9 categorical features.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-from keras.layers import StringLookup\n-from keras import ops\n-\n-\n-from tensorflow import data as tf_data\n-import numpy as np\n-import pandas as pd\n-\n-import math\n-\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-CSV_HEADER = [\n-    \"age\",\n-    \"workclass\",\n-    \"fnlwgt\",\n-    \"education\",\n-    \"education_num\",\n-    \"marital_status\",\n-    \"occupation\",\n-    \"relationship\",\n-    \"race\",\n-    \"gender\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-    \"native_country\",\n-    \"income_bracket\",\n-]\n-\n-train_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n-)\n-train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n-\n-test_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n-)\n-test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n-\n-print(f\"Train dataset shape: {train_data.shape}\")\n-print(f\"Test dataset shape: {test_data.shape}\")\n-\n-\"\"\"\n-Remove the first record (because it is not a valid data example) and a trailing\n-'dot' in the class labels.\n-\"\"\"\n-\n-test_data = test_data[1:]\n-test_data.income_bracket = test_data.income_bracket.apply(\n-    lambda value: value.replace(\".\", \"\")\n-)\n-\n-\"\"\"\n-We store the training and test data splits locally as CSV files.\n-\"\"\"\n-\n-train_data_file = \"train_data.csv\"\n-test_data_file = \"test_data.csv\"\n-\n-train_data.to_csv(train_data_file, index=False, header=False)\n-test_data.to_csv(test_data_file, index=False, header=False)\n-\n-\"\"\"\n-## Define dataset metadata\n-\n-Here, we define the metadata of the dataset that will be useful for reading and parsing\n-and encoding input features.\n-\"\"\"\n-\n-# A list of the numerical feature names.\n-NUMERIC_FEATURE_NAMES = [\n-    \"age\",\n-    \"education_num\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-]\n-# A dictionary of the categorical features and their vocabulary.\n-CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n-    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n-    \"education\": sorted(list(train_data[\"education\"].unique())),\n-    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n-    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n-    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n-    \"race\": sorted(list(train_data[\"race\"].unique())),\n-    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n-    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n-}\n-# A list of the columns to ignore from the dataset.\n-IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n-# A list of the categorical feature names.\n-CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n-# A list of all the input features.\n-FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n-# A list of column default values for each feature.\n-COLUMN_DEFAULTS = [\n-    [0.0]\n-    if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES\n-    else [\"NA\"]\n-    for feature_name in CSV_HEADER\n-]\n-# The name of the target feature.\n-TARGET_FEATURE_NAME = \"income_bracket\"\n-# A list of the labels of the target features.\n-TARGET_LABELS = [\" <=50K\", \" >50K\"]\n-\n-\"\"\"\n-## Create `tf_data.Dataset` objects for training and validation\n-\n-We create an input function to read and parse the file, and convert features and labels\n-into a [`tf_data.Dataset`](https://www.tensorflow.org/guide/datasets)\n-for training and validation. We also preprocess the input by mapping the target label\n-to an index.\n-\"\"\"\n-\n-\n-target_label_lookup = StringLookup(\n-    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n-)\n-\n-\n-lookup_dict = {}\n-for feature_name in CATEGORICAL_FEATURE_NAMES:\n-    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-    # Create a lookup to convert a string values to an integer indices.\n-    # Since we are not using a mask token, nor expecting any out of vocabulary\n-    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-    lookup = StringLookup(\n-        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n-    )\n-    lookup_dict[feature_name] = lookup\n-\n-\n-def encode_categorical(batch_x, batch_y):\n-    for feature_name in CATEGORICAL_FEATURE_NAMES:\n-        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n-\n-    return batch_x, batch_y\n-\n-\n-def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n-    dataset = (\n-        tf_data.experimental.make_csv_dataset(\n-            csv_file_path,\n-            batch_size=batch_size,\n-            column_names=CSV_HEADER,\n-            column_defaults=COLUMN_DEFAULTS,\n-            label_name=TARGET_FEATURE_NAME,\n-            num_epochs=1,\n-            header=False,\n-            na_value=\"?\",\n-            shuffle=shuffle,\n-        )\n-        .map(lambda features, target: (features, target_label_lookup(target)))\n-        .map(encode_categorical)\n-    )\n-\n-    return dataset.cache()\n-\n-\n-\"\"\"\n-## Create model inputs\n-\"\"\"\n-\n-\n-def create_model_inputs():\n-    inputs = {}\n-    for feature_name in FEATURE_NAMES:\n-        if feature_name in NUMERIC_FEATURE_NAMES:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=\"float32\"\n-            )\n-        else:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=\"int32\"\n-            )\n-    return inputs\n-\n-\n-\"\"\"\n-## Encode input features\n-\"\"\"\n-\n-\n-def encode_inputs(inputs):\n-    encoded_features = []\n-    for feature_name in inputs:\n-        if feature_name in CATEGORICAL_FEATURE_NAMES:\n-            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-            # Create a lookup to convert a string values to an integer indices.\n-            # Since we are not using a mask token, nor expecting any out of vocabulary\n-            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-            value_index = inputs[feature_name]\n-            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n-            # Create an embedding layer with the specified dimensions.\n-            embedding = layers.Embedding(\n-                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n-            )\n-            # Convert the index values to embedding representations.\n-            encoded_feature = embedding(value_index)\n-        else:\n-            # Use the numerical features as-is.\n-            encoded_feature = inputs[feature_name]\n-            if inputs[feature_name].shape[-1] is None:\n-                encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n-\n-        encoded_features.append(encoded_feature)\n-\n-    encoded_features = layers.concatenate(encoded_features)\n-    return encoded_features\n-\n-\n-\"\"\"\n-## Deep Neural Decision Tree\n-\n-A neural decision tree model has two sets of weights to learn. The first set is `pi`,\n-which represents the probability distribution of the classes in the tree leaves.\n-The second set is the weights of the routing layer `decision_fn`, which represents the probability\n-of going to each leave. The forward pass of the model works as follows:\n-\n-1. The model expects input `features` as a single vector encoding all the features of an instance\n-in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images\n-or dense transformations applied to structured data features.\n-2. The model first applies a `used_features_mask` to randomly select a subset of input features to use.\n-3. Then, the model computes the probabilities (`mu`) for the input instances to reach the tree leaves\n-by iteratively performing a *stochastic* routing throughout the tree levels.\n-4. Finally, the probabilities of reaching the leaves are combined by the class probabilities at the\n-leaves to produce the final `outputs`.\n-\"\"\"\n-\n-\n-class NeuralDecisionTree(keras.Model):\n-    def __init__(self, depth, num_features, used_features_rate, num_classes):\n-        super().__init__()\n-        self.depth = depth\n-        self.num_leaves = 2**depth\n-        self.num_classes = num_classes\n-\n-        # Create a mask for the randomly selected features.\n-        num_used_features = int(num_features * used_features_rate)\n-        one_hot = np.eye(num_features)\n-        sampled_feature_indices = np.random.choice(\n-            np.arange(num_features), num_used_features, replace=False\n-        )\n-        self.used_features_mask = ops.convert_to_tensor(\n-            one_hot[sampled_feature_indices], dtype=\"float32\"\n-        )\n-\n-        # Initialize the weights of the classes in leaves.\n-        self.pi = self.add_weight(\n-            initializer=\"random_normal\",\n-            shape=[self.num_leaves, self.num_classes],\n-            dtype=\"float32\",\n-            trainable=True,\n-        )\n-\n-        # Initialize the stochastic routing layer.\n-        self.decision_fn = layers.Dense(\n-            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n-        )\n-\n-    def call(self, features):\n-        batch_size = ops.shape(features)[0]\n-\n-        # Apply the feature mask to the input features.\n-        features = ops.matmul(\n-            features, ops.transpose(self.used_features_mask)\n-        )  # [batch_size, num_used_features]\n-        # Compute the routing probabilities.\n-        decisions = ops.expand_dims(\n-            self.decision_fn(features), axis=2\n-        )  # [batch_size, num_leaves, 1]\n-        # Concatenate the routing probabilities with their complements.\n-        decisions = layers.concatenate(\n-            [decisions, 1 - decisions], axis=2\n-        )  # [batch_size, num_leaves, 2]\n-\n-        mu = ops.ones([batch_size, 1, 1])\n-\n-        begin_idx = 1\n-        end_idx = 2\n-        # Traverse the tree in breadth-first order.\n-        for level in range(self.depth):\n-            mu = ops.reshape(\n-                mu, [batch_size, -1, 1]\n-            )  # [batch_size, 2 ** level, 1]\n-            mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n-            level_decisions = decisions[\n-                :, begin_idx:end_idx, :\n-            ]  # [batch_size, 2 ** level, 2]\n-            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n-            begin_idx = end_idx\n-            end_idx = begin_idx + 2 ** (level + 1)\n-\n-        mu = ops.reshape(\n-            mu, [batch_size, self.num_leaves]\n-        )  # [batch_size, num_leaves]\n-        probabilities = keras.activations.softmax(\n-            self.pi\n-        )  # [num_leaves, num_classes]\n-        outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]\n-        return outputs\n-\n-\n-\"\"\"\n-## Deep Neural Decision Forest\n-\n-The neural decision forest model consists of a set of neural decision trees that are\n-trained simultaneously. The output of the forest model is the average outputs of its trees.\n-\"\"\"\n-\n-\n-class NeuralDecisionForest(keras.Model):\n-    def __init__(\n-        self, num_trees, depth, num_features, used_features_rate, num_classes\n-    ):\n-        super().__init__()\n-        self.ensemble = []\n-        # Initialize the ensemble by adding NeuralDecisionTree instances.\n-        # Each tree will have its own randomly selected input features to use.\n-        for _ in range(num_trees):\n-            self.ensemble.append(\n-                NeuralDecisionTree(\n-                    depth, num_features, used_features_rate, num_classes\n-                )\n-            )\n-\n-    def call(self, inputs):\n-        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n-        batch_size = ops.shape(inputs)[0]\n-        outputs = ops.zeros([batch_size, num_classes])\n-\n-        # Aggregate the outputs of trees in the ensemble.\n-        for tree in self.ensemble:\n-            outputs += tree(inputs)\n-        # Divide the outputs by the ensemble size to get the average.\n-        outputs /= len(self.ensemble)\n-        return outputs\n-\n-\n-\"\"\"\n-Finally, let's set up the code that will train and evaluate the model.\n-\"\"\"\n-\n-learning_rate = 0.01\n-batch_size = 265\n-num_epochs = 10\n-\n-\n-def run_experiment(model):\n-    model.compile(\n-        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n-        loss=keras.losses.SparseCategoricalCrossentropy(),\n-        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-    )\n-\n-    print(\"Start training the model...\")\n-    train_dataset = get_dataset_from_csv(\n-        train_data_file, shuffle=True, batch_size=batch_size\n-    )\n-\n-    model.fit(train_dataset, epochs=num_epochs)\n-    print(\"Model training finished\")\n-\n-    print(\"Evaluating the model on the test data...\")\n-    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n-\n-    _, accuracy = model.evaluate(test_dataset)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-\n-\"\"\"\n-## Experiment 1: train a decision tree model\n-\n-In this experiment, we train a single neural decision tree model\n-where we use all input features.\n-\"\"\"\n-\n-num_trees = 10\n-depth = 10\n-used_features_rate = 1.0\n-num_classes = len(TARGET_LABELS)\n-\n-\n-def create_tree_model():\n-    inputs = create_model_inputs()\n-    features = encode_inputs(inputs)\n-    features = layers.BatchNormalization()(features)\n-    num_features = features.shape[1]\n-\n-    tree = NeuralDecisionTree(\n-        depth, num_features, used_features_rate, num_classes\n-    )\n-\n-    outputs = tree(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-tree_model = create_tree_model()\n-run_experiment(tree_model)\n-\n-\n-\"\"\"\n-## Experiment 2: train a forest model\n-\n-In this experiment, we train a neural decision forest with `num_trees` trees\n-where each tree uses randomly selected 50% of the input features. You can control the number\n-of features to be used in each tree by setting the `used_features_rate` variable.\n-In addition, we set the depth to 5 instead of 10 compared to the previous experiment.\n-\"\"\"\n-\n-num_trees = 25\n-depth = 5\n-used_features_rate = 0.5\n-\n-\n-def create_forest_model():\n-    inputs = create_model_inputs()\n-    features = encode_inputs(inputs)\n-    features = layers.BatchNormalization()(features)\n-    num_features = features.shape[1]\n-\n-    forest_model = NeuralDecisionForest(\n-        num_trees, depth, num_features, used_features_rate, num_classes\n-    )\n-\n-    outputs = forest_model(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-forest_model = create_forest_model()\n-\n-run_experiment(forest_model)\n\n@@ -1,576 +0,0 @@\n-\"\"\"\n-Title: Structured data learning with TabTransformer\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2022/01/18\n-Last modified: 2022/01/18\n-Description: Using contextual embeddings for structured data classification.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to do structured data classification using\n-[TabTransformer](https://arxiv.org/abs/2012.06678), a deep tabular data modeling\n-architecture for supervised and semi-supervised learning.\n-The TabTransformer is built upon self-attention based Transformers.\n-The Transformer layers transform the embeddings of categorical features\n-into robust contextual embeddings to achieve higher predictive accuracy.\n-\n-\n-\n-## Setup\n-\"\"\"\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-import math\n-import numpy as np\n-import pandas as pd\n-from tensorflow import data as tf_data\n-import matplotlib.pyplot as plt\n-from functools import partial\n-\n-\"\"\"\n-## Prepare the data\n-\n-This example uses the\n-[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n-provided by the\n-[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n-The task is binary classification\n-to predict whether a person is likely to be making over USD 50,000 a year.\n-\n-The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n-\n-First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n-DataFrame:\n-\"\"\"\n-\n-CSV_HEADER = [\n-    \"age\",\n-    \"workclass\",\n-    \"fnlwgt\",\n-    \"education\",\n-    \"education_num\",\n-    \"marital_status\",\n-    \"occupation\",\n-    \"relationship\",\n-    \"race\",\n-    \"gender\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-    \"native_country\",\n-    \"income_bracket\",\n-]\n-\n-train_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n-)\n-train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n-\n-test_data_url = (\n-    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n-)\n-test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n-\n-print(f\"Train dataset shape: {train_data.shape}\")\n-print(f\"Test dataset shape: {test_data.shape}\")\n-\n-\"\"\"\n-Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels.\n-\"\"\"\n-\n-test_data = test_data[1:]\n-test_data.income_bracket = test_data.income_bracket.apply(\n-    lambda value: value.replace(\".\", \"\")\n-)\n-\n-\"\"\"\n-Now we store the training and test data in separate CSV files.\n-\"\"\"\n-\n-train_data_file = \"train_data.csv\"\n-test_data_file = \"test_data.csv\"\n-\n-train_data.to_csv(train_data_file, index=False, header=False)\n-test_data.to_csv(test_data_file, index=False, header=False)\n-\n-\"\"\"\n-## Define dataset metadata\n-\n-Here, we define the metadata of the dataset that will be useful for reading and parsing\n-the data into input features, and encoding the input features with respect to their types.\n-\"\"\"\n-\n-# A list of the numerical feature names.\n-NUMERIC_FEATURE_NAMES = [\n-    \"age\",\n-    \"education_num\",\n-    \"capital_gain\",\n-    \"capital_loss\",\n-    \"hours_per_week\",\n-]\n-# A dictionary of the categorical features and their vocabulary.\n-CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n-    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n-    \"education\": sorted(list(train_data[\"education\"].unique())),\n-    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n-    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n-    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n-    \"race\": sorted(list(train_data[\"race\"].unique())),\n-    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n-    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n-}\n-# Name of the column to be used as instances weight.\n-WEIGHT_COLUMN_NAME = \"fnlwgt\"\n-# A list of the categorical feature names.\n-CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n-# A list of all the input features.\n-FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n-# A list of column default values for each feature.\n-COLUMN_DEFAULTS = [\n-    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n-    for feature_name in CSV_HEADER\n-]\n-# The name of the target feature.\n-TARGET_FEATURE_NAME = \"income_bracket\"\n-# A list of the labels of the target features.\n-TARGET_LABELS = [\" <=50K\", \" >50K\"]\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\n-The hyperparameters includes model architecture and training configurations.\n-\"\"\"\n-\n-LEARNING_RATE = 0.001\n-WEIGHT_DECAY = 0.0001\n-DROPOUT_RATE = 0.2\n-BATCH_SIZE = 265\n-NUM_EPOCHS = 15\n-\n-NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n-NUM_HEADS = 4  # Number of attention heads.\n-EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n-MLP_HIDDEN_UNITS_FACTORS = [\n-    2,\n-    1,\n-]  # MLP hidden layer units, as factors of the number of inputs.\n-NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n-\n-\"\"\"\n-## Implement data reading pipeline\n-\n-We define an input function that reads and parses the file, then converts features\n-and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n-for training or evaluation.\n-\"\"\"\n-\n-target_label_lookup = layers.StringLookup(\n-    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n-)\n-\n-\n-def prepare_example(features, target):\n-    target_index = target_label_lookup(target)\n-    weights = features.pop(WEIGHT_COLUMN_NAME)\n-    return features, target_index, weights\n-\n-\n-lookup_dict = {}\n-for feature_name in CATEGORICAL_FEATURE_NAMES:\n-    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-    # Create a lookup to convert a string values to an integer indices.\n-    # Since we are not using a mask token, nor expecting any out of vocabulary\n-    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-    lookup = layers.StringLookup(\n-        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n-    )\n-    lookup_dict[feature_name] = lookup\n-\n-\n-def encode_categorical(batch_x, batch_y, weights):\n-    for feature_name in CATEGORICAL_FEATURE_NAMES:\n-        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n-\n-    return batch_x, batch_y, weights\n-\n-\n-def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n-    dataset = (\n-        tf.data.experimental.make_csv_dataset(\n-            csv_file_path,\n-            batch_size=batch_size,\n-            column_names=CSV_HEADER,\n-            column_defaults=COLUMN_DEFAULTS,\n-            label_name=TARGET_FEATURE_NAME,\n-            num_epochs=1,\n-            header=False,\n-            na_value=\"?\",\n-            shuffle=shuffle,\n-        )\n-        .map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n-        .map(encode_categorical)\n-    )\n-    return dataset.cache()\n-\n-\n-\"\"\"\n-## Implement a training and evaluation procedure\n-\"\"\"\n-\n-\n-def run_experiment(\n-    model,\n-    train_data_file,\n-    test_data_file,\n-    num_epochs,\n-    learning_rate,\n-    weight_decay,\n-    batch_size,\n-):\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    )\n-\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=keras.losses.BinaryCrossentropy(),\n-        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n-    )\n-\n-    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n-    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n-\n-    print(\"Start training the model...\")\n-    history = model.fit(\n-        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n-    )\n-    print(\"Model training finished\")\n-\n-    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n-\n-    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n-\n-    return history\n-\n-\n-\"\"\"\n-## Create model inputs\n-\n-Now, define the inputs for the models as a dictionary, where the key is the feature name,\n-and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n-and data type.\n-\"\"\"\n-\n-\n-def create_model_inputs():\n-    inputs = {}\n-    for feature_name in FEATURE_NAMES:\n-        if feature_name in NUMERIC_FEATURE_NAMES:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=\"float32\"\n-            )\n-        else:\n-            inputs[feature_name] = layers.Input(\n-                name=feature_name, shape=(), dtype=\"float32\"\n-            )\n-    return inputs\n-\n-\n-\"\"\"\n-## Encode features\n-\n-The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n-We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n-regardless their vocabulary sizes. This is required for the Transformer model.\n-\"\"\"\n-\n-\n-def encode_inputs(inputs, embedding_dims):\n-    encoded_categorical_feature_list = []\n-    numerical_feature_list = []\n-\n-    for feature_name in inputs:\n-        if feature_name in CATEGORICAL_FEATURE_NAMES:\n-            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-            # Create a lookup to convert a string values to an integer indices.\n-            # Since we are not using a mask token, nor expecting any out of vocabulary\n-            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n-\n-            # Convert the string input values into integer indices.\n-\n-            # Create an embedding layer with the specified dimensions.\n-            embedding = layers.Embedding(\n-                input_dim=len(vocabulary), output_dim=embedding_dims\n-            )\n-\n-            # Convert the index values to embedding representations.\n-            encoded_categorical_feature = embedding(inputs[feature_name])\n-            encoded_categorical_feature_list.append(encoded_categorical_feature)\n-\n-        else:\n-            # Use the numerical features as-is.\n-            numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n-            numerical_feature_list.append(numerical_feature)\n-\n-    return encoded_categorical_feature_list, numerical_feature_list\n-\n-\n-\"\"\"\n-## Implement an MLP block\n-\"\"\"\n-\n-\n-def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n-    mlp_layers = []\n-    for units in hidden_units:\n-        mlp_layers.append(normalization_layer()),\n-        mlp_layers.append(layers.Dense(units, activation=activation))\n-        mlp_layers.append(layers.Dropout(dropout_rate))\n-\n-    return keras.Sequential(mlp_layers, name=name)\n-\n-\n-\"\"\"\n-## Experiment 1: a baseline model\n-\n-In the first experiment, we create a simple multi-layer feed-forward network.\n-\"\"\"\n-\n-\n-def create_baseline_model(\n-    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n-):\n-    # Create model inputs.\n-    inputs = create_model_inputs()\n-    # encode features.\n-    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n-        inputs, embedding_dims\n-    )\n-    # Concatenate all features.\n-    features = layers.concatenate(\n-        encoded_categorical_feature_list + numerical_feature_list\n-    )\n-    # Compute Feedforward layer units.\n-    feedforward_units = [features.shape[-1]]\n-\n-    # Create several feedforwad layers with skip connections.\n-    for layer_idx in range(num_mlp_blocks):\n-        features = create_mlp(\n-            hidden_units=feedforward_units,\n-            dropout_rate=dropout_rate,\n-            activation=keras.activations.gelu,\n-            normalization_layer=layers.LayerNormalization,\n-            name=f\"feedforward_{layer_idx}\",\n-        )(features)\n-\n-    # Compute MLP hidden_units.\n-    mlp_hidden_units = [\n-        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n-    ]\n-    # Create final MLP.\n-    features = create_mlp(\n-        hidden_units=mlp_hidden_units,\n-        dropout_rate=dropout_rate,\n-        activation=keras.activations.selu,\n-        normalization_layer=layers.BatchNormalization,\n-        name=\"MLP\",\n-    )(features)\n-\n-    # Add a sigmoid as a binary classifer.\n-    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-baseline_model = create_baseline_model(\n-    embedding_dims=EMBEDDING_DIMS,\n-    num_mlp_blocks=NUM_MLP_BLOCKS,\n-    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n-    dropout_rate=DROPOUT_RATE,\n-)\n-\n-print(\"Total model weights:\", baseline_model.count_params())\n-keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n-\n-\"\"\"\n-Let's train and evaluate the baseline model:\n-\"\"\"\n-\n-history = run_experiment(\n-    model=baseline_model,\n-    train_data_file=train_data_file,\n-    test_data_file=test_data_file,\n-    num_epochs=NUM_EPOCHS,\n-    learning_rate=LEARNING_RATE,\n-    weight_decay=WEIGHT_DECAY,\n-    batch_size=BATCH_SIZE,\n-)\n-\n-\"\"\"\n-The baseline linear model achieves ~81% validation accuracy.\n-\"\"\"\n-\n-\"\"\"\n-## Experiment 2: TabTransformer\n-\n-The TabTransformer architecture works as follows:\n-\n-1. All the categorical features are encoded as embeddings, using the same `embedding_dims`.\n-This means that each value in each categorical feature will have its own embedding vector.\n-2. A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n-3. The embedded categorical features are fed into a stack of Transformer blocks.\n-Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n-3. The outputs of the final Transformer layer, which are the *contextual embeddings* of the categorical features,\n-are concatenated with the input numerical features, and fed into a final MLP block.\n-4. A `softmax` classifer is applied at the end of the model.\n-\n-The [paper](https://arxiv.org/abs/2012.06678) discusses both addition and concatenation of the column embedding in the\n-*Appendix: Experiment and Model Details* section.\n-The architecture of TabTransformer is shown below, as presented in the paper.\n-\n-<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n-\"\"\"\n-\n-\n-def create_tabtransformer_classifier(\n-    num_transformer_blocks,\n-    num_heads,\n-    embedding_dims,\n-    mlp_hidden_units_factors,\n-    dropout_rate,\n-    use_column_embedding=False,\n-):\n-    # Create model inputs.\n-    inputs = create_model_inputs()\n-    # encode features.\n-    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n-        inputs, embedding_dims\n-    )\n-    # Stack categorical feature embeddings for the Tansformer.\n-    encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n-    # Concatenate numerical features.\n-    numerical_features = layers.concatenate(numerical_feature_list)\n-\n-    # Add column embedding to categorical feature embeddings.\n-    if use_column_embedding:\n-        num_columns = encoded_categorical_features.shape[1]\n-        column_embedding = layers.Embedding(\n-            input_dim=num_columns, output_dim=embedding_dims\n-        )\n-        column_indices = ops.arange(start=0, stop=num_columns, step=1)\n-        encoded_categorical_features = encoded_categorical_features + column_embedding(\n-            column_indices\n-        )\n-\n-    # Create multiple layers of the Transformer block.\n-    for block_idx in range(num_transformer_blocks):\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads,\n-            key_dim=embedding_dims,\n-            dropout=dropout_rate,\n-            name=f\"multihead_attention_{block_idx}\",\n-        )(encoded_categorical_features, encoded_categorical_features)\n-        # Skip connection 1.\n-        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n-            [attention_output, encoded_categorical_features]\n-        )\n-        # Layer normalization 1.\n-        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n-        # Feedforward.\n-        feedforward_output = create_mlp(\n-            hidden_units=[embedding_dims],\n-            dropout_rate=dropout_rate,\n-            activation=keras.activations.gelu,\n-            normalization_layer=partial(\n-                layers.LayerNormalization, epsilon=1e-6\n-            ),  # using partial to provide keyword arguments before initialization\n-            name=f\"feedforward_{block_idx}\",\n-        )(x)\n-        # Skip connection 2.\n-        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n-        # Layer normalization 2.\n-        encoded_categorical_features = layers.LayerNormalization(\n-            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n-        )(x)\n-\n-    # Flatten the \"contextualized\" embeddings of the categorical features.\n-    categorical_features = layers.Flatten()(encoded_categorical_features)\n-    # Apply layer normalization to the numerical features.\n-    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n-    # Prepare the input for the final MLP block.\n-    features = layers.concatenate([categorical_features, numerical_features])\n-\n-    # Compute MLP hidden_units.\n-    mlp_hidden_units = [\n-        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n-    ]\n-    # Create final MLP.\n-    features = create_mlp(\n-        hidden_units=mlp_hidden_units,\n-        dropout_rate=dropout_rate,\n-        activation=keras.activations.selu,\n-        normalization_layer=layers.BatchNormalization,\n-        name=\"MLP\",\n-    )(features)\n-\n-    # Add a sigmoid as a binary classifer.\n-    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-tabtransformer_model = create_tabtransformer_classifier(\n-    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n-    num_heads=NUM_HEADS,\n-    embedding_dims=EMBEDDING_DIMS,\n-    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n-    dropout_rate=DROPOUT_RATE,\n-)\n-\n-print(\"Total model weights:\", tabtransformer_model.count_params())\n-keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n-\n-\"\"\"\n-Let's train and evaluate the TabTransformer model:\n-\"\"\"\n-\n-history = run_experiment(\n-    model=tabtransformer_model,\n-    train_data_file=train_data_file,\n-    test_data_file=test_data_file,\n-    num_epochs=NUM_EPOCHS,\n-    learning_rate=LEARNING_RATE,\n-    weight_decay=WEIGHT_DECAY,\n-    batch_size=BATCH_SIZE,\n-)\n-\n-\"\"\"\n-The TabTransformer model achieves ~85% validation accuracy.\n-Note that, with the default parameter configurations, both the baseline and the TabTransformer\n-have similar number of trainable weights: 109,629 and 92,151 respectively, and both use the same training hyperparameters.\n-\"\"\"\n-\n-\"\"\"\n-## Conclusion\n-\n-TabTransformer significantly outperforms MLP and recent\n-deep networks for tabular data while matching the performance of tree-based ensemble models.\n-TabTransformer can be learned in end-to-end supervised training using labeled examples.\n-For a scenario where there are a few labeled examples and a large number of unlabeled\n-examples, a pre-training procedure can be employed to train the Transformer layers using unlabeled data.\n-This is followed by fine-tuning of the pre-trained Transformer layers along with\n-the top MLP layer using the labeled data.\n-\n-Example available on HuggingFace.\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/🤗%20Model-TabTransformer-black.svg)](https://huggingface.co/keras-io/tab_transformer) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-TabTransformer-black.svg)](https://huggingface.co/spaces/keras-io/TabTransformer_Classification) |\n-\n-\"\"\"\n\n@@ -1,499 +0,0 @@\n-\"\"\"\n-Title: Speaker Recognition\n-Author: [Fadi Badine](https://twitter.com/fadibadine)\n-Date created: 14/06/2020\n-Last modified: 19/07/2023\n-Description: Classify speakers using Fast Fourier Transform (FFT) and a 1D Convnet.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Fadi Badine](https://twitter.com/fadibadine)\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to create a model to classify speakers from the\n-frequency domain representation of speech recordings, obtained via Fast Fourier\n-Transform (FFT).\n-\n-It shows the following:\n-\n-- How to use `tf.data` to load, preprocess and feed audio streams into a model\n-- How to create a 1D convolutional network with residual\n-connections for audio classification.\n-\n-Our process:\n-\n-- We prepare a dataset of speech samples from different speakers, with the speaker as label.\n-- We add background noise to these samples to augment our data.\n-- We take the FFT of these samples.\n-- We train a 1D convnet to predict the correct speaker given a noisy FFT speech sample.\n-\n-Note:\n-\n-- This example should be run with TensorFlow 2.3 or higher, or `tf-nightly`.\n-- The noise samples in the dataset need to be resampled to a sampling rate of 16000 Hz\n-before using the code in this example. In order to do this, you will need to have\n-installed `ffmpeg`.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import shutil\n-import numpy as np\n-\n-import tensorflow as tf\n-import keras\n-\n-from pathlib import Path\n-from IPython.display import display, Audio\n-\n-# Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/\n-# and save it to ./speaker-recognition-dataset.zip\n-# then unzip it to ./16000_pcm_speeches\n-\"\"\"shell\n-kaggle datasets download -d kongaevans/speaker-recognition-dataset\n-unzip -qq speaker-recognition-dataset.zip\n-\"\"\"\n-\n-DATASET_ROOT = \"16000_pcm_speeches\"\n-\n-# The folders in which we will put the audio samples and the noise samples\n-AUDIO_SUBFOLDER = \"audio\"\n-NOISE_SUBFOLDER = \"noise\"\n-\n-DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n-DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n-\n-# Percentage of samples to use for validation\n-VALID_SPLIT = 0.1\n-\n-# Seed to use when shuffling the dataset and the noise\n-SHUFFLE_SEED = 43\n-\n-# The sampling rate to use.\n-# This is the one used in all the audio samples.\n-# We will resample all the noise to this sampling rate.\n-# This will also be the output size of the audio wave samples\n-# (since all samples are of 1 second long)\n-SAMPLING_RATE = 16000\n-\n-# The factor to multiply the noise with according to:\n-#   noisy_sample = sample + noise * prop * scale\n-#      where prop = sample_amplitude / noise_amplitude\n-SCALE = 0.5\n-\n-BATCH_SIZE = 128\n-EPOCHS = 1\n-\n-\n-\"\"\"\n-## Data preparation\n-\n-The dataset is composed of 7 folders, divided into 2 groups:\n-\n-- Speech samples, with 5 folders for 5 different speakers. Each folder contains\n-1500 audio files, each 1 second long and sampled at 16000 Hz.\n-- Background noise samples, with 2 folders and a total of 6 files. These files\n-are longer than 1 second (and originally not sampled at 16000 Hz, but we will resample them to 16000 Hz).\n-We will use those 6 files to create 354 1-second-long noise samples to be used for training.\n-\n-Let's sort these 2 categories into 2 folders:\n-\n-- An `audio` folder which will contain all the per-speaker speech sample folders\n-- A `noise` folder which will contain all the noise samples\n-\"\"\"\n-\n-\"\"\"\n-Before sorting the audio and noise categories into 2 folders,\n-we have the following directory structure:\n-\n-```\n-main_directory/\n-...speaker_a/\n-...speaker_b/\n-...speaker_c/\n-...speaker_d/\n-...speaker_e/\n-...other/\n-..._background_noise_/\n-```\n-\n-After sorting, we end up with the following structure:\n-\n-```\n-main_directory/\n-...audio/\n-......speaker_a/\n-......speaker_b/\n-......speaker_c/\n-......speaker_d/\n-......speaker_e/\n-...noise/\n-......other/\n-......_background_noise_/\n-```\n-\"\"\"\n-\n-for folder in os.listdir(DATASET_ROOT):\n-    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n-        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n-            # If folder is `audio` or `noise`, do nothing\n-            continue\n-        elif folder in [\"other\", \"_background_noise_\"]:\n-            # If folder is one of the folders that contains noise samples,\n-            # move it to the `noise` folder\n-            shutil.move(\n-                os.path.join(DATASET_ROOT, folder),\n-                os.path.join(DATASET_NOISE_PATH, folder),\n-            )\n-        else:\n-            # Otherwise, it should be a speaker folder, then move it to\n-            # `audio` folder\n-            shutil.move(\n-                os.path.join(DATASET_ROOT, folder),\n-                os.path.join(DATASET_AUDIO_PATH, folder),\n-            )\n-\n-\"\"\"\n-## Noise preparation\n-\n-In this section:\n-\n-- We load all noise samples (which should have been resampled to 16000)\n-- We split those noise samples to chunks of 16000 samples which\n-correspond to 1 second duration each\n-\"\"\"\n-\n-# Get the list of all noise files\n-noise_paths = []\n-for subdir in os.listdir(DATASET_NOISE_PATH):\n-    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n-    if os.path.isdir(subdir_path):\n-        noise_paths += [\n-            os.path.join(subdir_path, filepath)\n-            for filepath in os.listdir(subdir_path)\n-            if filepath.endswith(\".wav\")\n-        ]\n-if not noise_paths:\n-    raise RuntimeError(f\"Could not find any files at {DATASET_NOISE_PATH}\")\n-print(\n-    \"Found {} files belonging to {} directories\".format(\n-        len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))\n-    )\n-)\n-\n-\"\"\"\n-Resample all noise samples to 16000 Hz.\n-Note that this requires `ffmpeg`.\n-\"\"\"\n-\n-command = (\n-    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n-    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n-    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n-    \"$file | grep sample_rate | cut -f2 -d=`; \"\n-    \"if [ $sample_rate -ne 16000 ]; then \"\n-    \"ffmpeg -hide_banner -loglevel panic -y \"\n-    \"-i $file -ar 16000 temp.wav; \"\n-    \"mv temp.wav $file; \"\n-    \"fi; done; done\"\n-)\n-os.system(command)\n-\n-\n-# Split noise into chunks of 16,000 steps each\n-def load_noise_sample(path):\n-    sample, sampling_rate = tf.audio.decode_wav(\n-        tf.io.read_file(path), desired_channels=1\n-    )\n-    if sampling_rate == SAMPLING_RATE:\n-        # Number of slices of 16000 each that can be generated from the noise sample\n-        slices = int(sample.shape[0] / SAMPLING_RATE)\n-        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n-        return sample\n-    else:\n-        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n-        return None\n-\n-\n-noises = []\n-for path in noise_paths:\n-    sample = load_noise_sample(path)\n-    if sample:\n-        noises.extend(sample)\n-noises = tf.stack(noises)\n-\n-print(\n-    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n-        len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n-    )\n-)\n-\n-\"\"\"\n-## Dataset generation\n-\"\"\"\n-\n-\n-def paths_and_labels_to_dataset(audio_paths, labels):\n-    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n-    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n-    audio_ds = path_ds.map(\n-        lambda x: path_to_audio(x), num_parallel_calls=tf.data.AUTOTUNE\n-    )\n-    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n-    return tf.data.Dataset.zip((audio_ds, label_ds))\n-\n-\n-def path_to_audio(path):\n-    \"\"\"Reads and decodes an audio file.\"\"\"\n-    audio = tf.io.read_file(path)\n-    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n-    return audio\n-\n-\n-def add_noise(audio, noises=None, scale=0.5):\n-    if noises is not None:\n-        # Create a random tensor of the same size as audio ranging from\n-        # 0 to the number of noise stream samples that we have.\n-        tf_rnd = tf.random.uniform(\n-            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n-        )\n-        noise = tf.gather(noises, tf_rnd, axis=0)\n-\n-        # Get the amplitude proportion between the audio and the noise\n-        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(\n-            noise, axis=1\n-        )\n-        prop = tf.repeat(\n-            tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1\n-        )\n-\n-        # Adding the rescaled noise to audio\n-        audio = audio + noise * prop * scale\n-\n-    return audio\n-\n-\n-def audio_to_fft(audio):\n-    # Since tf.signal.fft applies FFT on the innermost dimension,\n-    # we need to squeeze the dimensions and then expand them again\n-    # after FFT\n-    audio = tf.squeeze(audio, axis=-1)\n-    fft = tf.signal.fft(\n-        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n-    )\n-    fft = tf.expand_dims(fft, axis=-1)\n-\n-    # Return the absolute value of the first half of the FFT\n-    # which represents the positive frequencies\n-    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n-\n-\n-# Get the list of audio file paths along with their corresponding labels\n-\n-class_names = os.listdir(DATASET_AUDIO_PATH)\n-print(\n-    \"Our class names: {}\".format(\n-        class_names,\n-    )\n-)\n-\n-audio_paths = []\n-labels = []\n-for label, name in enumerate(class_names):\n-    print(\n-        \"Processing speaker {}\".format(\n-            name,\n-        )\n-    )\n-    dir_path = Path(DATASET_AUDIO_PATH) / name\n-    speaker_sample_paths = [\n-        os.path.join(dir_path, filepath)\n-        for filepath in os.listdir(dir_path)\n-        if filepath.endswith(\".wav\")\n-    ]\n-    audio_paths += speaker_sample_paths\n-    labels += [label] * len(speaker_sample_paths)\n-\n-print(\n-    \"Found {} files belonging to {} classes.\".format(\n-        len(audio_paths), len(class_names)\n-    )\n-)\n-\n-# Shuffle\n-rng = np.random.RandomState(SHUFFLE_SEED)\n-rng.shuffle(audio_paths)\n-rng = np.random.RandomState(SHUFFLE_SEED)\n-rng.shuffle(labels)\n-\n-# Split into training and validation\n-num_val_samples = int(VALID_SPLIT * len(audio_paths))\n-print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n-train_audio_paths = audio_paths[:-num_val_samples]\n-train_labels = labels[:-num_val_samples]\n-\n-print(\"Using {} files for validation.\".format(num_val_samples))\n-valid_audio_paths = audio_paths[-num_val_samples:]\n-valid_labels = labels[-num_val_samples:]\n-\n-# Create 2 datasets, one for training and the other for validation\n-train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n-train_ds = train_ds.shuffle(\n-    buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED\n-).batch(BATCH_SIZE)\n-\n-valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n-valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n-\n-\n-# Add noise to the training set\n-train_ds = train_ds.map(\n-    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n-    num_parallel_calls=tf.data.AUTOTUNE,\n-)\n-\n-# Transform audio wave to the frequency domain using `audio_to_fft`\n-train_ds = train_ds.map(\n-    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n-)\n-train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n-\n-valid_ds = valid_ds.map(\n-    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n-)\n-valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n-\n-\"\"\"\n-## Model Definition\n-\"\"\"\n-\n-\n-def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n-    # Shortcut\n-    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n-    for i in range(conv_num - 1):\n-        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n-        x = keras.layers.Activation(activation)(x)\n-    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n-    x = keras.layers.Add()([x, s])\n-    x = keras.layers.Activation(activation)(x)\n-    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n-\n-\n-def build_model(input_shape, num_classes):\n-    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n-\n-    x = residual_block(inputs, 16, 2)\n-    x = residual_block(x, 32, 2)\n-    x = residual_block(x, 64, 3)\n-    x = residual_block(x, 128, 3)\n-    x = residual_block(x, 128, 3)\n-\n-    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n-    x = keras.layers.Flatten()(x)\n-    x = keras.layers.Dense(256, activation=\"relu\")(x)\n-    x = keras.layers.Dense(128, activation=\"relu\")(x)\n-\n-    outputs = keras.layers.Dense(\n-        num_classes, activation=\"softmax\", name=\"output\"\n-    )(x)\n-\n-    return keras.models.Model(inputs=inputs, outputs=outputs)\n-\n-\n-model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n-\n-model.summary()\n-\n-# Compile the model using Adam's default learning rate\n-model.compile(\n-    optimizer=\"Adam\",\n-    loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"accuracy\"],\n-)\n-\n-# Add callbacks:\n-# 'EarlyStopping' to stop training when the model is not enhancing anymore\n-# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n-model_save_filename = \"model.keras\"\n-\n-earlystopping_cb = keras.callbacks.EarlyStopping(\n-    patience=10, restore_best_weights=True\n-)\n-mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n-    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n-)\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-history = model.fit(\n-    train_ds,\n-    epochs=EPOCHS,\n-    validation_data=valid_ds,\n-    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n-)\n-\n-\"\"\"\n-## Evaluation\n-\"\"\"\n-\n-print(model.evaluate(valid_ds))\n-\n-\"\"\"\n-We get ~ 98% validation accuracy.\n-\"\"\"\n-\n-\"\"\"\n-## Demonstration\n-\n-Let's take some samples and:\n-\n-- Predict the speaker\n-- Compare the prediction with the real speaker\n-- Listen to the audio to see that despite the samples being noisy,\n-the model is still pretty accurate\n-\"\"\"\n-\n-SAMPLES_TO_DISPLAY = 10\n-\n-test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n-test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n-    BATCH_SIZE\n-)\n-\n-test_ds = test_ds.map(\n-    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n-    num_parallel_calls=tf.data.AUTOTUNE,\n-)\n-\n-for audios, labels in test_ds.take(1):\n-    # Get the signal FFT\n-    ffts = audio_to_fft(audios)\n-    # Predict\n-    y_pred = model.predict(ffts)\n-    # Take random samples\n-    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n-    audios = audios.numpy()[rnd, :, :]\n-    labels = labels.numpy()[rnd]\n-    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n-\n-    for index in range(SAMPLES_TO_DISPLAY):\n-        # For every sample, print the true and predicted label\n-        # as well as run the voice with the noise\n-        print(\n-            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n-                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n-                class_names[labels[index]],\n-                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n-                class_names[y_pred[index]],\n-            )\n-        )\n-        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))\n\n@@ -1,538 +0,0 @@\n-\"\"\"\n-Title: Automatic Speech Recognition with Transformer\n-Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n-Date created: 2021/01/13\n-Last modified: 2021/01/13\n-Description: Training a sequence-to-sequence Transformer for automatic speech recognition.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n-ASR can be treated as a sequence-to-sequence problem, where the\n-audio can be represented as a sequence of feature vectors\n-and the text as a sequence of characters, words, or subword tokens.\n-\n-For this demonstration, we will use the LJSpeech dataset from the\n-[LibriVox](https://librivox.org/) project. It consists of short\n-audio clips of a single speaker reading passages from 7 non-fiction books.\n-Our model will be similar to the original Transformer (both encoder and decoder)\n-as proposed in the paper, \"Attention is All You Need\".\n-\n-\n-**References:**\n-\n-- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n-- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/abs/1904.13377)\n-- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n-- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n-\"\"\"\n-\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-from glob import glob\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\n-\"\"\"\n-## Define the Transformer Input Layer\n-\n-When processing past target tokens for the decoder, we compute the sum of\n-position embeddings and token embeddings.\n-\n-When processing audio features, we apply convolutional layers to downsample\n-them (via convolution strides) and process local relationships.\n-\"\"\"\n-\n-\n-class TokenEmbedding(layers.Layer):\n-    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n-        super().__init__()\n-        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n-        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n-\n-    def call(self, x):\n-        maxlen = tf.shape(x)[-1]\n-        x = self.emb(x)\n-        positions = tf.range(start=0, limit=maxlen, delta=1)\n-        positions = self.pos_emb(positions)\n-        return x + positions\n-\n-\n-class SpeechFeatureEmbedding(layers.Layer):\n-    def __init__(self, num_hid=64, maxlen=100):\n-        super().__init__()\n-        self.conv1 = keras.layers.Conv1D(\n-            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n-        )\n-        self.conv2 = keras.layers.Conv1D(\n-            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n-        )\n-        self.conv3 = keras.layers.Conv1D(\n-            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n-        )\n-\n-    def call(self, x):\n-        x = self.conv1(x)\n-        x = self.conv2(x)\n-        return self.conv3(x)\n-\n-\n-\"\"\"\n-## Transformer Encoder Layer\n-\"\"\"\n-\n-\n-class TransformerEncoder(layers.Layer):\n-    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n-        super().__init__()\n-        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n-        self.ffn = keras.Sequential(\n-            [\n-                layers.Dense(feed_forward_dim, activation=\"relu\"),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n-        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n-        self.dropout1 = layers.Dropout(rate)\n-        self.dropout2 = layers.Dropout(rate)\n-\n-    def call(self, inputs, training=False):\n-        attn_output = self.att(inputs, inputs)\n-        attn_output = self.dropout1(attn_output, training=training)\n-        out1 = self.layernorm1(inputs + attn_output)\n-        ffn_output = self.ffn(out1)\n-        ffn_output = self.dropout2(ffn_output, training=training)\n-        return self.layernorm2(out1 + ffn_output)\n-\n-\n-\"\"\"\n-## Transformer Decoder Layer\n-\"\"\"\n-\n-\n-class TransformerDecoder(layers.Layer):\n-    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n-        super().__init__()\n-        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n-        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n-        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n-        self.self_att = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim\n-        )\n-        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n-        self.self_dropout = layers.Dropout(0.5)\n-        self.enc_dropout = layers.Dropout(0.1)\n-        self.ffn_dropout = layers.Dropout(0.1)\n-        self.ffn = keras.Sequential(\n-            [\n-                layers.Dense(feed_forward_dim, activation=\"relu\"),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-\n-    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n-        \"\"\"Masks the upper half of the dot product matrix in self attention.\n-\n-        This prevents flow of information from future tokens to current token.\n-        1's in the lower triangle, counting from the lower right corner.\n-        \"\"\"\n-        i = tf.range(n_dest)[:, None]\n-        j = tf.range(n_src)\n-        m = i >= j - n_src + n_dest\n-        mask = tf.cast(m, dtype)\n-        mask = tf.reshape(mask, [1, n_dest, n_src])\n-        mult = tf.concat(\n-            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n-        )\n-        return tf.tile(mask, mult)\n-\n-    def call(self, enc_out, target):\n-        input_shape = tf.shape(target)\n-        batch_size = input_shape[0]\n-        seq_len = input_shape[1]\n-        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n-        target_att = self.self_att(target, target, attention_mask=causal_mask)\n-        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n-        enc_out = self.enc_att(target_norm, enc_out)\n-        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n-        ffn_out = self.ffn(enc_out_norm)\n-        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n-        return ffn_out_norm\n-\n-\n-\"\"\"\n-## Complete the Transformer model\n-\n-Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n-During training, we give the decoder the target character sequence shifted to the left\n-as input. During inference, the decoder uses its own past predictions to predict the\n-next token.\n-\"\"\"\n-\n-\n-class Transformer(keras.Model):\n-    def __init__(\n-        self,\n-        num_hid=64,\n-        num_head=2,\n-        num_feed_forward=128,\n-        source_maxlen=100,\n-        target_maxlen=100,\n-        num_layers_enc=4,\n-        num_layers_dec=1,\n-        num_classes=10,\n-    ):\n-        super().__init__()\n-        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n-        self.num_layers_enc = num_layers_enc\n-        self.num_layers_dec = num_layers_dec\n-        self.target_maxlen = target_maxlen\n-        self.num_classes = num_classes\n-\n-        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n-        self.dec_input = TokenEmbedding(\n-            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n-        )\n-\n-        self.encoder = keras.Sequential(\n-            [self.enc_input]\n-            + [\n-                TransformerEncoder(num_hid, num_head, num_feed_forward)\n-                for _ in range(num_layers_enc)\n-            ]\n-        )\n-\n-        for i in range(num_layers_dec):\n-            setattr(\n-                self,\n-                f\"dec_layer_{i}\",\n-                TransformerDecoder(num_hid, num_head, num_feed_forward),\n-            )\n-\n-        self.classifier = layers.Dense(num_classes)\n-\n-    def decode(self, enc_out, target):\n-        y = self.dec_input(target)\n-        for i in range(self.num_layers_dec):\n-            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n-        return y\n-\n-    def call(self, inputs):\n-        source = inputs[0]\n-        target = inputs[1]\n-        x = self.encoder(source)\n-        y = self.decode(x, target)\n-        return self.classifier(y)\n-\n-    @property\n-    def metrics(self):\n-        return [self.loss_metric]\n-\n-    def train_step(self, batch):\n-        \"\"\"Processes one batch inside model.fit().\"\"\"\n-        source = batch[\"source\"]\n-        target = batch[\"target\"]\n-        dec_input = target[:, :-1]\n-        dec_target = target[:, 1:]\n-        with tf.GradientTape() as tape:\n-            preds = self([source, dec_input])\n-            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n-            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n-            loss = model.compute_loss(None, one_hot, preds, sample_weight=mask)\n-        trainable_vars = self.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-        self.loss_metric.update_state(loss)\n-        return {\"loss\": self.loss_metric.result()}\n-\n-    def test_step(self, batch):\n-        source = batch[\"source\"]\n-        target = batch[\"target\"]\n-        dec_input = target[:, :-1]\n-        dec_target = target[:, 1:]\n-        preds = self([source, dec_input])\n-        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n-        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n-        loss = model.compute_loss(None, one_hot, preds, sample_weight=mask)\n-        self.loss_metric.update_state(loss)\n-        return {\"loss\": self.loss_metric.result()}\n-\n-    def generate(self, source, target_start_token_idx):\n-        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n-        bs = tf.shape(source)[0]\n-        enc = self.encoder(source)\n-        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n-        dec_logits = []\n-        for i in range(self.target_maxlen - 1):\n-            dec_out = self.decode(enc, dec_input)\n-            logits = self.classifier(dec_out)\n-            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n-            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n-            dec_logits.append(last_logit)\n-            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n-        return dec_input\n-\n-\n-\"\"\"\n-## Download the dataset\n-\n-Note: This requires ~3.6 GB of disk space and\n-takes ~5 minutes for the extraction of files.\n-\"\"\"\n-\n-keras.utils.get_file(\n-    os.path.join(os.getcwd(), \"data.tar.gz\"),\n-    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n-    extract=True,\n-    archive_format=\"tar\",\n-    cache_dir=\".\",\n-)\n-\n-\n-saveto = \"./datasets/LJSpeech-1.1\"\n-wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n-\n-id_to_text = {}\n-with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n-    for line in f:\n-        id = line.strip().split(\"|\")[0]\n-        text = line.strip().split(\"|\")[2]\n-        id_to_text[id] = text\n-\n-\n-def get_data(wavs, id_to_text, maxlen=50):\n-    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n-    data = []\n-    for w in wavs:\n-        id = w.split(\"/\")[-1].split(\".\")[0]\n-        if len(id_to_text[id]) < maxlen:\n-            data.append({\"audio\": w, \"text\": id_to_text[id]})\n-    return data\n-\n-\n-\"\"\"\n-## Preprocess the dataset\n-\"\"\"\n-\n-\n-class VectorizeChar:\n-    def __init__(self, max_len=50):\n-        self.vocab = (\n-            [\"-\", \"#\", \"<\", \">\"]\n-            + [chr(i + 96) for i in range(1, 27)]\n-            + [\" \", \".\", \",\", \"?\"]\n-        )\n-        self.max_len = max_len\n-        self.char_to_idx = {}\n-        for i, ch in enumerate(self.vocab):\n-            self.char_to_idx[ch] = i\n-\n-    def __call__(self, text):\n-        text = text.lower()\n-        text = text[: self.max_len - 2]\n-        text = \"<\" + text + \">\"\n-        pad_len = self.max_len - len(text)\n-        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n-\n-    def get_vocabulary(self):\n-        return self.vocab\n-\n-\n-max_target_len = 200  # all transcripts in out data are < 200 characters\n-data = get_data(wavs, id_to_text, max_target_len)\n-vectorizer = VectorizeChar(max_target_len)\n-print(\"vocab size\", len(vectorizer.get_vocabulary()))\n-\n-\n-def create_text_ds(data):\n-    texts = [_[\"text\"] for _ in data]\n-    text_ds = [vectorizer(t) for t in texts]\n-    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n-    return text_ds\n-\n-\n-def path_to_audio(path):\n-    # spectrogram using stft\n-    audio = tf.io.read_file(path)\n-    audio, _ = tf.audio.decode_wav(audio, 1)\n-    audio = tf.squeeze(audio, axis=-1)\n-    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n-    x = tf.math.pow(tf.abs(stfts), 0.5)\n-    # normalisation\n-    means = tf.math.reduce_mean(x, 1, keepdims=True)\n-    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n-    x = (x - means) / stddevs\n-    audio_len = tf.shape(x)[0]\n-    # padding to 10 seconds\n-    pad_len = 2754\n-    paddings = tf.constant([[0, pad_len], [0, 0]])\n-    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n-    return x\n-\n-\n-def create_audio_ds(data):\n-    flist = [_[\"audio\"] for _ in data]\n-    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n-    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n-    return audio_ds\n-\n-\n-def create_tf_dataset(data, bs=4):\n-    audio_ds = create_audio_ds(data)\n-    text_ds = create_text_ds(data)\n-    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n-    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n-    ds = ds.batch(bs)\n-    ds = ds.prefetch(tf.data.AUTOTUNE)\n-    return ds\n-\n-\n-split = int(len(data) * 0.99)\n-train_data = data[:split]\n-test_data = data[split:]\n-ds = create_tf_dataset(train_data, bs=64)\n-val_ds = create_tf_dataset(test_data, bs=4)\n-\n-\"\"\"\n-## Callbacks to display predictions\n-\"\"\"\n-\n-\n-class DisplayOutputs(keras.callbacks.Callback):\n-    def __init__(\n-        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n-    ):\n-        \"\"\"Displays a batch of outputs after every epoch\n-\n-        Args:\n-            batch: A test batch containing the keys \"source\" and \"target\"\n-            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n-            target_start_token_idx: A start token index in the target vocabulary\n-            target_end_token_idx: An end token index in the target vocabulary\n-        \"\"\"\n-        self.batch = batch\n-        self.target_start_token_idx = target_start_token_idx\n-        self.target_end_token_idx = target_end_token_idx\n-        self.idx_to_char = idx_to_token\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        if epoch % 5 != 0:\n-            return\n-        source = self.batch[\"source\"]\n-        target = self.batch[\"target\"].numpy()\n-        bs = tf.shape(source)[0]\n-        preds = self.model.generate(source, self.target_start_token_idx)\n-        preds = preds.numpy()\n-        for i in range(bs):\n-            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n-            prediction = \"\"\n-            for idx in preds[i, :]:\n-                prediction += self.idx_to_char[idx]\n-                if idx == self.target_end_token_idx:\n-                    break\n-            print(f\"target:     {target_text.replace('-','')}\")\n-            print(f\"prediction: {prediction}\\n\")\n-\n-\n-\"\"\"\n-## Learning rate schedule\n-\"\"\"\n-\n-\n-class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n-    def __init__(\n-        self,\n-        init_lr=0.00001,\n-        lr_after_warmup=0.001,\n-        final_lr=0.00001,\n-        warmup_epochs=15,\n-        decay_epochs=85,\n-        steps_per_epoch=203,\n-    ):\n-        super().__init__()\n-        self.init_lr = init_lr\n-        self.lr_after_warmup = lr_after_warmup\n-        self.final_lr = final_lr\n-        self.warmup_epochs = warmup_epochs\n-        self.decay_epochs = decay_epochs\n-        self.steps_per_epoch = steps_per_epoch\n-\n-    def calculate_lr(self, epoch):\n-        \"\"\"linear warm up - linear decay\"\"\"\n-        warmup_lr = (\n-            self.init_lr\n-            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n-        )\n-        decay_lr = tf.math.maximum(\n-            self.final_lr,\n-            self.lr_after_warmup\n-            - (epoch - self.warmup_epochs)\n-            * (self.lr_after_warmup - self.final_lr)\n-            / self.decay_epochs,\n-        )\n-        return tf.math.minimum(warmup_lr, decay_lr)\n-\n-    def __call__(self, step):\n-        epoch = step // self.steps_per_epoch\n-        epoch = tf.cast(epoch, \"float32\")\n-        return self.calculate_lr(epoch)\n-\n-\n-\"\"\"\n-## Create & train the end-to-end model\n-\"\"\"\n-\n-batch = next(iter(val_ds))\n-\n-# The vocabulary to convert predicted indices into characters\n-idx_to_char = vectorizer.get_vocabulary()\n-display_cb = DisplayOutputs(\n-    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n-)  # set the arguments as per vocabulary index for '<' and '>'\n-\n-model = Transformer(\n-    num_hid=200,\n-    num_head=2,\n-    num_feed_forward=400,\n-    target_maxlen=max_target_len,\n-    num_layers_enc=4,\n-    num_layers_dec=1,\n-    num_classes=34,\n-)\n-loss_fn = keras.losses.CategoricalCrossentropy(\n-    from_logits=True,\n-    label_smoothing=0.1,\n-)\n-\n-learning_rate = CustomSchedule(\n-    init_lr=0.00001,\n-    lr_after_warmup=0.001,\n-    final_lr=0.00001,\n-    warmup_epochs=15,\n-    decay_epochs=85,\n-    steps_per_epoch=len(ds),\n-)\n-optimizer = keras.optimizers.Adam(learning_rate)\n-model.compile(optimizer=optimizer, loss=loss_fn)\n-\n-history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)\n-\n-\"\"\"\n-In practice, you should train for around 100 epochs or more.\n-\n-Some of the predicted text at or around epoch 35 may look as follows:\n-```\n-target:     <as they sat in the car, frazier asked oswald where his lunch was>\n-prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n-\n-target:     <under the entry for may one, nineteen sixty,>\n-prediction: <under the introus for may monee, nin the sixty,>\n-```\n-\"\"\"\n\n@@ -1,727 +0,0 @@\n-\"\"\"\n-Title: English speaker accent recognition using Transfer Learning\n-Author: [Fadi Badine](https://twitter.com/fadibadine)\n-Converted to Keras 3 by: [Fadi Badine](https://twitter.com/fadibadine)\n-Date created: 2022/04/16\n-Last modified: 2023/07/19\n-Description: Training a model to classify UK & Ireland accents using feature extraction from Yamnet.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The following example shows how to use feature extraction in order to\n-train a model to classify the English accent spoken in an audio wave.\n-\n-Instead of training a model from scratch, transfer learning enables us to\n-take advantage of existing state-of-the-art deep learning models and use them as feature extractors.\n-\n-Our process:\n-\n-* Use a TF Hub pre-trained model (Yamnet) and apply it as part of the tf.data pipeline which transforms\n-the audio files into feature vectors.\n-* Train a dense model on the feature vectors.\n-* Use the trained model for inference on a new audio file.\n-\n-Note:\n-\n-* We need to install TensorFlow IO in order to resample audio files to 16 kHz as required by Yamnet model.\n-* In the test section, ffmpeg is used to convert the mp3 file to wav.\n-\n-You can install TensorFlow IO with the following command:\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -U -q tensorflow_io\n-\"\"\"\n-\n-\"\"\"\n-## Configuration\n-\"\"\"\n-\n-SEED = 1337\n-EPOCHS = 1\n-BATCH_SIZE = 64\n-VALIDATION_RATIO = 0.1\n-MODEL_NAME = \"uk_irish_accent_recognition\"\n-\n-# Location where the dataset will be downloaded.\n-# By default (None), keras.utils.get_file will use ~/.keras/ as the CACHE_DIR\n-CACHE_DIR = None\n-\n-# The location of the dataset\n-URL_PATH = \"https://www.openslr.org/resources/83/\"\n-\n-# List of datasets compressed files that contain the audio files\n-zip_files = {\n-    0: \"irish_english_male.zip\",\n-    1: \"midlands_english_female.zip\",\n-    2: \"midlands_english_male.zip\",\n-    3: \"northern_english_female.zip\",\n-    4: \"northern_english_male.zip\",\n-    5: \"scottish_english_female.zip\",\n-    6: \"scottish_english_male.zip\",\n-    7: \"southern_english_female.zip\",\n-    8: \"southern_english_male.zip\",\n-    9: \"welsh_english_female.zip\",\n-    10: \"welsh_english_male.zip\",\n-}\n-\n-# We see that there are 2 compressed files for each accent (except Irish):\n-# - One for male speakers\n-# - One for female speakers\n-# However, we will be using a gender agnostic dataset.\n-\n-# List of gender agnostic categories\n-gender_agnostic_categories = [\n-    \"ir\",  # Irish\n-    \"mi\",  # Midlands\n-    \"no\",  # Northern\n-    \"sc\",  # Scottish\n-    \"so\",  # Southern\n-    \"we\",  # Welsh\n-]\n-\n-class_names = [\n-    \"Irish\",\n-    \"Midlands\",\n-    \"Northern\",\n-    \"Scottish\",\n-    \"Southern\",\n-    \"Welsh\",\n-    \"Not a speech\",\n-]\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import io\n-import csv\n-import numpy as np\n-import pandas as pd\n-import tensorflow as tf\n-import tensorflow_hub as hub\n-import tensorflow_io as tfio\n-import keras\n-import matplotlib.pyplot as plt\n-import seaborn as sns\n-from scipy import stats\n-from IPython.display import Audio\n-\n-\n-# Set all random seeds in order to get reproducible results\n-keras.utils.set_random_seed(SEED)\n-\n-# Where to download the dataset\n-DATASET_DESTINATION = os.path.join(\n-    CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\"\n-)\n-\n-\"\"\"\n-## Yamnet Model\n-\n-Yamnet is an audio event classifier trained on the AudioSet dataset to predict audio\n-events from the AudioSet ontology. It is available on TensorFlow Hub.\n-\n-Yamnet accepts a 1-D tensor of audio samples with a sample rate of 16 kHz.\n-As output, the model returns a 3-tuple:\n-\n-* Scores of shape `(N, 521)` representing the scores of the 521 classes.\n-* Embeddings of shape `(N, 1024)`.\n-* The log-mel spectrogram of the entire audio frame.\n-\n-We will use the embeddings, which are the features extracted from the audio samples, as the input to our dense model.\n-\n-For more detailed information about Yamnet, please refer to its [TensorFlow Hub](https://tfhub.dev/google/yamnet/1) page.\n-\"\"\"\n-\n-yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n-\n-\"\"\"\n-## Dataset\n-\n-The dataset used is the\n-[Crowdsourced high-quality UK and Ireland English Dialect speech data set](https://openslr.org/83/)\n-which consists of a total of 17,877 high-quality audio wav files.\n-\n-This dataset includes over 31 hours of recording from 120 volunteers who self-identify as\n-native speakers of Southern England, Midlands, Northern England, Wales, Scotland and Ireland.\n-\n-For more info, please refer to the above link or to the following paper:\n-[Open-source Multi-speaker Corpora of the English Accents in the British Isles](https://aclanthology.org/2020.lrec-1.804.pdf)\n-\"\"\"\n-\n-\"\"\"\n-## Download the data\n-\"\"\"\n-\n-# CSV file that contains information about the dataset. For each entry, we have:\n-# - ID\n-# - wav file name\n-# - transcript\n-line_index_file = keras.utils.get_file(\n-    fname=\"line_index_file\", origin=URL_PATH + \"line_index_all.csv\"\n-)\n-\n-# Download the list of compressed files that contain the audio wav files\n-for i in zip_files:\n-    fname = zip_files[i].split(\".\")[0]\n-    url = URL_PATH + zip_files[i]\n-\n-    zip_file = keras.utils.get_file(fname=fname, origin=url, extract=True)\n-    os.remove(zip_file)\n-\n-\"\"\"\n-## Load the data in a Dataframe\n-\n-Of the 3 columns (ID, filename and transcript), we are only interested in the filename column in order to read the audio file.\n-We will ignore the other two.\n-\"\"\"\n-\n-dataframe = pd.read_csv(\n-    line_index_file,\n-    names=[\"id\", \"filename\", \"transcript\"],\n-    usecols=[\"filename\"],\n-)\n-dataframe.head()\n-\n-\"\"\"\n-Let's now preprocess the dataset by:\n-\n-* Adjusting the filename (removing a leading space & adding \".wav\" extension to the\n-filename).\n-* Creating a label using the first 2 characters of the filename which indicate the\n-accent.\n-* Shuffling the samples.\n-\"\"\"\n-\n-\n-# The purpose of this function is to preprocess the dataframe by applying the following:\n-# - Cleaning the filename from a leading space\n-# - Generating a label column that is gender agnostic i.e.\n-#   welsh english male and welsh english female for example are both labeled as\n-#   welsh english\n-# - Add extension .wav to the filename\n-# - Shuffle samples\n-def preprocess_dataframe(dataframe):\n-    # Remove leading space in filename column\n-    dataframe[\"filename\"] = dataframe.apply(\n-        lambda row: row[\"filename\"].strip(), axis=1\n-    )\n-\n-    # Create gender agnostic labels based on the filename first 2 letters\n-    dataframe[\"label\"] = dataframe.apply(\n-        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]),\n-        axis=1,\n-    )\n-\n-    # Add the file path to the name\n-    dataframe[\"filename\"] = dataframe.apply(\n-        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"),\n-        axis=1,\n-    )\n-\n-    # Shuffle the samples\n-    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(\n-        drop=True\n-    )\n-\n-    return dataframe\n-\n-\n-dataframe = preprocess_dataframe(dataframe)\n-dataframe.head()\n-\n-\"\"\"\n-## Prepare training & validation sets\n-\n-Let's split the samples creating training and validation sets.\n-\"\"\"\n-\n-split = int(len(dataframe) * (1 - VALIDATION_RATIO))\n-train_df = dataframe[:split]\n-valid_df = dataframe[split:]\n-\n-print(\n-    f\"We have {train_df.shape[0]} training samples & {valid_df.shape[0]} validation ones\"\n-)\n-\n-\"\"\"\n-## Prepare a TensorFlow Dataset\n-\n-Next, we need to create a `tf.data.Dataset`.\n-This is done by creating a `dataframe_to_dataset` function that does the following:\n-\n-* Create a dataset using filenames and labels.\n-* Get the Yamnet embeddings by calling another function `filepath_to_embeddings`.\n-* Apply caching, reshuffling and setting batch size.\n-\n-The `filepath_to_embeddings` does the following:\n-\n-* Load audio file.\n-* Resample audio to 16 kHz.\n-* Generate scores and embeddings from Yamnet model.\n-* Since Yamnet generates multiple samples for each audio file,\n-this function also duplicates the label for all the generated samples\n-that have `score=0` (speech) whereas sets the label for the others as\n-'other' indicating that this audio segment is not a speech and we won't label it as one of the accents.\n-\n-The below `load_16k_audio_file` is copied from the following tutorial\n-[Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n-\"\"\"\n-\n-\n-@tf.function\n-def load_16k_audio_wav(filename):\n-    # Read file content\n-    file_content = tf.io.read_file(filename)\n-\n-    # Decode audio wave\n-    audio_wav, sample_rate = tf.audio.decode_wav(\n-        file_content, desired_channels=1\n-    )\n-    audio_wav = tf.squeeze(audio_wav, axis=-1)\n-    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n-\n-    # Resample to 16k\n-    audio_wav = tfio.audio.resample(\n-        audio_wav, rate_in=sample_rate, rate_out=16000\n-    )\n-\n-    return audio_wav\n-\n-\n-def filepath_to_embeddings(filename, label):\n-    # Load 16k audio wave\n-    audio_wav = load_16k_audio_wav(filename)\n-\n-    # Get audio embeddings & scores.\n-    # The embeddings are the audio features extracted using transfer learning\n-    # while scores will be used to identify time slots that are not speech\n-    # which will then be gathered into a specific new category 'other'\n-    scores, embeddings, _ = yamnet_model(audio_wav)\n-\n-    # Number of embeddings in order to know how many times to repeat the label\n-    embeddings_num = tf.shape(embeddings)[0]\n-    labels = tf.repeat(label, embeddings_num)\n-\n-    # Change labels for time-slots that are not speech into a new category 'other'\n-    labels = tf.where(\n-        tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1\n-    )\n-\n-    # Using one-hot in order to use AUC\n-    return (embeddings, tf.one_hot(labels, len(class_names)))\n-\n-\n-def dataframe_to_dataset(dataframe, batch_size=64):\n-    dataset = tf.data.Dataset.from_tensor_slices(\n-        (dataframe[\"filename\"], dataframe[\"label\"])\n-    )\n-\n-    dataset = dataset.map(\n-        lambda x, y: filepath_to_embeddings(x, y),\n-        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n-    ).unbatch()\n-\n-    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n-\n-\n-train_ds = dataframe_to_dataset(train_df)\n-valid_ds = dataframe_to_dataset(valid_df)\n-\n-\"\"\"\n-## Build the model\n-\n-The model that we use consists of:\n-\n-* An input layer which is the embedding output of the Yamnet classifier.\n-* 4 dense hidden layers and 4 dropout layers.\n-* An output dense layer.\n-\n-The model's hyperparameters were selected using\n-[KerasTuner](https://keras.io/keras_tuner/).\n-\"\"\"\n-\n-keras.backend.clear_session()\n-\n-\n-def build_and_compile_model():\n-    inputs = keras.layers.Input(shape=(1024,), name=\"embedding\")\n-\n-    x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)\n-    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n-\n-    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n-    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n-\n-    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n-    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n-\n-    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n-    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n-\n-    outputs = keras.layers.Dense(\n-        len(class_names), activation=\"softmax\", name=\"ouput\"\n-    )(x)\n-\n-    model = keras.Model(\n-        inputs=inputs, outputs=outputs, name=\"accent_recognition\"\n-    )\n-\n-    model.compile(\n-        optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n-        loss=keras.losses.CategoricalCrossentropy(),\n-        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n-    )\n-\n-    return model\n-\n-\n-model = build_and_compile_model()\n-model.summary()\n-\n-\"\"\"\n-## Class weights calculation\n-\n-Since the dataset is quite unbalanced, we wil use `class_weight` argument during training.\n-\n-Getting the class weights is a little tricky because even though we know the number of\n-audio files for each class, it does not represent the number of samples for that class\n-since Yamnet transforms each audio file into multiple audio samples of 0.96 seconds each.\n-So every audio file will be split into a number of samples that is proportional to its length.\n-\n-Therefore, to get those weights, we have to calculate the number of samples for each class\n-after preprocessing through Yamnet.\n-\"\"\"\n-\n-class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n-\n-for x, y in iter(train_ds):\n-    class_counts = class_counts + tf.math.bincount(\n-        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n-    )\n-\n-class_weight = {\n-    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n-    for i in range(len(class_counts))\n-}\n-\n-print(class_weight)\n-\n-\"\"\"\n-## Callbacks\n-\n-We use Keras callbacks in order to:\n-\n-* Stop whenever the validation AUC stops improving.\n-* Save the best model.\n-* Call TensorBoard in order to later view the training and validation logs.\n-\"\"\"\n-\n-early_stopping_cb = keras.callbacks.EarlyStopping(\n-    monitor=\"val_auc\", patience=10, restore_best_weights=True\n-)\n-\n-model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n-    MODEL_NAME + \".keras\", monitor=\"val_auc\", save_best_only=True\n-)\n-\n-tensorboard_cb = keras.callbacks.TensorBoard(\n-    os.path.join(os.curdir, \"logs\", model.name)\n-)\n-\n-callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-history = model.fit(\n-    train_ds,\n-    epochs=EPOCHS,\n-    validation_data=valid_ds,\n-    class_weight=class_weight,\n-    callbacks=callbacks,\n-    verbose=2,\n-)\n-\n-\"\"\"\n-## Results\n-\n-Let's plot the training and validation AUC and accuracy.\n-\"\"\"\n-\n-fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n-\n-axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n-axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n-axs[0].set_xlabel(\"Epochs\")\n-axs[0].set_title(\"Training & Validation Accuracy\")\n-axs[0].legend()\n-axs[0].grid(True)\n-\n-axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n-axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n-axs[1].set_xlabel(\"Epochs\")\n-axs[1].set_title(\"Training & Validation AUC\")\n-axs[1].legend()\n-axs[1].grid(True)\n-\n-plt.show()\n-\n-\"\"\"\n-## Evaluation\n-\"\"\"\n-\n-train_loss, train_acc, train_auc = model.evaluate(train_ds)\n-valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)\n-\n-\"\"\"\n-Let's try to compare our model's performance to Yamnet's using one of Yamnet metrics (d-prime)\n-Yamnet achieved a d-prime value of 2.318.\n-Let's check our model's performance.\n-\"\"\"\n-\n-\n-# The following function calculates the d-prime score from the AUC\n-def d_prime(auc):\n-    standard_normal = stats.norm()\n-    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n-    return d_prime\n-\n-\n-print(\n-    \"train d-prime: {0:.3f}, validation d-prime: {1:.3f}\".format(\n-        d_prime(train_auc), d_prime(valid_auc)\n-    )\n-)\n-\n-\"\"\"\n-We can see that the model achieves the following results:\n-\n-Results    | Training  | Validation\n------------|-----------|------------\n-Accuracy   | 54%       | 51%\n-AUC        | 0.91      | 0.89\n-d-prime    | 1.882     | 1.740\n-\n-\"\"\"\n-\n-\"\"\"\n-## Confusion Matrix\n-\n-Let's now plot the confusion matrix for the validation dataset.\n-\n-The confusion matrix lets us see, for every class, not only how many samples were correctly classified,\n-but also which other classes were the samples confused with.\n-\n-It allows us to calculate the precision and recall for every class.\n-\"\"\"\n-\n-# Create x and y tensors\n-x_valid = None\n-y_valid = None\n-\n-for x, y in iter(valid_ds):\n-    if x_valid is None:\n-        x_valid = x.numpy()\n-        y_valid = y.numpy()\n-    else:\n-        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n-        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n-\n-# Generate predictions\n-y_pred = model.predict(x_valid)\n-\n-# Calculate confusion matrix\n-confusion_mtx = tf.math.confusion_matrix(\n-    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n-)\n-\n-# Plot the confusion matrix\n-plt.figure(figsize=(10, 8))\n-sns.heatmap(\n-    confusion_mtx,\n-    xticklabels=class_names,\n-    yticklabels=class_names,\n-    annot=True,\n-    fmt=\"g\",\n-)\n-plt.xlabel(\"Prediction\")\n-plt.ylabel(\"Label\")\n-plt.title(\"Validation Confusion Matrix\")\n-plt.show()\n-\n-\"\"\"\n-## Precision & recall\n-\n-For every class:\n-\n-* Recall is the ratio of correctly classified samples i.e. it shows how many samples\n-of this specific class, the model is able to detect.\n-It is the ratio of diagonal elements to the sum of all elements in the row.\n-* Precision shows the accuracy of the classifier. It is the ratio of correctly predicted\n-samples among the ones classified as belonging to this class.\n-It is the ratio of diagonal elements to the sum of all elements in the column.\n-\"\"\"\n-\n-for i, label in enumerate(class_names):\n-    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n-    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n-    print(\n-        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n-            label, precision * 100, recall * 100\n-        )\n-    )\n-\n-\"\"\"\n-## Run inference on test data\n-\n-Let's now run a test on a single audio file.\n-Let's check this example from [The Scottish Voice](https://www.thescottishvoice.org.uk/home/)\n-\n-We will:\n-\n-* Download the mp3 file.\n-* Convert it to a 16k wav file.\n-* Run the model on the wav file.\n-* Plot the results.\n-\"\"\"\n-\n-filename = \"audio-sample-Stuart\"\n-url = \"https://www.thescottishvoice.org.uk/files/cm/files/\"\n-\n-if os.path.exists(filename + \".wav\") == False:\n-    print(f\"Downloading {filename}.mp3 from {url}\")\n-    command = f\"wget {url}{filename}.mp3\"\n-    os.system(command)\n-\n-    print(f\"Converting mp3 to wav and resampling to 16 kHZ\")\n-    command = (\n-        f\"ffmpeg -hide_banner -loglevel panic -y -i {filename}.mp3 -acodec \"\n-        f\"pcm_s16le -ac 1 -ar 16000 {filename}.wav\"\n-    )\n-    os.system(command)\n-\n-filename = filename + \".wav\"\n-\n-\n-\"\"\"\n-The below function `yamnet_class_names_from_csv` was copied and very slightly changed\n-from this [Yamnet Notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/yamnet.ipynb).\n-\"\"\"\n-\n-\n-def yamnet_class_names_from_csv(yamnet_class_map_csv_text):\n-    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n-    yamnet_class_map_csv = io.StringIO(yamnet_class_map_csv_text)\n-    yamnet_class_names = [\n-        name for (class_index, mid, name) in csv.reader(yamnet_class_map_csv)\n-    ]\n-    yamnet_class_names = yamnet_class_names[1:]  # Skip CSV header\n-    return yamnet_class_names\n-\n-\n-yamnet_class_map_path = yamnet_model.class_map_path().numpy()\n-yamnet_class_names = yamnet_class_names_from_csv(\n-    tf.io.read_file(yamnet_class_map_path).numpy().decode(\"utf-8\")\n-)\n-\n-\n-def calculate_number_of_non_speech(scores):\n-    number_of_non_speech = tf.math.reduce_sum(\n-        tf.where(\n-            tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0\n-        )\n-    )\n-\n-    return number_of_non_speech\n-\n-\n-def filename_to_predictions(filename):\n-    # Load 16k audio wave\n-    audio_wav = load_16k_audio_wav(filename)\n-\n-    # Get audio embeddings & scores.\n-    scores, embeddings, mel_spectrogram = yamnet_model(audio_wav)\n-\n-    print(\n-        \"Out of {} samples, {} are not speech\".format(\n-            scores.shape[0], calculate_number_of_non_speech(scores)\n-        )\n-    )\n-\n-    # Predict the output of the accent recognition model with embeddings as input\n-    predictions = model.predict(embeddings)\n-\n-    return audio_wav, predictions, mel_spectrogram\n-\n-\n-\"\"\"\n-Let's run the model on the audio file:\n-\"\"\"\n-\n-audio_wav, predictions, mel_spectrogram = filename_to_predictions(filename)\n-\n-infered_class = class_names[predictions.mean(axis=0).argmax()]\n-print(f\"The main accent is: {infered_class} English\")\n-\n-\"\"\"\n-Listen to the audio\n-\"\"\"\n-\n-Audio(audio_wav, rate=16000)\n-\n-\"\"\"\n-The below function was copied from this [Yamnet notebook](tinyurl.com/4a8xn7at) and adjusted to our need.\n-\n-This function plots the following:\n-\n-* Audio waveform\n-* Mel spectrogram\n-* Predictions for every time step\n-\"\"\"\n-\n-plt.figure(figsize=(10, 6))\n-\n-# Plot the waveform.\n-plt.subplot(3, 1, 1)\n-plt.plot(audio_wav)\n-plt.xlim([0, len(audio_wav)])\n-\n-# Plot the log-mel spectrogram (returned by the model).\n-plt.subplot(3, 1, 2)\n-plt.imshow(\n-    mel_spectrogram.numpy().T,\n-    aspect=\"auto\",\n-    interpolation=\"nearest\",\n-    origin=\"lower\",\n-)\n-\n-# Plot and label the model output scores for the top-scoring classes.\n-mean_predictions = np.mean(predictions, axis=0)\n-\n-top_class_indices = np.argsort(mean_predictions)[::-1]\n-plt.subplot(3, 1, 3)\n-plt.imshow(\n-    predictions[:, top_class_indices].T,\n-    aspect=\"auto\",\n-    interpolation=\"nearest\",\n-    cmap=\"gray_r\",\n-)\n-\n-# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n-# values from the model documentation\n-patch_padding = (0.025 / 2) / 0.01\n-plt.xlim([-patch_padding - 0.5, predictions.shape[0] + patch_padding - 0.5])\n-# Label the top_N classes.\n-yticks = range(0, len(class_names), 1)\n-plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n-_ = plt.ylim(-0.5 + np.array([len(class_names), 0]))\n\n@@ -1,694 +0,0 @@\n-\"\"\"\n-Title: CycleGAN\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2020/08/12\n-Last modified: 2020/08/12\n-Description: Implementation of CycleGAN.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## CycleGAN\n-\n-CycleGAN is a model that aims to solve the image-to-image translation\n-problem. The goal of the image-to-image translation problem is to learn the\n-mapping between an input image and an output image using a training set of\n-aligned image pairs. However, obtaining paired examples isn't always feasible.\n-CycleGAN tries to learn this mapping without requiring paired input-output images,\n-using cycle-consistent adversarial networks.\n-\n-- [Paper](https://arxiv.org/abs/1703.10593)\n-- [Original implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-\n-tfds.disable_progress_bar()\n-autotune = tf.data.AUTOTUNE\n-\n-\"\"\"\n-## Prepare the dataset\n-\n-In this example, we will be using the\n-[horse to zebra](https://www.tensorflow.org/datasets/catalog/cycle_gan#cycle_ganhorse2zebra)\n-dataset.\n-\"\"\"\n-\n-# Load the horse-zebra dataset using tensorflow-datasets.\n-dataset, _ = tfds.load(\n-    \"cycle_gan/horse2zebra\", with_info=True, as_supervised=True\n-)\n-train_horses, train_zebras = dataset[\"trainA\"], dataset[\"trainB\"]\n-test_horses, test_zebras = dataset[\"testA\"], dataset[\"testB\"]\n-\n-# Define the standard image size.\n-orig_img_size = (286, 286)\n-# Size of the random crops to be used during training.\n-input_img_size = (256, 256, 3)\n-# Weights initializer for the layers.\n-kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n-# Gamma initializer for instance normalization.\n-gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n-\n-buffer_size = 256\n-batch_size = 1\n-\n-\n-def normalize_img(img):\n-    img = ops.cast(img, dtype=\"float32\")\n-    # Map values in the range [-1, 1]\n-    return (img / 127.5) - 1.0\n-\n-\n-def preprocess_train_image(img, label):\n-    # Random flip\n-    img = tf.image.random_flip_left_right(img)\n-    # Resize to the original size first\n-    img = tf.image.resize(img, [*orig_img_size])\n-    # Random crop to 256X256\n-    img = tf.image.random_crop(img, size=[*input_img_size])\n-    # Normalize the pixel values in the range [-1, 1]\n-    img = normalize_img(img)\n-    return img\n-\n-\n-def preprocess_test_image(img, label):\n-    # Only resizing and normalization for the test images.\n-    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n-    img = normalize_img(img)\n-    return img\n-\n-\n-\"\"\"\n-## Create `Dataset` objects\n-\"\"\"\n-\n-\n-# Apply the preprocessing operations to the training data\n-train_horses = (\n-    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n-    .cache()\n-    .shuffle(buffer_size)\n-    .batch(batch_size)\n-)\n-train_zebras = (\n-    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n-    .cache()\n-    .shuffle(buffer_size)\n-    .batch(batch_size)\n-)\n-\n-# Apply the preprocessing operations to the test data\n-test_horses = (\n-    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n-    .cache()\n-    .shuffle(buffer_size)\n-    .batch(batch_size)\n-)\n-test_zebras = (\n-    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n-    .cache()\n-    .shuffle(buffer_size)\n-    .batch(batch_size)\n-)\n-\n-\n-\"\"\"\n-## Visualize some samples\n-\"\"\"\n-\n-\n-_, ax = plt.subplots(4, 2, figsize=(10, 15))\n-for i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\n-    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n-    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n-    ax[i, 0].imshow(horse)\n-    ax[i, 1].imshow(zebra)\n-plt.show()\n-\n-\n-\"\"\"\n-## Building blocks used in the CycleGAN generators and discriminators\n-\"\"\"\n-\n-\n-class ReflectionPadding2D(layers.Layer):\n-    \"\"\"Implements Reflection Padding as a layer.\n-\n-    Args:\n-        padding(tuple): Amount of padding for the\n-        spatial dimensions.\n-\n-    Returns:\n-        A padded tensor with the same type as the input tensor.\n-    \"\"\"\n-\n-    def __init__(self, padding=(1, 1), **kwargs):\n-        self.padding = tuple(padding)\n-        super().__init__(**kwargs)\n-\n-    def call(self, input_tensor, mask=None):\n-        padding_width, padding_height = self.padding\n-        padding_tensor = [\n-            [0, 0],\n-            [padding_height, padding_height],\n-            [padding_width, padding_width],\n-            [0, 0],\n-        ]\n-        return ops.pad(\n-            input_tensor, ops.convert_to_tensor(padding_tensor), mode=\"reflect\"\n-        )\n-\n-\n-def residual_block(\n-    x,\n-    activation,\n-    kernel_initializer=kernel_init,\n-    kernel_size=(3, 3),\n-    strides=(1, 1),\n-    padding=\"valid\",\n-    gamma_initializer=gamma_init,\n-    use_bias=False,\n-):\n-    dim = x.shape[-1]\n-    input_tensor = x\n-\n-    x = ReflectionPadding2D()(input_tensor)\n-    x = layers.Conv2D(\n-        dim,\n-        kernel_size,\n-        strides=strides,\n-        kernel_initializer=kernel_initializer,\n-        padding=padding,\n-        use_bias=use_bias,\n-    )(x)\n-    x = layers.GroupNormalization(\n-        groups=x.shape[-1], gamma_initializer=gamma_initializer\n-    )(x)\n-    x = activation(x)\n-\n-    x = ReflectionPadding2D()(x)\n-    x = layers.Conv2D(\n-        dim,\n-        kernel_size,\n-        strides=strides,\n-        kernel_initializer=kernel_initializer,\n-        padding=padding,\n-        use_bias=use_bias,\n-    )(x)\n-    x = layers.GroupNormalization(\n-        groups=x.shape[-1], gamma_initializer=gamma_initializer\n-    )(x)\n-    x = layers.add([input_tensor, x])\n-    return x\n-\n-\n-def downsample(\n-    x,\n-    filters,\n-    activation,\n-    kernel_initializer=kernel_init,\n-    kernel_size=(3, 3),\n-    strides=(2, 2),\n-    padding=\"same\",\n-    gamma_initializer=gamma_init,\n-    use_bias=False,\n-):\n-    x = layers.Conv2D(\n-        filters,\n-        kernel_size,\n-        strides=strides,\n-        kernel_initializer=kernel_initializer,\n-        padding=padding,\n-        use_bias=use_bias,\n-    )(x)\n-    x = layers.GroupNormalization(\n-        groups=x.shape[-1], gamma_initializer=gamma_initializer\n-    )(x)\n-    if activation:\n-        x = activation(x)\n-    return x\n-\n-\n-def upsample(\n-    x,\n-    filters,\n-    activation,\n-    kernel_size=(3, 3),\n-    strides=(2, 2),\n-    padding=\"same\",\n-    kernel_initializer=kernel_init,\n-    gamma_initializer=gamma_init,\n-    use_bias=False,\n-):\n-    x = layers.Conv2DTranspose(\n-        filters,\n-        kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        kernel_initializer=kernel_initializer,\n-        use_bias=use_bias,\n-    )(x)\n-    x = layers.GroupNormalization(\n-        groups=x.shape[-1], gamma_initializer=gamma_initializer\n-    )(x)\n-    if activation:\n-        x = activation(x)\n-    return x\n-\n-\n-\"\"\"\n-## Build the generators\n-\n-The generator consists of downsampling blocks: nine residual blocks\n-and upsampling blocks. The structure of the generator is the following:\n-\n-```\n-c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n-d128 ====|\n-         |-> 2 downsampling blocks\n-d256 ====|\n-R256 ====|\n-R256     |\n-R256     |\n-R256     |\n-R256     |-> 9 residual blocks\n-R256     |\n-R256     |\n-R256     |\n-R256 ====|\n-u128 ====|\n-         |-> 2 upsampling blocks\n-u64  ====|\n-c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n-```\n-\"\"\"\n-\n-\n-def get_resnet_generator(\n-    filters=64,\n-    num_downsampling_blocks=2,\n-    num_residual_blocks=9,\n-    num_upsample_blocks=2,\n-    gamma_initializer=gamma_init,\n-    name=None,\n-):\n-    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n-    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n-    x = layers.Conv2D(\n-        filters, (7, 7), kernel_initializer=kernel_init, use_bias=False\n-    )(x)\n-    x = layers.GroupNormalization(\n-        groups=x.shape[-1], gamma_initializer=gamma_initializer\n-    )(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    # Downsampling\n-    for _ in range(num_downsampling_blocks):\n-        filters *= 2\n-        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n-\n-    # Residual blocks\n-    for _ in range(num_residual_blocks):\n-        x = residual_block(x, activation=layers.Activation(\"relu\"))\n-\n-    # Upsampling\n-    for _ in range(num_upsample_blocks):\n-        filters //= 2\n-        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n-\n-    # Final block\n-    x = ReflectionPadding2D(padding=(3, 3))(x)\n-    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n-    x = layers.Activation(\"tanh\")(x)\n-\n-    model = keras.models.Model(img_input, x, name=name)\n-    return model\n-\n-\n-\"\"\"\n-## Build the discriminators\n-\n-The discriminators implement the following architecture:\n-`C64->C128->C256->C512`\n-\"\"\"\n-\n-\n-def get_discriminator(\n-    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n-):\n-    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n-    x = layers.Conv2D(\n-        filters,\n-        (4, 4),\n-        strides=(2, 2),\n-        padding=\"same\",\n-        kernel_initializer=kernel_initializer,\n-    )(img_input)\n-    x = layers.LeakyReLU(0.2)(x)\n-\n-    num_filters = filters\n-    for num_downsample_block in range(3):\n-        num_filters *= 2\n-        if num_downsample_block < 2:\n-            x = downsample(\n-                x,\n-                filters=num_filters,\n-                activation=layers.LeakyReLU(0.2),\n-                kernel_size=(4, 4),\n-                strides=(2, 2),\n-            )\n-        else:\n-            x = downsample(\n-                x,\n-                filters=num_filters,\n-                activation=layers.LeakyReLU(0.2),\n-                kernel_size=(4, 4),\n-                strides=(1, 1),\n-            )\n-\n-    x = layers.Conv2D(\n-        1,\n-        (4, 4),\n-        strides=(1, 1),\n-        padding=\"same\",\n-        kernel_initializer=kernel_initializer,\n-    )(x)\n-\n-    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n-    return model\n-\n-\n-# Get the generators\n-gen_G = get_resnet_generator(name=\"generator_G\")\n-gen_F = get_resnet_generator(name=\"generator_F\")\n-\n-# Get the discriminators\n-disc_X = get_discriminator(name=\"discriminator_X\")\n-disc_Y = get_discriminator(name=\"discriminator_Y\")\n-\n-\n-\"\"\"\n-## Build the CycleGAN model\n-\n-We will override the `train_step()` method of the `Model` class\n-for training via `fit()`.\n-\"\"\"\n-\n-\n-class CycleGan(keras.Model):\n-    def __init__(\n-        self,\n-        generator_G,\n-        generator_F,\n-        discriminator_X,\n-        discriminator_Y,\n-        lambda_cycle=10.0,\n-        lambda_identity=0.5,\n-    ):\n-        super().__init__()\n-        self.gen_G = generator_G\n-        self.gen_F = generator_F\n-        self.disc_X = discriminator_X\n-        self.disc_Y = discriminator_Y\n-        self.lambda_cycle = lambda_cycle\n-        self.lambda_identity = lambda_identity\n-\n-    def call(self, inputs):\n-        return (\n-            self.disc_X(inputs),\n-            self.disc_Y(inputs),\n-            self.gen_G(inputs),\n-            self.gen_F(inputs),\n-        )\n-\n-    def compile(\n-        self,\n-        gen_G_optimizer,\n-        gen_F_optimizer,\n-        disc_X_optimizer,\n-        disc_Y_optimizer,\n-        gen_loss_fn,\n-        disc_loss_fn,\n-    ):\n-        super().compile()\n-        self.gen_G_optimizer = gen_G_optimizer\n-        self.gen_F_optimizer = gen_F_optimizer\n-        self.disc_X_optimizer = disc_X_optimizer\n-        self.disc_Y_optimizer = disc_Y_optimizer\n-        self.generator_loss_fn = gen_loss_fn\n-        self.discriminator_loss_fn = disc_loss_fn\n-        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n-        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n-\n-    def train_step(self, batch_data):\n-        # x is Horse and y is zebra\n-        real_x, real_y = batch_data\n-\n-        # For CycleGAN, we need to calculate different\n-        # kinds of losses for the generators and discriminators.\n-        # We will perform the following steps here:\n-        #\n-        # 1. Pass real images through the generators and get the generated images\n-        # 2. Pass the generated images back to the generators to check if we\n-        #    can predict the original image from the generated image.\n-        # 3. Do an identity mapping of the real images using the generators.\n-        # 4. Pass the generated images in 1) to the corresponding discriminators.\n-        # 5. Calculate the generators total loss (adversarial + cycle + identity)\n-        # 6. Calculate the discriminators loss\n-        # 7. Update the weights of the generators\n-        # 8. Update the weights of the discriminators\n-        # 9. Return the losses in a dictionary\n-\n-        with tf.GradientTape(persistent=True) as tape:\n-            # Horse to fake zebra\n-            fake_y = self.gen_G(real_x, training=True)\n-            # Zebra to fake horse -> y2x\n-            fake_x = self.gen_F(real_y, training=True)\n-\n-            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n-            cycled_x = self.gen_F(fake_y, training=True)\n-            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n-            cycled_y = self.gen_G(fake_x, training=True)\n-\n-            # Identity mapping\n-            same_x = self.gen_F(real_x, training=True)\n-            same_y = self.gen_G(real_y, training=True)\n-\n-            # Discriminator output\n-            disc_real_x = self.disc_X(real_x, training=True)\n-            disc_fake_x = self.disc_X(fake_x, training=True)\n-\n-            disc_real_y = self.disc_Y(real_y, training=True)\n-            disc_fake_y = self.disc_Y(fake_y, training=True)\n-\n-            # Generator adversarial loss\n-            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n-            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n-\n-            # Generator cycle loss\n-            cycle_loss_G = (\n-                self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n-            )\n-            cycle_loss_F = (\n-                self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n-            )\n-\n-            # Generator identity loss\n-            id_loss_G = (\n-                self.identity_loss_fn(real_y, same_y)\n-                * self.lambda_cycle\n-                * self.lambda_identity\n-            )\n-            id_loss_F = (\n-                self.identity_loss_fn(real_x, same_x)\n-                * self.lambda_cycle\n-                * self.lambda_identity\n-            )\n-\n-            # Total generator loss\n-            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n-            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n-\n-            # Discriminator loss\n-            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n-            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n-\n-        # Get the gradients for the generators\n-        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n-        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n-\n-        # Get the gradients for the discriminators\n-        disc_X_grads = tape.gradient(\n-            disc_X_loss, self.disc_X.trainable_variables\n-        )\n-        disc_Y_grads = tape.gradient(\n-            disc_Y_loss, self.disc_Y.trainable_variables\n-        )\n-\n-        # Update the weights of the generators\n-        self.gen_G_optimizer.apply_gradients(\n-            zip(grads_G, self.gen_G.trainable_variables)\n-        )\n-        self.gen_F_optimizer.apply_gradients(\n-            zip(grads_F, self.gen_F.trainable_variables)\n-        )\n-\n-        # Update the weights of the discriminators\n-        self.disc_X_optimizer.apply_gradients(\n-            zip(disc_X_grads, self.disc_X.trainable_variables)\n-        )\n-        self.disc_Y_optimizer.apply_gradients(\n-            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n-        )\n-\n-        return {\n-            \"G_loss\": total_loss_G,\n-            \"F_loss\": total_loss_F,\n-            \"D_X_loss\": disc_X_loss,\n-            \"D_Y_loss\": disc_Y_loss,\n-        }\n-\n-\n-\"\"\"\n-## Create a callback that periodically saves generated images\n-\"\"\"\n-\n-\n-class GANMonitor(keras.callbacks.Callback):\n-    \"\"\"A callback to generate and save images after each epoch\"\"\"\n-\n-    def __init__(self, num_img=4):\n-        self.num_img = num_img\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n-        for i, img in enumerate(test_horses.take(self.num_img)):\n-            prediction = self.model.gen_G(img)[0].numpy()\n-            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n-            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n-\n-            ax[i, 0].imshow(img)\n-            ax[i, 1].imshow(prediction)\n-            ax[i, 0].set_title(\"Input image\")\n-            ax[i, 1].set_title(\"Translated image\")\n-            ax[i, 0].axis(\"off\")\n-            ax[i, 1].axis(\"off\")\n-\n-            prediction = keras.utils.array_to_img(prediction)\n-            prediction.save(\n-                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n-            )\n-        plt.show()\n-        plt.close()\n-\n-\n-\"\"\"\n-## Train the end-to-end model\n-\"\"\"\n-\n-\n-# Loss function for evaluating adversarial loss\n-adv_loss_fn = keras.losses.MeanSquaredError()\n-\n-# Define the loss function for the generators\n-\n-\n-def generator_loss_fn(fake):\n-    fake_loss = adv_loss_fn(ops.ones_like(fake), fake)\n-    return fake_loss\n-\n-\n-# Define the loss function for the discriminators\n-def discriminator_loss_fn(real, fake):\n-    real_loss = adv_loss_fn(ops.ones_like(real), real)\n-    fake_loss = adv_loss_fn(ops.zeros_like(fake), fake)\n-    return (real_loss + fake_loss) * 0.5\n-\n-\n-# Create cycle gan model\n-cycle_gan_model = CycleGan(\n-    generator_G=gen_G,\n-    generator_F=gen_F,\n-    discriminator_X=disc_X,\n-    discriminator_Y=disc_Y,\n-)\n-\n-# Compile the model\n-cycle_gan_model.compile(\n-    gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n-    gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n-    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n-    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n-    gen_loss_fn=generator_loss_fn,\n-    disc_loss_fn=discriminator_loss_fn,\n-)\n-# Callbacks\n-plotter = GANMonitor()\n-checkpoint_filepath = (\n-    \"cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n-)\n-model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-    filepath=checkpoint_filepath, save_weights_only=True\n-)\n-\n-# Here we will train the model for just one epoch as each epoch takes around\n-# 7 minutes on a single P100 backed machine.\n-cycle_gan_model.fit(\n-    tf.data.Dataset.zip((train_horses, train_zebras)),\n-    epochs=1,\n-    callbacks=[plotter, model_checkpoint_callback],\n-)\n-\n-\"\"\"\n-## Test the performance of the model.\n-\"\"\"\n-\n-\n-# This model was trained for 90 epochs. We will be loading those weights\n-# here. Once the weights are loaded, we will take a few samples from the test\n-# data and check the model's performance.\n-\n-\"\"\"shell\n-curl -LO https://github.com/freedomtan/cyclegan-keras/archive/refs/tags/2.0.zip\n-unzip -qq 2.0.zip\n-\"\"\"\n-\n-\n-# Load the checkpoints\n-weight_file = \"./cyclegan-keras-2.0/model_checkpoints/cyclegan_checkpoints.090.weights.h5\"\n-cycle_gan_model.load_weights(weight_file)\n-print(\"Weights loaded successfully\")\n-\n-_, ax = plt.subplots(4, 2, figsize=(10, 15))\n-for i, img in enumerate(test_horses.take(4)):\n-    prediction = cycle_gan_model.gen_G.predict(img)[0]\n-    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n-    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n-\n-    ax[i, 0].imshow(img)\n-    ax[i, 1].imshow(prediction)\n-    ax[i, 0].set_title(\"Input image\")\n-    ax[i, 0].set_title(\"Input image\")\n-    ax[i, 1].set_title(\"Translated image\")\n-    ax[i, 0].axis(\"off\")\n-    ax[i, 1].axis(\"off\")\n-\n-    prediction = keras.utils.array_to_img(prediction)\n-    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n-plt.tight_layout()\n-plt.show()\n\n@@ -1,235 +0,0 @@\n-\"\"\"\n-Title: DCGAN to generate face images\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2019/04/29\n-Last modified: 2021/01/01\n-Description: A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-import matplotlib.pyplot as plt\n-import os\n-import gdown\n-from zipfile import ZipFile\n-\n-\"\"\"\n-## Prepare CelebA data\n-\n-We'll use face images from the CelebA dataset, resized to 64x64.\n-\"\"\"\n-\n-os.makedirs(\"celeba_gan\")\n-\n-url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n-output = \"celeba_gan/data.zip\"\n-gdown.download(url, output, quiet=True)\n-\n-with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n-    zipobj.extractall(\"celeba_gan\")\n-\n-\"\"\"\n-Create a dataset from our folder, and rescale the images to the [0-1] range:\n-\"\"\"\n-\n-dataset = keras.utils.image_dataset_from_directory(\n-    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n-)\n-dataset = dataset.map(lambda x: x / 255.0)\n-\n-\n-\"\"\"\n-Let's display a sample image:\n-\"\"\"\n-\n-\n-for x in dataset:\n-    plt.axis(\"off\")\n-    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n-    break\n-\n-\n-\"\"\"\n-## Create the discriminator\n-\n-It maps a 64x64 image to a binary classification score.\n-\"\"\"\n-\n-discriminator = keras.Sequential(\n-    [\n-        keras.Input(shape=(64, 64, 3)),\n-        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Flatten(),\n-        layers.Dropout(0.2),\n-        layers.Dense(1, activation=\"sigmoid\"),\n-    ],\n-    name=\"discriminator\",\n-)\n-discriminator.summary()\n-\n-\"\"\"\n-## Create the generator\n-\n-It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers.\n-\"\"\"\n-\n-latent_dim = 128\n-\n-generator = keras.Sequential(\n-    [\n-        keras.Input(shape=(latent_dim,)),\n-        layers.Dense(8 * 8 * 128),\n-        layers.Reshape((8, 8, 128)),\n-        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n-        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n-    ],\n-    name=\"generator\",\n-)\n-generator.summary()\n-\n-\"\"\"\n-## Override `train_step`\n-\"\"\"\n-\n-\n-class GAN(keras.Model):\n-    def __init__(self, discriminator, generator, latent_dim):\n-        super().__init__()\n-        self.discriminator = discriminator\n-        self.generator = generator\n-        self.latent_dim = latent_dim\n-\n-    def compile(self, d_optimizer, g_optimizer, loss_fn):\n-        super().compile()\n-        self.d_optimizer = d_optimizer\n-        self.g_optimizer = g_optimizer\n-        self.loss_fn = loss_fn\n-        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n-        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [self.d_loss_metric, self.g_loss_metric]\n-\n-    def train_step(self, real_images):\n-        # Sample random points in the latent space\n-        batch_size = tf.shape(real_images)[0]\n-        random_latent_vectors = tf.random.normal(\n-            shape=(batch_size, self.latent_dim)\n-        )\n-\n-        # Decode them to fake images\n-        generated_images = self.generator(random_latent_vectors)\n-\n-        # Combine them with real images\n-        combined_images = tf.concat([generated_images, real_images], axis=0)\n-\n-        # Assemble labels discriminating real from fake images\n-        labels = tf.concat(\n-            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n-        )\n-        # Add random noise to the labels - important trick!\n-        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n-\n-        # Train the discriminator\n-        with tf.GradientTape() as tape:\n-            predictions = self.discriminator(combined_images)\n-            d_loss = self.loss_fn(labels, predictions)\n-        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n-        self.d_optimizer.apply_gradients(\n-            zip(grads, self.discriminator.trainable_weights)\n-        )\n-\n-        # Sample random points in the latent space\n-        random_latent_vectors = tf.random.normal(\n-            shape=(batch_size, self.latent_dim)\n-        )\n-\n-        # Assemble labels that say \"all real images\"\n-        misleading_labels = tf.zeros((batch_size, 1))\n-\n-        # Train the generator (note that we should *not* update the weights\n-        # of the discriminator)!\n-        with tf.GradientTape() as tape:\n-            predictions = self.discriminator(\n-                self.generator(random_latent_vectors)\n-            )\n-            g_loss = self.loss_fn(misleading_labels, predictions)\n-        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n-        self.g_optimizer.apply_gradients(\n-            zip(grads, self.generator.trainable_weights)\n-        )\n-\n-        # Update metrics\n-        self.d_loss_metric.update_state(d_loss)\n-        self.g_loss_metric.update_state(g_loss)\n-        return {\n-            \"d_loss\": self.d_loss_metric.result(),\n-            \"g_loss\": self.g_loss_metric.result(),\n-        }\n-\n-\n-\"\"\"\n-## Create a callback that periodically saves generated images\n-\"\"\"\n-\n-\n-class GANMonitor(keras.callbacks.Callback):\n-    def __init__(self, num_img=3, latent_dim=128):\n-        self.num_img = num_img\n-        self.latent_dim = latent_dim\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        random_latent_vectors = tf.random.normal(\n-            shape=(self.num_img, self.latent_dim)\n-        )\n-        generated_images = self.model.generator(random_latent_vectors)\n-        generated_images *= 255\n-        generated_images.numpy()\n-        for i in range(self.num_img):\n-            img = keras.utils.array_to_img(generated_images[i])\n-            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))\n-\n-\n-\"\"\"\n-## Train the end-to-end model\n-\"\"\"\n-\n-epochs = 1  # In practice, use ~100 epochs\n-\n-gan = GAN(\n-    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n-)\n-gan.compile(\n-    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n-    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n-    loss_fn=keras.losses.BinaryCrossentropy(),\n-)\n-\n-gan.fit(\n-    dataset,\n-    epochs=epochs,\n-    callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)],\n-)\n-\n-\"\"\"\n-Some of the last generated images around epoch 30\n-(results keep improving after that):\n-\n-![results](https://i.imgur.com/h5MtQZ7l.png)\n-\"\"\"\n\n@@ -1,887 +0,0 @@\n-\"\"\"\n-Title: Denoising Diffusion Implicit Models\n-Author: [András Béres](https://www.linkedin.com/in/andras-beres-789190210)\n-Date created: 2022/06/24\n-Last modified: 2022/06/24\n-Description: Generating images of flowers with denoising diffusion implicit models.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-### What are diffusion models?\n-\n-Recently, [denoising diffusion models](https://arxiv.org/abs/2006.11239), including\n-[score-based generative models](https://arxiv.org/abs/1907.05600), gained popularity as a\n-powerful class of generative models, that can [rival](https://arxiv.org/abs/2105.05233)\n-even [generative adversarial networks (GANs)](https://arxiv.org/abs/1406.2661) in image\n-synthesis quality. They tend to generate more diverse samples, while being stable to\n-train and easy to scale. Recent large diffusion models, such as\n-[DALL-E 2](https://openai.com/dall-e-2/) and [Imagen](https://imagen.research.google/),\n-have shown incredible text-to-image generation capability. One of their drawbacks is\n-however, that they are slower to sample from, because they require multiple forward passes\n-for generating an image.\n-\n-Diffusion refers to the process of turning a structured signal (an image) into noise\n-step-by-step. By simulating diffusion, we can generate noisy images from our training\n-images, and can train a neural network to try to denoise them. Using the trained network\n-we can simulate the opposite of diffusion, reverse diffusion, which is the process of an\n-image emerging from noise.\n-\n-![diffusion process gif](https://i.imgur.com/dipPOfa.gif)\n-\n-One-sentence summary: **diffusion models are trained to denoise noisy images, and can\n-generate images by iteratively denoising pure noise.**\n-\n-### Goal of this example\n-\n-This code example intends to be a minimal but feature-complete (with a generation quality\n-metric) implementation of diffusion models, with modest compute requirements and\n-reasonable performance. My implementation choices and hyperparameter tuning were done\n-with these goals in mind.\n-\n-Since currently the literature of diffusion models is\n-[mathematically quite complex](https://arxiv.org/abs/2206.00364)\n-with multiple theoretical frameworks\n-([score matching](https://arxiv.org/abs/1907.05600),\n-[differential equations](https://arxiv.org/abs/2011.13456),\n-[Markov chains](https://arxiv.org/abs/2006.11239)) and sometimes even\n-[conflicting notations (see Appendix C.2)](https://arxiv.org/abs/2010.02502),\n-it can be daunting trying to understand\n-them. My view of these models in this example will be that they learn to separate a\n-noisy image into its image and Gaussian noise components.\n-\n-In this example I made effort to break down all long mathematical expressions into\n-digestible pieces and gave all variables explanatory names. I also included numerous\n-links to relevant literature to help interested readers dive deeper into the topic, in\n-the hope that this code example will become a good starting point for practitioners\n-learning about diffusion models.\n-\n-In the following sections, we will implement a continuous time version of\n-[Denoising Diffusion Implicit Models (DDIMs)](https://arxiv.org/abs/2010.02502)\n-with deterministic sampling.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import math\n-import matplotlib.pyplot as plt\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-\"\"\"\n-## Hyperparameters\n-\"\"\"\n-\n-# data\n-dataset_name = \"oxford_flowers102\"\n-dataset_repetitions = 5\n-num_epochs = 1  # train for at least 50 epochs for good results\n-image_size = 64\n-# KID = Kernel Inception Distance, see related section\n-kid_image_size = 75\n-kid_diffusion_steps = 5\n-plot_diffusion_steps = 20\n-\n-# sampling\n-min_signal_rate = 0.02\n-max_signal_rate = 0.95\n-\n-# architecture\n-embedding_dims = 32\n-embedding_max_frequency = 1000.0\n-widths = [32, 64, 96, 128]\n-block_depth = 2\n-\n-# optimization\n-batch_size = 64\n-ema = 0.999\n-learning_rate = 1e-3\n-weight_decay = 1e-4\n-\n-\"\"\"\n-## Data pipeline\n-\n-We will use the\n-[Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)\n-dataset for\n-generating images of flowers, which is a diverse natural dataset containing around 8,000\n-images. Unfortunately the official splits are imbalanced, as most of the images are\n-contained in the test split. We create new splits (80% train, 20% validation) using the\n-[Tensorflow Datasets slicing API](https://www.tensorflow.org/datasets/splits). We apply\n-center crops as preprocessing, and repeat the dataset multiple times (reason given in the\n-next section).\n-\"\"\"\n-\n-\n-def preprocess_image(data):\n-    # center crop image\n-    height = ops.shape(data[\"image\"])[0]\n-    width = ops.shape(data[\"image\"])[1]\n-    crop_size = ops.minimum(height, width)\n-    image = tf.image.crop_to_bounding_box(\n-        data[\"image\"],\n-        (height - crop_size) // 2,\n-        (width - crop_size) // 2,\n-        crop_size,\n-        crop_size,\n-    )\n-\n-    # resize and clip\n-    # for image downsampling it is important to turn on antialiasing\n-    image = tf.image.resize(\n-        image, size=[image_size, image_size], antialias=True\n-    )\n-    return ops.clip(image / 255.0, 0.0, 1.0)\n-\n-\n-def prepare_dataset(split):\n-    # the validation dataset is shuffled as well, because data order matters\n-    # for the KID estimation\n-    return (\n-        tfds.load(dataset_name, split=split, shuffle_files=True)\n-        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n-        .cache()\n-        .repeat(dataset_repetitions)\n-        .shuffle(10 * batch_size)\n-        .batch(batch_size, drop_remainder=True)\n-        .prefetch(buffer_size=tf.data.AUTOTUNE)\n-    )\n-\n-\n-# load dataset\n-train_dataset = prepare_dataset(\"train[:80%]+validation[:80%]+test[:80%]\")\n-val_dataset = prepare_dataset(\"train[80%:]+validation[80%:]+test[80%:]\")\n-\n-\"\"\"\n-## Kernel inception distance\n-\n-[Kernel Inception Distance (KID)](https://arxiv.org/abs/1801.01401) is an image quality\n-metric which was proposed as a replacement for the popular\n-[Frechet Inception Distance (FID)](https://arxiv.org/abs/1706.08500).\n-I prefer KID to FID because it is simpler to\n-implement, can be estimated per-batch, and is computationally lighter. More details\n-[here](https://keras.io/examples/generative/gan_ada/#kernel-inception-distance).\n-\n-In this example, the images are evaluated at the minimal possible resolution of the\n-Inception network (75x75 instead of 299x299), and the metric is only measured on the\n-validation set for computational efficiency. We also limit the number of sampling steps\n-at evaluation to 5 for the same reason.\n-\n-Since the dataset is relatively small, we go over the train and validation splits\n-multiple times per epoch, because the KID estimation is noisy and compute-intensive, so\n-we want to evaluate only after many iterations, but for many iterations.\n-\n-\"\"\"\n-\n-\n-@keras.saving.register_keras_serializable()\n-class KID(keras.metrics.Metric):\n-    def __init__(self, name, **kwargs):\n-        super().__init__(name=name, **kwargs)\n-\n-        # KID is estimated per batch and is averaged across batches\n-        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n-\n-        # a pretrained InceptionV3 is used without its classification layer\n-        # transform the pixel values to the 0-255 range, then use the same\n-        # preprocessing as during pretraining\n-        self.encoder = keras.Sequential(\n-            [\n-                keras.Input(shape=(image_size, image_size, 3)),\n-                layers.Rescaling(255.0),\n-                layers.Resizing(height=kid_image_size, width=kid_image_size),\n-                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n-                keras.applications.InceptionV3(\n-                    include_top=False,\n-                    input_shape=(kid_image_size, kid_image_size, 3),\n-                    weights=\"imagenet\",\n-                ),\n-                layers.GlobalAveragePooling2D(),\n-            ],\n-            name=\"inception_encoder\",\n-        )\n-\n-    def polynomial_kernel(self, features_1, features_2):\n-        feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype=\"float32\")\n-        return (\n-            features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0\n-        ) ** 3.0\n-\n-    def update_state(self, real_images, generated_images, sample_weight=None):\n-        real_features = self.encoder(real_images, training=False)\n-        generated_features = self.encoder(generated_images, training=False)\n-\n-        # compute polynomial kernels using the two sets of features\n-        kernel_real = self.polynomial_kernel(real_features, real_features)\n-        kernel_generated = self.polynomial_kernel(\n-            generated_features, generated_features\n-        )\n-        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n-\n-        # estimate the squared maximum mean discrepancy using the average kernel values\n-        batch_size = real_features.shape[0]\n-        batch_size_f = ops.cast(batch_size, dtype=\"float32\")\n-        mean_kernel_real = ops.sum(\n-            kernel_real * (1.0 - ops.eye(batch_size))\n-        ) / (batch_size_f * (batch_size_f - 1.0))\n-        mean_kernel_generated = ops.sum(\n-            kernel_generated * (1.0 - ops.eye(batch_size))\n-        ) / (batch_size_f * (batch_size_f - 1.0))\n-        mean_kernel_cross = ops.mean(kernel_cross)\n-        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n-\n-        # update the average KID estimate\n-        self.kid_tracker.update_state(kid)\n-\n-    def result(self):\n-        return self.kid_tracker.result()\n-\n-    def reset_state(self):\n-        self.kid_tracker.reset_state()\n-\n-\n-\"\"\"\n-## Network architecture\n-\n-Here we specify the architecture of the neural network that we will use for denoising. We\n-build a [U-Net](https://arxiv.org/abs/1505.04597) with identical input and output\n-dimensions. U-Net is a popular semantic segmentation architecture, whose main idea is\n-that it progressively downsamples and then upsamples its input image, and adds skip\n-connections between layers having the same resolution. These help with gradient flow and\n-avoid introducing a representation bottleneck, unlike usual\n-[autoencoders](https://www.deeplearningbook.org/contents/autoencoders.html). Based on\n-this, one can view\n-[diffusion models as denoising autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)\n-without a bottleneck.\n-\n-The network takes two inputs, the noisy images and the variances of their noise\n-components. The latter is required since denoising a signal requires different operations\n-at different levels of noise. We transform the noise variances using sinusoidal\n-embeddings, similarly to positional encodings used both in\n-[transformers](https://arxiv.org/abs/1706.03762) and\n-[NeRF](https://arxiv.org/abs/2003.08934). This helps the network to be\n-[highly sensitive](https://arxiv.org/abs/2006.10739) to the noise level, which is\n-crucial for good performance. We implement sinusoidal embeddings using a\n-[Lambda layer](https://keras.io/api/layers/core_layers/lambda/).\n-\n-Some other considerations:\n-\n-* We build the network using the\n-[Keras Functional API](https://keras.io/guides/functional_api/), and use\n-[closures](https://twitter.com/fchollet/status/1441927912836321280) to build blocks of\n-layers in a consistent style.\n-* [Diffusion models](https://arxiv.org/abs/2006.11239) embed the index of the timestep of\n-the diffusion process instead of the noise variance, while\n-[score-based models (Table 1)](https://arxiv.org/abs/2206.00364)\n-usually use some function of the noise level. I\n-prefer the latter so that we can change the sampling schedule at inference time, without\n-retraining the network.\n-* [Diffusion models](https://arxiv.org/abs/2006.11239) input the embedding to each\n-convolution block separately. We only input it at the start of the network for\n-simplicity, which in my experience barely decreases performance, because the skip and\n-residual connections help the information propagate through the network properly.\n-* In the literature it is common to use\n-[attention layers](https://keras.io/api/layers/attention_layers/multi_head_attention/)\n-at lower resolutions for better global coherence. I omitted it for simplicity.\n-* We disable the learnable center and scale parameters of the batch normalization layers,\n-since the following convolution layers make them redundant.\n-* We initialize the last convolution's kernel to all zeros as a good practice, making the\n-network predict only zeros after initialization, which is the mean of its targets. This\n-will improve behaviour at the start of training and make the mean squared error loss\n-start at exactly 1.\n-\"\"\"\n-\n-\n-@keras.saving.register_keras_serializable()\n-def sinusoidal_embedding(x):\n-    embedding_min_frequency = 1.0\n-    frequencies = ops.exp(\n-        ops.linspace(\n-            ops.log(embedding_min_frequency),\n-            ops.log(embedding_max_frequency),\n-            embedding_dims // 2,\n-        )\n-    )\n-    angular_speeds = ops.cast(2.0 * math.pi * frequencies, \"float32\")\n-    embeddings = ops.concatenate(\n-        [ops.sin(angular_speeds * x), ops.cos(angular_speeds * x)], axis=3\n-    )\n-    return embeddings\n-\n-\n-def ResidualBlock(width):\n-    def apply(x):\n-        input_width = x.shape[3]\n-        if input_width == width:\n-            residual = x\n-        else:\n-            residual = layers.Conv2D(width, kernel_size=1)(x)\n-        x = layers.BatchNormalization(center=False, scale=False)(x)\n-        x = layers.Conv2D(\n-            width, kernel_size=3, padding=\"same\", activation=\"swish\"\n-        )(x)\n-        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n-        x = layers.Add()([x, residual])\n-        return x\n-\n-    return apply\n-\n-\n-def DownBlock(width, block_depth):\n-    def apply(x):\n-        x, skips = x\n-        for _ in range(block_depth):\n-            x = ResidualBlock(width)(x)\n-            skips.append(x)\n-        x = layers.AveragePooling2D(pool_size=2)(x)\n-        return x\n-\n-    return apply\n-\n-\n-def UpBlock(width, block_depth):\n-    def apply(x):\n-        x, skips = x\n-        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n-        for _ in range(block_depth):\n-            x = layers.Concatenate()([x, skips.pop()])\n-            x = ResidualBlock(width)(x)\n-        return x\n-\n-    return apply\n-\n-\n-def get_network(image_size, widths, block_depth):\n-    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n-    noise_variances = keras.Input(shape=(1, 1, 1))\n-\n-    e = layers.Lambda(sinusoidal_embedding, output_shape=(1, 1, 32))(\n-        noise_variances\n-    )\n-    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n-\n-    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n-    x = layers.Concatenate()([x, e])\n-\n-    skips = []\n-    for width in widths[:-1]:\n-        x = DownBlock(width, block_depth)([x, skips])\n-\n-    for _ in range(block_depth):\n-        x = ResidualBlock(widths[-1])(x)\n-\n-    for width in reversed(widths[:-1]):\n-        x = UpBlock(width, block_depth)([x, skips])\n-\n-    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n-\n-    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n-\n-\n-\"\"\"\n-This showcases the power of the Functional API. Note how we built a relatively complex\n-U-Net with skip connections, residual blocks, multiple inputs, and sinusoidal embeddings\n-in 80 lines of code!\n-\"\"\"\n-\n-\"\"\"\n-## Diffusion model\n-\n-### Diffusion schedule\n-\n-Let us say, that a diffusion process starts at time = 0, and ends at time = 1. This\n-variable will be called diffusion time, and can be either discrete (common in diffusion\n-models) or continuous (common in score-based models). I choose the latter, so that the\n-number of sampling steps can be changed at inference time.\n-\n-We need to have a function that tells us at each point in the diffusion process the noise\n-levels and signal levels of the noisy image corresponding to the actual diffusion time.\n-This will be called the diffusion schedule (see `diffusion_schedule()`).\n-\n-This schedule outputs two quantities: the `noise_rate` and the `signal_rate`\n-(corresponding to sqrt(1 - alpha) and sqrt(alpha) in the DDIM paper, respectively). We\n-generate the noisy image by weighting the random noise and the training image by their\n-corresponding rates and adding them together.\n-\n-Since the (standard normal) random noises and the (normalized) images both have zero mean\n-and unit variance, the noise rate and signal rate can be interpreted as the standard\n-deviation of their components in the noisy image, while the squares of their rates can be\n-interpreted as their variance (or their power in the signal processing sense). The rates\n-will always be set so that their squared sum is 1, meaning that the noisy images will\n-always have unit variance, just like its unscaled components.\n-\n-We will use a simplified, continuous version of the\n-[cosine schedule (Section 3.2)](https://arxiv.org/abs/2102.09672),\n-that is quite commonly used in the literature.\n-This schedule is symmetric, slow towards the start and end of the diffusion process, and\n-it also has a nice geometric interpretation, using the\n-[trigonometric properties of the unit circle](https://en.wikipedia.org/wiki/Unit_circle#/media/File:Circle-trig6.svg):\n-\n-![diffusion schedule gif](https://i.imgur.com/JW9W0fA.gif)\n-\n-### Training process\n-\n-The training procedure (see `train_step()` and `denoise()`) of denoising diffusion models\n-is the following: we sample random diffusion times uniformly, and mix the training images\n-with random gaussian noises at rates corresponding to the diffusion times. Then, we train\n-the model to separate the noisy image to its two components.\n-\n-Usually, the neural network is trained to predict the unscaled noise component, from\n-which the predicted image component can be calculated using the signal and noise rates.\n-Pixelwise\n-[mean squared error](https://keras.io/api/losses/regression_losses/#mean_squared_error-function) should\n-be used theoretically, however I recommend using\n-[mean absolute error](https://keras.io/api/losses/regression_losses/#mean_absolute_error-function)\n-instead (similarly to\n-[this](https://github.com/lucidrains/denoising-diffusion-pytorch/blob/master/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L371)\n-implementation), which produces better results on this dataset.\n-\n-### Sampling (reverse diffusion)\n-\n-When sampling (see `reverse_diffusion()`), at each step we take the previous estimate of\n-the noisy image and separate it into image and noise using our network. Then we recombine\n-these components using the signal and noise rate of the following step.\n-\n-Though a similar view is shown in\n-[Equation 12 of DDIMs](https://arxiv.org/abs/2010.02502), I believe the above explanation\n-of the sampling equation is not widely known.\n-\n-This example only implements the deterministic sampling procedure from DDIM, which\n-corresponds to *eta = 0* in the paper. One can also use stochastic sampling (in which\n-case the model becomes a\n-[Denoising Diffusion Probabilistic Model (DDPM)](https://arxiv.org/abs/2006.11239)),\n-where a part of the predicted noise is\n-replaced with the same or larger amount of random noise\n-([see Equation 16 and below](https://arxiv.org/abs/2010.02502)).\n-\n-Stochastic sampling can be used without retraining the network (since both models are\n-trained the same way), and it can improve sample quality, while on the other hand\n-requiring more sampling steps usually.\n-\"\"\"\n-\n-\n-@keras.saving.register_keras_serializable()\n-class DiffusionModel(keras.Model):\n-    def __init__(self, image_size, widths, block_depth):\n-        super().__init__()\n-\n-        self.normalizer = layers.Normalization()\n-        self.network = get_network(image_size, widths, block_depth)\n-        self.ema_network = keras.models.clone_model(self.network)\n-\n-    def compile(self, **kwargs):\n-        super().compile(**kwargs)\n-\n-        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n-        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n-        self.kid = KID(name=\"kid\")\n-\n-    @property\n-    def metrics(self):\n-        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n-\n-    def denormalize(self, images):\n-        # convert the pixel values back to 0-1 range\n-        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n-        return ops.clip(images, 0.0, 1.0)\n-\n-    def diffusion_schedule(self, diffusion_times):\n-        # diffusion times -> angles\n-        start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")\n-        end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")\n-\n-        diffusion_angles = start_angle + diffusion_times * (\n-            end_angle - start_angle\n-        )\n-\n-        # angles -> signal and noise rates\n-        signal_rates = ops.cos(diffusion_angles)\n-        noise_rates = ops.sin(diffusion_angles)\n-        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n-\n-        return noise_rates, signal_rates\n-\n-    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n-        # the exponential moving average weights are used at evaluation\n-        if training:\n-            network = self.network\n-        else:\n-            network = self.ema_network\n-\n-        # predict noise component and calculate the image component using it\n-        pred_noises = network(\n-            [noisy_images, noise_rates**2], training=training\n-        )\n-        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n-\n-        return pred_noises, pred_images\n-\n-    def reverse_diffusion(self, initial_noise, diffusion_steps):\n-        # reverse diffusion = sampling\n-        num_images = initial_noise.shape[0]\n-        step_size = 1.0 / diffusion_steps\n-\n-        # important line:\n-        # at the first sampling step, the \"noisy image\" is pure noise\n-        # but its signal rate is assumed to be nonzero (min_signal_rate)\n-        next_noisy_images = initial_noise\n-        for step in range(diffusion_steps):\n-            noisy_images = next_noisy_images\n-\n-            # separate the current noisy image to its components\n-            diffusion_times = ops.ones((num_images, 1, 1, 1)) - step * step_size\n-            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n-            pred_noises, pred_images = self.denoise(\n-                noisy_images, noise_rates, signal_rates, training=False\n-            )\n-            # network used in eval mode\n-\n-            # remix the predicted components using the next signal and noise rates\n-            next_diffusion_times = diffusion_times - step_size\n-            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n-                next_diffusion_times\n-            )\n-            next_noisy_images = (\n-                next_signal_rates * pred_images + next_noise_rates * pred_noises\n-            )\n-            # this new noisy image will be used in the next step\n-\n-        return pred_images\n-\n-    def generate(self, num_images, diffusion_steps):\n-        # noise -> images -> denormalized images\n-        initial_noise = keras.random.normal(\n-            shape=(num_images, image_size, image_size, 3)\n-        )\n-        generated_images = self.reverse_diffusion(\n-            initial_noise, diffusion_steps\n-        )\n-        generated_images = self.denormalize(generated_images)\n-        return generated_images\n-\n-    def train_step(self, images):\n-        # normalize images to have standard deviation of 1, like the noises\n-        images = self.normalizer(images, training=True)\n-        noises = keras.random.normal(\n-            shape=(batch_size, image_size, image_size, 3)\n-        )\n-\n-        # sample uniform random diffusion times\n-        diffusion_times = keras.random.uniform(\n-            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n-        )\n-        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n-        # mix the images with noises accordingly\n-        noisy_images = signal_rates * images + noise_rates * noises\n-\n-        with tf.GradientTape() as tape:\n-            # train the network to separate noisy images to their components\n-            pred_noises, pred_images = self.denoise(\n-                noisy_images, noise_rates, signal_rates, training=True\n-            )\n-\n-            noise_loss = self.loss(noises, pred_noises)  # used for training\n-            image_loss = self.loss(images, pred_images)  # only used as metric\n-\n-        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n-        self.optimizer.apply_gradients(\n-            zip(gradients, self.network.trainable_weights)\n-        )\n-\n-        self.noise_loss_tracker.update_state(noise_loss)\n-        self.image_loss_tracker.update_state(image_loss)\n-\n-        # track the exponential moving averages of weights\n-        for weight, ema_weight in zip(\n-            self.network.weights, self.ema_network.weights\n-        ):\n-            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n-\n-        # KID is not measured during the training phase for computational efficiency\n-        return {m.name: m.result() for m in self.metrics[:-1]}\n-\n-    def test_step(self, images):\n-        # normalize images to have standard deviation of 1, like the noises\n-        images = self.normalizer(images, training=False)\n-        noises = keras.random.normal(\n-            shape=(batch_size, image_size, image_size, 3)\n-        )\n-\n-        # sample uniform random diffusion times\n-        diffusion_times = keras.random.uniform(\n-            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n-        )\n-        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n-        # mix the images with noises accordingly\n-        noisy_images = signal_rates * images + noise_rates * noises\n-\n-        # use the network to separate noisy images to their components\n-        pred_noises, pred_images = self.denoise(\n-            noisy_images, noise_rates, signal_rates, training=False\n-        )\n-\n-        noise_loss = self.loss(noises, pred_noises)\n-        image_loss = self.loss(images, pred_images)\n-\n-        self.image_loss_tracker.update_state(image_loss)\n-        self.noise_loss_tracker.update_state(noise_loss)\n-\n-        # measure KID between real and generated images\n-        # this is computationally demanding, kid_diffusion_steps has to be small\n-        images = self.denormalize(images)\n-        generated_images = self.generate(\n-            num_images=batch_size, diffusion_steps=kid_diffusion_steps\n-        )\n-        self.kid.update_state(images, generated_images)\n-\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n-        # plot random generated images for visual evaluation of generation quality\n-        generated_images = self.generate(\n-            num_images=num_rows * num_cols,\n-            diffusion_steps=plot_diffusion_steps,\n-        )\n-\n-        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n-        for row in range(num_rows):\n-            for col in range(num_cols):\n-                index = row * num_cols + col\n-                plt.subplot(num_rows, num_cols, index + 1)\n-                plt.imshow(generated_images[index])\n-                plt.axis(\"off\")\n-        plt.tight_layout()\n-        plt.show()\n-        plt.close()\n-\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-# create and compile the model\n-model = DiffusionModel(image_size, widths, block_depth)\n-# below tensorflow 2.9:\n-# pip install tensorflow_addons\n-# import tensorflow_addons as tfa\n-# optimizer=tfa.optimizers.AdamW\n-model.compile(\n-    optimizer=keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    ),\n-    loss=keras.losses.mean_absolute_error,\n-)\n-# pixelwise mean absolute error is used as loss\n-\n-# save the best model based on the validation KID metric\n-checkpoint_path = \"checkpoints/diffusion_model.weights.h5\"\n-checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-    filepath=checkpoint_path,\n-    save_weights_only=True,\n-    monitor=\"val_kid\",\n-    mode=\"min\",\n-    save_best_only=True,\n-)\n-\n-# calculate mean and variance of training dataset for normalization\n-model.normalizer.adapt(train_dataset)\n-\n-# run training and plot generated images periodically\n-model.fit(\n-    train_dataset,\n-    epochs=num_epochs,\n-    validation_data=val_dataset,\n-    callbacks=[\n-        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n-        checkpoint_callback,\n-    ],\n-)\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-# load the best model and generate images\n-model.load_weights(checkpoint_path)\n-model.plot_images()\n-\n-\"\"\"\n-## Results\n-\n-By running the training for at least 50 epochs (takes 2 hours on a T4 GPU and 30 minutes\n-on an A100 GPU), one can get high quality image generations using this code example.\n-\n-The evolution of a batch of images over a 80 epoch training (color artifacts are due to\n-GIF compression):\n-\n-![flowers training gif](https://i.imgur.com/FSCKtZq.gif)\n-\n-Images generated using between 1 and 20 sampling steps from the same initial noise:\n-\n-![flowers sampling steps gif](https://i.imgur.com/tM5LyH3.gif)\n-\n-Interpolation (spherical) between initial noise samples:\n-\n-![flowers interpolation gif](https://i.imgur.com/hk5Hd5o.gif)\n-\n-Deterministic sampling process (noisy images on top, predicted images on bottom, 40\n-steps):\n-\n-![flowers deterministic generation gif](https://i.imgur.com/wCvzynh.gif)\n-\n-Stochastic sampling process (noisy images on top, predicted images on bottom, 80 steps):\n-\n-![flowers stochastic generation gif](https://i.imgur.com/kRXOGzd.gif)\n-\n-\"\"\"\n-\n-\"\"\"\n-## Lessons learned\n-\n-During preparation for this code example I have run numerous experiments using\n-[this repository](https://github.com/beresandras/clear-diffusion-keras).\n-In this section I list\n-the lessons learned and my recommendations in my subjective order of importance.\n-\n-### Algorithmic tips\n-\n-* **min. and max. signal rates**: I found the min. signal rate to be an important\n-hyperparameter. Setting it too low will make the generated images oversaturated, while\n-setting it too high will make them undersaturated. I recommend tuning it carefully. Also,\n-setting it to 0 will lead to a division by zero error. The max. signal rate can be set to\n-1, but I found that setting it lower slightly improves generation quality.\n-* **loss function**: While large models tend to use mean squared error (MSE) loss, I\n-recommend using mean absolute error (MAE) on this dataset. In my experience MSE loss\n-generates more diverse samples (it also seems to hallucinate more\n-[Section 3](https://arxiv.org/abs/2111.05826)), while MAE loss leads to smoother images.\n-I recommend trying both.\n-* **weight decay**: I did occasionally run into diverged trainings when scaling up the\n-model, and found that weight decay helps in avoiding instabilities at a low performance\n-cost. This is why I use\n-[AdamW](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/AdamW)\n-instead of [Adam](https://keras.io/api/optimizers/adam/) in this example.\n-* **exponential moving average of weights**: This helps to reduce the variance of the KID\n-metric, and helps in averaging out short-term changes during training.\n-* **image augmentations**: Though I did not use image augmentations in this example, in\n-my experience adding horizontal flips to the training increases generation performance,\n-while random crops do not. Since we use a supervised denoising loss, overfitting can be\n-an issue, so image augmentations might be important on small datasets. One should also be\n-careful not to use\n-[leaky augmentations](https://keras.io/examples/generative/gan_ada/#invertible-data-augmentation),\n-which can be done following\n-[this method (end of Section 5)](https://arxiv.org/abs/2206.00364) for instance.\n-* **data normalization**: In the literature the pixel values of images are usually\n-converted to the -1 to 1 range. For theoretical correctness, I normalize the images to\n-have zero mean and unit variance instead, exactly like the random noises.\n-* **noise level input**: I chose to input the noise variance to the network, as it is\n-symmetrical under our sampling schedule. One could also input the noise rate (similar\n-performance), the signal rate (lower performance), or even the\n-[log-signal-to-noise ratio (Appendix B.1)](https://arxiv.org/abs/2107.00630)\n-(did not try, as its range is highly\n-dependent on the min. and max. signal rates, and would require adjusting the min.\n-embedding frequency accordingly).\n-* **gradient clipping**: Using global gradient clipping with a value of 1 can help with\n-training stability for large models, but decreased performance significantly in my\n-experience.\n-* **residual connection downscaling**: For\n-[deeper models (Appendix B)](https://arxiv.org/abs/2205.11487), scaling the residual\n-connections with 1/sqrt(2) can be helpful, but did not help in my case.\n-* **learning rate**: For me, [Adam optimizer's](https://keras.io/api/optimizers/adam/)\n-default learning rate of 1e-3 worked very well, but lower learning rates are more common\n-in the [literature (Tables 11-13)](https://arxiv.org/abs/2105.05233).\n-\n-### Architectural tips\n-\n-* **sinusoidal embedding**: Using sinusoidal embeddings on the noise level input of the\n-network is crucial for good performance. I recommend setting the min. embedding frequency\n-to the reciprocal of the range of this input, and since we use the noise variance in this\n-example, it can be left always at 1. The max. embedding frequency controls the smallest\n-change in the noise variance that the network will be sensitive to, and the embedding\n-dimensions set the number of frequency components in the embedding. In my experience the\n-performance is not too sensitive to these values.\n-* **skip connections**: Using skip connections in the network architecture is absolutely\n-critical, without them the model will fail to learn to denoise at a good performance.\n-* **residual connections**: In my experience residual connections also significantly\n-improve performance, but this might be due to the fact that we only input the noise\n-level embeddings to the first layer of the network instead of to all of them.\n-* **normalization**: When scaling up the model, I did occasionally encounter diverged\n-trainings, using normalization layers helped to mitigate this issue. In the literature it\n-is common to use\n-[GroupNormalization](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GroupNormalization)\n-(with 8 groups for example) or\n-[LayerNormalization](https://keras.io/api/layers/normalization_layers/layer_normalization/)\n-in the network, I however chose to use\n-[BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/),\n-as it gave similar benefits in my experiments but was computationally lighter.\n-* **activations**: The choice of activation functions had a larger effect on generation\n-quality than I expected. In my experiments using non-monotonic activation functions\n-outperformed monotonic ones (such as\n-[ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)), with\n-[Swish](https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish) performing\n-the best (this is also what [Imagen uses, page 41](https://arxiv.org/abs/2205.11487)).\n-* **attention**: As mentioned earlier, it is common in the literature to use\n-[attention layers](https://keras.io/api/layers/attention_layers/multi_head_attention/) at low\n-resolutions for better global coherence. I omitted them for simplicity.\n-* **upsampling**:\n-[Bilinear and nearest neighbour upsampling](https://keras.io/api/layers/reshaping_layers/up_sampling2d/)\n-in the network performed similarly, however I did not try\n-[transposed convolutions](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/).\n-\n-For a similar list about GANs check out\n-[this Keras tutorial](https://keras.io/examples/generative/gan_ada/#gan-tips-and-tricks).\n-\"\"\"\n-\n-\"\"\"\n-## What to try next?\n-\n-If you would like to dive in deeper to the topic, I recommend checking out\n-[this repository](https://github.com/beresandras/clear-diffusion-keras) that I created in\n-preparation for this code example, which implements a wider range of features in a\n-similar style, such as:\n-\n-* stochastic sampling\n-* second-order sampling based on the\n-[differential equation view of DDIMs (Equation 13)](https://arxiv.org/abs/2010.02502)\n-* more diffusion schedules\n-* more network output types: predicting image or\n-[velocity (Appendix D)](https://arxiv.org/abs/2202.00512) instead of noise\n-* more datasets\n-\"\"\"\n-\n-\"\"\"\n-## Related works\n-\n-* [Score-based generative modeling](https://yang-song.github.io/blog/2021/score/)\n-(blogpost)\n-* [What are diffusion models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n-(blogpost)\n-* [Annotated diffusion model](https://huggingface.co/blog/annotated-diffusion) (blogpost)\n-* [CVPR 2022 tutorial on diffusion models](https://cvpr2022-tutorial-diffusion-models.github.io/)\n-(slides available)\n-* [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364):\n-attempts unifying diffusion methods under a common framework\n-* High-level video overviews: [1](https://www.youtube.com/watch?v=yTAMrHVG1ew),\n-[2](https://www.youtube.com/watch?v=344w5h24-h8)\n-* Detailed technical videos: [1](https://www.youtube.com/watch?v=fbLgFrlTnGU),\n-[2](https://www.youtube.com/watch?v=W-O7AZNzbzQ)\n-* Score-based generative models: [NCSN](https://arxiv.org/abs/1907.05600),\n-[NCSN+](https://arxiv.org/abs/2006.09011), [NCSN++](https://arxiv.org/abs/2011.13456)\n-* Denoising diffusion models: [DDPM](https://arxiv.org/abs/2006.11239),\n-[DDIM](https://arxiv.org/abs/2010.02502), [DDPM+](https://arxiv.org/abs/2102.09672),\n-[DDPM++](https://arxiv.org/abs/2105.05233)\n-* Large diffusion models: [GLIDE](https://arxiv.org/abs/2112.10741),\n-[DALL-E 2](https://arxiv.org/abs/2204.06125/), [Imagen](https://arxiv.org/abs/2205.11487)\n-\n-\n-\"\"\"\n\n@@ -1,864 +0,0 @@\n-\"\"\"\n-Title: Denoising Diffusion Probabilistic Model\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2022/11/30\n-Last modified: 2022/12/07\n-Description: Generating images of flowers with denoising diffusion probabilistic models.\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-Generative modeling experienced tremendous growth in the last five years. Models like\n-VAEs, GANs, and flow-based models proved to be a great success in generating\n-high-quality content, especially images. Diffusion models are a new type of generative\n-model that has proven to be better than previous approaches.\n-\n-Diffusion models are inspired by non-equilibrium thermodynamics, and they learn to\n-generate by denoising. Learning by denoising consists of two processes,\n-each of which is a Markov Chain. These are:\n-\n-1. The forward process: In the forward process, we slowly add random noise to the data\n-in a series of time steps `(t1, t2, ..., tn )`. Samples at the current time step are\n-drawn from a Gaussian distribution where the mean of the distribution is conditioned\n-on the sample at the previous time step, and the variance of the distribution follows\n-a fixed schedule. At the end of the forward process, the samples end up with a pure\n-noise distribution.\n-\n-2. The reverse process: During the reverse process, we try to undo the added noise at\n-every time step. We start with the pure noise distribution (the last step of the\n-forward process) and try to denoise the samples in the backward direction\n-`(tn, tn-1, ..., t1)`.\n-\n-We implement the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n-paper or DDPMs for short in this code example. It was the first paper demonstrating\n-the use of diffusion models for generating high-quality images. The authors proved\n-that a certain parameterization of diffusion models reveals an equivalence with\n-denoising score matching over multiple noise levels during training and with annealed\n-Langevin dynamics during sampling that generates the best quality results.\n-\n-This paper replicates both the Markov chains (forward process and reverse process)\n-involved in the diffusion process but for images. The forward process is fixed and\n-gradually adds Gaussian noise to the images according to a fixed variance schedule\n-denoted by beta in the paper. This is what the diffusion process looks like in case\n-of images: (image -> noise::noise -> image)\n-\n-![diffusion process gif](https://imgur.com/Yn7tho9.gif)\n-\n-\n-The paper describes two algorithms, one for training the model, and the other for\n-sampling from the trained model. Training is performed by optimizing the usual\n-variational bound on negative log-likelihood. The objective function is further\n-simplified, and the network is treated as a noise prediction network. Once optimized,\n-we can sample from the network to generate new images from noise samples. Here is an\n-overview of both algorithms as presented in the paper:\n-\n-![ddpms](https://i.imgur.com/S7KH5hZ.png)\n-\n-\n-**Note:** DDPM is just one way of implementing a diffusion model. Also, the sampling\n-algorithm in the DDPM replicates the complete Markov chain. Hence it's slow in\n-generating new samples compared to other generative models like GANs. Lots of research\n-efforts have been made to address this issue. One such example is Denoising Diffusion\n-Implicit Models, or DDIM for short, where the authors replaced the Markov chain with a\n-non-Markovian process to sample faster. You can find the code example for DDIM\n-[here](https://keras.io/examples/generative/ddim/)\n-\n-Implementing a DDPM model is simple. We define a model that takes\n-two inputs: Images and the randomly sampled time steps. At each training step, we\n-perform the following operations to train our model:\n-\n-1. Sample random noise to be added to the inputs.\n-2. Apply the forward process to diffuse the inputs with the sampled noise.\n-3. Your model takes these noisy samples as inputs and outputs the noise\n-prediction for each time step.\n-4. Given true noise and predicted noise, we calculate the loss values\n-5. We then calculate the gradients and update the model weights.\n-\n-Given that our model knows how to denoise a noisy sample at a given time step,\n-we can leverage this idea to generate new samples, starting from a pure noise\n-distribution.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import math\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-# Requires TensorFlow >=2.11 for the GroupNormalization layer.\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-import tensorflow_datasets as tfds\n-\n-\"\"\"\n-## Hyperparameters\n-\"\"\"\n-\n-batch_size = 32\n-num_epochs = 1  # Just for the sake of demonstration\n-total_timesteps = 1000\n-norm_groups = 8  # Number of groups used in GroupNormalization layer\n-learning_rate = 2e-4\n-\n-img_size = 64\n-img_channels = 3\n-clip_min = -1.0\n-clip_max = 1.0\n-\n-first_conv_channels = 64\n-channel_multiplier = [1, 2, 4, 8]\n-widths = [first_conv_channels * mult for mult in channel_multiplier]\n-has_attention = [False, False, True, True]\n-num_res_blocks = 2  # Number of residual blocks\n-\n-dataset_name = \"oxford_flowers102\"\n-splits = [\"train\"]\n-\n-\n-\"\"\"\n-## Dataset\n-\n-We use the [Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)\n-dataset for generating images of flowers. In terms of preprocessing, we use center\n-cropping for resizing the images to the desired image size, and we rescale the pixel\n-values in the range `[-1.0, 1.0]`. This is in line with the range of the pixel values that\n-was applied by the authors of the [DDPMs paper](https://arxiv.org/abs/2006.11239). For\n-augmenting training data, we randomly flip the images left/right.\n-\"\"\"\n-\n-\n-# Load the dataset\n-(ds,) = tfds.load(\n-    dataset_name, split=splits, with_info=False, shuffle_files=True\n-)\n-\n-\n-def augment(img):\n-    \"\"\"Flips an image left/right randomly.\"\"\"\n-    return tf.image.random_flip_left_right(img)\n-\n-\n-def resize_and_rescale(img, size):\n-    \"\"\"Resize the image to the desired size first and then\n-    rescale the pixel values in the range [-1.0, 1.0].\n-\n-    Args:\n-        img: Image tensor\n-        size: Desired image size for resizing\n-    Returns:\n-        Resized and rescaled image tensor\n-    \"\"\"\n-\n-    height = tf.shape(img)[0]\n-    width = tf.shape(img)[1]\n-    crop_size = tf.minimum(height, width)\n-\n-    img = tf.image.crop_to_bounding_box(\n-        img,\n-        (height - crop_size) // 2,\n-        (width - crop_size) // 2,\n-        crop_size,\n-        crop_size,\n-    )\n-\n-    # Resize\n-    img = tf.cast(img, dtype=tf.float32)\n-    img = tf.image.resize(img, size=size, antialias=True)\n-\n-    # Rescale the pixel values\n-    img = img / 127.5 - 1.0\n-    img = tf.clip_by_value(img, clip_min, clip_max)\n-    return img\n-\n-\n-def train_preprocessing(x):\n-    img = x[\"image\"]\n-    img = resize_and_rescale(img, size=(img_size, img_size))\n-    img = augment(img)\n-    return img\n-\n-\n-train_ds = (\n-    ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n-    .batch(batch_size, drop_remainder=True)\n-    .shuffle(batch_size * 2)\n-    .prefetch(tf.data.AUTOTUNE)\n-)\n-\n-\n-\"\"\"\n-## Gaussian diffusion utilities\n-\n-We define the forward process and the reverse process\n-as a separate utility. Most of the code in this utility has been borrowed\n-from the original implementation with some slight modifications.\n-\"\"\"\n-\n-\n-class GaussianDiffusion:\n-    \"\"\"Gaussian diffusion utility.\n-\n-    Args:\n-        beta_start: Start value of the scheduled variance\n-        beta_end: End value of the scheduled variance\n-        timesteps: Number of time steps in the forward process\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        beta_start=1e-4,\n-        beta_end=0.02,\n-        timesteps=1000,\n-        clip_min=-1.0,\n-        clip_max=1.0,\n-    ):\n-        self.beta_start = beta_start\n-        self.beta_end = beta_end\n-        self.timesteps = timesteps\n-        self.clip_min = clip_min\n-        self.clip_max = clip_max\n-\n-        # Define the linear variance schedule\n-        self.betas = betas = np.linspace(\n-            beta_start,\n-            beta_end,\n-            timesteps,\n-            dtype=np.float64,  # Using float64 for better precision\n-        )\n-        self.num_timesteps = int(timesteps)\n-\n-        alphas = 1.0 - betas\n-        alphas_cumprod = np.cumprod(alphas, axis=0)\n-        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n-\n-        self.betas = tf.constant(betas, dtype=tf.float32)\n-        self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf.float32)\n-        self.alphas_cumprod_prev = tf.constant(\n-            alphas_cumprod_prev, dtype=tf.float32\n-        )\n-\n-        # Calculations for diffusion q(x_t | x_{t-1}) and others\n-        self.sqrt_alphas_cumprod = tf.constant(\n-            np.sqrt(alphas_cumprod), dtype=tf.float32\n-        )\n-\n-        self.sqrt_one_minus_alphas_cumprod = tf.constant(\n-            np.sqrt(1.0 - alphas_cumprod), dtype=tf.float32\n-        )\n-\n-        self.log_one_minus_alphas_cumprod = tf.constant(\n-            np.log(1.0 - alphas_cumprod), dtype=tf.float32\n-        )\n-\n-        self.sqrt_recip_alphas_cumprod = tf.constant(\n-            np.sqrt(1.0 / alphas_cumprod), dtype=tf.float32\n-        )\n-        self.sqrt_recipm1_alphas_cumprod = tf.constant(\n-            np.sqrt(1.0 / alphas_cumprod - 1), dtype=tf.float32\n-        )\n-\n-        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n-        posterior_variance = (\n-            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n-        )\n-        self.posterior_variance = tf.constant(\n-            posterior_variance, dtype=tf.float32\n-        )\n-\n-        # Log calculation clipped because the posterior variance is 0 at the beginning\n-        # of the diffusion chain\n-        self.posterior_log_variance_clipped = tf.constant(\n-            np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf.float32\n-        )\n-\n-        self.posterior_mean_coef1 = tf.constant(\n-            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),\n-            dtype=tf.float32,\n-        )\n-\n-        self.posterior_mean_coef2 = tf.constant(\n-            (1.0 - alphas_cumprod_prev)\n-            * np.sqrt(alphas)\n-            / (1.0 - alphas_cumprod),\n-            dtype=tf.float32,\n-        )\n-\n-    def _extract(self, a, t, x_shape):\n-        \"\"\"Extract some coefficients at specified timesteps,\n-        then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n-\n-        Args:\n-            a: Tensor to extract from\n-            t: Timestep for which the coefficients are to be extracted\n-            x_shape: Shape of the current batched samples\n-        \"\"\"\n-        batch_size = x_shape[0]\n-        out = tf.gather(a, t)\n-        return tf.reshape(out, [batch_size, 1, 1, 1])\n-\n-    def q_mean_variance(self, x_start, t):\n-        \"\"\"Extracts the mean, and the variance at current timestep.\n-\n-        Args:\n-            x_start: Initial sample (before the first diffusion step)\n-            t: Current timestep\n-        \"\"\"\n-        x_start_shape = tf.shape(x_start)\n-        mean = (\n-            self._extract(self.sqrt_alphas_cumprod, t, x_start_shape) * x_start\n-        )\n-        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start_shape)\n-        log_variance = self._extract(\n-            self.log_one_minus_alphas_cumprod, t, x_start_shape\n-        )\n-        return mean, variance, log_variance\n-\n-    def q_sample(self, x_start, t, noise):\n-        \"\"\"Diffuse the data.\n-\n-        Args:\n-            x_start: Initial sample (before the first diffusion step)\n-            t: Current timestep\n-            noise: Gaussian noise to be added at the current timestep\n-        Returns:\n-            Diffused samples at timestep `t`\n-        \"\"\"\n-        x_start_shape = tf.shape(x_start)\n-        return (\n-            self._extract(self.sqrt_alphas_cumprod, t, tf.shape(x_start))\n-            * x_start\n-            + self._extract(\n-                self.sqrt_one_minus_alphas_cumprod, t, x_start_shape\n-            )\n-            * noise\n-        )\n-\n-    def predict_start_from_noise(self, x_t, t, noise):\n-        x_t_shape = tf.shape(x_t)\n-        return (\n-            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t_shape) * x_t\n-            - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t_shape)\n-            * noise\n-        )\n-\n-    def q_posterior(self, x_start, x_t, t):\n-        \"\"\"Compute the mean and variance of the diffusion\n-        posterior q(x_{t-1} | x_t, x_0).\n-\n-        Args:\n-            x_start: Stating point(sample) for the posterior computation\n-            x_t: Sample at timestep `t`\n-            t: Current timestep\n-        Returns:\n-            Posterior mean and variance at current timestep\n-        \"\"\"\n-\n-        x_t_shape = tf.shape(x_t)\n-        posterior_mean = (\n-            self._extract(self.posterior_mean_coef1, t, x_t_shape) * x_start\n-            + self._extract(self.posterior_mean_coef2, t, x_t_shape) * x_t\n-        )\n-        posterior_variance = self._extract(\n-            self.posterior_variance, t, x_t_shape\n-        )\n-        posterior_log_variance_clipped = self._extract(\n-            self.posterior_log_variance_clipped, t, x_t_shape\n-        )\n-        return (\n-            posterior_mean,\n-            posterior_variance,\n-            posterior_log_variance_clipped,\n-        )\n-\n-    def p_mean_variance(self, pred_noise, x, t, clip_denoised=True):\n-        x_recon = self.predict_start_from_noise(x, t=t, noise=pred_noise)\n-        if clip_denoised:\n-            x_recon = tf.clip_by_value(x_recon, self.clip_min, self.clip_max)\n-\n-        (\n-            model_mean,\n-            posterior_variance,\n-            posterior_log_variance,\n-        ) = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n-        return model_mean, posterior_variance, posterior_log_variance\n-\n-    def p_sample(self, pred_noise, x, t, clip_denoised=True):\n-        \"\"\"Sample from the diffuison model.\n-\n-        Args:\n-            pred_noise: Noise predicted by the diffusion model\n-            x: Samples at a given timestep for which the noise was predicted\n-            t: Current timestep\n-            clip_denoised (bool): Whether to clip the predicted noise\n-                within the specified range or not.\n-        \"\"\"\n-        model_mean, _, model_log_variance = self.p_mean_variance(\n-            pred_noise, x=x, t=t, clip_denoised=clip_denoised\n-        )\n-        noise = tf.random.normal(shape=x.shape, dtype=x.dtype)\n-        # No noise when t == 0\n-        nonzero_mask = tf.reshape(\n-            1 - tf.cast(tf.equal(t, 0), tf.float32), [tf.shape(x)[0], 1, 1, 1]\n-        )\n-        return (\n-            model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise\n-        )\n-\n-\n-\"\"\"\n-## Network architecture\n-\n-U-Net, originally developed for semantic segmentation, is an architecture that is\n-widely used for implementing diffusion models but with some slight modifications:\n-\n-1. The network accepts two inputs: Image and time step\n-2. Self-attention between the convolution blocks once we reach a specific resolution\n-(16x16 in the paper)\n-3. Group Normalization instead of weight normalization\n-\n-We implement most of the things as used in the original paper. We use the\n-`swish` activation function throughout the network. We use the variance scaling\n-kernel initializer.\n-\n-The only difference here is the number of groups used for the\n-`GroupNormalization` layer. For the flowers dataset,\n-we found that a value of `groups=8` produces better results\n-compared to the default value of `groups=32`. Dropout is optional and should be\n-used where chances of over fitting is high. In the paper, the authors used dropout\n-only when training on CIFAR10.\n-\"\"\"\n-\n-\n-# Kernel initializer to use\n-def kernel_init(scale):\n-    scale = max(scale, 1e-10)\n-    return keras.initializers.VarianceScaling(\n-        scale, mode=\"fan_avg\", distribution=\"uniform\"\n-    )\n-\n-\n-class AttentionBlock(layers.Layer):\n-    \"\"\"Applies self-attention.\n-\n-    Args:\n-        units: Number of units in the dense layers\n-        groups: Number of groups to be used for GroupNormalization layer\n-    \"\"\"\n-\n-    def __init__(self, units, groups=8, **kwargs):\n-        self.units = units\n-        self.groups = groups\n-        super().__init__(**kwargs)\n-\n-        self.norm = layers.GroupNormalization(groups=groups)\n-        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n-        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n-        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n-        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))\n-\n-    def call(self, inputs):\n-        batch_size = tf.shape(inputs)[0]\n-        height = tf.shape(inputs)[1]\n-        width = tf.shape(inputs)[2]\n-        scale = tf.cast(self.units, tf.float32) ** (-0.5)\n-\n-        inputs = self.norm(inputs)\n-        q = self.query(inputs)\n-        k = self.key(inputs)\n-        v = self.value(inputs)\n-\n-        attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale\n-        attn_score = tf.reshape(\n-            attn_score, [batch_size, height, width, height * width]\n-        )\n-\n-        attn_score = tf.nn.softmax(attn_score, -1)\n-        attn_score = tf.reshape(\n-            attn_score, [batch_size, height, width, height, width]\n-        )\n-\n-        proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)\n-        proj = self.proj(proj)\n-        return inputs + proj\n-\n-\n-class TimeEmbedding(layers.Layer):\n-    def __init__(self, dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.dim = dim\n-        self.half_dim = dim // 2\n-        self.emb = math.log(10000) / (self.half_dim - 1)\n-        self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)\n-\n-    def call(self, inputs):\n-        inputs = tf.cast(inputs, dtype=tf.float32)\n-        emb = inputs[:, None] * self.emb[None, :]\n-        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n-        return emb\n-\n-\n-def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):\n-    def apply(inputs):\n-        x, t = inputs\n-        input_width = x.shape[3]\n-\n-        if input_width == width:\n-            residual = x\n-        else:\n-            residual = layers.Conv2D(\n-                width, kernel_size=1, kernel_initializer=kernel_init(1.0)\n-            )(x)\n-\n-        temb = activation_fn(t)\n-        temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[\n-            :, None, None, :\n-        ]\n-\n-        x = layers.GroupNormalization(groups=groups)(x)\n-        x = activation_fn(x)\n-        x = layers.Conv2D(\n-            width,\n-            kernel_size=3,\n-            padding=\"same\",\n-            kernel_initializer=kernel_init(1.0),\n-        )(x)\n-\n-        x = layers.Add()([x, temb])\n-        x = layers.GroupNormalization(groups=groups)(x)\n-        x = activation_fn(x)\n-\n-        x = layers.Conv2D(\n-            width,\n-            kernel_size=3,\n-            padding=\"same\",\n-            kernel_initializer=kernel_init(0.0),\n-        )(x)\n-        x = layers.Add()([x, residual])\n-        return x\n-\n-    return apply\n-\n-\n-def DownSample(width):\n-    def apply(x):\n-        x = layers.Conv2D(\n-            width,\n-            kernel_size=3,\n-            strides=2,\n-            padding=\"same\",\n-            kernel_initializer=kernel_init(1.0),\n-        )(x)\n-        return x\n-\n-    return apply\n-\n-\n-def UpSample(width, interpolation=\"nearest\"):\n-    def apply(x):\n-        x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)\n-        x = layers.Conv2D(\n-            width,\n-            kernel_size=3,\n-            padding=\"same\",\n-            kernel_initializer=kernel_init(1.0),\n-        )(x)\n-        return x\n-\n-    return apply\n-\n-\n-def TimeMLP(units, activation_fn=keras.activations.swish):\n-    def apply(inputs):\n-        temb = layers.Dense(\n-            units, activation=activation_fn, kernel_initializer=kernel_init(1.0)\n-        )(inputs)\n-        temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)\n-        return temb\n-\n-    return apply\n-\n-\n-def build_model(\n-    img_size,\n-    img_channels,\n-    widths,\n-    has_attention,\n-    num_res_blocks=2,\n-    norm_groups=8,\n-    interpolation=\"nearest\",\n-    activation_fn=keras.activations.swish,\n-):\n-    image_input = layers.Input(\n-        shape=(img_size, img_size, img_channels), name=\"image_input\"\n-    )\n-    time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")\n-\n-    x = layers.Conv2D(\n-        first_conv_channels,\n-        kernel_size=(3, 3),\n-        padding=\"same\",\n-        kernel_initializer=kernel_init(1.0),\n-    )(image_input)\n-\n-    temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)\n-    temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(\n-        temb\n-    )\n-\n-    skips = [x]\n-\n-    # DownBlock\n-    for i in range(len(widths)):\n-        for _ in range(num_res_blocks):\n-            x = ResidualBlock(\n-                widths[i], groups=norm_groups, activation_fn=activation_fn\n-            )([x, temb])\n-            if has_attention[i]:\n-                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n-            skips.append(x)\n-\n-        if widths[i] != widths[-1]:\n-            x = DownSample(widths[i])(x)\n-            skips.append(x)\n-\n-    # MiddleBlock\n-    x = ResidualBlock(\n-        widths[-1], groups=norm_groups, activation_fn=activation_fn\n-    )([x, temb])\n-    x = AttentionBlock(widths[-1], groups=norm_groups)(x)\n-    x = ResidualBlock(\n-        widths[-1], groups=norm_groups, activation_fn=activation_fn\n-    )([x, temb])\n-\n-    # UpBlock\n-    for i in reversed(range(len(widths))):\n-        for _ in range(num_res_blocks + 1):\n-            x = layers.Concatenate(axis=-1)([x, skips.pop()])\n-            x = ResidualBlock(\n-                widths[i], groups=norm_groups, activation_fn=activation_fn\n-            )([x, temb])\n-            if has_attention[i]:\n-                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n-\n-        if i != 0:\n-            x = UpSample(widths[i], interpolation=interpolation)(x)\n-\n-    # End block\n-    x = layers.GroupNormalization(groups=norm_groups)(x)\n-    x = activation_fn(x)\n-    x = layers.Conv2D(\n-        3, (3, 3), padding=\"same\", kernel_initializer=kernel_init(0.0)\n-    )(x)\n-    return keras.Model([image_input, time_input], x, name=\"unet\")\n-\n-\n-\"\"\"\n-## Training\n-\n-We follow the same setup for training the diffusion model as described\n-in the paper. We use `Adam` optimizer with a learning rate of `2e-4`.\n-We use EMA on model parameters with a decay factor of 0.999. We\n-treat our model as noise prediction network i.e. at every training step, we\n-input a batch of images and corresponding time steps to our UNet,\n-and the network outputs the noise as predictions.\n-\n-The only difference is that we aren't using the Kernel Inception Distance (KID)\n-or Frechet Inception Distance (FID) for evaluating the quality of generated\n-samples during training. This is because both these metrics are compute heavy\n-and are skipped for the brevity of implementation.\n-\n-**Note: ** We are using mean squared error as the loss function which is aligned with\n-the paper, and theoretically makes sense. In practice, though, it is also common to\n-use mean absolute error or Huber loss as the loss function.\n-\"\"\"\n-\n-\n-class DiffusionModel(keras.Model):\n-    def __init__(self, network, ema_network, timesteps, gdf_util, ema=0.999):\n-        super().__init__()\n-        self.network = network\n-        self.ema_network = ema_network\n-        self.timesteps = timesteps\n-        self.gdf_util = gdf_util\n-        self.ema = ema\n-\n-    def train_step(self, images):\n-        # 1. Get the batch size\n-        batch_size = tf.shape(images)[0]\n-\n-        # 2. Sample timesteps uniformly\n-        t = tf.random.uniform(\n-            minval=0, maxval=self.timesteps, shape=(batch_size,), dtype=tf.int64\n-        )\n-\n-        with tf.GradientTape() as tape:\n-            # 3. Sample random noise to be added to the images in the batch\n-            noise = tf.random.normal(shape=tf.shape(images), dtype=images.dtype)\n-\n-            # 4. Diffuse the images with noise\n-            images_t = self.gdf_util.q_sample(images, t, noise)\n-\n-            # 5. Pass the diffused images and time steps to the network\n-            pred_noise = self.network([images_t, t], training=True)\n-\n-            # 6. Calculate the loss\n-            loss = self.loss(noise, pred_noise)\n-\n-        # 7. Get the gradients\n-        gradients = tape.gradient(loss, self.network.trainable_weights)\n-\n-        # 8. Update the weights of the network\n-        self.optimizer.apply_gradients(\n-            zip(gradients, self.network.trainable_weights)\n-        )\n-\n-        # 9. Updates the weight values for the network with EMA weights\n-        for weight, ema_weight in zip(\n-            self.network.weights, self.ema_network.weights\n-        ):\n-            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)\n-\n-        # 10. Return loss values\n-        return {\"loss\": loss}\n-\n-    def generate_images(self, num_images=16):\n-        # 1. Randomly sample noise (starting point for reverse process)\n-        samples = tf.random.normal(\n-            shape=(num_images, img_size, img_size, img_channels),\n-            dtype=tf.float32,\n-        )\n-        # 2. Sample from the model iteratively\n-        for t in reversed(range(0, self.timesteps)):\n-            tt = tf.cast(tf.fill(num_images, t), dtype=tf.int64)\n-            pred_noise = self.ema_network.predict(\n-                [samples, tt], verbose=0, batch_size=num_images\n-            )\n-            samples = self.gdf_util.p_sample(\n-                pred_noise, samples, tt, clip_denoised=True\n-            )\n-        # 3. Return generated samples\n-        return samples\n-\n-    def plot_images(\n-        self, epoch=None, logs=None, num_rows=2, num_cols=8, figsize=(12, 5)\n-    ):\n-        \"\"\"Utility to plot images using the diffusion model during training.\"\"\"\n-        generated_samples = self.generate_images(num_images=num_rows * num_cols)\n-        generated_samples = (\n-            tf.clip_by_value(generated_samples * 127.5 + 127.5, 0.0, 255.0)\n-            .numpy()\n-            .astype(np.uint8)\n-        )\n-\n-        _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n-        for i, image in enumerate(generated_samples):\n-            if num_rows == 1:\n-                ax[i].imshow(image)\n-                ax[i].axis(\"off\")\n-            else:\n-                ax[i // num_cols, i % num_cols].imshow(image)\n-                ax[i // num_cols, i % num_cols].axis(\"off\")\n-\n-        plt.tight_layout()\n-        plt.show()\n-\n-\n-# Build the unet model\n-network = build_model(\n-    img_size=img_size,\n-    img_channels=img_channels,\n-    widths=widths,\n-    has_attention=has_attention,\n-    num_res_blocks=num_res_blocks,\n-    norm_groups=norm_groups,\n-    activation_fn=keras.activations.swish,\n-)\n-ema_network = build_model(\n-    img_size=img_size,\n-    img_channels=img_channels,\n-    widths=widths,\n-    has_attention=has_attention,\n-    num_res_blocks=num_res_blocks,\n-    norm_groups=norm_groups,\n-    activation_fn=keras.activations.swish,\n-)\n-ema_network.set_weights(\n-    network.get_weights()\n-)  # Initially the weights are the same\n-\n-# Get an instance of the Gaussian Diffusion utilities\n-gdf_util = GaussianDiffusion(timesteps=total_timesteps)\n-\n-# Get the model\n-model = DiffusionModel(\n-    network=network,\n-    ema_network=ema_network,\n-    gdf_util=gdf_util,\n-    timesteps=total_timesteps,\n-)\n-\n-# Compile the model\n-model.compile(\n-    loss=keras.losses.MeanSquaredError(),\n-    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n-    jit_compile=False,\n-)\n-\n-# Train the model\n-model.fit(\n-    train_ds,\n-    epochs=num_epochs,\n-    batch_size=batch_size,\n-    callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images)],\n-)\n-\n-\"\"\"\n-## Results\n-\n-We trained this model for 800 epochs on a V100 GPU,\n-and each epoch took almost 8 seconds to finish. We load those weights\n-here, and we generate a few samples starting from pure noise.\n-\"\"\"\n-\n-\"\"\"shell\n-curl -LO https://github.com/AakashKumarNain/ddpms/releases/download/v3.0.0/checkpoints.zip\n-unzip -qq checkpoints.zip\n-\"\"\"\n-\n-# Load the model weights\n-model.ema_network.load_weights(\"checkpoints/diffusion_model_checkpoint\")\n-\n-# Generate and plot some samples\n-model.plot_images(num_rows=4, num_cols=8)\n-\n-\n-\"\"\"\n-## Conclusion\n-\n-We successfully implemented and trained a diffusion model exactly in the same\n-fashion as implemented by the authors of the DDPMs paper. You can find the\n-original implementation [here](https://github.com/hojonathanho/diffusion).\n-\n-There are a few things that you can try to improve the model:\n-\n-1. Increasing the width of each block. A bigger model can learn to denoise\n-in fewer epochs, though you may have to take care of overfitting.\n-\n-2. We implemented the linear schedule for variance scheduling. You can implement\n-other schemes like cosine scheduling and compare the performance.\n-\"\"\"\n-\n-\"\"\"\n-## References\n-\n-1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n-2. [Author's implementation](https://github.com/hojonathanho/diffusion)\n-3. [A deep dive into DDPMs](https://magic-with-latents.github.io/latent/posts/ddpms/part3/)\n-4. [Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/)\n-5. [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n-6. [AIAIART](https://www.youtube.com/watch?v=XTs7M6TSK9I&t=14s)\n-\"\"\"\n\n@@ -1,211 +0,0 @@\n-\"\"\"\n-Title: Deep Dream\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2016/01/13\n-Last modified: 2020/05/02\n-Description: Generating Deep Dreams with Keras.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-\"Deep dream\" is an image-filtering technique which consists of taking an image\n-classification model, and running gradient ascent over an input image to\n-try to maximize the activations of specific layers (and sometimes, specific units in\n-specific layers) for this input. It produces hallucination-like visuals.\n-\n-It was first introduced by Alexander Mordvintsev from Google in July 2015.\n-\n-Process:\n-\n-- Load the original image.\n-- Define a number of processing scales (\"octaves\"),\n-from smallest to largest.\n-- Resize the original image to the smallest scale.\n-- For every scale, starting with the smallest (i.e. current one):\n-    - Run gradient ascent\n-    - Upscale image to the next scale\n-    - Reinject the detail that was lost at upscaling time\n-- Stop when we are back to the original size.\n-To obtain the detail lost during upscaling, we simply\n-take the original image, shrink it down, upscale it,\n-and compare the result to the (resized) original image.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras.applications import inception_v3\n-\n-base_image_path = keras.utils.get_file(\n-    \"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\"\n-)\n-result_prefix = \"sky_dream\"\n-\n-# These are the names of the layers\n-# for which we try to maximize activation,\n-# as well as their weight in the final loss\n-# we try to maximize.\n-# You can tweak these setting to obtain new visual effects.\n-layer_settings = {\n-    \"mixed4\": 1.0,\n-    \"mixed5\": 1.5,\n-    \"mixed6\": 2.0,\n-    \"mixed7\": 2.5,\n-}\n-\n-# Playing with these hyperparameters will also allow you to achieve new effects\n-step = 0.01  # Gradient ascent step size\n-num_octave = 3  # Number of scales at which to run gradient ascent\n-octave_scale = 1.4  # Size ratio between scales\n-iterations = 20  # Number of ascent steps per scale\n-max_loss = 15.0\n-\n-\"\"\"\n-This is our base image:\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-display(Image(base_image_path))\n-\n-\"\"\"\n-Let's set up some image preprocessing/deprocessing utilities:\n-\"\"\"\n-\n-\n-def preprocess_image(image_path):\n-    # Util function to open, resize and format pictures\n-    # into appropriate arrays.\n-    img = keras.utils.load_img(image_path)\n-    img = keras.utils.img_to_array(img)\n-    img = np.expand_dims(img, axis=0)\n-    img = inception_v3.preprocess_input(img)\n-    return img\n-\n-\n-def deprocess_image(x):\n-    # Util function to convert a NumPy array into a valid image.\n-    x = x.reshape((x.shape[1], x.shape[2], 3))\n-    # Undo inception v3 preprocessing\n-    x /= 2.0\n-    x += 0.5\n-    x *= 255.0\n-    # Convert to uint8 and clip to the valid range [0, 255]\n-    x = np.clip(x, 0, 255).astype(\"uint8\")\n-    return x\n-\n-\n-\"\"\"\n-## Compute the Deep Dream loss\n-\n-First, build a feature extraction model to retrieve the activations of our target layers\n-given an input image.\n-\"\"\"\n-\n-# Build an InceptionV3 model loaded with pre-trained ImageNet weights\n-model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)\n-\n-# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-outputs_dict = dict(\n-    [\n-        (layer.name, layer.output)\n-        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n-    ]\n-)\n-\n-# Set up a model that returns the activation values for every target layer\n-# (as a dict)\n-feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n-\n-\"\"\"\n-The actual loss computation is very simple:\n-\"\"\"\n-\n-\n-def compute_loss(input_image):\n-    features = feature_extractor(input_image)\n-    # Initialize the loss\n-    loss = tf.zeros(shape=())\n-    for name in features.keys():\n-        coeff = layer_settings[name]\n-        activation = features[name]\n-        # We avoid border artifacts by only involving non-border pixels in the loss.\n-        scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n-        loss += (\n-            coeff\n-            * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :]))\n-            / scaling\n-        )\n-    return loss\n-\n-\n-\"\"\"\n-## Set up the gradient ascent loop for one octave\n-\"\"\"\n-\n-\n-@tf.function\n-def gradient_ascent_step(img, learning_rate):\n-    with tf.GradientTape() as tape:\n-        tape.watch(img)\n-        loss = compute_loss(img)\n-    # Compute gradients.\n-    grads = tape.gradient(loss, img)\n-    # Normalize gradients.\n-    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n-    img += learning_rate * grads\n-    return loss, img\n-\n-\n-def gradient_ascent_loop(img, iterations, learning_rate, max_loss=None):\n-    for i in range(iterations):\n-        loss, img = gradient_ascent_step(img, learning_rate)\n-        if max_loss is not None and loss > max_loss:\n-            break\n-        print(\"... Loss value at step %d: %.2f\" % (i, loss))\n-    return img\n-\n-\n-\"\"\"\n-## Run the training loop, iterating over different octaves\n-\"\"\"\n-\n-original_img = preprocess_image(base_image_path)\n-original_shape = original_img.shape[1:3]\n-\n-successive_shapes = [original_shape]\n-for i in range(1, num_octave):\n-    shape = tuple([int(dim / (octave_scale**i)) for dim in original_shape])\n-    successive_shapes.append(shape)\n-successive_shapes = successive_shapes[::-1]\n-shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n-\n-img = tf.identity(original_img)  # Make a copy\n-for i, shape in enumerate(successive_shapes):\n-    print(\"Processing octave %d with shape %s\" % (i, shape))\n-    img = tf.image.resize(img, shape)\n-    img = gradient_ascent_loop(\n-        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n-    )\n-    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n-    same_size_original = tf.image.resize(original_img, shape)\n-    lost_detail = same_size_original - upscaled_shrunk_original_img\n-\n-    img += lost_detail\n-    shrunk_original_img = tf.image.resize(original_img, shape)\n-\n-keras.utils.save_img(result_prefix + \".png\", deprocess_image(img.numpy()))\n-\n-\"\"\"\n-Display the result.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deep-dream)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/deep-dream).\n-\"\"\"\n-\n-display(Image(result_prefix + \".png\"))\n\n@@ -1,131 +0,0 @@\n-\"\"\"\n-Title: Character-level text generation with LSTM\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2015/06/15\n-Last modified: 2020/04/30\n-Description: Generate text from Nietzsche's writings with a character-level LSTM.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to use a LSTM model to generate\n-text character-by-character.\n-\n-At least 20 epochs are required before the generated text\n-starts sounding locally coherent.\n-\n-It is recommended to run this script on GPU, as recurrent\n-networks are quite computationally intensive.\n-\n-If you try this script on new data, make sure your corpus\n-has at least ~100k characters. ~1M is better.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-import keras\n-from keras import layers\n-\n-import numpy as np\n-import random\n-import io\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-path = keras.utils.get_file(\n-    \"nietzsche.txt\",\n-    origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\",\n-)\n-with io.open(path, encoding=\"utf-8\") as f:\n-    text = f.read().lower()\n-text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n-print(\"Corpus length:\", len(text))\n-\n-chars = sorted(list(set(text)))\n-print(\"Total chars:\", len(chars))\n-char_indices = dict((c, i) for i, c in enumerate(chars))\n-indices_char = dict((i, c) for i, c in enumerate(chars))\n-\n-# cut the text in semi-redundant sequences of maxlen characters\n-maxlen = 40\n-step = 3\n-sentences = []\n-next_chars = []\n-for i in range(0, len(text) - maxlen, step):\n-    sentences.append(text[i : i + maxlen])\n-    next_chars.append(text[i + maxlen])\n-print(\"Number of sequences:\", len(sentences))\n-\n-x = np.zeros((len(sentences), maxlen, len(chars)), dtype=\"bool\")\n-y = np.zeros((len(sentences), len(chars)), dtype=\"bool\")\n-for i, sentence in enumerate(sentences):\n-    for t, char in enumerate(sentence):\n-        x[i, t, char_indices[char]] = 1\n-    y[i, char_indices[next_chars[i]]] = 1\n-\n-\n-\"\"\"\n-## Build the model: a single LSTM layer\n-\"\"\"\n-\n-model = keras.Sequential(\n-    [\n-        keras.Input(shape=(maxlen, len(chars))),\n-        layers.LSTM(128),\n-        layers.Dense(len(chars), activation=\"softmax\"),\n-    ]\n-)\n-optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n-model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n-\n-\"\"\"\n-## Prepare the text sampling function\n-\"\"\"\n-\n-\n-def sample(preds, temperature=1.0):\n-    # helper function to sample an index from a probability array\n-    preds = np.asarray(preds).astype(\"float64\")\n-    preds = np.log(preds) / temperature\n-    exp_preds = np.exp(preds)\n-    preds = exp_preds / np.sum(exp_preds)\n-    probas = np.random.multinomial(1, preds, 1)\n-    return np.argmax(probas)\n-\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 40\n-batch_size = 128\n-\n-for epoch in range(epochs):\n-    model.fit(x, y, batch_size=batch_size, epochs=1)\n-    print()\n-    print(\"Generating text after epoch: %d\" % epoch)\n-\n-    start_index = random.randint(0, len(text) - maxlen - 1)\n-    for diversity in [0.2, 0.5, 1.0, 1.2]:\n-        print(\"...Diversity:\", diversity)\n-\n-        generated = \"\"\n-        sentence = text[start_index : start_index + maxlen]\n-        print('...Generating with seed: \"' + sentence + '\"')\n-\n-        for i in range(400):\n-            x_pred = np.zeros((1, maxlen, len(chars)))\n-            for t, char in enumerate(sentence):\n-                x_pred[0, t, char_indices[char]] = 1.0\n-            preds = model.predict(x_pred, verbose=0)[0]\n-            next_index = sample(preds, diversity)\n-            next_char = indices_char[next_index]\n-            sentence = sentence[1:] + next_char\n-            generated += next_char\n-\n-        print(\"...Generated: \", generated)\n-        print(\"-\")\n\n@@ -1,285 +0,0 @@\n-\"\"\"\n-Title: Neural style transfer\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2016/01/11\n-Last modified: 2020/05/02\n-Description: Transferring the style of a reference image to target image using gradient descent.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-Style transfer consists in generating an image\n-with the same \"content\" as a base image, but with the\n-\"style\" of a different picture (typically artistic).\n-This is achieved through the optimization of a loss function\n-that has 3 components: \"style loss\", \"content loss\",\n-and \"total variation loss\":\n-\n-- The total variation loss imposes local spatial continuity between\n-the pixels of the combination image, giving it visual coherence.\n-- The style loss is where the deep learning keeps in --that one is defined\n-using a deep convolutional neural network. Precisely, it consists in a sum of\n-L2 distances between the Gram matrices of the representations of\n-the base image and the style reference image, extracted from\n-different layers of a convnet (trained on ImageNet). The general idea\n-is to capture color/texture information at different spatial\n-scales (fairly large scales --defined by the depth of the layer considered).\n-- The content loss is a L2 distance between the features of the base\n-image (extracted from a deep layer) and the features of the combination image,\n-keeping the generated image close enough to the original one.\n-\n-**Reference:** [A Neural Algorithm of Artistic Style](\n-  http://arxiv.org/abs/1508.06576)\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras.applications import vgg19\n-\n-base_image_path = keras.utils.get_file(\n-    \"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\"\n-)\n-style_reference_image_path = keras.utils.get_file(\n-    \"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n-)\n-result_prefix = \"paris_generated\"\n-\n-# Weights of the different loss components\n-total_variation_weight = 1e-6\n-style_weight = 1e-6\n-content_weight = 2.5e-8\n-\n-# Dimensions of the generated picture.\n-width, height = keras.utils.load_img(base_image_path).size\n-img_nrows = 400\n-img_ncols = int(width * img_nrows / height)\n-\n-\"\"\"\n-## Let's take a look at our base (content) image and our style reference image\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-display(Image(base_image_path))\n-display(Image(style_reference_image_path))\n-\n-\"\"\"\n-## Image preprocessing / deprocessing utilities\n-\"\"\"\n-\n-\n-def preprocess_image(image_path):\n-    # Util function to open, resize and format pictures into appropriate tensors\n-    img = keras.utils.load_img(image_path, target_size=(img_nrows, img_ncols))\n-    img = keras.utils.img_to_array(img)\n-    img = np.expand_dims(img, axis=0)\n-    img = vgg19.preprocess_input(img)\n-    return tf.convert_to_tensor(img)\n-\n-\n-def deprocess_image(x):\n-    # Util function to convert a tensor into a valid image\n-    x = x.reshape((img_nrows, img_ncols, 3))\n-    # Remove zero-center by mean pixel\n-    x[:, :, 0] += 103.939\n-    x[:, :, 1] += 116.779\n-    x[:, :, 2] += 123.68\n-    # 'BGR'->'RGB'\n-    x = x[:, :, ::-1]\n-    x = np.clip(x, 0, 255).astype(\"uint8\")\n-    return x\n-\n-\n-\"\"\"\n-## Compute the style transfer loss\n-\n-First, we need to define 4 utility functions:\n-\n-- `gram_matrix` (used to compute the style loss)\n-- The `style_loss` function, which keeps the generated image close to the local textures\n-of the style reference image\n-- The `content_loss` function, which keeps the high-level representation of the\n-generated image close to that of the base image\n-- The `total_variation_loss` function, a regularization loss which keeps the generated\n-image locally-coherent\n-\"\"\"\n-\n-# The gram matrix of an image tensor (feature-wise outer product)\n-\n-\n-def gram_matrix(x):\n-    x = tf.transpose(x, (2, 0, 1))\n-    features = tf.reshape(x, (tf.shape(x)[0], -1))\n-    gram = tf.matmul(features, tf.transpose(features))\n-    return gram\n-\n-\n-# The \"style loss\" is designed to maintain\n-# the style of the reference image in the generated image.\n-# It is based on the gram matrices (which capture style) of\n-# feature maps from the style reference image\n-# and from the generated image\n-\n-\n-def style_loss(style, combination):\n-    S = gram_matrix(style)\n-    C = gram_matrix(combination)\n-    channels = 3\n-    size = img_nrows * img_ncols\n-    return tf.reduce_sum(tf.square(S - C)) / (\n-        4.0 * (channels**2) * (size**2)\n-    )\n-\n-\n-# An auxiliary loss function\n-# designed to maintain the \"content\" of the\n-# base image in the generated image\n-\n-\n-def content_loss(base, combination):\n-    return tf.reduce_sum(tf.square(combination - base))\n-\n-\n-# The 3rd loss function, total variation loss,\n-# designed to keep the generated image locally coherent\n-\n-\n-def total_variation_loss(x):\n-    a = tf.square(\n-        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n-    )\n-    b = tf.square(\n-        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n-    )\n-    return tf.reduce_sum(tf.pow(a + b, 1.25))\n-\n-\n-\"\"\"\n-Next, let's create a feature extraction model that retrieves the intermediate activations\n-of VGG19 (as a dict, by name).\n-\"\"\"\n-\n-# Build a VGG19 model loaded with pre-trained ImageNet weights\n-model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n-\n-# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n-\n-# Set up a model that returns the activation values for every layer in\n-# VGG19 (as a dict).\n-feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n-\n-\"\"\"\n-Finally, here's the code that computes the style transfer loss.\n-\"\"\"\n-\n-# List of layers to use for the style loss.\n-style_layer_names = [\n-    \"block1_conv1\",\n-    \"block2_conv1\",\n-    \"block3_conv1\",\n-    \"block4_conv1\",\n-    \"block5_conv1\",\n-]\n-# The layer to use for the content loss.\n-content_layer_name = \"block5_conv2\"\n-\n-\n-def compute_loss(combination_image, base_image, style_reference_image):\n-    input_tensor = tf.concat(\n-        [base_image, style_reference_image, combination_image], axis=0\n-    )\n-    features = feature_extractor(input_tensor)\n-\n-    # Initialize the loss\n-    loss = tf.zeros(shape=())\n-\n-    # Add content loss\n-    layer_features = features[content_layer_name]\n-    base_image_features = layer_features[0, :, :, :]\n-    combination_features = layer_features[2, :, :, :]\n-    loss = loss + content_weight * content_loss(\n-        base_image_features, combination_features\n-    )\n-    # Add style loss\n-    for layer_name in style_layer_names:\n-        layer_features = features[layer_name]\n-        style_reference_features = layer_features[1, :, :, :]\n-        combination_features = layer_features[2, :, :, :]\n-        sl = style_loss(style_reference_features, combination_features)\n-        loss += (style_weight / len(style_layer_names)) * sl\n-\n-    # Add total variation loss\n-    loss += total_variation_weight * total_variation_loss(combination_image)\n-    return loss\n-\n-\n-\"\"\"\n-## Add a tf.function decorator to loss & gradient computation\n-\n-To compile it, and thus make it fast.\n-\"\"\"\n-\n-\n-@tf.function\n-def compute_loss_and_grads(\n-    combination_image, base_image, style_reference_image\n-):\n-    with tf.GradientTape() as tape:\n-        loss = compute_loss(\n-            combination_image, base_image, style_reference_image\n-        )\n-    grads = tape.gradient(loss, combination_image)\n-    return loss, grads\n-\n-\n-\"\"\"\n-## The training loop\n-\n-Repeatedly run vanilla gradient descent steps to minimize the loss, and save the\n-resulting image every 100 iterations.\n-\n-We decay the learning rate by 0.96 every 100 steps.\n-\"\"\"\n-\n-optimizer = keras.optimizers.SGD(\n-    keras.optimizers.schedules.ExponentialDecay(\n-        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n-    )\n-)\n-\n-base_image = preprocess_image(base_image_path)\n-style_reference_image = preprocess_image(style_reference_image_path)\n-combination_image = tf.Variable(preprocess_image(base_image_path))\n-\n-iterations = 4000\n-for i in range(1, iterations + 1):\n-    loss, grads = compute_loss_and_grads(\n-        combination_image, base_image, style_reference_image\n-    )\n-    optimizer.apply_gradients([(grads, combination_image)])\n-    if i % 100 == 0:\n-        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n-        img = deprocess_image(combination_image.numpy())\n-        fname = result_prefix + \"_at_iteration_%d.png\" % i\n-        keras.utils.save_img(fname, img)\n-\n-\"\"\"\n-After 4000 iterations, you get the following result:\n-\"\"\"\n-\n-display(Image(result_prefix + \"_at_iteration_4000.png\"))\n-\n-\"\"\"\n-**Example available on HuggingFace**\n-Trained Model | Demo \n---- | --- \n-[![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Neural%20style%20transfer-black.svg)](https://huggingface.co/keras-io/VGG19) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Neural%20style%20transfer-black.svg)](https://huggingface.co/spaces/keras-io/neural-style-transfer)\n-\"\"\"\n\n@@ -1,201 +0,0 @@\n-\"\"\"\n-Title: Variational AutoEncoder\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/05/03\n-Last modified: 2023/11/22\n-Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Create a sampling layer\n-\"\"\"\n-\n-\n-class Sampling(layers.Layer):\n-    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n-\n-    def call(self, inputs):\n-        z_mean, z_log_var = inputs\n-        batch = tf.shape(z_mean)[0]\n-        dim = tf.shape(z_mean)[1]\n-        epsilon = tf.random.normal(shape=(batch, dim))\n-        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n-\n-\n-\"\"\"\n-## Build the encoder\n-\"\"\"\n-\n-latent_dim = 2\n-\n-encoder_inputs = keras.Input(shape=(28, 28, 1))\n-x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n-    encoder_inputs\n-)\n-x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n-x = layers.Flatten()(x)\n-x = layers.Dense(16, activation=\"relu\")(x)\n-z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n-z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n-z = Sampling()([z_mean, z_log_var])\n-encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n-encoder.summary()\n-\n-\"\"\"\n-## Build the decoder\n-\"\"\"\n-\n-latent_inputs = keras.Input(shape=(latent_dim,))\n-x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n-x = layers.Reshape((7, 7, 64))(x)\n-x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n-    x\n-)\n-x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n-    x\n-)\n-decoder_outputs = layers.Conv2DTranspose(\n-    1, 3, activation=\"sigmoid\", padding=\"same\"\n-)(x)\n-decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n-decoder.summary()\n-\n-\"\"\"\n-## Define the VAE as a `Model` with a custom `train_step`\n-\"\"\"\n-\n-\n-class VAE(keras.Model):\n-    def __init__(self, encoder, decoder, **kwargs):\n-        super().__init__(**kwargs)\n-        self.encoder = encoder\n-        self.decoder = decoder\n-        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n-        self.reconstruction_loss_tracker = keras.metrics.Mean(\n-            name=\"reconstruction_loss\"\n-        )\n-        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.total_loss_tracker,\n-            self.reconstruction_loss_tracker,\n-            self.kl_loss_tracker,\n-        ]\n-\n-    def train_step(self, data):\n-        with tf.GradientTape() as tape:\n-            z_mean, z_log_var, z = self.encoder(data)\n-            reconstruction = self.decoder(z)\n-            reconstruction_loss = tf.reduce_mean(\n-                tf.reduce_sum(\n-                    keras.losses.binary_crossentropy(data, reconstruction),\n-                    axis=(1, 2),\n-                )\n-            )\n-            kl_loss = -0.5 * (\n-                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n-            )\n-            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n-            total_loss = reconstruction_loss + kl_loss\n-        grads = tape.gradient(total_loss, self.trainable_weights)\n-        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n-        self.total_loss_tracker.update_state(total_loss)\n-        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n-        self.kl_loss_tracker.update_state(kl_loss)\n-        return {\n-            \"loss\": self.total_loss_tracker.result(),\n-            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n-            \"kl_loss\": self.kl_loss_tracker.result(),\n-        }\n-\n-\n-\"\"\"\n-## Train the VAE\n-\"\"\"\n-\n-(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n-mnist_digits = np.concatenate([x_train, x_test], axis=0)\n-mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n-\n-vae = VAE(encoder, decoder)\n-vae.compile(optimizer=keras.optimizers.Adam())\n-vae.fit(mnist_digits, epochs=30, batch_size=128)\n-\n-\"\"\"\n-## Display a grid of sampled digits\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-\n-\n-def plot_latent_space(vae, n=30, figsize=15):\n-    # display a n*n 2D manifold of digits\n-    digit_size = 28\n-    scale = 1.0\n-    figure = np.zeros((digit_size * n, digit_size * n))\n-    # linearly spaced coordinates corresponding to the 2D plot\n-    # of digit classes in the latent space\n-    grid_x = np.linspace(-scale, scale, n)\n-    grid_y = np.linspace(-scale, scale, n)[::-1]\n-\n-    for i, yi in enumerate(grid_y):\n-        for j, xi in enumerate(grid_x):\n-            z_sample = np.array([[xi, yi]])\n-            x_decoded = vae.decoder.predict(z_sample, verbose=0)\n-            digit = x_decoded[0].reshape(digit_size, digit_size)\n-            figure[\n-                i * digit_size : (i + 1) * digit_size,\n-                j * digit_size : (j + 1) * digit_size,\n-            ] = digit\n-\n-    plt.figure(figsize=(figsize, figsize))\n-    start_range = digit_size // 2\n-    end_range = n * digit_size + start_range\n-    pixel_range = np.arange(start_range, end_range, digit_size)\n-    sample_range_x = np.round(grid_x, 1)\n-    sample_range_y = np.round(grid_y, 1)\n-    plt.xticks(pixel_range, sample_range_x)\n-    plt.yticks(pixel_range, sample_range_y)\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.imshow(figure, cmap=\"Greys_r\")\n-    plt.show()\n-\n-\n-plot_latent_space(vae)\n-\n-\"\"\"\n-## Display how the latent space clusters different digit classes\n-\"\"\"\n-\n-\n-def plot_label_clusters(vae, data, labels):\n-    # display a 2D plot of the digit classes in the latent space\n-    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n-    plt.figure(figsize=(12, 10))\n-    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n-    plt.colorbar()\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.show()\n-\n-\n-(x_train, y_train), _ = keras.datasets.mnist.load_data()\n-x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n-\n-plot_label_clusters(vae, x_train, y_train)\n\n@@ -1,476 +0,0 @@\n-\"\"\"\n-Title: WGAN-GP overriding `Model.train_step`\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2020/05/9\n-Last modified: 2020/05/9\n-Description: Implementation of Wasserstein GAN with Gradient Penalty.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Wasserstein GAN (WGAN) with Gradient Penalty (GP)\n-\n-The original [Wasserstein GAN](https://arxiv.org/abs/1701.07875) leverages the\n-Wasserstein distance to produce a value function that has better theoretical\n-properties than the value function used in the original GAN paper. WGAN requires\n-that the discriminator (aka the critic) lie within the space of 1-Lipschitz\n-functions. The authors proposed the idea of weight clipping to achieve this\n-constraint. Though weight clipping works, it can be a problematic way to enforce\n-1-Lipschitz constraint and can cause undesirable behavior, e.g. a very deep WGAN\n-discriminator (critic) often fails to converge.\n-\n-The [WGAN-GP](https://arxiv.org/abs/1704.00028) method proposes an\n-alternative to weight clipping to ensure smooth training. Instead of clipping\n-the weights, the authors proposed a \"gradient penalty\" by adding a loss term\n-that keeps the L2 norm of the discriminator gradients close to 1.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\n-\"\"\"\n-## Prepare the Fashion-MNIST data\n-\n-To demonstrate how to train WGAN-GP, we will be using the\n-[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. Each\n-sample in this dataset is a 28x28 grayscale image associated with a label from\n-10 classes (e.g. trouser, pullover, sneaker, etc.)\n-\"\"\"\n-\n-IMG_SHAPE = (28, 28, 1)\n-BATCH_SIZE = 512\n-\n-# Size of the noise vector\n-noise_dim = 128\n-\n-fashion_mnist = keras.datasets.fashion_mnist\n-(train_images, train_labels), (\n-    test_images,\n-    test_labels,\n-) = fashion_mnist.load_data()\n-print(f\"Number of examples: {len(train_images)}\")\n-print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n-\n-# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n-train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\n-    \"float32\"\n-)\n-train_images = (train_images - 127.5) / 127.5\n-\n-\"\"\"\n-## Create the discriminator (the critic in the original WGAN)\n-\n-The samples in the dataset have a (28, 28, 1) shape. Because we will be\n-using strided convolutions, this can result in a shape with odd dimensions.\n-For example,\n-`(28, 28) -> Conv_s2 -> (14, 14) -> Conv_s2 -> (7, 7) -> Conv_s2 ->(3, 3)`.\n-\n-While peforming upsampling in the generator part of the network, we won't get\n-the same input shape as the original images if we aren't careful. To avoid this,\n-we will do something much simpler:\n-- In the discriminator: \"zero pad\" the input to change the shape to `(32, 32, 1)`\n-for each sample; and\n-- Ihe generator: crop the final output to match the shape with input shape.\n-\"\"\"\n-\n-\n-def conv_block(\n-    x,\n-    filters,\n-    activation,\n-    kernel_size=(3, 3),\n-    strides=(1, 1),\n-    padding=\"same\",\n-    use_bias=True,\n-    use_bn=False,\n-    use_dropout=False,\n-    drop_value=0.5,\n-):\n-    x = layers.Conv2D(\n-        filters,\n-        kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        use_bias=use_bias,\n-    )(x)\n-    if use_bn:\n-        x = layers.BatchNormalization()(x)\n-    x = activation(x)\n-    if use_dropout:\n-        x = layers.Dropout(drop_value)(x)\n-    return x\n-\n-\n-def get_discriminator_model():\n-    img_input = layers.Input(shape=IMG_SHAPE)\n-    # Zero pad the input to make the input images size to (32, 32, 1).\n-    x = layers.ZeroPadding2D((2, 2))(img_input)\n-    x = conv_block(\n-        x,\n-        64,\n-        kernel_size=(5, 5),\n-        strides=(2, 2),\n-        use_bn=False,\n-        use_bias=True,\n-        activation=layers.LeakyReLU(0.2),\n-        use_dropout=False,\n-        drop_value=0.3,\n-    )\n-    x = conv_block(\n-        x,\n-        128,\n-        kernel_size=(5, 5),\n-        strides=(2, 2),\n-        use_bn=False,\n-        activation=layers.LeakyReLU(0.2),\n-        use_bias=True,\n-        use_dropout=True,\n-        drop_value=0.3,\n-    )\n-    x = conv_block(\n-        x,\n-        256,\n-        kernel_size=(5, 5),\n-        strides=(2, 2),\n-        use_bn=False,\n-        activation=layers.LeakyReLU(0.2),\n-        use_bias=True,\n-        use_dropout=True,\n-        drop_value=0.3,\n-    )\n-    x = conv_block(\n-        x,\n-        512,\n-        kernel_size=(5, 5),\n-        strides=(2, 2),\n-        use_bn=False,\n-        activation=layers.LeakyReLU(0.2),\n-        use_bias=True,\n-        use_dropout=False,\n-        drop_value=0.3,\n-    )\n-\n-    x = layers.Flatten()(x)\n-    x = layers.Dropout(0.2)(x)\n-    x = layers.Dense(1)(x)\n-\n-    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n-    return d_model\n-\n-\n-d_model = get_discriminator_model()\n-d_model.summary()\n-\n-\"\"\"\n-## Create the generator\n-\"\"\"\n-\n-\n-def upsample_block(\n-    x,\n-    filters,\n-    activation,\n-    kernel_size=(3, 3),\n-    strides=(1, 1),\n-    up_size=(2, 2),\n-    padding=\"same\",\n-    use_bn=False,\n-    use_bias=True,\n-    use_dropout=False,\n-    drop_value=0.3,\n-):\n-    x = layers.UpSampling2D(up_size)(x)\n-    x = layers.Conv2D(\n-        filters,\n-        kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        use_bias=use_bias,\n-    )(x)\n-\n-    if use_bn:\n-        x = layers.BatchNormalization()(x)\n-\n-    if activation:\n-        x = activation(x)\n-    if use_dropout:\n-        x = layers.Dropout(drop_value)(x)\n-    return x\n-\n-\n-def get_generator_model():\n-    noise = layers.Input(shape=(noise_dim,))\n-    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.LeakyReLU(0.2)(x)\n-\n-    x = layers.Reshape((4, 4, 256))(x)\n-    x = upsample_block(\n-        x,\n-        128,\n-        layers.LeakyReLU(0.2),\n-        strides=(1, 1),\n-        use_bias=False,\n-        use_bn=True,\n-        padding=\"same\",\n-        use_dropout=False,\n-    )\n-    x = upsample_block(\n-        x,\n-        64,\n-        layers.LeakyReLU(0.2),\n-        strides=(1, 1),\n-        use_bias=False,\n-        use_bn=True,\n-        padding=\"same\",\n-        use_dropout=False,\n-    )\n-    x = upsample_block(\n-        x,\n-        1,\n-        layers.Activation(\"tanh\"),\n-        strides=(1, 1),\n-        use_bias=False,\n-        use_bn=True,\n-    )\n-    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n-    # We will use a Cropping2D layer to make it (28, 28, 1).\n-    x = layers.Cropping2D((2, 2))(x)\n-\n-    g_model = keras.models.Model(noise, x, name=\"generator\")\n-    return g_model\n-\n-\n-g_model = get_generator_model()\n-g_model.summary()\n-\n-\"\"\"\n-## Create the WGAN-GP model\n-\n-Now that we have defined our generator and discriminator, it's time to implement\n-the WGAN-GP model. We will also override the `train_step` for training.\n-\"\"\"\n-\n-\n-class WGAN(keras.Model):\n-    def __init__(\n-        self,\n-        discriminator,\n-        generator,\n-        latent_dim,\n-        discriminator_extra_steps=3,\n-        gp_weight=10.0,\n-    ):\n-        super().__init__()\n-        self.discriminator = discriminator\n-        self.generator = generator\n-        self.latent_dim = latent_dim\n-        self.d_steps = discriminator_extra_steps\n-        self.gp_weight = gp_weight\n-\n-    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n-        super().compile()\n-        self.d_optimizer = d_optimizer\n-        self.g_optimizer = g_optimizer\n-        self.d_loss_fn = d_loss_fn\n-        self.g_loss_fn = g_loss_fn\n-\n-    def gradient_penalty(self, batch_size, real_images, fake_images):\n-        \"\"\"Calculates the gradient penalty.\n-\n-        This loss is calculated on an interpolated image\n-        and added to the discriminator loss.\n-        \"\"\"\n-        # Get the interpolated image\n-        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n-        diff = fake_images - real_images\n-        interpolated = real_images + alpha * diff\n-\n-        with tf.GradientTape() as gp_tape:\n-            gp_tape.watch(interpolated)\n-            # 1. Get the discriminator output for this interpolated image.\n-            pred = self.discriminator(interpolated, training=True)\n-\n-        # 2. Calculate the gradients w.r.t to this interpolated image.\n-        grads = gp_tape.gradient(pred, [interpolated])[0]\n-        # 3. Calculate the norm of the gradients.\n-        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n-        gp = tf.reduce_mean((norm - 1.0) ** 2)\n-        return gp\n-\n-    def train_step(self, real_images):\n-        if isinstance(real_images, tuple):\n-            real_images = real_images[0]\n-\n-        # Get the batch size\n-        batch_size = tf.shape(real_images)[0]\n-\n-        # For each batch, we are going to perform the\n-        # following steps as laid out in the original paper:\n-        # 1. Train the generator and get the generator loss\n-        # 2. Train the discriminator and get the discriminator loss\n-        # 3. Calculate the gradient penalty\n-        # 4. Multiply this gradient penalty with a constant weight factor\n-        # 5. Add the gradient penalty to the discriminator loss\n-        # 6. Return the generator and discriminator losses as a loss dictionary\n-\n-        # Train the discriminator first. The original paper recommends training\n-        # the discriminator for `x` more steps (typically 5) as compared to\n-        # one step of the generator. Here we will train it for 3 extra steps\n-        # as compared to 5 to reduce the training time.\n-        for i in range(self.d_steps):\n-            # Get the latent vector\n-            random_latent_vectors = tf.random.normal(\n-                shape=(batch_size, self.latent_dim)\n-            )\n-            with tf.GradientTape() as tape:\n-                # Generate fake images from the latent vector\n-                fake_images = self.generator(\n-                    random_latent_vectors, training=True\n-                )\n-                # Get the logits for the fake images\n-                fake_logits = self.discriminator(fake_images, training=True)\n-                # Get the logits for the real images\n-                real_logits = self.discriminator(real_images, training=True)\n-\n-                # Calculate the discriminator loss using the fake and real image logits\n-                d_cost = self.d_loss_fn(\n-                    real_img=real_logits, fake_img=fake_logits\n-                )\n-                # Calculate the gradient penalty\n-                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n-                # Add the gradient penalty to the original discriminator loss\n-                d_loss = d_cost + gp * self.gp_weight\n-\n-            # Get the gradients w.r.t the discriminator loss\n-            d_gradient = tape.gradient(\n-                d_loss, self.discriminator.trainable_variables\n-            )\n-            # Update the weights of the discriminator using the discriminator optimizer\n-            self.d_optimizer.apply_gradients(\n-                zip(d_gradient, self.discriminator.trainable_variables)\n-            )\n-\n-        # Train the generator\n-        # Get the latent vector\n-        random_latent_vectors = tf.random.normal(\n-            shape=(batch_size, self.latent_dim)\n-        )\n-        with tf.GradientTape() as tape:\n-            # Generate fake images using the generator\n-            generated_images = self.generator(\n-                random_latent_vectors, training=True\n-            )\n-            # Get the discriminator logits for fake images\n-            gen_img_logits = self.discriminator(generated_images, training=True)\n-            # Calculate the generator loss\n-            g_loss = self.g_loss_fn(gen_img_logits)\n-\n-        # Get the gradients w.r.t the generator loss\n-        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n-        # Update the weights of the generator using the generator optimizer\n-        self.g_optimizer.apply_gradients(\n-            zip(gen_gradient, self.generator.trainable_variables)\n-        )\n-        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n-\n-\n-\"\"\"\n-## Create a Keras callback that periodically saves generated images\n-\"\"\"\n-\n-\n-class GANMonitor(keras.callbacks.Callback):\n-    def __init__(self, num_img=6, latent_dim=128):\n-        self.num_img = num_img\n-        self.latent_dim = latent_dim\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        random_latent_vectors = tf.random.normal(\n-            shape=(self.num_img, self.latent_dim)\n-        )\n-        generated_images = self.model.generator(random_latent_vectors)\n-        generated_images = (generated_images * 127.5) + 127.5\n-\n-        for i in range(self.num_img):\n-            img = generated_images[i].numpy()\n-            img = keras.utils.array_to_img(img)\n-            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n-\n-\n-\"\"\"\n-## Train the end-to-end model\n-\"\"\"\n-\n-# Instantiate the optimizer for both networks\n-# (learning_rate=0.0002, beta_1=0.5 are recommended)\n-generator_optimizer = keras.optimizers.Adam(\n-    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n-)\n-discriminator_optimizer = keras.optimizers.Adam(\n-    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n-)\n-\n-\n-# Define the loss functions for the discriminator,\n-# which should be (fake_loss - real_loss).\n-# We will add the gradient penalty later to this loss function.\n-def discriminator_loss(real_img, fake_img):\n-    real_loss = tf.reduce_mean(real_img)\n-    fake_loss = tf.reduce_mean(fake_img)\n-    return fake_loss - real_loss\n-\n-\n-# Define the loss functions for the generator.\n-def generator_loss(fake_img):\n-    return -tf.reduce_mean(fake_img)\n-\n-\n-# Set the number of epochs for trainining.\n-epochs = 20\n-\n-# Instantiate the customer `GANMonitor` Keras callback.\n-cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n-\n-# Get the wgan model\n-wgan = WGAN(\n-    discriminator=d_model,\n-    generator=g_model,\n-    latent_dim=noise_dim,\n-    discriminator_extra_steps=3,\n-)\n-\n-# Compile the wgan model\n-wgan.compile(\n-    d_optimizer=discriminator_optimizer,\n-    g_optimizer=generator_optimizer,\n-    g_loss_fn=generator_loss,\n-    d_loss_fn=discriminator_loss,\n-)\n-\n-# Start training\n-wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])\n-\n-\"\"\"\n-Display the last generated images:\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-display(Image(\"generated_img_0_19.png\"))\n-display(Image(\"generated_img_1_19.png\"))\n-display(Image(\"generated_img_2_19.png\"))\n-\n-\"\"\"\n-Example available on HuggingFace.\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/🤗%20Model-WGAN%20GP-black.svg)](https://huggingface.co/keras-io/WGAN-GP) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-WGAN%20GP-black.svg)](https://huggingface.co/spaces/keras-io/WGAN-GP) |\n-\"\"\"\n\n@@ -1,114 +0,0 @@\n-\"\"\"\n-Title: Simple custom layer example: Antirectifier\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2016/01/06\n-Last modified: 2020/04/20\n-Description: Demonstration of custom layer creation.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to create custom layers, using the Antirectifier layer\n- (originally proposed as a Keras example script in January 2016), an alternative\n-to ReLU. Instead of zeroing-out the negative part of the input, it splits the negative\n- and positive parts and returns the concatenation of the absolute value\n-of both. This avoids loss of information, at the cost of an increase in dimensionality.\n- To fix the dimensionality increase, we linearly combine the\n-features back to a space of the original size.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## The Antirectifier layer\n-\"\"\"\n-\n-\n-class Antirectifier(layers.Layer):\n-    def __init__(self, initializer=\"he_normal\", **kwargs):\n-        super().__init__(**kwargs)\n-        self.initializer = keras.initializers.get(initializer)\n-\n-    def build(self, input_shape):\n-        output_dim = input_shape[-1]\n-        self.kernel = self.add_weight(\n-            shape=(output_dim * 2, output_dim),\n-            initializer=self.initializer,\n-            name=\"kernel\",\n-            trainable=True,\n-        )\n-\n-    def call(self, inputs):\n-        inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)\n-        pos = tf.nn.relu(inputs)\n-        neg = tf.nn.relu(-inputs)\n-        concatenated = tf.concat([pos, neg], axis=-1)\n-        mixed = tf.matmul(concatenated, self.kernel)\n-        return mixed\n-\n-    def get_config(self):\n-        # Implement get_config to enable serialization. This is optional.\n-        base_config = super().get_config()\n-        config = {\"initializer\": keras.initializers.serialize(self.initializer)}\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-\"\"\"\n-## Let's test-drive it on MNIST\n-\"\"\"\n-\n-# Training parameters\n-batch_size = 128\n-num_classes = 10\n-epochs = 20\n-\n-# The data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n-\n-x_train = x_train.reshape(-1, 784)\n-x_test = x_test.reshape(-1, 784)\n-x_train = x_train.astype(\"float32\")\n-x_test = x_test.astype(\"float32\")\n-x_train /= 255\n-x_test /= 255\n-print(x_train.shape[0], \"train samples\")\n-print(x_test.shape[0], \"test samples\")\n-\n-# Build the model\n-model = keras.Sequential(\n-    [\n-        keras.Input(shape=(784,)),\n-        layers.Dense(256),\n-        Antirectifier(),\n-        layers.Dense(256),\n-        Antirectifier(),\n-        layers.Dropout(0.5),\n-        layers.Dense(10),\n-    ]\n-)\n-\n-# Compile the model\n-model.compile(\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    optimizer=keras.optimizers.RMSprop(),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-)\n-\n-# Train the model\n-model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=epochs,\n-    validation_split=0.15,\n-)\n-\n-# Test the model\n-model.evaluate(x_test, y_test)\n\n@@ -1,116 +0,0 @@\n-\"\"\"\n-Title: Endpoint layer pattern\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2019/05/10\n-Last modified: 2023/11/22\n-Description: Demonstration of the \"endpoint layer\" pattern (layer that handles loss management).\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import keras\n-import numpy as np\n-\n-\"\"\"\n-## Usage of endpoint layers in the Functional API\n-\n-An \"endpoint layer\" has access to the model's targets, and creates arbitrary losses\n-in `call()` using `self.add_loss()` and `Metric.update_state()`.\n-This enables you to define losses and\n-metrics that don't match the usual signature `fn(y_true, y_pred, sample_weight=None)`.\n-\n-Note that you could have separate metrics for training and eval with this pattern.\n-\"\"\"\n-\n-\n-class LogisticEndpoint(keras.layers.Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n-        self.accuracy_metric = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n-\n-    def call(self, logits, targets=None, sample_weight=None):\n-        if targets is not None:\n-            # Compute the training-time loss value and add it\n-            # to the layer using `self.add_loss()`.\n-            loss = self.loss_fn(targets, logits, sample_weight)\n-            self.add_loss(loss)\n-\n-            # Log the accuracy as a metric (we could log arbitrary metrics,\n-            # including different metrics for training and inference.)\n-            self.accuracy_metric.update_state(targets, logits, sample_weight)\n-\n-        # Return the inference-time prediction tensor (for `.predict()`).\n-        return tf.nn.softmax(logits)\n-\n-\n-inputs = keras.Input((764,), name=\"inputs\")\n-logits = keras.layers.Dense(1)(inputs)\n-targets = keras.Input((1,), name=\"targets\")\n-sample_weight = keras.Input((1,), name=\"sample_weight\")\n-preds = LogisticEndpoint()(logits, targets, sample_weight)\n-model = keras.Model([inputs, targets, sample_weight], preds)\n-\n-data = {\n-    \"inputs\": np.random.random((1000, 764)),\n-    \"targets\": np.random.random((1000, 1)),\n-    \"sample_weight\": np.random.random((1000, 1)),\n-}\n-\n-model.compile(keras.optimizers.Adam(1e-3))\n-model.fit(data, epochs=2)\n-\n-\"\"\"\n-## Exporting an inference-only model\n-\n-Simply don't include `targets` in the model. The weights stay the same.\n-\"\"\"\n-\n-inputs = keras.Input((764,), name=\"inputs\")\n-logits = keras.layers.Dense(1)(inputs)\n-preds = LogisticEndpoint()(logits, targets=None, sample_weight=None)\n-inference_model = keras.Model(inputs, preds)\n-\n-inference_model.set_weights(model.get_weights())\n-\n-preds = inference_model.predict(np.random.random((1000, 764)))\n-\n-\"\"\"\n-## Usage of loss endpoint layers in subclassed models\n-\"\"\"\n-\n-\n-class LogReg(keras.Model):\n-    def __init__(self):\n-        super().__init__()\n-        self.dense = keras.layers.Dense(1)\n-        self.logistic_endpoint = LogisticEndpoint()\n-\n-    def call(self, inputs):\n-        # Note that all inputs should be in the first argument\n-        # since we want to be able to call `model.fit(inputs)`.\n-        logits = self.dense(inputs[\"inputs\"])\n-        preds = self.logistic_endpoint(\n-            logits=logits,\n-            targets=inputs[\"targets\"],\n-            sample_weight=inputs[\"sample_weight\"],\n-        )\n-        return preds\n-\n-\n-model = LogReg()\n-data = {\n-    \"inputs\": np.random.random((1000, 764)),\n-    \"targets\": np.random.random((1000, 1)),\n-    \"sample_weight\": np.random.random((1000, 1)),\n-}\n-\n-model.compile(keras.optimizers.Adam(1e-3))\n-model.fit(data, epochs=2)\n\n@@ -1,129 +0,0 @@\n-\"\"\"\n-Title: Customizing the convolution operation of a Conv2D layer\n-Author: [lukewood](https://lukewood.xyz)\n-Date created: 11/03/2021\n-Last modified: 11/03/2021\n-Description: This example shows how to implement custom convolution layers using the `Conv.convolution_op()` API.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-You may sometimes need to implement custom versions of convolution layers like `Conv1D` and `Conv2D`.\n-Keras enables you do this without implementing the entire layer from scratch: you can reuse\n-most of the base convolution layer and just customize the convolution op itself via the\n-`convolution_op()` method.\n-\n-This method was introduced in Keras 2.7. So before using the\n-`convolution_op()` API, ensure that you are running Keras version 2.7.0 or greater.\n-\"\"\"\n-\n-\"\"\"\n-## A Simple `StandardizedConv2D` implementation\n-\n-There are two ways to use the `Conv.convolution_op()` API. The first way\n-is to override the `convolution_op()` method on a convolution layer subclass.\n-Using this approach, we can quickly implement a\n-[StandardizedConv2D](https://arxiv.org/abs/1903.10520) as shown below.\n-\"\"\"\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-import numpy as np\n-\n-\n-class StandardizedConv2DWithOverride(layers.Conv2D):\n-    def convolution_op(self, inputs, kernel):\n-        mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n-        return tf.nn.conv2d(\n-            inputs,\n-            (kernel - mean) / tf.sqrt(var + 1e-10),\n-            padding=\"valid\",\n-            strides=list(self.strides),\n-            name=self.__class__.__name__,\n-        )\n-\n-\n-\"\"\"\n-The other way to use the `Conv.convolution_op()` API is to directly call the\n-`convolution_op()` method from the `call()` method of a convolution layer subclass.\n-A comparable class implemented using this approach is shown below.\n-\"\"\"\n-\n-\n-class StandardizedConv2DWithCall(layers.Conv2D):\n-    def call(self, inputs):\n-        mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)\n-        result = self.convolution_op(\n-            inputs, (self.kernel - mean) / tf.sqrt(var + 1e-10)\n-        )\n-        if self.use_bias:\n-            result = result + self.bias\n-        return result\n-\n-\n-\"\"\"\n-## Example Usage\n-\n-Both of these layers work as drop-in replacements for `Conv2D`. The following\n-demonstration performs classification on the MNIST dataset.\n-\"\"\"\n-\n-# Model / data parameters\n-num_classes = 10\n-input_shape = (28, 28, 1)\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n-\n-# Scale images to the [0, 1] range\n-x_train = x_train.astype(\"float32\") / 255\n-x_test = x_test.astype(\"float32\") / 255\n-# Make sure images have shape (28, 28, 1)\n-x_train = np.expand_dims(x_train, -1)\n-x_test = np.expand_dims(x_test, -1)\n-print(\"x_train shape:\", x_train.shape)\n-print(x_train.shape[0], \"train samples\")\n-print(x_test.shape[0], \"test samples\")\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-model = keras.Sequential(\n-    [\n-        keras.layers.Input(shape=input_shape),\n-        StandardizedConv2DWithCall(32, kernel_size=(3, 3), activation=\"relu\"),\n-        layers.MaxPooling2D(pool_size=(2, 2)),\n-        StandardizedConv2DWithOverride(\n-            64, kernel_size=(3, 3), activation=\"relu\"\n-        ),\n-        layers.MaxPooling2D(pool_size=(2, 2)),\n-        layers.Flatten(),\n-        layers.Dropout(0.5),\n-        layers.Dense(num_classes, activation=\"softmax\"),\n-    ]\n-)\n-\n-model.summary()\n-\"\"\"\n-\n-\"\"\"\n-batch_size = 128\n-epochs = 5\n-\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-\n-model.fit(\n-    x_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.1\n-)\n-\n-\"\"\"\n-## Conclusion\n-\n-The `Conv.convolution_op()` API provides an easy and readable way to implement custom\n-convolution layers. A `StandardizedConvolution` implementation using the API is quite\n-terse, consisting of only four lines of code.\n-\"\"\"\n\n@@ -1,346 +0,0 @@\n-\"\"\"\n-Title: Writing Keras Models With TensorFlow NumPy\n-Author: [lukewood](https://lukewood.xyz)\n-Date created: 2021/08/28\n-Last modified: 2021/08/28\n-Description: Overview of how to use the TensorFlow NumPy API to write Keras models.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-[NumPy](https://numpy.org/) is a hugely successful Python linear algebra library.\n-\n-TensorFlow recently launched [tf_numpy](https://www.tensorflow.org/guide/tf_numpy), a\n-TensorFlow implementation of a large subset of the NumPy API.\n-Thanks to `tf_numpy`, you can write Keras layers or models in the NumPy style!\n-\n-The TensorFlow NumPy API has full integration with the TensorFlow ecosystem.\n-Features such as automatic differentiation, TensorBoard, Keras model callbacks,\n-TPU distribution and model exporting are all supported.\n-\n-Let's run through a few examples.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import tensorflow.experimental.numpy as tnp\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-To test our models we will use the Boston housing prices regression dataset.\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data(\n-    path=\"boston_housing.npz\", test_split=0.2, seed=113\n-)\n-input_dim = x_train.shape[1]\n-\n-\n-def evaluate_model(model: keras.Model):\n-    loss, percent_error = model.evaluate(x_test, y_test, verbose=0)\n-    print(\"Mean absolute percent error before training: \", percent_error)\n-    model.fit(x_train, y_train, epochs=200, verbose=0)\n-    loss, percent_error = model.evaluate(x_test, y_test, verbose=0)\n-    print(\"Mean absolute percent error after training:\", percent_error)\n-\n-\n-\"\"\"\n-## Subclassing keras.Model with TNP\n-\n-The most flexible way to make use of the Keras API is to subclass the\n-[`keras.Model`](https://keras.io/api/models/model/) class.  Subclassing the Model class\n-gives you the ability to fully customize what occurs in the training loop.  This makes\n-subclassing Model a popular option for researchers.\n-\n-In this example, we will implement a `Model` subclass that performs regression over the\n-boston housing dataset using the TNP API.  Note that differentiation and gradient\n-descent is handled automatically when using the TNP API alongside keras.\n-\n-First let's define a simple `TNPForwardFeedRegressionNetwork` class.\n-\"\"\"\n-\n-\n-class TNPForwardFeedRegressionNetwork(keras.Model):\n-    def __init__(self, blocks=None, **kwargs):\n-        super().__init__(**kwargs)\n-        if not isinstance(blocks, list):\n-            raise ValueError(f\"blocks must be a list, got blocks={blocks}\")\n-        self.blocks = blocks\n-        self.block_weights = None\n-        self.biases = None\n-\n-    def build(self, input_shape):\n-        current_shape = input_shape[1]\n-        self.block_weights = []\n-        self.biases = []\n-        for i, block in enumerate(self.blocks):\n-            self.block_weights.append(\n-                self.add_weight(\n-                    shape=(current_shape, block),\n-                    trainable=True,\n-                    name=f\"block-{i}\",\n-                    initializer=\"glorot_normal\",\n-                )\n-            )\n-            self.biases.append(\n-                self.add_weight(\n-                    shape=(block,),\n-                    trainable=True,\n-                    name=f\"bias-{i}\",\n-                    initializer=\"zeros\",\n-                )\n-            )\n-            current_shape = block\n-\n-        self.linear_layer = self.add_weight(\n-            shape=(current_shape, 1),\n-            name=\"linear_projector\",\n-            trainable=True,\n-            initializer=\"glorot_normal\",\n-        )\n-\n-    def call(self, inputs):\n-        activations = inputs\n-        for w, b in zip(self.block_weights, self.biases):\n-            activations = tnp.matmul(activations, w) + b\n-            # ReLu activation function\n-            activations = tnp.maximum(activations, 0.0)\n-\n-        return tnp.matmul(activations, self.linear_layer)\n-\n-\n-\"\"\"\n-Just like with any other Keras model we can utilize any supported optimizer, loss,\n-metrics or callbacks that we want.\n-\n-Let's see how the model performs!\n-\"\"\"\n-\n-model = TNPForwardFeedRegressionNetwork(blocks=[3, 3])\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=\"mean_squared_error\",\n-    metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-)\n-evaluate_model(model)\n-\n-\"\"\"\n-Great! Our model seems to be effectively learning to solve the problem at hand.\n-\n-We can also write our own custom loss function using TNP.\n-\"\"\"\n-\n-\n-def tnp_mse(y_true, y_pred):\n-    return tnp.mean(tnp.square(y_true - y_pred), axis=0)\n-\n-\n-keras.backend.clear_session()\n-model = TNPForwardFeedRegressionNetwork(blocks=[3, 3])\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=tnp_mse,\n-    metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-)\n-evaluate_model(model)\n-\n-\"\"\"\n-## Implementing a Keras Layer Based Model with TNP\n-\n-If desired, TNP can also be used in layer oriented Keras code structure.  Let's\n-implement the same model, but using a layered approach!\n-\"\"\"\n-\n-\n-def tnp_relu(x):\n-    return tnp.maximum(x, 0)\n-\n-\n-class TNPDense(keras.layers.Layer):\n-    def __init__(self, units, activation=None):\n-        super().__init__()\n-        self.units = units\n-        self.activation = activation\n-\n-    def build(self, input_shape):\n-        self.w = self.add_weight(\n-            name=\"weights\",\n-            shape=(input_shape[1], self.units),\n-            initializer=\"random_normal\",\n-            trainable=True,\n-        )\n-        self.bias = self.add_weight(\n-            name=\"bias\",\n-            shape=(self.units,),\n-            initializer=\"zeros\",\n-            trainable=True,\n-        )\n-\n-    def call(self, inputs):\n-        outputs = tnp.matmul(inputs, self.w) + self.bias\n-        if self.activation:\n-            return self.activation(outputs)\n-        return outputs\n-\n-\n-def create_layered_tnp_model():\n-    return keras.Sequential(\n-        [\n-            TNPDense(3, activation=tnp_relu),\n-            TNPDense(3, activation=tnp_relu),\n-            TNPDense(1),\n-        ]\n-    )\n-\n-\n-model = create_layered_tnp_model()\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=\"mean_squared_error\",\n-    metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-)\n-model.build((None, input_dim))\n-model.summary()\n-\n-evaluate_model(model)\n-\n-\"\"\"\n-You can also seamlessly switch between TNP layers and native Keras layers!\n-\"\"\"\n-\n-\n-def create_mixed_model():\n-    return keras.Sequential(\n-        [\n-            TNPDense(3, activation=tnp_relu),\n-            # The model will have no issue using a normal Dense layer\n-            layers.Dense(3, activation=\"relu\"),\n-            # ... or switching back to tnp layers!\n-            TNPDense(1),\n-        ]\n-    )\n-\n-\n-model = create_mixed_model()\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=\"mean_squared_error\",\n-    metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-)\n-model.build((None, input_dim))\n-model.summary()\n-\n-evaluate_model(model)\n-\n-\"\"\"\n-The Keras API offers a wide variety of layers.  The ability to use them alongside NumPy\n-code can be a huge time saver in projects.\n-\"\"\"\n-\n-\"\"\"\n-## Distribution Strategy\n-\n-TensorFlow NumPy and Keras integrate with\n-[TensorFlow Distribution Strategies](https://www.tensorflow.org/guide/distributed_training).\n-This makes it simple to perform distributed training across multiple GPUs,\n-or even an entire TPU Pod.\n-\"\"\"\n-\n-gpus = tf.config.list_logical_devices(\"GPU\")\n-if gpus:\n-    strategy = tf.distribute.MirroredStrategy(gpus)\n-else:\n-    # We can fallback to a no-op CPU strategy.\n-    strategy = tf.distribute.get_strategy()\n-print(\"Running with strategy:\", str(strategy.__class__.__name__))\n-\n-with strategy.scope():\n-    model = create_layered_tnp_model()\n-    model.compile(\n-        optimizer=\"adam\",\n-        loss=\"mean_squared_error\",\n-        metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-    )\n-    model.build((None, input_dim))\n-    model.summary()\n-    evaluate_model(model)\n-\n-\"\"\"\n-## TensorBoard Integration\n-\n-One of the many benefits of using the Keras API is the ability to monitor training\n-through TensorBoard.  Using the TensorFlow NumPy API alongside Keras allows you to easily\n-leverage TensorBoard.\n-\"\"\"\n-\n-keras.backend.clear_session()\n-\n-\"\"\"\n-To load the TensorBoard from a Jupyter notebook, you can run the following magic:\n-```\n-%load_ext tensorboard\n-```\n-\n-\"\"\"\n-\n-models = [\n-    (\n-        TNPForwardFeedRegressionNetwork(blocks=[3, 3]),\n-        \"TNPForwardFeedRegressionNetwork\",\n-    ),\n-    (create_layered_tnp_model(), \"layered_tnp_model\"),\n-    (create_mixed_model(), \"mixed_model\"),\n-]\n-for model, model_name in models:\n-    model.compile(\n-        optimizer=\"adam\",\n-        loss=\"mean_squared_error\",\n-        metrics=[keras.metrics.MeanAbsolutePercentageError()],\n-    )\n-    model.fit(\n-        x_train,\n-        y_train,\n-        epochs=200,\n-        verbose=0,\n-        callbacks=[keras.callbacks.TensorBoard(log_dir=f\"logs/{model_name}\")],\n-    )\n-\n-\"\"\"\n-To load the TensorBoard from a Jupyter notebook you can use the `%tensorboard` magic:\n-\n-```\n-%tensorboard --logdir logs\n-```\n-\n-The TensorBoard monitor metrics and examine the training curve.\n-\n-![Tensorboard training graph](https://i.imgur.com/wsOuFnz.png)\n-\n-The TensorBoard also allows you to explore the computation graph used in your models.\n-\n-![Tensorboard graph exploration](https://i.imgur.com/tOrezDL.png)\n-\n-The ability to introspect into your models can be valuable during debugging.\n-\"\"\"\n-\n-\"\"\"\n-## Conclusion\n-\n-Porting existing NumPy code to Keras models using the `tensorflow_numpy` API is easy!\n-By integrating with Keras you gain the ability to use existing Keras callbacks, metrics\n-and optimizers, easily distribute your training and use Tensorboard.\n-\n-Migrating a more complex model, such as a ResNet, to the TensorFlow NumPy API would be a\n-great follow up learning exercise.\n-\n-Several open source NumPy ResNet implementations are available online.\n-\"\"\"\n\n@@ -1,557 +0,0 @@\n-\"\"\"\n-Title: Serving TensorFlow models with TFServing\n-Author: [Dimitre Oliveira](https://www.linkedin.com/in/dimitre-oliveira-7a1a0113a/)\n-Date created: 2023/01/02\n-Last modified: 2023/01/02\n-Description: How to serve TensorFlow models with TensorFlow Serving.\n-Accelerator: None\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-Once you build a machine learning model, the next step is to serve it.\n-You may want to do that by exposing your model as an endpoint service.\n-There are many frameworks that you can use to do that, but the TensorFlow\n-ecosystem has its own solution called\n-[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n-\n-From the TensorFlow Serving\n-[GitHub page](https://github.com/tensorflow/serving):\n-\n-> TensorFlow Serving is a flexible, high-performance serving system for machine\n-learning models, designed for production environments. It deals with the\n-inference aspect of machine learning, taking models after training and\n-managing their lifetimes, providing clients with versioned access via a\n-high-performance, reference-counted lookup table. TensorFlow Serving provides\n-out-of-the-box integration with TensorFlow models, but can be easily extended\n-to serve other types of models and data.\"\n-\n-To note a few features:\n-\n-- It can serve multiple models, or multiple versions of the same model\n-simultaneously\n-- It exposes both gRPC as well as HTTP inference endpoints\n-- It allows deployment of new model versions without changing any client code\n-- It supports canarying new versions and A/B testing experimental models\n-- It adds minimal latency to inference time due to efficient, low-overhead\n-implementation\n-- It features a scheduler that groups individual inference requests into batches\n-for joint execution on GPU, with configurable latency controls\n-- It supports many servables: Tensorflow models, embeddings, vocabularies,\n-feature transformations and even non-Tensorflow-based machine learning models\n-\n-This guide creates a simple [MobileNet](https://arxiv.org/abs/1704.04861)\n-model using the [Keras applications API](https://keras.io/api/applications/),\n-and then serves it with [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n-The focus is on TensorFlow Serving, rather than the modeling and training in\n-TensorFlow.\n-\n-> Note: you can find a Colab notebook with the full working code at\n-[this link](https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing).\n-\"\"\"\n-\"\"\"\n-## Dependencies\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import json\n-import shutil\n-import requests\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Model\n-\n-Here we load a pre-trained [MobileNet](https://arxiv.org/abs/1704.04861)\n-from the [Keras applications](https://keras.io/api/applications/), this is the\n-model that we are going to serve.\n-\"\"\"\n-\n-model = keras.applications.MobileNet()\n-\n-\"\"\"\n-## Preprocessing\n-\n-Most models don't work out of the box on raw data, they usually require some\n-kind of preprocessing step to adjust the data to the model requirements,\n-in the case of this MobileNet we can see from its\n-[API page](https://keras.io/api/applications/mobilenet/) that it requires\n-three basic steps for its input images:\n-\n-- Pixel values normalized to the `[0, 1]` range\n-- Pixel values scaled to the `[-1, 1]` range\n-- Images with the shape of `(224, 224, 3)` meaning `(height, width, channels)`\n-\n-We can do all of that with the following function:\n-\"\"\"\n-\n-\n-def preprocess(image, mean=0.5, std=0.5, shape=(224, 224)):\n-    \"\"\"Scale, normalize and resizes images.\"\"\"\n-    image = image / 255.0  # Scale\n-    image = (image - mean) / std  # Normalize\n-    image = tf.image.resize(image, shape)  # Resize\n-    return image\n-\n-\n-\"\"\"\n-**A note regarding preprocessing and postprocessing using the \"keras.applications\" API**\n-\n-All models that are available at the [Keras applications](https://keras.io/api/applications/)\n-API also provide `preprocess_input` and `decode_predictions` functions, those\n-functions are respectively responsible for the preprocessing and postprocessing\n-of each model, and already contains all the logic necessary for those steps.\n-That is the recommended way to process inputs and outputs when using Keras\n-applications models.\n-For this guide, we are not using them to present the advantages of custom\n-signatures in a clearer way.\n-\"\"\"\n-\n-\n-\"\"\"\n-## Postprocessing\n-\n-In the same context most models output values that need extra processing to\n-meet the user requirements, for instance, the user does not want to know the\n-logits values for each class given an image, what the user wants is to know\n-from which class it belongs. For our model, this translates to the following\n-transformations on top of the model outputs:\n-\n-- Get the index of the class with the highest prediction\n-- Get the name of the class from that index\n-\"\"\"\n-\n-# Download human-readable labels for ImageNet.\n-imagenet_labels_url = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n-response = requests.get(imagenet_labels_url)\n-# Skiping backgroung class\n-labels = [x for x in response.text.split(\"\\n\") if x != \"\"][1:]\n-# Convert the labels to the TensorFlow data format\n-tf_labels = tf.constant(labels, dtype=tf.string)\n-\n-\n-def postprocess(prediction, labels=tf_labels):\n-    \"\"\"Convert from probs to labels.\"\"\"\n-    indices = tf.argmax(prediction, axis=-1)  # Index with highest prediction\n-    label = tf.gather(params=labels, indices=indices)  # Class name\n-    return label\n-\n-\n-\"\"\"\n-Now let's download a banana picture and see how everything comes together.\n-\"\"\"\n-\n-response = requests.get(\"https://i.imgur.com/j9xCCzn.jpeg\", stream=True)\n-\n-with open(\"banana.jpeg\", \"wb\") as f:\n-    shutil.copyfileobj(response.raw, f)\n-\n-sample_img = plt.imread(\"./banana.jpeg\")\n-print(f\"Original image shape: {sample_img.shape}\")\n-print(f\"Original image pixel range: ({sample_img.min()}, {sample_img.max()})\")\n-plt.imshow(sample_img)\n-plt.show()\n-\n-preprocess_img = preprocess(sample_img)\n-print(f\"Preprocessed image shape: {preprocess_img.shape}\")\n-print(\n-    f\"Preprocessed image pixel range: ({preprocess_img.numpy().min()},\",\n-    f\"{preprocess_img.numpy().max()})\",\n-)\n-\n-batched_img = tf.expand_dims(preprocess_img, axis=0)\n-batched_img = tf.cast(batched_img, tf.float32)\n-print(f\"Batched image shape: {batched_img.shape}\")\n-\n-model_outputs = model(batched_img)\n-print(f\"Model output shape: {model_outputs.shape}\")\n-print(f\"Predicted class: {postprocess(model_outputs)}\")\n-\n-\"\"\"\n-## Save the model\n-\n-To load our trained model into TensorFlow Serving, we first need to save it in\n-[SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model)\n-format. This will create a protobuf file in a well-defined directory hierarchy,\n-and will include a version number.\n-[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) allows us\n-to select which version of a model, or \"servable\" we want to use when we make\n-inference requests. Each version will be exported to a different sub-directory\n-under the given path.\n-\"\"\"\n-\n-model_dir = \"./model\"\n-model_version = 1\n-model_export_path = f\"{model_dir}/{model_version}\"\n-\n-tf.saved_model.save(\n-    model,\n-    export_dir=model_export_path,\n-)\n-\n-print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n-\n-\"\"\"\n-## Examine your saved model\n-\n-We'll use the command line utility `saved_model_cli` to look at the\n-[MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef)\n-(the models) and [SignatureDefs](https://www.tensorflow.org/tfx/serving/signature_defs)\n-(the methods you can call) in our SavedModel. See\n-[this discussion of the SavedModel CLI](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel)\n-in the TensorFlow Guide.\n-\"\"\"\n-\n-\n-\"\"\"shell\n-saved_model_cli show --dir {model_export_path} --tag_set serve --signature_def serving_default\n-\"\"\"\n-\n-\"\"\"\n-That tells us a lot about our model! For instance, we can see that its inputs\n-have a 4D shape `(-1, 224, 224, 3)` which means\n-`(batch_size, height, width, channels)`, also note that this model requires a\n-specific image shape `(224, 224, 3)` this means that we may need to reshape\n-our images before sending them to the model. We can also see that the model's\n-outputs have a `(-1, 1000)` shape which are the logits for the 1000 classes of\n-the [ImageNet](https://www.image-net.org) dataset.\n-\n-This information doesn't tell us everything, like the fact that the pixel\n-values needs to be in the `[-1, 1]` range, but it's a great start.\n-\n-## Serve your model with TensorFlow Serving\n-\n-### Install TFServing\n-\n-We're preparing to install TensorFlow Serving using\n-[Aptitude](https://wiki.debian.org/Aptitude) since this Colab runs in a Debian\n-environment. We'll add the `tensorflow-model-server` package to the list of\n-packages that Aptitude knows about. Note that we're running as root.\n-\n-\n-> Note: This example is running TensorFlow Serving natively, but [you can also\n-run it in a Docker container](https://www.tensorflow.org/tfx/serving/docker),\n-which is one of the easiest ways to get started using TensorFlow Serving.\n-\n-```shell\n-wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-universal-2.8.0/t/tensorflow-model-server-universal/tensorflow-model-server-universal_2.8.0_all.deb'\n-dpkg -i tensorflow-model-server-universal_2.8.0_all.deb\n-```\n-\"\"\"\n-\n-\"\"\"\n-### Start running TensorFlow Serving\n-\n-This is where we start running TensorFlow Serving and load our model. After it\n-loads, we can start making inference requests using REST. There are some\n-important parameters:\n-\n-- `port`: The port that you'll use for gRPC requests.\n-- `rest_api_port`: The port that you'll use for REST requests.\n-- `model_name`: You'll use this in the URL of REST requests. It can be\n-anything.\n-- `model_base_path`: This is the path to the directory where you've saved your\n-model.\n-\n-Check the [TFServing API reference](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc)\n-to get all the parameters available.\n-\"\"\"\n-\n-# Environment variable with the path to the model\n-os.environ[\"MODEL_DIR\"] = f\"{model_dir}\"\n-\n-\"\"\"\n-```shell\n-%%bash --bg\n-nohup tensorflow_model_server \\\n-  --port=8500 \\\n-  --rest_api_port=8501 \\\n-  --model_name=model \\\n-  --model_base_path=$MODEL_DIR >server.log 2>&1\n-```\n-\n-```shell\n-# We can check the logs to the server to help troubleshooting\n-!cat server.log\n-```\n-outputs:\n-```\n-[warn] getaddrinfo: address family for nodename not supported\n-[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n-```\n-\n-```shell\n-# Now we can check if tensorflow is in the active services\n-!sudo lsof -i -P -n | grep LISTEN\n-```\n-outputs:\n-```\n-node         7 root   21u  IPv6  19100      0t0  TCP *:8080 (LISTEN)\n-kernel_ma   34 root    7u  IPv4  18874      0t0  TCP 172.28.0.12:6000 (LISTEN)\n-colab-fil   63 root    5u  IPv4  17975      0t0  TCP *:3453 (LISTEN)\n-colab-fil   63 root    6u  IPv6  17976      0t0  TCP *:3453 (LISTEN)\n-jupyter-n   81 root    6u  IPv4  18092      0t0  TCP 172.28.0.12:9000 (LISTEN)\n-python3    101 root   23u  IPv4  18252      0t0  TCP 127.0.0.1:44915 (LISTEN)\n-python3    132 root    3u  IPv4  20548      0t0  TCP 127.0.0.1:15264 (LISTEN)\n-python3    132 root    4u  IPv4  20549      0t0  TCP 127.0.0.1:37977 (LISTEN)\n-python3    132 root    9u  IPv4  20662      0t0  TCP 127.0.0.1:40689 (LISTEN)\n-tensorflo 1101 root    5u  IPv4  35543      0t0  TCP *:8500 (LISTEN)\n-tensorflo 1101 root   12u  IPv4  35548      0t0  TCP *:8501 (LISTEN)\n-```\n-\n-## Make a request to your model in TensorFlow Serving\n-\n-Now let's create the JSON object for an inference request, and see how well\n-our model classifies it:\n-\n-### REST API\n-\n-#### Newest version of the servable\n-\n-We'll send a predict request as a POST to our server's REST endpoint, and pass\n-it as an example. We'll ask our server to give us the latest version of our\n-servable by not specifying a particular version.\n-\"\"\"\n-\n-data = json.dumps(\n-    {\n-        \"signature_name\": \"serving_default\",\n-        \"instances\": batched_img.numpy().tolist(),\n-    }\n-)\n-url = \"http://localhost:8501/v1/models/model:predict\"\n-\n-\n-def predict_rest(json_data, url):\n-    json_response = requests.post(url, data=json_data)\n-    response = json.loads(json_response.text)\n-    rest_outputs = np.array(response[\"predictions\"])\n-    return rest_outputs\n-\n-\n-\"\"\"\n-```python\n-rest_outputs = predict_rest(data, url)\n-\n-print(f\"REST output shape: {rest_outputs.shape}\")\n-print(f\"Predicted class: {postprocess(rest_outputs)}\")\n-```\n-\n-outputs:\n-```\n-REST output shape: (1, 1000)\n-Predicted class: [b'banana']\n-```\n-\n-### gRPC API\n-\n-[gRPC](https://grpc.io/) is based on the Remote Procedure Call (RPC) model and\n-is a technology for implementing RPC APIs that uses HTTP 2.0 as its underlying\n-transport protocol. gRPC is usually preferred for low-latency, highly scalable,\n-and distributed systems. If you wanna know more about the REST vs gRPC\n-tradeoffs, checkout\n-[this article](https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them).\n-\"\"\"\n-\n-import grpc\n-\n-# Create a channel that will be connected to the gRPC port of the container\n-channel = grpc.insecure_channel(\"localhost:8500\")\n-\n-\"\"\"\n-```shell\n-pip install -q tensorflow_serving_api\n-```\n-\n-```python\n-from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n-\n-# Create a stub made for prediction\n-# This stub will be used to send the gRPCrequest to the TF Server\n-stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n-```\n-\"\"\"\n-\n-# Get the serving_input key\n-loaded_model = tf.saved_model.load(model_export_path)\n-input_name = list(\n-    loaded_model.signatures[\"serving_default\"]\n-    .structured_input_signature[1]\n-    .keys()\n-)[0]\n-\n-\n-\"\"\"\n-```python\n-def predict_grpc(data, input_name, stub):\n-    # Create a gRPC request made for prediction\n-    request = predict_pb2.PredictRequest()\n-\n-    # Set the name of the model, for this use case it is \"model\"\n-    request.model_spec.name = \"model\"\n-\n-    # Set which signature is used to format the gRPC query\n-    # here the default one \"serving_default\"\n-    request.model_spec.signature_name = \"serving_default\"\n-\n-    # Set the input as the data\n-    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n-    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data.numpy().tolist()))\n-\n-    # Send the gRPC request to the TF Server\n-    result = stub.Predict(request)\n-    return result\n-\n-\n-grpc_outputs = predict_grpc(batched_img, input_name, stub)\n-grpc_outputs = np.array([grpc_outputs.outputs['predictions'].float_val])\n-\n-print(f\"gRPC output shape: {grpc_outputs.shape}\")\n-print(f\"Predicted class: {postprocess(grpc_outputs)}\")\n-```\n-\n-outputs:\n-```\n-gRPC output shape: (1, 1000)\n-Predicted class: [b'banana']\n-```\n-\"\"\"\n-\n-\"\"\"\n-## Custom signature\n-\n-Note that for this model we always need to preprocess and postprocess all\n-samples to get the desired output, this can get quite tricky if are\n-maintaining and serving several models developed by a large team, and each one\n-of them might require different processing logic.\n-\n-TensorFlow allows us to customize the model graph to embed all of that\n-processing logic, which makes model serving much easier, there are different\n-ways to achieve this, but since we are going to server the models using\n-TFServing we can customize the model graph straight into the serving signature.\n-\n-We can just use the following code to export the same model that already\n-contains the preprocessing and postprocessing logic as the default signature,\n-this allows this model to make predictions on raw data.\n-\"\"\"\n-\n-\n-def export_model(model, labels):\n-    @tf.function(\n-        input_signature=[tf.TensorSpec([None, None, None, 3], tf.float32)]\n-    )\n-    def serving_fn(image):\n-        processed_img = preprocess(image)\n-        probs = model(processed_img)\n-        label = postprocess(probs)\n-        return {\"label\": label}\n-\n-    return serving_fn\n-\n-\n-model_sig_version = 2\n-model_sig_export_path = f\"{model_dir}/{model_sig_version}\"\n-\n-tf.saved_model.save(\n-    model,\n-    export_dir=model_sig_export_path,\n-    signatures={\"serving_default\": export_model(model, labels)},\n-)\n-\n-\"\"\"shell\n-saved_model_cli show --dir {model_sig_export_path} --tag_set serve --signature_def serving_default\n-\"\"\"\n-\n-\"\"\"\n-Note that this model has a different signature, its input is still 4D but now\n-with a `(-1, -1, -1, 3)` shape, which means that it supports images with any\n-height and width size. Its output also has a different shape, it no longer\n-outputs the 1000-long logits.\n-\n-We can test the model's prediction using a specific signature using this API\n-below:\n-\"\"\"\n-\n-batched_raw_img = tf.expand_dims(sample_img, axis=0)\n-batched_raw_img = tf.cast(batched_raw_img, tf.float32)\n-\n-loaded_model = tf.saved_model.load(model_sig_export_path)\n-loaded_model.signatures[\"serving_default\"](**{\"image\": batched_raw_img})\n-\n-\"\"\"\n-## Prediction using a particular version of the servable\n-\n-Now let's specify a particular version of our servable. Note that when we\n-saved the model with a custom signature we used a different folder, the first\n-model was saved in folder `/1` (version 1), and the one with a custom\n-signature in folder `/2` (version 2). By default, TFServing will serve all\n-models that share the same base parent folder.\n-\n-### REST API\n-\"\"\"\n-\n-data = json.dumps(\n-    {\n-        \"signature_name\": \"serving_default\",\n-        \"instances\": batched_raw_img.numpy().tolist(),\n-    }\n-)\n-url_sig = \"http://localhost:8501/v1/models/model/versions/2:predict\"\n-\n-\"\"\"\n-```python\n-print(f\"REST output shape: {rest_outputs.shape}\")\n-print(f\"Predicted class: {rest_outputs}\")\n-```\n-\n-outputs:\n-```\n-REST output shape: (1,)\n-Predicted class: ['banana']\n-```\n-\n-### gRPC API\n-\"\"\"\n-\n-\"\"\"\n-```python\n-channel = grpc.insecure_channel(\"localhost:8500\")\n-stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n-```\n-\"\"\"\n-\n-input_name = list(\n-    loaded_model.signatures[\"serving_default\"]\n-    .structured_input_signature[1]\n-    .keys()\n-)[0]\n-\n-\"\"\"\n-```python\n-grpc_outputs = predict_grpc(batched_raw_img, input_name, stub)\n-grpc_outputs = np.array([grpc_outputs.outputs['label'].string_val])\n-\n-print(f\"gRPC output shape: {grpc_outputs.shape}\")\n-print(f\"Predicted class: {grpc_outputs}\")\n-```\n-\n-outputs:\n-\n-```\n-gRPC output shape: (1, 1)\n-Predicted class: [[b'banana']]\n-```\n-\n-## Additional resources\n-\n-- [Colab notebook with the full working code](https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing)\n-- [Train and serve a TensorFlow model with TensorFlow Serving - TensorFlow blog](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#make_a_request_to_your_model_in_tensorflow_serving)\n-- [TensorFlow Serving playlist - TensorFlow YouTube channel](https://www.youtube.com/playlist?list=PLQY2H8rRoyvwHdpVQVohY7-qcYf2s1UYK)\n-\"\"\"\n\n@@ -1,143 +0,0 @@\n-\"\"\"\n-Title: Trainer pattern\n-Author: [nkovela1](https://nkovela1.github.io/)\n-Date created: 2022/09/19\n-Last modified: 2022/09/26\n-Description: Guide on how to share a custom training step across multiple Keras models.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to create a custom training step using the \"Trainer pattern\",\n-which can then be shared across multiple Keras models. This pattern overrides the\n-`train_step()` method of the `keras.Model` class, allowing for training loops\n-beyond plain supervised learning.\n-\n-The Trainer pattern can also easily be adapted to more complex models with larger\n-custom training steps, such as\n-[this end-to-end GAN model](https://keras.io/guides/customizing_what_happens_in_fit/#wrapping-up-an-endtoend-gan-example),\n-by putting the custom training step in the Trainer class definition.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import keras\n-\n-# Load MNIST dataset and standardize the data\n-mnist = keras.datasets.mnist\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-x_train, x_test = x_train / 255.0, x_test / 255.0\n-\n-\n-\"\"\"\n-## Define the Trainer class\n-\n-A custom training and evaluation step can be created by overriding\n-the `train_step()` and `test_step()` method of a `Model` subclass:\n-\"\"\"\n-\n-\n-class MyTrainer(keras.Model):\n-    def __init__(self, model):\n-        super().__init__()\n-        self.model = model\n-        # Create loss and metrics here.\n-        self.loss_fn = keras.losses.SparseCategoricalCrossentropy()\n-        self.accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n-\n-    @property\n-    def metrics(self):\n-        # List metrics here.\n-        return [self.accuracy_metric]\n-\n-    def train_step(self, data):\n-        x, y = data\n-        with tf.GradientTape() as tape:\n-            y_pred = self.model(x, training=True)  # Forward pass\n-            # Compute loss value\n-            loss = self.loss_fn(y, y_pred)\n-\n-        # Compute gradients\n-        trainable_vars = self.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-\n-        # Update weights\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-\n-        # Update metrics\n-        for metric in self.metrics:\n-            metric.update_state(y, y_pred)\n-\n-        # Return a dict mapping metric names to current value.\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, data):\n-        x, y = data\n-\n-        # Inference step\n-        y_pred = self.model(x, training=False)\n-\n-        # Update metrics\n-        for metric in self.metrics:\n-            metric.update_state(y, y_pred)\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def call(self, x):\n-        # Equivalent to `call()` of the wrapped keras.Model\n-        x = self.model(x)\n-        return x\n-\n-\n-\"\"\"\n-## Define multiple models to share the custom training step\n-\n-Let's define two different models that can share our Trainer class and its custom `train_step()`:\n-\"\"\"\n-\n-# A model defined using Sequential API\n-model_a = keras.models.Sequential(\n-    [\n-        keras.layers.Flatten(input_shape=(28, 28)),\n-        keras.layers.Dense(256, activation=\"relu\"),\n-        keras.layers.Dropout(0.2),\n-        keras.layers.Dense(10, activation=\"softmax\"),\n-    ]\n-)\n-\n-# A model defined using Functional API\n-func_input = keras.Input(shape=(28, 28, 1))\n-x = keras.layers.Flatten(input_shape=(28, 28))(func_input)\n-x = keras.layers.Dense(512, activation=\"relu\")(x)\n-x = keras.layers.Dropout(0.4)(x)\n-func_output = keras.layers.Dense(10, activation=\"softmax\")(x)\n-\n-model_b = keras.Model(func_input, func_output)\n-\n-\"\"\"\n-## Create Trainer class objects from the models\n-\"\"\"\n-\n-trainer_1 = MyTrainer(model_a)\n-trainer_2 = MyTrainer(model_b)\n-\n-\"\"\"\n-## Compile and fit the models to the MNIST dataset\n-\"\"\"\n-\n-trainer_1.compile(optimizer=keras.optimizers.SGD())\n-trainer_1.fit(\n-    x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test)\n-)\n-\n-trainer_2.compile(optimizer=keras.optimizers.Adam())\n-trainer_2.fit(\n-    x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test)\n-)\n\n@@ -1,528 +0,0 @@\n-\"\"\"\n-Title: End-to-end Masked Language Modeling with BERT\n-Author: [Ankur Singh](https://twitter.com/ankur310794)\n-Converted to Keras-Core: [Mrutyunjay Biswal](https://twitter.com/LearnStochastic)\n-Date created: 2020/09/18\n-Last modified: 2023/09/06\n-Description: Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Masked Language Modeling is a fill-in-the-blank task,\n-where a model uses the context words surrounding a mask token to try to predict what the\n-masked word should be.\n-\n-For an input that contains one or more mask tokens,\n-the model will generate the most likely substitution for each.\n-\n-Example:\n-\n-- Input: \"I have watched this [MASK] and it was awesome.\"\n-- Output: \"I have watched this movie and it was awesome.\"\n-\n-Masked language modeling is a great way to train a language\n-model in a self-supervised setting (without human-annotated labels).\n-Such a model can then be fine-tuned to accomplish various supervised\n-NLP tasks.\n-\n-This example teaches you how to build a BERT model from scratch,\n-train it with the masked language modeling task,\n-and then fine-tune this model on a sentiment classification task.\n-\n-We will use the Keras-Core `TextVectorization` and `MultiHeadAttention` layers\n-to create a BERT Transformer-Encoder network architecture.\n-\n-Note: This is only tensorflow backend compatible.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import re\n-import glob\n-import numpy as np\n-import pandas as pd\n-from pathlib import Path\n-from dataclasses import dataclass\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Configuration\n-\"\"\"\n-\n-@dataclass\n-class Config:\n-    MAX_LEN = 256\n-    BATCH_SIZE = 32\n-    LR = 0.001\n-    VOCAB_SIZE = 30000\n-    EMBED_DIM = 128\n-    NUM_HEAD = 8  # used in bert model\n-    FF_DIM = 128  # used in bert model\n-    NUM_LAYERS = 1\n-    NUM_EPOCHS = 1\n-    STEPS_PER_EPOCH = 2\n-\n-\n-config = Config()\n-\n-\"\"\"\n-## Download the Data: IMDB Movie Review Sentiment Classification\n-\n-Download the IMDB data and load into a Pandas DataFrame.\n-\"\"\"\n-\n-fpath = keras.utils.get_file(\n-    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n-)\n-dirpath = Path(fpath).parent.absolute()\n-os.system(f\"tar -xf {fpath} -C {dirpath}\")\n-\n-\"\"\"\n-The `aclImdb` folder contains a `train` and `test` subfolder:\n-\"\"\"\n-\n-os.system(f\"ls {dirpath}/aclImdb\")\n-os.system(f\"ls {dirpath}/aclImdb/train\")\n-os.system(f\"ls {dirpath}/aclImdb/test\")\n-\n-\"\"\"\n-We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:\n-\"\"\"\n-\n-os.system(f\"rm -r {dirpath}/aclImdb/train/unsup\")\n-os.system(f\"rm -r {dirpath}/aclImdb/train/*.feat\")\n-os.system(f\"rm -r {dirpath}/aclImdb/train/*.txt\")\n-os.system(f\"rm -r {dirpath}/aclImdb/test/*.feat\")\n-os.system(f\"rm -r {dirpath}/aclImdb/test/*.txt\")\n-\n-\"\"\"\n-Let's read the dataset from the text files to a DataFrame.\n-\"\"\"\n-\n-def get_text_list_from_files(files):\n-    text_list = []\n-    for name in files:\n-        with open(name) as f:\n-            for line in f:\n-                text_list.append(line)\n-    return text_list\n-\n-\n-def get_data_from_text_files(folder_name):\n-\n-    pos_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/pos/*.txt\")\n-    pos_texts = get_text_list_from_files(pos_files)\n-    neg_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/neg/*.txt\")\n-    neg_texts = get_text_list_from_files(neg_files)\n-    df = pd.DataFrame(\n-        {\n-            \"review\": pos_texts + neg_texts,\n-            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n-        }\n-    )\n-    df = df.sample(len(df)).reset_index(drop=True)\n-    return df\n-\n-train_df = get_data_from_text_files(\"train\")\n-test_df = get_data_from_text_files(\"test\")\n-\n-all_data = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n-assert len(all_data) != 0, f'{all_data} is empty'\n-\n-\"\"\"\n-## Dataset preparation\n-\n-We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n-It transforms a batch of strings into either\n-a sequence of token indices (one sample = 1D array of integer token indices, in order)\n-or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n-\n-Below, we define 3 preprocessing functions.\n-\n-1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n-2.  The `encode` function encodes raw text into integer token ids.\n-3.  The `get_masked_input_and_labels` function will mask input token ids. It masks 15% of all input tokens in each sequence at random.\n-\"\"\"\n-\n-def custom_standardization(input_data):\n-    lowercase = tf.strings.lower(input_data)\n-    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n-    return tf.strings.regex_replace(\n-        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n-    )\n-\n-\n-def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n-    \"\"\"Build Text vectorization layer\n-\n-    Args:\n-      texts (list): List of string i.e input texts\n-      vocab_size (int): vocab size\n-      max_seq (int): Maximum sequence lenght.\n-      special_tokens (list, optional): List of special tokens. Defaults to `['[MASK]']`.\n-\n-    Returns:\n-        layers.Layer: Return TextVectorization Keras Layer\n-    \"\"\"\n-    vectorize_layer = layers.TextVectorization(\n-        max_tokens=vocab_size,\n-        output_mode=\"int\",\n-        standardize=custom_standardization,\n-        output_sequence_length=max_seq,\n-    )\n-    vectorize_layer.adapt(texts)\n-\n-    # Insert mask token in vocabulary\n-    vocab = vectorize_layer.get_vocabulary()\n-    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n-    vectorize_layer.set_vocabulary(vocab)\n-    return vectorize_layer\n-\n-\n-vectorize_layer = get_vectorize_layer(\n-    all_data.review.values.tolist(),\n-    config.VOCAB_SIZE,\n-    config.MAX_LEN,\n-    special_tokens=[\"[mask]\"],\n-)\n-\n-# Get mask token id for masked language model\n-mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n-\n-\n-def encode(texts):\n-    encoded_texts = vectorize_layer(texts)\n-    return encoded_texts.numpy()\n-\n-\n-def get_masked_input_and_labels(encoded_texts):\n-    # 15% BERT masking\n-    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n-    # Do not mask special tokens\n-    inp_mask[encoded_texts <= 2] = False\n-    # Set targets to -1 by default, it means ignore\n-    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n-    # Set labels for masked tokens\n-    labels[inp_mask] = encoded_texts[inp_mask]\n-\n-    # Prepare input\n-    encoded_texts_masked = np.copy(encoded_texts)\n-    # Set input to [MASK] which is the last token for the 90% of tokens\n-    # This means leaving 10% unchanged\n-    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n-    encoded_texts_masked[\n-        inp_mask_2mask\n-    ] = mask_token_id  # mask token is the last in the dict\n-\n-    # Set 10% to a random token\n-    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n-    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n-        3, mask_token_id, inp_mask_2random.sum()\n-    )\n-\n-    # Prepare sample_weights to pass to .fit() method\n-    sample_weights = np.ones(labels.shape)\n-    sample_weights[labels == -1] = 0\n-\n-    # y_labels would be same as encoded_texts i.e input tokens\n-    y_labels = np.copy(encoded_texts)\n-\n-    return encoded_texts_masked, y_labels, sample_weights\n-\n-\n-# We have 25000 examples for training\n-x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n-y_train = train_df.sentiment.values\n-train_classifier_ds = (\n-    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-    .shuffle(1000)\n-    .batch(config.BATCH_SIZE)\n-)\n-\n-# We have 25000 examples for testing\n-x_test = encode(test_df.review.values)\n-y_test = test_df.sentiment.values\n-test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n-    config.BATCH_SIZE\n-)\n-\n-# Build dataset for end to end model input (will be used at the end)\n-test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n-    (test_df.review.values, y_test)\n-).batch(config.BATCH_SIZE)\n-\n-# Prepare data for masked language model\n-x_all_review = encode(all_data.review.values)\n-x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n-    x_all_review\n-)\n-\n-mlm_ds = tf.data.Dataset.from_tensor_slices(\n-    (x_masked_train, y_masked_labels, sample_weights)\n-)\n-mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n-\n-id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n-token2id = {y: x for x, y in id2token.items()}\n-\n-class MaskedTextGenerator(keras.callbacks.Callback):\n-    def __init__(self, sample_tokens, top_k=5):\n-        self.sample_tokens = sample_tokens\n-        self.k = top_k\n-\n-    def decode(self, tokens):\n-        return \" \".join([id2token[t] for t in tokens if t != 0])\n-\n-    def convert_ids_to_tokens(self, id):\n-        return id2token[id]\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        prediction = self.model.predict(self.sample_tokens)\n-\n-        masked_index = np.where(self.sample_tokens == mask_token_id)\n-        masked_index = masked_index[1]\n-        mask_prediction = prediction[0][masked_index]\n-\n-        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n-        values = mask_prediction[0][top_indices]\n-\n-        for i in range(len(top_indices)):\n-            p = top_indices[i]\n-            v = values[i]\n-            tokens = np.copy(self.sample_tokens[0])\n-            tokens[masked_index[0]] = p\n-            result = {\n-                \"input_text\": self.decode(self.sample_tokens[0]),\n-                \"prediction\": self.decode(tokens),\n-                \"probability\": v,\n-                \"predicted mask token\": self.convert_ids_to_tokens(p),\n-            }\n-\n-sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n-generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n-\n-\"\"\"\n-## Create BERT model (Pretraining Model) for masked language modeling\n-\n-We will create a BERT-like pretraining model architecture\n-using the `MultiHeadAttention` layer.\n-It will take token ids as inputs (including masked tokens)\n-and it will predict the correct ids for the masked input tokens.\n-\"\"\"\n-\n-def bert_module(query, key, value, layer_num):\n-    # Multi headed self-attention\n-    attention_output = layers.MultiHeadAttention(\n-        num_heads=config.NUM_HEAD,\n-        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n-        name=f\"encoder_{layer_num}_multiheadattention\",\n-    )(query, key, value)\n-    attention_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_att_dropout\")(\n-        attention_output\n-    )\n-    attention_output = layers.LayerNormalization(\n-        epsilon=1e-6, name=f\"encoder_{layer_num}_att_layernormalization\"\n-    )(query + attention_output)\n-\n-    # Feed-forward layer\n-    ffn = keras.Sequential(\n-        [\n-            layers.Dense(config.FF_DIM, activation=\"relu\"),\n-            layers.Dense(config.EMBED_DIM),\n-        ],\n-        name=f\"encoder_{layer_num}_ffn\",\n-    )\n-    ffn_output = ffn(attention_output)\n-    ffn_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_ffn_dropout\")(\n-        ffn_output\n-    )\n-    sequence_output = layers.LayerNormalization(\n-        epsilon=1e-6, name=f\"encoder_{layer_num}_ffn_layernormalization\"\n-    )(attention_output + ffn_output)\n-    return sequence_output\n-\n-\n-def get_pos_encoding_matrix(max_len, d_emb):\n-    pos_enc = np.array(\n-        [\n-            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n-            if pos != 0\n-            else np.zeros(d_emb)\n-            for pos in range(max_len)\n-        ]\n-    )\n-    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n-    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n-    return pos_enc\n-\n-\n-loss_fn = keras.losses.SparseCategoricalCrossentropy(\n-    reduction=None\n-)\n-loss_tracker = keras.metrics.Mean(name=\"loss\")\n-\n-\n-class MaskedLanguageModel(keras.Model):\n-    def train_step(self, inputs):\n-        if len(inputs) == 3:\n-            features, labels, sample_weight = inputs\n-        else:\n-            features, labels = inputs\n-            sample_weight = None\n-\n-        with tf.GradientTape() as tape:\n-            predictions = self(features, training=True)\n-            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n-\n-        # Compute gradients\n-        trainable_vars = self.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-\n-        # Update weights\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-\n-        # Compute our own metrics\n-        loss_tracker.update_state(loss, sample_weight=sample_weight)\n-\n-        # Return a dict mapping metric names to current value\n-        return {\"loss\": loss_tracker.result()}\n-\n-    @property\n-    def metrics(self):\n-        # We list our `Metric` objects here so that `reset_states()` can be\n-        # called automatically at the start of each epoch\n-        # or at the start of `evaluate()`.\n-        # If you don't implement this property, you have to call\n-        # `reset_states()` yourself at the time of your choosing.\n-        return [loss_tracker]\n-\n-\n-def create_masked_language_bert_model():\n-    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n-\n-    word_embeddings = layers.Embedding(\n-        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n-    )(inputs)\n-    \n-    position_embeddings = layers.Embedding(\n-        input_dim=config.MAX_LEN,\n-        output_dim=config.EMBED_DIM,\n-        embeddings_initializer=keras.initializers.Constant(get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)),\n-        name=\"position_embedding\",\n-    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n-    \n-    embeddings = word_embeddings + position_embeddings\n-\n-    encoder_output = embeddings\n-    for i in range(config.NUM_LAYERS):\n-        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n-\n-    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n-        encoder_output\n-    )\n-    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n-\n-    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n-    mlm_model.compile(optimizer=optimizer)\n-    return mlm_model\n-\n-bert_masked_model = create_masked_language_bert_model()\n-bert_masked_model.summary()\n-\n-\"\"\"\n-## Train and Save\n-\"\"\"\n-\n-bert_masked_model.fit(mlm_ds, epochs=Config.NUM_EPOCHS, steps_per_epoch=Config.STEPS_PER_EPOCH, callbacks=[generator_callback])\n-bert_masked_model.save(\"bert_mlm_imdb.keras\")\n-\n-\"\"\"\n-## Fine-tune a sentiment classification model\n-\n-We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n-To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n-pretrained BERT features.\n-\"\"\"\n-\n-# Load pretrained bert model\n-mlm_model = keras.models.load_model(\n-    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n-)\n-pretrained_bert_model = keras.Model(\n-    mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output\n-)\n-\n-# Freeze it\n-pretrained_bert_model.trainable = False\n-\n-\n-def create_classifier_bert_model():\n-    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n-    sequence_output = pretrained_bert_model(inputs)\n-    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n-    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n-    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n-    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n-    optimizer = keras.optimizers.Adam()\n-    classifer_model.compile(\n-        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n-    )\n-    return classifer_model\n-\n-\n-classifer_model = create_classifier_bert_model()\n-classifer_model.summary()\n-\n-# Train the classifier with frozen BERT stage\n-classifer_model.fit(\n-    train_classifier_ds,\n-    epochs=Config.NUM_EPOCHS,\n-    steps_per_epoch=Config.STEPS_PER_EPOCH,\n-    validation_data=test_classifier_ds,\n-)\n-\n-# Unfreeze the BERT model for fine-tuning\n-pretrained_bert_model.trainable = True\n-optimizer = keras.optimizers.Adam()\n-classifer_model.compile(\n-    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n-)\n-classifer_model.fit(\n-    train_classifier_ds,\n-    epochs=Config.NUM_EPOCHS,\n-    steps_per_epoch=Config.STEPS_PER_EPOCH,\n-    validation_data=test_classifier_ds,\n-)\n-\n-\"\"\"\n-## Create an end-to-end model and evaluate it\n-\n-When you want to deploy a model, it's best if it already includes its preprocessing\n-pipeline, so that you don't have to reimplement the preprocessing logic in your\n-production environment. Let's create an end-to-end model that incorporates\n-the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n-as input.\n-\"\"\"\n-\n-def get_end_to_end(model):\n-    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n-    indices = vectorize_layer(inputs_string)\n-    outputs = model(indices)\n-    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n-    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n-    end_to_end_model.compile(\n-        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n-    )\n-    return end_to_end_model\n-\n-\n-end_to_end_classification_model = get_end_to_end(classifer_model)\n-end_to_end_classification_model.evaluate(test_raw_classifier_ds)\n\\ No newline at end of file\n\n@@ -1,355 +0,0 @@\n-\"\"\"\n-Title: Named Entity Recognition using Transformers\n-Author: [Varun Singh](https://www.linkedin.com/in/varunsingh2/)\n-Date created: 2021/6/23\n-Last modified: 2023/7/25\n-Description: NER using the Transformers and data from CoNLL 2003 shared task.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Named Entity Recognition (NER) is the process of identifying named entities in text.\n-Example of named entities are: \"Person\", \"Location\", \"Organization\", \"Dates\" etc. NER is\n-essentially a token classification task where every token is classified into one or more\n-predetermined categories.\n-\n-In this exercise, we will train a simple Transformer based model to perform NER. We will\n-be using the data from CoNLL 2003 shared task. For more information about the dataset,\n-please visit [the dataset website](https://www.clips.uantwerpen.be/conll2003/ner/).\n-However, since obtaining this data requires an additional step of getting a free license, we will be using\n-HuggingFace's datasets library which contains a processed version of this dataset.\n-\"\"\"\n-\n-\"\"\"\n-## Install the open source datasets library from HuggingFace\n-\n-We also download the script used to evaluate NER models.\n-\"\"\"\n-\n-\"\"\"shell\n-pip3 install datasets\n-wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n-\"\"\"\n-\n-import os\n-import numpy as np\n-import keras\n-from keras import layers\n-from datasets import load_dataset\n-from collections import Counter\n-from conlleval import evaluate\n-\n-# imports for data preprocessing\n-from tensorflow import data as tf_data\n-from tensorflow import strings as tf_strings\n-\n-\n-class TransformerBlock(layers.Layer):\n-    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n-        super().__init__()\n-        self.att = keras.layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim\n-        )\n-        self.ffn = keras.Sequential(\n-            [\n-                keras.layers.Dense(ff_dim, activation=\"relu\"),\n-                keras.layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n-        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n-        self.dropout1 = keras.layers.Dropout(rate)\n-        self.dropout2 = keras.layers.Dropout(rate)\n-\n-    def call(self, inputs, training=False):\n-        attn_output = self.att(inputs, inputs)\n-        attn_output = self.dropout1(attn_output, training=training)\n-        out1 = self.layernorm1(inputs + attn_output)\n-        ffn_output = self.ffn(out1)\n-        ffn_output = self.dropout2(ffn_output, training=training)\n-        return self.layernorm2(out1 + ffn_output)\n-\n-\n-\"\"\"\n-Next, let's define a `TokenAndPositionEmbedding` layer:\n-\"\"\"\n-\n-\n-class TokenAndPositionEmbedding(layers.Layer):\n-    def __init__(self, maxlen, vocab_size, embed_dim):\n-        super().__init__()\n-        self.token_emb = keras.layers.Embedding(\n-            input_dim=vocab_size, output_dim=embed_dim\n-        )\n-        self.pos_emb = keras.layers.Embedding(\n-            input_dim=maxlen, output_dim=embed_dim\n-        )\n-\n-    def call(self, inputs):\n-        maxlen = keras.ops.backend.shape(inputs)[-1]\n-        positions = keras.ops.arange(start=0, stop=maxlen, step=1)\n-        position_embeddings = self.pos_emb(positions)\n-        token_embeddings = self.token_emb(inputs)\n-        return token_embeddings + position_embeddings\n-\n-\n-\"\"\"\n-## Build the NER model class as a `keras.Model` subclass\n-\"\"\"\n-\n-\n-class NERModel(keras.Model):\n-    def __init__(\n-        self,\n-        num_tags,\n-        vocab_size,\n-        maxlen=128,\n-        embed_dim=32,\n-        num_heads=2,\n-        ff_dim=32,\n-    ):\n-        super().__init__()\n-        self.embedding_layer = TokenAndPositionEmbedding(\n-            maxlen, vocab_size, embed_dim\n-        )\n-        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n-        self.dropout1 = layers.Dropout(0.1)\n-        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n-        self.dropout2 = layers.Dropout(0.1)\n-        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n-\n-    def call(self, inputs, training=False):\n-        x = self.embedding_layer(inputs)\n-        x = self.transformer_block(x)\n-        x = self.dropout1(x, training=training)\n-        x = self.ff(x)\n-        x = self.dropout2(x, training=training)\n-        x = self.ff_final(x)\n-        return x\n-\n-\n-\"\"\"\n-## Load the CoNLL 2003 dataset from the datasets library and process it\n-\"\"\"\n-\n-conll_data = load_dataset(\"conll2003\")\n-\n-\"\"\"\n-We will export this data to a tab-separated file format which will be easy to read as a\n-`tf.data.Dataset` object.\n-\"\"\"\n-\n-\n-def export_to_file(export_file_path, data):\n-    with open(export_file_path, \"w\") as f:\n-        for record in data:\n-            ner_tags = record[\"ner_tags\"]\n-            tokens = record[\"tokens\"]\n-            if len(tokens) > 0:\n-                f.write(\n-                    str(len(tokens))\n-                    + \"\\t\"\n-                    + \"\\t\".join(tokens)\n-                    + \"\\t\"\n-                    + \"\\t\".join(map(str, ner_tags))\n-                    + \"\\n\"\n-                )\n-\n-\n-os.mkdir(\"data\")\n-export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n-export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])\n-\n-\"\"\"\n-## Make the NER label lookup table\n-\n-NER labels are usually provided in IOB, IOB2 or IOBES formats. Checkout this link for\n-more information:\n-[Wikipedia](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n-\n-Note that we start our label numbering from 1 since 0 will be reserved for padding. We\n-have a total of 10 labels: 9 from the NER dataset and one for padding.\n-\"\"\"\n-\n-\n-def make_tag_lookup_table():\n-    iob_labels = [\"B\", \"I\"]\n-    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n-    all_labels = [\n-        (label1, label2) for label2 in ner_labels for label1 in iob_labels\n-    ]\n-    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n-    all_labels = [\"[PAD]\", \"O\"] + all_labels\n-    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n-\n-\n-mapping = make_tag_lookup_table()\n-print(mapping)\n-\n-\"\"\"\n-Get a list of all tokens in the training dataset. This will be used to create the\n-vocabulary.\n-\"\"\"\n-\n-all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n-all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n-\n-counter = Counter(all_tokens_array)\n-print(len(counter))\n-\n-num_tags = len(mapping)\n-vocab_size = 20000\n-\n-# We only take (vocab_size - 2) most commons words from the training data since\n-# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n-# token and another one denoting a masking token\n-vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n-\n-# The StringLook class will convert tokens to token IDs\n-lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n-\n-\"\"\"\n-Create 2 new `Dataset` objects from the training and validation data\n-\"\"\"\n-\n-train_data = tf_data.TextLineDataset(\"./data/conll_train.txt\")\n-val_data = tf_data.TextLineDataset(\"./data/conll_val.txt\")\n-\n-\"\"\"\n-Print out one line to make sure it looks good. The first record in the line is the number of tokens. \n-After that we will have all the tokens followed by all the ner tags.\n-\"\"\"\n-\n-print(list(train_data.take(1).as_numpy_iterator()))\n-\n-\"\"\"\n-We will be using the following map function to transform the data in the dataset:\n-\"\"\"\n-\n-\n-# Data preprocessing using Tensorflow\n-def map_record_to_training_data(record):\n-    record = tf_strings.split(record, sep=\"\\t\")\n-    length = tf_strings.to_number(record[0], out_type=\"int32\")\n-    tokens = record[1 : length + 1]\n-    tags = record[length + 1 :]\n-    tags = tf_strings.to_number(tags, out_type=\"int64\")\n-    tags += 1\n-    return tokens, tags\n-\n-\n-def lowercase_and_convert_to_ids(tokens):\n-    tokens = tf_strings.lower(tokens)\n-    return lookup_layer(tokens)\n-\n-\n-# We use `padded_batch` here because each record in the dataset has a\n-# different length.\n-batch_size = 32\n-train_dataset = (\n-    train_data.map(map_record_to_training_data)\n-    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n-    .padded_batch(batch_size)\n-)\n-val_dataset = (\n-    val_data.map(map_record_to_training_data)\n-    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n-    .padded_batch(batch_size)\n-)\n-\n-ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n-\n-\"\"\"\n-We will be using a custom loss function that will ignore the loss from padded tokens.\n-\"\"\"\n-\n-\n-class CustomNonPaddingTokenLoss(keras.losses.Loss):\n-    def __init__(self, name=\"custom_ner_loss\"):\n-        super().__init__(name=name)\n-\n-    def call(self, y_true, y_pred):\n-        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n-            from_logits=True, reduction=None\n-        )\n-        loss = loss_fn(y_true, y_pred)\n-        mask = keras.backend.cast((y_true > 0), dtype=\"float32\")\n-        loss = loss * mask\n-        return keras.ops.sum(loss) / keras.ops.sum(mask)\n-\n-\n-loss = CustomNonPaddingTokenLoss()\n-\n-\"\"\"\n-## Compile and fit the model\n-\"\"\"\n-\n-ner_model.compile(optimizer=\"adam\", loss=loss)\n-ner_model.fit(train_dataset, epochs=10)\n-\n-\n-def tokenize_and_convert_to_ids(text):\n-    tokens = text.split()\n-    return lowercase_and_convert_to_ids(tokens)\n-\n-\n-# Sample inference using the trained model\n-sample_input = tokenize_and_convert_to_ids(\n-    \"eu rejects german call to boycott british lamb\"\n-)\n-sample_input = keras.ops.reshape(sample_input, new_shape=[1, -1])\n-print(sample_input)\n-\n-output = ner_model.predict(sample_input)\n-prediction = np.argmax(output, axis=-1)[0]\n-prediction = [mapping[i] for i in prediction]\n-\n-# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n-print(prediction)\n-\n-\"\"\"\n-## Metrics calculation\n-\n-Here is a function to calculate the metrics. The function calculates F1 score for the\n-overall NER dataset as well as individual scores for each NER tag.\n-\"\"\"\n-\n-\n-def calculate_metrics(dataset):\n-    all_true_tag_ids, all_predicted_tag_ids = [], []\n-\n-    for x, y in dataset:\n-        output = ner_model.predict(x)\n-        predictions = np.argmax(output, axis=-1)\n-        predictions = np.reshape(predictions, [-1])\n-\n-        true_tag_ids = np.reshape(y, [-1])\n-\n-        mask = (true_tag_ids > 0) & (predictions > 0)\n-        true_tag_ids = true_tag_ids[mask]\n-        predicted_tag_ids = predictions[mask]\n-\n-        all_true_tag_ids.append(true_tag_ids)\n-        all_predicted_tag_ids.append(predicted_tag_ids)\n-\n-    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n-    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n-\n-    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n-    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n-\n-    evaluate(real_tags, predicted_tags)\n-\n-\n-calculate_metrics(val_dataset)\n-\n-\"\"\"\n-## Conclusions\n-\n-In this exercise, we created a simple transformer based named entity recognition model.\n-We trained it on the CoNLL 2003 shared task data and got an overall F1 score of around 70%.\n-State of the art NER models fine-tuned on pretrained models such as BERT or ELECTRA can easily\n-get much higher F1 score -between 90-95% on this dataset owing to the inherent knowledge\n-of words as part of the pretraining process and the usage of subword tokenization.\n-\"\"\"\n\n@@ -1,307 +0,0 @@\n-\"\"\"\n-Title: Using pre-trained word embeddings\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/05/05\n-Last modified: 2020/05/05\n-Description: Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-# Only the TensorFlow backend supports string inputs.\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import pathlib\n-import numpy as np\n-import tensorflow.data as tf_data\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Introduction\n-\n-In this example, we show how to train a text classification model that uses pre-trained\n-word embeddings.\n-\n-We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n-belonging to 20 different topic categories.\n-\n-For the pre-trained word embeddings, we'll use\n-[GloVe embeddings](http://nlp.stanford.edu/projects/glove/).\n-\"\"\"\n-\n-\"\"\"\n-## Download the Newsgroup20 data\n-\"\"\"\n-\n-data_path = keras.utils.get_file(\n-    \"news20.tar.gz\",\n-    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n-    untar=True,\n-)\n-\n-\"\"\"\n-## Let's take a look at the data\n-\"\"\"\n-\n-data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n-dirnames = os.listdir(data_dir)\n-print(\"Number of directories:\", len(dirnames))\n-print(\"Directory names:\", dirnames)\n-\n-fnames = os.listdir(data_dir / \"comp.graphics\")\n-print(\"Number of files in comp.graphics:\", len(fnames))\n-print(\"Some example filenames:\", fnames[:5])\n-\n-\"\"\"\n-Here's a example of what one file contains:\n-\"\"\"\n-\n-print(open(data_dir / \"comp.graphics\" / \"38987\").read())\n-\n-\"\"\"\n-As you can see, there are header lines that are leaking the file's category, either\n-explicitly (the first line is literally the category name), or implicitly, e.g. via the\n-`Organization` filed. Let's get rid of the headers:\n-\"\"\"\n-\n-samples = []\n-labels = []\n-class_names = []\n-class_index = 0\n-for dirname in sorted(os.listdir(data_dir)):\n-    class_names.append(dirname)\n-    dirpath = data_dir / dirname\n-    fnames = os.listdir(dirpath)\n-    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n-    for fname in fnames:\n-        fpath = dirpath / fname\n-        f = open(fpath, encoding=\"latin-1\")\n-        content = f.read()\n-        lines = content.split(\"\\n\")\n-        lines = lines[10:]\n-        content = \"\\n\".join(lines)\n-        samples.append(content)\n-        labels.append(class_index)\n-    class_index += 1\n-\n-print(\"Classes:\", class_names)\n-print(\"Number of samples:\", len(samples))\n-\n-\"\"\"\n-There's actually one category that doesn't have the expected number of files, but the\n-difference is small enough that the problem remains a balanced classification problem.\n-\"\"\"\n-\n-\"\"\"\n-## Shuffle and split the data into training & validation sets\n-\"\"\"\n-\n-# Shuffle the data\n-seed = 1337\n-rng = np.random.RandomState(seed)\n-rng.shuffle(samples)\n-rng = np.random.RandomState(seed)\n-rng.shuffle(labels)\n-\n-# Extract a training & validation split\n-validation_split = 0.2\n-num_validation_samples = int(validation_split * len(samples))\n-train_samples = samples[:-num_validation_samples]\n-val_samples = samples[-num_validation_samples:]\n-train_labels = labels[:-num_validation_samples]\n-val_labels = labels[-num_validation_samples:]\n-\n-\"\"\"\n-## Create a vocabulary index\n-\n-Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n-Later, we'll use the same layer instance to vectorize the samples.\n-\n-Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n-be actually 200 tokens long.\n-\"\"\"\n-\n-vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=200)\n-text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n-vectorizer.adapt(text_ds)\n-\n-\"\"\"\n-You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n-print the top 5 words:\n-\"\"\"\n-\n-vectorizer.get_vocabulary()[:5]\n-\n-\"\"\"\n-Let's vectorize a test sentence:\n-\"\"\"\n-\n-output = vectorizer([[\"the cat sat on the mat\"]])\n-output.numpy()[0, :6]\n-\n-\"\"\"\n-As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n-word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n-reserved for \"out of vocabulary\" tokens.\n-\n-Here's a dict mapping words to their indices:\n-\"\"\"\n-\n-voc = vectorizer.get_vocabulary()\n-word_index = dict(zip(voc, range(len(voc))))\n-\n-\"\"\"\n-As you can see, we obtain the same encoding as above for our test sentence:\n-\"\"\"\n-\n-test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n-[word_index[w] for w in test]\n-\n-\"\"\"\n-## Load pre-trained word embeddings\n-\"\"\"\n-\n-\"\"\"\n-Let's download pre-trained GloVe embeddings (a 822M zip file).\n-\n-You'll need to run the following commands:\n-\"\"\"\n-\n-\"\"\"shell\n-wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n-unzip -q glove.6B.zip\n-\"\"\"\n-\n-\"\"\"\n-The archive contains text-encoded vectors of various sizes: 50-dimensional,\n-100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n-\n-Let's make a dict mapping words (strings) to their NumPy vector representation:\n-\"\"\"\n-\n-path_to_glove_file = \"glove.6B.100d.txt\"\n-\n-embeddings_index = {}\n-with open(path_to_glove_file) as f:\n-    for line in f:\n-        word, coefs = line.split(maxsplit=1)\n-        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n-        embeddings_index[word] = coefs\n-\n-print(\"Found %s word vectors.\" % len(embeddings_index))\n-\n-\"\"\"\n-Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n-`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n-vector for the word of index `i` in our `vectorizer`'s vocabulary.\n-\"\"\"\n-\n-num_tokens = len(voc) + 2\n-embedding_dim = 100\n-hits = 0\n-misses = 0\n-\n-# Prepare embedding matrix\n-embedding_matrix = np.zeros((num_tokens, embedding_dim))\n-for word, i in word_index.items():\n-    embedding_vector = embeddings_index.get(word)\n-    if embedding_vector is not None:\n-        # Words not found in embedding index will be all-zeros.\n-        # This includes the representation for \"padding\" and \"OOV\"\n-        embedding_matrix[i] = embedding_vector\n-        hits += 1\n-    else:\n-        misses += 1\n-print(\"Converted %d words (%d misses)\" % (hits, misses))\n-\n-\n-\"\"\"\n-Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n-\n-Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n-update them during training).\n-\"\"\"\n-\n-from keras.layers import Embedding\n-\n-embedding_layer = Embedding(\n-    num_tokens,\n-    embedding_dim,\n-    trainable=False,\n-)\n-embedding_layer.build((1,))\n-embedding_layer.set_weights([embedding_matrix])\n-\n-\"\"\"\n-## Build the model\n-\n-A simple 1D convnet with global max pooling and a classifier at the end.\n-\"\"\"\n-\n-int_sequences_input = keras.Input(shape=(None,), dtype=\"int32\")\n-embedded_sequences = embedding_layer(int_sequences_input)\n-x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n-x = layers.MaxPooling1D(5)(x)\n-x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n-x = layers.MaxPooling1D(5)(x)\n-x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n-x = layers.GlobalMaxPooling1D()(x)\n-x = layers.Dense(128, activation=\"relu\")(x)\n-x = layers.Dropout(0.5)(x)\n-preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n-model = keras.Model(int_sequences_input, preds)\n-model.summary()\n-\n-\"\"\"\n-## Train the model\n-\n-First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n-are right-padded.\n-\"\"\"\n-\n-x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n-x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n-\n-y_train = np.array(train_labels)\n-y_val = np.array(val_labels)\n-\n-\"\"\"\n-We use categorical crossentropy as our loss since we're doing softmax classification.\n-Moreover, we use `sparse_categorical_crossentropy` since our labels are integers.\n-\"\"\"\n-\n-model.compile(\n-    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n-)\n-model.fit(\n-    x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val)\n-)\n-\n-\"\"\"\n-## Export an end-to-end model\n-\n-Now, we may want to export a `Model` object that takes as input a string of arbitrary\n-length, rather than a sequence of indices. It would make the model much more portable,\n-since you wouldn't have to worry about the input preprocessing pipeline.\n-\n-Our `vectorizer` is actually a Keras layer, so it's simple:\n-\"\"\"\n-\n-string_input = keras.Input(shape=(1,), dtype=\"string\")\n-x = vectorizer(string_input)\n-preds = model(x)\n-end_to_end_model = keras.Model(string_input, preds)\n-\n-probabilities = end_to_end_model(\n-    keras.ops.convert_to_tensor(\n-        [[\"this message is about computer graphics and 3D modeling\"]]\n-    )\n-)\n-\n-print(class_names[np.argmax(probabilities[0])])\n\n@@ -1,279 +0,0 @@\n-\"\"\"\n-Title: Text classification from scratch\n-Authors: Mark Omernick, Francois Chollet\n-Date created: 2019/11/06\n-Last modified: 2020/05/17\n-Description: Text sentiment classification starting from raw text files.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to do text classification starting from raw text (as\n-a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n-classification dataset (unprocessed version). We use the `TextVectorization` layer for\n- word splitting & indexing.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-from keras.layers import TextVectorization\n-from keras import layers\n-import string\n-import re\n-import os\n-from pathlib import Path\n-\n-\"\"\"\n-## Load the data: IMDB movie review sentiment classification\n-\n-Let's download the data and inspect its structure.\n-\"\"\"\n-\n-fpath = keras.utils.get_file(\n-    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n-)\n-dirpath = Path(fpath).parent.absolute()\n-os.system(f\"tar -xf {fpath} -C {dirpath}\")\n-\n-\n-\"\"\"\n-The `aclImdb` folder contains a `train` and `test` subfolder:\n-\"\"\"\n-\n-os.system(f\"ls {dirpath}/aclImdb\")\n-os.system(f\"ls {dirpath}/aclImdb/train\")\n-os.system(f\"ls {dirpath}/aclImdb/test\")\n-\n-\"\"\"\n-The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n- which represents one review (either positive or negative):\n-\"\"\"\n-\n-os.system(f\"cat {dirpath}/aclImdb/train/pos/6248_7.txt\")\n-\n-\"\"\"\n-We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:\n-\"\"\"\n-\n-os.system(f\"rm -r {dirpath}/aclImdb/train/unsup\")\n-\n-\n-\"\"\"\n-You can use the utility `keras.utils.text_dataset_from_directory` to\n-generate a labeled `tf.data.Dataset` object from a set of text files on disk filed\n- into class-specific folders.\n-\n-Let's use it to generate the training, validation, and test datasets. The validation\n-and training datasets are generated from two subsets of the `train` directory, with 20%\n-of samples going to the validation dataset and 80% going to the training dataset.\n-\n-Having a validation dataset in addition to the test dataset is useful for tuning\n-hyperparameters, such as the model architecture, for which the test dataset should not\n-be used.\n-\n-Before putting the model out into the real world however, it should be retrained using all\n-available training data (without creating a validation dataset), so its performance is maximized.\n-\n-When using the `validation_split` & `subset` arguments, make sure to either specify a\n-random seed, or to pass `shuffle=False`, so that the validation & training splits you\n-get have no overlap.\n-\"\"\"\n-\n-batch_size = 32\n-raw_train_ds, raw_val_ds = keras.utils.text_dataset_from_directory(\n-    f\"{dirpath}/aclImdb/train\",\n-    batch_size=batch_size,\n-    validation_split=0.2,\n-    subset=\"both\",\n-    seed=1337,\n-)\n-raw_test_ds = keras.utils.text_dataset_from_directory(\n-    f\"{dirpath}/aclImdb/test\", batch_size=batch_size\n-)\n-\n-print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n-print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n-print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n-\n-\"\"\"\n-Let's preview a few samples:\n-\"\"\"\n-\n-# It's important to take a look at your raw data to ensure your normalization\n-# and tokenization will work as expected. We can do that by taking a few\n-# examples from the training set and looking at them.\n-# This is one of the places where eager execution shines:\n-# we can just evaluate these tensors using .numpy()\n-# instead of needing to evaluate them in a Session/Graph context.\n-for text_batch, label_batch in raw_train_ds.take(1):\n-    for i in range(5):\n-        print(text_batch.numpy()[i])\n-        print(label_batch.numpy()[i])\n-\n-\"\"\"\n-## Prepare the data\n-\n-In particular, we remove `<br />` tags.\n-\"\"\"\n-\n-\n-# Having looked at our data above, we see that the raw text contains HTML break\n-# tags of the form '<br />'. These tags will not be removed by the default\n-# standardizer (which doesn't strip HTML). Because of this, we will need to\n-# create a custom standardization function.\n-def custom_standardization(input_data):\n-    lowercase = tf.strings.lower(input_data)\n-    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n-    return tf.strings.regex_replace(\n-        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n-    )\n-\n-\n-# Model constants.\n-max_features = 20000\n-embedding_dim = 128\n-sequence_length = 500\n-\n-# Now that we have our custom standardization, we can instantiate our text\n-# vectorization layer. We are using this layer to normalize, split, and map\n-# strings to integers, so we set our 'output_mode' to 'int'.\n-# Note that we're using the default split function,\n-# and the custom standardization defined above.\n-# We also set an explicit maximum sequence length, since the CNNs later in our\n-# model won't support ragged sequences.\n-vectorize_layer = TextVectorization(\n-    standardize=custom_standardization,\n-    max_tokens=max_features,\n-    output_mode=\"int\",\n-    output_sequence_length=sequence_length,\n-)\n-\n-# Now that the vocab layer has been created, call `adapt` on a text-only\n-# dataset to create the vocabulary. You don't have to batch, but for very large\n-# datasets this means you're not keeping spare copies of the dataset in memory.\n-\n-# Let's make a text-only dataset (no labels):\n-text_ds = raw_train_ds.map(lambda x, y: x)\n-# Let's call `adapt`:\n-vectorize_layer.adapt(text_ds)\n-\n-\"\"\"\n-## Two options to vectorize the data\n-\n-There are 2 ways we can use our text vectorization layer:\n-\n-**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n-strings, like this:\n-\"\"\"\n-\n-\"\"\"\n-\n-```python\n-text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n-x = vectorize_layer(text_input)\n-x = layers.Embedding(max_features + 1, embedding_dim)(x)\n-...\n-```\n-\n-**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n- feed it into a model that expects integer sequences as inputs.\n-\n-An important difference between the two is that option 2 enables you to do\n-**asynchronous CPU processing and buffering** of your data when training on GPU.\n-So if you're training the model on GPU, you probably want to go with this option to get\n-the best performance. This is what we will do below.\n-\n-If we were to export our model to production, we'd ship a model that accepts raw\n-strings as input, like in the code snippet for option 1 above. This can be done after\n-training. We do this in the last section.\n-\"\"\"\n-\n-\n-def vectorize_text(text, label):\n-    text = tf.expand_dims(text, -1)\n-    return vectorize_layer(text), label\n-\n-\n-# Vectorize the data.\n-train_ds = raw_train_ds.map(vectorize_text)\n-val_ds = raw_val_ds.map(vectorize_text)\n-test_ds = raw_test_ds.map(vectorize_text)\n-\n-# Do async prefetching / buffering of the data for best performance on GPU.\n-train_ds = train_ds.cache().prefetch(buffer_size=10)\n-val_ds = val_ds.cache().prefetch(buffer_size=10)\n-test_ds = test_ds.cache().prefetch(buffer_size=10)\n-\n-\"\"\"\n-## Build a model\n-\n-We choose a simple 1D convnet starting with an `Embedding` layer.\n-\"\"\"\n-\n-# A integer input for vocab indices.\n-inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n-# Next, we add a layer to map those vocab indices into a space of dimensionality\n-# 'embedding_dim'.\n-x = keras.layers.Embedding(max_features, embedding_dim)(inputs)\n-x = layers.Dropout(0.5)(x)\n-\n-# Conv1D + global max pooling\n-x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n-x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n-x = layers.GlobalMaxPooling1D()(x)\n-\n-# We add a vanilla hidden layer:\n-x = layers.Dense(128, activation=\"relu\")(x)\n-x = layers.Dropout(0.5)(x)\n-\n-# We project onto a single unit output layer, and squash it with a sigmoid:\n-predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n-model = keras.Model(inputs, predictions)\n-model.summary()\n-model.compile(\n-    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 3\n-\n-# Fit the model using the train and test datasets.\n-model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n-\n-\"\"\"\n-## Evaluate the model on the test set\n-\"\"\"\n-\n-model.evaluate(test_ds)\n-\n-\"\"\"\n-## Make an end-to-end model\n-\n-If you want to obtain a model capable of processing raw strings, you can simply\n-create a new model (using the weights we just trained):\n-\"\"\"\n-\n-# A string input\n-inputs = keras.Input(shape=(1,), dtype=\"string\")\n-# Turn strings into vocab indices\n-indices = vectorize_layer(inputs)\n-# Turn vocab indices into predictions\n-outputs = model(indices)\n-\n-# Our end to end model\n-end_to_end_model = keras.Model(inputs, outputs)\n-end_to_end_model.compile(\n-    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-\n-# Test it with `raw_test_ds`, which yields raw strings\n-end_to_end_model.evaluate(raw_test_ds)\n\n@@ -1,192 +0,0 @@\n-\"\"\"\n-Title: Actor Critic Method\n-Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n-Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 2020/05/13\n-Last modified: 2023/07/19\n-Description: Implement Actor Critic Method in CartPole environment.\n-Accelerator: NONE\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n-\n-### Actor Critic Method\n-\n-As an agent takes actions and moves through an environment, it learns to map\n-the observed state of the environment to two possible outputs:\n-\n-1. Recommended action: A probability value for each action in the action space.\n-   The part of the agent responsible for this output is called the **actor**.\n-2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n-   future. The part of the agent responsible for this output is the **critic**.\n-\n-Agent and Critic learn to perform their tasks, such that the recommended actions\n-from the actor maximize the rewards.\n-\n-### CartPole-V1\n-\n-A pole is attached to a cart placed on a frictionless track. The agent has to apply\n-force to move the cart. It is rewarded for every time step the pole\n-remains upright. The agent, therefore, must learn to keep the pole from falling over.\n-\n-### References\n-\n-- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n-- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-from keras import layers\n-\n-import gym\n-import numpy as np\n-import tensorflow as tf\n-\n-\n-# Configuration parameters for the whole setup\n-seed = 42\n-gamma = 0.99  # Discount factor for past rewards\n-max_steps_per_episode = 10000\n-env = gym.make(\"CartPole-v1\", new_step_api=True)  # Create the environment\n-env.reset(seed=seed)\n-eps = np.finfo(\n-    np.float32\n-).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n-\n-\"\"\"\n-## Implement Actor Critic network\n-\n-This network learns two functions:\n-\n-1. Actor: This takes as input the state of our environment and returns a\n-probability value for each action in its action space.\n-2. Critic: This takes as input the state of our environment and returns\n-an estimate of total rewards in the future.\n-\n-In our implementation, they share the initial layer.\n-\"\"\"\n-\n-num_inputs = 4\n-num_actions = 2\n-num_hidden = 128\n-\n-inputs = layers.Input(shape=(num_inputs,))\n-common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n-action = layers.Dense(num_actions, activation=\"softmax\")(common)\n-critic = layers.Dense(1)(common)\n-\n-model = keras.Model(inputs=inputs, outputs=[action, critic])\n-\n-\"\"\"\n-## Train\n-\"\"\"\n-\n-optimizer = keras.optimizers.Adam(learning_rate=0.01)\n-huber_loss = keras.losses.Huber()\n-action_probs_history = []\n-critic_value_history = []\n-rewards_history = []\n-running_reward = 0\n-episode_count = 0\n-\n-while True:  # Run until solved\n-    state = env.reset()\n-    episode_reward = 0\n-    with tf.GradientTape() as tape:\n-        for timestep in range(1, max_steps_per_episode):\n-            # env.render(); Adding this line would show the attempts\n-            # of the agent in a pop up window.\n-\n-            state = tf.convert_to_tensor(state)\n-            state = tf.expand_dims(state, 0)\n-\n-            # Predict action probabilities and estimated future rewards\n-            # from environment state\n-            action_probs, critic_value = model(state)\n-            critic_value_history.append(critic_value[0, 0])\n-\n-            # Sample action from action probability distribution\n-            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n-            action_probs_history.append(tf.math.log(action_probs[0, action]))\n-\n-            # Apply the sampled action in our environment\n-            state, reward, done, _, _ = env.step(action)\n-            rewards_history.append(reward)\n-            episode_reward += reward\n-\n-            if done:\n-                break\n-\n-        # Update running reward to check condition for solving\n-        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n-\n-        # Calculate expected value from rewards\n-        # - At each timestep what was the total reward received after that timestep\n-        # - Rewards in the past are discounted by multiplying them with gamma\n-        # - These are the labels for our critic\n-        returns = []\n-        discounted_sum = 0\n-        for r in rewards_history[::-1]:\n-            discounted_sum = r + gamma * discounted_sum\n-            returns.insert(0, discounted_sum)\n-\n-        # Normalize\n-        returns = np.array(returns)\n-        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n-        returns = returns.tolist()\n-\n-        # Calculating loss values to update our network\n-        history = zip(action_probs_history, critic_value_history, returns)\n-        actor_losses = []\n-        critic_losses = []\n-        for log_prob, value, ret in history:\n-            # At this point in history, the critic estimated that we would get a\n-            # total reward = `value` in the future. We took an action with log probability\n-            # of `log_prob` and ended up recieving a total reward = `ret`.\n-            # The actor must be updated so that it predicts an action that leads to\n-            # high rewards (compared to critic's estimate) with high probability.\n-            diff = ret - value\n-            actor_losses.append(-log_prob * diff)  # actor loss\n-\n-            # The critic must be updated so that it predicts a better estimate of\n-            # the future rewards.\n-            critic_losses.append(\n-                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n-            )\n-\n-        # Backpropagation\n-        loss_value = sum(actor_losses) + sum(critic_losses)\n-        grads = tape.gradient(loss_value, model.trainable_variables)\n-        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n-\n-        # Clear the loss and reward history\n-        action_probs_history.clear()\n-        critic_value_history.clear()\n-        rewards_history.clear()\n-\n-    # Log details\n-    episode_count += 1\n-    if episode_count % 10 == 0:\n-        template = \"running reward: {:.2f} at episode {}\"\n-        print(template.format(running_reward, episode_count))\n-\n-    if running_reward > 195:  # Condition to consider the task solved\n-        print(\"Solved at episode {}!\".format(episode_count))\n-        break\n-\"\"\"\n-## Visualizations\n-In early stages of training:\n-![Imgur](https://i.imgur.com/5gCs5kH.gif)\n-\n-In later stages of training:\n-![Imgur](https://i.imgur.com/5ziiZUD.gif)\n-\"\"\"\n\n@@ -1,157 +0,0 @@\n-\"\"\"\n-Title: Imbalanced classification: credit card fraud detection\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2019/05/28\n-Last modified: 2020/04/17\n-Description: Demonstration of how to handle highly imbalanced classification problems.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example looks at the\n-[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)\n-dataset to demonstrate how\n-to train a classification model on data with highly imbalanced classes.\n-\"\"\"\n-\n-\"\"\"\n-## First, vectorize the CSV data\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-\n-# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n-fname = \"/Users/fchollet/Downloads/creditcard.csv\"\n-\n-all_features = []\n-all_targets = []\n-with open(fname) as f:\n-    for i, line in enumerate(f):\n-        if i == 0:\n-            print(\"HEADER:\", line.strip())\n-            continue  # Skip header\n-        fields = line.strip().split(\",\")\n-        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n-        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n-        if i == 1:\n-            print(\"EXAMPLE FEATURES:\", all_features[-1])\n-\n-features = np.array(all_features, dtype=\"float32\")\n-targets = np.array(all_targets, dtype=\"uint8\")\n-print(\"features.shape:\", features.shape)\n-print(\"targets.shape:\", targets.shape)\n-\n-\"\"\"\n-## Prepare a validation set\n-\"\"\"\n-\n-num_val_samples = int(len(features) * 0.2)\n-train_features = features[:-num_val_samples]\n-train_targets = targets[:-num_val_samples]\n-val_features = features[-num_val_samples:]\n-val_targets = targets[-num_val_samples:]\n-\n-print(\"Number of training samples:\", len(train_features))\n-print(\"Number of validation samples:\", len(val_features))\n-\n-\"\"\"\n-## Analyze class imbalance in the targets\n-\"\"\"\n-\n-counts = np.bincount(train_targets[:, 0])\n-print(\n-    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n-        counts[1], 100 * float(counts[1]) / len(train_targets)\n-    )\n-)\n-\n-weight_for_0 = 1.0 / counts[0]\n-weight_for_1 = 1.0 / counts[1]\n-\n-\"\"\"\n-## Normalize the data using training set statistics\n-\"\"\"\n-\n-mean = np.mean(train_features, axis=0)\n-train_features -= mean\n-val_features -= mean\n-std = np.std(train_features, axis=0)\n-train_features /= std\n-val_features /= std\n-\n-\"\"\"\n-## Build a binary classification model\n-\"\"\"\n-\n-model = keras.Sequential(\n-    [\n-        keras.layers.Dense(\n-            256, activation=\"relu\", input_shape=(train_features.shape[-1],)\n-        ),\n-        keras.layers.Dense(256, activation=\"relu\"),\n-        keras.layers.Dropout(0.3),\n-        keras.layers.Dense(256, activation=\"relu\"),\n-        keras.layers.Dropout(0.3),\n-        keras.layers.Dense(1, activation=\"sigmoid\"),\n-    ]\n-)\n-model.summary()\n-\n-\"\"\"\n-## Train the model with `class_weight` argument\n-\"\"\"\n-\n-metrics = [\n-    keras.metrics.FalseNegatives(name=\"fn\"),\n-    keras.metrics.FalsePositives(name=\"fp\"),\n-    keras.metrics.TrueNegatives(name=\"tn\"),\n-    keras.metrics.TruePositives(name=\"tp\"),\n-    keras.metrics.Precision(name=\"precision\"),\n-    keras.metrics.Recall(name=\"recall\"),\n-]\n-\n-model.compile(\n-    optimizer=keras.optimizers.Adam(1e-2),\n-    loss=\"binary_crossentropy\",\n-    metrics=metrics,\n-)\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")\n-]\n-class_weight = {0: weight_for_0, 1: weight_for_1}\n-\n-model.fit(\n-    train_features,\n-    train_targets,\n-    batch_size=2048,\n-    epochs=30,\n-    verbose=2,\n-    callbacks=callbacks,\n-    validation_data=(val_features, val_targets),\n-    class_weight=class_weight,\n-)\n-\n-\"\"\"\n-## Conclusions\n-\n-At the end of training, out of 56,961 validation transactions, we are:\n-\n-- Correctly identifying 66 of them as fraudulent\n-- Missing 9 fraudulent transactions\n-- At the cost of incorrectly flagging 441 legitimate transactions\n-\n-In the real world, one would put an even higher weight on class 1,\n-so as to reflect that False Negatives are more costly than False Positives.\n-\n-Next time your credit card gets  declined in an online purchase -- this is why.\n-\n-Example available on HuggingFace.\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/🤗%20Model-Imbalanced%20Classification-black.svg)](https://huggingface.co/keras-io/imbalanced_classification) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-Imbalanced%20Classification-black.svg)](https://huggingface.co/spaces/keras-io/Credit_Card_Fraud_Detection) |\n-\n-\"\"\"\n\n@@ -1,567 +0,0 @@\n-\"\"\"\n-Title: A Transformer-based recommendation system\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2020/12/30\n-Last modified: 2023/10/25\n-Description: Rating rate prediction using the Behavior Sequence Transformer (BST) model on the Movielens.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates the [Behavior Sequence Transformer (BST)](https://arxiv.org/abs/1905.06874)\n-model, by Qiwei Chen et al., using the [Movielens dataset](https://grouplens.org/datasets/movielens/).\n-The BST model leverages the sequential behaviour of the users in watching and rating movies,\n-as well as user profile and movie features, to predict the rating of the user to a target movie.\n-\n-More precisely, the BST model aims to predict the rating of a target movie by accepting\n-the following inputs:\n-\n-1. A fixed-length *sequence* of `movie_ids` watched by a user.\n-2. A fixed-length *sequence* of the `ratings` for the movies watched by a user.\n-3. A *set* of user features, including `user_id`, `sex`, `occupation`, and `age_group`.\n-4. A *set* of `genres` for each movie in the input sequence and the target movie.\n-5. A `target_movie_id` for which to predict the rating.\n-\n-This example modifies the original BST model in the following ways:\n-\n-1. We incorporate the movie features (genres) into the processing of the embedding of each\n-movie of the input sequence and the target movie, rather than treating them as \"other features\"\n-outside the transformer layer.\n-2. We utilize the ratings of movies in the input sequence, along with the their positions\n-in the sequence, to update them before feeding them into the self-attention layer.\n-\n-\n-Note that this example should be run with TensorFlow 2.4 or higher.\n-\"\"\"\n-\n-\"\"\"\n-## The dataset\n-\n-We use the [1M version of the Movielens dataset](https://grouplens.org/datasets/movielens/1m/).\n-The dataset includes around 1 million ratings from 6000 users on 4000 movies,\n-along with some user features, movie genres. In addition, the timestamp of each user-movie\n-rating is provided, which allows creating sequences of movie ratings for each user,\n-as expected by the BST model.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import math\n-from zipfile import ZipFile\n-from urllib.request import urlretrieve\n-import numpy as np\n-import pandas as pd\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-from keras.layers import StringLookup\n-\n-\n-\"\"\"\n-## Prepare the data\n-\n-### Download and prepare the DataFrames\n-\n-First, let's download the movielens data.\n-\n-The downloaded folder will contain three data files: `users.dat`, `movies.dat`,\n-and `ratings.dat`.\n-\"\"\"\n-\n-urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n-ZipFile(\"movielens.zip\", \"r\").extractall()\n-\n-\"\"\"\n-Then, we load the data into pandas DataFrames with their proper column names.\n-\"\"\"\n-\n-users = pd.read_csv(\n-    \"ml-1m/users.dat\",\n-    sep=\"::\",\n-    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n-)\n-\n-ratings = pd.read_csv(\n-    \"ml-1m/ratings.dat\",\n-    sep=\"::\",\n-    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n-)\n-\n-movies = pd.read_csv(\n-    \"ml-1m/movies.dat\", sep=\"::\", names=[\"movie_id\", \"title\", \"genres\"], encoding='latin-1'\n-)\n-\n-\"\"\"\n-Here, we do some simple data processing to fix the data types of the columns.\n-\"\"\"\n-\n-users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n-users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n-users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n-\n-movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n-\n-ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n-ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n-ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n-\n-\"\"\"\n-Each movie has multiple genres. We split them into separate columns in the `movies`\n-DataFrame.\n-\"\"\"\n-\n-genres = [\n-    \"Action\",\n-    \"Adventure\",\n-    \"Animation\",\n-    \"Children's\",\n-    \"Comedy\",\n-    \"Crime\",\n-    \"Documentary\",\n-    \"Drama\",\n-    \"Fantasy\",\n-    \"Film-Noir\",\n-    \"Horror\",\n-    \"Musical\",\n-    \"Mystery\",\n-    \"Romance\",\n-    \"Sci-Fi\",\n-    \"Thriller\",\n-    \"War\",\n-    \"Western\",\n-]\n-\n-for genre in genres:\n-    movies[genre] = movies[\"genres\"].apply(\n-        lambda values: int(genre in values.split(\"|\"))\n-    )\n-\n-\n-\"\"\"\n-### Transform the movie ratings data into sequences\n-\n-First, let's sort the the ratings data using the `unix_timestamp`, and then group the\n-`movie_id` values and the `rating` values by `user_id`.\n-\n-The output DataFrame will have a record for each `user_id`, with two ordered lists\n-(sorted by rating datetime): the movies they have rated, and their ratings of these movies.\n-\"\"\"\n-\n-ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n-\n-ratings_data = pd.DataFrame(\n-    data={\n-        \"user_id\": list(ratings_group.groups.keys()),\n-        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n-        \"ratings\": list(ratings_group.rating.apply(list)),\n-        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n-    }\n-)\n-\n-\n-\"\"\"\n-Now, let's split the `movie_ids` list into a set of sequences of a fixed length.\n-We do the same for the `ratings`. Set the `sequence_length` variable to change the length\n-of the input sequence to the model. You can also change the `step_size` to control the\n-number of sequences to generate for each user.\n-\"\"\"\n-\n-sequence_length = 4\n-step_size = 2\n-\n-\n-def create_sequences(values, window_size, step_size):\n-    sequences = []\n-    start_index = 0\n-    while True:\n-        end_index = start_index + window_size\n-        seq = values[start_index:end_index]\n-        if len(seq) < window_size:\n-            seq = values[-window_size:]\n-            if len(seq) == window_size:\n-                sequences.append(seq)\n-            break\n-        sequences.append(seq)\n-        start_index += step_size\n-    return sequences\n-\n-\n-ratings_data.movie_ids = ratings_data.movie_ids.apply(\n-    lambda ids: create_sequences(ids, sequence_length, step_size)\n-)\n-\n-ratings_data.ratings = ratings_data.ratings.apply(\n-    lambda ids: create_sequences(ids, sequence_length, step_size)\n-)\n-\n-del ratings_data[\"timestamps\"]\n-\n-\"\"\"\n-After that, we process the output to have each sequence in a separate records in\n-the DataFrame. In addition, we join the user features with the ratings data.\n-\"\"\"\n-\n-ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n-    \"movie_ids\", ignore_index=True\n-)\n-ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n-ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n-ratings_data_transformed = ratings_data_transformed.join(\n-    users.set_index(\"user_id\"), on=\"user_id\"\n-)\n-ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n-    lambda x: \",\".join(x)\n-)\n-ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n-    lambda x: \",\".join([str(v) for v in x])\n-)\n-\n-del ratings_data_transformed[\"zip_code\"]\n-\n-ratings_data_transformed.rename(\n-    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n-    inplace=True,\n-)\n-\n-\"\"\"\n-With `sequence_length` of 4 and `step_size` of 2, we end up with 498,623 sequences.\n-\n-Finally, we split the data into training and testing splits, with 85% and 15% of\n-the instances, respectively, and store them to CSV files.\n-\"\"\"\n-\n-random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n-train_data = ratings_data_transformed[random_selection]\n-test_data = ratings_data_transformed[~random_selection]\n-\n-train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n-test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)\n-\n-\"\"\"\n-## Define metadata\n-\"\"\"\n-\n-CSV_HEADER = list(ratings_data_transformed.columns)\n-\n-CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n-    \"user_id\": list(users.user_id.unique()),\n-    \"movie_id\": list(movies.movie_id.unique()),\n-    \"sex\": list(users.sex.unique()),\n-    \"age_group\": list(users.age_group.unique()),\n-    \"occupation\": list(users.occupation.unique()),\n-}\n-\n-USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n-\n-MOVIE_FEATURES = [\"genres\"]\n-\n-\"\"\"\n-## Create `tf.data.Dataset` for training and evaluation\n-\"\"\"\n-\n-\n-def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n-    def process(features):\n-        movie_ids_string = features[\"sequence_movie_ids\"]\n-        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n-\n-        # The last movie id in the sequence is the target movie.\n-        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n-        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n-\n-        ratings_string = features[\"sequence_ratings\"]\n-        sequence_ratings = tf.strings.to_number(\n-            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n-        ).to_tensor()\n-\n-        # The last rating in the sequence is the target for the model to predict.\n-        target = sequence_ratings[:, -1]\n-        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n-\n-        return features, target\n-\n-    dataset = tf.data.experimental.make_csv_dataset(\n-        csv_file_path,\n-        batch_size=batch_size,\n-        column_names=CSV_HEADER,\n-        num_epochs=1,\n-        header=False,\n-        field_delim=\"|\",\n-        shuffle=shuffle,\n-    ).map(process)\n-\n-    return dataset\n-\n-\n-\"\"\"\n-## Create model inputs\n-\"\"\"\n-\n-\n-def create_model_inputs():\n-    return {\n-        \"user_id\": layers.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n-        \"sequence_movie_ids\": layers.Input(\n-            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n-        ),\n-        \"target_movie_id\": layers.Input(\n-            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n-        ),\n-        \"sequence_ratings\": layers.Input(\n-            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=\"float32\"\n-        ),\n-        \"sex\": layers.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n-        \"age_group\": layers.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n-        \"occupation\": layers.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n-    }\n-\n-\n-\"\"\"\n-## Encode input features\n-\n-The `encode_input_features` method works as follows:\n-\n-1. Each categorical user feature is encoded using `layers.Embedding`, with embedding\n-dimension equals to the square root of the vocabulary size of the feature.\n-The embeddings of these features are concatenated to form a single input tensor.\n-\n-2. Each movie in the movie sequence and the target movie is encoded `layers.Embedding`,\n-where the dimension size is the square root of the number of movies.\n-\n-3. A multi-hot genres vector for each movie is concatenated with its embedding vector,\n-and processed using a non-linear `layers.Dense` to output a vector of the same movie\n-embedding dimensions.\n-\n-4. A positional embedding is added to each movie embedding in the sequence, and then\n-multiplied by its rating from the ratings sequence.\n-\n-5. The target movie embedding is concatenated to the sequence movie embeddings, producing\n-a tensor with the shape of `[batch size, sequence length, embedding size]`, as expected\n-by the attention layer for the transformer architecture.\n-\n-6. The method returns a tuple of two elements:  `encoded_transformer_features` and\n-`encoded_other_features`.\n-\"\"\"\n-\n-\n-def encode_input_features(\n-    inputs,\n-    include_user_id=True,\n-    include_user_features=True,\n-    include_movie_features=True,\n-):\n-    encoded_transformer_features = []\n-    encoded_other_features = []\n-\n-    other_feature_names = []\n-    if include_user_id:\n-        other_feature_names.append(\"user_id\")\n-    if include_user_features:\n-        other_feature_names.extend(USER_FEATURES)\n-\n-    ## Encode user features\n-    for feature_name in other_feature_names:\n-        # Convert the string input values into integer indices.\n-        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n-        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n-            inputs[feature_name]\n-        )\n-        # Compute embedding dimensions\n-        embedding_dims = int(math.sqrt(len(vocabulary)))\n-        # Create an embedding layer with the specified dimensions.\n-        embedding_encoder = layers.Embedding(\n-            input_dim=len(vocabulary),\n-            output_dim=embedding_dims,\n-            name=f\"{feature_name}_embedding\",\n-        )\n-        # Convert the index values to embedding representations.\n-        encoded_other_features.append(embedding_encoder(idx))\n-\n-    ## Create a single embedding vector for the user features\n-    if len(encoded_other_features) > 1:\n-        encoded_other_features = layers.concatenate(encoded_other_features)\n-    elif len(encoded_other_features) == 1:\n-        encoded_other_features = encoded_other_features[0]\n-    else:\n-        encoded_other_features = None\n-\n-    ## Create a movie embedding encoder\n-    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n-    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n-    # Create a lookup to convert string values to integer indices.\n-    movie_index_lookup = StringLookup(\n-        vocabulary=movie_vocabulary,\n-        mask_token=None,\n-        num_oov_indices=0,\n-        name=\"movie_index_lookup\",\n-    )\n-    # Create an embedding layer with the specified dimensions.\n-    movie_embedding_encoder = layers.Embedding(\n-        input_dim=len(movie_vocabulary),\n-        output_dim=movie_embedding_dims,\n-        name=f\"movie_embedding\",\n-    )\n-    # Create a vector lookup for movie genres.\n-    genre_vectors = movies[genres].to_numpy()\n-    movie_genres_lookup = layers.Embedding(\n-        input_dim=genre_vectors.shape[0],\n-        output_dim=genre_vectors.shape[1],\n-        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n-        trainable=False,\n-        name=\"genres_vector\",\n-    )\n-    # Create a processing layer for genres.\n-    movie_embedding_processor = layers.Dense(\n-        units=movie_embedding_dims,\n-        activation=\"relu\",\n-        name=\"process_movie_embedding_with_genres\",\n-    )\n-\n-    ## Define a function to encode a given movie id.\n-    def encode_movie(movie_id):\n-        # Convert the string input values into integer indices.\n-        movie_idx = movie_index_lookup(movie_id)\n-        movie_embedding = movie_embedding_encoder(movie_idx)\n-        encoded_movie = movie_embedding\n-        if include_movie_features:\n-            movie_genres_vector = movie_genres_lookup(movie_idx)\n-            encoded_movie = movie_embedding_processor(\n-                layers.concatenate([movie_embedding, movie_genres_vector])\n-            )\n-        return encoded_movie\n-\n-    ## Encoding target_movie_id\n-    target_movie_id = inputs[\"target_movie_id\"]\n-    encoded_target_movie = encode_movie(target_movie_id)\n-\n-    ## Encoding sequence movie_ids.\n-    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n-    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n-    # Create positional embedding.\n-    position_embedding_encoder = layers.Embedding(\n-        input_dim=sequence_length,\n-        output_dim=movie_embedding_dims,\n-        name=\"position_embedding\",\n-    )\n-    positions = keras.ops.arange(start=0, stop=sequence_length - 1, step=1)\n-    encodded_positions = position_embedding_encoder(positions)\n-    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n-    sequence_ratings = keras.ops.expand_dims(inputs[\"sequence_ratings\"], -1)\n-    # Add the positional encoding to the movie encodings and multiply them by rating.\n-    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n-        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n-    )\n-\n-    # Construct the transformer inputs.\n-    for encoded_movie in keras.ops.unstack(\n-        encoded_sequence_movies_with_poistion_and_rating, axis=1\n-    ):\n-        encoded_transformer_features.append(keras.ops.expand_dims(encoded_movie, 1))\n-    encoded_transformer_features.append(encoded_target_movie)\n-\n-    encoded_transformer_features = layers.concatenate(\n-        encoded_transformer_features, axis=1\n-    )\n-\n-    return encoded_transformer_features, encoded_other_features\n-\n-\n-\"\"\"\n-## Create a BST model\n-\"\"\"\n-\n-include_user_id = False\n-include_user_features = False\n-include_movie_features = False\n-\n-hidden_units = [256, 128]\n-dropout_rate = 0.1\n-num_heads = 3\n-\n-\n-def create_model():\n-    inputs = create_model_inputs()\n-    transformer_features, other_features = encode_input_features(\n-        inputs, include_user_id, include_user_features, include_movie_features\n-    )\n-\n-    # Create a multi-headed attention layer.\n-    attention_output = layers.MultiHeadAttention(\n-        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n-    )(transformer_features, transformer_features)\n-\n-    # Transformer block.\n-    attention_output = layers.Dropout(dropout_rate)(attention_output)\n-    x1 = layers.Add()([transformer_features, attention_output])\n-    x1 = layers.LayerNormalization()(x1)\n-    x2 = layers.LeakyReLU()(x1)\n-    x2 = layers.Dense(units=x2.shape[-1])(x2)\n-    x2 = layers.Dropout(dropout_rate)(x2)\n-    transformer_features = layers.Add()([x1, x2])\n-    transformer_features = layers.LayerNormalization()(transformer_features)\n-    features = layers.Flatten()(transformer_features)\n-\n-    # Included the other features.\n-    if other_features is not None:\n-        features = layers.concatenate(\n-            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n-        )\n-\n-    # Fully-connected layers.\n-    for num_units in hidden_units:\n-        features = layers.Dense(num_units)(features)\n-        features = layers.BatchNormalization()(features)\n-        features = layers.LeakyReLU()(features)\n-        features = layers.Dropout(dropout_rate)(features)\n-\n-    outputs = layers.Dense(units=1)(features)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-model = create_model()\n-\n-\"\"\"\n-## Run training and evaluation experiment\n-\"\"\"\n-\n-# Compile the model.\n-model.compile(\n-    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n-    loss=keras.losses.MeanSquaredError(),\n-    metrics=[keras.metrics.MeanAbsoluteError()],\n-)\n-\n-# Read the training data.\n-train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n-\n-# Fit the model with the training data.\n-model.fit(train_dataset, epochs=5)\n-\n-# Read the test data.\n-test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n-\n-# Evaluate the model on the test data.\n-_, rmse = model.evaluate(test_dataset, verbose=0)\n-print(f\"Test MAE: {round(rmse, 3)}\")\n-\n-\"\"\"\n-You should achieve a Mean Absolute Error (MAE) at or around 0.7 on the test data.\n-\"\"\"\n-\n-\"\"\"\n-## Conclusion\n-\n-The BST model uses the Transformer layer in its architecture to capture the sequential signals underlying\n-users’ behavior sequences for recommendation.\n-\n-You can try training this model with different configurations, for example, by increasing\n-the input sequence length and training the model for a larger number of epochs. In addition,\n-you can try including other features like movie release year and customer\n-zipcode, and including cross features like sex X genre.\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,352 +0,0 @@\n-\"\"\"\n-Title: Structured data classification from scratch\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/06/09\n-Last modified: 2020/06/09\n-Description: Binary classification of structured data including numerical and categorical features.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to do structured data classification, starting from a raw\n-CSV file. Our data includes both numerical and categorical features. We will use Keras\n-preprocessing layers to normalize the numerical features and vectorize the categorical\n-ones.\n-\n-Note that this example should be run with TensorFlow 2.5 or higher.\n-\n-### The dataset\n-\n-[Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\n-Cleveland Clinic Foundation for Heart Disease.\n-It's a CSV file with 303 rows. Each row contains information about a patient (a\n-**sample**), and each column describes an attribute of the patient (a **feature**). We\n-use the features to predict whether a patient has a heart disease (**binary\n-classification**).\n-\n-Here's the description of each feature:\n-\n-Column| Description| Feature Type\n-------------|--------------------|----------------------\n-Age | Age in years | Numerical\n-Sex | (1 = male; 0 = female) | Categorical\n-CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n-Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n-Chol | Serum cholesterol in mg/dl | Numerical\n-FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n-RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n-Thalach | Maximum heart rate achieved | Numerical\n-Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n-Oldpeak | ST depression induced by exercise relative to rest | Numerical\n-Slope | Slope of the peak exercise ST segment | Numerical\n-CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n-Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n-Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-# TensorFlow is the only backend that supports string inputs.\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import pandas as pd\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Preparing the data\n-\n-Let's download the data and load it into a Pandas dataframe:\n-\"\"\"\n-\n-file_url = (\n-    \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n-)\n-dataframe = pd.read_csv(file_url)\n-\n-\"\"\"\n-The dataset includes 303 samples with 14 columns per sample (13 features, plus the target\n-label):\n-\"\"\"\n-\n-dataframe.shape\n-\n-\"\"\"\n-Here's a preview of a few samples:\n-\"\"\"\n-\n-dataframe.head()\n-\n-\"\"\"\n-The last column, \"target\", indicates whether the patient has a heart disease (1) or not\n-(0).\n-\n-Let's split the data into a training and validation set:\n-\"\"\"\n-\n-val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n-train_dataframe = dataframe.drop(val_dataframe.index)\n-\n-print(\n-    f\"Using {len(train_dataframe)} samples for training \"\n-    f\"and {len(val_dataframe)} for validation\"\n-)\n-\n-\"\"\"\n-Let's generate `tf.data.Dataset` objects for each dataframe:\n-\"\"\"\n-\n-\n-def dataframe_to_dataset(dataframe):\n-    dataframe = dataframe.copy()\n-    labels = dataframe.pop(\"target\")\n-    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n-    ds = ds.shuffle(buffer_size=len(dataframe))\n-    return ds\n-\n-\n-train_ds = dataframe_to_dataset(train_dataframe)\n-val_ds = dataframe_to_dataset(val_dataframe)\n-\n-\"\"\"\n-Each `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\n-and `target` is the value `0` or `1`:\n-\"\"\"\n-\n-for x, y in train_ds.take(1):\n-    print(\"Input:\", x)\n-    print(\"Target:\", y)\n-\n-\"\"\"\n-Let's batch the datasets:\n-\"\"\"\n-\n-train_ds = train_ds.batch(32)\n-val_ds = val_ds.batch(32)\n-\n-\"\"\"\n-## Feature preprocessing with Keras layers\n-\n-\n-The following features are categorical features encoded as integers:\n-\n-- `sex`\n-- `cp`\n-- `fbs`\n-- `restecg`\n-- `exang`\n-- `ca`\n-\n-We will encode these features using **one-hot encoding**. We have two options\n-here:\n-\n- - Use `CategoryEncoding()`, which requires knowing the range of input values\n- and will error on input outside the range.\n- - Use `IntegerLookup()` which will build a lookup table for inputs and reserve\n- an output index for unkown input values.\n-\n-For this example, we want a simple solution that will handle out of range inputs\n-at inference, so we will use `IntegerLookup()`.\n-\n-We also have a categorical feature encoded as a string: `thal`. We will create an\n-index of all possible features and encode output using the `StringLookup()` layer.\n-\n-Finally, the following feature are continuous numerical features:\n-\n-- `age`\n-- `trestbps`\n-- `chol`\n-- `thalach`\n-- `oldpeak`\n-- `slope`\n-\n-For each of these features, we will use a `Normalization()` layer to make sure the mean\n-of each feature is 0 and its standard deviation is 1.\n-\n-Below, we define 3 utility functions to do the operations:\n-\n-- `encode_numerical_feature` to apply featurewise normalization to numerical features.\n-- `encode_string_categorical_feature` to first turn string inputs into integer indices,\n-then one-hot encode these integer indices.\n-- `encode_integer_categorical_feature` to one-hot encode integer categorical features.\n-\"\"\"\n-\n-\n-def encode_numerical_feature(feature, name, dataset):\n-    # Create a Normalization layer for our feature\n-    normalizer = layers.Normalization()\n-\n-    # Prepare a Dataset that only yields our feature\n-    feature_ds = dataset.map(lambda x, y: x[name])\n-    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n-\n-    # Learn the statistics of the data\n-    normalizer.adapt(feature_ds)\n-\n-    # Normalize the input feature\n-    encoded_feature = normalizer(feature)\n-    return encoded_feature\n-\n-\n-def encode_categorical_feature(feature, name, dataset, is_string):\n-    lookup_class = layers.StringLookup if is_string else layers.IntegerLookup\n-    # Create a lookup layer which will turn strings into integer indices\n-    lookup = lookup_class(output_mode=\"binary\")\n-\n-    # Prepare a Dataset that only yields our feature\n-    feature_ds = dataset.map(lambda x, y: x[name])\n-    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n-\n-    # Learn the set of possible string values and assign them a fixed integer index\n-    lookup.adapt(feature_ds)\n-\n-    # Turn the string input into integer indices\n-    encoded_feature = lookup(feature)\n-    return encoded_feature\n-\n-\n-\"\"\"\n-## Build a model\n-\n-With this done, we can create our end-to-end model:\n-\"\"\"\n-\n-# Categorical features encoded as integers\n-sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n-cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n-fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n-restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n-exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n-ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n-\n-# Categorical feature encoded as string\n-thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n-\n-# Numerical features\n-age = keras.Input(shape=(1,), name=\"age\")\n-trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n-chol = keras.Input(shape=(1,), name=\"chol\")\n-thalach = keras.Input(shape=(1,), name=\"thalach\")\n-oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n-slope = keras.Input(shape=(1,), name=\"slope\")\n-\n-all_inputs = [\n-    sex,\n-    cp,\n-    fbs,\n-    restecg,\n-    exang,\n-    ca,\n-    thal,\n-    age,\n-    trestbps,\n-    chol,\n-    thalach,\n-    oldpeak,\n-    slope,\n-]\n-\n-# Integer categorical features\n-sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n-cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n-fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n-restecg_encoded = encode_categorical_feature(\n-    restecg, \"restecg\", train_ds, False\n-)\n-exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n-ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n-\n-# String categorical features\n-thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n-\n-# Numerical features\n-age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n-trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n-chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n-thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n-oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n-slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n-\n-all_features = layers.concatenate(\n-    [\n-        sex_encoded,\n-        cp_encoded,\n-        fbs_encoded,\n-        restecg_encoded,\n-        exang_encoded,\n-        slope_encoded,\n-        ca_encoded,\n-        thal_encoded,\n-        age_encoded,\n-        trestbps_encoded,\n-        chol_encoded,\n-        thalach_encoded,\n-        oldpeak_encoded,\n-    ]\n-)\n-x = layers.Dense(32, activation=\"relu\")(all_features)\n-x = layers.Dropout(0.5)(x)\n-output = layers.Dense(1, activation=\"sigmoid\")(x)\n-model = keras.Model(all_inputs, output)\n-model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n-\n-\"\"\"\n-Let's visualize our connectivity graph:\n-\"\"\"\n-\n-# `rankdir='LR'` is to make the graph horizontal.\n-keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-model.fit(train_ds, epochs=50, validation_data=val_ds)\n-\n-\"\"\"\n-We quickly get to 80% validation accuracy.\n-\"\"\"\n-\n-\"\"\"\n-## Inference on new data\n-\n-To get a prediction for a new sample, you can simply call `model.predict()`. There are\n-just two things you need to do:\n-\n-1. wrap scalars into a list so as to have a batch dimension (models only process batches\n-of data, not single samples)\n-2. Call `convert_to_tensor` on each feature\n-\"\"\"\n-\n-sample = {\n-    \"age\": 60,\n-    \"sex\": 1,\n-    \"cp\": 1,\n-    \"trestbps\": 145,\n-    \"chol\": 233,\n-    \"fbs\": 1,\n-    \"restecg\": 2,\n-    \"thalach\": 150,\n-    \"exang\": 0,\n-    \"oldpeak\": 2.3,\n-    \"slope\": 3,\n-    \"ca\": 0,\n-    \"thal\": \"fixed\",\n-}\n-\n-input_dict = {\n-    name: tf.convert_to_tensor([value]) for name, value in sample.items()\n-}\n-predictions = model.predict(input_dict)\n-\n-print(\n-    f\"This particular patient had a {100 * predictions[0][0]:.1f} \"\n-    \"percent probability of having a heart disease, \"\n-    \"as evaluated by our model.\"\n-)\n\n@@ -1,388 +0,0 @@\n-\"\"\"\n-Title: Structured data classification with FeatureSpace\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2022/11/09\n-Last modified: 2022/11/09\n-Description: Classify tabular data in a few lines of code.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to do structured data classification\n-(also known as tabular data classification), starting from a raw\n-CSV file. Our data includes numerical features,\n-and integer categorical features, and string categorical features.\n-We will use the utility `keras.utils.FeatureSpace` to index,\n-preprocess, and encode our features.\n-\n-The code is adapted from the example\n-[Structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/).\n-While the previous example managed its own low-level feature preprocessing and\n-encoding with Keras preprocessing layers, in this example we\n-delegate everything to `FeatureSpace`, making the workflow\n-extremely quick and easy.\n-\n-Note that this example should be run with TensorFlow 2.12 or higher.\n-Before the release of TensorFlow 2.12, you can use `tf-nightly`.\n-\n-### The dataset\n-\n-[Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\n-Cleveland Clinic Foundation for Heart Disease.\n-It's a CSV file with 303 rows. Each row contains information about a patient (a\n-**sample**), and each column describes an attribute of the patient (a **feature**). We\n-use the features to predict whether a patient has a heart disease\n-(**binary classification**).\n-\n-Here's the description of each feature:\n-\n-Column| Description| Feature Type\n-------------|--------------------|----------------------\n-Age | Age in years | Numerical\n-Sex | (1 = male; 0 = female) | Categorical\n-CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n-Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n-Chol | Serum cholesterol in mg/dl | Numerical\n-FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n-RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n-Thalach | Maximum heart rate achieved | Numerical\n-Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n-Oldpeak | ST depression induced by exercise relative to rest | Numerical\n-Slope | Slope of the peak exercise ST segment | Numerical\n-CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n-Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n-Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import pandas as pd\n-import keras\n-from keras.utils import FeatureSpace\n-\n-\"\"\"\n-## Preparing the data\n-\n-Let's download the data and load it into a Pandas dataframe:\n-\"\"\"\n-\n-file_url = (\n-    \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n-)\n-dataframe = pd.read_csv(file_url)\n-\n-\"\"\"\n-The dataset includes 303 samples with 14 columns per sample\n-(13 features, plus the target label):\n-\"\"\"\n-\n-print(dataframe.shape)\n-\n-\"\"\"\n-Here's a preview of a few samples:\n-\"\"\"\n-\n-dataframe.head()\n-\n-\"\"\"\n-The last column, \"target\", indicates whether the patient\n-has a heart disease (1) or not (0).\n-\n-Let's split the data into a training and validation set:\n-\"\"\"\n-\n-val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n-train_dataframe = dataframe.drop(val_dataframe.index)\n-\n-print(\n-    \"Using %d samples for training and %d for validation\"\n-    % (len(train_dataframe), len(val_dataframe))\n-)\n-\n-\"\"\"\n-Let's generate `tf.data.Dataset` objects for each dataframe:\n-\"\"\"\n-\n-\n-def dataframe_to_dataset(dataframe):\n-    dataframe = dataframe.copy()\n-    labels = dataframe.pop(\"target\")\n-    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n-    ds = ds.shuffle(buffer_size=len(dataframe))\n-    return ds\n-\n-\n-train_ds = dataframe_to_dataset(train_dataframe)\n-val_ds = dataframe_to_dataset(val_dataframe)\n-\n-\"\"\"\n-Each `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\n-and `target` is the value `0` or `1`:\n-\"\"\"\n-\n-for x, y in train_ds.take(1):\n-    print(\"Input:\", x)\n-    print(\"Target:\", y)\n-\n-\"\"\"\n-Let's batch the datasets:\n-\"\"\"\n-\n-train_ds = train_ds.batch(32)\n-val_ds = val_ds.batch(32)\n-\n-\"\"\"\n-## Configuring a `FeatureSpace`\n-\n-To configure how each feature should be preprocessed,\n-we instantiate a `keras.utils.FeatureSpace`, and we\n-pass to it a dictionary that maps the name of our features\n-to a string that describes the feature type.\n-\n-We have a few \"integer categorical\" features such as `\"FBS\"`,\n-one \"string categorical\" feature (`\"thal\"`),\n-and a few numerical features, which we'd like to normalize\n--- except `\"age\"`, which we'd like to discretize into\n-a number of bins.\n-\n-We also use the `crosses` argument\n-to capture *feature interactions* for some categorical\n-features, that is to say, create additional features\n-that represent value co-occurrences for these categorical features.\n-You can compute feature crosses like this for arbitrary sets of\n-categorical features -- not just tuples of two features.\n-Because the resulting co-occurences are hashed\n-into a fixed-sized vector, you don't need to worry about whether\n-the co-occurence space is too large.\n-\"\"\"\n-\n-feature_space = FeatureSpace(\n-    features={\n-        # Categorical features encoded as integers\n-        \"sex\": \"integer_categorical\",\n-        \"cp\": \"integer_categorical\",\n-        \"fbs\": \"integer_categorical\",\n-        \"restecg\": \"integer_categorical\",\n-        \"exang\": \"integer_categorical\",\n-        \"ca\": \"integer_categorical\",\n-        # Categorical feature encoded as string\n-        \"thal\": \"string_categorical\",\n-        # Numerical features to discretize\n-        \"age\": \"float_discretized\",\n-        # Numerical features to normalize\n-        \"trestbps\": \"float_normalized\",\n-        \"chol\": \"float_normalized\",\n-        \"thalach\": \"float_normalized\",\n-        \"oldpeak\": \"float_normalized\",\n-        \"slope\": \"float_normalized\",\n-    },\n-    # We create additional features by hashing\n-    # value co-occurrences for the\n-    # following groups of categorical features.\n-    crosses=[(\"sex\", \"age\"), (\"thal\", \"ca\")],\n-    # The hashing space for these co-occurrences\n-    # wil be 32-dimensional.\n-    crossing_dim=32,\n-    # Our utility will one-hot encode all categorical\n-    # features and concat all features into a single\n-    # vector (one vector per sample).\n-    output_mode=\"concat\",\n-)\n-\n-\"\"\"\n-## Further customizing a `FeatureSpace`\n-\n-Specifying the feature type via a string name is quick and easy,\n-but sometimes you may want to further configure the preprocessing\n-of each feature. For instance, in our case, our categorical\n-features don't have a large set of possible values -- it's only\n-a handful of values per feature (e.g. `1` and `0` for the feature `\"FBS\"`),\n-and all possible values are represented in the training set.\n-As a result, we don't need to reserve an index to represent \"out of vocabulary\" values\n-for these features -- which would have been the default behavior.\n-Below, we just specify `num_oov_indices=0` in each of these features\n-to tell the feature preprocessor to skip \"out of vocabulary\" indexing.\n-\n-Other customizations you have access to include specifying the number of\n-bins for discretizing features of type `\"float_discretized\"`,\n-or the dimensionality of the hashing space for feature crossing.\n-\"\"\"\n-\n-feature_space = FeatureSpace(\n-    features={\n-        # Categorical features encoded as integers\n-        \"sex\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        \"cp\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        \"fbs\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        \"restecg\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        \"exang\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        \"ca\": FeatureSpace.integer_categorical(num_oov_indices=0),\n-        # Categorical feature encoded as string\n-        \"thal\": FeatureSpace.string_categorical(num_oov_indices=0),\n-        # Numerical features to discretize\n-        \"age\": FeatureSpace.float_discretized(num_bins=30),\n-        # Numerical features to normalize\n-        \"trestbps\": FeatureSpace.float_normalized(),\n-        \"chol\": FeatureSpace.float_normalized(),\n-        \"thalach\": FeatureSpace.float_normalized(),\n-        \"oldpeak\": FeatureSpace.float_normalized(),\n-        \"slope\": FeatureSpace.float_normalized(),\n-    },\n-    # Specify feature cross with a custom crossing dim.\n-    crosses=[\n-        FeatureSpace.cross(feature_names=(\"sex\", \"age\"), crossing_dim=64),\n-        FeatureSpace.cross(\n-            feature_names=(\"thal\", \"ca\"),\n-            crossing_dim=16,\n-        ),\n-    ],\n-    output_mode=\"concat\",\n-)\n-\n-\"\"\"\n-## Adapt the `FeatureSpace` to the training data\n-\n-Before we start using the `FeatureSpace` to build a model, we have\n-to adapt it to the training data. During `adapt()`, the `FeatureSpace` will:\n-\n-- Index the set of possible values for categorical features.\n-- Compute the mean and variance for numerical features to normalize.\n-- Compute the value boundaries for the different bins for numerical features to discretize.\n-\n-Note that `adapt()` should be called on a `tf.data.Dataset` which yields dicts\n-of feature values -- no labels.\n-\"\"\"\n-\n-train_ds_with_no_labels = train_ds.map(lambda x, _: x)\n-feature_space.adapt(train_ds_with_no_labels)\n-\n-\"\"\"\n-At this point, the `FeatureSpace` can be called on a dict of raw feature values, and will return a\n-single concatenate vector for each sample, combining encoded features and feature crosses.\n-\"\"\"\n-\n-for x, _ in train_ds.take(1):\n-    preprocessed_x = feature_space(x)\n-    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n-    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)\n-\n-\"\"\"\n-## Two ways to manage preprocessing: as part of the `tf.data` pipeline, or in the model itself\n-\n-There are two ways in which you can leverage your `FeatureSpace`:\n-\n-### Asynchronous preprocessing in `tf.data`\n-\n-You can make it part of your data pipeline, before the model. This enables asynchronous parallel\n-preprocessing of the data on CPU before it hits the model. Do this if you're training on GPU or TPU,\n-or if you want to speed up preprocessing. Usually, this is always the right thing to do during training.\n-\n-### Synchronous preprocessing in the model\n-\n-You can make it part of your model. This means that the model will expect dicts of raw feature\n-values, and the preprocessing batch will be done synchronously (in a blocking manner) before the\n-rest of the forward pass. Do this if you want to have an end-to-end model that can process\n-raw feature values -- but keep in mind that your model will only be able to run on CPU,\n-since most types of feature preprocessing (e.g. string preprocessing) are not GPU or TPU compatible.\n-\n-Do not do this on GPU / TPU or in performance-sensitive settings. In general, you want to do in-model\n-preprocessing when you do inference on CPU.\n-\n-In our case, we will apply the `FeatureSpace` in the tf.data pipeline during training, but we will\n-do inference with an end-to-end model that includes the `FeatureSpace`.\n-\"\"\"\n-\n-\"\"\"\n-Let's create a training and validation dataset of preprocessed batches:\n-\"\"\"\n-\n-preprocessed_train_ds = train_ds.map(\n-    lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE\n-)\n-preprocessed_train_ds = preprocessed_train_ds.prefetch(tf.data.AUTOTUNE)\n-\n-preprocessed_val_ds = val_ds.map(\n-    lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE\n-)\n-preprocessed_val_ds = preprocessed_val_ds.prefetch(tf.data.AUTOTUNE)\n-\n-\"\"\"\n-## Build a model\n-\n-Time to build a model -- or rather two models:\n-\n-- A training model that expects preprocessed features (one sample = one vector)\n-- An inference model that expects raw features (one sample = dict of raw feature values)\n-\"\"\"\n-\n-dict_inputs = feature_space.get_inputs()\n-encoded_features = feature_space.get_encoded_features()\n-\n-x = keras.layers.Dense(32, activation=\"relu\")(encoded_features)\n-x = keras.layers.Dropout(0.5)(x)\n-predictions = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n-\n-training_model = keras.Model(inputs=encoded_features, outputs=predictions)\n-training_model.compile(\n-    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n-)\n-\n-inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)\n-\n-\"\"\"\n-## Train the model\n-\n-Let's train our model for 50 epochs. Note that feature preprocessing is happening\n-as part of the tf.data pipeline, not as part of the model.\n-\"\"\"\n-\n-training_model.fit(\n-    preprocessed_train_ds,\n-    epochs=20,\n-    validation_data=preprocessed_val_ds,\n-    verbose=2,\n-)\n-\n-\"\"\"\n-We quickly get to 80% validation accuracy.\n-\"\"\"\n-\n-\"\"\"\n-## Inference on new data with the end-to-end model\n-\n-Now, we can use our inference model (which includes the `FeatureSpace`)\n-to make predictions based on dicts of raw features values, as follows:\n-\"\"\"\n-\n-sample = {\n-    \"age\": 60,\n-    \"sex\": 1,\n-    \"cp\": 1,\n-    \"trestbps\": 145,\n-    \"chol\": 233,\n-    \"fbs\": 1,\n-    \"restecg\": 2,\n-    \"thalach\": 150,\n-    \"exang\": 0,\n-    \"oldpeak\": 2.3,\n-    \"slope\": 3,\n-    \"ca\": 0,\n-    \"thal\": \"fixed\",\n-}\n-\n-input_dict = {\n-    name: tf.convert_to_tensor([value]) for name, value in sample.items()\n-}\n-predictions = inference_model.predict(input_dict)\n-\n-print(\n-    f\"This particular patient had a {100 * predictions[0][0]:.2f}% probability \"\n-    \"of having a heart disease, as evaluated by our model.\"\n-)\n\n@@ -1,563 +0,0 @@\n-\"\"\"\n-Title: Electroencephalogram Signal Classification for action identification\n-Author: [Suvaditya Mukherjee](https://github.com/suvadityamuk)\n-Date created: 2022/11/03\n-Last modified: 2022/11/05\n-Description: Training a Convolutional model to classify EEG signals produced by exposure to certain stimuli.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The following example explores how we can make a Convolution-based Neural Network to\n-perform classification on Electroencephalogram signals captured when subjects were\n-exposed to different stimuli.\n-We train a model from scratch since such signal-classification models are fairly scarce\n-in pre-trained format.\n-The data we use is sourced from the UC Berkeley-Biosense Lab where the data was collected\n-from 15 subjects at the same time.\n-Our process is as follows:\n-\n-- Load the [UC Berkeley-Biosense Synchronized Brainwave Dataset](https://www.kaggle.com/datasets/berkeley-biosense/synchronized-brainwave-dataset)\n-- Visualize random samples from the data\n-- Pre-process, collate and scale the data to finally make a `tf.data.Dataset`\n-- Prepare class weights in order to tackle major imbalances\n-- Create a Conv1D and Dense-based model to perform classification\n-- Define callbacks and hyperparameters\n-- Train the model\n-- Plot metrics from History and perform evaluation\n-\n-This example needs the following external dependencies (Gdown, Scikit-learn, Pandas,\n-Numpy, Matplotlib). You can install it via the following commands.\n-\n-Gdown is an external package used to download large files from Google Drive. To know\n-more, you can refer to its [PyPi page here](https://pypi.org/project/gdown)\n-\"\"\"\n-\n-\n-\"\"\"\n-## Setup and Data Downloads\n-\n-First, lets install our dependencies:\n-\"\"\"\n-\n-\"\"\"shell\n-pip install gdown -q\n-pip install sklearn -q\n-pip install pandas -q\n-pip install numpy -q\n-pip install matplotlib -q\n-\"\"\"\n-\n-\"\"\"\n-Next, lets download our dataset.\n-The gdown package makes it easy to download the data from Google Drive:\n-\"\"\"\n-\n-\"\"\"shell\n-gdown 1V5B7Bt6aJm0UHbR7cRKBEK8jx7lYPVuX\n-# gdown will download eeg-data.csv onto the local drive for use. Total size of\n-# eeg-data.csv is 105.7 MB\n-\"\"\"\n-\n-import pandas as pd\n-import matplotlib.pyplot as plt\n-import json\n-import numpy as np\n-import keras\n-from keras import layers\n-import tensorflow as tf\n-from sklearn import preprocessing, model_selection\n-import random\n-\n-QUALITY_THRESHOLD = 128\n-BATCH_SIZE = 64\n-SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 2\n-\n-\"\"\"\n-## Read data from `eeg-data.csv`\n-\n-We use the Pandas library to read the `eeg-data.csv` file and display the first 5 rows\n-using the `.head()` command\n-\"\"\"\n-\n-eeg = pd.read_csv(\"eeg-data.csv\")\n-\n-\"\"\"\n-We remove unlabeled samples from our dataset as they do not contribute to the model. We\n-also perform a `.drop()` operation on the columns that are not required for training data\n-preparation\n-\"\"\"\n-\n-unlabeled_eeg = eeg[eeg[\"label\"] == \"unlabeled\"]\n-eeg = eeg.loc[eeg[\"label\"] != \"unlabeled\"]\n-eeg = eeg.loc[eeg[\"label\"] != \"everyone paired\"]\n-\n-eeg.drop(\n-    [\n-        \"indra_time\",\n-        \"Unnamed: 0\",\n-        \"browser_latency\",\n-        \"reading_time\",\n-        \"attention_esense\",\n-        \"meditation_esense\",\n-        \"updatedAt\",\n-        \"createdAt\",\n-    ],\n-    axis=1,\n-    inplace=True,\n-)\n-\n-eeg.reset_index(drop=True, inplace=True)\n-eeg.head()\n-\n-\"\"\"\n-In the data, the samples recorded are given a score from 0 to 128 based on how\n-well-calibrated the sensor was (0 being best, 200 being worst). We filter the values\n-based on an arbitrary cutoff limit of 128.\n-\"\"\"\n-\n-\n-def convert_string_data_to_values(value_string):\n-    str_list = json.loads(value_string)\n-    return str_list\n-\n-\n-eeg[\"raw_values\"] = eeg[\"raw_values\"].apply(convert_string_data_to_values)\n-\n-eeg = eeg.loc[eeg[\"signal_quality\"] < QUALITY_THRESHOLD]\n-eeg.head()\n-\n-\"\"\"\n-## Visualize one random sample from the data\n-\"\"\"\n-\n-\"\"\"\n-We visualize one sample from the data to understand how the stimulus-induced signal looks\n-like\n-\"\"\"\n-\n-\n-def view_eeg_plot(idx):\n-    data = eeg.loc[idx, \"raw_values\"]\n-    plt.plot(data)\n-    plt.title(f\"Sample random plot\")\n-    plt.show()\n-\n-\n-view_eeg_plot(7)\n-\n-\"\"\"\n-## Pre-process and collate data\n-\"\"\"\n-\n-\"\"\"\n-There are a total of 67 different labels present in the data, where there are numbered\n-sub-labels. We collate them under a single label as per their numbering and replace them\n-in the data itself. Following this process, we perform simple Label encoding to get them\n-in an integer format.\n-\"\"\"\n-\n-print(\"Before replacing labels\")\n-print(eeg[\"label\"].unique(), \"\\n\")\n-print(len(eeg[\"label\"].unique()), \"\\n\")\n-\n-\n-eeg.replace(\n-    {\n-        \"label\": {\n-            \"blink1\": \"blink\",\n-            \"blink2\": \"blink\",\n-            \"blink3\": \"blink\",\n-            \"blink4\": \"blink\",\n-            \"blink5\": \"blink\",\n-            \"math1\": \"math\",\n-            \"math2\": \"math\",\n-            \"math3\": \"math\",\n-            \"math4\": \"math\",\n-            \"math5\": \"math\",\n-            \"math6\": \"math\",\n-            \"math7\": \"math\",\n-            \"math8\": \"math\",\n-            \"math9\": \"math\",\n-            \"math10\": \"math\",\n-            \"math11\": \"math\",\n-            \"math12\": \"math\",\n-            \"thinkOfItems-ver1\": \"thinkOfItems\",\n-            \"thinkOfItems-ver2\": \"thinkOfItems\",\n-            \"video-ver1\": \"video\",\n-            \"video-ver2\": \"video\",\n-            \"thinkOfItemsInstruction-ver1\": \"thinkOfItemsInstruction\",\n-            \"thinkOfItemsInstruction-ver2\": \"thinkOfItemsInstruction\",\n-            \"colorRound1-1\": \"colorRound1\",\n-            \"colorRound1-2\": \"colorRound1\",\n-            \"colorRound1-3\": \"colorRound1\",\n-            \"colorRound1-4\": \"colorRound1\",\n-            \"colorRound1-5\": \"colorRound1\",\n-            \"colorRound1-6\": \"colorRound1\",\n-            \"colorRound2-1\": \"colorRound2\",\n-            \"colorRound2-2\": \"colorRound2\",\n-            \"colorRound2-3\": \"colorRound2\",\n-            \"colorRound2-4\": \"colorRound2\",\n-            \"colorRound2-5\": \"colorRound2\",\n-            \"colorRound2-6\": \"colorRound2\",\n-            \"colorRound3-1\": \"colorRound3\",\n-            \"colorRound3-2\": \"colorRound3\",\n-            \"colorRound3-3\": \"colorRound3\",\n-            \"colorRound3-4\": \"colorRound3\",\n-            \"colorRound3-5\": \"colorRound3\",\n-            \"colorRound3-6\": \"colorRound3\",\n-            \"colorRound4-1\": \"colorRound4\",\n-            \"colorRound4-2\": \"colorRound4\",\n-            \"colorRound4-3\": \"colorRound4\",\n-            \"colorRound4-4\": \"colorRound4\",\n-            \"colorRound4-5\": \"colorRound4\",\n-            \"colorRound4-6\": \"colorRound4\",\n-            \"colorRound5-1\": \"colorRound5\",\n-            \"colorRound5-2\": \"colorRound5\",\n-            \"colorRound5-3\": \"colorRound5\",\n-            \"colorRound5-4\": \"colorRound5\",\n-            \"colorRound5-5\": \"colorRound5\",\n-            \"colorRound5-6\": \"colorRound5\",\n-            \"colorInstruction1\": \"colorInstruction\",\n-            \"colorInstruction2\": \"colorInstruction\",\n-            \"readyRound1\": \"readyRound\",\n-            \"readyRound2\": \"readyRound\",\n-            \"readyRound3\": \"readyRound\",\n-            \"readyRound4\": \"readyRound\",\n-            \"readyRound5\": \"readyRound\",\n-            \"colorRound1\": \"colorRound\",\n-            \"colorRound2\": \"colorRound\",\n-            \"colorRound3\": \"colorRound\",\n-            \"colorRound4\": \"colorRound\",\n-            \"colorRound5\": \"colorRound\",\n-        }\n-    },\n-    inplace=True,\n-)\n-\n-print(\"After replacing labels\")\n-print(eeg[\"label\"].unique())\n-print(len(eeg[\"label\"].unique()))\n-\n-le = preprocessing.LabelEncoder()  # Generates a look-up table\n-le.fit(eeg[\"label\"])\n-eeg[\"label\"] = le.transform(eeg[\"label\"])\n-\n-\"\"\"\n-We extract the number of unique classes present in the data\n-\"\"\"\n-\n-num_classes = len(eeg[\"label\"].unique())\n-print(num_classes)\n-\n-\"\"\"\n-We now visualize the number of samples present in each class using a Bar plot.\n-\"\"\"\n-\n-plt.bar(range(num_classes), eeg[\"label\"].value_counts())\n-plt.title(\"Number of samples per class\")\n-plt.show()\n-\n-\"\"\"\n-## Scale and split data\n-\"\"\"\n-\n-\"\"\"\n-We perform a simple Min-Max scaling to bring the value-range between 0 and 1. We do not\n-use Standard Scaling as the data does not follow a Gaussian distribution.\n-\"\"\"\n-\n-scaler = preprocessing.MinMaxScaler()\n-series_list = [\n-    scaler.fit_transform(np.asarray(i).reshape(-1, 1))\n-    for i in eeg[\"raw_values\"]\n-]\n-\n-labels_list = [i for i in eeg[\"label\"]]\n-\n-\"\"\"\n-We now create a Train-test split with a 15% holdout set. Following this, we reshape the\n-data to create a sequence of length 512. We also convert the labels from their current\n-label-encoded form to a one-hot encoding to enable use of several different\n-`keras.metrics` functions.\n-\"\"\"\n-\n-x_train, x_test, y_train, y_test = model_selection.train_test_split(\n-    series_list, labels_list, test_size=0.15, random_state=42, shuffle=True\n-)\n-\n-print(\n-    f\"Length of x_train : {len(x_train)}\\nLength of x_test : {len(x_test)}\\nLength of y_train : {len(y_train)}\\nLength of y_test : {len(y_test)}\"\n-)\n-\n-x_train = np.asarray(x_train).astype(np.float32).reshape(-1, 512, 1)\n-y_train = np.asarray(y_train).astype(np.float32).reshape(-1, 1)\n-y_train = keras.utils.to_categorical(y_train)\n-\n-x_test = np.asarray(x_test).astype(np.float32).reshape(-1, 512, 1)\n-y_test = np.asarray(y_test).astype(np.float32).reshape(-1, 1)\n-y_test = keras.utils.to_categorical(y_test)\n-\n-\"\"\"\n-## Prepare `tf.data.Dataset`\n-\"\"\"\n-\n-\"\"\"\n-We now create a `tf.data.Dataset` from this data to prepare it for training. We also\n-shuffle and batch the data for use later.\n-\"\"\"\n-\n-train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n-\n-train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n-test_dataset = test_dataset.batch(BATCH_SIZE)\n-\n-\"\"\"\n-## Make Class Weights using Naive method\n-\"\"\"\n-\n-\"\"\"\n-As we can see from the plot of number of samples per class, the dataset is imbalanced.\n-Hence, we **calculate weights for each class** to make sure that the model is trained in\n-a fair manner without preference to any specific class due to greater number of samples.\n-\n-We use a naive method to calculate these weights, finding an **inverse proportion** of\n-each class and using that as the weight.\n-\"\"\"\n-\n-vals_dict = {}\n-for i in eeg[\"label\"]:\n-    if i in vals_dict.keys():\n-        vals_dict[i] += 1\n-    else:\n-        vals_dict[i] = 1\n-total = sum(vals_dict.values())\n-\n-# Formula used - Naive method where\n-# weight = 1 - (no. of samples present / total no. of samples)\n-# So more the samples, lower the weight\n-\n-weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n-print(weight_dict)\n-\n-\"\"\"\n-## Define simple function to plot all the metrics present in a `keras.callbacks.History`\n-object\n-\"\"\"\n-\n-\n-def plot_history_metrics(history: keras.callbacks.History):\n-    total_plots = len(history.history)\n-    cols = total_plots // 2\n-\n-    rows = total_plots // cols\n-\n-    if total_plots % cols != 0:\n-        rows += 1\n-\n-    pos = range(1, total_plots + 1)\n-    plt.figure(figsize=(15, 10))\n-    for i, (key, value) in enumerate(history.history.items()):\n-        plt.subplot(rows, cols, pos[i])\n-        plt.plot(range(len(value)), value)\n-        plt.title(str(key))\n-    plt.show()\n-\n-\n-\"\"\"\n-## Define function to generate Convolutional model\n-\"\"\"\n-\n-\n-def create_model():\n-    input_layer = keras.Input(shape=(512, 1))\n-\n-    x = layers.Conv1D(\n-        filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n-    )(input_layer)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Conv1D(\n-        filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Conv1D(\n-        filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Conv1D(\n-        filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Conv1D(\n-        filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Conv1D(\n-        filters=1024,\n-        kernel_size=7,\n-        strides=2,\n-        activation=\"relu\",\n-        padding=\"same\",\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    x = layers.Dropout(0.2)(x)\n-\n-    x = layers.Flatten()(x)\n-\n-    x = layers.Dense(4096, activation=\"relu\")(x)\n-    x = layers.Dropout(0.2)(x)\n-\n-    x = layers.Dense(\n-        2048, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n-    )(x)\n-    x = layers.Dropout(0.2)(x)\n-\n-    x = layers.Dense(\n-        1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n-    )(x)\n-    x = layers.Dropout(0.2)(x)\n-    x = layers.Dense(\n-        128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n-    )(x)\n-    output_layer = layers.Dense(num_classes, activation=\"softmax\")(x)\n-\n-    return keras.Model(inputs=input_layer, outputs=output_layer)\n-\n-\n-\"\"\"\n-## Get Model summary\n-\"\"\"\n-\n-conv_model = create_model()\n-conv_model.summary()\n-\n-\"\"\"\n-## Define callbacks, optimizer, loss and metrics\n-\"\"\"\n-\n-\"\"\"\n-We set the number of epochs at 30 after performing extensive experimentation. It was seen\n-that this was the optimal number, after performing Early-Stopping analysis as well.\n-We define a Model Checkpoint callback to make sure that we only get the best model\n-weights.\n-We also define a ReduceLROnPlateau as there were several cases found during\n-experimentation where the loss stagnated after a certain point. On the other hand, a\n-direct LRScheduler was found to be too aggressive in its decay.\n-\"\"\"\n-\n-epochs = 30\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\n-        \"best_model.keras\", save_best_only=True, monitor=\"loss\"\n-    ),\n-    keras.callbacks.ReduceLROnPlateau(\n-        monitor=\"val_top_k_categorical_accuracy\",\n-        factor=0.2,\n-        patience=2,\n-        min_lr=0.000001,\n-    ),\n-]\n-\n-optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n-loss = keras.losses.CategoricalCrossentropy()\n-\n-\"\"\"\n-## Compile model and call `model.fit()`\n-\"\"\"\n-\n-\"\"\"\n-We use the `Adam` optimizer since it is commonly considered the best choice for\n-preliminary training, and was found to be the best optimizer.\n-We use `CategoricalCrossentropy` as the loss as our labels are in a one-hot-encoded form.\n-\n-We define the `TopKCategoricalAccuracy(k=3)`, `AUC`, `Precision` and `Recall` metrics to\n-further aid in understanding the model better.\n-\"\"\"\n-\n-conv_model.compile(\n-    optimizer=optimizer,\n-    loss=loss,\n-    metrics=[\n-        keras.metrics.TopKCategoricalAccuracy(k=3),\n-        keras.metrics.AUC(),\n-        keras.metrics.Precision(),\n-        keras.metrics.Recall(),\n-    ],\n-)\n-\n-conv_model_history = conv_model.fit(\n-    train_dataset,\n-    epochs=epochs,\n-    callbacks=callbacks,\n-    validation_data=test_dataset,\n-    class_weight=weight_dict,\n-)\n-\n-\"\"\"\n-## Visualize model metrics during training\n-\"\"\"\n-\n-\"\"\"\n-We use the function defined above to see model metrics during training.\n-\"\"\"\n-\n-plot_history_metrics(conv_model_history)\n-\n-\"\"\"\n-## Evaluate model on test data\n-\"\"\"\n-\n-loss, accuracy, auc, precision, recall = conv_model.evaluate(test_dataset)\n-print(f\"Loss : {loss}\")\n-print(f\"Top 3 Categorical Accuracy : {accuracy}\")\n-print(f\"Area under the Curve (ROC) : {auc}\")\n-print(f\"Precision : {precision}\")\n-print(f\"Recall : {recall}\")\n-\n-\n-def view_evaluated_eeg_plots(model):\n-    start_index = random.randint(10, len(eeg))\n-    end_index = start_index + 11\n-    data = eeg.loc[start_index:end_index, \"raw_values\"]\n-    data_array = [\n-        scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data\n-    ]\n-    data_array = [np.asarray(data_array).astype(np.float32).reshape(-1, 512, 1)]\n-    original_labels = eeg.loc[start_index:end_index, \"label\"]\n-    predicted_labels = np.argmax(model.predict(data_array, verbose=0), axis=1)\n-    original_labels = [\n-        le.inverse_transform(np.array(label).reshape(-1))[0]\n-        for label in original_labels\n-    ]\n-    predicted_labels = [\n-        le.inverse_transform(np.array(label).reshape(-1))[0]\n-        for label in predicted_labels\n-    ]\n-    total_plots = 12\n-    cols = total_plots // 3\n-    rows = total_plots // cols\n-    if total_plots % cols != 0:\n-        rows += 1\n-    pos = range(1, total_plots + 1)\n-    fig = plt.figure(figsize=(20, 10))\n-    for i, (plot_data, og_label, pred_label) in enumerate(\n-        zip(data, original_labels, predicted_labels)\n-    ):\n-        plt.subplot(rows, cols, pos[i])\n-        plt.plot(plot_data)\n-        plt.title(f\"Actual Label : {og_label}\\nPredicted Label : {pred_label}\")\n-        fig.subplots_adjust(hspace=0.5)\n-    plt.show()\n-\n-\n-view_evaluated_eeg_plots(conv_model)\n\n@@ -1,674 +0,0 @@\n-\"\"\"\n-Title: Traffic forecasting using graph neural networks and LSTM\n-Author: [Arash Khodadadi](https://www.linkedin.com/in/arash-khodadadi-08a02490/)\n-Date created: 2021/12/28\n-Last modified: 2023/11/22\n-Description: This example demonstrates how to do timeseries forecasting over graphs.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to forecast traffic condition using graph neural networks and LSTM.\n-Specifically, we are interested in predicting the future values of the traffic speed given\n-a history of the traffic speed for a collection of road segments.\n-\n-One popular method to\n-solve this problem is to consider each road segment's traffic speed as a separate\n-timeseries and predict the future values of each timeseries\n-using the past values of the same timeseries.\n-\n-This method, however, ignores the dependency of the traffic speed of one road segment on\n-the neighboring segments. To be able to take into account the complex interactions between\n-the traffic speed on a collection of neighboring roads, we can define the traffic network\n-as a graph and consider the traffic speed as a signal on this graph. In this example,\n-we implement a neural network architecture which can process timeseries data over a graph.\n-We first show how to process the data and create a\n-[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for\n-forecasting over graphs. Then, we implement a model which uses graph convolution and\n-LSTM layers to perform forecasting over a graph.\n-\n-The data processing and the model architecture are inspired by this paper:\n-\n-Yu, Bing, Haoteng Yin, and Zhanxing Zhu. \"Spatio-temporal graph convolutional networks:\n-a deep learning framework for traffic forecasting.\" Proceedings of the 27th International\n-Joint Conference on Artificial Intelligence, 2018.\n-([github](https://github.com/VeritasYin/STGCN_IJCAI-18))\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import pandas as pd\n-import numpy as np\n-import typing\n-import matplotlib.pyplot as plt\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-\"\"\"\n-## Data preparation\n-\"\"\"\n-\n-\"\"\"\n-### Data description\n-\n-We use a real-world traffic speed dataset named `PeMSD7`. We use the version\n-collected and prepared by [Yu et al., 2018](https://arxiv.org/abs/1709.04875)\n-and available\n-[here](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master/dataset).\n-\n-The data consists of two files:\n-\n-- `PeMSD7_W_228.csv` contains the distances between 228\n-stations across the District 7 of California.\n-- `PeMSD7_V_228.csv` contains traffic\n-speed collected for those stations in the weekdays of May and June of 2012.\n-\n-The full description of the dataset can be found in\n-[Yu et al., 2018](https://arxiv.org/abs/1709.04875).\n-\"\"\"\n-\n-\"\"\"\n-### Loading data\n-\"\"\"\n-\n-url = \"https://github.com/VeritasYin/STGCN_IJCAI-18/raw/master/dataset/PeMSD7_Full.zip\"\n-data_dir = keras.utils.get_file(origin=url, extract=True, archive_format=\"zip\")\n-data_dir = data_dir.rstrip(\"PeMSD7_Full.zip\")\n-\n-route_distances = pd.read_csv(\n-    os.path.join(data_dir, \"PeMSD7_W_228.csv\"), header=None\n-).to_numpy()\n-speeds_array = pd.read_csv(\n-    os.path.join(data_dir, \"PeMSD7_V_228.csv\"), header=None\n-).to_numpy()\n-\n-print(f\"route_distances shape={route_distances.shape}\")\n-print(f\"speeds_array shape={speeds_array.shape}\")\n-\n-\"\"\"\n-### sub-sampling roads\n-\n-To reduce the problem size and make the training faster, we will only\n-work with a sample of 26 roads out of the 228 roads in the dataset.\n-We have chosen the roads by starting from road 0, choosing the 5 closest\n-roads to it, and continuing this process until we get 25 roads. You can choose\n-any other subset of the roads. We chose the roads in this way to increase the likelihood\n-of having roads with correlated speed timeseries.\n-`sample_routes` contains the IDs of the selected roads.\n-\"\"\"\n-\n-sample_routes = [\n-    0,\n-    1,\n-    4,\n-    7,\n-    8,\n-    11,\n-    15,\n-    108,\n-    109,\n-    114,\n-    115,\n-    118,\n-    120,\n-    123,\n-    124,\n-    126,\n-    127,\n-    129,\n-    130,\n-    132,\n-    133,\n-    136,\n-    139,\n-    144,\n-    147,\n-    216,\n-]\n-route_distances = route_distances[np.ix_(sample_routes, sample_routes)]\n-speeds_array = speeds_array[:, sample_routes]\n-\n-print(f\"route_distances shape={route_distances.shape}\")\n-print(f\"speeds_array shape={speeds_array.shape}\")\n-\n-\"\"\"\n-### Data visualization\n-\n-Here are the timeseries of the traffic speed for two of the routes:\n-\"\"\"\n-\n-plt.figure(figsize=(18, 6))\n-plt.plot(speeds_array[:, [0, -1]])\n-plt.legend([\"route_0\", \"route_25\"])\n-\n-\"\"\"\n-We can also visualize the correlation between the timeseries in different routes.\n-\"\"\"\n-\n-plt.figure(figsize=(8, 8))\n-plt.matshow(np.corrcoef(speeds_array.T), 0)\n-plt.xlabel(\"road number\")\n-plt.ylabel(\"road number\")\n-\n-\"\"\"\n-Using this correlation heatmap, we can see that for example the speed in\n-routes 4, 5, 6 are highly correlated.\n-\"\"\"\n-\n-\"\"\"\n-### Splitting and normalizing data\n-\n-Next, we split the speed values array into train/validation/test sets,\n-and normalize the resulting arrays:\n-\"\"\"\n-\n-train_size, val_size = 0.5, 0.2\n-\n-\n-def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n-    \"\"\"Splits data into train/val/test sets and normalizes the data.\n-\n-    Args:\n-        data_array: ndarray of shape `(num_time_steps, num_routes)`\n-        train_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n-            to include in the train split.\n-        val_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n-            to include in the validation split.\n-\n-    Returns:\n-        `train_array`, `val_array`, `test_array`\n-    \"\"\"\n-\n-    num_time_steps = data_array.shape[0]\n-    num_train, num_val = (\n-        int(num_time_steps * train_size),\n-        int(num_time_steps * val_size),\n-    )\n-    train_array = data_array[:num_train]\n-    mean, std = train_array.mean(axis=0), train_array.std(axis=0)\n-\n-    train_array = (train_array - mean) / std\n-    val_array = (data_array[num_train : (num_train + num_val)] - mean) / std\n-    test_array = (data_array[(num_train + num_val) :] - mean) / std\n-\n-    return train_array, val_array, test_array\n-\n-\n-train_array, val_array, test_array = preprocess(\n-    speeds_array, train_size, val_size\n-)\n-\n-print(f\"train set size: {train_array.shape}\")\n-print(f\"validation set size: {val_array.shape}\")\n-print(f\"test set size: {test_array.shape}\")\n-\n-\"\"\"\n-### Creating TensorFlow Datasets\n-\n-Next, we create the datasets for our forecasting problem. The forecasting problem\n-can be stated as follows: given a sequence of the\n-road speed values at times `t+1, t+2, ..., t+T`, we want to predict the future values of\n-the roads speed for times `t+T+1, ..., t+T+h`. So for each time `t` the inputs to our\n-model are `T` vectors each of size `N` and the targets are `h` vectors each of size `N`,\n-where `N` is the number of roads.\n-\"\"\"\n-\n-\"\"\"\n-We use the Keras built-in function\n-`keras.utils.timeseries_dataset_from_array`.\n-The function `create_tf_dataset()` below takes as input a `numpy.ndarray` and returns a\n-`tf.data.Dataset`. In this function `input_sequence_length=T` and `forecast_horizon=h`.\n-\n-The argument `multi_horizon` needs more explanation. Assume `forecast_horizon=3`.\n-If `multi_horizon=True` then the model will make a forecast for time steps\n-`t+T+1, t+T+2, t+T+3`. So the target will have shape `(T,3)`. But if\n-`multi_horizon=False`, the model will make a forecast only for time step `t+T+3` and\n-so the target will have shape `(T, 1)`.\n-\n-You may notice that the input tensor in each batch has shape\n-`(batch_size, input_sequence_length, num_routes, 1)`. The last dimension is added to\n-make the model more general: at each time step, the input features for each raod may\n-contain multiple timeseries. For instance, one might want to use temperature timeseries\n-in addition to historical values of the speed as input features. In this example,\n-however, the last dimension of the input is always 1.\n-\n-We use the last 12 values of the speed in each road to forecast the speed for 3 time\n-steps ahead:\n-\"\"\"\n-\n-batch_size = 64\n-input_sequence_length = 12\n-forecast_horizon = 3\n-multi_horizon = False\n-\n-\n-def create_tf_dataset(\n-    data_array: np.ndarray,\n-    input_sequence_length: int,\n-    forecast_horizon: int,\n-    batch_size: int = 128,\n-    shuffle=True,\n-    multi_horizon=True,\n-):\n-    \"\"\"Creates tensorflow dataset from numpy array.\n-\n-    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n-    `inputs` is a Tensor\n-    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n-    the `input_sequence_length` past values of the timeseries for each node.\n-    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n-    containing the `forecast_horizon`\n-    future values of the timeseries for each node.\n-\n-    Args:\n-        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n-        input_sequence_length: Length of the input sequence (in number of timesteps).\n-        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n-            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n-            timeseries `forecast_horizon` steps ahead (only one value).\n-        batch_size: Number of timeseries samples in each batch.\n-        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n-        multi_horizon: See `forecast_horizon`.\n-\n-    Returns:\n-        A tf.data.Dataset instance.\n-    \"\"\"\n-\n-    inputs = keras.utils.timeseries_dataset_from_array(\n-        np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n-        None,\n-        sequence_length=input_sequence_length,\n-        shuffle=False,\n-        batch_size=batch_size,\n-    )\n-\n-    target_offset = (\n-        input_sequence_length\n-        if multi_horizon\n-        else input_sequence_length + forecast_horizon - 1\n-    )\n-    target_seq_length = forecast_horizon if multi_horizon else 1\n-    targets = keras.utils.timeseries_dataset_from_array(\n-        data_array[target_offset:],\n-        None,\n-        sequence_length=target_seq_length,\n-        shuffle=False,\n-        batch_size=batch_size,\n-    )\n-\n-    dataset = tf.data.Dataset.zip((inputs, targets))\n-    if shuffle:\n-        dataset = dataset.shuffle(100)\n-\n-    return dataset.prefetch(16).cache()\n-\n-\n-train_dataset, val_dataset = (\n-    create_tf_dataset(\n-        data_array, input_sequence_length, forecast_horizon, batch_size\n-    )\n-    for data_array in [train_array, val_array]\n-)\n-\n-test_dataset = create_tf_dataset(\n-    test_array,\n-    input_sequence_length,\n-    forecast_horizon,\n-    batch_size=test_array.shape[0],\n-    shuffle=False,\n-    multi_horizon=multi_horizon,\n-)\n-\n-\n-\"\"\"\n-### Roads Graph\n-\n-As mentioned before, we assume that the road segments form a graph.\n-The `PeMSD7` dataset has the road segments distance. The next step\n-is to create the graph adjacency matrix from these distances. Following\n-[Yu et al., 2018](https://arxiv.org/abs/1709.04875) (equation 10) we assume there\n-is an edge between two nodes in the graph if the distance between the corresponding roads\n-is less than a threshold.\n-\"\"\"\n-\n-\n-def compute_adjacency_matrix(\n-    route_distances: np.ndarray, sigma2: float, epsilon: float\n-):\n-    \"\"\"Computes the adjacency matrix from distances matrix.\n-\n-    It uses the formula in https://github.com/VeritasYin/STGCN_IJCAI-18#data-preprocessing to\n-    compute an adjacency matrix from the distance matrix.\n-    The implementation follows that paper.\n-\n-    Args:\n-        route_distances: np.ndarray of shape `(num_routes, num_routes)`. Entry `i,j` of this array is the\n-            distance between roads `i,j`.\n-        sigma2: Determines the width of the Gaussian kernel applied to the square distances matrix.\n-        epsilon: A threshold specifying if there is an edge between two nodes. Specifically, `A[i,j]=1`\n-            if `np.exp(-w2[i,j] / sigma2) >= epsilon` and `A[i,j]=0` otherwise, where `A` is the adjacency\n-            matrix and `w2=route_distances * route_distances`\n-\n-    Returns:\n-        A boolean graph adjacency matrix.\n-    \"\"\"\n-    num_routes = route_distances.shape[0]\n-    route_distances = route_distances / 10000.0\n-    w2, w_mask = (\n-        route_distances * route_distances,\n-        np.ones([num_routes, num_routes]) - np.identity(num_routes),\n-    )\n-    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask\n-\n-\n-\"\"\"\n-The function `compute_adjacency_matrix()` returns a boolean adjacency matrix\n-where 1 means there is an edge between two nodes. We use the following class\n-to store the information about the graph.\n-\"\"\"\n-\n-\n-class GraphInfo:\n-    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n-        self.edges = edges\n-        self.num_nodes = num_nodes\n-\n-\n-sigma2 = 0.1\n-epsilon = 0.5\n-adjacency_matrix = compute_adjacency_matrix(route_distances, sigma2, epsilon)\n-node_indices, neighbor_indices = np.where(adjacency_matrix == 1)\n-graph = GraphInfo(\n-    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n-    num_nodes=adjacency_matrix.shape[0],\n-)\n-print(\n-    f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\"\n-)\n-\n-\"\"\"\n-## Network architecture\n-\n-Our model for forecasting over the graph consists of a graph convolution\n-layer and a LSTM layer.\n-\"\"\"\n-\n-\"\"\"\n-### Graph convolution layer\n-\n-Our implementation of the graph convolution layer resembles the implementation\n-in [this Keras example](https://keras.io/examples/graph/gnn_citations/). Note that\n-in that example input to the layer is a 2D tensor of shape `(num_nodes,in_feat)`\n-but in our example the input to the layer is a 4D tensor of shape\n-`(num_nodes, batch_size, input_seq_length, in_feat)`. The graph convolution layer\n-performs the following steps:\n-\n-- The nodes' representations are computed in `self.compute_nodes_representation()`\n-by multiplying the input features by `self.weight`\n-- The aggregated neighbors' messages are computed in `self.compute_aggregated_messages()`\n-by first aggregating the neighbors' representations and then multiplying the results by\n-`self.weight`\n-- The final output of the layer is computed in `self.update()` by combining the nodes\n-representations and the neighbors' aggregated messages\n-\"\"\"\n-\n-\n-class GraphConv(layers.Layer):\n-    def __init__(\n-        self,\n-        in_feat,\n-        out_feat,\n-        graph_info: GraphInfo,\n-        aggregation_type=\"mean\",\n-        combination_type=\"concat\",\n-        activation: typing.Optional[str] = None,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.in_feat = in_feat\n-        self.out_feat = out_feat\n-        self.graph_info = graph_info\n-        self.aggregation_type = aggregation_type\n-        self.combination_type = combination_type\n-        self.weight = self.add_weight(\n-            initializer=keras.initializers.GlorotUniform(),\n-            shape=(in_feat, out_feat),\n-            dtype=\"float32\",\n-            trainable=True,\n-        )\n-        self.activation = layers.Activation(activation)\n-\n-    def aggregate(self, neighbour_representations):\n-        aggregation_func = {\n-            \"sum\": tf.math.unsorted_segment_sum,\n-            \"mean\": tf.math.unsorted_segment_mean,\n-            \"max\": tf.math.unsorted_segment_max,\n-        }.get(self.aggregation_type)\n-\n-        if aggregation_func:\n-            return aggregation_func(\n-                neighbour_representations,\n-                self.graph_info.edges[0],\n-                num_segments=self.graph_info.num_nodes,\n-            )\n-\n-        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n-\n-    def compute_nodes_representation(self, features):\n-        \"\"\"Computes each node's representation.\n-\n-        The nodes' representations are obtained by multiplying the features tensor with\n-        `self.weight`. Note that\n-        `self.weight` has shape `(in_feat, out_feat)`.\n-\n-        Args:\n-            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n-\n-        Returns:\n-            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n-        \"\"\"\n-        return ops.matmul(features, self.weight)\n-\n-    def compute_aggregated_messages(self, features):\n-        neighbour_representations = tf.gather(\n-            features, self.graph_info.edges[1]\n-        )\n-        aggregated_messages = self.aggregate(neighbour_representations)\n-        return ops.matmul(aggregated_messages, self.weight)\n-\n-    def update(\n-        self, nodes_representation, aggregated_messages\n-    ):\n-        if self.combination_type == \"concat\":\n-            h = ops.concatenate([nodes_representation, aggregated_messages], axis=-1)\n-        elif self.combination_type == \"add\":\n-            h = nodes_representation + aggregated_messages\n-        else:\n-            raise ValueError(\n-                f\"Invalid combination type: {self.combination_type}.\"\n-            )\n-        return self.activation(h)\n-\n-    def call(self, features):\n-        \"\"\"Forward pass.\n-\n-        Args:\n-            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n-\n-        Returns:\n-            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n-        \"\"\"\n-        nodes_representation = self.compute_nodes_representation(features)\n-        aggregated_messages = self.compute_aggregated_messages(features)\n-        return self.update(nodes_representation, aggregated_messages)\n-\n-\n-\"\"\"\n-### LSTM plus graph convolution\n-\n-By applying the graph convolution layer to the input tensor, we get another tensor\n-containing the nodes' representations over time (another 4D tensor). For each time\n-step, a node's representation is informed by the information from its neighbors.\n-\n-To make good forecasts, however, we need not only information from the neighbors\n-but also we need to process the information over time. To this end, we can pass each\n-node's tensor through a recurrent layer. The `LSTMGC` layer below, first applies\n-a graph convolution layer to the inputs and then passes the results through a\n-`LSTM` layer.\n-\"\"\"\n-\n-\n-class LSTMGC(layers.Layer):\n-    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n-\n-    def __init__(\n-        self,\n-        in_feat,\n-        out_feat,\n-        lstm_units: int,\n-        input_seq_len: int,\n-        output_seq_len: int,\n-        graph_info: GraphInfo,\n-        graph_conv_params: typing.Optional[dict] = None,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        # graph conv layer\n-        if graph_conv_params is None:\n-            graph_conv_params = {\n-                \"aggregation_type\": \"mean\",\n-                \"combination_type\": \"concat\",\n-                \"activation\": None,\n-            }\n-        self.graph_conv = GraphConv(\n-            in_feat, out_feat, graph_info, **graph_conv_params\n-        )\n-\n-        self.lstm = layers.LSTM(lstm_units, activation=\"relu\")\n-        self.dense = layers.Dense(output_seq_len)\n-\n-        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n-\n-    def call(self, inputs):\n-        \"\"\"Forward pass.\n-\n-        Args:\n-            inputs: tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n-\n-        Returns:\n-            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n-        \"\"\"\n-\n-        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n-        inputs = ops.transpose(inputs, [2, 0, 1, 3])\n-\n-        gcn_out = self.graph_conv(\n-            inputs\n-        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n-        shape = ops.shape(gcn_out)\n-        num_nodes, batch_size, input_seq_len, out_feat = (\n-            shape[0],\n-            shape[1],\n-            shape[2],\n-            shape[3],\n-        )\n-\n-        # LSTM takes only 3D tensors as input\n-        gcn_out = ops.reshape(\n-            gcn_out, (batch_size * num_nodes, input_seq_len, out_feat)\n-        )\n-        lstm_out = self.lstm(\n-            gcn_out\n-        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n-\n-        dense_output = self.dense(\n-            lstm_out\n-        )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n-        output = ops.reshape(\n-            dense_output, (num_nodes, batch_size, self.output_seq_len)\n-        )\n-        return ops.transpose(\n-            output, [1, 2, 0]\n-        )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n-\n-\n-\"\"\"\n-## Model training\n-\"\"\"\n-\n-in_feat = 1\n-batch_size = 64\n-epochs = 20\n-input_sequence_length = 12\n-forecast_horizon = 3\n-multi_horizon = False\n-out_feat = 10\n-lstm_units = 64\n-graph_conv_params = {\n-    \"aggregation_type\": \"mean\",\n-    \"combination_type\": \"concat\",\n-    \"activation\": None,\n-}\n-\n-st_gcn = LSTMGC(\n-    in_feat,\n-    out_feat,\n-    lstm_units,\n-    input_sequence_length,\n-    forecast_horizon,\n-    graph,\n-    graph_conv_params,\n-)\n-inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n-outputs = st_gcn(inputs)\n-\n-model = keras.models.Model(inputs, outputs)\n-model.compile(\n-    optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n-    loss=keras.losses.MeanSquaredError(),\n-)\n-model.fit(\n-    train_dataset,\n-    validation_data=val_dataset,\n-    epochs=epochs,\n-    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n-)\n-\n-\"\"\"\n-## Making forecasts on test set\n-\n-Now we can use the trained model to make forecasts for the test set. Below, we\n-compute the MAE of the model and compare it to the MAE of naive forecasts.\n-The naive forecasts are the last value of the speed for each node.\n-\"\"\"\n-\n-x_test, y = next(test_dataset.as_numpy_iterator())\n-y_pred = model.predict(x_test)\n-plt.figure(figsize=(18, 6))\n-plt.plot(y[:, 0, 0])\n-plt.plot(y_pred[:, 0, 0])\n-plt.legend([\"actual\", \"forecast\"])\n-\n-naive_mse, model_mse = (\n-    np.square(x_test[:, -1, :, 0] - y[:, 0, :]).mean(),\n-    np.square(y_pred[:, 0, :] - y[:, 0, :]).mean(),\n-)\n-print(f\"naive MAE: {naive_mse}, model MAE: {model_mse}\")\n-\n-\"\"\"\n-Of course, the goal here is to demonstrate the method,\n-not to achieve the best performance. To improve the\n-model's accuracy, all model hyperparameters should be tuned carefully. In addition,\n-several of the `LSTMGC` blocks can be stacked to increase the representation power\n-of the model.\n-\"\"\"\n\n@@ -1,307 +0,0 @@\n-\"\"\"\n-Title: Image Classification using BigTransfer (BiT)\n-Author: [Sayan Nath](https://twitter.com/sayannath2350)\n-Date created: 2021/09/24\n-Last modified: 2021/09/24\n-Description: BigTransfer (BiT) State-of-the-art transfer learning for image classification.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-BigTransfer (also known as BiT) is a state-of-the-art transfer learning method for image\n-classification. Transfer of pre-trained representations improves sample efficiency and\n-simplifies hyperparameter tuning when training deep neural networks for vision. BiT\n-revisit the paradigm of pre-training on large supervised datasets and fine-tuning the\n-model on a target task. The importance of appropriately choosing normalization layers and\n-scaling the architecture capacity as the amount of pre-training data increases.\n-\n-BigTransfer(BiT) is trained on public datasets, along with code in\n-[TF2, Jax and Pytorch](https://github.com/google-research/big_transfer). This will help anyone to reach\n-state of the art performance on their task of interest, even with just a handful of\n-labeled images per class.\n-\n-You can find BiT models pre-trained on\n-[ImageNet](https://image-net.org/challenges/LSVRC/2012/index) and ImageNet-21k in\n-[TFHub](https://tfhub.dev/google/collections/bit/1) as TensorFlow2 SavedModels that you\n-can use easily as Keras Layers. There are a variety of sizes ranging from a standard\n-ResNet50 to a ResNet152x4 (152 layers deep, 4x wider than a typical ResNet50) for users\n-with larger computational and memory budgets but higher accuracy requirements.\n-\n-![](https://i.imgur.com/XeWVfe7.jpeg)\n-Figure: The x-axis shows the number of images used per class, ranging from 1 to the full\n-dataset. On the plots on the left, the curve in blue above is our BiT-L model, whereas\n-the curve below is a ResNet-50 pre-trained on ImageNet (ILSVRC-2012).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-import tensorflow as tf\n-import tensorflow_hub as hub\n-import tensorflow_datasets as tfds\n-\n-tfds.disable_progress_bar()\n-\n-SEEDS = 42\n-\n-np.random.seed(SEEDS)\n-keras.utils.set_random_seed(SEEDS)\n-\n-\"\"\"\n-## Gather Flower Dataset\n-\"\"\"\n-\n-train_ds, validation_ds = tfds.load(\n-    \"tf_flowers\",\n-    split=[\"train[:85%]\", \"train[85%:]\"],\n-    as_supervised=True,\n-)\n-\n-\"\"\"\n-## Visualise the dataset\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-for i, (image, label) in enumerate(train_ds.take(9)):\n-    ax = plt.subplot(3, 3, i + 1)\n-    plt.imshow(image)\n-    plt.title(int(label))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-RESIZE_TO = 384\n-CROP_TO = 224\n-BATCH_SIZE = 64\n-STEPS_PER_EPOCH = 10\n-AUTO = tf.data.AUTOTUNE  # optimise the pipeline performance\n-NUM_CLASSES = 5  # number of classes\n-SCHEDULE_LENGTH = 500  # we will train on lower resolution images and will still attain good results\n-SCHEDULE_BOUNDARIES = [\n-    200,\n-    300,\n-    400,\n-]  # more the dataset size the schedule length increase\n-\n-\"\"\"\n-The hyperparamteres like `SCHEDULE_LENGTH` and `SCHEDULE_BOUNDARIES` are determined based\n-on empirical results. The method has been explained in the [original\n-paper](https://arxiv.org/abs/1912.11370) and in their [Google AI Blog\n-Post](https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html).\n-\n-The `SCHEDULE_LENGTH` is aslo determined whether to use [MixUp\n-Augmentation](https://arxiv.org/abs/1710.09412) or not. You can also find an easy MixUp\n-Implementation in [Keras Coding Examples](https://keras.io/examples/vision/mixup/).\n-\n-![](https://i.imgur.com/oSaIBYZ.jpeg)\n-\"\"\"\n-\n-\"\"\"\n-## Define preprocessing helper functions\n-\"\"\"\n-\n-SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n-\n-\n-@tf.function\n-def preprocess_train(image, label):\n-    image = tf.image.random_flip_left_right(image)\n-    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n-    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n-    image = image / 255.0\n-    return (image, label)\n-\n-\n-@tf.function\n-def preprocess_test(image, label):\n-    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n-    image = image / 255.0\n-    return (image, label)\n-\n-\n-DATASET_NUM_TRAIN_EXAMPLES = train_ds.cardinality().numpy()\n-\n-repeat_count = int(\n-    SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH\n-)\n-repeat_count += 50 + 1  # To ensure at least there are 50 epochs of training\n-\n-\"\"\"\n-## Define the data pipeline\n-\"\"\"\n-\n-# Training pipeline\n-pipeline_train = (\n-    train_ds.shuffle(10000)\n-    .repeat(repeat_count)  # Repeat dataset_size / num_steps\n-    .map(preprocess_train, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-# Validation pipeline\n-pipeline_validation = (\n-    validation_ds.map(preprocess_test, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-\"\"\"\n-## Visualise the training samples\n-\"\"\"\n-\n-image_batch, label_batch = next(iter(pipeline_train))\n-\n-plt.figure(figsize=(10, 10))\n-for n in range(25):\n-    ax = plt.subplot(5, 5, n + 1)\n-    plt.imshow(image_batch[n])\n-    plt.title(label_batch[n].numpy())\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Load pretrained TF-Hub model into a `KerasLayer`\n-\"\"\"\n-\n-bit_model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n-bit_module = hub.KerasLayer(bit_model_url)\n-\n-\"\"\"\n-## Create BigTransfer (BiT) model\n-\n-To create the new model, we:\n-\n-1. Cut off the BiT model’s original head. This leaves us with the “pre-logits” output.\n-We do not have to do this if we use the ‘feature extractor’ models (i.e. all those in\n-subdirectories titled `feature_vectors`), since for those models the head has already\n-been cut off.\n-\n-2. Add a new head with the number of outputs equal to the number of classes of our new\n-task. Note that it is important that we initialise the head to all zeroes.\n-\"\"\"\n-\n-\n-class MyBiTModel(keras.Model):\n-    def __init__(self, num_classes, module, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.num_classes = num_classes\n-        self.head = keras.layers.Dense(num_classes, kernel_initializer=\"zeros\")\n-        self.bit_model = module\n-\n-    def call(self, images):\n-        bit_embedding = self.bit_model(images)\n-        return self.head(bit_embedding)\n-\n-\n-model = MyBiTModel(num_classes=NUM_CLASSES, module=bit_module)\n-\n-\"\"\"\n-## Define optimizer and loss\n-\"\"\"\n-\n-learning_rate = 0.003 * BATCH_SIZE / 512\n-\n-# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n-lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n-    boundaries=SCHEDULE_BOUNDARIES,\n-    values=[\n-        learning_rate,\n-        learning_rate * 0.1,\n-        learning_rate * 0.01,\n-        learning_rate * 0.001,\n-    ],\n-)\n-optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n-\n-loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-\n-\"\"\"\n-## Compile the model\n-\"\"\"\n-\n-model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n-\n-\"\"\"\n-## Set up callbacks\n-\"\"\"\n-\n-train_callbacks = [\n-    keras.callbacks.EarlyStopping(\n-        monitor=\"val_accuracy\", patience=2, restore_best_weights=True\n-    )\n-]\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-history = model.fit(\n-    pipeline_train,\n-    batch_size=BATCH_SIZE,\n-    epochs=int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),\n-    steps_per_epoch=STEPS_PER_EPOCH,\n-    validation_data=pipeline_validation,\n-    callbacks=train_callbacks,\n-)\n-\n-\"\"\"\n-## Plot the training and validation metrics\n-\"\"\"\n-\n-\n-def plot_hist(hist):\n-    plt.plot(hist.history[\"accuracy\"])\n-    plt.plot(hist.history[\"val_accuracy\"])\n-    plt.plot(hist.history[\"loss\"])\n-    plt.plot(hist.history[\"val_loss\"])\n-    plt.title(\"Training Progress\")\n-    plt.ylabel(\"Accuracy/Loss\")\n-    plt.xlabel(\"Epochs\")\n-    plt.legend(\n-        [\"train_acc\", \"val_acc\", \"train_loss\", \"val_loss\"], loc=\"upper left\"\n-    )\n-    plt.show()\n-\n-\n-plot_hist(history)\n-\n-\"\"\"\n-## Evaluate the model\n-\"\"\"\n-\n-accuracy = model.evaluate(pipeline_validation)[1] * 100\n-print(\"Accuracy: {:.2f}%\".format(accuracy))\n-\n-\"\"\"\n-## Conclusion\n-\n-BiT performs well across a surprisingly wide range of data regimes\n--- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on\n-ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark\n-(VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class,\n-and 97.0% on CIFAR-10 with 10 examples per class.\n-\n-![](https://i.imgur.com/b1Lw5fz.png)\n-\n-You can experiment further with the BigTransfer Method by following the\n-[original paper](https://arxiv.org/abs/1912.11370).\n-\n-\n-**Example available on HuggingFace**\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-bit-black.svg)](https://huggingface.co/keras-io/bit) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-bit-black.svg)](https://huggingface.co/spaces/keras-io/siamese-contrastive) |\n-\"\"\"\n\n@@ -1,370 +0,0 @@\n-\"\"\"\n-Title: OCR model for reading Captchas\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2020/06/14\n-Last modified: 2020/06/26\n-Description: How to implement an OCR model using CNNs, RNNs and CTC loss.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates a simple OCR model built with the Functional API. Apart from\n-combining CNN and RNN, it also illustrates how you can instantiate a new layer\n-and use it as an \"Endpoint layer\" for implementing CTC loss. For a detailed\n-guide to layer subclassing, please check out\n-[this page](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)\n-in the developer guides.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-from pathlib import Path\n-from collections import Counter\n-\n-import tensorflow as tf\n-from tensorflow import keras\n-from tensorflow.keras import layers\n-\n-\n-\"\"\"\n-## Load the data: [Captcha Images](https://www.kaggle.com/fournierp/captcha-version-2-images)\n-Let's download the data.\n-\"\"\"\n-\n-\n-\"\"\"shell\n-curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\n-unzip -qq captcha_images_v2.zip\n-\"\"\"\n-\n-\n-\"\"\"\n-The dataset contains 1040 captcha files as `png` images. The label for each sample is a string,\n-the name of the file (minus the file extension).\n-We will map each character in the string to an integer for training the model. Similary,\n-we will need to map the predictions of the model back to strings. For this purpose\n-we will maintain two dictionaries, mapping characters to integers, and integers to characters,\n-respectively.\n-\"\"\"\n-\n-\n-# Path to the data directory\n-data_dir = Path(\"./captcha_images_v2/\")\n-\n-# Get list of all the images\n-images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n-labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n-characters = set(char for label in labels for char in label)\n-characters = sorted(list(characters))\n-\n-print(\"Number of images found: \", len(images))\n-print(\"Number of labels found: \", len(labels))\n-print(\"Number of unique characters: \", len(characters))\n-print(\"Characters present: \", characters)\n-\n-# Batch size for training and validation\n-batch_size = 16\n-\n-# Desired image dimensions\n-img_width = 200\n-img_height = 50\n-\n-# Factor by which the image is going to be downsampled\n-# by the convolutional blocks. We will be using two\n-# convolution blocks and each block will have\n-# a pooling layer which downsample the features by a factor of 2.\n-# Hence total downsampling factor would be 4.\n-downsample_factor = 4\n-\n-# Maximum length of any captcha in the dataset\n-max_length = max([len(label) for label in labels])\n-\n-\n-\"\"\"\n-## Preprocessing\n-\"\"\"\n-\n-\n-# Mapping characters to integers\n-char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None)\n-\n-# Mapping integers back to original characters\n-num_to_char = layers.StringLookup(\n-    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n-)\n-\n-\n-def split_data(images, labels, train_size=0.9, shuffle=True):\n-    # 1. Get the total size of the dataset\n-    size = len(images)\n-    # 2. Make an indices array and shuffle it, if required\n-    indices = np.arange(size)\n-    if shuffle:\n-        np.random.shuffle(indices)\n-    # 3. Get the size of training samples\n-    train_samples = int(size * train_size)\n-    # 4. Split data into training and validation sets\n-    x_train, y_train = (\n-        images[indices[:train_samples]],\n-        labels[indices[:train_samples]],\n-    )\n-    x_valid, y_valid = (\n-        images[indices[train_samples:]],\n-        labels[indices[train_samples:]],\n-    )\n-    return x_train, x_valid, y_train, y_valid\n-\n-\n-# Splitting data into training and validation sets\n-x_train, x_valid, y_train, y_valid = split_data(\n-    np.array(images), np.array(labels)\n-)\n-\n-\n-def encode_single_sample(img_path, label):\n-    # 1. Read image\n-    img = tf.io.read_file(img_path)\n-    # 2. Decode and convert to grayscale\n-    img = tf.io.decode_png(img, channels=1)\n-    # 3. Convert to float32 in [0, 1] range\n-    img = tf.image.convert_image_dtype(img, tf.float32)\n-    # 4. Resize to the desired size\n-    img = tf.image.resize(img, [img_height, img_width])\n-    # 5. Transpose the image because we want the time\n-    # dimension to correspond to the width of the image.\n-    img = tf.transpose(img, perm=[1, 0, 2])\n-    # 6. Map the characters in label to numbers\n-    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n-    # 7. Return a dict as our model is expecting two inputs\n-    return {\"image\": img, \"label\": label}\n-\n-\n-\"\"\"\n-## Create `Dataset` objects\n-\"\"\"\n-\n-\n-train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-train_dataset = (\n-    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n-    .batch(batch_size)\n-    .prefetch(buffer_size=tf.data.AUTOTUNE)\n-)\n-\n-validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n-validation_dataset = (\n-    validation_dataset.map(\n-        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n-    )\n-    .batch(batch_size)\n-    .prefetch(buffer_size=tf.data.AUTOTUNE)\n-)\n-\n-\"\"\"\n-## Visualize the data\n-\"\"\"\n-\n-\n-_, ax = plt.subplots(4, 4, figsize=(10, 5))\n-for batch in train_dataset.take(1):\n-    images = batch[\"image\"]\n-    labels = batch[\"label\"]\n-    for i in range(16):\n-        img = (images[i] * 255).numpy().astype(\"uint8\")\n-        label = (\n-            tf.strings.reduce_join(num_to_char(labels[i]))\n-            .numpy()\n-            .decode(\"utf-8\")\n-        )\n-        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n-        ax[i // 4, i % 4].set_title(label)\n-        ax[i // 4, i % 4].axis(\"off\")\n-plt.show()\n-\n-\"\"\"\n-## Model\n-\"\"\"\n-\n-\n-class CTCLayer(layers.Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.loss_fn = keras.backend.ctc_batch_cost\n-\n-    def call(self, y_true, y_pred):\n-        # Compute the training-time loss value and add it\n-        # to the layer using `self.add_loss()`.\n-        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n-        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n-        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n-\n-        input_length = input_length * tf.ones(\n-            shape=(batch_len, 1), dtype=\"int64\"\n-        )\n-        label_length = label_length * tf.ones(\n-            shape=(batch_len, 1), dtype=\"int64\"\n-        )\n-\n-        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n-        self.add_loss(loss)\n-\n-        # At test time, just return the computed predictions\n-        return y_pred\n-\n-\n-def build_model():\n-    # Inputs to the model\n-    input_img = layers.Input(\n-        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n-    )\n-    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n-\n-    # First conv block\n-    x = layers.Conv2D(\n-        32,\n-        (3, 3),\n-        activation=\"relu\",\n-        kernel_initializer=\"he_normal\",\n-        padding=\"same\",\n-        name=\"Conv1\",\n-    )(input_img)\n-    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n-\n-    # Second conv block\n-    x = layers.Conv2D(\n-        64,\n-        (3, 3),\n-        activation=\"relu\",\n-        kernel_initializer=\"he_normal\",\n-        padding=\"same\",\n-        name=\"Conv2\",\n-    )(x)\n-    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n-\n-    # We have used two max pool with pool size and strides 2.\n-    # Hence, downsampled feature maps are 4x smaller. The number of\n-    # filters in the last layer is 64. Reshape accordingly before\n-    # passing the output to the RNN part of the model\n-    new_shape = ((img_width // 4), (img_height // 4) * 64)\n-    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n-    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n-    x = layers.Dropout(0.2)(x)\n-\n-    # RNNs\n-    x = layers.Bidirectional(\n-        layers.LSTM(128, return_sequences=True, dropout=0.25)\n-    )(x)\n-    x = layers.Bidirectional(\n-        layers.LSTM(64, return_sequences=True, dropout=0.25)\n-    )(x)\n-\n-    # Output layer\n-    x = layers.Dense(\n-        len(char_to_num.get_vocabulary()) + 1,\n-        activation=\"softmax\",\n-        name=\"dense2\",\n-    )(x)\n-\n-    # Add CTC layer for calculating CTC loss at each step\n-    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n-\n-    # Define the model\n-    model = keras.models.Model(\n-        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n-    )\n-    # Optimizer\n-    opt = keras.optimizers.Adam()\n-    # Compile the model and return\n-    model.compile(optimizer=opt)\n-    return model\n-\n-\n-# Get the model\n-model = build_model()\n-model.summary()\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-\n-epochs = 1\n-early_stopping_patience = 10\n-# Add early stopping\n-early_stopping = keras.callbacks.EarlyStopping(\n-    monitor=\"val_loss\",\n-    patience=early_stopping_patience,\n-    restore_best_weights=True,\n-)\n-\n-# Train the model\n-history = model.fit(\n-    train_dataset,\n-    validation_data=validation_dataset,\n-    epochs=epochs,\n-    callbacks=[early_stopping],\n-)\n-\n-\n-\"\"\"\n-## Inference\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/ocr-for-captcha)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/ocr-for-captcha).\n-\"\"\"\n-\n-\n-# Get the prediction model by extracting layers till the output layer\n-prediction_model = keras.models.Model(\n-    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n-)\n-prediction_model.summary()\n-\n-\n-# A utility function to decode the output of the network\n-def decode_batch_predictions(pred):\n-    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n-    # Use greedy search. For complex tasks, you can use beam search\n-    results = keras.backend.ctc_decode(\n-        pred, input_length=input_len, greedy=True\n-    )[0][0][:, :max_length]\n-    # Iterate over the results and get back the text\n-    output_text = []\n-    for res in results:\n-        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n-        output_text.append(res)\n-    return output_text\n-\n-\n-#  Let's check results on some validation samples\n-for batch in validation_dataset.take(1):\n-    batch_images = batch[\"image\"]\n-    batch_labels = batch[\"label\"]\n-\n-    preds = prediction_model.predict(batch_images)\n-    pred_texts = decode_batch_predictions(preds)\n-\n-    orig_texts = []\n-    for label in batch_labels:\n-        label = (\n-            tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n-        )\n-        orig_texts.append(label)\n-\n-    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n-    for i in range(len(pred_texts)):\n-        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n-        img = img.T\n-        title = f\"Prediction: {pred_texts[i]}\"\n-        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n-        ax[i // 4, i % 4].set_title(title)\n-        ax[i // 4, i % 4].axis(\"off\")\n-plt.show()\n\n@@ -1,391 +0,0 @@\n-\"\"\"\n-Title: CutMix data augmentation for image classification\n-Author: [Sayan Nath](https://twitter.com/sayannath2350)\n-Date created: 2021/06/08\n-Last modified: 2023/11/14\n-Description: Data augmentation with CutMix for image classification on CIFAR-10.\n-Accelerator: GPU\n-Converted to Keras 3 By: [Piyush Thakur](https://github.com/cosmo3769)\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\"\"\"\n-\n-\"\"\"\n-_CutMix_ is a data augmentation technique that addresses the issue of information loss\n-and inefficiency present in regional dropout strategies.\n-Instead of removing pixels and filling them with black or grey pixels or Gaussian noise,\n-you replace the removed regions with a patch from another image,\n-while the ground truth labels are mixed proportionally to the number of pixels of combined images.\n-CutMix was proposed in\n-[CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/abs/1905.04899)\n-(Yun et al., 2019)\n-\n-It's implemented via the following formulas:\n-\n-<img src=\"https://i.imgur.com/cGvd13V.png\" width=\"200\"/>\n-\n-where `M` is the binary mask which indicates the cutout and the fill-in\n-regions from the two randomly drawn images and `λ` (in `[0, 1]`) is drawn from a\n-[`Beta(α, α)` distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n-\n-The coordinates of bounding boxes are:\n-\n-<img src=\"https://i.imgur.com/eNisep4.png\" width=\"150\"/>\n-\n-which indicates the cutout and fill-in regions in case of the images.\n-The bounding box sampling is represented by:\n-\n-<img src=\"https://i.imgur.com/Snph9aj.png\" width=\"200\"/>\n-\n-where `rx, ry` are randomly drawn from a uniform distribution with upper bound.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-import matplotlib.pyplot as plt\n-\n-from keras import layers\n-\n-# TF imports related to tf.data preprocessing\n-from tensorflow import clip_by_value\n-from tensorflow import data as tf_data\n-from tensorflow import image as tf_image\n-from tensorflow import random as tf_random\n-\n-keras.utils.set_random_seed(42)\n-\n-\"\"\"\n-## Load the CIFAR-10 dataset\n-\n-In this example, we will use the\n-[CIFAR-10 image classification dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-y_train = keras.utils.to_categorical(y_train, num_classes=10)\n-y_test = keras.utils.to_categorical(y_test, num_classes=10)\n-\n-print(x_train.shape)\n-print(y_train.shape)\n-print(x_test.shape)\n-print(y_test.shape)\n-\n-class_names = [\n-    \"Airplane\",\n-    \"Automobile\",\n-    \"Bird\",\n-    \"Cat\",\n-    \"Deer\",\n-    \"Dog\",\n-    \"Frog\",\n-    \"Horse\",\n-    \"Ship\",\n-    \"Truck\",\n-]\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-AUTO = tf_data.AUTOTUNE\n-BATCH_SIZE = 32\n-IMG_SIZE = 32\n-\n-\"\"\"\n-## Define the image preprocessing function\n-\"\"\"\n-\n-\n-def preprocess_image(image, label):\n-    image = tf_image.resize(image, (IMG_SIZE, IMG_SIZE))\n-    image = tf_image.convert_image_dtype(image, \"float32\") / 255.0\n-    label = keras.ops.cast(label, dtype=\"float32\")\n-    return image, label\n-\n-\n-\"\"\"\n-## Convert the data into TensorFlow `Dataset` objects\n-\"\"\"\n-\n-train_ds_one = (\n-    tf_data.Dataset.from_tensor_slices((x_train, y_train))\n-    .shuffle(1024)\n-    .map(preprocess_image, num_parallel_calls=AUTO)\n-)\n-train_ds_two = (\n-    tf_data.Dataset.from_tensor_slices((x_train, y_train))\n-    .shuffle(1024)\n-    .map(preprocess_image, num_parallel_calls=AUTO)\n-)\n-\n-train_ds_simple = tf_data.Dataset.from_tensor_slices((x_train, y_train))\n-\n-test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))\n-\n-train_ds_simple = (\n-    train_ds_simple.map(preprocess_image, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-# Combine two shuffled datasets from the same training data.\n-train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))\n-\n-test_ds = (\n-    test_ds.map(preprocess_image, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-\"\"\"\n-## Define the CutMix data augmentation function\n-\n-The CutMix function takes two `image` and `label` pairs to perform the augmentation.\n-It samples `λ(l)` from the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n-and returns a bounding box from `get_box` function. We then crop the second image (`image2`)\n-and pad this image in the final padded image at the same location.\n-\"\"\"\n-\n-\n-def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n-    gamma_1_sample = tf_random.gamma(shape=[size], alpha=concentration_1)\n-    gamma_2_sample = tf_random.gamma(shape=[size], alpha=concentration_0)\n-    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n-\n-\n-def get_box(lambda_value):\n-    cut_rat = keras.ops.sqrt(1.0 - lambda_value)\n-\n-    cut_w = IMG_SIZE * cut_rat  # rw\n-    cut_w = keras.ops.cast(cut_w, \"int32\")\n-\n-    cut_h = IMG_SIZE * cut_rat  # rh\n-    cut_h = keras.ops.cast(cut_h, \"int32\")\n-\n-    cut_x = keras.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # rx\n-    cut_x = keras.ops.cast(cut_x, \"int32\")\n-    cut_y = keras.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # ry\n-    cut_y = keras.ops.cast(cut_y, \"int32\")\n-\n-    boundaryx1 = clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n-    boundaryy1 = clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n-    bbx2 = clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n-    bby2 = clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n-\n-    target_h = bby2 - boundaryy1\n-    if target_h == 0:\n-        target_h += 1\n-\n-    target_w = bbx2 - boundaryx1\n-    if target_w == 0:\n-        target_w += 1\n-\n-    return boundaryx1, boundaryy1, target_h, target_w\n-\n-\n-def cutmix(train_ds_one, train_ds_two):\n-    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n-\n-    alpha = [0.25]\n-    beta = [0.25]\n-\n-    # Get a sample from the Beta distribution\n-    lambda_value = sample_beta_distribution(1, alpha, beta)\n-\n-    # Define Lambda\n-    lambda_value = lambda_value[0][0]\n-\n-    # Get the bounding box offsets, heights and widths\n-    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n-\n-    # Get a patch from the second image (`image2`)\n-    crop2 = tf_image.crop_to_bounding_box(\n-        image2, boundaryy1, boundaryx1, target_h, target_w\n-    )\n-    # Pad the `image2` patch (`crop2`) with the same offset\n-    image2 = tf_image.pad_to_bounding_box(\n-        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n-    )\n-    # Get a patch from the first image (`image1`)\n-    crop1 = tf_image.crop_to_bounding_box(\n-        image1, boundaryy1, boundaryx1, target_h, target_w\n-    )\n-    # Pad the `image1` patch (`crop1`) with the same offset\n-    img1 = tf_image.pad_to_bounding_box(\n-        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n-    )\n-\n-    # Modify the first image by subtracting the patch from `image1`\n-    # (before applying the `image2` patch)\n-    image1 = image1 - img1\n-    # Add the modified `image1` and `image2`  together to get the CutMix image\n-    image = image1 + image2\n-\n-    # Adjust Lambda in accordance to the pixel ration\n-    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n-    lambda_value = keras.ops.cast(lambda_value, \"float32\")\n-\n-    # Combine the labels of both images\n-    label = lambda_value * label1 + (1 - lambda_value) * label2\n-    return image, label\n-\n-\n-\"\"\"\n-**Note**: we are combining two images to create a single one.\n-\n-## Visualize the new dataset after applying the CutMix augmentation\n-\"\"\"\n-\n-# Create the new dataset using our `cutmix` utility\n-train_ds_cmu = (\n-    train_ds.shuffle(1024)\n-    .map(cutmix, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-# Let's preview 9 samples from the dataset\n-image_batch, label_batch = next(iter(train_ds_cmu))\n-plt.figure(figsize=(10, 10))\n-for i in range(9):\n-    ax = plt.subplot(3, 3, i + 1)\n-    plt.title(class_names[np.argmax(label_batch[i])])\n-    plt.imshow(image_batch[i])\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Define a ResNet-20 model\n-\"\"\"\n-\n-\n-def resnet_layer(\n-    inputs,\n-    num_filters=16,\n-    kernel_size=3,\n-    strides=1,\n-    activation=\"relu\",\n-    batch_normalization=True,\n-    conv_first=True,\n-):\n-    conv = layers.Conv2D(\n-        num_filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=\"same\",\n-        kernel_initializer=\"he_normal\",\n-        kernel_regularizer=keras.regularizers.L2(1e-4),\n-    )\n-    x = inputs\n-    if conv_first:\n-        x = conv(x)\n-        if batch_normalization:\n-            x = layers.BatchNormalization()(x)\n-        if activation is not None:\n-            x = layers.Activation(activation)(x)\n-    else:\n-        if batch_normalization:\n-            x = layers.BatchNormalization()(x)\n-        if activation is not None:\n-            x = layers.Activation(activation)(x)\n-        x = conv(x)\n-    return x\n-\n-\n-def resnet_v20(input_shape, depth, num_classes=10):\n-    if (depth - 2) % 6 != 0:\n-        raise ValueError(\"depth should be 6n+2 (eg 20, 32, 44 in [a])\")\n-    # Start model definition.\n-    num_filters = 16\n-    num_res_blocks = int((depth - 2) / 6)\n-\n-    inputs = layers.Input(shape=input_shape)\n-    x = resnet_layer(inputs=inputs)\n-    # Instantiate the stack of residual units\n-    for stack in range(3):\n-        for res_block in range(num_res_blocks):\n-            strides = 1\n-            if stack > 0 and res_block == 0:  # first layer but not first stack\n-                strides = 2  # downsample\n-            y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n-            y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n-            if stack > 0 and res_block == 0:  # first layer but not first stack\n-                # linear projection residual shortcut connection to match\n-                # changed dims\n-                x = resnet_layer(\n-                    inputs=x,\n-                    num_filters=num_filters,\n-                    kernel_size=1,\n-                    strides=strides,\n-                    activation=None,\n-                    batch_normalization=False,\n-                )\n-            x = layers.add([x, y])\n-            x = layers.Activation(\"relu\")(x)\n-        num_filters *= 2\n-\n-    # Add classifier on top.\n-    # v1 does not use BN after last shortcut connection-ReLU\n-    x = layers.AveragePooling2D(pool_size=8)(x)\n-    y = layers.Flatten()(x)\n-    outputs = layers.Dense(\n-        num_classes, activation=\"softmax\", kernel_initializer=\"he_normal\"\n-    )(y)\n-\n-    # Instantiate model.\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-def training_model():\n-    return resnet_v20((32, 32, 3), 20)\n-\n-\n-initial_model = training_model()\n-initial_model.save_weights(\"initial_weights.weights.h5\")\n-\n-\"\"\"\n-## Train the model with the dataset augmented by CutMix\n-\"\"\"\n-\n-model = training_model()\n-model.load_weights(\"initial_weights.weights.h5\")\n-\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-model.fit(train_ds_cmu, validation_data=test_ds, epochs=15)\n-\n-test_loss, test_accuracy = model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n-\n-\"\"\"\n-## Train the model using the original non-augmented dataset\n-\"\"\"\n-\n-model = training_model()\n-model.load_weights(\"initial_weights.weights.h5\")\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-model.fit(train_ds_simple, validation_data=test_ds, epochs=15)\n-\n-test_loss, test_accuracy = model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n-\n-\"\"\"\n-## Notes\n-\n-In this example, we trained our model for 15 epochs.\n-In our experiment, the model with CutMix achieves a better accuracy on the CIFAR-10 dataset\n-(77.34% in our experiment) compared to the model that doesn't use the augmentation (66.90%).\n-You may notice it takes less time to train the model with the CutMix augmentation.\n-\n-You can experiment further with the CutMix technique by following the\n-[original paper](https://arxiv.org/abs/1905.04899).\n-\"\"\"\n\n@@ -1,625 +0,0 @@\n-\"\"\"\n-Title: Distilling Vision Transformers\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2022/04/05\n-Last modified: 2023/09/16\n-Description: Distillation of Vision Transformers through attention.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In the original *Vision Transformers* (ViT) paper\n-([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)),\n-the authors concluded that to perform on par with Convolutional Neural Networks (CNNs),\n-ViTs need to be pre-trained on larger datasets. The larger the better. This is mainly\n-due to the lack of inductive biases in the ViT architecture -- unlike CNNs,\n-they don't have layers that exploit locality. In a follow-up paper\n-([Steiner et al.](https://arxiv.org/abs/2106.10270)),\n-the authors show that it is possible to substantially improve the performance of ViTs\n-with stronger regularization and longer training.\n-\n-Many groups have proposed different ways to deal with the problem\n-of data-intensiveness of ViT training.\n-One such way was shown in the *Data-efficient image Transformers*,\n-(DeiT) paper ([Touvron et al.](https://arxiv.org/abs/2012.12877)). The\n-authors introduced a distillation technique that is specific to transformer-based vision\n-models. DeiT is among the first works to show that it's possible to train ViTs well\n-without using larger datasets.\n-\n-In this example, we implement the distillation recipe proposed in DeiT. This\n-requires us to slightly tweak the original ViT architecture and write a custom training\n-loop to implement the distillation recipe.\n-\n-To comfortably navigate through this example, you'll be expected to know how a ViT and\n-knowledge distillation work. The following are good resources in case you needed a\n-refresher:\n-\n-* [ViT on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer)\n-* [Knowledge distillation on keras.io](https://keras.io/examples/vision/knowledge_distillation/)\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-import keras\n-from keras import layers\n-\n-tfds.disable_progress_bar()\n-keras.utils.set_random_seed(42)\n-\n-\"\"\"\n-## Constants\n-\"\"\"\n-\n-# Model\n-MODEL_TYPE = \"deit_distilled_tiny_patch16_224\"\n-RESOLUTION = 224\n-PATCH_SIZE = 16\n-NUM_PATCHES = (RESOLUTION // PATCH_SIZE) ** 2\n-LAYER_NORM_EPS = 1e-6\n-PROJECTION_DIM = 192\n-NUM_HEADS = 3\n-NUM_LAYERS = 12\n-MLP_UNITS = [\n-    PROJECTION_DIM * 4,\n-    PROJECTION_DIM,\n-]\n-DROPOUT_RATE = 0.0\n-DROP_PATH_RATE = 0.1\n-\n-# Training\n-NUM_EPOCHS = 20\n-BASE_LR = 0.0005\n-WEIGHT_DECAY = 0.0001\n-\n-# Data\n-BATCH_SIZE = 256\n-AUTO = tf.data.AUTOTUNE\n-NUM_CLASSES = 5\n-\n-\"\"\"\n-You probably noticed that `DROPOUT_RATE` has been set 0.0. Dropout has been used\n-in the implementation to keep it complete. For smaller models (like the one used in\n-this example), you don't need it, but for bigger models, using dropout helps.\n-\"\"\"\n-\n-\"\"\"\n-## Load the `tf_flowers` dataset and prepare preprocessing utilities\n-\n-The authors use an array of different augmentation techniques, including MixUp\n-([Zhang et al.](https://arxiv.org/abs/1710.09412)),\n-RandAugment ([Cubuk et al.](https://arxiv.org/abs/1909.13719)),\n-and so on. However, to keep the example simple to work through, we'll discard them.\n-\"\"\"\n-\n-\n-def preprocess_dataset(is_training=True):\n-    def fn(image, label):\n-        if is_training:\n-            # Resize to a bigger spatial resolution and take the random\n-            # crops.\n-            image = tf.image.resize(image, (RESOLUTION + 20, RESOLUTION + 20))\n-            image = tf.image.random_crop(image, (RESOLUTION, RESOLUTION, 3))\n-            image = tf.image.random_flip_left_right(image)\n-        else:\n-            image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n-        label = tf.one_hot(label, depth=NUM_CLASSES)\n-        return image, label\n-\n-    return fn\n-\n-\n-def prepare_dataset(dataset, is_training=True):\n-    if is_training:\n-        dataset = dataset.shuffle(BATCH_SIZE * 10)\n-    dataset = dataset.map(\n-        preprocess_dataset(is_training), num_parallel_calls=AUTO\n-    )\n-    return dataset.batch(BATCH_SIZE).prefetch(AUTO)\n-\n-\n-train_dataset, val_dataset = tfds.load(\n-    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n-)\n-num_train = train_dataset.cardinality()\n-num_val = val_dataset.cardinality()\n-print(f\"Number of training examples: {num_train}\")\n-print(f\"Number of validation examples: {num_val}\")\n-\n-train_dataset = prepare_dataset(train_dataset, is_training=True)\n-val_dataset = prepare_dataset(val_dataset, is_training=False)\n-\n-\"\"\"\n-## Implementing the DeiT variants of ViT\n-\n-Since DeiT is an extension of ViT it'd make sense to first implement ViT and then extend\n-it to support DeiT's components.\n-\n-First, we'll implement a layer for Stochastic Depth\n-([Huang et al.](https://arxiv.org/abs/1603.09382))\n-which is used in DeiT for regularization.\n-\"\"\"\n-\n-\n-# Referred from: github.com:rwightman/pytorch-image-models.\n-class StochasticDepth(layers.Layer):\n-    def __init__(self, drop_prop, **kwargs):\n-        super().__init__(**kwargs)\n-        self.drop_prob = drop_prop\n-\n-    def call(self, x, training=True):\n-        if training:\n-            keep_prob = 1 - self.drop_prob\n-            shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n-            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n-            random_tensor = tf.floor(random_tensor)\n-            return (x / keep_prob) * random_tensor\n-        return x\n-\n-\n-\"\"\"\n-Now, we'll implement the MLP and Transformer blocks.\n-\"\"\"\n-\n-\n-def mlp(x, dropout_rate: float, hidden_units):\n-    \"\"\"FFN for a Transformer block.\"\"\"\n-    # Iterate over the hidden units and\n-    # add Dense => Dropout.\n-    for idx, units in enumerate(hidden_units):\n-        x = layers.Dense(\n-            units,\n-            activation=tf.nn.gelu if idx == 0 else None,\n-        )(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-def transformer(drop_prob: float, name: str) -> keras.Model:\n-    \"\"\"Transformer block with pre-norm.\"\"\"\n-    num_patches = (\n-        NUM_PATCHES + 2 if \"distilled\" in MODEL_TYPE else NUM_PATCHES + 1\n-    )\n-    encoded_patches = layers.Input((num_patches, PROJECTION_DIM))\n-\n-    # Layer normalization 1.\n-    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n-\n-    # Multi Head Self Attention layer 1.\n-    attention_output = layers.MultiHeadAttention(\n-        num_heads=NUM_HEADS,\n-        key_dim=PROJECTION_DIM,\n-        dropout=DROPOUT_RATE,\n-    )(x1, x1)\n-    attention_output = (\n-        StochasticDepth(drop_prob)(attention_output)\n-        if drop_prob\n-        else attention_output\n-    )\n-\n-    # Skip connection 1.\n-    x2 = layers.Add()([attention_output, encoded_patches])\n-\n-    # Layer normalization 2.\n-    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n-\n-    # MLP layer 1.\n-    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=DROPOUT_RATE)\n-    x4 = StochasticDepth(drop_prob)(x4) if drop_prob else x4\n-\n-    # Skip connection 2.\n-    outputs = layers.Add()([x2, x4])\n-\n-    return keras.Model(encoded_patches, outputs, name=name)\n-\n-\n-\"\"\"\n-We'll now implement a `ViTClassifier` class building on top of the components we just\n-developed. Here we'll be following the original pooling strategy used in the ViT paper --\n-use a class token and use the feature representations corresponding to it for\n-classification.\n-\"\"\"\n-\n-\n-class ViTClassifier(keras.Model):\n-    \"\"\"Vision Transformer base class.\"\"\"\n-\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        # Patchify + linear projection + reshaping.\n-        self.projection = keras.Sequential(\n-            [\n-                layers.Conv2D(\n-                    filters=PROJECTION_DIM,\n-                    kernel_size=(PATCH_SIZE, PATCH_SIZE),\n-                    strides=(PATCH_SIZE, PATCH_SIZE),\n-                    padding=\"VALID\",\n-                    name=\"conv_projection\",\n-                ),\n-                layers.Reshape(\n-                    target_shape=(NUM_PATCHES, PROJECTION_DIM),\n-                    name=\"flatten_projection\",\n-                ),\n-            ],\n-            name=\"projection\",\n-        )\n-\n-        # Positional embedding.\n-        init_shape = (\n-            1,\n-            NUM_PATCHES + 1,\n-            PROJECTION_DIM,\n-        )\n-        self.positional_embedding = tf.Variable(\n-            tf.zeros(init_shape), name=\"pos_embedding\"\n-        )\n-\n-        # Transformer blocks.\n-        dpr = [x for x in tf.linspace(0.0, DROP_PATH_RATE, NUM_LAYERS)]\n-        self.transformer_blocks = [\n-            transformer(drop_prob=dpr[i], name=f\"transformer_block_{i}\")\n-            for i in range(NUM_LAYERS)\n-        ]\n-\n-        # CLS token.\n-        initial_value = tf.zeros((1, 1, PROJECTION_DIM))\n-        self.cls_token = tf.Variable(\n-            initial_value=initial_value, trainable=True, name=\"cls\"\n-        )\n-\n-        # Other layers.\n-        self.dropout = layers.Dropout(DROPOUT_RATE)\n-        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n-        self.head = layers.Dense(\n-            NUM_CLASSES,\n-            name=\"classification_head\",\n-        )\n-\n-    def call(self, inputs):\n-        n = tf.shape(inputs)[0]\n-\n-        # Create patches and project the patches.\n-        projected_patches = self.projection(inputs)\n-\n-        # Append class token if needed.\n-        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n-        cls_token = tf.cast(cls_token, projected_patches.dtype)\n-        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n-\n-        # Add positional embeddings to the projected patches.\n-        encoded_patches = (\n-            self.positional_embedding + projected_patches\n-        )  # (B, number_patches, projection_dim)\n-        encoded_patches = self.dropout(encoded_patches)\n-\n-        # Iterate over the number of layers and stack up blocks of\n-        # Transformer.\n-        for transformer_module in self.transformer_blocks:\n-            # Add a Transformer block.\n-            encoded_patches = transformer_module(encoded_patches)\n-\n-        # Final layer normalization.\n-        representation = self.layer_norm(encoded_patches)\n-\n-        # Pool representation.\n-        encoded_patches = representation[:, 0]\n-\n-        # Classification head.\n-        output = self.head(encoded_patches)\n-        return output\n-\n-\n-\"\"\"\n-This class can be used standalone as ViT and is end-to-end trainable. Just remove the\n-`distilled` phrase in `MODEL_TYPE` and it should work with `vit_tiny = ViTClassifier()`.\n-Let's now extend it to DeiT. The following figure presents the schematic of DeiT (taken\n-from the DeiT paper):\n-\n-![](https://i.imgur.com/5lmg2Xs.png)\n-\n-Apart from the class token, DeiT has another token for distillation. During distillation,\n-the logits corresponding to the class token are compared to the true labels, and the\n-logits corresponding to the distillation token are compared to the teacher's predictions.\n-\"\"\"\n-\n-\n-class ViTDistilled(ViTClassifier):\n-    def __init__(self, regular_training=False, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_tokens = 2\n-        self.regular_training = regular_training\n-\n-        # CLS and distillation tokens, positional embedding.\n-        init_value = tf.zeros((1, 1, PROJECTION_DIM))\n-        self.dist_token = tf.Variable(init_value, name=\"dist_token\")\n-        self.positional_embedding = tf.Variable(\n-            tf.zeros(\n-                (\n-                    1,\n-                    NUM_PATCHES + self.num_tokens,\n-                    PROJECTION_DIM,\n-                )\n-            ),\n-            name=\"pos_embedding\",\n-        )\n-\n-        # Head layers.\n-        self.head = layers.Dense(\n-            NUM_CLASSES,\n-            name=\"classification_head\",\n-        )\n-        self.head_dist = layers.Dense(\n-            NUM_CLASSES,\n-            name=\"distillation_head\",\n-        )\n-\n-    def call(self, inputs, training=False):\n-        n = tf.shape(inputs)[0]\n-\n-        # Create patches and project the patches.\n-        projected_patches = self.projection(inputs)\n-\n-        # Append the tokens.\n-        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n-        dist_token = tf.tile(self.dist_token, (n, 1, 1))\n-        cls_token = tf.cast(cls_token, projected_patches.dtype)\n-        dist_token = tf.cast(dist_token, projected_patches.dtype)\n-        projected_patches = tf.concat(\n-            [cls_token, dist_token, projected_patches], axis=1\n-        )\n-\n-        # Add positional embeddings to the projected patches.\n-        encoded_patches = (\n-            self.positional_embedding + projected_patches\n-        )  # (B, number_patches, projection_dim)\n-        encoded_patches = self.dropout(encoded_patches)\n-\n-        # Iterate over the number of layers and stack up blocks of\n-        # Transformer.\n-        for transformer_module in self.transformer_blocks:\n-            # Add a Transformer block.\n-            encoded_patches = transformer_module(encoded_patches)\n-\n-        # Final layer normalization.\n-        representation = self.layer_norm(encoded_patches)\n-\n-        # Classification heads.\n-        x, x_dist = (\n-            self.head(representation[:, 0]),\n-            self.head_dist(representation[:, 1]),\n-        )\n-\n-        if not training or self.regular_training:\n-            # During standard train / finetune, inference average the classifier\n-            # predictions.\n-            return (x + x_dist) / 2\n-\n-        elif training:\n-            # Only return separate classification predictions when training in distilled\n-            # mode.\n-            return x, x_dist\n-\n-\n-\"\"\"\n-Let's verify if the `ViTDistilled` class can be initialized and called as expected.\n-\"\"\"\n-\n-deit_tiny_distilled = ViTDistilled()\n-\n-dummy_inputs = tf.ones((2, 224, 224, 3))\n-outputs = deit_tiny_distilled(dummy_inputs, training=False)\n-print(f\"output_shape: {outputs.shape}\")\n-\n-\"\"\"\n-## Implementing the trainer\n-\n-Unlike what happens in standard knowledge distillation\n-([Hinton et al.](https://arxiv.org/abs/1503.02531)),\n-where a temperature-scaled softmax is used as well as KL divergence,\n-DeiT authors use the following loss function:\n-\n-![](https://i.imgur.com/bXdxsBq.png)\n-\n-\n-Here,\n-\n-* CE is cross-entropy\n-* `psi` is the softmax function\n-* Z_s denotes student predictions\n-* y denotes true labels\n-* y_t denotes teacher predictions\n-\"\"\"\n-\n-\n-class DeiT(keras.Model):\n-    # Reference:\n-    # https://keras.io/examples/vision/knowledge_distillation/\n-    def __init__(self, student, teacher, **kwargs):\n-        super().__init__(**kwargs)\n-        self.student = student\n-        self.teacher = teacher\n-        self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")\n-        self.distillation_loss_tracker = keras.metrics.Mean(\n-            name=\"distillation_loss\"\n-        )\n-        self.accuracy = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.accuracy,\n-            self.student_loss_tracker,\n-            self.distillation_loss_tracker,\n-        ]\n-\n-    def compile(\n-        self,\n-        optimizer,\n-        student_loss_fn,\n-        distillation_loss_fn,\n-        run_eagerly=False,\n-        jit_compile=False,\n-    ):\n-        super().compile(\n-            optimizer=optimizer,\n-            run_eagerly=run_eagerly,\n-            jit_compile=jit_compile,\n-        )\n-        self.student_loss_fn = student_loss_fn\n-        self.distillation_loss_fn = distillation_loss_fn\n-\n-    def train_step(self, data):\n-        # Unpack data.\n-        x, y = data\n-\n-        # Forward pass of teacher\n-        teacher_predictions = self.teacher(x)[\"dense\"]\n-        teacher_predictions = tf.nn.softmax(teacher_predictions, axis=-1)\n-\n-        with tf.GradientTape() as tape:\n-            # Forward pass of student.\n-            cls_predictions, dist_predictions = self.student(\n-                x / 255.0, training=True\n-            )\n-\n-            # Compute losses.\n-            student_loss = self.student_loss_fn(y, cls_predictions)\n-            distillation_loss = self.distillation_loss_fn(\n-                teacher_predictions, dist_predictions\n-            )\n-            loss = (student_loss + distillation_loss) / 2\n-\n-        # Compute gradients.\n-        trainable_vars = self.student.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-\n-        # Update weights.\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-\n-        # Update the metrics configured in `compile()`.\n-        student_predictions = (cls_predictions + dist_predictions) / 2\n-        self.student_loss_tracker.update_state(student_loss)\n-        self.distillation_loss_tracker.update_state(distillation_loss)\n-        self.accuracy.update_state(y, student_predictions)\n-\n-        # Return a dict of performance.\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, data):\n-        # Unpack the data.\n-        x, y = data\n-\n-        # Compute predictions.\n-        y_prediction = self.student(x / 255.0)\n-\n-        # Calculate the loss.\n-        student_loss = self.student_loss_fn(y, y_prediction)\n-\n-        # Update the metrics.\n-        self.student_loss_tracker.update_state(student_loss)\n-        self.accuracy.update_state(y, y_prediction)\n-\n-        # Return a dict of performance.\n-        results = {m.name: m.result() for m in self.metrics}\n-        return results\n-\n-    def call(self, inputs):\n-        return self.student(inputs / 255.0)\n-\n-\n-\"\"\"\n-## Load the teacher model\n-\n-This model is based on the BiT family of ResNets\n-([Kolesnikov et al.](https://arxiv.org/abs/1912.11370))\n-fine-tuned on the `tf_flowers` dataset. You can refer to\n-[this notebook](https://github.com/sayakpaul/deit-tf/blob/main/notebooks/bit-teacher.ipynb)\n-to know how the training was performed. The teacher model has about 212 Million parameters\n-which is about **40x more** than the student.\n-\"\"\"\n-\n-\"\"\"shell\n-wget -q https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip\n-unzip -q bit_teacher_flowers.zip\n-\"\"\"\n-\n-bit_teacher_flowers = keras.layers.TFSMLayer(\n-    filepath=\"bit_teacher_flowers\",\n-    call_endpoint=\"serving_default\",\n-)\n-\n-\n-\"\"\"\n-## Training through distillation\n-\"\"\"\n-\n-deit_tiny = ViTDistilled()\n-deit_distiller = DeiT(student=deit_tiny, teacher=bit_teacher_flowers)\n-\n-lr_scaled = (BASE_LR / 512) * BATCH_SIZE\n-deit_distiller.compile(\n-    optimizer=keras.optimizers.AdamW(\n-        weight_decay=WEIGHT_DECAY, learning_rate=lr_scaled\n-    ),\n-    student_loss_fn=keras.losses.CategoricalCrossentropy(\n-        from_logits=True, label_smoothing=0.1\n-    ),\n-    distillation_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n-)\n-_ = deit_distiller.fit(\n-    train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS\n-)\n-\n-\"\"\"\n-If we had trained the same model (the `ViTClassifier`) from scratch with the exact same\n-hyperparameters, the model would have scored about 59% accuracy. You can adapt the following code\n-to reproduce this result:\n-\n-```\n-vit_tiny = ViTClassifier()\n-\n-inputs = keras.Input((RESOLUTION, RESOLUTION, 3))\n-x = keras.layers.Rescaling(scale=1./255)(inputs)\n-outputs = deit_tiny(x)\n-model = keras.Model(inputs, outputs)\n-\n-model.compile(...)\n-model.fit(...)\n-```\n-\"\"\"\n-\n-\"\"\"\n-## Notes\n-\n-* Through the use of distillation, we're effectively transferring the inductive biases of\n-a CNN-based teacher model.\n-* Interestingly enough, this distillation strategy works better with a CNN as the teacher\n-model rather than a Transformer as shown in the paper.\n-* The use of regularization to train DeiT models is very important.\n-* ViT models are initialized with a combination of different initializers including\n-truncated normal, random normal, Glorot uniform, etc. If you're looking for\n-end-to-end reproduction of the original results, don't forget to initialize the ViTs well.\n-* If you want to explore the pre-trained DeiT models in TensorFlow and Keras with code\n-for fine-tuning, [check out these models on TF-Hub](https://tfhub.dev/sayakpaul/collections/deit/1).\n-\n-## Acknowledgements\n-\n-* Ross Wightman for keeping\n-[`timm`](https://github.com/rwightman/pytorch-image-models)\n-updated with readable implementations. I referred to the implementations of ViT and DeiT\n-a lot during implementing them in TensorFlow.\n-* [Aritra Roy Gosthipaty](https://github.com/ariG23498)\n-who implemented some portions of the `ViTClassifier` in another project.\n-* [Google Developers Experts](https://developers.google.com/programs/experts/)\n-program for supporting me with GCP credits which were used to run experiments for this\n-example.\n-\"\"\"\n\n@@ -1,211 +0,0 @@\n-\"\"\"\n-Title: Grad-CAM class activation visualization\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/04/26\n-Last modified: 2021/03/07\n-Description: How to obtain a class activation heatmap for an image classification model.\n-Accelerator: NONE\n-\"\"\"\n-\"\"\"\n-Adapted from Deep Learning with Python (2017).\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-\n-# Display\n-from IPython.display import Image, display\n-import matplotlib.pyplot as plt\n-import matplotlib.cm as cm\n-\n-keras.config.disable_traceback_filtering()\n-\n-\n-\"\"\"\n-## Configurable parameters\n-\n-You can change these to another model.\n-\n-To get the values for `last_conv_layer_name` use `model.summary()`\n-to see the names of all layers in the model.\n-\n-\"\"\"\n-\n-model_builder = keras.applications.xception.Xception\n-img_size = (299, 299)\n-preprocess_input = keras.applications.xception.preprocess_input\n-decode_predictions = keras.applications.xception.decode_predictions\n-\n-last_conv_layer_name = \"block14_sepconv2_act\"\n-\n-# The local path to our target image\n-img_path = keras.utils.get_file(\n-    fname=\"african_elephant.jpg\", origin=\"https://i.imgur.com/Bvro0YD.png\"\n-)\n-\n-display(Image(img_path))\n-\n-\n-\"\"\"\n-## The Grad-CAM algorithm\n-\"\"\"\n-\n-\n-def get_img_array(img_path, size):\n-    # `img` is a PIL image of size 299x299\n-    img = keras.utils.load_img(img_path, target_size=size)\n-    # `array` is a float32 Numpy array of shape (299, 299, 3)\n-    array = keras.utils.img_to_array(img)\n-    # We add a dimension to transform our array into a \"batch\"\n-    # of size (1, 299, 299, 3)\n-    array = np.expand_dims(array, axis=0)\n-    return array\n-\n-\n-def make_gradcam_heatmap(\n-    img_array, model, last_conv_layer_name, pred_index=None\n-):\n-    # First, we create a model that maps the input image to the activations\n-    # of the last conv layer as well as the output predictions\n-    grad_model = keras.models.Model(\n-        model.inputs,\n-        [model.get_layer(last_conv_layer_name).output, model.output],\n-    )\n-\n-    # Then, we compute the gradient of the top predicted class for our input image\n-    # with respect to the activations of the last conv layer\n-    with tf.GradientTape() as tape:\n-        last_conv_layer_output, preds = grad_model(img_array)\n-        if pred_index is None:\n-            pred_index = tf.argmax(preds[0])\n-        class_channel = preds[:, pred_index]\n-\n-    # This is the gradient of the output neuron (top predicted or chosen)\n-    # with regard to the output feature map of the last conv layer\n-    grads = tape.gradient(class_channel, last_conv_layer_output)\n-\n-    # This is a vector where each entry is the mean intensity of the gradient\n-    # over a specific feature map channel\n-    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n-\n-    # We multiply each channel in the feature map array\n-    # by \"how important this channel is\" with regard to the top predicted class\n-    # then sum all the channels to obtain the heatmap class activation\n-    last_conv_layer_output = last_conv_layer_output[0]\n-    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n-    heatmap = tf.squeeze(heatmap)\n-\n-    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n-    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n-    return heatmap.numpy()\n-\n-\n-\"\"\"\n-## Let's test-drive it\n-\n-\"\"\"\n-\n-# Prepare image\n-img_array = preprocess_input(get_img_array(img_path, size=img_size))\n-\n-# Make model\n-model = model_builder(weights=\"imagenet\")\n-\n-# Remove last layer's softmax\n-model.layers[-1].activation = None\n-\n-# Print what the top predicted class is\n-preds = model.predict(img_array)\n-print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n-\n-# Generate class activation heatmap\n-heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n-\n-# Display heatmap\n-plt.matshow(heatmap)\n-plt.show()\n-\n-\n-\"\"\"\n-## Create a superimposed visualization\n-\n-\"\"\"\n-\n-\n-def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n-    # Load the original image\n-    img = keras.utils.load_img(img_path)\n-    img = keras.utils.img_to_array(img)\n-\n-    # Rescale heatmap to a range 0-255\n-    heatmap = np.uint8(255 * heatmap)\n-\n-    # Use jet colormap to colorize heatmap\n-    jet = cm.get_cmap(\"jet\")\n-\n-    # Use RGB values of the colormap\n-    jet_colors = jet(np.arange(256))[:, :3]\n-    jet_heatmap = jet_colors[heatmap]\n-\n-    # Create an image with RGB colorized heatmap\n-    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n-    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n-    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n-\n-    # Superimpose the heatmap on original image\n-    superimposed_img = jet_heatmap * alpha + img\n-    superimposed_img = keras.utils.array_to_img(superimposed_img)\n-\n-    # Save the superimposed image\n-    superimposed_img.save(cam_path)\n-\n-    # Display Grad CAM\n-    display(Image(cam_path))\n-\n-\n-save_and_display_gradcam(img_path, heatmap)\n-\n-\"\"\"\n-## Let's try another image\n-\n-We will see how the grad cam explains the model's outputs for a multi-label image. Let's\n-try an image with a cat and a dog together, and see how the grad cam behaves.\n-\"\"\"\n-\n-img_path = keras.utils.get_file(\n-    fname=\"cat_and_dog.jpg\",\n-    origin=\"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\",\n-)\n-\n-display(Image(img_path))\n-\n-# Prepare image\n-img_array = preprocess_input(get_img_array(img_path, size=img_size))\n-\n-# Print what the two top predicted classes are\n-preds = model.predict(img_array)\n-print(\"Predicted:\", decode_predictions(preds, top=2)[0])\n-\n-\"\"\"\n-We generate class activation heatmap for \"chow,\" the class index is 260\n-\n-\"\"\"\n-\n-heatmap = make_gradcam_heatmap(\n-    img_array, model, last_conv_layer_name, pred_index=260\n-)\n-\n-save_and_display_gradcam(img_path, heatmap)\n-\n-\"\"\"\n-We generate class activation heatmap for \"egyptian cat,\" the class index is 285\n-\n-\"\"\"\n-\n-heatmap = make_gradcam_heatmap(\n-    img_array, model, last_conv_layer_name, pred_index=285\n-)\n-\n-save_and_display_gradcam(img_path, heatmap)\n\n@@ -1,672 +0,0 @@\n-\"\"\"\n-Title: Image Captioning\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2021/05/29\n-Last modified: 2021/10/31\n-Description: Implement an image captioning model using a CNN and a Transformer.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import re\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-from keras.applications import efficientnet\n-from keras.layers import TextVectorization\n-\n-keras.utils.set_random_seed(111)\n-\n-\"\"\"\n-## Download the dataset\n-\n-We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n-8,000 images, that are each paired with five different captions.\n-\"\"\"\n-\n-\n-\"\"\"shell\n-wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n-wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n-unzip -qq Flickr8k_Dataset.zip\n-unzip -qq Flickr8k_text.zip\n-rm Flickr8k_Dataset.zip Flickr8k_text.zip\n-\"\"\"\n-\n-\n-# Path to the images\n-IMAGES_PATH = \"Flicker8k_Dataset\"\n-\n-# Desired image dimensions\n-IMAGE_SIZE = (299, 299)\n-\n-# Vocabulary size\n-VOCAB_SIZE = 10000\n-\n-# Fixed length allowed for any sequence\n-SEQ_LENGTH = 25\n-\n-# Dimension for the image embeddings and token embeddings\n-EMBED_DIM = 512\n-\n-# Per-layer units in the feed-forward network\n-FF_DIM = 512\n-\n-# Other training parameters\n-BATCH_SIZE = 64\n-EPOCHS = 30\n-AUTOTUNE = tf.data.AUTOTUNE\n-\n-\"\"\"\n-## Preparing the dataset\n-\"\"\"\n-\n-\n-def load_captions_data(filename):\n-    \"\"\"Loads captions (text) data and maps them to corresponding images.\n-\n-    Args:\n-        filename: Path to the text file containing caption data.\n-\n-    Returns:\n-        caption_mapping: Dictionary mapping image names and the corresponding captions\n-        text_data: List containing all the available captions\n-    \"\"\"\n-\n-    with open(filename) as caption_file:\n-        caption_data = caption_file.readlines()\n-        caption_mapping = {}\n-        text_data = []\n-        images_to_skip = set()\n-\n-        for line in caption_data:\n-            line = line.rstrip(\"\\n\")\n-            # Image name and captions are separated using a tab\n-            img_name, caption = line.split(\"\\t\")\n-\n-            # Each image is repeated five times for the five different captions.\n-            # Each image name has a suffix `#(caption_number)`\n-            img_name = img_name.split(\"#\")[0]\n-            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n-\n-            # We will remove caption that are either too short to too long\n-            tokens = caption.strip().split()\n-\n-            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n-                images_to_skip.add(img_name)\n-                continue\n-\n-            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n-                # We will add a start and an end token to each caption\n-                caption = \"<start> \" + caption.strip() + \" <end>\"\n-                text_data.append(caption)\n-\n-                if img_name in caption_mapping:\n-                    caption_mapping[img_name].append(caption)\n-                else:\n-                    caption_mapping[img_name] = [caption]\n-\n-        for img_name in images_to_skip:\n-            if img_name in caption_mapping:\n-                del caption_mapping[img_name]\n-\n-        return caption_mapping, text_data\n-\n-\n-def train_val_split(caption_data, train_size=0.8, shuffle=True):\n-    \"\"\"Split the captioning dataset into train and validation sets.\n-\n-    Args:\n-        caption_data (dict): Dictionary containing the mapped caption data\n-        train_size (float): Fraction of all the full dataset to use as training data\n-        shuffle (bool): Whether to shuffle the dataset before splitting\n-\n-    Returns:\n-        Traning and validation datasets as two separated dicts\n-    \"\"\"\n-\n-    # 1. Get the list of all image names\n-    all_images = list(caption_data.keys())\n-\n-    # 2. Shuffle if necessary\n-    if shuffle:\n-        np.random.shuffle(all_images)\n-\n-    # 3. Split into training and validation sets\n-    train_size = int(len(caption_data) * train_size)\n-\n-    training_data = {\n-        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n-    }\n-    validation_data = {\n-        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n-    }\n-\n-    # 4. Return the splits\n-    return training_data, validation_data\n-\n-\n-# Load the dataset\n-captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n-\n-# Split the dataset into training and validation sets\n-train_data, valid_data = train_val_split(captions_mapping)\n-print(\"Number of training samples: \", len(train_data))\n-print(\"Number of validation samples: \", len(valid_data))\n-\n-\"\"\"\n-## Vectorizing the text data\n-\n-We'll use the `TextVectorization` layer to vectorize the text data,\n-that is to say, to turn the\n-original strings into integer sequences where each integer represents the index of\n-a word in a vocabulary. We will use a custom string standardization scheme\n-(strip punctuation characters except `<` and `>`) and the default\n-splitting scheme (split on whitespace).\n-\"\"\"\n-\n-\n-def custom_standardization(input_string):\n-    lowercase = tf.strings.lower(input_string)\n-    return tf.strings.regex_replace(\n-        lowercase, \"[%s]\" % re.escape(strip_chars), \"\"\n-    )\n-\n-\n-strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n-strip_chars = strip_chars.replace(\"<\", \"\")\n-strip_chars = strip_chars.replace(\">\", \"\")\n-\n-vectorization = TextVectorization(\n-    max_tokens=VOCAB_SIZE,\n-    output_mode=\"int\",\n-    output_sequence_length=SEQ_LENGTH,\n-    standardize=custom_standardization,\n-)\n-vectorization.adapt(text_data)\n-\n-# Data augmentation for image data\n-image_augmentation = keras.Sequential(\n-    [\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(0.2),\n-        layers.RandomContrast(0.3),\n-    ]\n-)\n-\n-\n-\"\"\"\n-## Building a `tf.data.Dataset` pipeline for training\n-\n-We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n-The pipeline consists of two steps:\n-\n-1. Read the image from the disk\n-2. Tokenize all the five captions corresponding to the image\n-\"\"\"\n-\n-\n-def decode_and_resize(img_path):\n-    img = tf.io.read_file(img_path)\n-    img = tf.image.decode_jpeg(img, channels=3)\n-    img = tf.image.resize(img, IMAGE_SIZE)\n-    img = tf.image.convert_image_dtype(img, tf.float32)\n-    return img\n-\n-\n-def process_input(img_path, captions):\n-    return decode_and_resize(img_path), vectorization(captions)\n-\n-\n-def make_dataset(images, captions):\n-    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n-    dataset = dataset.shuffle(BATCH_SIZE * 8)\n-    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n-\n-    return dataset\n-\n-\n-# Pass the list of images and the list of corresponding captions\n-train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n-\n-valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n-\n-\n-\"\"\"\n-## Building the model\n-\n-Our image captioning architecture consists of three models:\n-\n-1. A CNN: used to extract the image features\n-2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n-                    based encoder that generates a new representation of the inputs\n-3. A TransformerDecoder: This model takes the encoder output and the text data\n-                    (sequences) as inputs and tries to learn to generate the caption.\n-\"\"\"\n-\n-\n-def get_cnn_model():\n-    base_model = efficientnet.EfficientNetB0(\n-        input_shape=(*IMAGE_SIZE, 3),\n-        include_top=False,\n-        weights=\"imagenet\",\n-    )\n-    # We freeze our feature extractor\n-    base_model.trainable = False\n-    base_model_out = base_model.output\n-    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(\n-        base_model_out\n-    )\n-    cnn_model = keras.models.Model(base_model.input, base_model_out)\n-    return cnn_model\n-\n-\n-class TransformerEncoderBlock(layers.Layer):\n-    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n-        super().__init__(**kwargs)\n-        self.embed_dim = embed_dim\n-        self.dense_dim = dense_dim\n-        self.num_heads = num_heads\n-        self.attention_1 = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n-        )\n-        self.layernorm_1 = layers.LayerNormalization()\n-        self.layernorm_2 = layers.LayerNormalization()\n-        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n-\n-    def call(self, inputs, training, mask=None):\n-        inputs = self.layernorm_1(inputs)\n-        inputs = self.dense_1(inputs)\n-\n-        attention_output_1 = self.attention_1(\n-            query=inputs,\n-            value=inputs,\n-            key=inputs,\n-            attention_mask=None,\n-            training=training,\n-        )\n-        out_1 = self.layernorm_2(inputs + attention_output_1)\n-        return out_1\n-\n-\n-class PositionalEmbedding(layers.Layer):\n-    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.token_embeddings = layers.Embedding(\n-            input_dim=vocab_size, output_dim=embed_dim\n-        )\n-        self.position_embeddings = layers.Embedding(\n-            input_dim=sequence_length, output_dim=embed_dim\n-        )\n-        self.sequence_length = sequence_length\n-        self.vocab_size = vocab_size\n-        self.embed_dim = embed_dim\n-        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n-\n-    def call(self, inputs):\n-        length = tf.shape(inputs)[-1]\n-        positions = tf.range(start=0, limit=length, delta=1)\n-        embedded_tokens = self.token_embeddings(inputs)\n-        embedded_tokens = embedded_tokens * self.embed_scale\n-        embedded_positions = self.position_embeddings(positions)\n-        return embedded_tokens + embedded_positions\n-\n-    def compute_mask(self, inputs, mask=None):\n-        return tf.math.not_equal(inputs, 0)\n-\n-\n-class TransformerDecoderBlock(layers.Layer):\n-    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n-        super().__init__(**kwargs)\n-        self.embed_dim = embed_dim\n-        self.ff_dim = ff_dim\n-        self.num_heads = num_heads\n-        self.attention_1 = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n-        )\n-        self.attention_2 = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n-        )\n-        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n-        self.ffn_layer_2 = layers.Dense(embed_dim)\n-\n-        self.layernorm_1 = layers.LayerNormalization()\n-        self.layernorm_2 = layers.LayerNormalization()\n-        self.layernorm_3 = layers.LayerNormalization()\n-\n-        self.embedding = PositionalEmbedding(\n-            embed_dim=EMBED_DIM,\n-            sequence_length=SEQ_LENGTH,\n-            vocab_size=VOCAB_SIZE,\n-        )\n-        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n-\n-        self.dropout_1 = layers.Dropout(0.3)\n-        self.dropout_2 = layers.Dropout(0.5)\n-        self.supports_masking = True\n-\n-    def call(self, inputs, encoder_outputs, training, mask=None):\n-        inputs = self.embedding(inputs)\n-        causal_mask = self.get_causal_attention_mask(inputs)\n-\n-        if mask is not None:\n-            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n-            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n-            combined_mask = tf.minimum(combined_mask, causal_mask)\n-\n-        attention_output_1 = self.attention_1(\n-            query=inputs,\n-            value=inputs,\n-            key=inputs,\n-            attention_mask=combined_mask,\n-            training=training,\n-        )\n-        out_1 = self.layernorm_1(inputs + attention_output_1)\n-\n-        attention_output_2 = self.attention_2(\n-            query=out_1,\n-            value=encoder_outputs,\n-            key=encoder_outputs,\n-            attention_mask=padding_mask,\n-            training=training,\n-        )\n-        out_2 = self.layernorm_2(out_1 + attention_output_2)\n-\n-        ffn_out = self.ffn_layer_1(out_2)\n-        ffn_out = self.dropout_1(ffn_out, training=training)\n-        ffn_out = self.ffn_layer_2(ffn_out)\n-\n-        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n-        ffn_out = self.dropout_2(ffn_out, training=training)\n-        preds = self.out(ffn_out)\n-        return preds\n-\n-    def get_causal_attention_mask(self, inputs):\n-        input_shape = tf.shape(inputs)\n-        batch_size, sequence_length = input_shape[0], input_shape[1]\n-        i = tf.range(sequence_length)[:, tf.newaxis]\n-        j = tf.range(sequence_length)\n-        mask = tf.cast(i >= j, dtype=\"int32\")\n-        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n-        mult = tf.concat(\n-            [\n-                tf.expand_dims(batch_size, -1),\n-                tf.constant([1, 1], dtype=tf.int32),\n-            ],\n-            axis=0,\n-        )\n-        return tf.tile(mask, mult)\n-\n-\n-class ImageCaptioningModel(keras.Model):\n-    def __init__(\n-        self,\n-        cnn_model,\n-        encoder,\n-        decoder,\n-        num_captions_per_image=5,\n-        image_aug=None,\n-    ):\n-        super().__init__()\n-        self.cnn_model = cnn_model\n-        self.encoder = encoder\n-        self.decoder = decoder\n-        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n-        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n-        self.num_captions_per_image = num_captions_per_image\n-        self.image_aug = image_aug\n-\n-    def calculate_loss(self, y_true, y_pred, mask):\n-        loss = self.loss(y_true, y_pred)\n-        mask = tf.cast(mask, dtype=loss.dtype)\n-        loss *= mask\n-        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n-\n-    def calculate_accuracy(self, y_true, y_pred, mask):\n-        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n-        accuracy = tf.math.logical_and(mask, accuracy)\n-        accuracy = tf.cast(accuracy, dtype=tf.float32)\n-        mask = tf.cast(mask, dtype=tf.float32)\n-        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n-\n-    def _compute_caption_loss_and_acc(\n-        self, img_embed, batch_seq, training=True\n-    ):\n-        encoder_out = self.encoder(img_embed, training=training)\n-        batch_seq_inp = batch_seq[:, :-1]\n-        batch_seq_true = batch_seq[:, 1:]\n-        mask = tf.math.not_equal(batch_seq_true, 0)\n-        batch_seq_pred = self.decoder(\n-            batch_seq_inp, encoder_out, training=training, mask=mask\n-        )\n-        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n-        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n-        return loss, acc\n-\n-    def train_step(self, batch_data):\n-        batch_img, batch_seq = batch_data\n-        batch_loss = 0\n-        batch_acc = 0\n-\n-        if self.image_aug:\n-            batch_img = self.image_aug(batch_img)\n-\n-        # 1. Get image embeddings\n-        img_embed = self.cnn_model(batch_img)\n-\n-        # 2. Pass each of the five captions one by one to the decoder\n-        # along with the encoder outputs and compute the loss as well as accuracy\n-        # for each caption.\n-        for i in range(self.num_captions_per_image):\n-            with tf.GradientTape() as tape:\n-                loss, acc = self._compute_caption_loss_and_acc(\n-                    img_embed, batch_seq[:, i, :], training=True\n-                )\n-\n-                # 3. Update loss and accuracy\n-                batch_loss += loss\n-                batch_acc += acc\n-\n-            # 4. Get the list of all the trainable weights\n-            train_vars = (\n-                self.encoder.trainable_variables\n-                + self.decoder.trainable_variables\n-            )\n-\n-            # 5. Get the gradients\n-            grads = tape.gradient(loss, train_vars)\n-\n-            # 6. Update the trainable weights\n-            self.optimizer.apply_gradients(zip(grads, train_vars))\n-\n-        # 7. Update the trackers\n-        batch_acc /= float(self.num_captions_per_image)\n-        self.loss_tracker.update_state(batch_loss)\n-        self.acc_tracker.update_state(batch_acc)\n-\n-        # 8. Return the loss and accuracy values\n-        return {\n-            \"loss\": self.loss_tracker.result(),\n-            \"acc\": self.acc_tracker.result(),\n-        }\n-\n-    def test_step(self, batch_data):\n-        batch_img, batch_seq = batch_data\n-        batch_loss = 0\n-        batch_acc = 0\n-\n-        # 1. Get image embeddings\n-        img_embed = self.cnn_model(batch_img)\n-\n-        # 2. Pass each of the five captions one by one to the decoder\n-        # along with the encoder outputs and compute the loss as well as accuracy\n-        # for each caption.\n-        for i in range(self.num_captions_per_image):\n-            loss, acc = self._compute_caption_loss_and_acc(\n-                img_embed, batch_seq[:, i, :], training=False\n-            )\n-\n-            # 3. Update batch loss and batch accuracy\n-            batch_loss += loss\n-            batch_acc += acc\n-\n-        batch_acc /= float(self.num_captions_per_image)\n-\n-        # 4. Update the trackers\n-        self.loss_tracker.update_state(batch_loss)\n-        self.acc_tracker.update_state(batch_acc)\n-\n-        # 5. Return the loss and accuracy values\n-        return {\n-            \"loss\": self.loss_tracker.result(),\n-            \"acc\": self.acc_tracker.result(),\n-        }\n-\n-    @property\n-    def metrics(self):\n-        # We need to list our metrics here so the `reset_states()` can be\n-        # called automatically.\n-        return [self.loss_tracker, self.acc_tracker]\n-\n-\n-cnn_model = get_cnn_model()\n-encoder = TransformerEncoderBlock(\n-    embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1\n-)\n-decoder = TransformerDecoderBlock(\n-    embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2\n-)\n-caption_model = ImageCaptioningModel(\n-    cnn_model=cnn_model,\n-    encoder=encoder,\n-    decoder=decoder,\n-    image_aug=image_augmentation,\n-)\n-\n-\"\"\"\n-## Model training\n-\"\"\"\n-\n-\n-# Define the loss function\n-cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n-    from_logits=False,\n-    reduction=None,\n-)\n-\n-# EarlyStopping criteria\n-early_stopping = keras.callbacks.EarlyStopping(\n-    patience=3, restore_best_weights=True\n-)\n-\n-\n-# Learning Rate Scheduler for the optimizer\n-class LRSchedule(\n-    keras.optimizers.schedules.LearningRateSchedule\n-):\n-    def __init__(self, post_warmup_learning_rate, warmup_steps):\n-        super().__init__()\n-        self.post_warmup_learning_rate = post_warmup_learning_rate\n-        self.warmup_steps = warmup_steps\n-\n-    def __call__(self, step):\n-        global_step = tf.cast(step, tf.float32)\n-        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n-        warmup_progress = global_step / warmup_steps\n-        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n-        return tf.cond(\n-            global_step < warmup_steps,\n-            lambda: warmup_learning_rate,\n-            lambda: self.post_warmup_learning_rate,\n-        )\n-\n-\n-# Create a learning rate schedule\n-num_train_steps = len(train_dataset) * EPOCHS\n-num_warmup_steps = num_train_steps // 15\n-lr_schedule = LRSchedule(\n-    post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps\n-)\n-\n-# Compile the model\n-caption_model.compile(\n-    optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy\n-)\n-\n-# Fit the model\n-caption_model.fit(\n-    train_dataset,\n-    epochs=EPOCHS,\n-    validation_data=valid_dataset,\n-    callbacks=[early_stopping],\n-)\n-\n-\"\"\"\n-## Check sample predictions\n-\"\"\"\n-\n-vocab = vectorization.get_vocabulary()\n-index_lookup = dict(zip(range(len(vocab)), vocab))\n-max_decoded_sentence_length = SEQ_LENGTH - 1\n-valid_images = list(valid_data.keys())\n-\n-\n-def generate_caption():\n-    # Select a random image from the validation dataset\n-    sample_img = np.random.choice(valid_images)\n-\n-    # Read the image from the disk\n-    sample_img = decode_and_resize(sample_img)\n-    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n-    plt.imshow(img)\n-    plt.show()\n-\n-    # Pass the image to the CNN\n-    img = tf.expand_dims(sample_img, 0)\n-    img = caption_model.cnn_model(img)\n-\n-    # Pass the image features to the Transformer encoder\n-    encoded_img = caption_model.encoder(img, training=False)\n-\n-    # Generate the caption using the Transformer decoder\n-    decoded_caption = \"<start> \"\n-    for i in range(max_decoded_sentence_length):\n-        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n-        mask = tf.math.not_equal(tokenized_caption, 0)\n-        predictions = caption_model.decoder(\n-            tokenized_caption, encoded_img, training=False, mask=mask\n-        )\n-        sampled_token_index = np.argmax(predictions[0, i, :])\n-        sampled_token = index_lookup[sampled_token_index]\n-        if sampled_token == \"<end>\":\n-            break\n-        decoded_caption += \" \" + sampled_token\n-\n-    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n-    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n-    print(\"Predicted Caption: \", decoded_caption)\n-\n-\n-# Check predictions for a few samples\n-generate_caption()\n-generate_caption()\n-generate_caption()\n-\n-\"\"\"\n-## End Notes\n-\n-We saw that the model starts to generate reasonable captions after a few epochs. To keep\n-this example easily runnable, we have trained it with a few constraints, like a minimal\n-number of attention heads. To improve the predictions, you can try changing these training\n-settings and find a good model for your use case.\n-\"\"\"\n\n@@ -1,318 +0,0 @@\n-\"\"\"\n-Title: Image classification from scratch\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/04/27\n-Last modified: 2022/11/10\n-Description: Training an image classifier from scratch on the Kaggle Cats vs Dogs dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to do image classification from scratch, starting from JPEG\n-image files on disk, without leveraging pre-trained weights or a pre-made Keras\n-Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary\n- classification dataset.\n-\n-We use the `image_dataset_from_directory` utility to generate the datasets, and\n-we use Keras image preprocessing layers for image standardization and data augmentation.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-import os\n-from pathlib import Path\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Load the data: the Cats vs Dogs dataset\n-\n-### Raw data download\n-\n-First, let's download the 786M ZIP archive of the raw data:\n-\"\"\"\n-\n-fpath = keras.utils.get_file(\n-    origin=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n-)\n-dirpath = Path(fpath).parent.absolute()\n-os.system(f\"unzip -q {fpath} -d {dirpath}\")\n-\n-\"\"\"\n-Now we have a `PetImages` folder which contain two subfolders, `Cat` and `Dog`. Each\n-subfolder contains image files for each category.\n-\"\"\"\n-\n-os.system(f\"ls {dirpath}/PetImages/\")\n-\n-\"\"\"\n-### Filter out corrupted images\n-\n-When working with lots of real-world image data, corrupted images are a common\n-occurence. Let's filter out badly-encoded images that do not feature the string \"JFIF\"\n-in their header.\n-\"\"\"\n-\n-import os\n-\n-num_skipped = 0\n-for folder_name in (\"Cat\", \"Dog\"):\n-    folder_path = os.path.join(dirpath, \"PetImages\", folder_name)\n-    for fname in os.listdir(folder_path):\n-        fpath = os.path.join(folder_path, fname)\n-        try:\n-            fobj = open(fpath, \"rb\")\n-            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n-        finally:\n-            fobj.close()\n-\n-        if not is_jfif:\n-            num_skipped += 1\n-            # Delete corrupted image\n-            os.remove(fpath)\n-\n-print(f\"Deleted {num_skipped} images\")\n-\n-\"\"\"\n-## Generate a `Dataset`\n-\"\"\"\n-\n-image_size = (180, 180)\n-batch_size = 128\n-\n-train_ds, val_ds = keras.utils.image_dataset_from_directory(\n-    os.path.join(dirpath, \"PetImages\"),\n-    validation_split=0.2,\n-    subset=\"both\",\n-    seed=1337,\n-    image_size=image_size,\n-    batch_size=batch_size,\n-)\n-\n-\"\"\"\n-## Visualize the data\n-\n-Here are the first 9 images in the training dataset. As you can see, label 1 is \"dog\"\n-and label 0 is \"cat\".\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-for images, labels in train_ds.take(1):\n-    for i in range(9):\n-        ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(images[i].numpy().astype(\"uint8\"))\n-        plt.title(int(labels[i]))\n-        plt.axis(\"off\")\n-\n-\"\"\"\n-## Using image data augmentation\n-\n-When you don't have a large image dataset, it's a good practice to artificially\n-introduce sample diversity by applying random yet realistic transformations to the\n-training images, such as random horizontal flipping or small random rotations. This\n-helps expose the model to different aspects of the training data while slowing down\n-overfitting.\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(0.1),\n-    ]\n-)\n-\n-\"\"\"\n-Let's visualize what the augmented samples look like, by applying `data_augmentation`\n-repeatedly to the first image in the dataset:\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-for images, _ in train_ds.take(1):\n-    for i in range(9):\n-        augmented_images = data_augmentation(images)\n-        ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n-        plt.axis(\"off\")\n-\n-\"\"\"\n-## Standardizing the data\n-\n-Our image are already in a standard size (180x180), as they are being yielded as\n-contiguous `float32` batches by our dataset. However, their RGB channel values are in\n-the `[0, 255]` range. This is not ideal for a neural network;\n-in general you should seek to make your input values small. Here, we will\n-standardize values to be in the `[0, 1]` by using a `Rescaling` layer at the start of\n-our model.\n-\"\"\"\n-\n-\"\"\"\n-## Two options to preprocess the data\n-\n-There are two ways you could be using the `data_augmentation` preprocessor:\n-\n-**Option 1: Make it part of the model**, like this:\n-\n-```python\n-inputs = keras.Input(shape=input_shape)\n-x = data_augmentation(inputs)\n-x = layers.Rescaling(1./255)(x)\n-...  # Rest of the model\n-```\n-\n-With this option, your data augmentation will happen *on device*, synchronously\n-with the rest of the model execution, meaning that it will benefit from GPU\n-acceleration.\n-\n-Note that data augmentation is inactive at test time, so the input samples will only be\n-augmented during `fit()`, not when calling `evaluate()` or `predict()`.\n-\n-If you're training on GPU, this may be a good option.\n-\n-**Option 2: apply it to the dataset**, so as to obtain a dataset that yields batches of\n-augmented images, like this:\n-\n-```python\n-augmented_train_ds = train_ds.map(\n-    lambda x, y: (data_augmentation(x, training=True), y))\n-```\n-\n-With this option, your data augmentation will happen **on CPU**, asynchronously, and will\n-be buffered before going into the model.\n-\n-If you're training on CPU, this is the better option, since it makes data augmentation\n-asynchronous and non-blocking.\n-\n-In our case, we'll go with the second option. If you're not sure\n-which one to pick, this second option (asynchronous preprocessing) is always a solid choice.\n-\"\"\"\n-\n-\"\"\"\n-## Configure the dataset for performance\n-\n-Let's apply data augmentation to our training dataset,\n-and let's make sure to use buffered prefetching so we can yield data from disk without\n-having I/O becoming blocking:\n-\"\"\"\n-\n-print(\"Make datasets\")\n-# Apply `data_augmentation` to the training images.\n-train_ds = train_ds.map(\n-    lambda img, label: (data_augmentation(img), label),\n-    num_parallel_calls=tf.data.AUTOTUNE,\n-)\n-# Prefetching samples in GPU memory helps maximize GPU utilization.\n-train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n-val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n-\n-\"\"\"\n-## Build a model\n-\n-We'll build a small version of the Xception network. We haven't particularly tried to\n-optimize the architecture; if you want to do a systematic search for the best model\n-configuration, consider using\n-[KerasTuner](https://github.com/keras-team/keras-tuner).\n-\n-Note that:\n-\n-- We start the model with the `data_augmentation` preprocessor, followed by a\n- `Rescaling` layer.\n-- We include a `Dropout` layer before the final classification layer.\n-\"\"\"\n-\n-\n-def make_model(input_shape, num_classes):\n-    inputs = keras.Input(shape=input_shape)\n-\n-    # Entry block\n-    x = layers.Rescaling(1.0 / 255)(inputs)\n-    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    previous_block_activation = x  # Set aside residual\n-\n-    for size in [256, 512, 728]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n-\n-        # Project residual\n-        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n-            previous_block_activation\n-        )\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    x = layers.GlobalAveragePooling2D()(x)\n-    if num_classes == 2:\n-        activation = \"sigmoid\"\n-        units = 1\n-    else:\n-        activation = \"softmax\"\n-        units = num_classes\n-\n-    x = layers.Dropout(0.5)(x)\n-    outputs = layers.Dense(units, activation=activation)(x)\n-    return keras.Model(inputs, outputs)\n-\n-\n-model = make_model(input_shape=image_size + (3,), num_classes=2)\n-keras.utils.plot_model(model, show_shapes=True)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 25\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n-]\n-\n-model.compile(\n-    optimizer=keras.optimizers.Adam(1e-3),\n-    loss=\"binary_crossentropy\",\n-    metrics=[\"accuracy\"],\n-)\n-model.fit(\n-    train_ds,\n-    epochs=epochs,\n-    callbacks=callbacks,\n-    validation_data=val_ds,\n-)\n-\n-\"\"\"\n-We get to >90% validation accuracy after training for 25 epochs on the full dataset\n-(in practice, you can train for 50+ epochs before validation performance starts degrading).\n-\"\"\"\n-\n-\"\"\"\n-## Run inference on new data\n-\n-Note that data augmentation and dropout are inactive at inference time.\n-\"\"\"\n-\n-img = keras.utils.load_img(\n-    f\"{dirpath}/PetImages/Cat/6779.jpg\", target_size=image_size\n-)\n-img_array = keras.utils.img_to_array(img)\n-img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n-\n-predictions = model.predict(img_array)\n-score = float(predictions[0])\n-print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")\n\n@@ -1,506 +0,0 @@\n-\"\"\"\n-Title: Model interpretability with Integrated Gradients\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2020/06/02\n-Last modified: 2020/06/02\n-Description: How to obtain integrated gradients for a classification model.\n-Accelerator: NONE\n-\"\"\"\n-\n-\"\"\"\n-## Integrated Gradients\n-\n-[Integrated Gradients](https://arxiv.org/abs/1703.01365) is a technique for\n-attributing a classification model's prediction to its input features. It is\n-a model interpretability technique: you can use it to visualize the relationship\n-between input features and model predictions.\n-\n-Integrated Gradients is a variation on computing\n-the gradient of the prediction output with regard to features of the input.\n-To compute integrated gradients, we need to perform the following steps:\n-\n-1. Identify the input and the output. In our case, the input is an image and the\n-output is the last layer of our model (dense layer with softmax activation).\n-\n-2. Compute which features are important to a neural network\n-when making a prediction on a particular data point. To identify these features, we\n-need to choose a baseline input. A baseline input can be a black image (all pixel\n-values set to zero) or random noise. The shape of the baseline input needs to be\n-the same as our input image, e.g. (299, 299, 3).\n-\n-3. Interpolate the baseline for a given number of steps. The number of steps represents\n-the steps we need in the gradient approximation for a given input image. The number of\n-steps is a hyperparameter. The authors recommend using anywhere between\n-20 and 1000 steps.\n-\n-4. Preprocess these interpolated images and do a forward pass.\n-5. Get the gradients for these interpolated images.\n-6. Approximate the gradients integral using the trapezoidal rule.\n-\n-To read in-depth about integrated gradients and why this method works,\n-consider reading this excellent\n-[article](https://distill.pub/2020/attribution-baselines/).\n-\n-**References:**\n-\n-- Integrated Gradients original [paper](https://arxiv.org/abs/1703.01365)\n-- [Original implementation](https://github.com/ankurtaly/Integrated-Gradients)\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-from scipy import ndimage\n-from IPython.display import Image, display\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-from keras.applications import xception\n-\n-keras.config.disable_traceback_filtering()\n-\n-\n-# Size of the input image\n-img_size = (299, 299, 3)\n-\n-# Load Xception model with imagenet weights\n-model = xception.Xception(weights=\"imagenet\")\n-\n-# The local path to our target image\n-img_path = keras.utils.get_file(\n-    \"elephant.jpg\", \"https://i.imgur.com/Bvro0YD.png\"\n-)\n-display(Image(img_path))\n-\n-\"\"\"\n-## Integrated Gradients algorithm\n-\"\"\"\n-\n-\n-def get_img_array(img_path, size=(299, 299)):\n-    # `img` is a PIL image of size 299x299\n-    img = keras.utils.load_img(img_path, target_size=size)\n-    # `array` is a float32 Numpy array of shape (299, 299, 3)\n-    array = keras.utils.img_to_array(img)\n-    # We add a dimension to transform our array into a \"batch\"\n-    # of size (1, 299, 299, 3)\n-    array = np.expand_dims(array, axis=0)\n-    return array\n-\n-\n-def get_gradients(img_input, top_pred_idx):\n-    \"\"\"Computes the gradients of outputs w.r.t input image.\n-\n-    Args:\n-        img_input: 4D image tensor\n-        top_pred_idx: Predicted label for the input image\n-\n-    Returns:\n-        Gradients of the predictions w.r.t img_input\n-    \"\"\"\n-    images = tf.cast(img_input, tf.float32)\n-\n-    with tf.GradientTape() as tape:\n-        tape.watch(images)\n-        preds = model(images)\n-        top_class = preds[:, top_pred_idx]\n-\n-    grads = tape.gradient(top_class, images)\n-    return grads\n-\n-\n-def get_integrated_gradients(\n-    img_input, top_pred_idx, baseline=None, num_steps=50\n-):\n-    \"\"\"Computes Integrated Gradients for a predicted label.\n-\n-    Args:\n-        img_input (ndarray): Original image\n-        top_pred_idx: Predicted label for the input image\n-        baseline (ndarray): The baseline image to start with for interpolation\n-        num_steps: Number of interpolation steps between the baseline\n-            and the input used in the computation of integrated gradients. These\n-            steps along determine the integral approximation error. By default,\n-            num_steps is set to 50.\n-\n-    Returns:\n-        Integrated gradients w.r.t input image\n-    \"\"\"\n-    # If baseline is not provided, start with a black image\n-    # having same size as the input image.\n-    if baseline is None:\n-        baseline = np.zeros(img_size).astype(np.float32)\n-    else:\n-        baseline = baseline.astype(np.float32)\n-\n-    # 1. Do interpolation.\n-    img_input = img_input.astype(np.float32)\n-    interpolated_image = [\n-        baseline + (step / num_steps) * (img_input - baseline)\n-        for step in range(num_steps + 1)\n-    ]\n-    interpolated_image = np.array(interpolated_image).astype(np.float32)\n-\n-    # 2. Preprocess the interpolated images\n-    interpolated_image = xception.preprocess_input(interpolated_image)\n-\n-    # 3. Get the gradients\n-    grads = []\n-    for i, img in enumerate(interpolated_image):\n-        img = tf.expand_dims(img, axis=0)\n-        grad = get_gradients(img, top_pred_idx=top_pred_idx)\n-        grads.append(grad[0])\n-    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n-\n-    # 4. Approximate the integral using the trapezoidal rule\n-    grads = (grads[:-1] + grads[1:]) / 2.0\n-    avg_grads = tf.reduce_mean(grads, axis=0)\n-\n-    # 5. Calculate integrated gradients and return\n-    integrated_grads = (img_input - baseline) * avg_grads\n-    return integrated_grads\n-\n-\n-def random_baseline_integrated_gradients(\n-    img_input, top_pred_idx, num_steps=50, num_runs=2\n-):\n-    \"\"\"Generates a number of random baseline images.\n-\n-    Args:\n-        img_input (ndarray): 3D image\n-        top_pred_idx: Predicted label for the input image\n-        num_steps: Number of interpolation steps between the baseline\n-            and the input used in the computation of integrated gradients. These\n-            steps along determine the integral approximation error. By default,\n-            num_steps is set to 50.\n-        num_runs: number of baseline images to generate\n-\n-    Returns:\n-        Averaged integrated gradients for `num_runs` baseline images\n-    \"\"\"\n-    # 1. List to keep track of Integrated Gradients (IG) for all the images\n-    integrated_grads = []\n-\n-    # 2. Get the integrated gradients for all the baselines\n-    for run in range(num_runs):\n-        baseline = np.random.random(img_size) * 255\n-        igrads = get_integrated_gradients(\n-            img_input=img_input,\n-            top_pred_idx=top_pred_idx,\n-            baseline=baseline,\n-            num_steps=num_steps,\n-        )\n-        integrated_grads.append(igrads)\n-\n-    # 3. Return the average integrated gradients for the image\n-    integrated_grads = tf.convert_to_tensor(integrated_grads)\n-    return tf.reduce_mean(integrated_grads, axis=0)\n-\n-\n-\"\"\"\n-## Helper class for visualizing gradients and integrated gradients\n-\"\"\"\n-\n-\n-class GradVisualizer:\n-    \"\"\"Plot gradients of the outputs w.r.t an input image.\"\"\"\n-\n-    def __init__(self, positive_channel=None, negative_channel=None):\n-        if positive_channel is None:\n-            self.positive_channel = [0, 255, 0]\n-        else:\n-            self.positive_channel = positive_channel\n-\n-        if negative_channel is None:\n-            self.negative_channel = [255, 0, 0]\n-        else:\n-            self.negative_channel = negative_channel\n-\n-    def apply_polarity(self, attributions, polarity):\n-        if polarity == \"positive\":\n-            return np.clip(attributions, 0, 1)\n-        else:\n-            return np.clip(attributions, -1, 0)\n-\n-    def apply_linear_transformation(\n-        self,\n-        attributions,\n-        clip_above_percentile=99.9,\n-        clip_below_percentile=70.0,\n-        lower_end=0.2,\n-    ):\n-        # 1. Get the thresholds\n-        m = self.get_thresholded_attributions(\n-            attributions, percentage=100 - clip_above_percentile\n-        )\n-        e = self.get_thresholded_attributions(\n-            attributions, percentage=100 - clip_below_percentile\n-        )\n-\n-        # 2. Transform the attributions by a linear function f(x) = a*x + b such that\n-        # f(m) = 1.0 and f(e) = lower_end\n-        transformed_attributions = (1 - lower_end) * (\n-            np.abs(attributions) - e\n-        ) / (m - e) + lower_end\n-\n-        # 3. Make sure that the sign of transformed attributions is the same as original attributions\n-        transformed_attributions *= np.sign(attributions)\n-\n-        # 4. Only keep values that are bigger than the lower_end\n-        transformed_attributions *= transformed_attributions >= lower_end\n-\n-        # 5. Clip values and return\n-        transformed_attributions = np.clip(transformed_attributions, 0.0, 1.0)\n-        return transformed_attributions\n-\n-    def get_thresholded_attributions(self, attributions, percentage):\n-        if percentage == 100.0:\n-            return np.min(attributions)\n-\n-        # 1. Flatten the attributions\n-        flatten_attr = attributions.flatten()\n-\n-        # 2. Get the sum of the attributions\n-        total = np.sum(flatten_attr)\n-\n-        # 3. Sort the attributions from largest to smallest.\n-        sorted_attributions = np.sort(np.abs(flatten_attr))[::-1]\n-\n-        # 4. Calculate the percentage of the total sum that each attribution\n-        # and the values about it contribute.\n-        cum_sum = 100.0 * np.cumsum(sorted_attributions) / total\n-\n-        # 5. Threshold the attributions by the percentage\n-        indices_to_consider = np.where(cum_sum >= percentage)[0][0]\n-\n-        # 6. Select the desired attributions and return\n-        attributions = sorted_attributions[indices_to_consider]\n-        return attributions\n-\n-    def binarize(self, attributions, threshold=0.001):\n-        return attributions > threshold\n-\n-    def morphological_cleanup_fn(self, attributions, structure=np.ones((4, 4))):\n-        closed = ndimage.grey_closing(attributions, structure=structure)\n-        opened = ndimage.grey_opening(closed, structure=structure)\n-        return opened\n-\n-    def draw_outlines(\n-        self,\n-        attributions,\n-        percentage=90,\n-        connected_component_structure=np.ones((3, 3)),\n-    ):\n-        # 1. Binarize the attributions.\n-        attributions = self.binarize(attributions)\n-\n-        # 2. Fill the gaps\n-        attributions = ndimage.binary_fill_holes(attributions)\n-\n-        # 3. Compute connected components\n-        connected_components, num_comp = ndimage.label(\n-            attributions, structure=connected_component_structure\n-        )\n-\n-        # 4. Sum up the attributions for each component\n-        total = np.sum(attributions[connected_components > 0])\n-        component_sums = []\n-        for comp in range(1, num_comp + 1):\n-            mask = connected_components == comp\n-            component_sum = np.sum(attributions[mask])\n-            component_sums.append((component_sum, mask))\n-\n-        # 5. Compute the percentage of top components to keep\n-        sorted_sums_and_masks = sorted(\n-            component_sums, key=lambda x: x[0], reverse=True\n-        )\n-        sorted_sums = list(zip(*sorted_sums_and_masks))[0]\n-        cumulative_sorted_sums = np.cumsum(sorted_sums)\n-        cutoff_threshold = percentage * total / 100\n-        cutoff_idx = np.where(cumulative_sorted_sums >= cutoff_threshold)[0][0]\n-        if cutoff_idx > 2:\n-            cutoff_idx = 2\n-\n-        # 6. Set the values for the kept components\n-        border_mask = np.zeros_like(attributions)\n-        for i in range(cutoff_idx + 1):\n-            border_mask[sorted_sums_and_masks[i][1]] = 1\n-\n-        # 7. Make the mask hollow and show only the border\n-        eroded_mask = ndimage.binary_erosion(border_mask, iterations=1)\n-        border_mask[eroded_mask] = 0\n-\n-        # 8. Return the outlined mask\n-        return border_mask\n-\n-    def process_grads(\n-        self,\n-        image,\n-        attributions,\n-        polarity=\"positive\",\n-        clip_above_percentile=99.9,\n-        clip_below_percentile=0,\n-        morphological_cleanup=False,\n-        structure=np.ones((3, 3)),\n-        outlines=False,\n-        outlines_component_percentage=90,\n-        overlay=True,\n-    ):\n-        if polarity not in [\"positive\", \"negative\"]:\n-            raise ValueError(\n-                f\"\"\" Allowed polarity values: 'positive' or 'negative'\n-                                    but provided {polarity}\"\"\"\n-            )\n-        if clip_above_percentile < 0 or clip_above_percentile > 100:\n-            raise ValueError(\"clip_above_percentile must be in [0, 100]\")\n-\n-        if clip_below_percentile < 0 or clip_below_percentile > 100:\n-            raise ValueError(\"clip_below_percentile must be in [0, 100]\")\n-\n-        # 1. Apply polarity\n-        if polarity == \"positive\":\n-            attributions = self.apply_polarity(attributions, polarity=polarity)\n-            channel = self.positive_channel\n-        else:\n-            attributions = self.apply_polarity(attributions, polarity=polarity)\n-            attributions = np.abs(attributions)\n-            channel = self.negative_channel\n-\n-        # 2. Take average over the channels\n-        attributions = np.average(attributions, axis=2)\n-\n-        # 3. Apply linear transformation to the attributions\n-        attributions = self.apply_linear_transformation(\n-            attributions,\n-            clip_above_percentile=clip_above_percentile,\n-            clip_below_percentile=clip_below_percentile,\n-            lower_end=0.0,\n-        )\n-\n-        # 4. Cleanup\n-        if morphological_cleanup:\n-            attributions = self.morphological_cleanup_fn(\n-                attributions, structure=structure\n-            )\n-        # 5. Draw the outlines\n-        if outlines:\n-            attributions = self.draw_outlines(\n-                attributions, percentage=outlines_component_percentage\n-            )\n-\n-        # 6. Expand the channel axis and convert to RGB\n-        attributions = np.expand_dims(attributions, 2) * channel\n-\n-        # 7.Superimpose on the original image\n-        if overlay:\n-            attributions = np.clip((attributions * 0.8 + image), 0, 255)\n-        return attributions\n-\n-    def visualize(\n-        self,\n-        image,\n-        gradients,\n-        integrated_gradients,\n-        polarity=\"positive\",\n-        clip_above_percentile=99.9,\n-        clip_below_percentile=0,\n-        morphological_cleanup=False,\n-        structure=np.ones((3, 3)),\n-        outlines=False,\n-        outlines_component_percentage=90,\n-        overlay=True,\n-        figsize=(15, 8),\n-    ):\n-        # 1. Make two copies of the original image\n-        img1 = np.copy(image)\n-        img2 = np.copy(image)\n-\n-        # 2. Process the normal gradients\n-        grads_attr = self.process_grads(\n-            image=img1,\n-            attributions=gradients,\n-            polarity=polarity,\n-            clip_above_percentile=clip_above_percentile,\n-            clip_below_percentile=clip_below_percentile,\n-            morphological_cleanup=morphological_cleanup,\n-            structure=structure,\n-            outlines=outlines,\n-            outlines_component_percentage=outlines_component_percentage,\n-            overlay=overlay,\n-        )\n-\n-        # 3. Process the integrated gradients\n-        igrads_attr = self.process_grads(\n-            image=img2,\n-            attributions=integrated_gradients,\n-            polarity=polarity,\n-            clip_above_percentile=clip_above_percentile,\n-            clip_below_percentile=clip_below_percentile,\n-            morphological_cleanup=morphological_cleanup,\n-            structure=structure,\n-            outlines=outlines,\n-            outlines_component_percentage=outlines_component_percentage,\n-            overlay=overlay,\n-        )\n-\n-        _, ax = plt.subplots(1, 3, figsize=figsize)\n-        ax[0].imshow(image)\n-        ax[1].imshow(grads_attr.astype(np.uint8))\n-        ax[2].imshow(igrads_attr.astype(np.uint8))\n-\n-        ax[0].set_title(\"Input\")\n-        ax[1].set_title(\"Normal gradients\")\n-        ax[2].set_title(\"Integrated gradients\")\n-        plt.show()\n-\n-\n-\"\"\"\n-## Let's test-drive it\n-\"\"\"\n-\n-# 1. Convert the image to numpy array\n-img = get_img_array(img_path)\n-\n-# 2. Keep a copy of the original image\n-orig_img = np.copy(img[0]).astype(np.uint8)\n-\n-# 3. Preprocess the image\n-img_processed = tf.cast(xception.preprocess_input(img), dtype=tf.float32)\n-\n-# 4. Get model predictions\n-preds = model.predict(img_processed)\n-top_pred_idx = tf.argmax(preds[0])\n-print(\"Predicted:\", top_pred_idx, xception.decode_predictions(preds, top=1)[0])\n-\n-# 5. Get the gradients of the last layer for the predicted label\n-grads = get_gradients(img_processed, top_pred_idx=top_pred_idx)\n-\n-# 6. Get the integrated gradients\n-igrads = random_baseline_integrated_gradients(\n-    np.copy(orig_img), top_pred_idx=top_pred_idx, num_steps=50, num_runs=2\n-)\n-\n-# 7. Process the gradients and plot\n-vis = GradVisualizer()\n-vis.visualize(\n-    image=orig_img,\n-    gradients=grads[0].numpy(),\n-    integrated_gradients=igrads.numpy(),\n-    clip_above_percentile=99,\n-    clip_below_percentile=0,\n-)\n-\n-vis.visualize(\n-    image=orig_img,\n-    gradients=grads[0].numpy(),\n-    integrated_gradients=igrads.numpy(),\n-    clip_above_percentile=95,\n-    clip_below_percentile=28,\n-    morphological_cleanup=True,\n-    outlines=True,\n-)\n\n@@ -1,521 +0,0 @@\n-\"\"\"\n-Title: Involutional neural networks\n-Author: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)\n-Date created: 2021/07/25\n-Last modified: 2021/07/25\n-Description: Deep dive into location-specific and channel-agnostic \"involution\" kernels.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Convolution has been the basis of most modern neural\n-networks for computer vision. A convolution kernel is\n-spatial-agnostic and channel-specific. Because of this, it isn't able\n-to adapt to different visual patterns with respect to\n-different spatial locations. Along with location-related problems, the\n-receptive field of convolution creates challenges with regard to capturing\n-long-range spatial interactions.\n-\n-To address the above issues, Li et. al. rethink the properties\n-of convolution in\n-[Involution: Inverting the Inherence of Convolution for VisualRecognition](https://arxiv.org/abs/2103.06255).\n-The authors propose the \"involution kernel\", that is location-specific and\n-channel-agnostic. Due to the location-specific nature of the operation,\n-the authors say that self-attention falls under the design paradigm of\n-involution.\n-\n-This example describes the involution kernel, compares two image\n-classification models, one with convolution and the other with\n-involution, and also tries drawing a parallel with the self-attention\n-layer.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import tensorflow as tf\n-import keras\n-import matplotlib.pyplot as plt\n-\n-# Set seed for reproducibility.\n-tf.random.set_seed(42)\n-\n-\"\"\"\n-## Convolution\n-\n-Convolution remains the mainstay of deep neural networks for computer vision.\n-To understand Involution, it is necessary to talk about the\n-convolution operation.\n-\n-![Imgur](https://i.imgur.com/MSKLsm5.png)\n-\n-Consider an input tensor **X** with dimensions **H**, **W** and\n-**C_in**. We take a collection of **C_out** convolution kernels each of\n-shape **K**, **K**, **C_in**. With the multiply-add operation between\n-the input tensor and the kernels we obtain an output tensor **Y** with\n-dimensions **H**, **W**, **C_out**.\n-\n-In the diagram above `C_out=3`. This makes the output tensor of shape H,\n-W and 3. One can notice that the convoltuion kernel does not depend on\n-the spatial position of the input tensor which makes it\n-**location-agnostic**. On the other hand, each channel in the output\n-tensor is based on a specific convolution filter which makes is\n-**channel-specific**.\n-\"\"\"\n-\n-\"\"\"\n-## Involution\n-\n-The idea is to have an operation that is both **location-specific**\n-and **channel-agnostic**. Trying to implement these specific properties poses\n-a challenge. With a fixed number of involution kernels (for each\n-spatial position) we will **not** be able to process variable-resolution\n-input tensors.\n-\n-To solve this problem, the authors have considered *generating* each\n-kernel conditioned on specific spatial positions. With this method, we\n-should be able to process variable-resolution input tensors with ease.\n-The diagram below provides an intuition on this kernel generation\n-method.\n-\n-![Imgur](https://i.imgur.com/jtrGGQg.png)\n-\"\"\"\n-\n-\n-class Involution(keras.layers.Layer):\n-    def __init__(\n-        self, channel, group_number, kernel_size, stride, reduction_ratio, name\n-    ):\n-        super().__init__(name=name)\n-\n-        # Initialize the parameters.\n-        self.channel = channel\n-        self.group_number = group_number\n-        self.kernel_size = kernel_size\n-        self.stride = stride\n-        self.reduction_ratio = reduction_ratio\n-\n-    def build(self, input_shape):\n-        # Get the shape of the input.\n-        (_, height, width, num_channels) = input_shape\n-\n-        # Scale the height and width with respect to the strides.\n-        height = height // self.stride\n-        width = width // self.stride\n-\n-        # Define a layer that average pools the input tensor\n-        # if stride is more than 1.\n-        self.stride_layer = (\n-            keras.layers.AveragePooling2D(\n-                pool_size=self.stride, strides=self.stride, padding=\"same\"\n-            )\n-            if self.stride > 1\n-            else tf.identity\n-        )\n-        # Define the kernel generation layer.\n-        self.kernel_gen = keras.Sequential(\n-            [\n-                keras.layers.Conv2D(\n-                    filters=self.channel // self.reduction_ratio, kernel_size=1\n-                ),\n-                keras.layers.BatchNormalization(),\n-                keras.layers.ReLU(),\n-                keras.layers.Conv2D(\n-                    filters=self.kernel_size\n-                    * self.kernel_size\n-                    * self.group_number,\n-                    kernel_size=1,\n-                ),\n-            ]\n-        )\n-        # Define reshape layers\n-        self.kernel_reshape = keras.layers.Reshape(\n-            target_shape=(\n-                height,\n-                width,\n-                self.kernel_size * self.kernel_size,\n-                1,\n-                self.group_number,\n-            )\n-        )\n-        self.input_patches_reshape = keras.layers.Reshape(\n-            target_shape=(\n-                height,\n-                width,\n-                self.kernel_size * self.kernel_size,\n-                num_channels // self.group_number,\n-                self.group_number,\n-            )\n-        )\n-        self.output_reshape = keras.layers.Reshape(\n-            target_shape=(height, width, num_channels)\n-        )\n-\n-    def call(self, x):\n-        # Generate the kernel with respect to the input tensor.\n-        # B, H, W, K*K*G\n-        kernel_input = self.stride_layer(x)\n-        kernel = self.kernel_gen(kernel_input)\n-\n-        # reshape the kerenl\n-        # B, H, W, K*K, 1, G\n-        kernel = self.kernel_reshape(kernel)\n-\n-        # Extract input patches.\n-        # B, H, W, K*K*C\n-        input_patches = tf.image.extract_patches(\n-            images=x,\n-            sizes=[1, self.kernel_size, self.kernel_size, 1],\n-            strides=[1, self.stride, self.stride, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-\n-        # Reshape the input patches to align with later operations.\n-        # B, H, W, K*K, C//G, G\n-        input_patches = self.input_patches_reshape(input_patches)\n-\n-        # Compute the multiply-add operation of kernels and patches.\n-        # B, H, W, K*K, C//G, G\n-        output = tf.multiply(kernel, input_patches)\n-        # B, H, W, C//G, G\n-        output = tf.reduce_sum(output, axis=3)\n-\n-        # Reshape the output kernel.\n-        # B, H, W, C\n-        output = self.output_reshape(output)\n-\n-        # Return the output tensor and the kernel.\n-        return output, kernel\n-\n-\n-\"\"\"\n-## Testing the Involution layer\n-\"\"\"\n-\n-# Define the input tensor.\n-input_tensor = tf.random.normal((32, 256, 256, 3))\n-\n-# Compute involution with stride 1.\n-output_tensor, _ = Involution(\n-    channel=3,\n-    group_number=1,\n-    kernel_size=5,\n-    stride=1,\n-    reduction_ratio=1,\n-    name=\"inv_1\",\n-)(input_tensor)\n-print(f\"with stride 1 ouput shape: {output_tensor.shape}\")\n-\n-# Compute involution with stride 2.\n-output_tensor, _ = Involution(\n-    channel=3,\n-    group_number=1,\n-    kernel_size=5,\n-    stride=2,\n-    reduction_ratio=1,\n-    name=\"inv_2\",\n-)(input_tensor)\n-print(f\"with stride 2 ouput shape: {output_tensor.shape}\")\n-\n-# Compute involution with stride 1, channel 16 and reduction ratio 2.\n-output_tensor, _ = Involution(\n-    channel=16,\n-    group_number=1,\n-    kernel_size=5,\n-    stride=1,\n-    reduction_ratio=2,\n-    name=\"inv_3\",\n-)(input_tensor)\n-print(\n-    \"with channel 16 and reduction ratio 2 ouput shape: {}\".format(\n-        output_tensor.shape\n-    )\n-)\n-\n-\"\"\"\n-## Image Classification\n-\n-In this section, we will build an image-classifier model. There will\n-be two models one with convolutions and the other with involutions.\n-\n-The image-classification model is heavily inspired by this\n-[Convolutional Neural Network (CNN)](https://www.tensorflow.org/tutorials/images/cnn)\n-tutorial from Google.\n-\"\"\"\n-\n-\"\"\"\n-## Get the CIFAR10 Dataset\n-\"\"\"\n-\n-# Load the CIFAR10 dataset.\n-print(\"loading the CIFAR10 dataset...\")\n-(\n-    (train_images, train_labels),\n-    (\n-        test_images,\n-        test_labels,\n-    ),\n-) = keras.datasets.cifar10.load_data()\n-\n-# Normalize pixel values to be between 0 and 1.\n-(train_images, test_images) = (train_images / 255.0, test_images / 255.0)\n-\n-# Shuffle and batch the dataset.\n-train_ds = (\n-    tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n-    .shuffle(256)\n-    .batch(256)\n-)\n-test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(\n-    256\n-)\n-\n-\"\"\"\n-## Visualise the data\n-\"\"\"\n-\n-class_names = [\n-    \"airplane\",\n-    \"automobile\",\n-    \"bird\",\n-    \"cat\",\n-    \"deer\",\n-    \"dog\",\n-    \"frog\",\n-    \"horse\",\n-    \"ship\",\n-    \"truck\",\n-]\n-\n-plt.figure(figsize=(10, 10))\n-for i in range(25):\n-    plt.subplot(5, 5, i + 1)\n-    plt.xticks([])\n-    plt.yticks([])\n-    plt.grid(False)\n-    plt.imshow(train_images[i])\n-    plt.xlabel(class_names[train_labels[i][0]])\n-plt.show()\n-\n-\"\"\"\n-## Convolutional Neural Network\n-\"\"\"\n-\n-# Build the conv model.\n-print(\"building the convolution model...\")\n-conv_model = keras.Sequential(\n-    [\n-        keras.layers.Conv2D(\n-            32, (3, 3), input_shape=(32, 32, 3), padding=\"same\"\n-        ),\n-        keras.layers.ReLU(name=\"relu1\"),\n-        keras.layers.MaxPooling2D((2, 2)),\n-        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n-        keras.layers.ReLU(name=\"relu2\"),\n-        keras.layers.MaxPooling2D((2, 2)),\n-        keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n-        keras.layers.ReLU(name=\"relu3\"),\n-        keras.layers.Flatten(),\n-        keras.layers.Dense(64, activation=\"relu\"),\n-        keras.layers.Dense(10),\n-    ]\n-)\n-\n-# Compile the mode with the necessary loss function and optimizer.\n-print(\"compiling the convolution model...\")\n-conv_model.compile(\n-    optimizer=\"adam\",\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[\"accuracy\"],\n-    jit_compile=False,\n-)\n-\n-# Train the model.\n-print(\"conv model training...\")\n-conv_hist = conv_model.fit(train_ds, epochs=20, validation_data=test_ds)\n-\n-\"\"\"\n-## Involutional Neural Network\n-\"\"\"\n-\n-# Build the involution model.\n-print(\"building the involution model...\")\n-\n-inputs = keras.Input(shape=(32, 32, 3))\n-x, _ = Involution(\n-    channel=3,\n-    group_number=1,\n-    kernel_size=3,\n-    stride=1,\n-    reduction_ratio=2,\n-    name=\"inv_1\",\n-)(inputs)\n-x = keras.layers.ReLU()(x)\n-x = keras.layers.MaxPooling2D((2, 2))(x)\n-x, _ = Involution(\n-    channel=3,\n-    group_number=1,\n-    kernel_size=3,\n-    stride=1,\n-    reduction_ratio=2,\n-    name=\"inv_2\",\n-)(x)\n-x = keras.layers.ReLU()(x)\n-x = keras.layers.MaxPooling2D((2, 2))(x)\n-x, _ = Involution(\n-    channel=3,\n-    group_number=1,\n-    kernel_size=3,\n-    stride=1,\n-    reduction_ratio=2,\n-    name=\"inv_3\",\n-)(x)\n-x = keras.layers.ReLU()(x)\n-x = keras.layers.Flatten()(x)\n-x = keras.layers.Dense(64, activation=\"relu\")(x)\n-outputs = keras.layers.Dense(10)(x)\n-\n-inv_model = keras.Model(inputs=[inputs], outputs=[outputs], name=\"inv_model\")\n-\n-# Compile the mode with the necessary loss function and optimizer.\n-print(\"compiling the involution model...\")\n-inv_model.compile(\n-    optimizer=\"adam\",\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[\"accuracy\"],\n-    jit_compile=False,\n-)\n-\n-# train the model\n-print(\"inv model training...\")\n-inv_hist = inv_model.fit(train_ds, epochs=20, validation_data=test_ds)\n-\n-\"\"\"\n-## Comparisons\n-\n-In this section, we will be looking at both the models and compare a\n-few pointers.\n-\"\"\"\n-\n-\"\"\"\n-### Parameters\n-\n-One can see that with a similar architecture the parameters in a CNN\n-is much larger than that of an INN (Involutional Neural Network).\n-\"\"\"\n-\n-conv_model.summary()\n-\n-inv_model.summary()\n-\n-\"\"\"\n-### Loss and Accuracy Plots\n-\n-Here, the loss and the accuracy plots demonstrate that INNs are slow\n-learners (with lower parameters).\n-\"\"\"\n-\n-plt.figure(figsize=(20, 5))\n-\n-plt.subplot(1, 2, 1)\n-plt.title(\"Convolution Loss\")\n-plt.plot(conv_hist.history[\"loss\"], label=\"loss\")\n-plt.plot(conv_hist.history[\"val_loss\"], label=\"val_loss\")\n-plt.legend()\n-\n-plt.subplot(1, 2, 2)\n-plt.title(\"Involution Loss\")\n-plt.plot(inv_hist.history[\"loss\"], label=\"loss\")\n-plt.plot(inv_hist.history[\"val_loss\"], label=\"val_loss\")\n-plt.legend()\n-\n-plt.show()\n-\n-plt.figure(figsize=(20, 5))\n-\n-plt.subplot(1, 2, 1)\n-plt.title(\"Convolution Accuracy\")\n-plt.plot(conv_hist.history[\"accuracy\"], label=\"accuracy\")\n-plt.plot(conv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n-plt.legend()\n-\n-plt.subplot(1, 2, 2)\n-plt.title(\"Involution Accuracy\")\n-plt.plot(inv_hist.history[\"accuracy\"], label=\"accuracy\")\n-plt.plot(inv_hist.history[\"val_accuracy\"], label=\"val_accuracy\")\n-plt.legend()\n-\n-plt.show()\n-\n-\"\"\"\n-## Visualizing Involution Kernels\n-\n-To visualize the kernels, we take the sum of **K×K** values from each\n-involution kernel. **All the representatives at different spatial\n-locations frame the corresponding heat map.**\n-\n-The authors mention:\n-\n-\"Our proposed involution is reminiscent of self-attention and\n-essentially could become a generalized version of it.\"\n-\n-With the visualization of the kernel we can indeed obtain an attention\n-map of the image. The learned involution kernels provides attention to\n-individual spatial positions of the input tensor. The\n-**location-specific** property makes involution a generic space of models\n-in which self-attention belongs.\n-\"\"\"\n-\n-layer_names = [\"inv_1\", \"inv_2\", \"inv_3\"]\n-# The kernel is the second output of the layer\n-outputs = [inv_model.get_layer(name).output[1] for name in layer_names]\n-vis_model = keras.Model(inv_model.input, outputs)\n-\n-fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(10, 30))\n-\n-for ax, test_image in zip(axes, test_images[:10]):\n-    inv_out = vis_model.predict(test_image[None, ...])\n-    inv1_kernel, inv2_kernel, inv3_kernel = inv_out\n-\n-    inv1_kernel = tf.reduce_sum(inv1_kernel, axis=[-1, -2, -3])\n-    inv2_kernel = tf.reduce_sum(inv2_kernel, axis=[-1, -2, -3])\n-    inv3_kernel = tf.reduce_sum(inv3_kernel, axis=[-1, -2, -3])\n-\n-    ax[0].imshow(keras.utils.array_to_img(test_image))\n-    ax[0].set_title(\"Input Image\")\n-\n-    ax[1].imshow(keras.utils.array_to_img(inv1_kernel[0, ..., None]))\n-    ax[1].set_title(\"Involution Kernel 1\")\n-\n-    ax[2].imshow(keras.utils.array_to_img(inv2_kernel[0, ..., None]))\n-    ax[2].set_title(\"Involution Kernel 2\")\n-\n-    ax[3].imshow(keras.utils.array_to_img(inv3_kernel[0, ..., None]))\n-    ax[3].set_title(\"Involution Kernel 3\")\n-\n-\"\"\"\n-## Conclusions\n-\n-In this example, the main focus was to build an `Involution` layer which\n-can be easily reused. While our comparisons were based on a specific\n-task, feel free to use the layer for different tasks and report your\n-results.\n-\n-According to me, the key take-away of involution is its\n-relationship with self-attention. The intuition behind location-specific\n-and channel-spefic processing makes sense in a lot of tasks.\n-\n-Moving forward one can:\n-\n-- Look at [Yannick's video](https://youtu.be/pH2jZun8MoY) on\n-    involution for a better understanding.\n-- Experiment with the various hyperparameters of the involution layer.\n-- Build different models with the involution layer.\n-- Try building a different kernel generation method altogether.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/involution)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/involution).\n-\"\"\"\n\n@@ -1,318 +0,0 @@\n-\"\"\"\n-Title: Metric learning for image similarity search\n-Author: [Mat Kelcey](https://twitter.com/mat_kelcey)\n-Date created: 2020/06/05\n-Last modified: 2020/06/09\n-Description: Example of using similarity metric learning on CIFAR-10 images.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Overview\n-\n-Metric learning aims to train models that can embed inputs into a high-dimensional space\n-such that \"similar\" inputs, as defined by the training scheme, are located close to each\n-other. These models once trained can produce embeddings for downstream systems where such\n-similarity is useful; examples include as a ranking signal for search or as a form of\n-pretrained embedding model for another supervised problem.\n-\n-For a more detailed overview of metric learning see:\n-\n-* [What is metric learning?](http://contrib.scikit-learn.org/metric-learn/introduction.html)\n-* [\"Using crossentropy for metric learning\" tutorial](https://www.youtube.com/watch?v=Jb4Ewl5RzkI)\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\n-Set Keras backend to tensorflow. \n-\"\"\"\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import random\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import tensorflow as tf\n-from collections import defaultdict\n-from PIL import Image\n-from sklearn.metrics import ConfusionMatrixDisplay\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Dataset\n-\n-For this example we will be using the\n-[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n-\"\"\"\n-\n-from keras.datasets import cifar10\n-\n-\n-(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-\n-x_train = x_train.astype(\"float32\") / 255.0\n-y_train = np.squeeze(y_train)\n-x_test = x_test.astype(\"float32\") / 255.0\n-y_test = np.squeeze(y_test)\n-\n-\"\"\"\n-To get a sense of the dataset we can visualise a grid of 25 random examples.\n-\n-\n-\"\"\"\n-\n-height_width = 32\n-\n-\n-def show_collage(examples):\n-    box_size = height_width + 2\n-    num_rows, num_cols = examples.shape[:2]\n-\n-    collage = Image.new(\n-        mode=\"RGB\",\n-        size=(num_cols * box_size, num_rows * box_size),\n-        color=(250, 250, 250),\n-    )\n-    for row_idx in range(num_rows):\n-        for col_idx in range(num_cols):\n-            array = (np.array(examples[row_idx, col_idx]) * 255).astype(np.uint8)\n-            collage.paste(\n-                Image.fromarray(array), (col_idx * box_size, row_idx * box_size)\n-            )\n-\n-    # Double size for visualisation.\n-    collage = collage.resize((2 * num_cols * box_size, 2 * num_rows * box_size))\n-    return collage\n-\n-\n-# Show a collage of 5x5 random images.\n-sample_idxs = np.random.randint(0, 50000, size=(5, 5))\n-examples = x_train[sample_idxs]\n-show_collage(examples)\n-\n-\"\"\"\n-Metric learning provides training data not as explicit `(X, y)` pairs but instead uses\n-multiple instances that are related in the way we want to express similarity. In our\n-example we will use instances of the same class to represent similarity; a single\n-training instance will not be one image, but a pair of images of the same class. When\n-referring to the images in this pair we'll use the common metric learning names of the\n-`anchor` (a randomly chosen image) and the `positive` (another randomly chosen image of\n-the same class).\n-\n-To facilitate this we need to build a form of lookup that maps from classes to the\n-instances of that class. When generating data for training we will sample from this\n-lookup.\n-\"\"\"\n-\n-class_idx_to_train_idxs = defaultdict(list)\n-for y_train_idx, y in enumerate(y_train):\n-    class_idx_to_train_idxs[y].append(y_train_idx)\n-\n-class_idx_to_test_idxs = defaultdict(list)\n-for y_test_idx, y in enumerate(y_test):\n-    class_idx_to_test_idxs[y].append(y_test_idx)\n-\n-\"\"\"\n-For this example we are using the simplest approach to training; a batch will consist of\n-`(anchor, positive)` pairs spread across the classes. The goal of learning will be to\n-move the anchor and positive pairs closer together and further away from other instances\n-in the batch. In this case the batch size will be dictated by the number of classes; for\n-CIFAR-10 this is 10.\n-\"\"\"\n-\n-num_classes = 10\n-\n-\n-class AnchorPositivePairs(keras.utils.Sequence):\n-    def __init__(self, num_batches):\n-        super().__init__()\n-        self.num_batches = num_batches\n-\n-    def __len__(self):\n-        return self.num_batches\n-\n-    def __getitem__(self, _idx):\n-        x = np.empty((2, num_classes, height_width, height_width, 3), dtype=np.float32)\n-        for class_idx in range(num_classes):\n-            examples_for_class = class_idx_to_train_idxs[class_idx]\n-            anchor_idx = random.choice(examples_for_class)\n-            positive_idx = random.choice(examples_for_class)\n-            while positive_idx == anchor_idx:\n-                positive_idx = random.choice(examples_for_class)\n-            x[0, class_idx] = x_train[anchor_idx]\n-            x[1, class_idx] = x_train[positive_idx]\n-        return x\n-\n-\n-\"\"\"\n-We can visualise a batch in another collage. The top row shows randomly chosen anchors\n-from the 10 classes, the bottom row shows the corresponding 10 positives.\n-\"\"\"\n-\n-examples = next(iter(AnchorPositivePairs(num_batches=1)))\n-\n-show_collage(examples)\n-\n-\"\"\"\n-## Embedding model\n-\n-We define a custom model with a `train_step` that first embeds both anchors and positives\n-and then uses their pairwise dot products as logits for a softmax.\n-\"\"\"\n-\n-\n-class EmbeddingModel(keras.Model):\n-    def train_step(self, data):\n-        # Note: Workaround for open issue, to be removed.\n-        if isinstance(data, tuple):\n-            data = data[0]\n-        anchors, positives = data[0], data[1]\n-\n-        with tf.GradientTape() as tape:\n-            # Run both anchors and positives through model.\n-            anchor_embeddings = self(anchors, training=True)\n-            positive_embeddings = self(positives, training=True)\n-\n-            # Calculate cosine similarity between anchors and positives. As they have\n-            # been normalised this is just the pair wise dot products.\n-            similarities = keras.ops.einsum(\n-                \"ae,pe->ap\", anchor_embeddings, positive_embeddings\n-            )\n-\n-            # Since we intend to use these as logits we scale them by a temperature.\n-            # This value would normally be chosen as a hyper parameter.\n-            temperature = 0.2\n-            similarities /= temperature\n-\n-            # We use these similarities as logits for a softmax. The labels for\n-            # this call are just the sequence [0, 1, 2, ..., num_classes] since we\n-            # want the main diagonal values, which correspond to the anchor/positive\n-            # pairs, to be high. This loss will move embeddings for the\n-            # anchor/positive pairs together and move all other pairs apart.\n-            sparse_labels = keras.ops.arange(num_classes)\n-            loss = self.compiled_loss(sparse_labels, similarities)\n-\n-        # Calculate gradients and apply via optimizer.\n-        gradients = tape.gradient(loss, self.trainable_variables)\n-        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n-\n-        # Update and return metrics (specifically the one for the loss value).\n-        self.compiled_metrics.update_state(sparse_labels, similarities)\n-        return {m.name: m.result() for m in self.metrics}\n-\n-\n-\"\"\"\n-Next we describe the architecture that maps from an image to an embedding. This model\n-simply consists of a sequence of 2d convolutions followed by global pooling with a final\n-linear projection to an embedding space. As is common in metric learning we normalise the\n-embeddings so that we can use simple dot products to measure similarity. For simplicity\n-this model is intentionally small.\n-\"\"\"\n-\n-inputs = layers.Input(shape=(height_width, height_width, 3))\n-x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\")(inputs)\n-x = layers.Conv2D(filters=64, kernel_size=3, strides=2, activation=\"relu\")(x)\n-x = layers.Conv2D(filters=128, kernel_size=3, strides=2, activation=\"relu\")(x)\n-x = layers.GlobalAveragePooling2D()(x)\n-embeddings = layers.Dense(units=8, activation=None)(x)\n-embeddings = layers.UnitNormalization()(embeddings)\n-\n-model = EmbeddingModel(inputs, embeddings)\n-\n-\"\"\"\n-Finally we run the training. On a Google Colab GPU instance this takes about a minute.\n-\"\"\"\n-\n-model.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-)\n-\n-history = model.fit(AnchorPositivePairs(num_batches=1000), epochs=20)\n-\n-plt.plot(history.history[\"loss\"])\n-plt.show()\n-\n-\"\"\"\n-## Testing\n-\n-We can review the quality of this model by applying it to the test set and considering\n-near neighbours in the embedding space.\n-\n-First we embed the test set and calculate all near neighbours. Recall that since the\n-embeddings are unit length we can calculate cosine similarity via dot products.\n-\"\"\"\n-\n-near_neighbours_per_example = 10\n-\n-embeddings = model.predict(x_test)\n-gram_matrix = np.einsum(\"ae,be->ab\", embeddings, embeddings)\n-near_neighbours = np.argsort(gram_matrix.T)[:, -(near_neighbours_per_example + 1) :]\n-\n-\"\"\"\n-As a visual check of these embeddings we can build a collage of the near neighbours for 5\n-random examples. The first column of the image below is a randomly selected image, the\n-following 10 columns show the nearest neighbours in order of similarity.\n-\"\"\"\n-\n-num_collage_examples = 5\n-\n-examples = np.empty(\n-    (\n-        num_collage_examples,\n-        near_neighbours_per_example + 1,\n-        height_width,\n-        height_width,\n-        3,\n-    ),\n-    dtype=np.float32,\n-)\n-for row_idx in range(num_collage_examples):\n-    examples[row_idx, 0] = x_test[row_idx]\n-    anchor_near_neighbours = reversed(near_neighbours[row_idx][:-1])\n-    for col_idx, nn_idx in enumerate(anchor_near_neighbours):\n-        examples[row_idx, col_idx + 1] = x_test[nn_idx]\n-\n-show_collage(examples)\n-\n-\"\"\"\n-We can also get a quantified view of the performance by considering the correctness of\n-near neighbours in terms of a confusion matrix.\n-\n-Let us sample 10 examples from each of the 10 classes and consider their near neighbours\n-as a form of prediction; that is, does the example and its near neighbours share the same\n-class?\n-\n-We observe that each animal class does generally well, and is confused the most with the\n-other animal classes. The vehicle classes follow the same pattern.\n-\"\"\"\n-\n-confusion_matrix = np.zeros((num_classes, num_classes))\n-\n-# For each class.\n-for class_idx in range(num_classes):\n-    # Consider 10 examples.\n-    example_idxs = class_idx_to_test_idxs[class_idx][:10]\n-    for y_test_idx in example_idxs:\n-        # And count the classes of its near neighbours.\n-        for nn_idx in near_neighbours[y_test_idx][:-1]:\n-            nn_class_idx = y_test[nn_idx]\n-            confusion_matrix[class_idx, nn_class_idx] += 1\n-\n-# Display a confusion matrix.\n-labels = [\n-    \"Airplane\",\n-    \"Automobile\",\n-    \"Bird\",\n-    \"Cat\",\n-    \"Deer\",\n-    \"Dog\",\n-    \"Frog\",\n-    \"Horse\",\n-    \"Ship\",\n-    \"Truck\",\n-]\n-disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n-disp.plot(include_values=True, cmap=\"viridis\", ax=None, xticks_rotation=\"vertical\")\n-plt.show()\n\\ No newline at end of file\n\n@@ -1,532 +0,0 @@\n-\"\"\"\n-Title: Low-light image enhancement using MIRNet\n-Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/09/11\n-Last modified: 2023/07/15\n-Description: Implementing the MIRNet architecture for low-light image enhancement.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-With the goal of recovering high-quality image content from its degraded version, image\n-restoration enjoys numerous applications, such as in\n-photography, security, medical imaging, and remote sensing. In this example, we implement the\n-**MIRNet** model for low-light image enhancement, a fully-convolutional architecture that\n-learns an enriched set of\n-features that combines contextual information from multiple scales, while\n-simultaneously preserving the high-resolution spatial details.\n-\n-### References:\n-\n-- [Learning Enriched Features for Real Image Restoration and Enhancement](https://arxiv.org/abs/2003.06792)\n-- [The Retinex Theory of Color Vision](http://www.cnbc.cmu.edu/~tai/cp_papers/E.Land_Retinex_Theory_ScientifcAmerican.pdf)\n-- [Two deterministic half-quadratic regularization algorithms for computed imaging](https://ieeexplore.ieee.org/document/413553)\n-\"\"\"\n-\n-\"\"\"\n-## Downloading LOLDataset\n-\n-The **LoL Dataset** has been created for low-light image enhancement.\n-It provides 485 images for training and 15 for testing. Each image pair in the dataset\n-consists of a low-light input image and its corresponding well-exposed reference image.\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -q git+https://github.com/keras-team/keras\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import random\n-import numpy as np\n-from glob import glob\n-from PIL import Image, ImageOps\n-import matplotlib.pyplot as plt\n-\n-import keras\n-from keras import layers\n-\n-import tensorflow as tf\n-\n-\"\"\"shell\n-wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n-unzip -q lol_dataset.zip && rm lol_dataset.zip\n-\"\"\"\n-\n-\"\"\"\n-## Creating a TensorFlow Dataset\n-\n-We use 300 image pairs from the LoL Dataset's training set for training,\n-and we use the remaining 185 image pairs for validation.\n-We generate random crops of size `128 x 128` from the image pairs to be\n-used for both training and validation.\n-\"\"\"\n-\n-random.seed(10)\n-\n-IMAGE_SIZE = 128\n-BATCH_SIZE = 4\n-MAX_TRAIN_IMAGES = 300\n-\n-\n-def read_image(image_path):\n-    image = tf.io.read_file(image_path)\n-    image = tf.image.decode_png(image, channels=3)\n-    image.set_shape([None, None, 3])\n-    image = tf.cast(image, dtype=tf.float32) / 255.0\n-    return image\n-\n-\n-def random_crop(low_image, enhanced_image):\n-    low_image_shape = tf.shape(low_image)[:2]\n-    low_w = tf.random.uniform(\n-        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n-    )\n-    low_h = tf.random.uniform(\n-        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n-    )\n-    low_image_cropped = low_image[\n-        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n-    ]\n-    enhanced_image_cropped = enhanced_image[\n-        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n-    ]\n-    # in order to avoid `NONE` during shape inference\n-    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n-    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n-    return low_image_cropped, enhanced_image_cropped\n-\n-\n-def load_data(low_light_image_path, enhanced_image_path):\n-    low_light_image = read_image(low_light_image_path)\n-    enhanced_image = read_image(enhanced_image_path)\n-    low_light_image, enhanced_image = random_crop(\n-        low_light_image, enhanced_image\n-    )\n-    return low_light_image, enhanced_image\n-\n-\n-def get_dataset(low_light_images, enhanced_images):\n-    dataset = tf.data.Dataset.from_tensor_slices(\n-        (low_light_images, enhanced_images)\n-    )\n-    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n-    return dataset\n-\n-\n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n-    :MAX_TRAIN_IMAGES\n-]\n-train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[\n-    :MAX_TRAIN_IMAGES\n-]\n-\n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n-    MAX_TRAIN_IMAGES:\n-]\n-val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[\n-    MAX_TRAIN_IMAGES:\n-]\n-\n-test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n-test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n-\n-\n-train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n-val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n-\n-\n-print(\"Train Dataset:\", train_dataset.element_spec)\n-print(\"Val Dataset:\", val_dataset.element_spec)\n-\n-\"\"\"\n-## MIRNet Model\n-\n-Here are the main features of the MIRNet model:\n-\n-- A feature extraction model that computes a complementary set of features across multiple\n-spatial scales, while maintaining the original high-resolution features to preserve\n-precise spatial details.\n-- A regularly repeated mechanism for information exchange, where the features across\n-multi-resolution branches are progressively fused together for improved representation\n-learning.\n-- A new approach to fuse multi-scale features using a selective kernel network\n-that dynamically combines variable receptive fields and faithfully preserves\n-the original feature information at each spatial resolution.\n-- A recursive residual design that progressively breaks down the input signal\n-in order to simplify the overall learning process, and allows the construction\n-of very deep networks.\n-\n-\n-![](https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png)\n-\"\"\"\n-\n-\"\"\"\n-### Selective Kernel Feature Fusion\n-\n-The Selective Kernel Feature Fusion or SKFF module performs dynamic adjustment of\n-receptive fields via two operations: **Fuse** and **Select**. The Fuse operator generates\n-global feature descriptors by combining the information from multi-resolution streams.\n-The Select operator uses these descriptors to recalibrate the feature maps (of different\n-streams) followed by their aggregation.\n-\n-**Fuse**: The SKFF receives inputs from three parallel convolution streams carrying\n-different scales of information. We first combine these multi-scale features using an\n-element-wise sum, on which we apply Global Average Pooling (GAP) across the spatial\n-dimension. Next, we apply a channel- downscaling convolution layer to generate a compact\n-feature representation which passes through three parallel channel-upscaling convolution\n-layers (one for each resolution stream) and provides us with three feature descriptors.\n-\n-**Select**: This operator applies the softmax function to the feature descriptors to\n-obtain the corresponding activations that are used to adaptively recalibrate multi-scale\n-feature maps. The aggregated features are defined as the sum of product of the corresponding\n-multi-scale feature and the feature descriptor.\n-\n-![](https://i.imgur.com/7U6ixF6.png)\n-\"\"\"\n-\n-\n-def selective_kernel_feature_fusion(\n-    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n-):\n-    channels = list(multi_scale_feature_1.shape)[-1]\n-    combined_feature = layers.Add()(\n-        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n-    )\n-    gap = layers.GlobalAveragePooling2D()(combined_feature)\n-    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n-    compact_feature_representation = layers.Conv2D(\n-        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n-    )(channel_wise_statistics)\n-    feature_descriptor_1 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_descriptor_2 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_descriptor_3 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n-    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n-    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n-    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n-    return aggregated_feature\n-\n-\n-\"\"\"\n-### Dual Attention Unit\n-\n-The Dual Attention Unit or DAU is used to extract features in the convolutional streams.\n-While the SKFF block fuses information across multi-resolution branches, we also need a\n-mechanism to share information within a feature tensor, both along the spatial and the\n-channel dimensions which is done by the DAU block. The DAU suppresses less useful\n-features and only allows more informative ones to pass further. This feature\n-recalibration is achieved by using **Channel Attention** and **Spatial Attention**\n-mechanisms.\n-\n-The **Channel Attention** branch exploits the inter-channel relationships of the\n-convolutional feature maps by applying squeeze and excitation operations. Given a feature\n-map, the squeeze operation applies Global Average Pooling across spatial dimensions to\n-encode global context, thus yielding a feature descriptor. The excitation operator passes\n-this feature descriptor through two convolutional layers followed by the sigmoid gating\n-and generates activations. Finally, the output of Channel Attention branch is obtained by\n-rescaling the input feature map with the output activations.\n-\n-The **Spatial Attention** branch is designed to exploit the inter-spatial dependencies of\n-convolutional features. The goal of Spatial Attention is to generate a spatial attention\n-map and use it to recalibrate the incoming features. To generate the spatial attention\n-map, the Spatial Attention branch first independently applies Global Average Pooling and\n-Max Pooling operations on input features along the channel dimensions and concatenates\n-the outputs to form a resultant feature map which is then passed through a convolution\n-and sigmoid activation to obtain the spatial attention map. This spatial attention map is\n-then used to rescale the input feature map.\n-\n-![](https://i.imgur.com/Dl0IwQs.png)\n-\"\"\"\n-\n-\n-class ChannelPooling(layers.Layer):\n-    def __init__(self, axis=-1, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self.axis = axis\n-        self.concat = layers.Concatenate(axis=self.axis)\n-\n-    def call(self, inputs):\n-        average_pooling = tf.expand_dims(\n-            tf.reduce_mean(inputs, axis=-1), axis=-1\n-        )\n-        max_pooling = tf.expand_dims(tf.reduce_max(inputs, axis=-1), axis=-1)\n-        return self.concat([average_pooling, max_pooling])\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"axis\": self.axis})\n-\n-\n-def spatial_attention_block(input_tensor):\n-    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n-    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n-    feature_map = keras.activations.sigmoid(feature_map)\n-    return input_tensor * feature_map\n-\n-\n-def channel_attention_block(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n-    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n-    feature_activations = layers.Conv2D(\n-        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n-    )(feature_descriptor)\n-    feature_activations = layers.Conv2D(\n-        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n-    )(feature_activations)\n-    return input_tensor * feature_activations\n-\n-\n-def dual_attention_unit_block(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    feature_map = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(input_tensor)\n-    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n-        feature_map\n-    )\n-    channel_attention = channel_attention_block(feature_map)\n-    spatial_attention = spatial_attention_block(feature_map)\n-    concatenation = layers.Concatenate(axis=-1)(\n-        [channel_attention, spatial_attention]\n-    )\n-    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n-    return layers.Add()([input_tensor, concatenation])\n-\n-\n-\"\"\"\n-### Multi-Scale Residual Block\n-\n-The Multi-Scale Residual Block is capable of generating a spatially-precise output by\n-maintaining high-resolution representations, while receiving rich contextual information\n-from low-resolutions. The MRB consists of multiple (three in this paper)\n-fully-convolutional streams connected in parallel. It allows information exchange across\n-parallel streams in order to consolidate the high-resolution features with the help of\n-low-resolution features, and vice versa. The MIRNet employs a recursive residual design\n-(with skip connections) to ease the flow of information during the learning process. In\n-order to maintain the residual nature of our architecture, residual resizing modules are\n-used to perform downsampling and upsampling operations that are used in the Multi-scale\n-Residual Block.\n-\n-![](https://i.imgur.com/wzZKV57.png)\n-\"\"\"\n-\n-# Recursive Residual Modules\n-\n-\n-def down_sampling_module(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"relu\"\n-    )(input_tensor)\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(main_branch)\n-    main_branch = layers.MaxPooling2D()(main_branch)\n-    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n-    skip_branch = layers.MaxPooling2D()(input_tensor)\n-    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n-    return layers.Add()([skip_branch, main_branch])\n-\n-\n-def up_sampling_module(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"relu\"\n-    )(input_tensor)\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(main_branch)\n-    main_branch = layers.UpSampling2D()(main_branch)\n-    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n-    skip_branch = layers.UpSampling2D()(input_tensor)\n-    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n-    return layers.Add()([skip_branch, main_branch])\n-\n-\n-# MRB Block\n-def multi_scale_residual_block(input_tensor, channels):\n-    # features\n-    level1 = input_tensor\n-    level2 = down_sampling_module(input_tensor)\n-    level3 = down_sampling_module(level2)\n-    # DAU\n-    level1_dau = dual_attention_unit_block(level1)\n-    level2_dau = dual_attention_unit_block(level2)\n-    level3_dau = dual_attention_unit_block(level3)\n-    # SKFF\n-    level1_skff = selective_kernel_feature_fusion(\n-        level1_dau,\n-        up_sampling_module(level2_dau),\n-        up_sampling_module(up_sampling_module(level3_dau)),\n-    )\n-    level2_skff = selective_kernel_feature_fusion(\n-        down_sampling_module(level1_dau),\n-        level2_dau,\n-        up_sampling_module(level3_dau),\n-    )\n-    level3_skff = selective_kernel_feature_fusion(\n-        down_sampling_module(down_sampling_module(level1_dau)),\n-        down_sampling_module(level2_dau),\n-        level3_dau,\n-    )\n-    # DAU 2\n-    level1_dau_2 = dual_attention_unit_block(level1_skff)\n-    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n-    level3_dau_2 = up_sampling_module(\n-        up_sampling_module(dual_attention_unit_block(level3_skff))\n-    )\n-    # SKFF 2\n-    skff_ = selective_kernel_feature_fusion(\n-        level1_dau_2, level2_dau_2, level3_dau_2\n-    )\n-    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n-    return layers.Add()([input_tensor, conv])\n-\n-\n-\"\"\"\n-### MIRNet Model\n-\"\"\"\n-\n-\n-def recursive_residual_group(input_tensor, num_mrb, channels):\n-    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n-        input_tensor\n-    )\n-    for _ in range(num_mrb):\n-        conv1 = multi_scale_residual_block(conv1, channels)\n-    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n-    return layers.Add()([conv2, input_tensor])\n-\n-\n-def mirnet_model(num_rrg, num_mrb, channels):\n-    input_tensor = keras.Input(shape=[None, None, 3])\n-    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n-        input_tensor\n-    )\n-    for _ in range(num_rrg):\n-        x1 = recursive_residual_group(x1, num_mrb, channels)\n-    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n-    output_tensor = layers.Add()([input_tensor, conv])\n-    return keras.Model(input_tensor, output_tensor)\n-\n-\n-model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n-\n-\"\"\"\n-## Training\n-\n-- We train MIRNet using **Charbonnier Loss** as the loss function and **Adam\n-Optimizer** with a learning rate of `1e-4`.\n-- We use **Peak Signal Noise Ratio** or PSNR as a metric which is an expression for the\n-ratio between the maximum possible value (power) of a signal and the power of distorting\n-noise that affects the quality of its representation.\n-\"\"\"\n-\n-\n-def charbonnier_loss(y_true, y_pred):\n-    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n-\n-\n-def peak_signal_noise_ratio(y_true, y_pred):\n-    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n-\n-\n-optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n-model.compile(\n-    optimizer=optimizer,\n-    loss=charbonnier_loss,\n-    metrics=[peak_signal_noise_ratio],\n-)\n-\n-history = model.fit(\n-    train_dataset,\n-    validation_data=val_dataset,\n-    epochs=50,\n-    callbacks=[\n-        keras.callbacks.ReduceLROnPlateau(\n-            monitor=\"val_peak_signal_noise_ratio\",\n-            factor=0.5,\n-            patience=5,\n-            verbose=1,\n-            min_delta=1e-7,\n-            mode=\"max\",\n-        )\n-    ],\n-)\n-\n-\n-def plot_history(value):\n-    plt.plot(history.history[value], label=f\"train_{value}\")\n-    plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(value)\n-    plt.title(f\"Train and Validation {value} Over Epochs\", fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_history(\"loss\")\n-plot_history(\"peak_signal_noise_ratio\")\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-\n-def plot_results(images, titles, figure_size=(12, 12)):\n-    fig = plt.figure(figsize=figure_size)\n-    for i in range(len(images)):\n-        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n-        _ = plt.imshow(images[i])\n-        plt.axis(\"off\")\n-    plt.show()\n-\n-\n-def infer(original_image):\n-    image = keras.utils.img_to_array(original_image)\n-    image = image.astype(\"float32\") / 255.0\n-    image = np.expand_dims(image, axis=0)\n-    output = model.predict(image, verbose=0)\n-    output_image = output[0] * 255.0\n-    output_image = output_image.clip(0, 255)\n-    output_image = output_image.reshape(\n-        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n-    )\n-    output_image = Image.fromarray(np.uint8(output_image))\n-    original_image = Image.fromarray(np.uint8(original_image))\n-    return output_image\n-\n-\n-\"\"\"\n-### Inference on Test Images\n-\n-We compare the test images from LOLDataset enhanced by MIRNet with images\n-enhanced via the `PIL.ImageOps.autocontrast()` function.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/lowlight-enhance-mirnet)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Enhance_Low_Light_Image).\n-\"\"\"\n-\n-\n-for low_light_image in random.sample(test_low_light_images, 6):\n-    original_image = Image.open(low_light_image)\n-    enhanced_image = infer(original_image)\n-    plot_results(\n-        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n-        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n-        (20, 12),\n-    )\n\n@@ -1,241 +0,0 @@\n-\"\"\"\n-Title: MixUp augmentation for image classification\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/03/06\n-Last modified: 2023/07/24\n-Description: Data augmentation using the mixup technique for image classification.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\"\"\"\n-\n-\"\"\"\n-_mixup_ is a *domain-agnostic* data augmentation technique proposed in [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n-by Zhang et al. It's implemented with the following formulas:\n-\n-![](https://i.ibb.co/DRyHYww/image.png)\n-\n-(Note that the lambda values are values with the [0, 1] range and are sampled from the\n-[Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).)\n-\n-The technique is quite systematically named. We are literally mixing up the features and\n-their corresponding labels. Implementation-wise it's simple. Neural networks are prone\n-to [memorizing corrupt labels](https://arxiv.org/abs/1611.03530). mixup relaxes this by\n-combining different features with one another (same happens for the labels too) so that\n-a network does not get overconfident about the relationship between the features and\n-their labels.\n-\n-mixup is specifically useful when we are not sure about selecting a set of augmentation\n-transforms for a given dataset, medical imaging datasets, for example. mixup can be\n-extended to a variety of data modalities such as computer vision, naturallanguage\n-processing, speech, and so on.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-import matplotlib.pyplot as plt\n-\n-from keras import layers\n-\n-# TF imports related to tf.data preprocessing\n-from tensorflow import data as tf_data\n-from tensorflow import image as tf_image\n-from tensorflow.random import gamma as tf_random_gamma\n-\n-\"\"\"\n-## Prepare the dataset\n-\n-In this example, we will be using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. But this same recipe can\n-be used for other classification datasets as well.\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n-\n-x_train = x_train.astype(\"float32\") / 255.0\n-x_train = np.reshape(x_train, (-1, 28, 28, 1))\n-y_train = keras.ops.one_hot(y_train, 10)\n-\n-x_test = x_test.astype(\"float32\") / 255.0\n-x_test = np.reshape(x_test, (-1, 28, 28, 1))\n-y_test = keras.ops.one_hot(y_test, 10)\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-AUTO = tf_data.AUTOTUNE\n-BATCH_SIZE = 64\n-EPOCHS = 10\n-\n-\"\"\"\n-## Convert the data into TensorFlow `Dataset` objects\n-\"\"\"\n-\n-# Put aside a few samples to create our validation set\n-val_samples = 2000\n-x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n-new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n-\n-train_ds_one = (\n-    tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n-    .shuffle(BATCH_SIZE * 100)\n-    .batch(BATCH_SIZE)\n-)\n-train_ds_two = (\n-    tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n-    .shuffle(BATCH_SIZE * 100)\n-    .batch(BATCH_SIZE)\n-)\n-# Because we will be mixing up the images and their corresponding labels, we will be\n-# combining two shuffled datasets from the same training data.\n-train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))\n-\n-val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n-\n-test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n-\n-\"\"\"\n-## Define the mixup technique function\n-\n-To perform the mixup routine, we create new virtual datasets using the training data from\n-the same dataset, and apply a lambda value within the [0, 1] range sampled from a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n-— such that, for example, `new_x = lambda * x1 + (1 - lambda) * x2` (where\n-`x1` and `x2` are images) and the same equation is applied to the labels as well.\n-\"\"\"\n-\n-\n-def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n-    gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)\n-    gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)\n-    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n-\n-\n-def mix_up(ds_one, ds_two, alpha=0.2):\n-    # Unpack two datasets\n-    images_one, labels_one = ds_one\n-    images_two, labels_two = ds_two\n-    batch_size = keras.backend.shape(images_one)[0]\n-\n-    # Sample lambda and reshape it to do the mixup\n-    l = sample_beta_distribution(batch_size, alpha, alpha)\n-    x_l = keras.ops.reshape(l, (batch_size, 1, 1, 1))\n-    y_l = keras.ops.reshape(l, (batch_size, 1))\n-\n-    # Perform mixup on both images and labels by combining a pair of images/labels\n-    # (one from each dataset) into one image/label\n-    images = images_one * x_l + images_two * (1 - x_l)\n-    labels = labels_one * y_l + labels_two * (1 - y_l)\n-    return (images, labels)\n-\n-\n-\"\"\"\n-**Note** that here , we are combining two images to create a single one. Theoretically,\n-we can combine as many we want but that comes at an increased computation cost. In\n-certain cases, it may not help improve the performance as well.\n-\"\"\"\n-\n-\"\"\"\n-## Visualize the new augmented dataset\n-\"\"\"\n-\n-# First create the new dataset using our `mix_up` utility\n-train_ds_mu = train_ds.map(\n-    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2),\n-    num_parallel_calls=AUTO,\n-)\n-\n-# Let's preview 9 samples from the dataset\n-sample_images, sample_labels = next(iter(train_ds_mu))\n-plt.figure(figsize=(10, 10))\n-for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n-    ax = plt.subplot(3, 3, i + 1)\n-    plt.imshow(image.numpy().squeeze())\n-    print(label.numpy().tolist())\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Model building\n-\"\"\"\n-\n-\n-def get_training_model():\n-    model = keras.Sequential(\n-        [\n-            layers.Conv2D(\n-                16, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)\n-            ),\n-            layers.MaxPooling2D(pool_size=(2, 2)),\n-            layers.Conv2D(32, (5, 5), activation=\"relu\"),\n-            layers.MaxPooling2D(pool_size=(2, 2)),\n-            layers.Dropout(0.2),\n-            layers.GlobalAveragePooling2D(),\n-            layers.Dense(128, activation=\"relu\"),\n-            layers.Dense(10, activation=\"softmax\"),\n-        ]\n-    )\n-    return model\n-\n-\n-\"\"\"\n-For the sake of reproducibility, we serialize the initial random weights of our shallow\n-network.\n-\"\"\"\n-\n-initial_model = get_training_model()\n-initial_model.save_weights(\"initial_weights.weights.h5\")\n-\n-\"\"\"\n-## 1. Train the model with the mixed up dataset\n-\"\"\"\n-\n-model = get_training_model()\n-model.load_weights(\"initial_weights.weights.h5\")\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-model.fit(train_ds_mu, validation_data=val_ds, epochs=EPOCHS)\n-_, test_acc = model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n-\n-\"\"\"\n-## 2. Train the model *without* the mixed up dataset\n-\"\"\"\n-\n-model = get_training_model()\n-model.load_weights(\"initial_weights.weights.h5\")\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-# Notice that we are NOT using the mixed up dataset here\n-model.fit(train_ds_one, validation_data=val_ds, epochs=EPOCHS)\n-_, test_acc = model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n-\n-\"\"\"\n-Readers are encouraged to try out mixup on different datasets from different domains and\n-experiment with the lambda parameter. You are strongly advised to check out the\n-[original paper](https://arxiv.org/abs/1710.09412) as well - the authors present several ablation studies on mixup\n-showing how it can improve generalization, as well as show their results of combining\n-more than two images to create a single one.\n-\"\"\"\n-\n-\"\"\"\n-## Notes\n-\n-* With mixup, you can create synthetic examples — especially when you lack a large\n-dataset - without incurring high computational costs.\n-* [Label smoothing](https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/) and mixup usually do not work well together because label smoothing\n-already modifies the hard labels by some factor.\n-* mixup does not work well when you are using [Supervised Contrastive\n-Learning](https://arxiv.org/abs/2004.11362) (SCL) since SCL expects the true labels\n-during its pre-training phase.\n-* A few other benefits of mixup include (as described in the [paper](https://arxiv.org/abs/1710.09412)) robustness to\n-adversarial examples and stabilized GAN (Generative Adversarial Networks) training.\n-* There are a number of data augmentation techniques that extend mixup such as\n-[CutMix](https://arxiv.org/abs/1905.04899) and [AugMix](https://arxiv.org/abs/1912.02781).\n-\"\"\"\n\n@@ -1,373 +0,0 @@\n-\"\"\"\n-Title: MobileViT: A mobile-friendly Transformer-based model for image classification\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/10/20\n-Last modified: 2023/11/23\n-Description: MobileViT for image classification with combined benefits of convolutions and Transformers.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Pavan Kumar Singh](https://github.com/pksX01)\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In this example, we implement the MobileViT architecture\n-([Mehta et al.](https://arxiv.org/abs/2110.02178)),\n-which combines the benefits of Transformers\n-([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n-and convolutions. With Transformers, we can capture long-range dependencies that result\n-in global representations. With convolutions, we can capture spatial relationships that\n-model locality.\n-\n-Besides combining the properties of Transformers and convolutions, the authors introduce\n-MobileViT as a general-purpose mobile-friendly backbone for different image recognition\n-tasks. Their findings suggest that, performance-wise, MobileViT is better than other\n-models with the same or higher complexity ([MobileNetV3](https://arxiv.org/abs/1905.02244),\n-for example), while being efficient on mobile devices.\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-import os\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import tensorflow as tf\n-\n-from keras import layers\n-import keras as keras\n-\n-import tensorflow_datasets as tfds\n-\n-tfds.disable_progress_bar()\n-\n-\"\"\"\n-## Hyperparameters\n-\"\"\"\n-\n-# Values are from table 4.\n-patch_size = 4  # 2x2, for the Transformer blocks.\n-image_size = 256\n-expansion_factor = 2  # expansion factor for the MobileNetV2 blocks.\n-\n-\"\"\"\n-## MobileViT utilities\n-\n-The MobileViT architecture is comprised of the following blocks:\n-\n-* Strided 3x3 convolutions that process the input image.\n-* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n-downsampling the resolution of the intermediate feature maps.\n-* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n-presented in the figure below (taken from the\n-[original paper](https://arxiv.org/abs/2110.02178)):\n-\n-\n-![](https://i.imgur.com/mANnhI7.png)\n-\"\"\"\n-\n-\n-def conv_block(x, filters=16, kernel_size=3, strides=2):\n-    conv_layer = layers.Conv2D(\n-        filters, kernel_size, strides=strides, activation=keras.activations.swish, padding=\"same\"\n-    )\n-    return conv_layer(x)\n-\n-\n-# Reference: https://git.io/JKgtC\n-\n-\n-def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n-    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n-    m = layers.BatchNormalization()(m)\n-    m = keras.activations.swish(m)\n-\n-    if strides == 2:\n-        m = layers.ZeroPadding2D()(m)\n-    m = layers.DepthwiseConv2D(\n-        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n-    )(m)\n-    m = layers.BatchNormalization()(m)\n-    m = keras.activations.swish(m)\n-\n-    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n-    m = layers.BatchNormalization()(m)\n-\n-    if x.shape[-1] == output_channels and strides == 1:\n-        return layers.Add()([m, x])\n-    return m\n-\n-\n-# Reference:\n-# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n-\n-\n-def mlp(x, hidden_units, dropout_rate):\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=keras.activations.swish)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n-    for _ in range(transformer_layers):\n-        # Layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-        # Skip connection 1.\n-        x2 = layers.Add()([attention_output, x])\n-        # Layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n-        # MLP.\n-        x3 = mlp(\n-            x3,\n-            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n-            dropout_rate=0.1,\n-        )\n-        # Skip connection 2.\n-        x = layers.Add()([x3, x2])\n-\n-    return x\n-\n-\n-def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n-    # Local projection with convolutions.\n-    local_features = conv_block(x, filters=projection_dim, strides=strides)\n-    local_features = conv_block(\n-        local_features, filters=projection_dim, kernel_size=1, strides=strides\n-    )\n-\n-    # Unfold into patches and then pass through Transformers.\n-    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n-    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n-        local_features\n-    )\n-    global_features = transformer_block(\n-        non_overlapping_patches, num_blocks, projection_dim\n-    )\n-\n-    # Fold into conv-like feature-maps.\n-    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n-        global_features\n-    )\n-\n-    # Apply point-wise conv -> concatenate with the input features.\n-    folded_feature_map = conv_block(\n-        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n-    )\n-    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n-\n-    # Fuse the local and global features using a convoluion layer.\n-    local_global_features = conv_block(\n-        local_global_features, filters=projection_dim, strides=strides\n-    )\n-\n-    return local_global_features\n-\n-\n-\"\"\"\n-**More on the MobileViT block**:\n-\n-* First, the feature representations (A) go through convolution blocks that capture local\n-relationships. The expected shape of a single entry here would be `(h, w, num_channels)`.\n-* Then they get unfolded into another vector with shape `(p, n, num_channels)`,\n-where `p` is the area of a small patch, and `n` is `(h * w) / p`. So, we end up with `n`\n-non-overlapping patches.\n-* This unfolded vector is then passed through a Tranformer block that captures global\n-relationships between the patches.\n-* The output vector (B) is again folded into a vector of shape `(h, w, num_channels)`\n-resembling a feature map coming out of convolutions.\n-\n-Vectors A and B are then passed through two more convolutional layers to fuse the local\n-and global representations. Notice how the spatial resolution of the final vector remains\n-unchanged at this point. The authors also present an explanation of how the MobileViT\n-block resembles a convolution block of a CNN. For more details, please refer to the\n-original paper.\n-\"\"\"\n-\n-\"\"\"\n-Next, we combine these blocks together and implement the MobileViT architecture (XXS\n-variant). The following figure (taken from the original paper) presents a schematic\n-representation of the architecture:\n-\n-![](https://i.ibb.co/sRbVRBN/image.png)\n-\"\"\"\n-\n-\n-def create_mobilevit(num_classes=5):\n-    inputs = keras.Input((image_size, image_size, 3))\n-    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n-\n-    # Initial conv-stem -> MV2 block.\n-    x = conv_block(x, filters=16)\n-    x = inverted_residual_block(\n-        x, expanded_channels=16 * expansion_factor, output_channels=16\n-    )\n-\n-    # Downsampling with MV2 block.\n-    x = inverted_residual_block(\n-        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n-    )\n-    x = inverted_residual_block(\n-        x, expanded_channels=24 * expansion_factor, output_channels=24\n-    )\n-    x = inverted_residual_block(\n-        x, expanded_channels=24 * expansion_factor, output_channels=24\n-    )\n-\n-    # First MV2 -> MobileViT block.\n-    x = inverted_residual_block(\n-        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n-    )\n-    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n-\n-    # Second MV2 -> MobileViT block.\n-    x = inverted_residual_block(\n-        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n-    )\n-    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n-\n-    # Third MV2 -> MobileViT block.\n-    x = inverted_residual_block(\n-        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n-    )\n-    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n-    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n-\n-    # Classification head.\n-    x = layers.GlobalAvgPool2D()(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n-\n-    return keras.Model(inputs, outputs)\n-\n-\n-mobilevit_xxs = create_mobilevit()\n-mobilevit_xxs.summary()\n-\n-\"\"\"\n-## Dataset preparation\n-\n-We will be using the\n-[`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers)\n-dataset to demonstrate the model. Unlike other Transformer-based architectures,\n-MobileViT uses a simple augmentation pipeline primarily because it has the properties\n-of a CNN.\n-\"\"\"\n-\n-batch_size = 64\n-auto = tf.data.AUTOTUNE\n-resize_bigger = 280\n-num_classes = 5\n-\n-\n-def preprocess_dataset(is_training=True):\n-    def _pp(image, label):\n-        if is_training:\n-            # Resize to a bigger spatial resolution and take the random\n-            # crops.\n-            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n-            image = tf.image.random_crop(image, (image_size, image_size, 3))\n-            image = tf.image.random_flip_left_right(image)\n-        else:\n-            image = tf.image.resize(image, (image_size, image_size))\n-        label = tf.one_hot(label, depth=num_classes)\n-        return image, label\n-\n-    return _pp\n-\n-\n-def prepare_dataset(dataset, is_training=True):\n-    if is_training:\n-        dataset = dataset.shuffle(batch_size * 10)\n-    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)\n-    return dataset.batch(batch_size).prefetch(auto)\n-\n-\n-\"\"\"\n-The authors use a multi-scale data sampler to help the model learn representations of\n-varied scales. In this example, we discard this part.\n-\"\"\"\n-\n-\"\"\"\n-## Load and prepare the dataset\n-\"\"\"\n-\n-train_dataset, val_dataset = tfds.load(\n-    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n-)\n-\n-num_train = train_dataset.cardinality()\n-num_val = val_dataset.cardinality()\n-print(f\"Number of training examples: {num_train}\")\n-print(f\"Number of validation examples: {num_val}\")\n-\n-train_dataset = prepare_dataset(train_dataset, is_training=True)\n-val_dataset = prepare_dataset(val_dataset, is_training=False)\n-\n-\"\"\"\n-## Train a MobileViT (XXS) model\n-\"\"\"\n-\n-learning_rate = 0.002\n-label_smoothing_factor = 0.1\n-epochs = 30\n-\n-optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n-loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n-\n-\n-def run_experiment(epochs=epochs):\n-    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n-    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n-\n-    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    mobilevit_xxs.fit(\n-        train_dataset,\n-        validation_data=val_dataset,\n-        epochs=epochs,\n-        callbacks=[checkpoint_callback],\n-    )\n-    mobilevit_xxs.load_weights(checkpoint_filepath)\n-    _, accuracy = mobilevit_xxs.evaluate(val_dataset)\n-    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n-    return mobilevit_xxs\n-\n-\n-mobilevit_xxs = run_experiment()\n-\n-\"\"\"\n-## Results and TFLite conversion\n-\n-With about one million parameters, getting to ~85% top-1 accuracy on 256x256 resolution is\n-a strong result. This MobileViT mobile is fully compatible with TensorFlow Lite (TFLite)\n-and can be converted with the following code:\n-\"\"\"\n-\n-# Serialize the model as a SavedModel.\n-tf.saved_model.save(mobilevit_xxs, \"mobilevit_xxs\")\n-\n-# Convert to TFLite. This form of quantization is called\n-# post-training dynamic-range quantization in TFLite.\n-converter = tf.lite.TFLiteConverter.from_saved_model(\"mobilevit_xxs\")\n-converter.optimizations = [tf.lite.Optimize.DEFAULT]\n-converter.target_spec.supported_ops = [\n-    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n-    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n-]\n-tflite_model = converter.convert()\n-open(\"mobilevit_xxs.tflite\", \"wb\").write(tflite_model)\n-\n-\"\"\"\n-To learn more about different quantization recipes available in TFLite and running\n-inference with TFLite models, check out\n-[this official resource](https://www.tensorflow.org/lite/performance/post_training_quantization).\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,488 +0,0 @@\n-\"\"\"\n-Title: Image classification with Perceiver\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2021/04/30\n-Last modified: 2021/01/30\n-Description: Implementing the Perceiver model for image classification.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example implements the\n-[Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)\n-model by Andrew Jaegle et al. for image classification,\n-and demonstrates it on the CIFAR-100 dataset.\n-\n-The Perceiver model leverages an asymmetric attention mechanism to iteratively\n-distill inputs into a tight latent bottleneck,\n-allowing it to scale to handle very large inputs.\n-\n-In other words: let's assume that your input data array (e.g. image) has `M` elements (i.e. patches), where `M` is large.\n-In a standard Transformer model, a self-attention operation is performed for the `M` elements.\n-The complexity of this operation is `O(M^2)`.\n-However, the Perceiver model creates a latent array of size `N` elements, where `N << M`,\n-and performs two operations iteratively:\n-\n-1. Cross-attention Transformer between the latent array and the data array - The complexity of this operation is `O(M.N)`.\n-2. Self-attention Transformer on the latent array -  The complexity of this operation is `O(N^2)`.\n-\n-This example requires TensorFlow 2.4 or higher, as well as\n-[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n-which can be installed using the following command:\n-\n-```python\n-pip install -U tensorflow-addons\n-```\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\"\"\"\n-\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 64\n-num_epochs = 50\n-dropout_rate = 0.2\n-image_size = 64  # We'll resize input images to this size.\n-patch_size = 2  # Size of the patches to be extract from the input images.\n-num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n-latent_dim = 256  # Size of the latent array.\n-projection_dim = (\n-    256  # Embedding size of each element in the data and latent arrays.\n-)\n-num_heads = 8  # Number of Transformer heads.\n-ffn_units = [\n-    projection_dim,\n-    projection_dim,\n-]  # Size of the Transformer Feedforward network.\n-num_transformer_blocks = 4\n-num_iterations = (\n-    2  # Repetitions of the cross-attention and Transformer modules.\n-)\n-classifier_units = [\n-    projection_dim,\n-    num_classes,\n-]  # Size of the Feedforward network of the final classifier.\n-\n-print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n-print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n-print(f\"Patches per image: {num_patches}\")\n-print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n-print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n-print(f\"Data array shape: {num_patches} X {projection_dim}\")\n-\n-\"\"\"\n-Note that, in order to use each pixel as an individual input in the data array,\n-set `patch_size` to 1.\n-\"\"\"\n-\n-\"\"\"\n-## Use data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.Resizing(image_size, image_size),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-# Compute the mean and the variance of the training data for normalization.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\"\"\"\n-## Implement Feedforward network (FFN)\n-\"\"\"\n-\n-\n-def create_ffn(hidden_units, dropout_rate):\n-    ffn_layers = []\n-    for units in hidden_units[:-1]:\n-        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n-\n-    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n-    ffn_layers.append(layers.Dropout(dropout_rate))\n-\n-    ffn = keras.Sequential(ffn_layers)\n-    return ffn\n-\n-\n-\"\"\"\n-## Implement patch creation as a layer\n-\"\"\"\n-\n-\n-class Patches(layers.Layer):\n-    def __init__(self, patch_size):\n-        super().__init__()\n-        self.patch_size = patch_size\n-\n-    def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=[1, self.patch_size, self.patch_size, 1],\n-            strides=[1, self.patch_size, self.patch_size, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"VALID\",\n-        )\n-        patch_dims = patches.shape[-1]\n-        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n-        return patches\n-\n-\n-\"\"\"\n-## Implement the patch encoding layer\n-\n-The `PatchEncoder` layer will linearly transform a patch by projecting it into\n-a vector of size `latent_dim`. In addition, it adds a learnable position embedding\n-to the projected vector.\n-\n-Note that the orginal Perceiver paper uses the Fourier feature positional encodings.\n-\"\"\"\n-\n-\n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.projection = layers.Dense(units=projection_dim)\n-        self.position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=projection_dim\n-        )\n-\n-    def call(self, patches):\n-        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n-        encoded = self.projection(patches) + self.position_embedding(positions)\n-        return encoded\n-\n-\n-\"\"\"\n-## Build the Perceiver model\n-\n-The Perceiver consists of two modules: a cross-attention\n-module and a standard Transformer with self-attention.\n-\"\"\"\n-\n-\"\"\"\n-### Cross-attention module\n-\n-The cross-attention expects a `(latent_dim, projection_dim)` latent array,\n-and the `(data_dim,  projection_dim)` data array as inputs,\n-to produce a `(latent_dim, projection_dim)` latent array as an output.\n-To apply cross-attention, the `query` vectors are generated from the latent array,\n-while the `key` and `value` vectors are generated from the encoded image.\n-\n-Note that the data array in this example is the image,\n-where the `data_dim` is set to the `num_patches`.\n-\"\"\"\n-\n-\n-def create_cross_attention_module(\n-    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n-):\n-    inputs = {\n-        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n-        \"latent_array\": layers.Input(\n-            shape=(latent_dim, projection_dim), name=\"latent_array\"\n-        ),\n-        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n-        \"data_array\": layers.Input(\n-            shape=(data_dim, projection_dim), name=\"data_array\"\n-        ),\n-    }\n-\n-    # Apply layer norm to the inputs\n-    latent_array = layers.LayerNormalization(epsilon=1e-6)(\n-        inputs[\"latent_array\"]\n-    )\n-    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n-\n-    # Create query tensor: [1, latent_dim, projection_dim].\n-    query = layers.Dense(units=projection_dim)(latent_array)\n-    # Create key tensor: [batch_size, data_dim, projection_dim].\n-    key = layers.Dense(units=projection_dim)(data_array)\n-    # Create value tensor: [batch_size, data_dim, projection_dim].\n-    value = layers.Dense(units=projection_dim)(data_array)\n-\n-    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n-    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n-        [query, key, value], return_attention_scores=False\n-    )\n-    # Skip connection 1.\n-    attention_output = layers.Add()([attention_output, latent_array])\n-\n-    # Apply layer norm.\n-    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n-    # Apply Feedforward network.\n-    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n-    outputs = ffn(attention_output)\n-    # Skip connection 2.\n-    outputs = layers.Add()([outputs, attention_output])\n-\n-    # Create the Keras model.\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-\"\"\"\n-### Transformer module\n-\n-The Transformer expects the output latent vector from the cross-attention module\n-as an input, applies multi-head self-attention to its `latent_dim` elements,\n-followed by feedforward network, to produce another `(latent_dim, projection_dim)` latent array.\n-\"\"\"\n-\n-\n-def create_transformer_module(\n-    latent_dim,\n-    projection_dim,\n-    num_heads,\n-    num_transformer_blocks,\n-    ffn_units,\n-    dropout_rate,\n-):\n-    # input_shape: [1, latent_dim, projection_dim]\n-    inputs = layers.Input(shape=(latent_dim, projection_dim))\n-\n-    x0 = inputs\n-    # Create multiple layers of the Transformer block.\n-    for _ in range(num_transformer_blocks):\n-        # Apply layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n-        # Create a multi-head self-attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-        # Skip connection 1.\n-        x2 = layers.Add()([attention_output, x0])\n-        # Apply layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n-        # Apply Feedforward network.\n-        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n-        x3 = ffn(x3)\n-        # Skip connection 2.\n-        x0 = layers.Add()([x3, x2])\n-\n-    # Create the Keras model.\n-    model = keras.Model(inputs=inputs, outputs=x0)\n-    return model\n-\n-\n-\"\"\"\n-### Perceiver model\n-\n-The Perceiver model repeats the cross-attention and Transformer modules\n-`num_iterations` times—with shared weights and skip connections—to allow\n-the latent array to iteratively extract information from the input image as it is needed.\n-\"\"\"\n-\n-\n-class Perceiver(keras.Model):\n-    def __init__(\n-        self,\n-        patch_size,\n-        data_dim,\n-        latent_dim,\n-        projection_dim,\n-        num_heads,\n-        num_transformer_blocks,\n-        ffn_units,\n-        dropout_rate,\n-        num_iterations,\n-        classifier_units,\n-    ):\n-        super().__init__()\n-\n-        self.latent_dim = latent_dim\n-        self.data_dim = data_dim\n-        self.patch_size = patch_size\n-        self.projection_dim = projection_dim\n-        self.num_heads = num_heads\n-        self.num_transformer_blocks = num_transformer_blocks\n-        self.ffn_units = ffn_units\n-        self.dropout_rate = dropout_rate\n-        self.num_iterations = num_iterations\n-        self.classifier_units = classifier_units\n-\n-    def build(self, input_shape):\n-        # Create latent array.\n-        self.latent_array = self.add_weight(\n-            shape=(self.latent_dim, self.projection_dim),\n-            initializer=\"random_normal\",\n-            trainable=True,\n-        )\n-\n-        # Create patching module.\n-        self.patcher = Patches(self.patch_size)\n-\n-        # Create patch encoder.\n-        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n-\n-        # Create cross-attenion module.\n-        self.cross_attention = create_cross_attention_module(\n-            self.latent_dim,\n-            self.data_dim,\n-            self.projection_dim,\n-            self.ffn_units,\n-            self.dropout_rate,\n-        )\n-\n-        # Create Transformer module.\n-        self.transformer = create_transformer_module(\n-            self.latent_dim,\n-            self.projection_dim,\n-            self.num_heads,\n-            self.num_transformer_blocks,\n-            self.ffn_units,\n-            self.dropout_rate,\n-        )\n-\n-        # Create global average pooling layer.\n-        self.global_average_pooling = layers.GlobalAveragePooling1D()\n-\n-        # Create a classification head.\n-        self.classification_head = create_ffn(\n-            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n-        )\n-\n-        super().build(input_shape)\n-\n-    def call(self, inputs):\n-        # Augment data.\n-        augmented = data_augmentation(inputs)\n-        # Create patches.\n-        patches = self.patcher(augmented)\n-        # Encode patches.\n-        encoded_patches = self.patch_encoder(patches)\n-        # Prepare cross-attention inputs.\n-        cross_attention_inputs = {\n-            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n-            \"data_array\": encoded_patches,\n-        }\n-        # Apply the cross-attention and the Transformer modules iteratively.\n-        for _ in range(self.num_iterations):\n-            # Apply cross-attention from the latent array to the data array.\n-            latent_array = self.cross_attention(cross_attention_inputs)\n-            # Apply self-attention Transformer to the latent array.\n-            latent_array = self.transformer(latent_array)\n-            # Set the latent array of the next iteration.\n-            cross_attention_inputs[\"latent_array\"] = latent_array\n-\n-        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n-        representation = self.global_average_pooling(latent_array)\n-        # Generate logits.\n-        logits = self.classification_head(representation)\n-        return logits\n-\n-\n-\"\"\"\n-## Compile, train, and evaluate the mode\n-\"\"\"\n-\n-\n-def run_experiment(model):\n-    # Create Adam optimizer with weight decay.\n-    optimizer = keras.optimizers.Adam(\n-        learning_rate=learning_rate,\n-        weight_decay=weight_decay,\n-    )\n-\n-    # Compile the model.\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-        metrics=[\n-            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n-            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n-        ],\n-    )\n-\n-    # Create a learning rate scheduler callback.\n-    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n-        monitor=\"val_loss\", factor=0.2, patience=3\n-    )\n-\n-    # Create an early stopping callback.\n-    early_stopping = keras.callbacks.EarlyStopping(\n-        monitor=\"val_loss\", patience=15, restore_best_weights=True\n-    )\n-\n-    # Fit the model.\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[early_stopping, reduce_lr],\n-    )\n-\n-    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-    # Return history to plot learning curves.\n-    return history\n-\n-\n-\"\"\"\n-Note that training the perceiver model with the current settings on a V100 GPUs takes\n-around 200 seconds.\n-\"\"\"\n-\n-perceiver_classifier = Perceiver(\n-    patch_size,\n-    num_patches,\n-    latent_dim,\n-    projection_dim,\n-    num_heads,\n-    num_transformer_blocks,\n-    ffn_units,\n-    dropout_rate,\n-    num_iterations,\n-    classifier_units,\n-)\n-\n-\n-history = run_experiment(perceiver_classifier)\n-\n-\"\"\"\n-After 40 epochs, the Perceiver model achieves around 53% accuracy and 81% top-5 accuracy on the test data.\n-\n-As mentioned in the ablations of the [Perceiver](https://arxiv.org/abs/2103.03206) paper,\n-you can obtain better results by increasing the latent array size,\n-increasing the (projection) dimensions of the latent array and data array elements,\n-increasing the number of blocks in the Transformer module, and increasing the number of iterations of applying\n-the cross-attention and the latent Transformer modules. You may also try to increase the size the input images\n-and use different patch sizes.\n-\n-The Perceiver benefits from inceasing the model size. However, larger models needs bigger accelerators\n-to fit in and train efficiently. This is why in the Perceiver paper they used 32 TPU cores to run the experiments.\n-\"\"\"\n\n@@ -1,301 +0,0 @@\n-\"\"\"\n-Title: Few-Shot learning with Reptile\n-Author: [ADMoreau](https://github.com/ADMoreau)\n-Date created: 2020/05/21\n-Last modified: 2023/07/20\n-Description: Few-shot classification on the Omniglot dataset using Reptile.\n-Accelerator: GPU\n-Converted to Keras 3 By: [Muhammad Anas Raza](https://anasrz.com)\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The [Reptile](https://arxiv.org/abs/1803.02999) algorithm was developed by OpenAI to\n-perform model-agnostic meta-learning. Specifically, this algorithm was designed to\n-quickly learn to perform new tasks with minimal training (few-shot learning).\n-The algorithm works by performing Stochastic Gradient Descent using the\n-difference between weights trained on a mini-batch of never-seen-before data and the\n-model weights prior to training over a fixed number of meta-iterations.\n-\"\"\"\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-from keras import layers\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import random\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-\n-\"\"\"\n-## Define the Hyperparameters\n-\"\"\"\n-\n-learning_rate = 0.003\n-meta_step_size = 0.25\n-\n-inner_batch_size = 25\n-eval_batch_size = 25\n-\n-meta_iters = 2000\n-eval_iters = 5\n-inner_iters = 4\n-\n-eval_interval = 1\n-train_shots = 20\n-shots = 5\n-classes = 5\n-\n-\"\"\"\n-## Prepare the data\n-\n-The [Omniglot dataset](https://github.com/brendenlake/omniglot/) is a dataset of 1,623\n-characters taken from 50 different alphabets, with 20 examples for each character.\n-The 20 samples for each character were drawn online via Amazon's Mechanical Turk. For the\n-few-shot learning task, `k` samples (or \"shots\") are drawn randomly from `n` randomly-chosen\n-classes. These `n` numerical values are used to create a new set of temporary labels to use\n-to test the model's ability to learn a new task given few examples. In other words, if you\n-are training on 5 classes, your new class labels will be either 0, 1, 2, 3, or 4.\n-Omniglot is a great dataset for this task since there are many different classes to draw\n-from, with a reasonable number of samples for each class.\n-\"\"\"\n-\n-\n-class Dataset:\n-    # This class will facilitate the creation of a few-shot dataset\n-    # from the Omniglot dataset that can be sampled from quickly while also\n-    # allowing to create new labels at the same time.\n-    def __init__(self, training):\n-        # Download the tfrecord files containing the omniglot data and convert to a\n-        # dataset.\n-        split = \"train\" if training else \"test\"\n-        ds = tfds.load(\n-            \"omniglot\", split=split, as_supervised=True, shuffle_files=False\n-        )\n-        # Iterate over the dataset to get each individual image and its class,\n-        # and put that data into a dictionary.\n-        self.data = {}\n-\n-        def extraction(image, label):\n-            # This function will shrink the Omniglot images to the desired size,\n-            # scale pixel values and convert the RGB image to grayscale\n-            image = tf.image.convert_image_dtype(image, tf.float32)\n-            image = tf.image.rgb_to_grayscale(image)\n-            image = tf.image.resize(image, [28, 28])\n-            return image, label\n-\n-        for image, label in ds.map(extraction):\n-            image = image.numpy()\n-            label = str(label.numpy())\n-            if label not in self.data:\n-                self.data[label] = []\n-            self.data[label].append(image)\n-        self.labels = list(self.data.keys())\n-\n-    def get_mini_dataset(\n-        self, batch_size, repetitions, shots, num_classes, split=False\n-    ):\n-        temp_labels = np.zeros(shape=(num_classes * shots))\n-        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n-        if split:\n-            test_labels = np.zeros(shape=(num_classes))\n-            test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n-\n-        # Get a random subset of labels from the entire label set.\n-        label_subset = random.choices(self.labels, k=num_classes)\n-        for class_idx, class_obj in enumerate(label_subset):\n-            # Use enumerated index value as a temporary label for mini-batch in\n-            # few shot learning.\n-            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n-            # If creating a split dataset for testing, select an extra sample from each\n-            # label to create the test dataset.\n-            if split:\n-                test_labels[class_idx] = class_idx\n-                images_to_split = random.choices(\n-                    self.data[label_subset[class_idx]], k=shots + 1\n-                )\n-                test_images[class_idx] = images_to_split[-1]\n-                temp_images[\n-                    class_idx * shots : (class_idx + 1) * shots\n-                ] = images_to_split[:-1]\n-            else:\n-                # For each index in the randomly selected label_subset, sample the\n-                # necessary number of images.\n-                temp_images[\n-                    class_idx * shots : (class_idx + 1) * shots\n-                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n-\n-        dataset = tf.data.Dataset.from_tensor_slices(\n-            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n-        )\n-        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n-        if split:\n-            return dataset, test_images, test_labels\n-        return dataset\n-\n-\n-import urllib3\n-\n-urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n-train_dataset = Dataset(training=True)\n-test_dataset = Dataset(training=False)\n-\n-\"\"\"\n-## Visualize some examples from the dataset\n-\"\"\"\n-\n-_, axarr = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n-\n-sample_keys = list(train_dataset.data.keys())\n-\n-for a in range(5):\n-    for b in range(5):\n-        temp_image = train_dataset.data[sample_keys[a]][b]\n-        temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n-        temp_image *= 255\n-        temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n-        if b == 2:\n-            axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n-        axarr[a, b].imshow(temp_image, cmap=\"gray\")\n-        axarr[a, b].xaxis.set_visible(False)\n-        axarr[a, b].yaxis.set_visible(False)\n-plt.show()\n-\n-\"\"\"\n-## Build the model\n-\"\"\"\n-\n-\n-def conv_bn(x):\n-    x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\n-    x = layers.BatchNormalization()(x)\n-    return layers.ReLU()(x)\n-\n-\n-inputs = layers.Input(shape=(28, 28, 1))\n-x = conv_bn(inputs)\n-x = conv_bn(x)\n-x = conv_bn(x)\n-x = conv_bn(x)\n-x = layers.Flatten()(x)\n-outputs = layers.Dense(classes, activation=\"softmax\")(x)\n-model = keras.Model(inputs=inputs, outputs=outputs)\n-model.compile()\n-optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-training = []\n-testing = []\n-for meta_iter in range(meta_iters):\n-    frac_done = meta_iter / meta_iters\n-    cur_meta_step_size = (1 - frac_done) * meta_step_size\n-    # Temporarily save the weights from the model.\n-    old_vars = model.get_weights()\n-    # Get a sample from the full dataset.\n-    mini_dataset = train_dataset.get_mini_dataset(\n-        inner_batch_size, inner_iters, train_shots, classes\n-    )\n-    for images, labels in mini_dataset:\n-        with tf.GradientTape() as tape:\n-            preds = model(images)\n-            loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n-        grads = tape.gradient(loss, model.trainable_weights)\n-        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n-    new_vars = model.get_weights()\n-    # Perform SGD for the meta step.\n-    for var in range(len(new_vars)):\n-        new_vars[var] = old_vars[var] + (\n-            (new_vars[var] - old_vars[var]) * cur_meta_step_size\n-        )\n-    # After the meta-learning step, reload the newly-trained weights into the model.\n-    model.set_weights(new_vars)\n-    # Evaluation loop\n-    if meta_iter % eval_interval == 0:\n-        accuracies = []\n-        for dataset in (train_dataset, test_dataset):\n-            # Sample a mini dataset from the full dataset.\n-            train_set, test_images, test_labels = dataset.get_mini_dataset(\n-                eval_batch_size, eval_iters, shots, classes, split=True\n-            )\n-            old_vars = model.get_weights()\n-            # Train on the samples and get the resulting accuracies.\n-            for images, labels in train_set:\n-                with tf.GradientTape() as tape:\n-                    preds = model(images)\n-                    loss = keras.losses.sparse_categorical_crossentropy(\n-                        labels, preds\n-                    )\n-                grads = tape.gradient(loss, model.trainable_weights)\n-                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n-            test_preds = model.predict(test_images)\n-            test_preds = tf.argmax(test_preds).numpy()\n-            num_correct = (test_preds == test_labels).sum()\n-            # Reset the weights after getting the evaluation accuracies.\n-            model.set_weights(old_vars)\n-            accuracies.append(num_correct / classes)\n-        training.append(accuracies[0])\n-        testing.append(accuracies[1])\n-        if meta_iter % 100 == 0:\n-            print(\n-                \"batch %d: train=%f test=%f\"\n-                % (meta_iter, accuracies[0], accuracies[1])\n-            )\n-\n-\"\"\"\n-## Visualize Results\n-\"\"\"\n-\n-# First, some preprocessing to smooth the training and testing arrays for display.\n-window_length = 100\n-train_s = np.r_[\n-    training[window_length - 1 : 0 : -1],\n-    training,\n-    training[-1:-window_length:-1],\n-]\n-test_s = np.r_[\n-    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\n-]\n-w = np.hamming(window_length)\n-train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\")\n-test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\n-\n-# Display the training accuracies.\n-x = np.arange(0, len(test_y), 1)\n-plt.plot(x, test_y, x, train_y)\n-plt.legend([\"test\", \"train\"])\n-plt.grid()\n-\n-train_set, test_images, test_labels = dataset.get_mini_dataset(\n-    eval_batch_size, eval_iters, shots, classes, split=True\n-)\n-for images, labels in train_set:\n-    with tf.GradientTape() as tape:\n-        preds = model(images)\n-        loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n-    grads = tape.gradient(loss, model.trainable_weights)\n-    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n-test_preds = model.predict(test_images)\n-test_preds = tf.argmax(test_preds).numpy()\n-\n-_, axarr = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n-\n-sample_keys = list(train_dataset.data.keys())\n-\n-for i, ax in zip(range(5), axarr):\n-    temp_image = np.stack((test_images[i, :, :, 0],) * 3, axis=2)\n-    temp_image *= 255\n-    temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n-    ax.set_title(\n-        \"Label : {}, Prediction : {}\".format(int(test_labels[i]), test_preds[i])\n-    )\n-    ax.imshow(temp_image, cmap=\"gray\")\n-    ax.xaxis.set_visible(False)\n-    ax.yaxis.set_visible(False)\n-plt.show()\n\n@@ -1,594 +0,0 @@\n-\"\"\"\n-Title: Semantic Image Clustering\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2021/02/28\n-Last modified: 2021/02/28\n-Description: Semantic Clustering by Adopting Nearest neighbors (SCAN) algorithm.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to apply the [Semantic Clustering by Adopting Nearest neighbors\n-(SCAN)](https://arxiv.org/abs/2005.12320) algorithm (Van Gansbeke et al., 2020) on the\n-[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The algorithm consists of\n-two phases:\n-\n-1. Self-supervised visual representation learning of images, in which we use the\n-[simCLR](https://arxiv.org/abs/2002.05709) technique.\n-2. Clustering of the learned visual representation vectors to maximize the agreement\n-between the cluster assignments of neighboring vectors.\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-from collections import defaultdict\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-import matplotlib.pyplot as plt\n-from tqdm import tqdm\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 10\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-x_data = np.concatenate([x_train, x_test])\n-y_data = np.concatenate([y_train, y_test])\n-\n-print(\"x_data shape:\", x_data.shape, \"- y_data shape:\", y_data.shape)\n-\n-classes = [\n-    \"airplane\",\n-    \"automobile\",\n-    \"bird\",\n-    \"cat\",\n-    \"deer\",\n-    \"dog\",\n-    \"frog\",\n-    \"horse\",\n-    \"ship\",\n-    \"truck\",\n-]\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-target_size = 32  # Resize the input images.\n-representation_dim = 512  # The dimensions of the features vector.\n-projection_units = 128  # The projection head of the representation learner.\n-num_clusters = 20  # Number of clusters.\n-k_neighbours = 5  # Number of neighbours to consider during cluster learning.\n-tune_encoder_during_clustering = False  # Freeze the encoder in the cluster learning.\n-\n-\"\"\"\n-## Implement data preprocessing\n-\n-The data preprocessing step resizes the input images to the desired `target_size` and applies\n-feature-wise normalization. Note that, when using `keras.applications.ResNet50V2` as the\n-visual encoder, resizing the images into 255 x 255 inputs would lead to more accurate results\n-but require a longer time to train.\n-\"\"\"\n-\n-data_preprocessing = keras.Sequential(\n-    [\n-        layers.Resizing(target_size, target_size),\n-        layers.Normalization(),\n-    ]\n-)\n-# Compute the mean and the variance from the data for normalization.\n-data_preprocessing.layers[-1].adapt(x_data)\n-\n-\"\"\"\n-## Data augmentation\n-\n-Unlike simCLR, which randomly picks a single data augmentation function to apply to an input\n-image, we apply a set of data augmentation functions randomly to the input image.\n-(You can experiment with other image augmentation techniques by following\n-the [data augmentation tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation).)\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.RandomTranslation(\n-            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=\"nearest\"\n-        ),\n-        layers.RandomFlip(mode=\"horizontal\"),\n-        layers.RandomRotation(factor=0.15, fill_mode=\"nearest\"),\n-        layers.RandomZoom(\n-            height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode=\"nearest\"\n-        ),\n-    ]\n-)\n-\n-\"\"\"\n-Display a random image\n-\"\"\"\n-\n-image_idx = np.random.choice(range(x_data.shape[0]))\n-image = x_data[image_idx]\n-image_class = classes[y_data[image_idx][0]]\n-plt.figure(figsize=(3, 3))\n-plt.imshow(x_data[image_idx].astype(\"uint8\"))\n-plt.title(image_class)\n-_ = plt.axis(\"off\")\n-\n-\"\"\"\n-Display a sample of augmented versions of the image\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-for i in range(9):\n-    augmented_images = data_augmentation(np.array([image]))\n-    ax = plt.subplot(3, 3, i + 1)\n-    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Self-supervised representation learning\n-\"\"\"\n-\n-\"\"\"\n-### Implement the vision encoder\n-\"\"\"\n-\n-\n-def create_encoder(representation_dim):\n-    encoder = keras.Sequential(\n-        [\n-            keras.applications.ResNet50V2(\n-                include_top=False, weights=None, pooling=\"avg\"\n-            ),\n-            layers.Dense(representation_dim),\n-        ]\n-    )\n-    return encoder\n-\n-\n-\"\"\"\n-### Implement the unsupervised contrastive loss\n-\"\"\"\n-\n-\n-class RepresentationLearner(keras.Model):\n-    def __init__(\n-        self,\n-        encoder,\n-        projection_units,\n-        num_augmentations,\n-        temperature=1.0,\n-        dropout_rate=0.1,\n-        l2_normalize=False,\n-        **kwargs\n-    ):\n-        super().__init__(**kwargs)\n-        self.encoder = encoder\n-        # Create projection head.\n-        self.projector = keras.Sequential(\n-            [\n-                layers.Dropout(dropout_rate),\n-                layers.Dense(units=projection_units, use_bias=False),\n-                layers.BatchNormalization(),\n-                layers.ReLU(),\n-            ]\n-        )\n-        self.num_augmentations = num_augmentations\n-        self.temperature = temperature\n-        self.l2_normalize = l2_normalize\n-        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [self.loss_tracker]\n-\n-    def compute_contrastive_loss(self, feature_vectors, batch_size):\n-        num_augmentations = keras.ops.shape(feature_vectors)[0] // batch_size\n-        if self.l2_normalize:\n-            feature_vectors = keras.utils.normalize(feature_vectors)\n-        # The logits shape is [num_augmentations * batch_size, num_augmentations * batch_size].\n-        logits = (\n-            tf.linalg.matmul(feature_vectors, feature_vectors, transpose_b=True)\n-            / self.temperature\n-        )\n-        # Apply log-max trick for numerical stability.\n-        logits_max = keras.ops.max(logits, axis=1)\n-        logits = logits - logits_max\n-        # The shape of targets is [num_augmentations * batch_size, num_augmentations * batch_size].\n-        # targets is a matrix consits of num_augmentations submatrices of shape [batch_size * batch_size].\n-        # Each [batch_size * batch_size] submatrix is an identity matrix (diagonal entries are ones).\n-        targets = keras.ops.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])\n-        # Compute cross entropy loss\n-        return keras.losses.categorical_crossentropy(\n-            y_true=targets, y_pred=logits, from_logits=True\n-        )\n-\n-    def call(self, inputs):\n-        # Preprocess the input images.\n-        preprocessed = data_preprocessing(inputs)\n-        # Create augmented versions of the images.\n-        augmented = []\n-        for _ in range(self.num_augmentations):\n-            augmented.append(data_augmentation(preprocessed))\n-        augmented = layers.Concatenate(axis=0)(augmented)\n-        # Generate embedding representations of the images.\n-        features = self.encoder(augmented)\n-        # Apply projection head.\n-        return self.projector(features)\n-\n-    def train_step(self, inputs):\n-        batch_size = keras.ops.shape(inputs)[0]\n-        # Run the forward pass and compute the contrastive loss\n-        with tf.GradientTape() as tape:\n-            feature_vectors = self(inputs, training=True)\n-            loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n-        # Compute gradients\n-        trainable_vars = self.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-        # Update weights\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-        # Update loss tracker metric\n-        self.loss_tracker.update_state(loss)\n-        # Return a dict mapping metric names to current value\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, inputs):\n-        batch_size = keras.ops.shape(inputs)[0]\n-        feature_vectors = self(inputs, training=False)\n-        loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n-        self.loss_tracker.update_state(loss)\n-        return {\"loss\": self.loss_tracker.result()}\n-\n-\n-\"\"\"\n-### Train the model\n-\"\"\"\n-# Create vision encoder.\n-encoder = create_encoder(representation_dim)\n-# Create representation learner.\n-representation_learner = RepresentationLearner(\n-    encoder, projection_units, num_augmentations=2, temperature=0.1\n-)\n-# Create a a Cosine decay learning rate scheduler.\n-lr_scheduler = keras.optimizers.schedules.CosineDecay(\n-    initial_learning_rate=0.001, decay_steps=500, alpha=0.1\n-)\n-# Compile the model.\n-representation_learner.compile(\n-    optimizer=keras.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=0.0001),\n-    jit_compile=False,\n-)\n-# Fit the model.\n-history = representation_learner.fit(\n-    x=x_data,\n-    batch_size=512,\n-    epochs=50,  # for better results, increase the number of epochs to 500.\n-)\n-\n-\n-\"\"\"\n-Plot training loss\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"])\n-plt.ylabel(\"loss\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-\"\"\"\n-## Compute the nearest neighbors\n-\"\"\"\n-\n-\"\"\"\n-### Generate the embeddings for the images\n-\"\"\"\n-\n-batch_size = 500\n-# Get the feature vector representations of the images.\n-feature_vectors = encoder.predict(x_data, batch_size=batch_size, verbose=1)\n-# Normalize the feature vectores.\n-feature_vectors = keras.utils.normalize(feature_vectors)\n-\n-\"\"\"\n-### Find the *k* nearest neighbours for each embedding\n-\"\"\"\n-\n-neighbours = []\n-num_batches = feature_vectors.shape[0] // batch_size\n-for batch_idx in tqdm(range(num_batches)):\n-    start_idx = batch_idx * batch_size\n-    end_idx = start_idx + batch_size\n-    current_batch = feature_vectors[start_idx:end_idx]\n-    # Compute the dot similarity.\n-    similarities = tf.linalg.matmul(current_batch, feature_vectors, transpose_b=True)\n-    # Get the indices of most similar vectors.\n-    _, indices = keras.ops.top_k(similarities, k=k_neighbours + 1, sorted=True)\n-    # Add the indices to the neighbours.\n-    neighbours.append(indices[..., 1:])\n-\n-neighbours = np.reshape(np.array(neighbours), (-1, k_neighbours))\n-\n-\"\"\"\n-Let's display some neighbors on each row\n-\"\"\"\n-\n-nrows = 4\n-ncols = k_neighbours + 1\n-\n-plt.figure(figsize=(12, 12))\n-position = 1\n-for _ in range(nrows):\n-    anchor_idx = np.random.choice(range(x_data.shape[0]))\n-    neighbour_indicies = neighbours[anchor_idx]\n-    indices = [anchor_idx] + neighbour_indicies.tolist()\n-    for j in range(ncols):\n-        plt.subplot(nrows, ncols, position)\n-        plt.imshow(x_data[indices[j]].astype(\"uint8\"))\n-        plt.title(classes[y_data[indices[j]][0]])\n-        plt.axis(\"off\")\n-        position += 1\n-\n-\"\"\"\n-You notice that images on each row are visually similar, and belong to similar classes.\n-\"\"\"\n-\n-\"\"\"\n-## Semantic clustering with nearest neighbours\n-\"\"\"\n-\n-\"\"\"\n-### Implement clustering consistency loss\n-\n-This loss tries to make sure that neighbours have the same clustering assignments.\n-\"\"\"\n-\n-\n-class ClustersConsistencyLoss(keras.losses.Loss):\n-    def __init__(self):\n-        super().__init__()\n-\n-    def __call__(self, target, similarity, sample_weight=None):\n-        # Set targets to be ones.\n-        target = keras.ops.ones_like(similarity)\n-        # Compute cross entropy loss.\n-        loss = keras.losses.binary_crossentropy(\n-            y_true=target, y_pred=similarity, from_logits=True\n-        )\n-        return keras.ops.mean(loss)\n-\n-\n-\"\"\"\n-### Implement the clusters entropy loss\n-\n-This loss tries to make sure that cluster distribution is roughly uniformed, to avoid\n-assigning most of the instances to one cluster.\n-\"\"\"\n-\n-\n-class ClustersEntropyLoss(keras.losses.Loss):\n-    def __init__(self, entropy_loss_weight=1.0):\n-        super().__init__()\n-        self.entropy_loss_weight = entropy_loss_weight\n-\n-    def __call__(self, target, cluster_probabilities, sample_weight=None):\n-        # Ideal entropy = log(num_clusters).\n-        num_clusters = keras.ops.cast(keras.ops.shape(cluster_probabilities)[-1], \"float32\")\n-        target = keras.ops.log(num_clusters)\n-        # Compute the overall clusters distribution.\n-        cluster_probabilities = keras.ops.mean(cluster_probabilities, axis=0)\n-        # Replacing zero probabilities - if any - with a very small value.\n-        cluster_probabilities = keras.ops.clip(\n-            cluster_probabilities, 1e-8, 1.0\n-        )\n-        # Compute the entropy over the clusters.\n-        entropy = -keras.ops.sum(\n-            cluster_probabilities * keras.ops.log(cluster_probabilities)\n-        )\n-        # Compute the difference between the target and the actual.\n-        loss = target - entropy\n-        return loss\n-\n-\n-\"\"\"\n-### Implement clustering model\n-\n-This model takes a raw image as an input, generated its feature vector using the trained\n-encoder, and produces a probability distribution of the clusters given the feature vector\n-as the cluster assignments.\n-\"\"\"\n-\n-\n-def create_clustering_model(encoder, num_clusters, name=None):\n-    inputs = keras.Input(shape=input_shape)\n-    # Preprocess the input images.\n-    preprocessed = data_preprocessing(inputs)\n-    # Apply data augmentation to the images.\n-    augmented = data_augmentation(preprocessed)\n-    # Generate embedding representations of the images.\n-    features = encoder(augmented)\n-    # Assign the images to clusters.\n-    outputs = layers.Dense(units=num_clusters, activation=\"softmax\")(features)\n-    # Create the model.\n-    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n-    return model\n-\n-\n-\"\"\"\n-### Implement clustering learner\n-\n-This model receives the input `anchor` image and its `neighbours`, produces the clusters\n-assignments for them using the `clustering_model`, and produces two outputs:\n-1. `similarity`: the similarity between the cluster assignments of the `anchor` image and\n-its `neighbours`. This output is fed to the `ClustersConsistencyLoss`.\n-2. `anchor_clustering`: cluster assignments of the `anchor` images. This is fed to the `ClustersEntropyLoss`.\n-\"\"\"\n-\n-\n-def create_clustering_learner(clustering_model):\n-    anchor = keras.Input(shape=input_shape, name=\"anchors\")\n-    neighbours = keras.Input(\n-        shape=tuple([k_neighbours]) + input_shape, name=\"neighbours\"\n-    )\n-    # Changes neighbours shape to [batch_size * k_neighbours, width, height, channels]\n-    neighbours_reshaped = keras.ops.reshape(neighbours, tuple([-1]) + input_shape)\n-    # anchor_clustering shape: [batch_size, num_clusters]\n-    anchor_clustering = clustering_model(anchor)\n-    # neighbours_clustering shape: [batch_size * k_neighbours, num_clusters]\n-    neighbours_clustering = clustering_model(neighbours_reshaped)\n-    # Convert neighbours_clustering shape to [batch_size, k_neighbours, num_clusters]\n-    neighbours_clustering = keras.ops.reshape(\n-        neighbours_clustering,\n-        (-1, k_neighbours, keras.ops.shape(neighbours_clustering)[-1]),\n-    )\n-    # similarity shape: [batch_size, 1, k_neighbours]\n-    similarity = keras.ops.einsum(\n-        \"bij,bkj->bik\", keras.ops.expand_dims(anchor_clustering, axis=1), neighbours_clustering\n-    )\n-    # similarity shape:  [batch_size, k_neighbours]\n-    similarity = layers.Lambda(lambda x: keras.ops.squeeze(x, axis=1), name=\"similarity\")(\n-        similarity\n-    )\n-    # Create the model.\n-    model = keras.Model(\n-        inputs=[anchor, neighbours],\n-        outputs=[similarity, anchor_clustering],\n-        name=\"clustering_learner\",\n-    )\n-    return model\n-\n-\n-\"\"\"\n-### Train model\n-\"\"\"\n-\n-# If tune_encoder_during_clustering is set to False,\n-# then freeze the encoder weights.\n-for layer in encoder.layers:\n-    layer.trainable = tune_encoder_during_clustering\n-# Create the clustering model and learner.\n-clustering_model = create_clustering_model(encoder, num_clusters, name=\"clustering\")\n-clustering_learner = create_clustering_learner(clustering_model)\n-# Instantiate the model losses.\n-losses = [ClustersConsistencyLoss(), ClustersEntropyLoss(entropy_loss_weight=5)]\n-# Create the model inputs and labels.\n-inputs = {\"anchors\": x_data, \"neighbours\": tf.gather(x_data, neighbours)}\n-labels = np.ones(shape=(x_data.shape[0]))\n-# Compile the model.\n-clustering_learner.compile(\n-    optimizer=keras.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001),\n-    loss=losses,\n-    jit_compile=False,\n-)\n-\n-# Begin training the model.\n-clustering_learner.fit(x=inputs, y=labels, batch_size=512, epochs=50)\n-\n-\"\"\"\n-Plot training loss\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"])\n-plt.ylabel(\"loss\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-\"\"\"\n-## Cluster analysis\n-\"\"\"\n-\n-\"\"\"\n-### Assign images to clusters\n-\"\"\"\n-\n-# Get the cluster probability distribution of the input images.\n-clustering_probs = clustering_model.predict(x_data, batch_size=batch_size, verbose=1)\n-# Get the cluster of the highest probability.\n-cluster_assignments = keras.ops.argmax(clustering_probs, axis=-1).numpy()\n-# Store the clustering confidence.\n-# Images with the highest clustering confidence are considered the 'prototypes'\n-# of the clusters.\n-cluster_confidence = keras.ops.max(clustering_probs, axis=-1).numpy()\n-\n-\"\"\"\n-Let's compute the cluster sizes\n-\"\"\"\n-\n-clusters = defaultdict(list)\n-for idx, c in enumerate(cluster_assignments):\n-    clusters[c].append((idx, cluster_confidence[idx]))\n-\n-non_empty_clusters = defaultdict(list)\n-for c in clusters.keys():\n-    if clusters[c]:\n-        non_empty_clusters[c] = clusters[c]\n-\n-for c in range(num_clusters):\n-    print(\"cluster\", c, \":\", len(clusters[c]))\n-\n-\"\"\"\n-### Visualize cluster images\n-\n-Display the *prototypes*—instances with the highest clustering confidence—of each cluster:\n-\"\"\"\n-\n-num_images = 8\n-plt.figure(figsize=(15, 15))\n-position = 1\n-for c in non_empty_clusters.keys():\n-    cluster_instances = sorted(non_empty_clusters[c], key=lambda kv: kv[1], reverse=True)\n-\n-    for j in range(num_images):\n-        image_idx = cluster_instances[j][0]\n-        plt.subplot(len(non_empty_clusters), num_images, position)\n-        plt.imshow(x_data[image_idx].astype(\"uint8\"))\n-        plt.title(classes[y_data[image_idx][0]])\n-        plt.axis(\"off\")\n-        position += 1\n-\n-\"\"\"\n-### Compute clustering accuracy\n-\n-First, we assign a label for each cluster based on the majority label of its images.\n-Then, we compute the accuracy of each cluster by dividing the number of image with the\n-majority label by the size of the cluster.\n-\"\"\"\n-\n-cluster_label_counts = dict()\n-\n-for c in range(num_clusters):\n-    cluster_label_counts[c] = [0] * num_classes\n-    instances = clusters[c]\n-    for i, _ in instances:\n-        cluster_label_counts[c][y_data[i][0]] += 1\n-\n-    cluster_label_idx = np.argmax(cluster_label_counts[c])\n-    correct_count = np.max(cluster_label_counts[c])\n-    cluster_size = len(clusters[c])\n-    accuracy = (\n-        np.round((correct_count / cluster_size) * 100, 2) if cluster_size > 0 else 0\n-    )\n-    cluster_label = classes[cluster_label_idx]\n-    print(\"cluster\", c, \"label is:\", cluster_label, \" -  accuracy:\", accuracy, \"%\")\n-\n-\"\"\"\n-## Conclusion\n-\n-To improve the accuracy results, you can: 1) increase the number\n-of epochs in the representation learning and the clustering phases; 2)\n-allow the encoder weights to be tuned during the clustering phase; and 3) perform a final\n-fine-tuning step through self-labeling, as described in the [original SCAN paper](https://arxiv.org/abs/2005.12320).\n-Note that unsupervised image clustering techniques are not expected to outperform the accuracy\n-of supervised image classification techniques, rather showing that they can learn the semantics\n-of the images and group them into clusters that are similar to their original classes.\n-\"\"\"\n\n@@ -1,703 +0,0 @@\n-\"\"\"\n-Title: Semi-supervised image classification using contrastive pretraining with SimCLR\n-Author: [András Béres](https://www.linkedin.com/in/andras-beres-789190210)\n-Date created: 2021/04/24\n-Last modified: 2021/04/24\n-Description: Contrastive pretraining with SimCLR for semi-supervised image classification on the STL-10 dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-### Semi-supervised learning\n-\n-Semi-supervised learning is a machine learning paradigm that deals with\n-**partially labeled datasets**. When applying deep learning in the real world,\n-one usually has to gather a large dataset to make it work well. However, while\n-the cost of labeling scales linearly with the dataset size (labeling each\n-example takes a constant time), model performance only scales\n-[sublinearly](https://arxiv.org/abs/2001.08361) with it. This means that\n-labeling more and more samples becomes less and less cost-efficient, while\n-gathering unlabeled data is generally cheap, as it is usually readily available\n-in large quantities.\n-\n-Semi-supervised learning offers to solve this problem by only requiring a\n-partially labeled dataset, and by being label-efficient by utilizing the\n-unlabeled examples for learning as well.\n-\n-In this example, we will pretrain an encoder with contrastive learning on the\n-[STL-10](https://ai.stanford.edu/~acoates/stl10/) semi-supervised dataset using\n-no labels at all, and then fine-tune it using only its labeled subset.\n-\n-### Contrastive learning\n-\n-On the highest level, the main idea behind contrastive learning is to **learn\n-representations that are invariant to image augmentations** in a self-supervised\n-manner. One problem with this objective is that it has a trivial degenerate\n-solution: the case where the representations are constant, and do not depend at all on the\n-input images.\n-\n-Contrastive learning avoids this trap by modifying the objective in the\n-following way: it pulls representations of augmented versions/views of the same\n-image closer to each other (contracting positives), while simultaneously pushing\n-different images away from each other (contrasting negatives) in representation\n-space.\n-\n-One such contrastive approach is [SimCLR](https://arxiv.org/abs/2002.05709),\n-which essentially identifies the core components needed to optimize this\n-objective, and can achieve high performance by scaling this simple approach.\n-\n-Another approach is [SimSiam](https://arxiv.org/abs/2011.10566)\n-([Keras example](https://keras.io/examples/vision/simsiam/)),\n-whose main difference from\n-SimCLR is that the former does not use any negatives in its loss. Therefore, it does not\n-explicitly prevent the trivial solution, and, instead, avoids it implicitly by\n-architecture design (asymmetric encoding paths using a predictor network and\n-batch normalization (BatchNorm) are applied in the final layers).\n-\n-For further reading about SimCLR, check out\n-[the official Google AI blog post](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html),\n-and for an overview of self-supervised learning across both vision and language\n-check out\n-[this blog post](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-# Make sure we are able to handle large datasets\n-import resource\n-\n-low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n-resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n-\n-import math\n-import matplotlib.pyplot as plt\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Hyperparameter setup\n-\"\"\"\n-# Dataset hyperparameters\n-unlabeled_dataset_size = 100000\n-labeled_dataset_size = 5000\n-image_size = 96\n-image_channels = 3\n-\n-# Algorithm hyperparameters\n-num_epochs = 1\n-batch_size = 525  # Corresponds to 200 steps per epoch\n-width = 128\n-temperature = 0.1\n-# Stronger augmentations for contrastive, weaker ones for supervised training\n-contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n-classification_augmentation = {\n-    \"min_area\": 0.75,\n-    \"brightness\": 0.3,\n-    \"jitter\": 0.1,\n-}\n-\n-\"\"\"\n-## Dataset\n-\n-During training we will simultaneously load a large batch of unlabeled images along with a\n-smaller batch of labeled images.\n-\"\"\"\n-\n-\n-def prepare_dataset():\n-    # Labeled and unlabeled samples are loaded synchronously\n-    # with batch sizes selected accordingly\n-    steps_per_epoch = (\n-        unlabeled_dataset_size + labeled_dataset_size\n-    ) // batch_size\n-    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n-    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n-    print(\n-        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n-    )\n-\n-    # Turning off shuffle to lower resource usage\n-    unlabeled_train_dataset = (\n-        tfds.load(\n-            \"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False\n-        )\n-        .shuffle(buffer_size=10 * unlabeled_batch_size)\n-        .batch(unlabeled_batch_size)\n-    )\n-    labeled_train_dataset = (\n-        tfds.load(\n-            \"stl10\", split=\"train\", as_supervised=True, shuffle_files=False\n-        )\n-        .shuffle(buffer_size=10 * labeled_batch_size)\n-        .batch(labeled_batch_size)\n-    )\n-    test_dataset = (\n-        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n-        .batch(batch_size)\n-        .prefetch(buffer_size=tf.data.AUTOTUNE)\n-    )\n-\n-    # Labeled and unlabeled datasets are zipped together\n-    train_dataset = tf.data.Dataset.zip(\n-        (unlabeled_train_dataset, labeled_train_dataset)\n-    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n-\n-    return train_dataset, labeled_train_dataset, test_dataset\n-\n-\n-# Load STL10 dataset\n-train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()\n-\n-\"\"\"\n-## Image augmentations\n-\n-The two most important image augmentations for contrastive learning are the\n-following:\n-\n-- Cropping: forces the model to encode different parts of the same image\n-similarly, we implement it with the\n-[RandomTranslation](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_translation/)\n-and\n-[RandomZoom](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_zoom/)\n-layers\n-- Color jitter: prevents a trivial color histogram-based solution to the task by\n-distorting color histograms. A principled way to implement that is by affine\n-transformations in color space.\n-\n-In this example we use random horizontal flips as well. Stronger augmentations\n-are applied for contrastive learning, along with weaker ones for supervised\n-classification to avoid overfitting on the few labeled examples.\n-\n-We implement random color jitter as a custom preprocessing layer. Using\n-preprocessing layers for data augmentation has the following two advantages:\n-\n-- The data augmentation will run on GPU in batches, so the training will not be\n-bottlenecked by the data pipeline in environments with constrained CPU\n-resources (such as a Colab Notebook, or a personal machine)\n-- Deployment is easier as the data preprocessing pipeline is encapsulated in the\n-model, and does not have to be reimplemented when deploying it\n-\"\"\"\n-\n-\n-# Distorts the color distibutions of images\n-class RandomColorAffine(layers.Layer):\n-    def __init__(self, brightness=0, jitter=0, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.brightness = brightness\n-        self.jitter = jitter\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n-        return config\n-\n-    def call(self, images, training=True):\n-        if training:\n-            batch_size = tf.shape(images)[0]\n-\n-            # Same for all colors\n-            brightness_scales = 1 + tf.random.uniform(\n-                (batch_size, 1, 1, 1),\n-                minval=-self.brightness,\n-                maxval=self.brightness,\n-            )\n-            # Different for all colors\n-            jitter_matrices = tf.random.uniform(\n-                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n-            )\n-\n-            color_transforms = (\n-                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n-                + jitter_matrices\n-            )\n-            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n-        return images\n-\n-\n-# Image augmentation module\n-def get_augmenter(min_area, brightness, jitter):\n-    zoom_factor = 1.0 - math.sqrt(min_area)\n-    return keras.Sequential(\n-        [\n-            keras.Input(shape=(image_size, image_size, image_channels)),\n-            layers.Rescaling(1 / 255, dtype=\"uint8\"),\n-            layers.RandomFlip(\"horizontal\"),\n-            layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n-            layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n-            RandomColorAffine(brightness, jitter),\n-        ]\n-    )\n-\n-\n-def visualize_augmentations(num_images):\n-    # Sample a batch from a dataset\n-    images = next(iter(train_dataset))[0][0][:num_images]\n-\n-    # Apply augmentations\n-    augmented_images = zip(\n-        images,\n-        get_augmenter(**classification_augmentation)(images),\n-        get_augmenter(**contrastive_augmentation)(images),\n-        get_augmenter(**contrastive_augmentation)(images),\n-    )\n-    row_titles = [\n-        \"Original:\",\n-        \"Weakly augmented:\",\n-        \"Strongly augmented:\",\n-        \"Strongly augmented:\",\n-    ]\n-    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n-    for column, image_row in enumerate(augmented_images):\n-        for row, image in enumerate(image_row):\n-            plt.subplot(4, num_images, row * num_images + column + 1)\n-            plt.imshow(image)\n-            if column == 0:\n-                plt.title(row_titles[row], loc=\"left\")\n-            plt.axis(\"off\")\n-    plt.tight_layout()\n-\n-\n-visualize_augmentations(num_images=8)\n-\n-\"\"\"\n-## Encoder architecture\n-\"\"\"\n-\n-\n-# Define the encoder architecture\n-def get_encoder():\n-    return keras.Sequential(\n-        [\n-            keras.Input(shape=(image_size, image_size, image_channels)),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Flatten(),\n-            layers.Dense(width, activation=\"relu\"),\n-        ],\n-        name=\"encoder\",\n-    )\n-\n-\n-\"\"\"\n-## Supervised baseline model\n-\n-A baseline supervised model is trained using random initialization.\n-\"\"\"\n-\n-# Baseline supervised training with random initialization\n-baseline_model = keras.Sequential(\n-    [\n-        keras.Input(shape=(image_size, image_size, image_channels)),\n-        get_augmenter(**classification_augmentation),\n-        get_encoder(),\n-        layers.Dense(10),\n-    ],\n-    name=\"baseline_model\",\n-)\n-baseline_model.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n-)\n-\n-baseline_history = baseline_model.fit(\n-    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(baseline_history.history[\"val_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Self-supervised model for contrastive pretraining\n-\n-We pretrain an encoder on unlabeled images with a contrastive loss.\n-A nonlinear projection head is attached to the top of the encoder, as it\n-improves the quality of representations of the encoder.\n-\n-We use the InfoNCE/NT-Xent/N-pairs loss, which can be interpreted in the\n-following way:\n-\n-1. We treat each image in the batch as if it had its own class.\n-2. Then, we have two examples (a pair of augmented views) for each \"class\".\n-3. Each view's representation is compared to every possible pair's one (for both\n-  augmented versions).\n-4. We use the temperature-scaled cosine similarity of compared representations as\n-  logits.\n-5. Finally, we use categorical cross-entropy as the \"classification\" loss\n-\n-The following two metrics are used for monitoring the pretraining performance:\n-\n-- [Contrastive accuracy (SimCLR Table 5)](https://arxiv.org/abs/2002.05709):\n-Self-supervised metric, the ratio of cases in which the representation of an\n-image is more similar to its differently augmented version's one, than to the\n-representation of any other image in the current batch. Self-supervised\n-metrics can be used for hyperparameter tuning even in the case when there are\n-no labeled examples.\n-- [Linear probing accuracy](https://arxiv.org/abs/1603.08511): Linear probing is\n-a popular metric to evaluate self-supervised classifiers. It is computed as\n-the accuracy of a logistic regression classifier trained on top of the\n-encoder's features. In our case, this is done by training a single dense layer\n-on top of the frozen encoder. Note that contrary to traditional approach where\n-the classifier is trained after the pretraining phase, in this example we\n-train it during pretraining. This might slightly decrease its accuracy, but\n-that way we can monitor its value during training, which helps with\n-experimentation and debugging.\n-\n-Another widely used supervised metric is the\n-[KNN accuracy](https://arxiv.org/abs/1805.01978), which is the accuracy of a KNN\n-classifier trained on top of the encoder's features, which is not implemented in\n-this example.\n-\"\"\"\n-\n-\n-# Define the contrastive model with model-subclassing\n-class ContrastiveModel(keras.Model):\n-    def __init__(self):\n-        super().__init__()\n-\n-        self.temperature = temperature\n-        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n-        self.classification_augmenter = get_augmenter(\n-            **classification_augmentation\n-        )\n-        self.encoder = get_encoder()\n-        # Non-linear MLP as projection head\n-        self.projection_head = keras.Sequential(\n-            [\n-                keras.Input(shape=(width,)),\n-                layers.Dense(width, activation=\"relu\"),\n-                layers.Dense(width),\n-            ],\n-            name=\"projection_head\",\n-        )\n-        # Single dense layer for linear probing\n-        self.linear_probe = keras.Sequential(\n-            [layers.Input(shape=(width,)), layers.Dense(10)],\n-            name=\"linear_probe\",\n-        )\n-\n-        self.encoder.summary()\n-        self.projection_head.summary()\n-        self.linear_probe.summary()\n-\n-    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n-        super().compile(**kwargs)\n-\n-        self.contrastive_optimizer = contrastive_optimizer\n-        self.probe_optimizer = probe_optimizer\n-\n-        # self.contrastive_loss will be defined as a method\n-        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(\n-            from_logits=True\n-        )\n-\n-        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n-        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n-            name=\"c_acc\"\n-        )\n-        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n-        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(\n-            name=\"p_acc\"\n-        )\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.contrastive_loss_tracker,\n-            self.contrastive_accuracy,\n-            self.probe_loss_tracker,\n-            self.probe_accuracy,\n-        ]\n-\n-    def contrastive_loss(self, projections_1, projections_2):\n-        # InfoNCE loss (information noise-contrastive estimation)\n-        # NT-Xent loss (normalized temperature-scaled cross entropy)\n-\n-        # Cosine similarity: the dot product of the l2-normalized feature vectors\n-        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n-        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n-        similarities = (\n-            tf.matmul(projections_1, projections_2, transpose_b=True)\n-            / self.temperature\n-        )\n-\n-        # The similarity between the representations of two augmented views of the\n-        # same image should be higher than their similarity with other views\n-        batch_size = tf.shape(projections_1)[0]\n-        contrastive_labels = tf.range(batch_size)\n-        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n-        self.contrastive_accuracy.update_state(\n-            contrastive_labels, tf.transpose(similarities)\n-        )\n-\n-        # The temperature-scaled similarities are used as logits for cross-entropy\n-        # a symmetrized version of the loss is used here\n-        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n-            contrastive_labels, similarities, from_logits=True\n-        )\n-        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n-            contrastive_labels, tf.transpose(similarities), from_logits=True\n-        )\n-        return (loss_1_2 + loss_2_1) / 2\n-\n-    def train_step(self, data):\n-        (unlabeled_images, _), (labeled_images, labels) = data\n-\n-        # Both labeled and unlabeled images are used, without labels\n-        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n-        # Each image is augmented twice, differently\n-        augmented_images_1 = self.contrastive_augmenter(images, training=True)\n-        augmented_images_2 = self.contrastive_augmenter(images, training=True)\n-        with tf.GradientTape() as tape:\n-            features_1 = self.encoder(augmented_images_1, training=True)\n-            features_2 = self.encoder(augmented_images_2, training=True)\n-            # The representations are passed through a projection mlp\n-            projections_1 = self.projection_head(features_1, training=True)\n-            projections_2 = self.projection_head(features_2, training=True)\n-            contrastive_loss = self.contrastive_loss(\n-                projections_1, projections_2\n-            )\n-        gradients = tape.gradient(\n-            contrastive_loss,\n-            self.encoder.trainable_weights\n-            + self.projection_head.trainable_weights,\n-        )\n-        self.contrastive_optimizer.apply_gradients(\n-            zip(\n-                gradients,\n-                self.encoder.trainable_weights\n-                + self.projection_head.trainable_weights,\n-            )\n-        )\n-        self.contrastive_loss_tracker.update_state(contrastive_loss)\n-\n-        # Labels are only used in evalutation for an on-the-fly logistic regression\n-        preprocessed_images = self.classification_augmenter(\n-            labeled_images, training=True\n-        )\n-        with tf.GradientTape() as tape:\n-            # the encoder is used in inference mode here to avoid regularization\n-            # and updating the batch normalization paramers if they are used\n-            features = self.encoder(preprocessed_images, training=False)\n-            class_logits = self.linear_probe(features, training=True)\n-            probe_loss = self.probe_loss(labels, class_logits)\n-        gradients = tape.gradient(\n-            probe_loss, self.linear_probe.trainable_weights\n-        )\n-        self.probe_optimizer.apply_gradients(\n-            zip(gradients, self.linear_probe.trainable_weights)\n-        )\n-        self.probe_loss_tracker.update_state(probe_loss)\n-        self.probe_accuracy.update_state(labels, class_logits)\n-\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, data):\n-        labeled_images, labels = data\n-\n-        # For testing the components are used with a training=False flag\n-        preprocessed_images = self.classification_augmenter(\n-            labeled_images, training=False\n-        )\n-        features = self.encoder(preprocessed_images, training=False)\n-        class_logits = self.linear_probe(features, training=False)\n-        probe_loss = self.probe_loss(labels, class_logits)\n-        self.probe_loss_tracker.update_state(probe_loss)\n-        self.probe_accuracy.update_state(labels, class_logits)\n-\n-        # Only the probe metrics are logged at test time\n-        return {m.name: m.result() for m in self.metrics[2:]}\n-\n-\n-# Contrastive pretraining\n-pretraining_model = ContrastiveModel()\n-pretraining_model.compile(\n-    contrastive_optimizer=keras.optimizers.Adam(),\n-    probe_optimizer=keras.optimizers.Adam(),\n-)\n-\n-pretraining_history = pretraining_model.fit(\n-    train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(pretraining_history.history[\"val_p_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Supervised finetuning of the pretrained encoder\n-\n-We then finetune the encoder on the labeled examples, by attaching\n-a single randomly initalized fully connected classification layer on its top.\n-\"\"\"\n-\n-# Supervised finetuning of the pretrained encoder\n-finetuning_model = keras.Sequential(\n-    [\n-        layers.Input(shape=(image_size, image_size, image_channels)),\n-        get_augmenter(**classification_augmentation),\n-        pretraining_model.encoder,\n-        layers.Dense(10),\n-    ],\n-    name=\"finetuning_model\",\n-)\n-finetuning_model.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n-)\n-\n-finetuning_history = finetuning_model.fit(\n-    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(finetuning_history.history[\"val_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Comparison against the baseline\n-\"\"\"\n-\n-\n-# The classification accuracies of the baseline and the pretraining + finetuning process:\n-def plot_training_curves(\n-    pretraining_history, finetuning_history, baseline_history\n-):\n-    for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n-        plt.figure(figsize=(8, 5), dpi=100)\n-        plt.plot(\n-            baseline_history.history[f\"val_{metric_key}\"],\n-            label=\"supervised baseline\",\n-        )\n-        plt.plot(\n-            pretraining_history.history[f\"val_p_{metric_key}\"],\n-            label=\"self-supervised pretraining\",\n-        )\n-        plt.plot(\n-            finetuning_history.history[f\"val_{metric_key}\"],\n-            label=\"supervised finetuning\",\n-        )\n-        plt.legend()\n-        plt.title(f\"Classification {metric_name} during training\")\n-        plt.xlabel(\"epochs\")\n-        plt.ylabel(f\"validation {metric_name}\")\n-\n-\n-plot_training_curves(pretraining_history, finetuning_history, baseline_history)\n-\n-\"\"\"\n-By comparing the training curves, we can see that when using contrastive\n-pretraining, a higher validation accuracy can be reached, paired with a lower\n-validation loss, which means that the pretrained network was able to generalize\n-better when seeing only a small amount of labeled examples.\n-\"\"\"\n-\n-\"\"\"\n-## Improving further\n-\n-### Architecture\n-\n-The experiment in the original paper demonstrated that increasing the width and depth of the\n-models improves performance at a higher rate than for supervised learning. Also,\n-using a [ResNet-50](https://keras.io/api/applications/resnet/#resnet50-function)\n-encoder is quite standard in the literature. However keep in mind, that more\n-powerful models will not only increase training time but will also require more\n-memory and will limit the maximal batch size you can use.\n-\n-It has [been](https://arxiv.org/abs/1905.09272)\n-[reported](https://arxiv.org/abs/1911.05722) that the usage of BatchNorm layers\n-could sometimes degrade performance, as it introduces an intra-batch dependency\n-between samples, which is why I did not have used them in this example. In my\n-experiments however, using BatchNorm, especially in the projection head,\n-improves performance.\n-\n-### Hyperparameters\n-\n-The hyperparameters used in this example have been tuned manually for this task and\n-architecture. Therefore, without changing them, only marginal gains can be expected\n-from further hyperparameter tuning.\n-\n-However for a different task or model architecture these would need tuning, so\n-here are my notes on the most important ones:\n-\n-- **Batch size**: since the objective can be interpreted as a classification\n-over a batch of images (loosely speaking), the batch size is actually a more\n-important hyperparameter than usual. The higher, the better.\n-- **Temperature**: the temperature defines the \"softness\" of the softmax\n-distribution that is used in the cross-entropy loss, and is an important\n-hyperparameter. Lower values generally lead to a higher contrastive accuracy.\n-A recent trick (in [ALIGN](https://arxiv.org/abs/2102.05918)) is to learn\n-the temperature's value as well (which can be done by defining it as a\n-tf.Variable, and applying gradients on it). Even though this provides a good baseline\n-value, in my experiments the learned temperature was somewhat lower\n-than optimal, as it is optimized with respect to the contrastive loss, which is not a\n-perfect proxy for representation quality.\n-- **Image augmentation strength**: during pretraining stronger augmentations\n-increase the difficulty of the task, however after a point too strong\n-augmentations will degrade performance. During finetuning stronger\n-augmentations reduce overfitting while in my experience too strong\n-augmentations decrease the performance gains from pretraining. The whole data\n-augmentation pipeline can be seen as an important hyperparameter of the\n-algorithm, implementations of other custom image augmentation layers in Keras\n-can be found in\n-[this repository](https://github.com/beresandras/image-augmentation-layers-keras).\n-- **Learning rate schedule**: a constant schedule is used here, but it is\n-quite common in the literature to use a\n-[cosine decay schedule](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay),\n-which can further improve performance.\n-- **Optimizer**: Adam is used in this example, as it provides good performance\n-with default parameters. SGD with momentum requires more tuning, however it\n-could slightly increase performance.\n-\"\"\"\n-\n-\"\"\"\n-## Related works\n-\n-Other instance-level (image-level) contrastive learning methods:\n-\n-- [MoCo](https://arxiv.org/abs/1911.05722)\n-([v2](https://arxiv.org/abs/2003.04297),\n-[v3](https://arxiv.org/abs/2104.02057)): uses a momentum-encoder as well,\n-whose weights are an exponential moving average of the target encoder\n-- [SwAV](https://arxiv.org/abs/2006.09882): uses clustering instead of pairwise\n-comparison\n-- [BarlowTwins](https://arxiv.org/abs/2103.03230): uses a cross\n-correlation-based objective instead of pairwise comparison\n-\n-Keras implementations of **MoCo** and **BarlowTwins** can be found in\n-[this repository](https://github.com/beresandras/contrastive-classification-keras),\n-which includes a Colab notebook.\n-\n-There is also a new line of works, which optimize a similar objective, but\n-without the use of any negatives:\n-\n-- [BYOL](https://arxiv.org/abs/2006.07733): momentum-encoder + no negatives\n-- [SimSiam](https://arxiv.org/abs/2011.10566)\n-([Keras example](https://keras.io/examples/vision/simsiam/)):\n-no momentum-encoder + no negatives\n-\n-In my experience, these methods are more brittle (they can collapse to a constant\n-representation, I could not get them to work using this encoder architecture).\n-Even though they are generally more dependent on the\n-[model](https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html)\n-[architecture](https://arxiv.org/abs/2010.10241), they can improve\n-performance at smaller batch sizes.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/semi-supervised-classification-simclr)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/semi-supervised-classification).\n-\"\"\"\n\n@@ -1,893 +0,0 @@\n-\"\"\"\n-Title: A Vision Transformer without Attention\n-Author: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Ritwik Raha](https://twitter.com/ritwik_raha)\n-Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 2022/02/24\n-Last modified: 2023/07/15\n-Description: A minimal implementation of ShiftViT.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-[Vision Transformers](https://arxiv.org/abs/2010.11929) (ViTs) have sparked a wave of\n-research at the intersection of Transformers and Computer Vision (CV).\n-\n-ViTs can simultaneously model long- and short-range dependencies, thanks to\n-the Multi-Head Self-Attention mechanism in the Transformer block. Many researchers believe\n-that the success of ViTs are purely due to the attention layer, and they seldom\n-think about other parts of the ViT model.\n-\n-In the academic paper\n-[When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism](https://arxiv.org/abs/2201.10801)\n-the authors propose to demystify the success of ViTs with the introduction of a **NO\n-PARAMETER** operation in place of the attention operation. They swap the attention\n-operation with a shifting operation.\n-\n-In this example, we minimally implement the paper with close alignement to the author's\n-[official implementation](https://github.com/microsoft/SPACH/blob/main/models/shiftvit.py).\n-\"\"\"\n-\n-\"\"\"\n-## Setup and imports\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\n-# Setting seed for reproducibiltiy\n-SEED = 42\n-keras.utils.set_random_seed(SEED)\n-\n-\"\"\"\n-## Hyperparameters\n-\n-These are the hyperparameters that we have chosen for the experiment.\n-Please feel free to tune them.\n-\"\"\"\n-\n-\n-class Config(object):\n-    # DATA\n-    batch_size = 256\n-    buffer_size = batch_size * 2\n-    input_shape = (32, 32, 3)\n-    num_classes = 10\n-\n-    # AUGMENTATION\n-    image_size = 48\n-\n-    # ARCHITECTURE\n-    patch_size = 4\n-    projected_dim = 96\n-    num_shift_blocks_per_stages = [2, 4, 8, 2]\n-    epsilon = 1e-5\n-    stochastic_depth_rate = 0.2\n-    mlp_dropout_rate = 0.2\n-    num_div = 12\n-    shift_pixel = 1\n-    mlp_expand_ratio = 2\n-\n-    # OPTIMIZER\n-    lr_start = 1e-5\n-    lr_max = 1e-3\n-    weight_decay = 1e-4\n-\n-    # TRAINING\n-    epochs = 100\n-\n-\n-config = Config()\n-\n-\"\"\"\n-## Load the CIFAR-10 dataset\n-\n-We use the CIFAR-10 dataset for our experiments.\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-(x_train, y_train), (x_val, y_val) = (\n-    (x_train[:40000], y_train[:40000]),\n-    (x_train[40000:], y_train[40000:]),\n-)\n-print(f\"Training samples: {len(x_train)}\")\n-print(f\"Validation samples: {len(x_val)}\")\n-print(f\"Testing samples: {len(x_test)}\")\n-\n-AUTO = tf.data.AUTOTUNE\n-train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-train_ds = (\n-    train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n-)\n-\n-val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n-val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)\n-\n-test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n-test_ds = test_ds.batch(config.batch_size).prefetch(AUTO)\n-\n-\"\"\"\n-## Data Augmentation\n-\n-The augmentation pipeline consists of:\n-\n-- Rescaling\n-- Resizing\n-- Random cropping\n-- Random horizontal flipping\n-\n-_Note_: The image data augmentation layers do not apply\n-data transformations at inference time. This means that\n-when these layers are called with `training=False` they\n-behave differently. Refer to the\n-[documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/)\n-for more details.\n-\"\"\"\n-\n-\n-def get_augmentation_model():\n-    \"\"\"Build the data augmentation model.\"\"\"\n-    data_augmentation = keras.Sequential(\n-        [\n-            layers.Resizing(\n-                config.input_shape[0] + 20, config.input_shape[0] + 20\n-            ),\n-            layers.RandomCrop(config.image_size, config.image_size),\n-            layers.RandomFlip(\"horizontal\"),\n-            layers.Rescaling(1 / 255.0),\n-        ]\n-    )\n-    return data_augmentation\n-\n-\n-\"\"\"\n-## The ShiftViT architecture\n-\n-In this section, we build the architecture proposed in\n-[the ShiftViT paper](https://arxiv.org/abs/2201.10801).\n-\n-| ![ShiftViT Architecture](https://i.imgur.com/CHU40HX.png) |\n-| :--: |\n-| Figure 1: The entire architecutre of ShiftViT.\n-[Source](https://arxiv.org/abs/2201.10801) |\n-\n-The architecture as shown in Fig. 1, is inspired by\n-[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030).\n-Here the authors propose a modular architecture with 4 stages. Each stage works on its\n-own spatial size, creating a hierarchical architecture.\n-\n-An input image of size `HxWx3` is split into non-overlapping patches of size `4x4`.\n-This is done via the patchify layer which results in individual tokens of feature size `48`\n-(`4x4x3`). Each stage comprises two parts:\n-\n-1. Embedding Generation\n-2. Stacked Shift Blocks\n-\n-We discuss the stages and the modules in detail in what follows.\n-\n-_Note_: Compared to the [official implementation](https://github.com/microsoft/SPACH/blob/main/models/shiftvit.py)\n-we restructure some key components to better fit the Keras API.\n-\"\"\"\n-\n-\"\"\"\n-### The ShiftViT Block\n-\n-| ![ShiftViT block](https://i.imgur.com/IDe35vo.gif) |\n-| :--: |\n-| Figure 2: From the Model to a Shift Block. |\n-\n-Each stage in the ShiftViT architecture comprises of a Shift Block as shown in Fig 2.\n-\n-| ![Shift Vit Block](https://i.imgur.com/0q13pLu.png) |\n-| :--: |\n-| Figure 3: The Shift ViT Block. [Source](https://arxiv.org/abs/2201.10801) |\n-\n-The Shift Block as shown in Fig. 3, comprises of the following:\n-\n-1. Shift Operation\n-2. Linear Normalization\n-3. MLP Layer\n-\"\"\"\n-\n-\"\"\"\n-#### The MLP block\n-\n-The MLP block is intended to be a stack of densely-connected layers.s\n-\"\"\"\n-\n-\n-class MLP(layers.Layer):\n-    \"\"\"Get the MLP layer for each shift block.\n-\n-    Args:\n-        mlp_expand_ratio (int): The ratio with which the first feature map is expanded.\n-        mlp_dropout_rate (float): The rate for dropout.\n-    \"\"\"\n-\n-    def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):\n-        super().__init__(**kwargs)\n-        self.mlp_expand_ratio = mlp_expand_ratio\n-        self.mlp_dropout_rate = mlp_dropout_rate\n-\n-    def build(self, input_shape):\n-        input_channels = input_shape[-1]\n-        initial_filters = int(self.mlp_expand_ratio * input_channels)\n-\n-        self.mlp = keras.Sequential(\n-            [\n-                layers.Dense(\n-                    units=initial_filters,\n-                    activation=tf.nn.gelu,\n-                ),\n-                layers.Dropout(rate=self.mlp_dropout_rate),\n-                layers.Dense(units=input_channels),\n-                layers.Dropout(rate=self.mlp_dropout_rate),\n-            ]\n-        )\n-\n-    def call(self, x):\n-        x = self.mlp(x)\n-        return x\n-\n-\n-\"\"\"\n-#### The DropPath layer\n-\n-Stochastic depth is a regularization technique that randomly drops a set of\n-layers. During inference, the layers are kept as they are. It is very\n-similar to Dropout, but it operates on a block of layers rather\n-than on individual nodes present inside a layer.\n-\"\"\"\n-\n-\n-class DropPath(layers.Layer):\n-    \"\"\"Drop Path also known as the Stochastic Depth layer.\n-\n-    Refernece:\n-        - https://keras.io/examples/vision/cct/#stochastic-depth-for-regularization\n-        - github.com:rwightman/pytorch-image-models\n-    \"\"\"\n-\n-    def __init__(self, drop_path_prob, **kwargs):\n-        super().__init__(**kwargs)\n-        self.drop_path_prob = drop_path_prob\n-\n-    def call(self, x, training=False):\n-        if training:\n-            keep_prob = 1 - self.drop_path_prob\n-            shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n-            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n-            random_tensor = tf.floor(random_tensor)\n-            return (x / keep_prob) * random_tensor\n-        return x\n-\n-\n-\"\"\"\n-#### Block\n-\n-The most important operation in this paper is the **shift opperation**. In this section,\n-we describe the shift operation and compare it with its original implementation provided\n-by the authors.\n-\n-A generic feature map is assumed to have the shape `[N, H, W, C]`. Here we choose a\n-`num_div` parameter that decides the division size of the channels. The first 4 divisions\n-are shifted (1 pixel) in the left, right, up, and down direction. The remaining splits\n-are kept as is. After partial shifting the shifted channels are padded and the overflown\n-pixels are chopped off. This completes the partial shifting operation.\n-\n-In the original implementation, the code is approximately:\n-\n-```python\n-out[:, g * 0:g * 1, :, :-1] = x[:, g * 0:g * 1, :, 1:]  # shift left\n-out[:, g * 1:g * 2, :, 1:] = x[:, g * 1:g * 2, :, :-1]  # shift right\n-out[:, g * 2:g * 3, :-1, :] = x[:, g * 2:g * 3, 1:, :]  # shift up\n-out[:, g * 3:g * 4, 1:, :] = x[:, g * 3:g * 4, :-1, :]  # shift down\n-\n-out[:, g * 4:, :, :] = x[:, g * 4:, :, :]  # no shift\n-```\n-\n-In TensorFlow it would be infeasible for us to assign shifted channels to a tensor in the\n-middle of the training process. This is why we have resorted to the following procedure:\n-\n-1. Split the channels with the `num_div` parameter.\n-2. Select each of the first four spilts and shift and pad them in the respective\n-directions.\n-3. After shifting and padding, we concatenate the channel back.\n-\n-| ![Manim rendered animation for shift operation](https://i.imgur.com/PReeULP.gif) |\n-| :--: |\n-| Figure 4: The TensorFlow style shifting |\n-\n-The entire procedure is explained in the Fig. 4.\n-\"\"\"\n-\n-\n-class ShiftViTBlock(layers.Layer):\n-    \"\"\"A unit ShiftViT Block\n-\n-    Args:\n-        shift_pixel (int): The number of pixels to shift. Defaults to `1`.\n-        mlp_expand_ratio (int): The ratio with which MLP features are\n-            expanded. Defaults to `2`.\n-        mlp_dropout_rate (float): The dropout rate used in MLP.\n-        num_div (int): The number of divisions of the feature map's channel.\n-            Totally, 4/num_div of channels will be shifted. Defaults to 12.\n-        epsilon (float): Epsilon constant.\n-        drop_path_prob (float): The drop probability for drop path.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        epsilon,\n-        drop_path_prob,\n-        mlp_dropout_rate,\n-        num_div=12,\n-        shift_pixel=1,\n-        mlp_expand_ratio=2,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.shift_pixel = shift_pixel\n-        self.mlp_expand_ratio = mlp_expand_ratio\n-        self.mlp_dropout_rate = mlp_dropout_rate\n-        self.num_div = num_div\n-        self.epsilon = epsilon\n-        self.drop_path_prob = drop_path_prob\n-\n-    def build(self, input_shape):\n-        self.H = input_shape[1]\n-        self.W = input_shape[2]\n-        self.C = input_shape[3]\n-        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n-        self.drop_path = (\n-            DropPath(drop_path_prob=self.drop_path_prob)\n-            if self.drop_path_prob > 0.0\n-            else layers.Activation(\"linear\")\n-        )\n-        self.mlp = MLP(\n-            mlp_expand_ratio=self.mlp_expand_ratio,\n-            mlp_dropout_rate=self.mlp_dropout_rate,\n-        )\n-\n-    def get_shift_pad(self, x, mode):\n-        \"\"\"Shifts the channels according to the mode chosen.\"\"\"\n-        if mode == \"left\":\n-            offset_height = 0\n-            offset_width = 0\n-            target_height = 0\n-            target_width = self.shift_pixel\n-        elif mode == \"right\":\n-            offset_height = 0\n-            offset_width = self.shift_pixel\n-            target_height = 0\n-            target_width = self.shift_pixel\n-        elif mode == \"up\":\n-            offset_height = 0\n-            offset_width = 0\n-            target_height = self.shift_pixel\n-            target_width = 0\n-        else:\n-            offset_height = self.shift_pixel\n-            offset_width = 0\n-            target_height = self.shift_pixel\n-            target_width = 0\n-        crop = tf.image.crop_to_bounding_box(\n-            x,\n-            offset_height=offset_height,\n-            offset_width=offset_width,\n-            target_height=self.H - target_height,\n-            target_width=self.W - target_width,\n-        )\n-        shift_pad = tf.image.pad_to_bounding_box(\n-            crop,\n-            offset_height=offset_height,\n-            offset_width=offset_width,\n-            target_height=self.H,\n-            target_width=self.W,\n-        )\n-        return shift_pad\n-\n-    def call(self, x, training=False):\n-        # Split the feature maps\n-        x_splits = tf.split(\n-            x, num_or_size_splits=self.C // self.num_div, axis=-1\n-        )\n-\n-        # Shift the feature maps\n-        x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")\n-        x_splits[1] = self.get_shift_pad(x_splits[1], mode=\"right\")\n-        x_splits[2] = self.get_shift_pad(x_splits[2], mode=\"up\")\n-        x_splits[3] = self.get_shift_pad(x_splits[3], mode=\"down\")\n-\n-        # Concatenate the shifted and unshifted feature maps\n-        x = tf.concat(x_splits, axis=-1)\n-\n-        # Add the residual connection\n-        shortcut = x\n-        x = shortcut + self.drop_path(\n-            self.mlp(self.layer_norm(x)), training=training\n-        )\n-        return x\n-\n-\n-\"\"\"\n-### The ShiftViT blocks\n-\n-| ![Shift Blokcs](https://i.imgur.com/FKy5NnD.png) |\n-| :--: |\n-| Figure 5: Shift Blocks in the architecture. [Source](https://arxiv.org/abs/2201.10801) |\n-\n-Each stage of the architecture has shift blocks as shown in Fig.5. Each of these blocks\n-contain a variable number of stacked ShiftViT block (as built in the earlier section).\n-\n-Shift blocks are followed by a PatchMerging layer that scales down feature inputs. The\n-PatchMerging layer helps in the pyramidal structure of the model.\n-\"\"\"\n-\n-\"\"\"\n-#### The PatchMerging layer\n-\n-This layer merges the two adjacent tokens. This layer helps in scaling the features down\n-spatially and increasing the features up channel wise. We use a Conv2D layer to merge the\n-patches.\n-\"\"\"\n-\n-\n-class PatchMerging(layers.Layer):\n-    \"\"\"The Patch Merging layer.\n-\n-    Args:\n-        epsilon (float): The epsilon constant.\n-    \"\"\"\n-\n-    def __init__(self, epsilon, **kwargs):\n-        super().__init__(**kwargs)\n-        self.epsilon = epsilon\n-\n-    def build(self, input_shape):\n-        filters = 2 * input_shape[-1]\n-        self.reduction = layers.Conv2D(\n-            filters=filters,\n-            kernel_size=2,\n-            strides=2,\n-            padding=\"same\",\n-            use_bias=False,\n-        )\n-        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n-\n-    def call(self, x):\n-        # Apply the patch merging algorithm on the feature maps\n-        x = self.layer_norm(x)\n-        x = self.reduction(x)\n-        return x\n-\n-\n-\"\"\"\n-#### Stacked Shift Blocks\n-\n-Each stage will have a variable number of stacked ShiftViT Blocks, as suggested in\n-the paper. This is a generic layer that will contain the stacked shift vit blocks\n-with the patch merging layer as well. Combining the two operations (shift ViT\n-block and patch merging) is a design choice we picked for better code reusability.\n-\"\"\"\n-\n-\n-# Note: This layer will have a different depth of stacking\n-# for different stages on the model.\n-class StackedShiftBlocks(layers.Layer):\n-    \"\"\"The layer containing stacked ShiftViTBlocks.\n-\n-    Args:\n-        epsilon (float): The epsilon constant.\n-        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n-        num_shift_blocks (int): The number of shift vit blocks for this stage.\n-        stochastic_depth_rate (float): The maximum drop path rate chosen.\n-        is_merge (boolean): A flag that determines the use of the Patch Merge\n-            layer after the shift vit blocks.\n-        num_div (int): The division of channels of the feature map. Defaults to `12`.\n-        shift_pixel (int): The number of pixels to shift. Defaults to `1`.\n-        mlp_expand_ratio (int): The ratio with which the initial dense layer of\n-            the MLP is expanded Defaults to `2`.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        epsilon,\n-        mlp_dropout_rate,\n-        num_shift_blocks,\n-        stochastic_depth_rate,\n-        is_merge,\n-        num_div=12,\n-        shift_pixel=1,\n-        mlp_expand_ratio=2,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.epsilon = epsilon\n-        self.mlp_dropout_rate = mlp_dropout_rate\n-        self.num_shift_blocks = num_shift_blocks\n-        self.stochastic_depth_rate = stochastic_depth_rate\n-        self.is_merge = is_merge\n-        self.num_div = num_div\n-        self.shift_pixel = shift_pixel\n-        self.mlp_expand_ratio = mlp_expand_ratio\n-\n-    def build(self, input_shapes):\n-        # Calculate stochastic depth probabilities.\n-        # Reference: https://keras.io/examples/vision/cct/#the-final-cct-model\n-        dpr = [\n-            x\n-            for x in np.linspace(\n-                start=0,\n-                stop=self.stochastic_depth_rate,\n-                num=self.num_shift_blocks,\n-            )\n-        ]\n-\n-        # Build the shift blocks as a list of ShiftViT Blocks\n-        self.shift_blocks = list()\n-        for num in range(self.num_shift_blocks):\n-            self.shift_blocks.append(\n-                ShiftViTBlock(\n-                    num_div=self.num_div,\n-                    epsilon=self.epsilon,\n-                    drop_path_prob=dpr[num],\n-                    mlp_dropout_rate=self.mlp_dropout_rate,\n-                    shift_pixel=self.shift_pixel,\n-                    mlp_expand_ratio=self.mlp_expand_ratio,\n-                )\n-            )\n-        if self.is_merge:\n-            self.patch_merge = PatchMerging(epsilon=self.epsilon)\n-\n-    def call(self, x, training=False):\n-        for shift_block in self.shift_blocks:\n-            x = shift_block(x, training=training)\n-        if self.is_merge:\n-            x = self.patch_merge(x)\n-        return x\n-\n-\n-\"\"\"\n-## The ShiftViT model\n-\n-Build the ShiftViT custom model.\n-\"\"\"\n-\n-\n-class ShiftViTModel(keras.Model):\n-    \"\"\"The ShiftViT Model.\n-\n-    Args:\n-        data_augmentation (keras.Model): A data augmentation model.\n-        projected_dim (int): The dimension to which the patches of the image are\n-            projected.\n-        patch_size (int): The patch size of the images.\n-        num_shift_blocks_per_stages (list[int]): A list of all the number of shit\n-            blocks per stage.\n-        epsilon (float): The epsilon constant.\n-        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n-        stochastic_depth_rate (float): The maximum drop rate probability.\n-        num_div (int): The number of divisions of the channesl of the feature\n-            map. Defaults to `12`.\n-        shift_pixel (int): The number of pixel to shift. Defaults to `1`.\n-        mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n-            is expanded to. Defaults to `2`.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        data_augmentation,\n-        projected_dim,\n-        patch_size,\n-        num_shift_blocks_per_stages,\n-        epsilon,\n-        mlp_dropout_rate,\n-        stochastic_depth_rate,\n-        num_div=12,\n-        shift_pixel=1,\n-        mlp_expand_ratio=2,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.data_augmentation = data_augmentation\n-        self.patch_projection = layers.Conv2D(\n-            filters=projected_dim,\n-            kernel_size=patch_size,\n-            strides=patch_size,\n-            padding=\"same\",\n-        )\n-        self.stages = list()\n-        for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):\n-            if index == len(num_shift_blocks_per_stages) - 1:\n-                # This is the last stage, do not use the patch merge here.\n-                is_merge = False\n-            else:\n-                is_merge = True\n-            # Build the stages.\n-            self.stages.append(\n-                StackedShiftBlocks(\n-                    epsilon=epsilon,\n-                    mlp_dropout_rate=mlp_dropout_rate,\n-                    num_shift_blocks=num_shift_blocks,\n-                    stochastic_depth_rate=stochastic_depth_rate,\n-                    is_merge=is_merge,\n-                    num_div=num_div,\n-                    shift_pixel=shift_pixel,\n-                    mlp_expand_ratio=mlp_expand_ratio,\n-                )\n-            )\n-        self.global_avg_pool = layers.GlobalAveragePooling2D()\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"data_augmentation\": self.data_augmentation,\n-                \"patch_projection\": self.patch_projection,\n-                \"stages\": self.stages,\n-                \"global_avg_pool\": self.global_avg_pool,\n-            }\n-        )\n-        return config\n-\n-    def _calculate_loss(self, data, training=False):\n-        (images, labels) = data\n-\n-        # Augment the images\n-        augmented_images = self.data_augmentation(images, training=training)\n-\n-        # Create patches and project the pathces.\n-        projected_patches = self.patch_projection(augmented_images)\n-\n-        # Pass through the stages\n-        x = projected_patches\n-        for stage in self.stages:\n-            x = stage(x, training=training)\n-\n-        # Get the logits.\n-        logits = self.global_avg_pool(x)\n-\n-        # Calculate the loss and return it.\n-        total_loss = self.compute_loss(data, labels, logits)\n-        return total_loss, labels, logits\n-\n-    def train_step(self, inputs):\n-        with tf.GradientTape() as tape:\n-            total_loss, labels, logits = self._calculate_loss(\n-                data=inputs, training=True\n-            )\n-\n-        # Apply gradients.\n-        train_vars = [\n-            self.data_augmentation.trainable_variables,\n-            self.patch_projection.trainable_variables,\n-            self.global_avg_pool.trainable_variables,\n-        ]\n-        train_vars = train_vars + [\n-            stage.trainable_variables for stage in self.stages\n-        ]\n-\n-        # Optimize the gradients.\n-        grads = tape.gradient(total_loss, train_vars)\n-        trainable_variable_list = []\n-        for grad, var in zip(grads, train_vars):\n-            for g, v in zip(grad, var):\n-                trainable_variable_list.append((g, v))\n-        self.optimizer.apply_gradients(trainable_variable_list)\n-\n-        # Update the metrics\n-        for metric in self.metrics:\n-            if metric.name == \"loss\":\n-                metric.update_state(total_loss)\n-            else:\n-                metric.update_state(labels, logits)\n-\n-        # Return a dict mapping metric names to current value\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, data):\n-        loss, labels, logits = self._calculate_loss(data=data, training=False)\n-\n-        # Update the metrics\n-        for metric in self.metrics:\n-            if metric.name == \"loss\":\n-                metric.update_state(loss)\n-            else:\n-                metric.update_state(labels, logits)\n-\n-        # Return a dict mapping metric names to current value\n-        return {m.name: m.result() for m in self.metrics}\n-\n-\n-\"\"\"\n-## Instantiate the model\n-\"\"\"\n-\n-model = ShiftViTModel(\n-    data_augmentation=get_augmentation_model(),\n-    projected_dim=config.projected_dim,\n-    patch_size=config.patch_size,\n-    num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,\n-    epsilon=config.epsilon,\n-    mlp_dropout_rate=config.mlp_dropout_rate,\n-    stochastic_depth_rate=config.stochastic_depth_rate,\n-    num_div=config.num_div,\n-    shift_pixel=config.shift_pixel,\n-    mlp_expand_ratio=config.mlp_expand_ratio,\n-)\n-\n-\"\"\"\n-## Learning rate schedule\n-\n-In many experiments, we want to warm up the model with a slowly increasing learning rate\n-and then cool down the model with a slowly decaying learning rate. In the warmup cosine\n-decay, the learning rate linearly increases for the warmup steps and then decays with a\n-cosine decay.\n-\"\"\"\n-\n-\n-# Some code is taken from:\n-# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n-class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n-    \"\"\"A LearningRateSchedule that uses a warmup cosine decay schedule.\"\"\"\n-\n-    def __init__(self, lr_start, lr_max, warmup_steps, total_steps):\n-        \"\"\"\n-        Args:\n-            lr_start: The initial learning rate\n-            lr_max: The maximum learning rate to which lr should increase to in\n-                the warmup steps\n-            warmup_steps: The number of steps for which the model warms up\n-            total_steps: The total number of steps for the model training\n-        \"\"\"\n-        super().__init__()\n-        self.lr_start = lr_start\n-        self.lr_max = lr_max\n-        self.warmup_steps = warmup_steps\n-        self.total_steps = total_steps\n-        self.pi = tf.constant(np.pi)\n-\n-    def __call__(self, step):\n-        # Check whether the total number of steps is larger than the warmup\n-        # steps. If not, then throw a value error.\n-        if self.total_steps < self.warmup_steps:\n-            raise ValueError(\n-                f\"Total number of steps {self.total_steps} must be\"\n-                + f\"larger or equal to warmup steps {self.warmup_steps}.\"\n-            )\n-\n-        # `cos_annealed_lr` is a graph that increases to 1 from the initial\n-        # step to the warmup step. After that this graph decays to -1 at the\n-        # final step mark.\n-        cos_annealed_lr = tf.cos(\n-            self.pi\n-            * (tf.cast(step, tf.float32) - self.warmup_steps)\n-            / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n-        )\n-\n-        # Shift the mean of the `cos_annealed_lr` graph to 1. Now the grpah goes\n-        # from 0 to 2. Normalize the graph with 0.5 so that now it goes from 0\n-        # to 1. With the normalized graph we scale it with `lr_max` such that\n-        # it goes from 0 to `lr_max`\n-        learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)\n-\n-        # Check whether warmup_steps is more than 0.\n-        if self.warmup_steps > 0:\n-            # Check whether lr_max is larger that lr_start. If not, throw a value\n-            # error.\n-            if self.lr_max < self.lr_start:\n-                raise ValueError(\n-                    f\"lr_start {self.lr_start} must be smaller or\"\n-                    + f\"equal to lr_max {self.lr_max}.\"\n-                )\n-\n-            # Calculate the slope with which the learning rate should increase\n-            # in the warumup schedule. The formula for slope is m = ((b-a)/steps)\n-            slope = (self.lr_max - self.lr_start) / self.warmup_steps\n-\n-            # With the formula for a straight line (y = mx+c) build the warmup\n-            # schedule\n-            warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start\n-\n-            # When the current step is lesser that warmup steps, get the line\n-            # graph. When the current step is greater than the warmup steps, get\n-            # the scaled cos graph.\n-            learning_rate = tf.where(\n-                step < self.warmup_steps, warmup_rate, learning_rate\n-            )\n-\n-        # When the current step is more that the total steps, return 0 else return\n-        # the calculated graph.\n-        return tf.where(\n-            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n-        )\n-\n-\n-\"\"\"\n-## Compile and train the model\n-\"\"\"\n-\n-# Get the total number of steps for training.\n-total_steps = int((len(x_train) / config.batch_size) * config.epochs)\n-\n-# Calculate the number of steps for warmup.\n-warmup_epoch_percentage = 0.15\n-warmup_steps = int(total_steps * warmup_epoch_percentage)\n-\n-# Initialize the warmupcosine schedule.\n-scheduled_lrs = WarmUpCosine(\n-    lr_start=1e-5,\n-    lr_max=1e-3,\n-    warmup_steps=warmup_steps,\n-    total_steps=total_steps,\n-)\n-\n-# Get the optimizer.\n-optimizer = keras.optimizers.AdamW(\n-    learning_rate=scheduled_lrs, weight_decay=config.weight_decay\n-)\n-\n-# Compile and pretrain the model.\n-model.compile(\n-    optimizer=optimizer,\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[\n-        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n-        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-    ],\n-)\n-\n-# Train the model\n-history = model.fit(\n-    train_ds,\n-    epochs=config.epochs,\n-    validation_data=val_ds,\n-    callbacks=[\n-        keras.callbacks.EarlyStopping(\n-            monitor=\"val_accuracy\",\n-            patience=5,\n-            mode=\"auto\",\n-        )\n-    ],\n-)\n-\n-# Evaluate the model with the test dataset.\n-print(\"TESTING\")\n-loss, acc_top1, acc_top5 = model.evaluate(test_ds)\n-print(f\"Loss: {loss:0.2f}\")\n-print(f\"Top 1 test accuracy: {acc_top1*100:0.2f}%\")\n-print(f\"Top 5 test accuracy: {acc_top5*100:0.2f}%\")\n-\n-\"\"\"\n-## Conclusion\n-\n-The most impactful contribution of the paper is not the novel architecture, but\n-the idea that hierarchical ViTs trained with no attention can perform quite well. This\n-opens up the question of how essential attention is to the performance of ViTs.\n-\n-For curious minds, we would suggest reading the\n-[ConvNexT](https://arxiv.org/abs/2201.03545) paper which attends more to the training\n-paradigms and architectural details of ViTs rather than providing a novel architecture\n-based on attention.\n-\n-Acknowledgements:\n-\n-- We would like to thank [PyImageSearch](https://pyimagesearch.com) for providing us with\n-resources that helped in the completion of this project.\n-- We would like to thank [JarvisLabs.ai](https://jarvislabs.ai/) for providing with the\n-GPU credits.\n-- We would like to thank [Manim Community](https://www.manim.community/) for the manim\n-library.\n-- A personal note of thanks to [Puja Roychowdhury](https://twitter.com/pleb_talks) for\n-helping us with the Learning Rate Schedule.\n-\"\"\"\n\n@@ -1,415 +0,0 @@\n-\"\"\"\n-Title: Image similarity estimation using a Siamese Network with a triplet loss\n-Authors: [Hazem Essam](https://twitter.com/hazemessamm) and [Santiago L. Valdarrama](https://twitter.com/svpino)\n-Date created: 2021/03/25\n-Last modified: 2021/03/25\n-Description: Training a Siamese Network to compare the similarity of images using a triplet loss function.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-A [Siamese Network](https://en.wikipedia.org/wiki/Siamese_neural_network) is a type of network architecture that\n-contains two or more identical subnetworks used to generate feature vectors for each input and compare them.\n-\n-Siamese Networks can be applied to different use cases, like detecting duplicates, finding anomalies, and face recognition.\n-\n-This example uses a Siamese Network with three identical subnetworks. We will provide three images to the model, where\n-two of them will be similar (_anchor_ and _positive_ samples), and the third will be unrelated (a _negative_ example.)\n-Our goal is for the model to learn to estimate the similarity between images.\n-\n-For the network to learn, we use a triplet loss function. You can find an introduction to triplet loss in the\n-[FaceNet paper](https://arxiv.org/pdf/1503.03832.pdf) by Schroff et al,. 2015. In this example, we define the triplet\n-loss function as follows:\n-\n-`L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)`\n-\n-This example uses the [Totally Looks Like dataset](https://sites.google.com/view/totally-looks-like-dataset)\n-by [Rosenfeld et al., 2018](https://arxiv.org/pdf/1803.01485v3.pdf).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import os\n-import tensorflow as tf\n-from pathlib import Path\n-from keras import layers\n-from keras import optimizers\n-from keras import metrics\n-from keras import Model\n-from keras.applications import resnet\n-\n-\n-target_shape = (200, 200)\n-\n-\n-\"\"\"\n-## Load the dataset\n-\n-We are going to load the *Totally Looks Like* dataset and unzip it inside the `~/.keras` directory\n-in the local environment.\n-\n-The dataset consists of two separate files:\n-\n-* `left.zip` contains the images that we will use as the anchor.\n-* `right.zip` contains the images that we will use as the positive sample (an image that looks like the anchor).\n-\"\"\"\n-\n-cache_dir = Path(Path.home()) / \".keras\"\n-anchor_images_path = cache_dir / \"left\"\n-positive_images_path = cache_dir / \"right\"\n-\n-\"\"\"shell\n-gdown --id 1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34\n-gdown --id 1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW\n-unzip -oq left.zip -d $cache_dir\n-unzip -oq right.zip -d $cache_dir\n-\"\"\"\n-\n-\"\"\"\n-## Preparing the data\n-\n-We are going to use a `tf.data` pipeline to load the data and generate the triplets that we\n-need to train the Siamese network.\n-\n-We'll set up the pipeline using a zipped list with anchor, positive, and negative filenames as\n-the source. The pipeline will load and preprocess the corresponding images.\n-\"\"\"\n-\n-\n-def preprocess_image(filename):\n-    \"\"\"\n-    Load the specified file as a JPEG image, preprocess it and\n-    resize it to the target shape.\n-    \"\"\"\n-\n-    image_string = tf.io.read_file(filename)\n-    image = tf.image.decode_jpeg(image_string, channels=3)\n-    image = tf.image.convert_image_dtype(image, tf.float32)\n-    image = tf.image.resize(image, target_shape)\n-    return image\n-\n-\n-def preprocess_triplets(anchor, positive, negative):\n-    \"\"\"\n-    Given the filenames corresponding to the three images, load and\n-    preprocess them.\n-    \"\"\"\n-\n-    return (\n-        preprocess_image(anchor),\n-        preprocess_image(positive),\n-        preprocess_image(negative),\n-    )\n-\n-\n-\"\"\"\n-Let's setup our data pipeline using a zipped list with an anchor, positive,\n-and negative image filename as the source. The output of the pipeline\n-contains the same triplet with every image loaded and preprocessed.\n-\"\"\"\n-\n-# We need to make sure both the anchor and positive images are loaded in\n-# sorted order so we can match them together.\n-anchor_images = sorted(\n-    [str(anchor_images_path / f) for f in os.listdir(anchor_images_path)]\n-)\n-\n-positive_images = sorted(\n-    [str(positive_images_path / f) for f in os.listdir(positive_images_path)]\n-)\n-\n-image_count = len(anchor_images)\n-\n-anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n-positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n-\n-# To generate the list of negative images, let's randomize the list of\n-# available images and concatenate them together.\n-rng = np.random.RandomState(seed=42)\n-rng.shuffle(anchor_images)\n-rng.shuffle(positive_images)\n-\n-negative_images = anchor_images + positive_images\n-np.random.RandomState(seed=32).shuffle(negative_images)\n-\n-negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n-negative_dataset = negative_dataset.shuffle(buffer_size=4096)\n-\n-dataset = tf.data.Dataset.zip(\n-    (anchor_dataset, positive_dataset, negative_dataset)\n-)\n-dataset = dataset.shuffle(buffer_size=1024)\n-dataset = dataset.map(preprocess_triplets)\n-\n-# Let's now split our dataset in train and validation.\n-train_dataset = dataset.take(round(image_count * 0.8))\n-val_dataset = dataset.skip(round(image_count * 0.8))\n-\n-train_dataset = train_dataset.batch(32, drop_remainder=False)\n-train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n-\n-val_dataset = val_dataset.batch(32, drop_remainder=False)\n-val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n-\n-\n-\"\"\"\n-Let's take a look at a few examples of triplets. Notice how the first two images\n-look alike while the third one is always different.\n-\"\"\"\n-\n-\n-def visualize(anchor, positive, negative):\n-    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n-\n-    def show(ax, image):\n-        ax.imshow(image)\n-        ax.get_xaxis().set_visible(False)\n-        ax.get_yaxis().set_visible(False)\n-\n-    fig = plt.figure(figsize=(9, 9))\n-\n-    axs = fig.subplots(3, 3)\n-    for i in range(3):\n-        show(axs[i, 0], anchor[i])\n-        show(axs[i, 1], positive[i])\n-        show(axs[i, 2], negative[i])\n-\n-\n-visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])\n-\n-\"\"\"\n-## Setting up the embedding generator model\n-\n-Our Siamese Network will generate embeddings for each of the images of the\n-triplet. To do this, we will use a ResNet50 model pretrained on ImageNet and\n-connect a few `Dense` layers to it so we can learn to separate these\n-embeddings.\n-\n-We will freeze the weights of all the layers of the model up until the layer `conv5_block1_out`.\n-This is important to avoid affecting the weights that the model has already learned.\n-We are going to leave the bottom few layers trainable, so that we can fine-tune their weights\n-during training.\n-\"\"\"\n-\n-base_cnn = resnet.ResNet50(\n-    weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n-)\n-\n-flatten = layers.Flatten()(base_cnn.output)\n-dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n-dense1 = layers.BatchNormalization()(dense1)\n-dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n-dense2 = layers.BatchNormalization()(dense2)\n-output = layers.Dense(256)(dense2)\n-\n-embedding = Model(base_cnn.input, output, name=\"Embedding\")\n-\n-trainable = False\n-for layer in base_cnn.layers:\n-    if layer.name == \"conv5_block1_out\":\n-        trainable = True\n-    layer.trainable = trainable\n-\n-\"\"\"\n-## Setting up the Siamese Network model\n-\n-The Siamese network will receive each of the triplet images as an input,\n-generate the embeddings, and output the distance between the anchor and the\n-positive embedding, as well as the distance between the anchor and the negative\n-embedding.\n-\n-To compute the distance, we can use a custom layer `DistanceLayer` that\n-returns both values as a tuple.\n-\"\"\"\n-\n-\n-class DistanceLayer(layers.Layer):\n-    \"\"\"\n-    This layer is responsible for computing the distance between the anchor\n-    embedding and the positive embedding, and the anchor embedding and the\n-    negative embedding.\n-    \"\"\"\n-\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-\n-    def call(self, anchor, positive, negative):\n-        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n-        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n-        return (ap_distance, an_distance)\n-\n-\n-anchor_input = layers.Input(name=\"anchor\", shape=target_shape + (3,))\n-positive_input = layers.Input(name=\"positive\", shape=target_shape + (3,))\n-negative_input = layers.Input(name=\"negative\", shape=target_shape + (3,))\n-\n-distances = DistanceLayer()(\n-    embedding(resnet.preprocess_input(anchor_input)),\n-    embedding(resnet.preprocess_input(positive_input)),\n-    embedding(resnet.preprocess_input(negative_input)),\n-)\n-\n-siamese_network = Model(\n-    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n-)\n-\n-\"\"\"\n-## Putting everything together\n-\n-We now need to implement a model with custom training loop so we can compute\n-the triplet loss using the three embeddings produced by the Siamese network.\n-\n-Let's create a `Mean` metric instance to track the loss of the training process.\n-\"\"\"\n-\n-\n-class SiameseModel(Model):\n-    \"\"\"The Siamese Network model with a custom training and testing loops.\n-\n-    Computes the triplet loss using the three embeddings produced by the\n-    Siamese Network.\n-\n-    The triplet loss is defined as:\n-       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n-    \"\"\"\n-\n-    def __init__(self, siamese_network, margin=0.5):\n-        super().__init__()\n-        self.siamese_network = siamese_network\n-        self.margin = margin\n-        self.loss_tracker = metrics.Mean(name=\"loss\")\n-\n-    def call(self, inputs):\n-        return self.siamese_network(inputs)\n-\n-    def train_step(self, data):\n-        # GradientTape is a context manager that records every operation that\n-        # you do inside. We are using it here to compute the loss so we can get\n-        # the gradients and apply them using the optimizer specified in\n-        # `compile()`.\n-        with tf.GradientTape() as tape:\n-            loss = self._compute_loss(data)\n-\n-        # Storing the gradients of the loss function with respect to the\n-        # weights/parameters.\n-        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n-\n-        # Applying the gradients on the model using the specified optimizer\n-        self.optimizer.apply_gradients(\n-            zip(gradients, self.siamese_network.trainable_weights)\n-        )\n-\n-        # Let's update and return the training loss metric.\n-        self.loss_tracker.update_state(loss)\n-        return {\"loss\": self.loss_tracker.result()}\n-\n-    def test_step(self, data):\n-        loss = self._compute_loss(data)\n-\n-        # Let's update and return the loss metric.\n-        self.loss_tracker.update_state(loss)\n-        return {\"loss\": self.loss_tracker.result()}\n-\n-    def _compute_loss(self, data):\n-        # The output of the network is a tuple containing the distances\n-        # between the anchor and the positive example, and the anchor and\n-        # the negative example.\n-        ap_distance, an_distance = self.siamese_network(data)\n-\n-        # Computing the Triplet Loss by subtracting both distances and\n-        # making sure we don't get a negative value.\n-        loss = ap_distance - an_distance\n-        loss = tf.maximum(loss + self.margin, 0.0)\n-        return loss\n-\n-    @property\n-    def metrics(self):\n-        # We need to list our metrics here so the `reset_states()` can be\n-        # called automatically.\n-        return [self.loss_tracker]\n-\n-\n-\"\"\"\n-## Training\n-\n-We are now ready to train our model.\n-\"\"\"\n-\n-siamese_model = SiameseModel(siamese_network)\n-siamese_model.compile(optimizer=optimizers.Adam(0.0001))\n-siamese_model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n-\n-\"\"\"\n-## Inspecting what the network has learned\n-\n-At this point, we can check how the network learned to separate the embeddings\n-depending on whether they belong to similar images.\n-\n-We can use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to measure the\n-similarity between embeddings.\n-\n-Let's pick a sample from the dataset to check the similarity between the\n-embeddings generated for each image.\n-\"\"\"\n-sample = next(iter(train_dataset))\n-visualize(*sample)\n-\n-anchor, positive, negative = sample\n-anchor_embedding, positive_embedding, negative_embedding = (\n-    embedding(resnet.preprocess_input(anchor)),\n-    embedding(resnet.preprocess_input(positive)),\n-    embedding(resnet.preprocess_input(negative)),\n-)\n-\n-\"\"\"\n-Finally, we can compute the cosine similarity between the anchor and positive\n-images and compare it with the similarity between the anchor and the negative\n-images.\n-\n-We should expect the similarity between the anchor and positive images to be\n-larger than the similarity between the anchor and the negative images.\n-\"\"\"\n-\n-cosine_similarity = metrics.CosineSimilarity()\n-\n-positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n-print(\"Positive similarity:\", positive_similarity.numpy())\n-\n-negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n-print(\"Negative similarity\", negative_similarity.numpy())\n-\n-\n-\"\"\"\n-## Summary\n-\n-1. The `tf.data` API enables you to build efficient input pipelines for your model. It is\n-particularly useful if you have a large dataset. You can learn more about `tf.data`\n-pipelines in [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data).\n-\n-2. In this example, we use a pre-trained ResNet50 as part of the subnetwork that generates\n-the feature embeddings. By using [transfer learning](https://www.tensorflow.org/guide/keras/transfer_learning?hl=en),\n-we can significantly reduce the training time and size of the dataset.\n-\n-3. Notice how we are [fine-tuning](https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#fine-tuning)\n-the weights of the final layers of the ResNet50 network but keeping the rest of the layers untouched.\n-Using the name assigned to each layer, we can freeze the weights to a certain point and keep the last few layers open.\n-\n-4. We can create custom layers by creating a class that inherits from `tf.keras.layers.Layer`,\n-as we did in the `DistanceLayer` class.\n-\n-5. We used a cosine similarity metric to measure how to 2 output embeddings are similar to each other.\n-\n-6. You can implement a custom training loop by overriding the `train_step()` method. `train_step()` uses\n-[`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape),\n-which records every operation that you perform inside it. In this example, we use it to access the\n-gradients passed to the optimizer to update the model weights at every step. For more details, check out the\n-[Intro to Keras for researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n-and [Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch?hl=en).\n-\n-\"\"\"\n\n@@ -1,443 +0,0 @@\n-\"\"\"\n-Title: Self-supervised contrastive learning with SimSiam\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/03/19\n-Last modified: 2021/03/20\n-Description: Implementation of a self-supervised learning method for computer vision.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-Self-supervised learning (SSL) is an interesting branch of study in the field of\n-representation learning. SSL systems try to formulate a supervised signal from a corpus\n-of unlabeled data points.  An example is we train a deep neural network to predict the\n-next word from a given set of words. In literature, these tasks are known as *pretext\n-tasks* or *auxiliary tasks*. If we [train such a network](https://arxiv.org/abs/1801.06146) on a huge dataset (such as\n-the [Wikipedia text corpus](https://www.corpusdata.org/wikipedia.asp)) it learns very effective\n-representations that transfer well to downstream tasks. Language models like\n-[BERT](https://arxiv.org/abs/1810.04805), [GPT-3](https://arxiv.org/abs/2005.14165),\n-[ELMo](https://allennlp.org/elmo) all benefit from this.\n-\n-Much like the language models we can train computer vision models using similar\n-approaches. To make things work in computer vision, we need to formulate the learning\n-tasks such that the underlying model (a deep neural network) is able to make sense of the\n-semantic information present in vision data. One such task is to a model to _contrast_\n-between two different versions of the same image. The hope is that in this way the model\n-will have learn representations where the similar images are grouped as together possible\n-while the dissimilar images are further away.\n-\n-In this example, we will be implementing one such system called **SimSiam** proposed in\n-[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566). It\n-is implemented as the following:\n-\n-1. We create two different versions of the same dataset with a stochastic data\n-augmentation pipeline. Note that the random initialization seed needs to be the same\n-during create these versions.\n-2. We take a ResNet without any classification head (**backbone**) and we add a shallow\n-fully-connected network (**projection head**) on top of it. Collectively, this is known\n-as the **encoder**.\n-3. We pass the output of the encoder through a **predictor** which is again a shallow\n-fully-connected network having an\n-[AutoEncoder](https://en.wikipedia.org/wiki/Autoencoder) like structure.\n-4. We then train our encoder to maximize the cosine similarity between the two different\n-versions of our dataset.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-os.environ['KERAS_BACKEND'] = 'tensorflow'\n-\n-from keras import layers\n-from keras import regularizers\n-import keras\n-import tensorflow as tf\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-AUTO = tf.data.AUTOTUNE\n-BATCH_SIZE = 128\n-EPOCHS = 5\n-CROP_TO = 32\n-SEED = 26\n-\n-PROJECT_DIM = 2048\n-LATENT_DIM = 512\n-WEIGHT_DECAY = 0.0005\n-\n-\"\"\"\n-## Load the CIFAR-10 dataset\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-print(f\"Total training examples: {len(x_train)}\")\n-print(f\"Total test examples: {len(x_test)}\")\n-\n-\"\"\"\n-## Defining our data augmentation pipeline\n-\n-As studied in [SimCLR](https://arxiv.org/abs/2002.05709) having the right data\n-augmentation pipeline is critical for SSL systems to work effectively in computer vision.\n-Two particular augmentation transforms that seem to matter the most are: 1.) Random\n-resized crops and 2.) Color distortions. Most of the other SSL systems for computer\n-vision (such as [BYOL](https://arxiv.org/abs/2006.07733),\n-[MoCoV2](https://arxiv.org/abs/2003.04297), [SwAV](https://arxiv.org/abs/2006.09882),\n-etc.) include these in their training pipelines.\n-\"\"\"\n-\n-\n-def flip_random_crop(image):\n-    # With random crops we also apply horizontal flipping.\n-    image = tf.image.random_flip_left_right(image)\n-    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n-    return image\n-\n-\n-def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):\n-    x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])\n-    x = tf.image.random_contrast(\n-        x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]\n-    )\n-    x = tf.image.random_saturation(\n-        x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]\n-    )\n-    x = tf.image.random_hue(x, max_delta=0.2 * strength[3])\n-    # Affine transformations can disturb the natural range of\n-    # RGB images, hence this is needed.\n-    x = tf.clip_by_value(x, 0, 255)\n-    return x\n-\n-\n-def color_drop(x):\n-    x = tf.image.rgb_to_grayscale(x)\n-    x = tf.tile(x, [1, 1, 3])\n-    return x\n-\n-\n-def random_apply(func, x, p):\n-    if tf.random.uniform([], minval=0, maxval=1) < p:\n-        return func(x)\n-    else:\n-        return x\n-\n-\n-def custom_augment(image):\n-    # As discussed in the SimCLR paper, the series of augmentation\n-    # transformations (except for random crops) need to be applied\n-    # randomly to impose translational invariance.\n-    image = flip_random_crop(image)\n-    image = random_apply(color_jitter, image, p=0.8)\n-    image = random_apply(color_drop, image, p=0.2)\n-    return image\n-\n-\n-\"\"\"\n-It should be noted that an augmentation pipeline is generally dependent on various\n-properties of the dataset we are dealing with. For example, if images in the dataset are\n-heavily object-centric then taking random crops with a very high probability may hurt the\n-training performance.\n-\n-Let's now apply our augmentation pipeline to our dataset and visualize a few outputs.\n-\"\"\"\n-\n-\"\"\"\n-## Convert the data into TensorFlow `Dataset` objects\n-\n-Here we create two different versions of our dataset *without* any ground-truth labels.\n-\"\"\"\n-\n-ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train)\n-ssl_ds_one = (\n-    ssl_ds_one.shuffle(1024, seed=SEED)\n-    .map(custom_augment, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train)\n-ssl_ds_two = (\n-    ssl_ds_two.shuffle(1024, seed=SEED)\n-    .map(custom_augment, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-# We then zip both of these datasets.\n-ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))\n-\n-# Visualize a few augmented images.\n-sample_images_one = next(iter(ssl_ds_one))\n-plt.figure(figsize=(10, 10))\n-for n in range(25):\n-    ax = plt.subplot(5, 5, n + 1)\n-    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n-    plt.axis(\"off\")\n-plt.show()\n-\n-# Ensure that the different versions of the dataset actually contain\n-# identical images.\n-sample_images_two = next(iter(ssl_ds_two))\n-plt.figure(figsize=(10, 10))\n-for n in range(25):\n-    ax = plt.subplot(5, 5, n + 1)\n-    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n-    plt.axis(\"off\")\n-plt.show()\n-\n-\"\"\"\n-Notice that the images in `samples_images_one` and `sample_images_two` are essentially\n-the same but are augmented differently.\n-\"\"\"\n-\n-\"\"\"\n-## Defining the encoder and the predictor\n-\n-We use an implementation of ResNet20 that is specifically configured for the CIFAR10\n-dataset. The code is taken from the\n-[keras-idiomatic-programmer](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/zoo/resnet/resnet_cifar10_v2.py) repository. The hyperparameters of\n-these architectures have been referred from Section 3 and Appendix A of [the original\n-paper](https://arxiv.org/abs/2011.10566).\n-\"\"\"\n-\n-\"\"\"shell\n-wget -q https://shorturl.at/QS369 -O resnet_cifar10_v2.py\n-\"\"\"\n-\n-import resnet_cifar10_v2\n-\n-N = 2\n-DEPTH = N * 9 + 2\n-NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n-\n-\n-def get_encoder():\n-    # Input and backbone.\n-    inputs = layers.Input((CROP_TO, CROP_TO, 3))\n-    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n-    x = resnet_cifar10_v2.stem(x)\n-    x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n-    x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n-\n-    # Projection head.\n-    x = layers.Dense(\n-        PROJECT_DIM,\n-        use_bias=False,\n-        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.ReLU()(x)\n-    x = layers.Dense(\n-        PROJECT_DIM,\n-        use_bias=False,\n-        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-    )(x)\n-    outputs = layers.BatchNormalization()(x)\n-    return keras.Model(inputs, outputs, name=\"encoder\")\n-\n-\n-def get_predictor():\n-    model = keras.Sequential(\n-        [\n-            # Note the AutoEncoder-like structure.\n-            layers.Input((PROJECT_DIM,)),\n-            layers.Dense(\n-                LATENT_DIM,\n-                use_bias=False,\n-                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n-            ),\n-            layers.ReLU(),\n-            layers.BatchNormalization(),\n-            layers.Dense(PROJECT_DIM),\n-        ],\n-        name=\"predictor\",\n-    )\n-    return model\n-\n-\n-\"\"\"\n-## Defining the (pre-)training loop\n-\n-One of the main reasons behind training networks with these kinds of approaches is to\n-utilize the learned representations for downstream tasks like classification. This is why\n-this particular training phase is also referred to as _pre-training_.\n-\n-We start by defining the loss function.\n-\"\"\"\n-\n-\n-def compute_loss(p, z):\n-    # The authors of SimSiam emphasize the impact of\n-    # the `stop_gradient` operator in the paper as it\n-    # has an important role in the overall optimization.\n-    z = tf.stop_gradient(z)\n-    p = tf.math.l2_normalize(p, axis=1)\n-    z = tf.math.l2_normalize(z, axis=1)\n-    # Negative cosine similarity (minimizing this is\n-    # equivalent to maximizing the similarity).\n-    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))\n-\n-\n-\"\"\"\n-We then define our training loop by overriding the `train_step()` function of the\n-`keras.Model` class.\n-\"\"\"\n-\n-\n-class SimSiam(keras.Model):\n-    def __init__(self, encoder, predictor):\n-        super().__init__()\n-        self.encoder = encoder\n-        self.predictor = predictor\n-        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [self.loss_tracker]\n-\n-    def train_step(self, data):\n-        # Unpack the data.\n-        ds_one, ds_two = data\n-\n-        # Forward pass through the encoder and predictor.\n-        with tf.GradientTape() as tape:\n-            z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n-            p1, p2 = self.predictor(z1), self.predictor(z2)\n-            # Note that here we are enforcing the network to match\n-            # the representations of two differently augmented batches\n-            # of data.\n-            loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n-\n-        # Compute gradients and update the parameters.\n-        learnable_params = (\n-            self.encoder.trainable_variables\n-            + self.predictor.trainable_variables\n-        )\n-        gradients = tape.gradient(loss, learnable_params)\n-        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n-\n-        # Monitor loss.\n-        self.loss_tracker.update_state(loss)\n-        return {\"loss\": self.loss_tracker.result()}\n-\n-\n-\"\"\"\n-## Pre-training our networks\n-\n-In the interest of this example, we will train the model for only 5 epochs. In reality,\n-this should at least be 100 epochs.\n-\"\"\"\n-\n-# Create a cosine decay learning scheduler.\n-num_training_samples = len(x_train)\n-steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n-lr_decayed_fn = keras.optimizers.schedules.CosineDecay(\n-    initial_learning_rate=0.03, decay_steps=steps\n-)\n-\n-# Create an early stopping callback.\n-early_stopping = keras.callbacks.EarlyStopping(\n-    monitor=\"loss\", patience=5, restore_best_weights=True\n-)\n-\n-# Compile model and start training.\n-simsiam = SimSiam(get_encoder(), get_predictor())\n-simsiam.compile(optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.6))\n-history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])\n-\n-# Visualize the training progress of the model.\n-plt.plot(history.history[\"loss\"])\n-plt.grid()\n-plt.title(\"Negative Cosine Similairty\")\n-plt.show()\n-\n-\"\"\"\n-If your solution gets very close to -1 (minimum value of our loss) very quickly with a\n-different dataset and a different backbone architecture that is likely because of\n-*representation collapse*. It is a phenomenon where the encoder yields similar output for\n-all the images. In that case additional hyperparameter tuning is required especially in\n-the following areas:\n-\n-* Strength of the color distortions and their probabilities.\n-* Learning rate and its schedule.\n-* Architecture of both the backbone and their projection head.\n-\n-\"\"\"\n-\n-\"\"\"\n-## Evaluating our SSL method\n-\n-The most popularly used method to evaluate a SSL method in computer vision (or any other\n-pre-training method as such) is to learn a linear classifier on the frozen features of\n-the trained backbone model (in this case it is ResNet20) and evaluate the classifier on\n-unseen images. Other methods include\n-[fine-tuning](https://keras.io/guides/transfer_learning/) on the source dataset or even a\n-target dataset with 5% or 10% labels present. Practically, we can use the backbone model\n-for any downstream task such as semantic segmentation, object detection, and so on where\n-the backbone models are usually pre-trained with *pure supervised learning*.\n-\"\"\"\n-\n-# We first create labeled `Dataset` objects.\n-train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n-\n-# Then we shuffle, batch, and prefetch this dataset for performance. We\n-# also apply random resized crops as an augmentation but only to the\n-# training set.\n-train_ds = (\n-    train_ds.shuffle(1024)\n-    .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n-\n-# Extract the backbone ResNet20.\n-backbone = keras.Model(\n-    simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n-)\n-\n-# We then create our linear classifier and train it.\n-backbone.trainable = False\n-inputs = layers.Input((CROP_TO, CROP_TO, 3))\n-x = backbone(inputs, training=False)\n-outputs = layers.Dense(10, activation=\"softmax\")(x)\n-linear_model = keras.Model(inputs, outputs, name=\"linear_model\")\n-\n-# Compile model and start training.\n-linear_model.compile(\n-    loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"accuracy\"],\n-    optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n-)\n-history = linear_model.fit(\n-    train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping]\n-)\n-_, test_acc = linear_model.evaluate(test_ds)\n-print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n-\n-\"\"\"\n-\n-## Notes\n-* More data and longer pre-training schedule benefit SSL in general.\n-* SSL is particularly very helpful when you do not have access to very limited *labeled*\n-training data but you can manage to build a large corpus of unlabeled data. Recently,\n-using an SSL method called [SwAV](https://arxiv.org/abs/2006.09882), a group of\n-researchers at Facebook trained a [RegNet](https://arxiv.org/abs/2006.09882) on 2 Billion\n-images. They were able to achieve downstream performance very close to those achieved by\n-pure supervised pre-training. For some downstream tasks, their method even outperformed\n-the supervised counterparts. You can check out [their\n-paper](https://arxiv.org/pdf/2103.01988.pdf) to know the details.\n-* If you are interested to understand why contrastive SSL helps networks learn meaningful\n-representations, you can check out the following resources:\n-   * [Self-supervised learning: The dark matter of\n-intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)\n-   * [Understanding self-supervised learning using controlled datasets with known\n-structure](https://sslneuips20.github.io/files/CameraReadys%203-77/64/CameraReady/Understanding_self_supervised_learning.pdf)\n-\n-\"\"\"\n\n@@ -1,583 +0,0 @@\n-\"\"\"\n-Title: Image classification with Swin Transformers\n-Author: [Rishit Dagli](https://twitter.com/rishit_dagli)\n-Date created: 2021/09/08\n-Last modified: 2021/09/08\n-Description: Image classification using Swin Transformers, a general-purpose backbone for computer vision.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n-by Liu et al. for image classification, and demonstrates it on the\n-[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n-\n-Swin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\n-for computer vision. Swin Transformer is a hierarchical Transformer whose\n-representations are computed with _shifted windows_. The shifted window scheme\n-brings greater efficiency by limiting self-attention computation to\n-non-overlapping local windows while also allowing for cross-window connections.\n-This architecture has the flexibility to model information at various scales and has\n-a linear computational complexity with respect to image size.\n-\n-This example requires TensorFlow 2.5 or higher.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import tensorflow as tf\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Prepare the data\n-\n-We load the CIFAR-100 dataset through `tf.keras.datasets`,\n-normalize the images, and convert the integer labels to one-hot encoded vectors.\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-x_train, x_test = x_train / 255.0, x_test / 255.0\n-y_train = keras.utils.numerical_utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.numerical_utils.to_categorical(y_test, num_classes)\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-plt.figure(figsize=(10, 10))\n-for i in range(25):\n-    plt.subplot(5, 5, i + 1)\n-    plt.xticks([])\n-    plt.yticks([])\n-    plt.grid(False)\n-    plt.imshow(x_train[i])\n-plt.show()\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\n-A key parameter to pick is the `patch_size`, the size of the input patches.\n-In order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\n-Below, we take inspiration from the original paper settings\n-for training on ImageNet-1K, keeping most of the original settings for this example.\n-\"\"\"\n-\n-patch_size = (2, 2)  # 2-by-2 sized patches\n-dropout_rate = 0.03  # Dropout rate\n-num_heads = 8  # Attention heads\n-embed_dim = 64  # Embedding dimension\n-num_mlp = 256  # MLP layer size\n-qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n-window_size = 2  # Size of attention window\n-shift_size = 1  # Size of shifting window\n-image_dimension = 32  # Initial image size\n-\n-num_patch_x = input_shape[0] // patch_size[0]\n-num_patch_y = input_shape[1] // patch_size[1]\n-\n-learning_rate = 1e-3\n-batch_size = 128\n-num_epochs = 1\n-validation_split = 0.1\n-weight_decay = 0.0001\n-label_smoothing = 0.1\n-\n-\"\"\"\n-## Helper functions\n-\n-We create two helper functions to help us get a sequence of\n-patches from the image, merge patches, and apply dropout.\n-\"\"\"\n-\n-\n-def window_partition(x, window_size):\n-    _, height, width, channels = x.shape\n-    patch_num_y = height // window_size\n-    patch_num_x = width // window_size\n-    x = tf.reshape(\n-        x,\n-        shape=(\n-            -1,\n-            patch_num_y,\n-            window_size,\n-            patch_num_x,\n-            window_size,\n-            channels,\n-        ),\n-    )\n-    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n-    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n-    return windows\n-\n-\n-def window_reverse(windows, window_size, height, width, channels):\n-    patch_num_y = height // window_size\n-    patch_num_x = width // window_size\n-    x = tf.reshape(\n-        windows,\n-        shape=(\n-            -1,\n-            patch_num_y,\n-            patch_num_x,\n-            window_size,\n-            window_size,\n-            channels,\n-        ),\n-    )\n-    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n-    x = tf.reshape(x, shape=(-1, height, width, channels))\n-    return x\n-\n-\n-class DropPath(layers.Layer):\n-    def __init__(self, drop_prob=None, **kwargs):\n-        super().__init__(**kwargs)\n-        self.drop_prob = drop_prob\n-\n-    def call(self, x):\n-        input_shape = tf.shape(x)\n-        batch_size = input_shape[0]\n-        rank = x.shape.rank\n-        shape = (batch_size,) + (1,) * (rank - 1)\n-        random_tensor = (1 - self.drop_prob) + tf.random.uniform(\n-            shape, dtype=x.dtype\n-        )\n-        path_mask = tf.floor(random_tensor)\n-        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n-        return output\n-\n-\n-\"\"\"\n-## Window based multi-head self-attention\n-\n-Usually Transformers perform global self-attention, where the relationships between\n-a token and all other tokens are computed. The global computation leads to quadratic\n-complexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\n-suggests, we compute self-attention within local windows, in a non-overlapping manner.\n-Global self-attention leads to quadratic computational complexity in the number of patches,\n-whereas window-based self-attention leads to linear complexity and is easily scalable.\n-\"\"\"\n-\n-\n-class WindowAttention(layers.Layer):\n-    def __init__(\n-        self,\n-        dim,\n-        window_size,\n-        num_heads,\n-        qkv_bias=True,\n-        dropout_rate=0.0,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.dim = dim\n-        self.window_size = window_size\n-        self.num_heads = num_heads\n-        self.scale = (dim // num_heads) ** -0.5\n-        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n-        self.dropout = layers.Dropout(dropout_rate)\n-        self.proj = layers.Dense(dim)\n-\n-    def build(self, input_shape):\n-        num_window_elements = (2 * self.window_size[0] - 1) * (\n-            2 * self.window_size[1] - 1\n-        )\n-        self.relative_position_bias_table = self.add_weight(\n-            shape=(num_window_elements, self.num_heads),\n-            initializer=tf.initializers.Zeros(),\n-            trainable=True,\n-        )\n-        coords_h = np.arange(self.window_size[0])\n-        coords_w = np.arange(self.window_size[1])\n-        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n-        coords = np.stack(coords_matrix)\n-        coords_flatten = coords.reshape(2, -1)\n-        relative_coords = (\n-            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        )\n-        relative_coords = relative_coords.transpose([1, 2, 0])\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-\n-        self.relative_position_index = tf.Variable(\n-            initial_value=lambda: tf.convert_to_tensor(relative_position_index),\n-            trainable=False,\n-        )\n-\n-    def call(self, x, mask=None):\n-        _, size, channels = x.shape\n-        head_dim = channels // self.num_heads\n-        x_qkv = self.qkv(x)\n-        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n-        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n-        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n-        q = q * self.scale\n-        k = tf.transpose(k, perm=(0, 1, 3, 2))\n-        attn = q @ k\n-\n-        num_window_elements = self.window_size[0] * self.window_size[1]\n-        relative_position_index_flat = tf.reshape(\n-            self.relative_position_index, shape=(-1,)\n-        )\n-        relative_position_bias = tf.gather(\n-            self.relative_position_bias_table, relative_position_index_flat\n-        )\n-        relative_position_bias = tf.reshape(\n-            relative_position_bias,\n-            shape=(num_window_elements, num_window_elements, -1),\n-        )\n-        relative_position_bias = tf.transpose(\n-            relative_position_bias, perm=(2, 0, 1)\n-        )\n-        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n-\n-        if mask is not None:\n-            nW = mask.shape[0]\n-            mask_float = tf.cast(\n-                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n-            )\n-            attn = (\n-                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n-                + mask_float\n-            )\n-            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n-            attn = keras.activations.softmax(attn, axis=-1)\n-        else:\n-            attn = keras.activations.softmax(attn, axis=-1)\n-        attn = self.dropout(attn)\n-\n-        x_qkv = attn @ v\n-        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n-        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n-        x_qkv = self.proj(x_qkv)\n-        x_qkv = self.dropout(x_qkv)\n-        return x_qkv\n-\n-\n-\"\"\"\n-## The complete Swin Transformer model\n-\n-Finally, we put together the complete Swin Transformer by replacing the standard multi-head\n-attention (MHA) with shifted windows attention. As suggested in the\n-original paper, we create a model comprising of a shifted window-based MHA\n-layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n-`LayerNormalization` before each MSA layer and each MLP, and a residual\n-connection after each of these layers.\n-\n-Notice that we only create a simple MLP with 2 Dense and\n-2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\n-quite standard in the literature. However in this paper the authors use a\n-2-layer MLP with GELU nonlinearity in between.\n-\"\"\"\n-\n-\n-class SwinTransformer(layers.Layer):\n-    def __init__(\n-        self,\n-        dim,\n-        num_patch,\n-        num_heads,\n-        window_size=7,\n-        shift_size=0,\n-        num_mlp=1024,\n-        qkv_bias=True,\n-        dropout_rate=0.0,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        self.dim = dim  # number of input dimensions\n-        self.num_patch = num_patch  # number of embedded patches\n-        self.num_heads = num_heads  # number of attention heads\n-        self.window_size = window_size  # size of window\n-        self.shift_size = shift_size  # size of window shift\n-        self.num_mlp = num_mlp  # number of MLP nodes\n-\n-        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n-        self.attn = WindowAttention(\n-            dim,\n-            window_size=(self.window_size, self.window_size),\n-            num_heads=num_heads,\n-            qkv_bias=qkv_bias,\n-            dropout_rate=dropout_rate,\n-        )\n-        self.drop_path = DropPath(dropout_rate)\n-        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n-\n-        self.mlp = keras.Sequential(\n-            [\n-                layers.Dense(num_mlp),\n-                layers.Activation(keras.activations.gelu),\n-                layers.Dropout(dropout_rate),\n-                layers.Dense(dim),\n-                layers.Dropout(dropout_rate),\n-            ]\n-        )\n-\n-        if min(self.num_patch) < self.window_size:\n-            self.shift_size = 0\n-            self.window_size = min(self.num_patch)\n-\n-    def build(self, input_shape):\n-        if self.shift_size == 0:\n-            self.attn_mask = None\n-        else:\n-            height, width = self.num_patch\n-            h_slices = (\n-                slice(0, -self.window_size),\n-                slice(-self.window_size, -self.shift_size),\n-                slice(-self.shift_size, None),\n-            )\n-            w_slices = (\n-                slice(0, -self.window_size),\n-                slice(-self.window_size, -self.shift_size),\n-                slice(-self.shift_size, None),\n-            )\n-            mask_array = np.zeros((1, height, width, 1))\n-            count = 0\n-            for h in h_slices:\n-                for w in w_slices:\n-                    mask_array[:, h, w, :] = count\n-                    count += 1\n-            mask_array = tf.convert_to_tensor(mask_array)\n-\n-            # mask array to windows\n-            mask_windows = window_partition(mask_array, self.window_size)\n-            mask_windows = tf.reshape(\n-                mask_windows, shape=[-1, self.window_size * self.window_size]\n-            )\n-            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n-                mask_windows, axis=2\n-            )\n-            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n-            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n-            self.attn_mask = tf.Variable(\n-                initial_value=attn_mask, trainable=False\n-            )\n-\n-    def call(self, x):\n-        height, width = self.num_patch\n-        _, num_patches_before, channels = x.shape\n-        x_skip = x\n-        x = self.norm1(x)\n-        x = tf.reshape(x, shape=(-1, height, width, channels))\n-        if self.shift_size > 0:\n-            shifted_x = tf.roll(\n-                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n-            )\n-        else:\n-            shifted_x = x\n-\n-        x_windows = window_partition(shifted_x, self.window_size)\n-        x_windows = tf.reshape(\n-            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n-        )\n-        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n-\n-        attn_windows = tf.reshape(\n-            attn_windows,\n-            shape=(-1, self.window_size, self.window_size, channels),\n-        )\n-        shifted_x = window_reverse(\n-            attn_windows, self.window_size, height, width, channels\n-        )\n-        if self.shift_size > 0:\n-            x = tf.roll(\n-                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n-            )\n-        else:\n-            x = shifted_x\n-\n-        x = tf.reshape(x, shape=(-1, height * width, channels))\n-        x = self.drop_path(x)\n-        x = x_skip + x\n-        x_skip = x\n-        x = self.norm2(x)\n-        x = self.mlp(x)\n-        x = self.drop_path(x)\n-        x = x_skip + x\n-        return x\n-\n-\n-\"\"\"\n-## Model training and evaluation\n-\n-### Extract and embed patches\n-\n-We first create 3 layers to help us extract, embed and merge patches from the\n-images on top of which we will later use the Swin Transformer class we built.\n-\"\"\"\n-\n-\n-class PatchExtract(layers.Layer):\n-    def __init__(self, patch_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.patch_size_x = patch_size[0]\n-        self.patch_size_y = patch_size[0]\n-\n-    def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n-            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n-            rates=(1, 1, 1, 1),\n-            padding=\"VALID\",\n-        )\n-        patch_dim = patches.shape[-1]\n-        patch_num = patches.shape[1]\n-        return tf.reshape(\n-            patches, (batch_size, patch_num * patch_num, patch_dim)\n-        )\n-\n-\n-class PatchEmbedding(layers.Layer):\n-    def __init__(self, num_patch, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_patch = num_patch\n-        self.proj = layers.Dense(embed_dim)\n-        self.pos_embed = layers.Embedding(\n-            input_dim=num_patch, output_dim=embed_dim\n-        )\n-\n-    def call(self, patch):\n-        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n-        return self.proj(patch) + self.pos_embed(pos)\n-\n-\n-class PatchMerging(keras.layers.Layer):\n-    def __init__(self, num_patch, embed_dim):\n-        super().__init__()\n-        self.num_patch = num_patch\n-        self.embed_dim = embed_dim\n-        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n-\n-    def call(self, x):\n-        height, width = self.num_patch\n-        _, _, C = x.shape\n-        x = tf.reshape(x, shape=(-1, height, width, C))\n-        x0 = x[:, 0::2, 0::2, :]\n-        x1 = x[:, 1::2, 0::2, :]\n-        x2 = x[:, 0::2, 1::2, :]\n-        x3 = x[:, 1::2, 1::2, :]\n-        x = tf.concat((x0, x1, x2, x3), axis=-1)\n-        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n-        return self.linear_trans(x)\n-\n-\n-\"\"\"\n-### Build the model\n-\n-We put together the Swin Transformer model.\n-\"\"\"\n-\n-input = layers.Input(input_shape)\n-x = layers.RandomCrop(image_dimension, image_dimension)(input)\n-x = layers.RandomFlip(\"horizontal\")(x)\n-x = PatchExtract(patch_size)(x)\n-x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n-x = SwinTransformer(\n-    dim=embed_dim,\n-    num_patch=(num_patch_x, num_patch_y),\n-    num_heads=num_heads,\n-    window_size=window_size,\n-    shift_size=0,\n-    num_mlp=num_mlp,\n-    qkv_bias=qkv_bias,\n-    dropout_rate=dropout_rate,\n-)(x)\n-x = SwinTransformer(\n-    dim=embed_dim,\n-    num_patch=(num_patch_x, num_patch_y),\n-    num_heads=num_heads,\n-    window_size=window_size,\n-    shift_size=shift_size,\n-    num_mlp=num_mlp,\n-    qkv_bias=qkv_bias,\n-    dropout_rate=dropout_rate,\n-)(x)\n-x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n-x = layers.GlobalAveragePooling1D()(x)\n-output = layers.Dense(num_classes, activation=\"softmax\")(x)\n-\n-\"\"\"\n-### Train on CIFAR-100\n-\n-We train the model on CIFAR-100. Here, we only train the model\n-for 40 epochs to keep the training time short in this example.\n-In practice, you should train for 150 epochs to reach convergence.\n-\"\"\"\n-\n-model = keras.Model(input, output)\n-model.compile(\n-    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n-    optimizer=keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    ),\n-    metrics=[\n-        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-    ],\n-)\n-\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=num_epochs,\n-    validation_split=validation_split,\n-)\n-\n-\"\"\"\n-Let's visualize the training progress of the model.\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"train_loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n-plt.xlabel(\"Epochs\")\n-plt.ylabel(\"Loss\")\n-plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n-plt.legend()\n-plt.grid()\n-plt.show()\n-\n-\"\"\"\n-Let's display the final results of the training on CIFAR-100.\n-\"\"\"\n-\n-loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-print(f\"Test loss: {round(loss, 2)}\")\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-\"\"\"\n-The Swin Transformer model we just trained has just 152K parameters, and it gets\n-us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting\n-as well as seen in above graph. This means we can train this network for longer\n-(perhaps with a bit more regularization) and obtain even better performance.\n-This performance can further be improved by additional techniques like cosine\n-decay learning rate schedule, other data augmentation techniques. While experimenting,\n-I tried training the model for 150 epochs with a slightly higher dropout and greater\n-embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100\n-as you can see in the screenshot.\n-\n-![Results of training for longer](https://i.imgur.com/9vnQesZ.png)\n-\n-The authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present\n-a number of experiments to study how input sizes, optimizers etc. affect the final\n-performance of this model. The authors further present using this model for object detection,\n-semantic segmentation and instance segmentation as well and report competitive results\n-for these. You are strongly advised to also check out the\n-[original paper](https://arxiv.org/abs/2103.14030).\n-\n-This example takes inspiration from the official\n-[PyTorch](https://github.com/microsoft/Swin-Transformer) and\n-[TensorFlow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow) implementations.\n-\"\"\"\n\n@@ -1,203 +0,0 @@\n-\"\"\"\n-Title: Visualizing what convnets learn\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/05/29\n-Last modified: 2020/05/29\n-Description: Displaying the visual patterns that convnet filters respond to.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-In this example, we look into what sort of visual patterns image classification models\n-learn. We'll be using the `ResNet50V2` model, trained on the ImageNet dataset.\n-\n-Our process is simple: we will create input images that maximize the activation of\n-specific filters in a target layer (picked somewhere in the middle of the model: layer\n-`conv3_block4_out`). Such images represent a visualization of the\n-pattern that the filter responds to.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-import numpy as np\n-import tensorflow as tf\n-\n-# The dimensions of our input image\n-img_width = 180\n-img_height = 180\n-# Our target layer: we will visualize the filters from this layer.\n-# See `model.summary()` for list of layer names, if you want to change this.\n-layer_name = \"conv3_block4_out\"\n-\n-\"\"\"\n-## Build a feature extraction model\n-\"\"\"\n-\n-# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\n-model = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)\n-\n-# Set up a model that returns the activation values for our target layer\n-layer = model.get_layer(name=layer_name)\n-feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)\n-\n-\"\"\"\n-## Set up the gradient ascent process\n-\n-The \"loss\" we will maximize is simply the mean of the activation of a specific filter in\n-our target layer. To avoid border effects, we exclude border pixels.\n-\"\"\"\n-\n-\n-def compute_loss(input_image, filter_index):\n-    activation = feature_extractor(input_image)\n-    # We avoid border artifacts by only involving non-border pixels in the loss.\n-    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n-    return tf.reduce_mean(filter_activation)\n-\n-\n-\"\"\"\n-Our gradient ascent function simply computes the gradients of the loss above\n-with regard to the input image, and update the update image so as to move it\n-towards a state that will activate the target filter more strongly.\n-\"\"\"\n-\n-\n-@tf.function\n-def gradient_ascent_step(img, filter_index, learning_rate):\n-    with tf.GradientTape() as tape:\n-        tape.watch(img)\n-        loss = compute_loss(img, filter_index)\n-    # Compute gradients.\n-    grads = tape.gradient(loss, img)\n-    # Normalize gradients.\n-    grads = tf.math.l2_normalize(grads)\n-    img += learning_rate * grads\n-    return loss, img\n-\n-\n-\"\"\"\n-## Set up the end-to-end filter visualization loop\n-\n-Our process is as follow:\n-\n-- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n-- Repeatedly apply the gradient ascent step function defined above\n-- Convert the resulting input image back to a displayable form, by normalizing it,\n-center-cropping it, and restricting it to the [0, 255] range.\n-\"\"\"\n-\n-\n-def initialize_image():\n-    # We start from a gray image with some random noise\n-    img = tf.random.uniform((1, img_width, img_height, 3))\n-    # ResNet50V2 expects inputs in the range [-1, +1].\n-    # Here we scale our random inputs to [-0.125, +0.125]\n-    return (img - 0.5) * 0.25\n-\n-\n-def visualize_filter(filter_index):\n-    # We run gradient ascent for 20 steps\n-    iterations = 30\n-    learning_rate = 10.0\n-    img = initialize_image()\n-    for iteration in range(iterations):\n-        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n-\n-    # Decode the resulting input image\n-    img = deprocess_image(img[0].numpy())\n-    return loss, img\n-\n-\n-def deprocess_image(img):\n-    # Normalize array: center on 0., ensure variance is 0.15\n-    img -= img.mean()\n-    img /= img.std() + 1e-5\n-    img *= 0.15\n-\n-    # Center crop\n-    img = img[25:-25, 25:-25, :]\n-\n-    # Clip to [0, 1]\n-    img += 0.5\n-    img = np.clip(img, 0, 1)\n-\n-    # Convert to RGB array\n-    img *= 255\n-    img = np.clip(img, 0, 255).astype(\"uint8\")\n-    return img\n-\n-\n-\"\"\"\n-Let's try it out with filter 0 in the target layer:\n-\"\"\"\n-\n-from IPython.display import Image, display\n-\n-loss, img = visualize_filter(0)\n-keras.utils.save_img(\"0.png\", img)\n-\n-\"\"\"\n-This is what an input that maximizes the response of filter 0 in the target layer would\n-look like:\n-\"\"\"\n-\n-display(Image(\"0.png\"))\n-\n-\"\"\"\n-## Visualize the first 64 filters in the target layer\n-\n-Now, let's make a 8x8 grid of the first 64 filters\n-in the target layer to get of feel for the range\n-of different visual patterns that the model has learned.\n-\"\"\"\n-\n-# Compute image inputs that maximize per-filter activations\n-# for the first 64 filters of our target layer\n-all_imgs = []\n-for filter_index in range(64):\n-    print(\"Processing filter %d\" % (filter_index,))\n-    loss, img = visualize_filter(filter_index)\n-    all_imgs.append(img)\n-\n-# Build a black picture with enough space for\n-# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n-margin = 5\n-n = 8\n-cropped_width = img_width - 25 * 2\n-cropped_height = img_height - 25 * 2\n-width = n * cropped_width + (n - 1) * margin\n-height = n * cropped_height + (n - 1) * margin\n-stitched_filters = np.zeros((width, height, 3))\n-\n-# Fill the picture with our saved filters\n-for i in range(n):\n-    for j in range(n):\n-        img = all_imgs[i * n + j]\n-        stitched_filters[\n-            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n-            (cropped_height + margin) * j : (cropped_height + margin) * j\n-            + cropped_height,\n-            :,\n-        ] = img\n-keras.utils.save_img(\"stiched_filters.png\", stitched_filters)\n-\n-from IPython.display import Image, display\n-\n-display(Image(\"stiched_filters.png\"))\n-\n-\"\"\"\n-Image classification models see the world by decomposing their inputs over a \"vector\n-basis\" of texture filters such as these.\n-\n-See also\n-[this old blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n-for analysis and interpretation.\n-\"\"\"\n\n@@ -1,549 +0,0 @@\n-\"\"\"\n-Title: Zero-DCE for low-light image enhancement\n-Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/09/18\n-Last modified: 2023/07/15\n-Description: Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-**Zero-Reference Deep Curve Estimation** or **Zero-DCE** formulates low-light image\n-enhancement as the task of estimating an image-specific\n-[*tonal curve*](https://en.wikipedia.org/wiki/Curve_(tonality)) with a deep neural network.\n-In this example, we train a lightweight deep network, **DCE-Net**, to estimate\n-pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.\n-\n-Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output.\n-These curves are then used for pixel-wise adjustment on the dynamic range of the input to\n-obtain an enhanced image. The curve estimation process is done in such a way that it maintains\n-the range of the enhanced image and preserves the contrast of neighboring pixels. This\n-curve estimation is inspired by curves adjustment used in photo editing software such as\n-Adobe Photoshop where users can adjust points throughout an image’s tonal range.\n-\n-Zero-DCE is appealing because of its relaxed assumptions with regard to reference images:\n-it does not require any input/output image pairs during training.\n-This is achieved through a set of carefully formulated non-reference loss functions,\n-which implicitly measure the enhancement quality and guide the training of the network.\n-\n-### References\n-\n-- [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](https://arxiv.org/pdf/2001.06826.pdf)\n-- [Curves adjustment in Adobe Photoshop](https://helpx.adobe.com/photoshop/using/curves-adjustment.html)\n-\"\"\"\n-\n-\"\"\"\n-## Downloading LOLDataset\n-\n-The **LoL Dataset** has been created for low-light image enhancement. It provides 485\n-images for training and 15 for testing. Each image pair in the dataset consists of a\n-low-light input image and its corresponding well-exposed reference image.\n-\"\"\"\n-\n-import os\n-import random\n-import numpy as np\n-from glob import glob\n-from PIL import Image, ImageOps\n-import matplotlib.pyplot as plt\n-\n-import keras\n-from keras import layers\n-\n-import tensorflow as tf\n-\n-\"\"\"shell\n-wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n-unzip -q lol_dataset.zip && rm lol_dataset.zip\n-\"\"\"\n-\n-\"\"\"\n-## Creating a TensorFlow Dataset\n-\n-We use 300 low-light images from the LoL Dataset training set for training, and we use\n-the remaining 185 low-light images for validation. We resize the images to size `256 x\n-256` to be used for both training and validation. Note that in order to train the DCE-Net,\n-we will not require the corresponding enhanced images.\n-\"\"\"\n-\n-IMAGE_SIZE = 256\n-BATCH_SIZE = 16\n-MAX_TRAIN_IMAGES = 400\n-\n-\n-def load_data(image_path):\n-    image = tf.io.read_file(image_path)\n-    image = tf.image.decode_png(image, channels=3)\n-    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n-    image = image / 255.0\n-    return image\n-\n-\n-def data_generator(low_light_images):\n-    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n-    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n-    return dataset\n-\n-\n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n-    :MAX_TRAIN_IMAGES\n-]\n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[\n-    MAX_TRAIN_IMAGES:\n-]\n-test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n-\n-\n-train_dataset = data_generator(train_low_light_images)\n-val_dataset = data_generator(val_low_light_images)\n-\n-print(\"Train Dataset:\", train_dataset)\n-print(\"Validation Dataset:\", val_dataset)\n-\n-\"\"\"\n-## The Zero-DCE Framework\n-\n-The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n-(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n-channels by applying the curves iteratively to obtain the final enhanced image.\n-\n-### Understanding light-enhancement curves\n-\n-A ligh-enhancement curve is a kind of curve that can map a low-light image\n-to its enhanced version automatically,\n-where the self-adaptive curve parameters are solely dependent on the input image.\n-When designing such a curve, three objectives should be taken into account:\n-\n-- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n-avoid information loss induced by overflow truncation.\n-- It should be monotonous, to preserve the contrast between neighboring pixels.\n-- The shape of this curve should be as simple as possible,\n-and the curve should be differentiable to allow backpropagation.\n-\n-The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n-illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n-the risk of over-saturation.\n-\n-![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n-\n-### DCE-Net\n-\n-The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n-image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n-image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n-higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n-concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n-followed by the ReLU activation function. The last convolutional layer is followed by the\n-Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n-iteration requires three curve parameter maps for the three channels.\n-\n-![](https://i.imgur.com/HtIg34W.png)\n-\"\"\"\n-\n-\n-def build_dce_net():\n-    input_img = keras.Input(shape=[None, None, 3])\n-    conv1 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(input_img)\n-    conv2 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv1)\n-    conv3 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv2)\n-    conv4 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv3)\n-    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n-    conv5 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(int_con1)\n-    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n-    conv6 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(int_con2)\n-    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n-    x_r = layers.Conv2D(\n-        24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\"\n-    )(int_con3)\n-    return keras.Model(inputs=input_img, outputs=x_r)\n-\n-\n-\"\"\"\n-## Loss functions\n-\n-To enable zero-reference learning in DCE-Net, we use a set of differentiable\n-zero-reference losses that allow us to evaluate the quality of enhanced images.\n-\"\"\"\n-\n-\"\"\"\n-### Color constancy loss\n-\n-The *color constancy loss* is used to correct the potential color deviations in the\n-enhanced image.\n-\"\"\"\n-\n-\n-def color_constancy_loss(x):\n-    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n-    mr, mg, mb = (\n-        mean_rgb[:, :, :, 0],\n-        mean_rgb[:, :, :, 1],\n-        mean_rgb[:, :, :, 2],\n-    )\n-    d_rg = tf.square(mr - mg)\n-    d_rb = tf.square(mr - mb)\n-    d_gb = tf.square(mb - mg)\n-    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n-\n-\n-\"\"\"\n-### Exposure loss\n-\n-To restrain under-/over-exposed regions, we use the *exposure control loss*.\n-It measures the distance between the average intensity value of a local region\n-and a preset well-exposedness level (set to `0.6`).\n-\"\"\"\n-\n-\n-def exposure_loss(x, mean_val=0.6):\n-    x = tf.reduce_mean(x, axis=3, keepdims=True)\n-    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n-    return tf.reduce_mean(tf.square(mean - mean_val))\n-\n-\n-\"\"\"\n-### Illumination smoothness loss\n-\n-To preserve the monotonicity relations between neighboring pixels, the\n-*illumination smoothness loss* is added to each curve parameter map.\n-\"\"\"\n-\n-\n-def illumination_smoothness_loss(x):\n-    batch_size = tf.shape(x)[0]\n-    h_x = tf.shape(x)[1]\n-    w_x = tf.shape(x)[2]\n-    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n-    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n-    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n-    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n-    batch_size = tf.cast(batch_size, dtype=tf.float32)\n-    count_h = tf.cast(count_h, dtype=tf.float32)\n-    count_w = tf.cast(count_w, dtype=tf.float32)\n-    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n-\n-\n-\"\"\"\n-### Spatial consistency loss\n-\n-The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n-preserving the contrast between neighboring regions across the input image and its enhanced version.\n-\"\"\"\n-\n-\n-class SpatialConsistencyLoss(keras.losses.Loss):\n-    def __init__(self, **kwargs):\n-        super().__init__(reduction=\"none\")\n-\n-        self.left_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.right_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.up_kernel = tf.constant(\n-            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.down_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n-        )\n-\n-    def call(self, y_true, y_pred):\n-        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n-        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n-        original_pool = tf.nn.avg_pool2d(\n-            original_mean, ksize=4, strides=4, padding=\"VALID\"\n-        )\n-        enhanced_pool = tf.nn.avg_pool2d(\n-            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n-        )\n-\n-        d_original_left = tf.nn.conv2d(\n-            original_pool,\n-            self.left_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-        d_original_right = tf.nn.conv2d(\n-            original_pool,\n-            self.right_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-        d_original_up = tf.nn.conv2d(\n-            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_original_down = tf.nn.conv2d(\n-            original_pool,\n-            self.down_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-\n-        d_enhanced_left = tf.nn.conv2d(\n-            enhanced_pool,\n-            self.left_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-        d_enhanced_right = tf.nn.conv2d(\n-            enhanced_pool,\n-            self.right_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-        d_enhanced_up = tf.nn.conv2d(\n-            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_enhanced_down = tf.nn.conv2d(\n-            enhanced_pool,\n-            self.down_kernel,\n-            strides=[1, 1, 1, 1],\n-            padding=\"SAME\",\n-        )\n-\n-        d_left = tf.square(d_original_left - d_enhanced_left)\n-        d_right = tf.square(d_original_right - d_enhanced_right)\n-        d_up = tf.square(d_original_up - d_enhanced_up)\n-        d_down = tf.square(d_original_down - d_enhanced_down)\n-        return d_left + d_right + d_up + d_down\n-\n-\n-\"\"\"\n-### Deep curve estimation model\n-\n-We implement the Zero-DCE framework as a Keras subclassed model.\n-\"\"\"\n-\n-\n-class ZeroDCE(keras.Model):\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-        self.dce_model = build_dce_net()\n-\n-    def compile(self, learning_rate, **kwargs):\n-        super().compile(**kwargs)\n-        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n-        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n-        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n-        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(\n-            name=\"illumination_smoothness_loss\"\n-        )\n-        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n-            name=\"spatial_constancy_loss\"\n-        )\n-        self.color_constancy_loss_tracker = keras.metrics.Mean(\n-            name=\"color_constancy_loss\"\n-        )\n-        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.total_loss_tracker,\n-            self.illumination_smoothness_loss_tracker,\n-            self.spatial_constancy_loss_tracker,\n-            self.color_constancy_loss_tracker,\n-            self.exposure_loss_tracker,\n-        ]\n-\n-    def get_enhanced_image(self, data, output):\n-        r1 = output[:, :, :, :3]\n-        r2 = output[:, :, :, 3:6]\n-        r3 = output[:, :, :, 6:9]\n-        r4 = output[:, :, :, 9:12]\n-        r5 = output[:, :, :, 12:15]\n-        r6 = output[:, :, :, 15:18]\n-        r7 = output[:, :, :, 18:21]\n-        r8 = output[:, :, :, 21:24]\n-        x = data + r1 * (tf.square(data) - data)\n-        x = x + r2 * (tf.square(x) - x)\n-        x = x + r3 * (tf.square(x) - x)\n-        enhanced_image = x + r4 * (tf.square(x) - x)\n-        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n-        x = x + r6 * (tf.square(x) - x)\n-        x = x + r7 * (tf.square(x) - x)\n-        enhanced_image = x + r8 * (tf.square(x) - x)\n-        return enhanced_image\n-\n-    def call(self, data):\n-        dce_net_output = self.dce_model(data)\n-        return self.get_enhanced_image(data, dce_net_output)\n-\n-    def compute_losses(self, data, output):\n-        enhanced_image = self.get_enhanced_image(data, output)\n-        loss_illumination = 200 * illumination_smoothness_loss(output)\n-        loss_spatial_constancy = tf.reduce_mean(\n-            self.spatial_constancy_loss(enhanced_image, data)\n-        )\n-        loss_color_constancy = 5 * tf.reduce_mean(\n-            color_constancy_loss(enhanced_image)\n-        )\n-        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n-        total_loss = (\n-            loss_illumination\n-            + loss_spatial_constancy\n-            + loss_color_constancy\n-            + loss_exposure\n-        )\n-\n-        return {\n-            \"total_loss\": total_loss,\n-            \"illumination_smoothness_loss\": loss_illumination,\n-            \"spatial_constancy_loss\": loss_spatial_constancy,\n-            \"color_constancy_loss\": loss_color_constancy,\n-            \"exposure_loss\": loss_exposure,\n-        }\n-\n-    def train_step(self, data):\n-        with tf.GradientTape() as tape:\n-            output = self.dce_model(data)\n-            losses = self.compute_losses(data, output)\n-\n-        gradients = tape.gradient(\n-            losses[\"total_loss\"], self.dce_model.trainable_weights\n-        )\n-        self.optimizer.apply_gradients(\n-            zip(gradients, self.dce_model.trainable_weights)\n-        )\n-\n-        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(\n-            losses[\"illumination_smoothness_loss\"]\n-        )\n-        self.spatial_constancy_loss_tracker.update_state(\n-            losses[\"spatial_constancy_loss\"]\n-        )\n-        self.color_constancy_loss_tracker.update_state(\n-            losses[\"color_constancy_loss\"]\n-        )\n-        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n-\n-        return {metric.name: metric.result() for metric in self.metrics}\n-\n-    def test_step(self, data):\n-        output = self.dce_model(data)\n-        losses = self.compute_losses(data, output)\n-\n-        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(\n-            losses[\"illumination_smoothness_loss\"]\n-        )\n-        self.spatial_constancy_loss_tracker.update_state(\n-            losses[\"spatial_constancy_loss\"]\n-        )\n-        self.color_constancy_loss_tracker.update_state(\n-            losses[\"color_constancy_loss\"]\n-        )\n-        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n-\n-        return {metric.name: metric.result() for metric in self.metrics}\n-\n-    def save_weights(\n-        self, filepath, overwrite=True, save_format=None, options=None\n-    ):\n-        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n-        self.dce_model.save_weights(\n-            filepath,\n-            overwrite=overwrite,\n-            save_format=save_format,\n-            options=options,\n-        )\n-\n-    def load_weights(\n-        self, filepath, by_name=False, skip_mismatch=False, options=None\n-    ):\n-        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n-        self.dce_model.load_weights(\n-            filepath=filepath,\n-            by_name=by_name,\n-            skip_mismatch=skip_mismatch,\n-            options=options,\n-        )\n-\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-zero_dce_model = ZeroDCE()\n-zero_dce_model.compile(learning_rate=1e-4)\n-history = zero_dce_model.fit(\n-    train_dataset, validation_data=val_dataset, epochs=100\n-)\n-\n-\n-def plot_result(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_result(\"total_loss\")\n-plot_result(\"illumination_smoothness_loss\")\n-plot_result(\"spatial_constancy_loss\")\n-plot_result(\"color_constancy_loss\")\n-plot_result(\"exposure_loss\")\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-\n-def plot_results(images, titles, figure_size=(12, 12)):\n-    fig = plt.figure(figsize=figure_size)\n-    for i in range(len(images)):\n-        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n-        _ = plt.imshow(images[i])\n-        plt.axis(\"off\")\n-    plt.show()\n-\n-\n-def infer(original_image):\n-    image = keras.utils.img_to_array(original_image)\n-    image = image.astype(\"float32\") / 255.0\n-    image = np.expand_dims(image, axis=0)\n-    output_image = zero_dce_model(image)\n-    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n-    output_image = Image.fromarray(output_image.numpy())\n-    return output_image\n-\n-\n-\"\"\"\n-### Inference on test images\n-\n-We compare the test images from LOLDataset enhanced by MIRNet with images enhanced via\n-the `PIL.ImageOps.autocontrast()` function.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/low-light-image-enhancement)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/low-light-image-enhancement).\n-\"\"\"\n-\n-for val_image_file in test_low_light_images:\n-    original_image = Image.open(val_image_file)\n-    enhanced_image = infer(original_image)\n-    plot_results(\n-        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n-        [\"Original\", \"PIL Autocontrast\", \"Enhanced\"],\n-        (20, 12),\n-    )\n\n@@ -1,308 +0,0 @@\n-\"\"\"\n-Title: Timeseries anomaly detection using an Autoencoder\n-Author: [pavithrasv](https://github.com/pavithrasv)\n-Date created: 2020/05/31\n-Last modified: 2020/05/31\n-Description: Detect anomalies in a timeseries using an Autoencoder.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This script demonstrates how you can use a reconstruction convolutional\n-autoencoder model to detect anomalies in timeseries data.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import pandas as pd\n-import keras\n-from keras import layers\n-from matplotlib import pyplot as plt\n-\n-\"\"\"\n-## Load the data\n-\n-We will use the [Numenta Anomaly Benchmark(NAB)](\n-https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artificial\n-timeseries data containing labeled anomalous periods of behavior. Data are\n-ordered, timestamped, single-valued metrics.\n-\n-We will use the `art_daily_small_noise.csv` file for training and the\n-`art_daily_jumpsup.csv` file for testing. The simplicity of this dataset\n-allows us to demonstrate anomaly detection effectively.\n-\"\"\"\n-\n-master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n-\n-df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n-df_small_noise_url = master_url_root + df_small_noise_url_suffix\n-df_small_noise = pd.read_csv(\n-    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n-)\n-\n-df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n-df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n-df_daily_jumpsup = pd.read_csv(\n-    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n-)\n-\n-\"\"\"\n-## Quick look at the data\n-\"\"\"\n-\n-print(df_small_noise.head())\n-\n-print(df_daily_jumpsup.head())\n-\n-\"\"\"\n-## Visualize the data\n-### Timeseries data without anomalies\n-\n-We will use the following data for training.\n-\"\"\"\n-fig, ax = plt.subplots()\n-df_small_noise.plot(legend=False, ax=ax)\n-plt.show()\n-\n-\"\"\"\n-### Timeseries data with anomalies\n-\n-We will use the following data for testing and see if the sudden jump up in the\n-data is detected as an anomaly.\n-\"\"\"\n-fig, ax = plt.subplots()\n-df_daily_jumpsup.plot(legend=False, ax=ax)\n-plt.show()\n-\n-\"\"\"\n-## Prepare training data\n-\n-Get data values from the training timeseries data file and normalize the\n-`value` data. We have a `value` for every 5 mins for 14 days.\n-\n--   24 * 60 / 5 = **288 timesteps per day**\n--   288 * 14 = **4032 data points** in total\n-\"\"\"\n-\n-\n-# Normalize and save the mean and std we get,\n-# for normalizing test data.\n-training_mean = df_small_noise.mean()\n-training_std = df_small_noise.std()\n-df_training_value = (df_small_noise - training_mean) / training_std\n-print(\"Number of training samples:\", len(df_training_value))\n-\n-\"\"\"\n-### Create sequences\n-Create sequences combining `TIME_STEPS` contiguous data values from the\n-training data.\n-\"\"\"\n-\n-TIME_STEPS = 288\n-\n-\n-# Generated training sequences for use in the model.\n-def create_sequences(values, time_steps=TIME_STEPS):\n-    output = []\n-    for i in range(len(values) - time_steps + 1):\n-        output.append(values[i : (i + time_steps)])\n-    return np.stack(output)\n-\n-\n-x_train = create_sequences(df_training_value.values)\n-print(\"Training input shape: \", x_train.shape)\n-\n-\"\"\"\n-## Build a model\n-\n-We will build a convolutional reconstruction autoencoder model. The model will\n-take input of shape `(batch_size, sequence_length, num_features)` and return\n-output of the same shape. In this case, `sequence_length` is 288 and\n-`num_features` is 1.\n-\"\"\"\n-\n-model = keras.Sequential(\n-    [\n-        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n-        layers.Conv1D(\n-            filters=32,\n-            kernel_size=7,\n-            padding=\"same\",\n-            strides=2,\n-            activation=\"relu\",\n-        ),\n-        layers.Dropout(rate=0.2),\n-        layers.Conv1D(\n-            filters=16,\n-            kernel_size=7,\n-            padding=\"same\",\n-            strides=2,\n-            activation=\"relu\",\n-        ),\n-        layers.Conv1DTranspose(\n-            filters=16,\n-            kernel_size=7,\n-            padding=\"same\",\n-            strides=2,\n-            activation=\"relu\",\n-        ),\n-        layers.Dropout(rate=0.2),\n-        layers.Conv1DTranspose(\n-            filters=32,\n-            kernel_size=7,\n-            padding=\"same\",\n-            strides=2,\n-            activation=\"relu\",\n-        ),\n-        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n-    ]\n-)\n-model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n-model.summary()\n-\n-\"\"\"\n-## Train the model\n-\n-Please note that we are using `x_train` as both the input and the target\n-since this is a reconstruction model.\n-\"\"\"\n-\n-history = model.fit(\n-    x_train,\n-    x_train,\n-    epochs=50,\n-    batch_size=128,\n-    validation_split=0.1,\n-    callbacks=[\n-        keras.callbacks.EarlyStopping(\n-            monitor=\"val_loss\", patience=5, mode=\"min\"\n-        )\n-    ],\n-)\n-\n-\"\"\"\n-Let's plot training and validation loss to see how the training went.\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n-plt.legend()\n-plt.show()\n-\n-\"\"\"\n-## Detecting anomalies\n-\n-We will detect anomalies by determining how well our model can reconstruct\n-the input data.\n-\n-\n-1.   Find MAE loss on training samples.\n-2.   Find max MAE loss value. This is the worst our model has performed trying\n-to reconstruct a sample. We will make this the `threshold` for anomaly\n-detection.\n-3.   If the reconstruction loss for a sample is greater than this `threshold`\n-value then we can infer that the model is seeing a pattern that it isn't\n-familiar with. We will label this sample as an `anomaly`.\n-\n-\n-\"\"\"\n-\n-# Get train MAE loss.\n-x_train_pred = model.predict(x_train)\n-train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n-\n-plt.hist(train_mae_loss, bins=50)\n-plt.xlabel(\"Train MAE loss\")\n-plt.ylabel(\"No of samples\")\n-plt.show()\n-\n-# Get reconstruction loss threshold.\n-threshold = np.max(train_mae_loss)\n-print(\"Reconstruction error threshold: \", threshold)\n-\n-\"\"\"\n-### Compare recontruction\n-\n-Just for fun, let's see how our model has recontructed the first sample.\n-This is the 288 timesteps from day 1 of our training dataset.\n-\"\"\"\n-\n-# Checking how the first sequence is learnt\n-plt.plot(x_train[0])\n-plt.plot(x_train_pred[0])\n-plt.show()\n-\n-\"\"\"\n-### Prepare test data\n-\"\"\"\n-\n-\n-df_test_value = (df_daily_jumpsup - training_mean) / training_std\n-fig, ax = plt.subplots()\n-df_test_value.plot(legend=False, ax=ax)\n-plt.show()\n-\n-# Create sequences from test values.\n-x_test = create_sequences(df_test_value.values)\n-print(\"Test input shape: \", x_test.shape)\n-\n-# Get test MAE loss.\n-x_test_pred = model.predict(x_test)\n-test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n-test_mae_loss = test_mae_loss.reshape((-1))\n-\n-plt.hist(test_mae_loss, bins=50)\n-plt.xlabel(\"test MAE loss\")\n-plt.ylabel(\"No of samples\")\n-plt.show()\n-\n-# Detect all the samples which are anomalies.\n-anomalies = test_mae_loss > threshold\n-print(\"Number of anomaly samples: \", np.sum(anomalies))\n-print(\"Indices of anomaly samples: \", np.where(anomalies))\n-\n-\"\"\"\n-## Plot anomalies\n-\n-We now know the samples of the data which are anomalies. With this, we will\n-find the corresponding `timestamps` from the original test data. We will be\n-using the following method to do that:\n-\n-Let's say time_steps = 3 and we have 10 training values. Our `x_train` will\n-look like this:\n-\n-- 0, 1, 2\n-- 1, 2, 3\n-- 2, 3, 4\n-- 3, 4, 5\n-- 4, 5, 6\n-- 5, 6, 7\n-- 6, 7, 8\n-- 7, 8, 9\n-\n-All except the initial and the final time_steps-1 data values, will appear in\n-`time_steps` number of samples. So, if we know that the samples\n-[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n-5 is an anomaly.\n-\"\"\"\n-\n-# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n-anomalous_data_indices = []\n-for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n-    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n-        anomalous_data_indices.append(data_idx)\n-\n-\"\"\"\n-Let's overlay the anomalies on the original test data plot.\n-\"\"\"\n-\n-df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n-fig, ax = plt.subplots()\n-df_daily_jumpsup.plot(legend=False, ax=ax)\n-df_subset.plot(legend=False, ax=ax, color=\"r\")\n-plt.show()\n\n@@ -1,232 +0,0 @@\n-\"\"\"\n-Title: Timeseries classification from scratch\n-Author: [hfawaz](https://github.com/hfawaz/)\n-Date created: 2020/07/21\n-Last modified: 2021/07/16\n-Description: Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to do timeseries classification from scratch, starting from raw\n-CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n-[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/).\n-\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\n-\"\"\"\n-import keras\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Load the data: the FordA dataset\n-\n-### Dataset description\n-\n-The dataset we are using here is called FordA.\n-The data comes from the UCR archive.\n-The dataset contains 3601 training instances and another 1320 testing instances.\n-Each timeseries corresponds to a measurement of engine noise captured by a motor sensor.\n-For this task, the goal is to automatically detect the presence of a specific issue with\n-the engine. The problem is a balanced binary classification task. The full description of\n-this dataset can be found [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n-\n-### Read the TSV data\n-\n-We will use the `FordA_TRAIN` file for training and the\n-`FordA_TEST` file for testing. The simplicity of this dataset\n-allows us to demonstrate effectively how to use ConvNets for timeseries classification.\n-In this file, the first column corresponds to the label.\n-\"\"\"\n-\n-\n-def readucr(filename):\n-    data = np.loadtxt(filename, delimiter=\"\\t\")\n-    y = data[:, 0]\n-    x = data[:, 1:]\n-    return x, y.astype(int)\n-\n-\n-root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n-\n-x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n-x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n-\n-\"\"\"\n-## Visualize the data\n-\n-Here we visualize one timeseries example for each class in the dataset.\n-\n-\"\"\"\n-\n-classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n-\n-plt.figure()\n-for c in classes:\n-    c_x_train = x_train[y_train == c]\n-    plt.plot(c_x_train[0], label=\"class \" + str(c))\n-plt.legend(loc=\"best\")\n-plt.show()\n-plt.close()\n-\n-\"\"\"\n-## Standardize the data\n-\n-Our timeseries are already in a single length (500). However, their values are\n-usually in various ranges. This is not ideal for a neural network;\n-in general we should seek to make the input values normalized.\n-For this specific dataset, the data is already z-normalized: each timeseries sample\n-has a mean equal to zero and a standard deviation equal to one. This type of\n-normalization is very common for timeseries classification problems, see\n-[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n-\n-Note that the timeseries data used here are univariate, meaning we only have one channel\n-per timeseries example.\n-We will therefore transform the timeseries into a multivariate one with one channel\n-using a simple reshaping via numpy.\n-This will allow us to construct a model that is easily applicable to multivariate time\n-series.\n-\"\"\"\n-\n-x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n-x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n-\n-\"\"\"\n-Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n-the number of classes beforehand.\n-\"\"\"\n-\n-num_classes = len(np.unique(y_train))\n-\n-\"\"\"\n-Now we shuffle the training set because we will be using the `validation_split` option\n-later when training.\n-\"\"\"\n-\n-idx = np.random.permutation(len(x_train))\n-x_train = x_train[idx]\n-y_train = y_train[idx]\n-\n-\"\"\"\n-Standardize the labels to positive integers.\n-The expected labels will then be 0 and 1.\n-\"\"\"\n-\n-y_train[y_train == -1] = 0\n-y_test[y_test == -1] = 0\n-\n-\"\"\"\n-## Build a model\n-\n-We build a Fully Convolutional Neural Network originally proposed in\n-[this paper](https://arxiv.org/abs/1611.06455).\n-The implementation is based on the TF 2 version provided\n-[here](https://github.com/hfawaz/dl-4-tsc/).\n-The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\n-via random search using [KerasTuner](https://github.com/keras-team/keras-tuner).\n-\n-\"\"\"\n-\n-\n-def make_model(input_shape):\n-    input_layer = keras.layers.Input(input_shape)\n-\n-    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n-        input_layer\n-    )\n-    conv1 = keras.layers.BatchNormalization()(conv1)\n-    conv1 = keras.layers.ReLU()(conv1)\n-\n-    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n-        conv1\n-    )\n-    conv2 = keras.layers.BatchNormalization()(conv2)\n-    conv2 = keras.layers.ReLU()(conv2)\n-\n-    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n-        conv2\n-    )\n-    conv3 = keras.layers.BatchNormalization()(conv3)\n-    conv3 = keras.layers.ReLU()(conv3)\n-\n-    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n-\n-    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n-\n-    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n-\n-\n-model = make_model(input_shape=x_train.shape[1:])\n-keras.utils.plot_model(model, show_shapes=True)\n-\n-\"\"\"\n-## Train the model\n-\n-\"\"\"\n-\n-epochs = 500\n-batch_size = 32\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\n-        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n-    ),\n-    keras.callbacks.ReduceLROnPlateau(\n-        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n-    ),\n-    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n-]\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"sparse_categorical_accuracy\"],\n-)\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=epochs,\n-    callbacks=callbacks,\n-    validation_split=0.2,\n-    verbose=1,\n-)\n-\n-\"\"\"\n-## Evaluate model on test data\n-\"\"\"\n-\n-model = keras.models.load_model(\"best_model.keras\")\n-\n-test_loss, test_acc = model.evaluate(x_test, y_test)\n-\n-print(\"Test accuracy\", test_acc)\n-print(\"Test loss\", test_loss)\n-\n-\"\"\"\n-## Plot the model's training and validation loss\n-\"\"\"\n-\n-metric = \"sparse_categorical_accuracy\"\n-plt.figure()\n-plt.plot(history.history[metric])\n-plt.plot(history.history[\"val_\" + metric])\n-plt.title(\"model \" + metric)\n-plt.ylabel(metric, fontsize=\"large\")\n-plt.xlabel(\"epoch\", fontsize=\"large\")\n-plt.legend([\"train\", \"val\"], loc=\"best\")\n-plt.show()\n-plt.close()\n-\n-\"\"\"\n-We can see how the training accuracy reaches almost 0.95 after 100 epochs.\n-However, by observing the validation accuracy we can see how the network still needs\n-training until it reaches almost 0.97 for both the validation and the training accuracy\n-after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation\n-accuracy will start decreasing while the training accuracy will continue on increasing:\n-the model starts overfitting.\n-\"\"\"\n\n@@ -1,173 +0,0 @@\n-\"\"\"\n-Title: Timeseries classification with a Transformer model\n-Author: [Theodoros Ntakouris](https://github.com/ntakouris)\n-Date created: 2021/06/25\n-Last modified: 2021/08/05\n-Description: This notebook demonstrates how to do timeseries classification using a Transformer model.\n-Accelerator: GPU\n-\"\"\"\n-\n-\n-\"\"\"\n-## Introduction\n-\n-This is the Transformer architecture from\n-[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n-applied to timeseries instead of natural language.\n-\n-This example requires TensorFlow 2.4 or higher.\n-\n-## Load the dataset\n-\n-We are going to use the same dataset and preprocessing as the\n-[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n-example.\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-\n-\n-def readucr(filename):\n-    data = np.loadtxt(filename, delimiter=\"\\t\")\n-    y = data[:, 0]\n-    x = data[:, 1:]\n-    return x, y.astype(int)\n-\n-\n-root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n-\n-x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n-x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n-\n-x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n-x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n-\n-n_classes = len(np.unique(y_train))\n-\n-idx = np.random.permutation(len(x_train))\n-x_train = x_train[idx]\n-y_train = y_train[idx]\n-\n-y_train[y_train == -1] = 0\n-y_test[y_test == -1] = 0\n-\n-\"\"\"\n-## Build the model\n-\n-Our model processes a tensor of shape `(batch size, sequence length, features)`,\n-where `sequence length` is the number of time steps and `features` is each input\n-timeseries.\n-\n-You can replace your classification RNN layers with this one: the\n-inputs are fully compatible!\n-\n-We include residual connections, layer normalization, and dropout.\n-The resulting layer can be stacked multiple times.\n-\n-The projection layers are implemented through `keras.layers.Conv1D`.\n-\"\"\"\n-\n-\n-def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n-    # Attention and Normalization\n-    x = layers.MultiHeadAttention(\n-        key_dim=head_size, num_heads=num_heads, dropout=dropout\n-    )(inputs, inputs)\n-    x = layers.Dropout(dropout)(x)\n-    x = layers.LayerNormalization(epsilon=1e-6)(x)\n-    res = x + inputs\n-\n-    # Feed Forward Part\n-    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n-    x = layers.Dropout(dropout)(x)\n-    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n-    x = layers.LayerNormalization(epsilon=1e-6)(x)\n-    return x + res\n-\n-\n-\"\"\"\n-The main part of our model is now complete. We can stack multiple of those\n-`transformer_encoder` blocks and we can also proceed to add the final\n-Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n-layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n-our model down to a vector of features for each data point in the current\n-batch. A common way to achieve this is to use a pooling layer. For\n-this example, a `GlobalAveragePooling1D` layer is sufficient.\n-\"\"\"\n-\n-\n-def build_model(\n-    input_shape,\n-    head_size,\n-    num_heads,\n-    ff_dim,\n-    num_transformer_blocks,\n-    mlp_units,\n-    dropout=0,\n-    mlp_dropout=0,\n-):\n-    inputs = keras.Input(shape=input_shape)\n-    x = inputs\n-    for _ in range(num_transformer_blocks):\n-        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n-\n-    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n-    for dim in mlp_units:\n-        x = layers.Dense(dim, activation=\"relu\")(x)\n-        x = layers.Dropout(mlp_dropout)(x)\n-    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n-    return keras.Model(inputs, outputs)\n-\n-\n-\"\"\"\n-## Train and evaluate\n-\"\"\"\n-\n-input_shape = x_train.shape[1:]\n-\n-model = build_model(\n-    input_shape,\n-    head_size=256,\n-    num_heads=4,\n-    ff_dim=4,\n-    num_transformer_blocks=4,\n-    mlp_units=[128],\n-    mlp_dropout=0.4,\n-    dropout=0.25,\n-)\n-\n-model.compile(\n-    loss=\"sparse_categorical_crossentropy\",\n-    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n-    metrics=[\"sparse_categorical_accuracy\"],\n-)\n-model.summary()\n-\n-callbacks = [\n-    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n-]\n-\n-model.fit(\n-    x_train,\n-    y_train,\n-    validation_split=0.2,\n-    epochs=150,\n-    batch_size=64,\n-    callbacks=callbacks,\n-)\n-\n-model.evaluate(x_test, y_test, verbose=1)\n-\n-\"\"\"\n-## Conclusions\n-\n-In about 110-120 epochs (25s each on Colab), the model reaches a training\n-accuracy of ~0.95, validation accuracy of ~84 and a testing\n-accuracy of ~85, without hyperparameter tuning. And that is for a model\n-with less than 100k parameters. Of course, parameter count and accuracy could be\n-improved by a hyperparameter search and a more sophisticated learning rate\n-schedule, or a different optimizer.\n-\n-\"\"\"\n\n@@ -1,371 +0,0 @@\n-\"\"\"\n-Title: Timeseries forecasting for weather prediction\n-Authors: [Prabhanshu Attri](https://prabhanshu.com/github), [Yashika Sharma](https://github.com/yashika51), [Kristi Takach](https://github.com/ktakattack), [Falak Shah](https://github.com/falaktheoptimist)\n-Date created: 2020/06/23\n-Last modified: 2023/11/22\n-Description: This notebook demonstrates how to do timeseries forecasting using a LSTM model.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import pandas as pd\n-import matplotlib.pyplot as plt\n-import keras\n-\n-\"\"\"\n-## Climate Data Time-Series\n-\n-We will be using Jena Climate dataset recorded by the\n-[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n-The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per\n-10 minutes.\n-\n-**Location**: Weather Station, Max Planck Institute for Biogeochemistry\n-in Jena, Germany\n-\n-**Time-frame Considered**: Jan 10, 2009 - December 31, 2016\n-\n-\n-The table below shows the column names, their value formats, and their description.\n-\n-Index| Features      |Format             |Description\n------|---------------|-------------------|-----------------------\n-1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n-2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n-3    |T (degC)       |-8.02              |Temperature in Celsius\n-4    |Tpot (K)       |265.4              |Temperature in Kelvin\n-5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n-6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n-7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n-8    |VPact (mbar)   |3.11               |Vapor pressure\n-9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n-10   |sh (g/kg)      |1.94               |Specific humidity\n-11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n-12   |rho (g/m ** 3) |1307.75            |Airtight\n-13   |wv (m/s)       |1.03               |Wind speed\n-14   |max. wv (m/s)  |1.75               |Maximum wind speed\n-15   |wd (deg)       |152.3              |Wind direction in degrees\n-\"\"\"\n-\n-from zipfile import ZipFile\n-\n-uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n-zip_path = keras.utils.get_file(\n-    origin=uri, fname=\"jena_climate_2009_2016.csv.zip\"\n-)\n-zip_file = ZipFile(zip_path)\n-zip_file.extractall()\n-csv_path = \"jena_climate_2009_2016.csv\"\n-\n-df = pd.read_csv(csv_path)\n-\n-\"\"\"\n-## Raw Data Visualization\n-\n-To give us a sense of the data we are working with, each feature has been plotted below.\n-This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n-It also shows where anomalies are present, which will be addressed during normalization.\n-\"\"\"\n-\n-titles = [\n-    \"Pressure\",\n-    \"Temperature\",\n-    \"Temperature in Kelvin\",\n-    \"Temperature (dew point)\",\n-    \"Relative Humidity\",\n-    \"Saturation vapor pressure\",\n-    \"Vapor pressure\",\n-    \"Vapor pressure deficit\",\n-    \"Specific humidity\",\n-    \"Water vapor concentration\",\n-    \"Airtight\",\n-    \"Wind speed\",\n-    \"Maximum wind speed\",\n-    \"Wind direction in degrees\",\n-]\n-\n-feature_keys = [\n-    \"p (mbar)\",\n-    \"T (degC)\",\n-    \"Tpot (K)\",\n-    \"Tdew (degC)\",\n-    \"rh (%)\",\n-    \"VPmax (mbar)\",\n-    \"VPact (mbar)\",\n-    \"VPdef (mbar)\",\n-    \"sh (g/kg)\",\n-    \"H2OC (mmol/mol)\",\n-    \"rho (g/m**3)\",\n-    \"wv (m/s)\",\n-    \"max. wv (m/s)\",\n-    \"wd (deg)\",\n-]\n-\n-colors = [\n-    \"blue\",\n-    \"orange\",\n-    \"green\",\n-    \"red\",\n-    \"purple\",\n-    \"brown\",\n-    \"pink\",\n-    \"gray\",\n-    \"olive\",\n-    \"cyan\",\n-]\n-\n-date_time_key = \"Date Time\"\n-\n-\n-def show_raw_visualization(data):\n-    time_data = data[date_time_key]\n-    fig, axes = plt.subplots(\n-        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n-    )\n-    for i in range(len(feature_keys)):\n-        key = feature_keys[i]\n-        c = colors[i % (len(colors))]\n-        t_data = data[key]\n-        t_data.index = time_data\n-        t_data.head()\n-        ax = t_data.plot(\n-            ax=axes[i // 2, i % 2],\n-            color=c,\n-            title=\"{} - {}\".format(titles[i], key),\n-            rot=25,\n-        )\n-        ax.legend([titles[i]])\n-    plt.tight_layout()\n-\n-\n-show_raw_visualization(df)\n-\n-\n-\"\"\"\n-## Data Preprocessing\n-\n-Here we are picking ~300,000 data points for training. Observation is recorded every\n-10 mins, that means 6 times per hour. We will resample one point per hour since no\n-drastic change is expected within 60 minutes. We do this via the `sampling_rate`\n-argument in `timeseries_dataset_from_array` utility.\n-\n-We are tracking data from past 720 timestamps (720/6=120 hours). This data will be\n-used to predict the temperature after 72 timestamps (72/6=12 hours).\n-\n-Since every feature has values with\n-varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before\n-training a neural network.\n-We do this by subtracting the mean and dividing by the standard deviation of each feature.\n-\n-71.5 % of the data will be used to train the model, i.e. 300,693 rows. `split_fraction` can\n-be changed to alter this percentage.\n-\n-The model is shown data for first 5 days i.e. 720 observations, that are sampled every\n-hour. The temperature after 72 (12 hours * 6 observation per hour) observation will be\n-used as a label.\n-\"\"\"\n-\n-split_fraction = 0.715\n-train_split = int(split_fraction * int(df.shape[0]))\n-step = 6\n-\n-past = 720\n-future = 72\n-learning_rate = 0.001\n-batch_size = 256\n-epochs = 10\n-\n-\n-def normalize(data, train_split):\n-    data_mean = data[:train_split].mean(axis=0)\n-    data_std = data[:train_split].std(axis=0)\n-    return (data - data_mean) / data_std\n-\n-\n-\"\"\"\n-We can see from the correlation heatmap, few parameters like Relative Humidity and\n-Specific Humidity are redundant. Hence we will be using select features, not all.\n-\"\"\"\n-\n-print(\n-    \"The selected parameters are:\",\n-    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n-)\n-selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n-features = df[selected_features]\n-features.index = df[date_time_key]\n-features.head()\n-\n-features = normalize(features.values, train_split)\n-features = pd.DataFrame(features)\n-features.head()\n-\n-train_data = features.loc[0 : train_split - 1]\n-val_data = features.loc[train_split:]\n-\n-\"\"\"\n-# Training dataset\n-\n-The training dataset labels starts from the 792nd observation (720 + 72).\n-\"\"\"\n-\n-start = past + future\n-end = start + train_split\n-\n-x_train = train_data[[i for i in range(7)]].values\n-y_train = features.iloc[start:end][[1]]\n-\n-sequence_length = int(past / step)\n-\n-\"\"\"\n-The `timeseries_dataset_from_array` function takes in a sequence of data-points gathered at\n-equal intervals, along with time series parameters such as length of the\n-sequences/windows, spacing between two sequence/windows, etc., to produce batches of\n-sub-timeseries inputs and targets sampled from the main timeseries.\n-\"\"\"\n-\n-dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n-    x_train,\n-    y_train,\n-    sequence_length=sequence_length,\n-    sampling_rate=step,\n-    batch_size=batch_size,\n-)\n-\n-\"\"\"\n-## Validation dataset\n-\n-The validation dataset must not contain the last 792 rows as we won't have label data for\n-those records, hence 792 must be subtracted from the end of the data.\n-\n-The validation label dataset must start from 792 after train_split, hence we must add\n-past + future (792) to label_start.\n-\"\"\"\n-\n-x_end = len(val_data) - past - future\n-\n-label_start = train_split + past + future\n-\n-x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n-y_val = features.iloc[label_start:][[1]]\n-\n-dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n-    x_val,\n-    y_val,\n-    sequence_length=sequence_length,\n-    sampling_rate=step,\n-    batch_size=batch_size,\n-)\n-\n-\n-for batch in dataset_train.take(1):\n-    inputs, targets = batch\n-\n-print(\"Input shape:\", inputs.numpy().shape)\n-print(\"Target shape:\", targets.numpy().shape)\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n-lstm_out = keras.layers.LSTM(32)(inputs)\n-outputs = keras.layers.Dense(1)(lstm_out)\n-\n-model = keras.Model(inputs=inputs, outputs=outputs)\n-model.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\"\n-)\n-model.summary()\n-\n-\"\"\"\n-We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n-the `EarlyStopping` callback to interrupt training when the validation loss\n-is not longer improving.\n-\"\"\"\n-\n-path_checkpoint = \"model_checkpoint.weights.h5\"\n-es_callback = keras.callbacks.EarlyStopping(\n-    monitor=\"val_loss\", min_delta=0, patience=5\n-)\n-\n-modelckpt_callback = keras.callbacks.ModelCheckpoint(\n-    monitor=\"val_loss\",\n-    filepath=path_checkpoint,\n-    verbose=1,\n-    save_weights_only=True,\n-    save_best_only=True,\n-)\n-\n-history = model.fit(\n-    dataset_train,\n-    epochs=epochs,\n-    validation_data=dataset_val,\n-    callbacks=[es_callback, modelckpt_callback],\n-)\n-\n-\"\"\"\n-We can visualize the loss with the function below. After one point, the loss stops\n-decreasing.\n-\"\"\"\n-\n-\n-def visualize_loss(history, title):\n-    loss = history.history[\"loss\"]\n-    val_loss = history.history[\"val_loss\"]\n-    epochs = range(len(loss))\n-    plt.figure()\n-    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n-    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n-    plt.title(title)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(\"Loss\")\n-    plt.legend()\n-    plt.show()\n-\n-\n-visualize_loss(history, \"Training and Validation Loss\")\n-\n-\"\"\"\n-## Prediction\n-\n-The trained model above is now able to make predictions for 5 sets of values from\n-validation set.\n-\"\"\"\n-\n-\n-def show_plot(plot_data, delta, title):\n-    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n-    marker = [\".-\", \"rx\", \"go\"]\n-    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n-    if delta:\n-        future = delta\n-    else:\n-        future = 0\n-\n-    plt.title(title)\n-    for i, val in enumerate(plot_data):\n-        if i:\n-            plt.plot(\n-                future, plot_data[i], marker[i], markersize=10, label=labels[i]\n-            )\n-        else:\n-            plt.plot(\n-                time_steps, plot_data[i].flatten(), marker[i], label=labels[i]\n-            )\n-    plt.legend()\n-    plt.xlim([time_steps[0], (future + 5) * 2])\n-    plt.xlabel(\"Time-Step\")\n-    plt.show()\n-    return\n-\n-\n-for x, y in dataset_val.take(5):\n-    show_plot(\n-        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n-        12,\n-        \"Single Step Prediction\",\n-    )\n\n@@ -1,578 +0,0 @@\n-\"\"\"\n-Title: Classification using Attention-based Deep Multiple Instance Learning (MIL).\n-Author: [Mohamad Jaber](https://www.linkedin.com/in/mohamadjaber1/)\n-Date created: 2021/08/16\n-Last modified: 2021/11/25\n-Description: MIL approach to classify bags of instances and get their individual instance score.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-### What is Multiple Instance Learning (MIL)?\n-\n-Usually, with supervised learning algorithms, the learner receives labels for a set of\n-instances. In the case of MIL, the learner receives labels for a set of bags, each of which\n-contains a set of instances. The bag is labeled positive if it contains at least\n-one positive instance, and negative if it does not contain any.\n-\n-### Motivation\n-\n-It is often assumed in image classification tasks that each image clearly represents a\n-class label. In medical imaging (e.g. computational pathology, etc.) an *entire image*\n-is represented by a single class label (cancerous/non-cancerous) or a region of interest\n-could be given. However, one will be interested in knowing which patterns in the image\n-is actually causing it to belong to that class. In this context, the image(s) will be\n-divided and the subimages will form the bag of instances.\n-\n-Therefore, the goals are to:\n-\n-1. Learn a model to predict a class label for a bag of instances.\n-2. Find out which instances within the bag caused a position class label\n-prediction.\n-\n-### Implementation\n-\n-The following steps describe how the model works:\n-\n-1. The feature extractor layers extract feature embeddings.\n-2. The embeddings are fed into the MIL attention layer to get\n-the attention scores. The layer is designed as permutation-invariant.\n-3. Input features and their corresponding attention scores are multiplied together.\n-4. The resulting output is passed to a softmax function for classification.\n-\n-### References\n-\n-- [Attention-based Deep Multiple Instance Learning](https://arxiv.org/abs/1802.04712).\n-- Some of the attention operator code implementation was inspired from https://github.com/utayao/Atten_Deep_MIL.\n-- Imbalanced data [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)\n-by TensorFlow.\n-\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-from keras import ops\n-from tqdm import tqdm\n-from matplotlib import pyplot as plt\n-\n-plt.style.use(\"ggplot\")\n-\n-\"\"\"\n-## Create dataset\n-\n-We will create a set of bags and assign their labels according to their contents.\n-If at least one positive instance\n-is available in a bag, the bag is considered as a positive bag. If it does not contain any\n-positive instance, the bag will be considered as negative.\n-\n-### Configuration parameters\n-\n-- `POSITIVE_CLASS`: The desired class to be kept in the positive bag.\n-- `BAG_COUNT`: The number of training bags.\n-- `VAL_BAG_COUNT`: The number of validation bags.\n-- `BAG_SIZE`: The number of instances in a bag.\n-- `PLOT_SIZE`: The number of bags to plot.\n-- `ENSEMBLE_AVG_COUNT`: The number of models to create and average together. (Optional:\n-often results in better performance - set to 1 for single model)\n-\"\"\"\n-\n-POSITIVE_CLASS = 1\n-BAG_COUNT = 1000\n-VAL_BAG_COUNT = 300\n-BAG_SIZE = 3\n-PLOT_SIZE = 3\n-ENSEMBLE_AVG_COUNT = 1\n-\n-\"\"\"\n-### Prepare bags\n-\n-Since the attention operator is a permutation-invariant operator, an instance with a\n-positive class label is randomly placed among the instances in the positive bag.\n-\"\"\"\n-\n-\n-def create_bags(\n-    input_data, input_labels, positive_class, bag_count, instance_count\n-):\n-    # Set up bags.\n-    bags = []\n-    bag_labels = []\n-\n-    # Normalize input data.\n-    input_data = np.divide(input_data, 255.0)\n-\n-    # Count positive samples.\n-    count = 0\n-\n-    for _ in range(bag_count):\n-        # Pick a fixed size random subset of samples.\n-        index = np.random.choice(\n-            input_data.shape[0], instance_count, replace=False\n-        )\n-        instances_data = input_data[index]\n-        instances_labels = input_labels[index]\n-\n-        # By default, all bags are labeled as 0.\n-        bag_label = 0\n-\n-        # Check if there is at least a positive class in the bag.\n-        if positive_class in instances_labels:\n-            # Positive bag will be labeled as 1.\n-            bag_label = 1\n-            count += 1\n-\n-        bags.append(instances_data)\n-        bag_labels.append(np.array([bag_label]))\n-\n-    print(f\"Positive bags: {count}\")\n-    print(f\"Negative bags: {bag_count - count}\")\n-\n-    return (list(np.swapaxes(bags, 0, 1)), np.array(bag_labels))\n-\n-\n-# Load the MNIST dataset.\n-(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n-\n-# Create training data.\n-train_data, train_labels = create_bags(\n-    x_train, y_train, POSITIVE_CLASS, BAG_COUNT, BAG_SIZE\n-)\n-\n-# Create validation data.\n-val_data, val_labels = create_bags(\n-    x_val, y_val, POSITIVE_CLASS, VAL_BAG_COUNT, BAG_SIZE\n-)\n-\n-\"\"\"\n-## Create the model\n-\n-We will now build the attention layer, prepare some utilities, then build and train the\n-entire model.\n-\n-### Attention operator implementation\n-\n-The output size of this layer is decided by the size of a single bag.\n-\n-The attention mechanism uses a weighted average of instances in a bag, in which the sum\n-of the weights must equal to 1 (invariant of the bag size).\n-\n-The weight matrices (parameters) are **w** and **v**. To include positive and negative\n-values, hyperbolic tangent element-wise non-linearity is utilized.\n-\n-A **Gated attention mechanism** can be used to deal with complex relations. Another weight\n-matrix, **u**, is added to the computation.\n-A sigmoid non-linearity is used to overcome approximately linear behavior for *x* ∈ [−1, 1]\n-by hyperbolic tangent non-linearity.\n-\"\"\"\n-\n-\n-class MILAttentionLayer(layers.Layer):\n-    \"\"\"Implementation of the attention-based Deep MIL layer.\n-\n-    Args:\n-      weight_params_dim: Positive Integer. Dimension of the weight matrix.\n-      kernel_initializer: Initializer for the `kernel` matrix.\n-      kernel_regularizer: Regularizer function applied to the `kernel` matrix.\n-      use_gated: Boolean, whether or not to use the gated mechanism.\n-\n-    Returns:\n-      List of 2D tensors with BAG_SIZE length.\n-      The tensors are the attention scores after softmax with shape `(batch_size, 1)`.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        weight_params_dim,\n-        kernel_initializer=\"glorot_uniform\",\n-        kernel_regularizer=None,\n-        use_gated=False,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        self.weight_params_dim = weight_params_dim\n-        self.use_gated = use_gated\n-\n-        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n-        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n-\n-        self.v_init = self.kernel_initializer\n-        self.w_init = self.kernel_initializer\n-        self.u_init = self.kernel_initializer\n-\n-        self.v_regularizer = self.kernel_regularizer\n-        self.w_regularizer = self.kernel_regularizer\n-        self.u_regularizer = self.kernel_regularizer\n-\n-    def build(self, input_shape):\n-        # Input shape.\n-        # List of 2D tensors with shape: (batch_size, input_dim).\n-        input_dim = input_shape[0][1]\n-\n-        self.v_weight_params = self.add_weight(\n-            shape=(input_dim, self.weight_params_dim),\n-            initializer=self.v_init,\n-            name=\"v\",\n-            regularizer=self.v_regularizer,\n-            trainable=True,\n-        )\n-\n-        self.w_weight_params = self.add_weight(\n-            shape=(self.weight_params_dim, 1),\n-            initializer=self.w_init,\n-            name=\"w\",\n-            regularizer=self.w_regularizer,\n-            trainable=True,\n-        )\n-\n-        if self.use_gated:\n-            self.u_weight_params = self.add_weight(\n-                shape=(input_dim, self.weight_params_dim),\n-                initializer=self.u_init,\n-                name=\"u\",\n-                regularizer=self.u_regularizer,\n-                trainable=True,\n-            )\n-        else:\n-            self.u_weight_params = None\n-\n-        self.input_built = True\n-\n-    def call(self, inputs):\n-        # Assigning variables from the number of inputs.\n-        instances = [\n-            self.compute_attention_scores(instance) for instance in inputs\n-        ]\n-\n-        # Stack instances into a single tensor.\n-        instances = ops.stack(instances)\n-\n-        # Apply softmax over instances such that the output summation is equal to 1.\n-        alpha = ops.softmax(instances, axis=0)\n-\n-        # Split to recreate the same array of tensors we had as inputs.\n-        return [alpha[i] for i in range(alpha.shape[0])]\n-\n-    def compute_attention_scores(self, instance):\n-        # Reserve in-case \"gated mechanism\" used.\n-        original_instance = instance\n-\n-        # tanh(v*h_k^T)\n-        instance = ops.tanh(\n-            ops.tensordot(instance, self.v_weight_params, axes=1)\n-        )\n-\n-        # for learning non-linear relations efficiently.\n-        if self.use_gated:\n-            instance = instance * ops.sigmoid(\n-                ops.tensordot(original_instance, self.u_weight_params, axes=1)\n-            )\n-\n-        # w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))\n-        return ops.tensordot(instance, self.w_weight_params, axes=1)\n-\n-\n-\"\"\"\n-## Visualizer tool\n-\n-Plot the number of bags (given by `PLOT_SIZE`) with respect to the class.\n-\n-Moreover, if activated, the class label prediction with its associated instance score\n-for each bag (after the model has been trained) can be seen.\n-\"\"\"\n-\n-\n-def plot(data, labels, bag_class, predictions=None, attention_weights=None):\n-    \"\"\" \"Utility for plotting bags and attention weights.\n-\n-    Args:\n-      data: Input data that contains the bags of instances.\n-      labels: The associated bag labels of the input data.\n-      bag_class: String name of the desired bag class.\n-        The options are: \"positive\" or \"negative\".\n-      predictions: Class labels model predictions.\n-      If you don't specify anything, ground truth labels will be used.\n-      attention_weights: Attention weights for each instance within the input data.\n-      If you don't specify anything, the values won't be displayed.\n-    \"\"\"\n-    return  ## TODO\n-    labels = np.array(labels).reshape(-1)\n-\n-    if bag_class == \"positive\":\n-        if predictions is not None:\n-            labels = np.where(predictions.argmax(1) == 1)[0]\n-            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n-\n-        else:\n-            labels = np.where(labels == 1)[0]\n-            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n-\n-    elif bag_class == \"negative\":\n-        if predictions is not None:\n-            labels = np.where(predictions.argmax(1) == 0)[0]\n-            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n-        else:\n-            labels = np.where(labels == 0)[0]\n-            bags = np.array(data)[:, labels[0:PLOT_SIZE]]\n-\n-    else:\n-        print(f\"There is no class {bag_class}\")\n-        return\n-\n-    print(f\"The bag class label is {bag_class}\")\n-    for i in range(PLOT_SIZE):\n-        figure = plt.figure(figsize=(8, 8))\n-        print(f\"Bag number: {labels[i]}\")\n-        for j in range(BAG_SIZE):\n-            image = bags[j][i]\n-            figure.add_subplot(1, BAG_SIZE, j + 1)\n-            plt.grid(False)\n-            if attention_weights is not None:\n-                plt.title(np.around(attention_weights[labels[i]][j], 2))\n-            plt.imshow(image)\n-        plt.show()\n-\n-\n-# Plot some of validation data bags per class.\n-plot(val_data, val_labels, \"positive\")\n-plot(val_data, val_labels, \"negative\")\n-\n-\"\"\"\n-## Create model\n-\n-First we will create some embeddings per instance, invoke the attention operator and then\n-use the softmax function to output the class probabilities.\n-\"\"\"\n-\n-\n-def create_model(instance_shape):\n-    # Extract features from inputs.\n-    inputs, embeddings = [], []\n-    shared_dense_layer_1 = layers.Dense(128, activation=\"relu\")\n-    shared_dense_layer_2 = layers.Dense(64, activation=\"relu\")\n-    for _ in range(BAG_SIZE):\n-        inp = layers.Input(instance_shape)\n-        flatten = layers.Flatten()(inp)\n-        dense_1 = shared_dense_layer_1(flatten)\n-        dense_2 = shared_dense_layer_2(dense_1)\n-        inputs.append(inp)\n-        embeddings.append(dense_2)\n-\n-    # Invoke the attention layer.\n-    alpha = MILAttentionLayer(\n-        weight_params_dim=256,\n-        kernel_regularizer=keras.regularizers.L2(0.01),\n-        use_gated=True,\n-        name=\"alpha\",\n-    )(embeddings)\n-\n-    # Multiply attention weights with the input layers.\n-    multiply_layers = [\n-        layers.multiply([alpha[i], embeddings[i]]) for i in range(len(alpha))\n-    ]\n-\n-    # Concatenate layers.\n-    concat = layers.concatenate(multiply_layers, axis=1)\n-\n-    # Classification output node.\n-    output = layers.Dense(2, activation=\"softmax\")(concat)\n-\n-    return keras.Model(inputs, output)\n-\n-\n-\"\"\"\n-## Class weights\n-\n-Since this kind of problem could simply turn into imbalanced data classification problem,\n-class weighting should be considered.\n-\n-Let's say there are 1000 bags. There often could be cases were ~90 % of the bags do not\n-contain any positive label and ~10 % do.\n-Such data can be referred to as **Imbalanced data**.\n-\n-Using class weights, the model will tend to give a higher weight to the rare class.\n-\"\"\"\n-\n-\n-def compute_class_weights(labels):\n-    # Count number of postive and negative bags.\n-    negative_count = len(np.where(labels == 0)[0])\n-    positive_count = len(np.where(labels == 1)[0])\n-    total_count = negative_count + positive_count\n-\n-    # Build class weight dictionary.\n-    return {\n-        0: (1 / negative_count) * (total_count / 2),\n-        1: (1 / positive_count) * (total_count / 2),\n-    }\n-\n-\n-\"\"\"\n-## Build and train model\n-\n-The model is built and trained in this section.\n-\"\"\"\n-\n-\n-def train(train_data, train_labels, val_data, val_labels, model):\n-    # Train model.\n-    # Prepare callbacks.\n-    # Path where to save best weights.\n-\n-    # Take the file name from the wrapper.\n-    file_path = \"/tmp/best_model.weights.h5\"\n-\n-    # Initialize model checkpoint callback.\n-    model_checkpoint = keras.callbacks.ModelCheckpoint(\n-        file_path,\n-        monitor=\"val_loss\",\n-        verbose=0,\n-        mode=\"min\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    # Initialize early stopping callback.\n-    # The model performance is monitored across the validation data and stops training\n-    # when the generalization error cease to decrease.\n-    early_stopping = keras.callbacks.EarlyStopping(\n-        monitor=\"val_loss\", patience=10, mode=\"min\"\n-    )\n-\n-    # Compile model.\n-    model.compile(\n-        optimizer=\"adam\",\n-        loss=\"sparse_categorical_crossentropy\",\n-        metrics=[\"accuracy\"],\n-    )\n-\n-    # Fit model.\n-    model.fit(\n-        train_data,\n-        train_labels,\n-        validation_data=(val_data, val_labels),\n-        epochs=20,\n-        class_weight=compute_class_weights(train_labels),\n-        batch_size=1,\n-        callbacks=[early_stopping, model_checkpoint],\n-        verbose=0,\n-    )\n-\n-    # Load best weights.\n-    model.load_weights(file_path)\n-\n-    return model\n-\n-\n-# Building model(s).\n-instance_shape = train_data[0][0].shape\n-models = [create_model(instance_shape) for _ in range(ENSEMBLE_AVG_COUNT)]\n-\n-# Show single model architecture.\n-print(models[0].summary())\n-\n-# Training model(s).\n-trained_models = [\n-    train(train_data, train_labels, val_data, val_labels, model)\n-    for model in tqdm(models)\n-]\n-\n-\"\"\"\n-## Model evaluation\n-\n-The models are now ready for evaluation.\n-With each model we also create an associated intermediate model to get the\n-weights from the attention layer.\n-\n-We will compute a prediction for each of our `ENSEMBLE_AVG_COUNT` models, and\n-average them together for our final prediction.\n-\"\"\"\n-\n-\n-def predict(data, labels, trained_models):\n-    # Collect info per model.\n-    models_predictions = []\n-    models_attention_weights = []\n-    models_losses = []\n-    models_accuracies = []\n-\n-    for model in trained_models:\n-        # Predict output classes on data.\n-        predictions = model.predict(data)\n-        models_predictions.append(predictions)\n-\n-        # Create intermediate model to get MIL attention layer weights.\n-        intermediate_model = keras.Model(\n-            model.input, model.get_layer(\"alpha\").output\n-        )\n-\n-        # Predict MIL attention layer weights.\n-        intermediate_predictions = intermediate_model.predict(data)\n-\n-        attention_weights = np.squeeze(\n-            np.swapaxes(intermediate_predictions, 1, 0)\n-        )\n-        models_attention_weights.append(attention_weights)\n-\n-        loss, accuracy = model.evaluate(data, labels, verbose=0)\n-        models_losses.append(loss)\n-        models_accuracies.append(accuracy)\n-\n-    print(\n-        f\"The average loss and accuracy are {np.sum(models_losses, axis=0) / ENSEMBLE_AVG_COUNT:.2f}\"\n-        f\" and {100 * np.sum(models_accuracies, axis=0) / ENSEMBLE_AVG_COUNT:.2f} % resp.\"\n-    )\n-\n-    return (\n-        np.sum(models_predictions, axis=0) / ENSEMBLE_AVG_COUNT,\n-        np.sum(models_attention_weights, axis=0) / ENSEMBLE_AVG_COUNT,\n-    )\n-\n-\n-# Evaluate and predict classes and attention scores on validation data.\n-class_predictions, attention_params = predict(\n-    val_data, val_labels, trained_models\n-)\n-\n-# Plot some results from our validation data.\n-plot(\n-    val_data,\n-    val_labels,\n-    \"positive\",\n-    predictions=class_predictions,\n-    attention_weights=attention_params,\n-)\n-plot(\n-    val_data,\n-    val_labels,\n-    \"negative\",\n-    predictions=class_predictions,\n-    attention_weights=attention_params,\n-)\n-\n-\"\"\"\n-## Conclusion\n-\n-From the above plot, you can notice that the weights always sum to 1. In a\n-positively predict bag, the instance which resulted in the positive labeling will have\n-a substantially higher attention score than the rest of the bag. However, in a negatively\n-predicted bag, there are two cases:\n-\n-* All instances will have approximately similar scores.\n-* An instance will have relatively higher score (but not as high as of a positive instance).\n-This is because the feature space of this instance is close to that of the positive instance.\n-\n-## Remarks\n-\n-- If the model is overfit, the weights will be equally distributed for all bags. Hence,\n-the regularization techniques are necessary.\n-- In the paper, the bag sizes can differ from one bag to another. For simplicity, the\n-bag sizes are fixed here.\n-- In order not to rely on the random initial weights of a single model, averaging ensemble\n-methods should be considered.\n-\"\"\"\n\n@@ -1,169 +0,0 @@\n-\"\"\"\n-Title: Convolutional autoencoder for image denoising\n-Author: [Santiago L. Valdarrama](https://twitter.com/svpino)\n-Date created: 2021/03/01\n-Last modified: 2021/03/01\n-Description: How to train a deep convolutional autoencoder for image denoising.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates how to implement a deep convolutional autoencoder\n-for image denoising, mapping noisy digits images from the MNIST dataset to\n-clean digits images. This implementation is based on an original blog post\n-titled [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n-by [François Chollet](https://twitter.com/fchollet).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-from keras import layers\n-from keras.datasets import mnist\n-from keras.models import Model\n-\n-\n-def preprocess(array):\n-    \"\"\"Normalizes the supplied array and reshapes it.\"\"\"\n-    array = array.astype(\"float32\") / 255.0\n-    array = np.reshape(array, (len(array), 28, 28, 1))\n-    return array\n-\n-\n-def noise(array):\n-    \"\"\"Adds random noise to each image in the supplied array.\"\"\"\n-    noise_factor = 0.4\n-    noisy_array = array + noise_factor * np.random.normal(\n-        loc=0.0, scale=1.0, size=array.shape\n-    )\n-\n-    return np.clip(noisy_array, 0.0, 1.0)\n-\n-\n-def display(array1, array2):\n-    \"\"\"Displays ten random images from each array.\"\"\"\n-    n = 10\n-    indices = np.random.randint(len(array1), size=n)\n-    images1 = array1[indices, :]\n-    images2 = array2[indices, :]\n-\n-    plt.figure(figsize=(20, 4))\n-    for i, (image1, image2) in enumerate(zip(images1, images2)):\n-        ax = plt.subplot(2, n, i + 1)\n-        plt.imshow(image1.reshape(28, 28))\n-        plt.gray()\n-        ax.get_xaxis().set_visible(False)\n-        ax.get_yaxis().set_visible(False)\n-\n-        ax = plt.subplot(2, n, i + 1 + n)\n-        plt.imshow(image2.reshape(28, 28))\n-        plt.gray()\n-        ax.get_xaxis().set_visible(False)\n-        ax.get_yaxis().set_visible(False)\n-\n-    plt.show()\n-\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-# Since we only need images from the dataset to encode and decode, we\n-# won't use the labels.\n-(train_data, _), (test_data, _) = mnist.load_data()\n-\n-# Normalize and reshape the data\n-train_data = preprocess(train_data)\n-test_data = preprocess(test_data)\n-\n-# Create a copy of the data with added noise\n-noisy_train_data = noise(train_data)\n-noisy_test_data = noise(test_data)\n-\n-# Display the train data and a version of it with added noise\n-display(train_data, noisy_train_data)\n-\n-\"\"\"\n-## Build the autoencoder\n-\n-We are going to use the Functional API to build our convolutional autoencoder.\n-\"\"\"\n-\n-input = layers.Input(shape=(28, 28, 1))\n-\n-# Encoder\n-x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n-x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n-x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n-x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n-\n-# Decoder\n-x = layers.Conv2DTranspose(\n-    32, (3, 3), strides=2, activation=\"relu\", padding=\"same\"\n-)(x)\n-x = layers.Conv2DTranspose(\n-    32, (3, 3), strides=2, activation=\"relu\", padding=\"same\"\n-)(x)\n-x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n-\n-# Autoencoder\n-autoencoder = Model(input, x)\n-autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n-autoencoder.summary()\n-\n-\"\"\"\n-Now we can train our autoencoder using `train_data` as both our input data\n-and target. Notice we are setting up the validation data using the same\n-format.\n-\"\"\"\n-\n-autoencoder.fit(\n-    x=train_data,\n-    y=train_data,\n-    epochs=50,\n-    batch_size=128,\n-    shuffle=True,\n-    validation_data=(test_data, test_data),\n-)\n-\n-\"\"\"\n-Let's predict on our test dataset and display the original image together with\n-the prediction from our autoencoder.\n-\n-Notice how the predictions are pretty close to the original images, although\n-not quite the same.\n-\"\"\"\n-\n-predictions = autoencoder.predict(test_data)\n-display(test_data, predictions)\n-\n-\"\"\"\n-Now that we know that our autoencoder works, let's retrain it using the noisy\n-data as our input and the clean data as our target. We want our autoencoder to\n-learn how to denoise the images.\n-\"\"\"\n-\n-autoencoder.fit(\n-    x=noisy_train_data,\n-    y=train_data,\n-    epochs=100,\n-    batch_size=128,\n-    shuffle=True,\n-    validation_data=(noisy_test_data, test_data),\n-)\n-\n-\"\"\"\n-Let's now predict on the noisy data and display the results of our autoencoder.\n-\n-Notice how the autoencoder does an amazing job at removing the noise from the\n-input images.\n-\"\"\"\n-\n-predictions = autoencoder.predict(noisy_test_data)\n-display(noisy_test_data, predictions)\n\n@@ -1,446 +0,0 @@\n-\"\"\"\n-Title: Compact Convolutional Transformers\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/06/30\n-Last modified: 2023/08/07\n-Description: Compact Convolutional Transformers for efficient image classification.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com), [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/)\n-\"\"\"\n-\"\"\"\n-As discussed in the [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) paper,\n-a Transformer-based architecture for vision typically requires a larger dataset than\n-usual, as well as a longer pre-training schedule. [ImageNet-1k](http://imagenet.org/)\n-(which has about a million images) is considered to fall under the medium-sized data regime with\n-respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a typical\n-Transformer-based architecture) do not have well-informed inductive biases (such as\n-convolutions for processing images). This begs the question: can't we combine the\n-benefits of convolution and the benefits of Transformers\n-in a single network architecture? These benefits include parameter-efficiency, and\n-self-attention to process long-range and global dependencies (interactions between\n-different regions in an image).\n-\n-In [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704),\n-Hassani et al. present an approach for doing exactly this. They proposed the\n-**Compact Convolutional Transformer** (CCT) architecture. In this example, we will work on an\n-implementation of CCT and we will see how well it performs on the CIFAR-10 dataset.\n-\n-If you are unfamiliar with the concept of self-attention or Transformers, you can read\n-[this chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)\n-from  François Chollet's book *Deep Learning with Python*. This example uses\n-code snippets from another example,\n-[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-from keras import layers\n-import keras\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-\"\"\"\n-## Hyperparameters and constants\n-\"\"\"\n-\n-positional_emb = True\n-conv_layers = 2\n-projection_dim = 128\n-\n-num_heads = 2\n-transformer_units = [\n-    projection_dim,\n-    projection_dim,\n-]\n-transformer_layers = 2\n-stochastic_depth_rate = 0.1\n-\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 128\n-num_epochs = 30\n-image_size = 32\n-\n-\"\"\"\n-## Load CIFAR-10 dataset\n-\"\"\"\n-\n-num_classes = 10\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\"\"\"\n-## The CCT tokenizer\n-\n-The first recipe introduced by the CCT authors is the tokenizer for processing the\n-images. In a standard ViT, images are organized into uniform *non-overlapping* patches.\n-This eliminates the boundary-level information present in between different patches. This\n-is important for a neural network to effectively exploit the locality information. The\n-figure below presents an illustration of how images are organized into patches.\n-\n-![](https://i.imgur.com/IkBK9oY.png)\n-\n-We already know that convolutions are quite good at exploiting locality information. So,\n-based on this, the authors introduce an all-convolution mini-network to produce image\n-patches.\n-\"\"\"\n-\n-\n-class CCTTokenizer(layers.Layer):\n-    def __init__(\n-        self,\n-        kernel_size=3,\n-        stride=1,\n-        padding=1,\n-        pooling_kernel_size=3,\n-        pooling_stride=2,\n-        num_conv_layers=conv_layers,\n-        num_output_channels=[64, 128],\n-        positional_emb=positional_emb,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        # This is our tokenizer.\n-        self.conv_model = keras.Sequential()\n-        for i in range(num_conv_layers):\n-            self.conv_model.add(\n-                layers.Conv2D(\n-                    num_output_channels[i],\n-                    kernel_size,\n-                    stride,\n-                    padding=\"valid\",\n-                    use_bias=False,\n-                    activation=\"relu\",\n-                    kernel_initializer=\"he_normal\",\n-                )\n-            )\n-            self.conv_model.add(layers.ZeroPadding2D(padding))\n-            self.conv_model.add(\n-                layers.MaxPooling2D(pooling_kernel_size, pooling_stride, \"same\")\n-            )\n-\n-        self.positional_emb = positional_emb\n-\n-    def call(self, images):\n-        outputs = self.conv_model(images)\n-        # After passing the images through our mini-network the spatial dimensions\n-        # are flattened to form sequences.\n-        reshaped = keras.ops.reshape(\n-            outputs,\n-            (\n-                -1,\n-                keras.ops.shape(outputs)[1] * keras.ops.shape(outputs)[2],\n-                keras.ops.shape(outputs)[-1],\n-            ),\n-        )\n-        return reshaped\n-\n-\n-\"\"\"\n-Positional embeddings are optional in CCT. If we want to use them, we can use\n-the Layer defined below.\n-\"\"\"\n-\n-\n-class PositionEmbedding(keras.layers.Layer):\n-    def __init__(\n-        self,\n-        sequence_length,\n-        initializer=\"glorot_uniform\",\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        if sequence_length is None:\n-            raise ValueError(\n-                \"`sequence_length` must be an Integer, received `None`.\"\n-            )\n-        self.sequence_length = int(sequence_length)\n-        self.initializer = keras.initializers.get(initializer)\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"sequence_length\": self.sequence_length,\n-                \"initializer\": keras.initializers.serialize(self.initializer),\n-            }\n-        )\n-        return config\n-\n-    def build(self, input_shape):\n-        feature_size = input_shape[-1]\n-        self.position_embeddings = self.add_weight(\n-            name=\"embeddings\",\n-            shape=[self.sequence_length, feature_size],\n-            initializer=self.initializer,\n-            trainable=True,\n-        )\n-\n-        super().build(input_shape)\n-\n-    def call(self, inputs, start_index=0):\n-        shape = keras.ops.shape(inputs)\n-        feature_length = shape[-1]\n-        sequence_length = shape[-2]\n-        # trim to match the length of the input sequence, which might be less\n-        # than the sequence_length of the layer.\n-        position_embeddings = keras.ops.convert_to_tensor(\n-            self.position_embeddings\n-        )\n-        position_embeddings = keras.ops.slice(\n-            position_embeddings,\n-            (start_index, 0),\n-            (sequence_length, feature_length),\n-        )\n-        return keras.ops.broadcast_to(position_embeddings, shape)\n-\n-    def compute_output_shape(self, input_shape):\n-        return input_shape\n-\n-\n-\"\"\"\n-## Sequence Pooling\n-Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only\n-the feature map corresponding to the class token is pooled and is then used for the\n-subsequent classification task (or any other downstream task).\n-\"\"\"\n-\n-\n-class SequencePooling(layers.Layer):\n-    def __init__(self):\n-        super().__init__()\n-        self.attention = layers.Dense(1)\n-\n-    def call(self, x):\n-        attention_weights = keras.ops.softmax(self.attention(x), axis=1)\n-        attention_weights = keras.ops.transpose(\n-            attention_weights, axes=(0, 2, 1)\n-        )\n-        weighted_representation = keras.ops.matmul(attention_weights, x)\n-        return keras.ops.squeeze(weighted_representation, -2)\n-\n-\n-\"\"\"\n-## Stochastic depth for regularization\n-\n-[Stochastic depth](https://arxiv.org/abs/1603.09382) is a regularization technique that\n-randomly drops a set of layers. During inference, the layers are kept as they are. It is\n-very much similar to [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) but only\n-that it operates on a block of layers rather than individual nodes present inside a\n-layer. In CCT, stochastic depth is used just before the residual blocks of a Transformers\n-encoder.\n-\"\"\"\n-\n-\n-# Referred from: github.com:rwightman/pytorch-image-models.\n-class StochasticDepth(layers.Layer):\n-    def __init__(self, drop_prop, **kwargs):\n-        super().__init__(**kwargs)\n-        self.drop_prob = drop_prop\n-        self.seed_generator = keras.random.SeedGenerator(1337)\n-\n-    def call(self, x, training=None):\n-        if training:\n-            keep_prob = 1 - self.drop_prob\n-            shape = (keras.ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n-            random_tensor = keep_prob + keras.random.uniform(\n-                shape, 0, 1, seed=self.seed_generator\n-            )\n-            random_tensor = keras.ops.floor(random_tensor)\n-            return (x / keep_prob) * random_tensor\n-        return x\n-\n-\n-\"\"\"\n-## MLP for the Transformers encoder\n-\"\"\"\n-\n-\n-def mlp(x, hidden_units, dropout_rate):\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=keras.ops.gelu)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Data augmentation\n-\n-In the [original paper](https://arxiv.org/abs/2104.05704), the authors use\n-[AutoAugment](https://arxiv.org/abs/1805.09501) to induce stronger regularization. For\n-this example, we will be using the standard geometric augmentations like random cropping\n-and flipping.\n-\"\"\"\n-\n-# Note the rescaling layer. These layers have pre-defined inference behavior.\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Rescaling(scale=1.0 / 255),\n-        layers.RandomCrop(image_size, image_size),\n-        layers.RandomFlip(\"horizontal\"),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-\n-\"\"\"\n-## The final CCT model\n-\n-In CCT, outputs from the Transformers encoder are weighted and then passed on to the final task-specific layer (in\n-this example, we do classification).\n-\"\"\"\n-\n-\n-def create_cct_model(\n-    image_size=image_size,\n-    input_shape=input_shape,\n-    num_heads=num_heads,\n-    projection_dim=projection_dim,\n-    transformer_units=transformer_units,\n-):\n-    inputs = layers.Input(input_shape)\n-\n-    # Augment data.\n-    augmented = data_augmentation(inputs)\n-\n-    # Encode patches.\n-    cct_tokenizer = CCTTokenizer()\n-    encoded_patches = cct_tokenizer(augmented)\n-\n-    # Apply positional embedding.\n-    if positional_emb:\n-        sequence_length = encoded_patches.shape[1]\n-        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n-            encoded_patches\n-        )\n-\n-    # Calculate Stochastic Depth probabilities.\n-    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n-\n-    # Create multiple layers of the Transformer block.\n-    for i in range(transformer_layers):\n-        # Layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n-\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-\n-        # Skip connection 1.\n-        attention_output = StochasticDepth(dpr[i])(attention_output)\n-        x2 = layers.Add()([attention_output, encoded_patches])\n-\n-        # Layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n-\n-        # MLP.\n-        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n-\n-        # Skip connection 2.\n-        x3 = StochasticDepth(dpr[i])(x3)\n-        encoded_patches = layers.Add()([x3, x2])\n-\n-    # Apply sequence pooling.\n-    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n-    weighted_representation = SequencePooling()(representation)\n-\n-    # Classify outputs.\n-    logits = layers.Dense(num_classes)(weighted_representation)\n-    # Create the Keras model.\n-    model = keras.Model(inputs=inputs, outputs=logits)\n-    return model\n-\n-\n-\"\"\"\n-## Model training and evaluation\n-\"\"\"\n-\n-\n-def run_experiment(model):\n-    optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n-\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=keras.losses.CategoricalCrossentropy(\n-            from_logits=True, label_smoothing=0.1\n-        ),\n-        metrics=[\n-            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-        ],\n-    )\n-\n-    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[checkpoint_callback],\n-    )\n-\n-    model.load_weights(checkpoint_filepath)\n-    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-    return history\n-\n-\n-cct_model = create_cct_model()\n-history = run_experiment(cct_model)\n-\n-\"\"\"\n-Let's now visualize the training progress of the model.\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"train_loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n-plt.xlabel(\"Epochs\")\n-plt.ylabel(\"Loss\")\n-plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n-plt.legend()\n-plt.grid()\n-plt.show()\n-\n-\"\"\"\n-The CCT model we just trained has just **0.4 million** parameters, and it gets us to\n-~79% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n-well. This means we can train this network for longer (perhaps with a bit more\n-regularization) and may obtain even better performance. This performance can further be\n-improved by additional recipes like cosine decay learning rate schedule, other data augmentation\n-techniques like [AutoAugment](https://arxiv.org/abs/1805.09501),\n-[MixUp](https://arxiv.org/abs/1710.09412) or\n-[Cutmix](https://arxiv.org/abs/1905.04899). With these modifications, the authors present\n-95.1% top-1 accuracy on the CIFAR-10 dataset. The authors also present a number of\n-experiments to study how the number of convolution blocks, Transformers layers, etc.\n-affect the final performance of CCTs.\n-\n-For a comparison, a ViT model takes about **4.7 million** parameters and **100\n-epochs** of training to reach a top-1 accuracy of 78.22% on the CIFAR-10 dataset. You can\n-refer to\n-[this notebook](https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb)\n-to know about the experimental setup.\n-\n-The authors also demonstrate the performance of Compact Convolutional Transformers on\n-NLP tasks and they report competitive results there.\n-\"\"\"\n\n@@ -1,298 +0,0 @@\n-\"\"\"\n-Title: Next-Frame Video Prediction with Convolutional LSTMs\n-Author: [Amogh Joshi](https://github.com/amogh7joshi)\n-Date created: 2021/06/02\n-Last modified: 2021/06/05\n-Description: How to build and train a convolutional LSTM model for next-frame video prediction.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The\n-[Convolutional LSTM](https://papers.nips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf)\n-architectures bring together time series processing and computer vision by\n-introducing a convolutional recurrent cell in a LSTM layer. In this example, we will explore the\n-Convolutional LSTM model in an application to next-frame prediction, the process\n-of predicting what video frames come next given a series of past frames.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-import keras\n-from keras import layers\n-\n-import io\n-import imageio\n-from IPython.display import Image, display\n-from ipywidgets import widgets, Layout, HBox\n-\n-\"\"\"\n-## Dataset Construction\n-\n-For this example, we will be using the\n-[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\n-dataset.\n-\n-We will download the dataset and then construct and\n-preprocess training and validation sets.\n-\n-For next-frame prediction, our model will be using a previous frame,\n-which we'll call `f_n`, to predict a new frame, called `f_(n + 1)`.\n-To allow the model to create these predictions, we'll need to process\n-the data such that we have \"shifted\" inputs and outputs, where the\n-input data is frame `x_n`, being used to predict frame `y_(n + 1)`.\n-\"\"\"\n-\n-# Download and load the dataset.\n-fpath = keras.utils.get_file(\n-    \"moving_mnist.npy\",\n-    \"http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\",\n-)\n-dataset = np.load(fpath)\n-\n-# Swap the axes representing the number of frames and number of data samples.\n-dataset = np.swapaxes(dataset, 0, 1)\n-# We'll pick out 1000 of the 10000 total examples and use those.\n-dataset = dataset[:1000, ...]\n-# Add a channel dimension since the images are grayscale.\n-dataset = np.expand_dims(dataset, axis=-1)\n-\n-# Split into train and validation sets using indexing to optimize memory.\n-indexes = np.arange(dataset.shape[0])\n-np.random.shuffle(indexes)\n-train_index = indexes[: int(0.9 * dataset.shape[0])]\n-val_index = indexes[int(0.9 * dataset.shape[0]) :]\n-train_dataset = dataset[train_index]\n-val_dataset = dataset[val_index]\n-\n-# Normalize the data to the 0-1 range.\n-train_dataset = train_dataset / 255\n-val_dataset = val_dataset / 255\n-\n-\n-# We'll define a helper function to shift the frames, where\n-# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n-def create_shifted_frames(data):\n-    x = data[:, 0 : data.shape[1] - 1, :, :]\n-    y = data[:, 1 : data.shape[1], :, :]\n-    return x, y\n-\n-\n-# Apply the processing function to the datasets.\n-x_train, y_train = create_shifted_frames(train_dataset)\n-x_val, y_val = create_shifted_frames(val_dataset)\n-\n-# Inspect the dataset.\n-print(\n-    \"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape)\n-)\n-print(\n-    \"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape)\n-)\n-\n-\"\"\"\n-## Data Visualization\n-\n-Our data consists of sequences of frames, each of which\n-are used to predict the upcoming frame. Let's take a look\n-at some of these sequential frames.\n-\"\"\"\n-\n-# Construct a figure on which we will visualize the images.\n-fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n-\n-# Plot each of the sequential images for one random data example.\n-data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n-for idx, ax in enumerate(axes.flat):\n-    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n-    ax.set_title(f\"Frame {idx + 1}\")\n-    ax.axis(\"off\")\n-\n-# Print information and display the figure.\n-print(f\"Displaying frames for example {data_choice}.\")\n-plt.show()\n-\n-\"\"\"\n-## Model Construction\n-\n-To build a Convolutional LSTM model, we will use the\n-`ConvLSTM2D` layer, which will accept inputs of shape\n-`(batch_size, num_frames, width, height, channels)`, and return\n-a prediction movie of the same shape.\n-\"\"\"\n-\n-# Construct the input layer with no definite frame size.\n-inp = layers.Input(shape=(None, *x_train.shape[2:]))\n-\n-# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n-# followed by a `Conv3D` layer for the spatiotemporal outputs.\n-x = layers.ConvLSTM2D(\n-    filters=64,\n-    kernel_size=(5, 5),\n-    padding=\"same\",\n-    return_sequences=True,\n-    activation=\"relu\",\n-)(inp)\n-x = layers.BatchNormalization()(x)\n-x = layers.ConvLSTM2D(\n-    filters=64,\n-    kernel_size=(3, 3),\n-    padding=\"same\",\n-    return_sequences=True,\n-    activation=\"relu\",\n-)(x)\n-x = layers.BatchNormalization()(x)\n-x = layers.ConvLSTM2D(\n-    filters=64,\n-    kernel_size=(1, 1),\n-    padding=\"same\",\n-    return_sequences=True,\n-    activation=\"relu\",\n-)(x)\n-x = layers.Conv3D(\n-    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n-)(x)\n-\n-# Next, we will build the complete model and compile it.\n-model = keras.models.Model(inp, x)\n-model.compile(\n-    loss=keras.losses.binary_crossentropy,\n-    optimizer=keras.optimizers.Adam(),\n-)\n-\n-\"\"\"\n-## Model Training\n-\n-With our model and data constructed, we can now train the model.\n-\"\"\"\n-\n-# Define some callbacks to improve training.\n-early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n-reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n-\n-# Define modifiable training hyperparameters.\n-epochs = 20\n-batch_size = 5\n-\n-# Fit the model to the training data.\n-model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=epochs,\n-    validation_data=(x_val, y_val),\n-    callbacks=[early_stopping, reduce_lr],\n-)\n-\n-\"\"\"\n-## Frame Prediction Visualizations\n-\n-With our model now constructed and trained, we can generate\n-some example frame predictions based on a new video.\n-\n-We'll pick a random example from the validation set and\n-then choose the first ten frames from them. From there, we can\n-allow the model to predict 10 new frames, which we can compare\n-to the ground truth frame predictions.\n-\"\"\"\n-\n-# Select a random example from the validation dataset.\n-example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n-\n-# Pick the first/last ten frames from the example.\n-frames = example[:10, ...]\n-original_frames = example[10:, ...]\n-\n-# Predict a new set of 10 frames.\n-for _ in range(10):\n-    # Extract the model's prediction and post-process it.\n-    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n-    new_prediction = np.squeeze(new_prediction, axis=0)\n-    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n-\n-    # Extend the set of prediction frames.\n-    frames = np.concatenate((frames, predicted_frame), axis=0)\n-\n-# Construct a figure for the original and new frames.\n-fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n-\n-# Plot the original frames.\n-for idx, ax in enumerate(axes[0]):\n-    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")\n-    ax.set_title(f\"Frame {idx + 11}\")\n-    ax.axis(\"off\")\n-\n-# Plot the new frames.\n-new_frames = frames[10:, ...]\n-for idx, ax in enumerate(axes[1]):\n-    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n-    ax.set_title(f\"Frame {idx + 11}\")\n-    ax.axis(\"off\")\n-\n-# Display the figure.\n-plt.show()\n-\n-\"\"\"\n-## Predicted Videos\n-\n-Finally, we'll pick a few examples from the validation set\n-and construct some GIFs with them to see the model's\n-predicted videos.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/conv-lstm)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/conv-lstm).\n-\"\"\"\n-\n-# Select a few random examples from the dataset.\n-examples = val_dataset[np.random.choice(range(len(val_dataset)), size=5)]\n-\n-# Iterate over the examples and predict the frames.\n-predicted_videos = []\n-for example in examples:\n-    # Pick the first/last ten frames from the example.\n-    frames = example[:10, ...]\n-    original_frames = example[10:, ...]\n-    new_predictions = np.zeros(shape=(10, *frames[0].shape))\n-\n-    # Predict a new set of 10 frames.\n-    for i in range(10):\n-        # Extract the model's prediction and post-process it.\n-        frames = example[: 10 + i + 1, ...]\n-        new_prediction = model.predict(np.expand_dims(frames, axis=0))\n-        new_prediction = np.squeeze(new_prediction, axis=0)\n-        predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n-\n-        # Extend the set of prediction frames.\n-        new_predictions[i] = predicted_frame\n-\n-    # Create and save GIFs for each of the ground truth/prediction images.\n-    for frame_set in [original_frames, new_predictions]:\n-        # Construct a GIF from the selected video frames.\n-        current_frames = np.squeeze(frame_set)\n-        current_frames = current_frames[..., np.newaxis] * np.ones(3)\n-        current_frames = (current_frames * 255).astype(np.uint8)\n-        current_frames = list(current_frames)\n-\n-        # Construct a GIF from the frames.\n-        with io.BytesIO() as gif:\n-            imageio.mimsave(gif, current_frames, \"GIF\", duration=200)\n-            predicted_videos.append(gif.getvalue())\n-\n-# Display the videos.\n-print(\" Truth\\tPrediction\")\n-for i in range(0, len(predicted_videos), 2):\n-    # Construct and display an `HBox` with the ground truth and prediction.\n-    box = HBox(\n-        [\n-            widgets.Image(value=predicted_videos[i]),\n-            widgets.Image(value=predicted_videos[i + 1]),\n-        ]\n-    )\n-    display(box)\n\n@@ -1,322 +0,0 @@\n-\"\"\"\n-Title: Image classification with ConvMixer\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/10/12\n-Last modified: 2021/10/12\n-Description: An all-convolutional network applied to patches of images.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Md Awsafur Rahman](https://awsaf49.github.io)\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Vision Transformers (ViT; [Dosovitskiy et al.](https://arxiv.org/abs/1612.00593)) extract\n-small patches from the input images, linearly project them, and then apply the\n-Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762)) blocks. The application\n-of ViTs to image recognition tasks is quickly becoming a promising area of research,\n-because ViTs eliminate the need to have strong inductive biases (such as convolutions) for\n-modeling locality. This presents them as a general computation primititive capable of\n-learning just from the training data with as minimal inductive priors as possible. ViTs\n-yield great downstream performance when trained with proper regularization, data\n-augmentation, and relatively large datasets.\n-\n-In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note:\n-at\n-the time of writing, it is a submission to the ICLR 2022 conference), the authors extend\n-the idea of using patches to train an all-convolutional network and demonstrate\n-competitive results. Their architecture namely **ConvMixer** uses recipes from the recent\n-isotrophic architectures like ViT, MLP-Mixer\n-([Tolstikhin et al.](https://arxiv.org/abs/2105.01601)), such as using the same\n-depth and resolution across different layers in the network, residual connections,\n-and so on.\n-\n-In this example, we will implement the ConvMixer model and demonstrate its performance on\n-the CIFAR-10 dataset.\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-\n-import matplotlib.pyplot as plt\n-import tensorflow as tf\n-import numpy as np\n-\n-\"\"\"\n-## Hyperparameters\n-\n-To keep run time short, we will train the model for only 10 epochs. To focus on\n-the core ideas of ConvMixer, we will not use other training-specific elements like\n-RandAugment ([Cubuk et al.](https://arxiv.org/abs/1909.13719)). If you are interested in\n-learning more about those details, please refer to the\n-[original paper](https://openreview.net/pdf?id=TVHS5Y4dNvM).\n-\"\"\"\n-\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 128\n-num_epochs = 10\n-\n-\"\"\"\n-## Load the CIFAR-10 dataset\n-\"\"\"\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-val_split = 0.1\n-\n-val_indices = int(len(x_train) * val_split)\n-new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\n-x_val, y_val = x_train[:val_indices], y_train[:val_indices]\n-\n-print(f\"Training data samples: {len(new_x_train)}\")\n-print(f\"Validation data samples: {len(x_val)}\")\n-print(f\"Test data samples: {len(x_test)}\")\n-\n-\"\"\"\n-## Prepare `tf.data.Dataset` objects\n-\n-Our data augmentation pipeline is different from what the authors used for the CIFAR-10\n-dataset, which is fine for the purpose of the example.\n-Note that, it's ok to use **TF APIs for data I/O and preprocessing** with other backends\n-(jax, torch) as it is feature-complete framework when it comes to data preprocessing.\n-\"\"\"\n-\n-image_size = 32\n-auto = tf.data.AUTOTUNE\n-\n-augmentation_layers = [\n-    keras.layers.RandomCrop(image_size, image_size),\n-    keras.layers.RandomFlip(\"horizontal\"),\n-]\n-\n-\n-def augment_images(images):\n-    for layer in augmentation_layers:\n-        images = layer(images, training=True)\n-    return images\n-\n-\n-def make_datasets(images, labels, is_train=False):\n-    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n-    if is_train:\n-        dataset = dataset.shuffle(batch_size * 10)\n-    dataset = dataset.batch(batch_size)\n-    if is_train:\n-        dataset = dataset.map(\n-            lambda x, y: (augment_images(x), y), num_parallel_calls=auto\n-        )\n-    return dataset.prefetch(auto)\n-\n-\n-train_dataset = make_datasets(new_x_train, new_y_train, is_train=True)\n-val_dataset = make_datasets(x_val, y_val)\n-test_dataset = make_datasets(x_test, y_test)\n-\n-\"\"\"\n-## ConvMixer utilities\n-\n-The following figure (taken from the original paper) depicts the ConvMixer model:\n-\n-![](https://i.imgur.com/yF8actg.png)\n-\n-ConvMixer is very similar to the MLP-Mixer, model with the following key\n-differences:\n-\n-* Instead of using fully-connected layers, it uses standard convolution layers.\n-* Instead of LayerNorm (which is typical for ViTs and MLP-Mixers), it uses BatchNorm.\n-\n-Two types of convolution layers are used in ConvMixer. **(1)**: Depthwise convolutions,\n-for mixing spatial locations of the images, **(2)**: Pointwise convolutions (which follow\n-the depthwise convolutions), for mixing channel-wise information across the patches.\n-Another keypoint is the use of *larger kernel sizes* to allow a larger receptive field.\n-\"\"\"\n-\n-\n-def activation_block(x):\n-    x = layers.Activation(\"gelu\")(x)\n-    return layers.BatchNormalization()(x)\n-\n-\n-def conv_stem(x, filters: int, patch_size: int):\n-    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n-    return activation_block(x)\n-\n-\n-def conv_mixer_block(x, filters: int, kernel_size: int):\n-    # Depthwise convolution.\n-    x0 = x\n-    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n-    x = layers.Add()([activation_block(x), x0])  # Residual.\n-\n-    # Pointwise convolution.\n-    x = layers.Conv2D(filters, kernel_size=1)(x)\n-    x = activation_block(x)\n-\n-    return x\n-\n-\n-def get_conv_mixer_256_8(\n-    image_size=32,\n-    filters=256,\n-    depth=8,\n-    kernel_size=5,\n-    patch_size=2,\n-    num_classes=10,\n-):\n-    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n-    The hyperparameter values are taken from the paper.\n-    \"\"\"\n-    inputs = keras.Input((image_size, image_size, 3))\n-    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n-\n-    # Extract patch embeddings.\n-    x = conv_stem(x, filters, patch_size)\n-\n-    # ConvMixer blocks.\n-    for _ in range(depth):\n-        x = conv_mixer_block(x, filters, kernel_size)\n-\n-    # Classification block.\n-    x = layers.GlobalAvgPool2D()(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n-\n-    return keras.Model(inputs, outputs)\n-\n-\n-\"\"\"\n-The model used in this experiment is termed as **ConvMixer-256/8** where 256 denotes the\n-number of channels and 8 denotes the depth. The resulting model only has 0.8 million\n-parameters.\n-\"\"\"\n-\n-\"\"\"\n-## Model training and evaluation utility\n-\"\"\"\n-\n-# Code reference:\n-# https://keras.io/examples/vision/image_classification_with_vision_transformer/.\n-\n-\n-def run_experiment(model):\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    )\n-\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=\"sparse_categorical_crossentropy\",\n-        metrics=[\"accuracy\"],\n-    )\n-\n-    checkpoint_filepath = \"/tmp/checkpoint.keras\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=False,\n-    )\n-\n-    history = model.fit(\n-        train_dataset,\n-        validation_data=val_dataset,\n-        epochs=num_epochs,\n-        callbacks=[checkpoint_callback],\n-    )\n-\n-    model.load_weights(checkpoint_filepath)\n-    _, accuracy = model.evaluate(test_dataset)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-    return history, model\n-\n-\n-\"\"\"\n-## Train and evaluate model\n-\"\"\"\n-\n-conv_mixer_model = get_conv_mixer_256_8()\n-history, conv_mixer_model = run_experiment(conv_mixer_model)\n-\n-\"\"\"\n-The gap in training and validation performance can be mitigated by using additional\n-regularization techniques. Nevertheless, being able to get to ~83% accuracy within 10\n-epochs with 0.8 million parameters is a strong result.\n-\"\"\"\n-\n-\"\"\"\n-## Visualizing the internals of ConvMixer\n-\n-We can visualize the patch embeddings and the learned convolution filters. Recall\n-that each patch embedding and intermediate feature map have the same number of channels\n-(256 in this case). This will make our visualization utility easier to implement.\n-\"\"\"\n-\n-# Code reference: https://bit.ly/3awIRbP.\n-\n-\n-def visualization_plot(weights, idx=1):\n-    # First, apply min-max normalization to the\n-    # given weights to avoid isotrophic scaling.\n-    p_min, p_max = weights.min(), weights.max()\n-    weights = (weights - p_min) / (p_max - p_min)\n-\n-    # Visualize all the filters.\n-    num_filters = 256\n-    plt.figure(figsize=(8, 8))\n-\n-    for i in range(num_filters):\n-        current_weight = weights[:, :, :, i]\n-        if current_weight.shape[-1] == 1:\n-            current_weight = current_weight.squeeze()\n-        ax = plt.subplot(16, 16, idx)\n-        ax.set_xticks([])\n-        ax.set_yticks([])\n-        plt.imshow(current_weight)\n-        idx += 1\n-\n-\n-# We first visualize the learned patch embeddings.\n-patch_embeddings = conv_mixer_model.layers[2].get_weights()[0]\n-visualization_plot(patch_embeddings)\n-\n-\"\"\"\n-Even though we did not train the network to convergence, we can notice that different\n-patches show different patterns. Some share similarity with others while some are very\n-different. These visualizations are more salient with larger image sizes.\n-\n-Similarly, we can visualize the raw convolution kernels. This can help us understand\n-the patterns to which a given kernel is receptive.\n-\"\"\"\n-\n-# First, print the indices of the convolution layers that are not\n-# pointwise convolutions.\n-for i, layer in enumerate(conv_mixer_model.layers):\n-    if isinstance(layer, layers.DepthwiseConv2D):\n-        if layer.get_config()[\"kernel_size\"] == (5, 5):\n-            print(i, layer)\n-\n-idx = 26  # Taking a kernel from the middle of the network.\n-\n-kernel = conv_mixer_model.layers[idx].get_weights()[0]\n-kernel = np.expand_dims(kernel.squeeze(), axis=2)\n-visualization_plot(kernel)\n-\n-\"\"\"\n-We see that different filters in the kernel have different locality spans, and this\n-pattern\n-is likely to evolve with more training.\n-\"\"\"\n-\n-\"\"\"\n-## Final notes\n-\n-There's been a recent trend on fusing convolutions with other data-agnostic operations\n-like self-attention. Following works are along this line of research:\n-\n-* ConViT ([d'Ascoli et al.](https://arxiv.org/abs/2103.10697))\n-* CCT ([Hassani et al.](https://arxiv.org/abs/2104.05704))\n-* CoAtNet ([Dai et al.](https://arxiv.org/abs/2106.04803))\n-\"\"\"\n\n@@ -1,341 +0,0 @@\n-\"\"\"\n-Title: Multiclass semantic segmentation using DeepLabV3+\n-Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/08/31\n-Last modified: 2023/07/19\n-Description: Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation.\n-Accelerator: GPU\n-Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Semantic segmentation, with the goal to assign semantic labels to every pixel in an image,\n-is an essential computer vision task. In this example, we implement\n-the **DeepLabV3+** model for multi-class semantic segmentation, a fully-convolutional\n-architecture that performs well on semantic segmentation benchmarks.\n-\n-### References:\n-\n-- [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611)\n-- [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)\n-- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)\n-\"\"\"\n-\n-\"\"\"\n-## Downloading the data\n-\n-We will use the [Crowd Instance-level Human Parsing Dataset](https://arxiv.org/abs/1811.12596)\n-for training our model. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images.\n-Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification.\n-This dataset can be used for the \"human part segmentation\" task.\n-\"\"\"\n-\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-import os\n-import numpy as np\n-from glob import glob\n-import cv2\n-from scipy.io import loadmat\n-import matplotlib.pyplot as plt\n-\n-# For data preprocessing\n-from tensorflow import image as tf_image\n-from tensorflow import data as tf_data\n-from tensorflow import io as tf_io\n-\n-\"\"\"shell\n-gdown \"1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&confirm=t\"\n-unzip -q instance-level-human-parsing.zip\n-\"\"\"\n-\n-\"\"\"\n-## Creating a TensorFlow Dataset\n-\n-Training on the entire CIHP dataset with 38,280 images takes a lot of time, hence we will be using\n-a smaller subset of 200 images for training our model in this example.\n-\"\"\"\n-\n-IMAGE_SIZE = 512\n-BATCH_SIZE = 4\n-NUM_CLASSES = 20\n-DATA_DIR = (\n-    \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n-)\n-NUM_TRAIN_IMAGES = 1000\n-NUM_VAL_IMAGES = 50\n-\n-train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n-    :NUM_TRAIN_IMAGES\n-]\n-train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n-    :NUM_TRAIN_IMAGES\n-]\n-val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n-    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n-]\n-val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n-    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n-]\n-\n-\n-def read_image(image_path, mask=False):\n-    image = tf_io.read_file(image_path)\n-    if mask:\n-        image = tf_image.decode_png(image, channels=1)\n-        image.set_shape([None, None, 1])\n-        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n-    else:\n-        image = tf_image.decode_png(image, channels=3)\n-        image.set_shape([None, None, 3])\n-        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n-    return image\n-\n-\n-def load_data(image_list, mask_list):\n-    image = read_image(image_list)\n-    mask = read_image(mask_list, mask=True)\n-    return image, mask\n-\n-\n-def data_generator(image_list, mask_list):\n-    dataset = tf_data.Dataset.from_tensor_slices((image_list, mask_list))\n-    dataset = dataset.map(load_data, num_parallel_calls=tf_data.AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n-    return dataset\n-\n-\n-train_dataset = data_generator(train_images, train_masks)\n-val_dataset = data_generator(val_images, val_masks)\n-\n-print(\"Train Dataset:\", train_dataset)\n-print(\"Val Dataset:\", val_dataset)\n-\n-\"\"\"\n-## Building the DeepLabV3+ model\n-\n-DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. The encoder module\n-processes multiscale contextual information by applying dilated convolution at multiple\n-scales, while the decoder module refines the segmentation results along object boundaries.\n-\n-![](https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png)\n-\n-**Dilated convolution:** With dilated convolution, as we go deeper in the network, we can keep the\n-stride constant but with larger field-of-view without increasing the number of parameters\n-or the amount of computation. Besides, it enables larger output feature maps, which is\n-useful for semantic segmentation.\n-\n-The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the\n-sampling rate becomes larger, the number of valid filter weights (i.e., weights that\n-are applied to the valid feature region, instead of padded zeros) becomes smaller.\n-\"\"\"\n-\n-\n-def convolution_block(\n-    block_input,\n-    num_filters=256,\n-    kernel_size=3,\n-    dilation_rate=1,\n-    padding=\"same\",\n-    use_bias=False,\n-):\n-    x = layers.Conv2D(\n-        num_filters,\n-        kernel_size=kernel_size,\n-        dilation_rate=dilation_rate,\n-        padding=\"same\",\n-        use_bias=use_bias,\n-        kernel_initializer=keras.initializers.HeNormal(),\n-    )(block_input)\n-    x = layers.BatchNormalization()(x)\n-    return ops.nn.relu(x)\n-\n-\n-def DilatedSpatialPyramidPooling(dspp_input):\n-    dims = dspp_input.shape\n-    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n-    x = convolution_block(x, kernel_size=1, use_bias=True)\n-    out_pool = layers.UpSampling2D(\n-        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]),\n-        interpolation=\"bilinear\",\n-    )(x)\n-\n-    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n-    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n-    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n-    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n-\n-    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n-    output = convolution_block(x, kernel_size=1)\n-    return output\n-\n-\n-\"\"\"\n-The encoder features are first bilinearly upsampled by a factor 4, and then\n-concatenated with the corresponding low-level features from the network backbone that\n-have the same spatial resolution. For this example, we\n-use a ResNet50 pretrained on ImageNet as the backbone model, and we use\n-the low-level features from the `conv4_block6_2_relu` block of the backbone.\n-\"\"\"\n-\n-\n-def DeeplabV3Plus(image_size, num_classes):\n-    model_input = keras.Input(shape=(image_size, image_size, 3))\n-    preprocessed = keras.applications.resnet50.preprocess_input(model_input)\n-    resnet50 = keras.applications.ResNet50(\n-        weights=\"imagenet\", include_top=False, input_tensor=preprocessed\n-    )\n-    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n-    x = DilatedSpatialPyramidPooling(x)\n-\n-    input_a = layers.UpSampling2D(\n-        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n-        interpolation=\"bilinear\",\n-    )(x)\n-    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n-    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n-\n-    x = layers.Concatenate(axis=-1)([input_a, input_b])\n-    x = convolution_block(x)\n-    x = convolution_block(x)\n-    x = layers.UpSampling2D(\n-        size=(image_size // x.shape[1], image_size // x.shape[2]),\n-        interpolation=\"bilinear\",\n-    )(x)\n-    model_output = layers.Conv2D(\n-        num_classes, kernel_size=(1, 1), padding=\"same\"\n-    )(x)\n-    return keras.Model(inputs=model_input, outputs=model_output)\n-\n-\n-model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n-model.summary()\n-\n-\"\"\"\n-## Training\n-\n-We train the model using sparse categorical crossentropy as the loss function, and\n-Adam as the optimizer.\n-\"\"\"\n-\n-loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-model.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n-    loss=loss,\n-    metrics=[\"accuracy\"],\n-)\n-\n-history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n-\n-plt.plot(history.history[\"loss\"])\n-plt.title(\"Training Loss\")\n-plt.ylabel(\"loss\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-plt.plot(history.history[\"accuracy\"])\n-plt.title(\"Training Accuracy\")\n-plt.ylabel(\"accuracy\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-plt.plot(history.history[\"val_loss\"])\n-plt.title(\"Validation Loss\")\n-plt.ylabel(\"val_loss\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-plt.plot(history.history[\"val_accuracy\"])\n-plt.title(\"Validation Accuracy\")\n-plt.ylabel(\"val_accuracy\")\n-plt.xlabel(\"epoch\")\n-plt.show()\n-\n-\"\"\"\n-## Inference using Colormap Overlay\n-\n-The raw predictions from the model represent a one-hot encoded tensor of shape `(N, 512, 512, 20)`\n-where each one of the 20 channels is a binary mask corresponding to a predicted label.\n-In order to visualize the results, we plot them as RGB segmentation masks where each pixel\n-is represented by a unique color corresponding to the particular label predicted. We can easily\n-find the color corresponding to each label from the `human_colormap.mat` file provided as part\n-of the dataset. We would also plot an overlay of the RGB segmentation mask on the input image as\n-this further helps us to identify the different categories present in the image more intuitively.\n-\"\"\"\n-\n-# Loading the Colormap\n-colormap = loadmat(\n-    \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\"\n-)[\"colormap\"]\n-colormap = colormap * 100\n-colormap = colormap.astype(np.uint8)\n-\n-\n-def infer(model, image_tensor):\n-    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n-    predictions = np.squeeze(predictions)\n-    predictions = np.argmax(predictions, axis=2)\n-    return predictions\n-\n-\n-def decode_segmentation_masks(mask, colormap, n_classes):\n-    r = np.zeros_like(mask).astype(np.uint8)\n-    g = np.zeros_like(mask).astype(np.uint8)\n-    b = np.zeros_like(mask).astype(np.uint8)\n-    for l in range(0, n_classes):\n-        idx = mask == l\n-        r[idx] = colormap[l, 0]\n-        g[idx] = colormap[l, 1]\n-        b[idx] = colormap[l, 2]\n-    rgb = np.stack([r, g, b], axis=2)\n-    return rgb\n-\n-\n-def get_overlay(image, colored_mask):\n-    image = keras.utils.array_to_img(image)\n-    image = np.array(image).astype(np.uint8)\n-    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n-    return overlay\n-\n-\n-def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n-    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n-    for i in range(len(display_list)):\n-        if display_list[i].shape[-1] == 3:\n-            axes[i].imshow(keras.utils.array_to_img(display_list[i]))\n-        else:\n-            axes[i].imshow(display_list[i])\n-    plt.show()\n-\n-\n-def plot_predictions(images_list, colormap, model):\n-    for image_file in images_list:\n-        image_tensor = read_image(image_file)\n-        prediction_mask = infer(image_tensor=image_tensor, model=model)\n-        prediction_colormap = decode_segmentation_masks(\n-            prediction_mask, colormap, 20\n-        )\n-        overlay = get_overlay(image_tensor, prediction_colormap)\n-        plot_samples_matplotlib(\n-            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n-        )\n-\n-\n-\"\"\"\n-### Inference on Train Images\n-\"\"\"\n-\n-plot_predictions(train_images[:4], colormap, model=model)\n-\n-\"\"\"\n-### Inference on Validation Images\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deeplabv3p-resnet50)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Human-Part-Segmentation).\n-\"\"\"\n-\n-plot_predictions(val_images[:4], colormap, model=model)\n\n@@ -1,326 +0,0 @@\n-\"\"\"\n-Title: Image classification with EANet (External Attention Transformer)\n-Author: [ZhiYong Chang](https://github.com/czy00000)\n-Date created: 2021/10/19\n-Last modified: 2023/07/18\n-Description: Image classification with a Transformer that leverages external attention.\n-Accelerator: GPU\n-Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example implements the [EANet](https://arxiv.org/abs/2105.02358)\n-model for image classification, and demonstrates it on the CIFAR-100 dataset.\n-EANet introduces a novel attention mechanism\n-named ***external attention***, based on two external, small, learnable, and\n-shared memories, which can be implemented easily by simply using two cascaded\n-linear layers and two normalization layers. It conveniently replaces self-attention\n-as used in existing architectures. External attention has linear complexity, as it only\n-implicitly considers the correlations between all samples.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-import matplotlib.pyplot as plt\n-\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\"\"\"\n-\n-weight_decay = 0.0001\n-learning_rate = 0.001\n-label_smoothing = 0.1\n-validation_split = 0.2\n-batch_size = 128\n-num_epochs = 50\n-patch_size = 2  # Size of the patches to be extracted from the input images.\n-num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n-embedding_dim = 64  # Number of hidden units.\n-mlp_dim = 64\n-dim_coefficient = 4\n-num_heads = 4\n-attention_dropout = 0.2\n-projection_dropout = 0.2\n-num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n-\n-print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n-print(f\"Patches per image: {num_patches}\")\n-\n-\n-\"\"\"\n-## Use data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(factor=0.1),\n-        layers.RandomContrast(factor=0.1),\n-        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-# Compute the mean and the variance of the training data for normalization.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\"\"\"\n-## Implement the patch extraction and encoding layer\n-\"\"\"\n-\n-\n-class PatchExtract(layers.Layer):\n-    def __init__(self, patch_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.patch_size = patch_size\n-\n-    def call(self, x):\n-        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n-        x = ops.image.extract_patches(x, self.patch_size)\n-        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n-        return x\n-\n-\n-class PatchEmbedding(layers.Layer):\n-    def __init__(self, num_patch, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_patch = num_patch\n-        self.proj = layers.Dense(embed_dim)\n-        self.pos_embed = layers.Embedding(\n-            input_dim=num_patch, output_dim=embed_dim\n-        )\n-\n-    def call(self, patch):\n-        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n-        return self.proj(patch) + self.pos_embed(pos)\n-\n-\n-\"\"\"\n-## Implement the external attention block\n-\"\"\"\n-\n-\n-def external_attention(\n-    x,\n-    dim,\n-    num_heads,\n-    dim_coefficient=4,\n-    attention_dropout=0,\n-    projection_dropout=0,\n-):\n-    _, num_patch, channel = x.shape\n-    assert dim % num_heads == 0\n-    num_heads = num_heads * dim_coefficient\n-\n-    x = layers.Dense(dim * dim_coefficient)(x)\n-    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n-    x = ops.reshape(\n-        x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n-    )\n-    x = ops.transpose(x, axes=[0, 2, 1, 3])\n-    # a linear layer M_k\n-    attn = layers.Dense(dim // dim_coefficient)(x)\n-    # normalize attention map\n-    attn = layers.Softmax(axis=2)(attn)\n-    # dobule-normalization\n-    attn = layers.Lambda(\n-        lambda attn: ops.divide(\n-            attn,\n-            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n-        )\n-    )(attn)\n-    attn = layers.Dropout(attention_dropout)(attn)\n-    # a linear layer M_v\n-    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n-    x = ops.transpose(x, axes=[0, 2, 1, 3])\n-    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n-    # a linear layer to project original dim\n-    x = layers.Dense(dim)(x)\n-    x = layers.Dropout(projection_dropout)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the MLP block\n-\"\"\"\n-\n-\n-def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n-    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n-    x = layers.Dropout(drop_rate)(x)\n-    x = layers.Dense(embedding_dim)(x)\n-    x = layers.Dropout(drop_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the Transformer block\n-\"\"\"\n-\n-\n-def transformer_encoder(\n-    x,\n-    embedding_dim,\n-    mlp_dim,\n-    num_heads,\n-    dim_coefficient,\n-    attention_dropout,\n-    projection_dropout,\n-    attention_type=\"external_attention\",\n-):\n-    residual_1 = x\n-    x = layers.LayerNormalization(epsilon=1e-5)(x)\n-    if attention_type == \"external_attention\":\n-        x = external_attention(\n-            x,\n-            embedding_dim,\n-            num_heads,\n-            dim_coefficient,\n-            attention_dropout,\n-            projection_dropout,\n-        )\n-    elif attention_type == \"self_attention\":\n-        x = layers.MultiHeadAttention(\n-            num_heads=num_heads,\n-            key_dim=embedding_dim,\n-            dropout=attention_dropout,\n-        )(x, x)\n-    x = layers.add([x, residual_1])\n-    residual_2 = x\n-    x = layers.LayerNormalization(epsilon=1e-5)(x)\n-    x = mlp(x, embedding_dim, mlp_dim)\n-    x = layers.add([x, residual_2])\n-    return x\n-\n-\n-\"\"\"\n-## Implement the EANet model\n-\"\"\"\n-\n-\"\"\"\n-The EANet model leverages external attention.\n-The computational complexity of traditional self attention is `O(d * N ** 2)`,\n-where `d` is the embedding size, and `N` is the number of patch.\n-the authors find that most pixels are closely related to just a few other\n-pixels, and an `N`-to-`N` attention matrix may be redundant.\n-So, they propose as an alternative an external\n-attention module where the computational complexity of external attention is `O(d * S * N)`.\n-As `d` and `S` are hyper-parameters,\n-the proposed algorithm is linear in the number of pixels. In fact, this is equivalent\n-to a drop patch operation, because a lot of information contained in a patch\n-in an image is redundant and unimportant.\n-\"\"\"\n-\n-\n-def get_model(attention_type=\"external_attention\"):\n-    inputs = layers.Input(shape=input_shape)\n-    # Image augment\n-    x = data_augmentation(inputs)\n-    # Extract patches.\n-    x = PatchExtract(patch_size)(x)\n-    # Create patch embedding.\n-    x = PatchEmbedding(num_patches, embedding_dim)(x)\n-    # Create Transformer block.\n-    for _ in range(num_transformer_blocks):\n-        x = transformer_encoder(\n-            x,\n-            embedding_dim,\n-            mlp_dim,\n-            num_heads,\n-            dim_coefficient,\n-            attention_dropout,\n-            projection_dropout,\n-            attention_type,\n-        )\n-\n-    x = layers.GlobalAveragePooling1D()(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-\"\"\"\n-## Train on CIFAR-100\n-\n-\"\"\"\n-\n-\n-model = get_model(attention_type=\"external_attention\")\n-\n-model.compile(\n-    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n-    optimizer=keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    ),\n-    metrics=[\n-        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-    ],\n-)\n-\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=num_epochs,\n-    validation_split=validation_split,\n-)\n-\n-\"\"\"\n-### Let's visualize the training progress of the model.\n-\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"train_loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n-plt.xlabel(\"Epochs\")\n-plt.ylabel(\"Loss\")\n-plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n-plt.legend()\n-plt.grid()\n-plt.show()\n-\n-\"\"\"\n-### Let's display the final results of the test on CIFAR-100.\n-\n-\"\"\"\n-\n-loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-print(f\"Test loss: {round(loss, 2)}\")\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-\"\"\"\n-EANet just replaces self attention in Vit with external attention.\n-The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1 accuracy after\n-training 50 epochs, but with 0.6M parameters. Under the same experimental environment\n-and the same hyperparameters, The EANet model we just trained has just 0.3M parameters,\n-and it gets us to ~73% test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the\n-effectiveness of external attention.\n-\n-We only show the training\n-process of EANet, you can train Vit under the same experimental conditions and observe\n-the test results.\n-\"\"\"\n\n@@ -1,346 +0,0 @@\n-\"\"\"\n-Title: FixRes: Fixing train-test resolution discrepancy\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/10/08\n-Last modified: 2021/10/10\n-Description: Mitigating resolution discrepancy between training and test sets.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-It is a common practice to use the same input image resolution while training and testing\n-vision models. However, as investigated in\n-[Fixing the train-test resolution discrepancy](https://arxiv.org/abs/1906.06423)\n-(Touvron et al.), this practice leads to suboptimal performance. Data augmentation\n-is an indispensable part of the training process of deep neural networks. For vision models, we\n-typically use random resized crops during training and center crops during inference.\n-This introduces a discrepancy in the object sizes seen during training and inference.\n-As shown by Touvron et al., if we can fix this discrepancy, we can significantly\n-boost model performance.\n-\n-In this example, we implement the **FixRes** techniques introduced by Touvron et al.\n-to fix this discrepancy.\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-import tensorflow as tf  # just for image processing and pipeline\n-\n-import tensorflow_datasets as tfds\n-\n-tfds.disable_progress_bar()\n-\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Load the `tf_flowers` dataset\n-\"\"\"\n-\n-train_dataset, val_dataset = tfds.load(\n-    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n-)\n-\n-num_train = train_dataset.cardinality()\n-num_val = val_dataset.cardinality()\n-print(f\"Number of training examples: {num_train}\")\n-print(f\"Number of validation examples: {num_val}\")\n-\n-\"\"\"\n-## Data preprocessing utilities\n-\"\"\"\n-\n-\"\"\"\n-We create three datasets:\n-\n-1. A dataset with a smaller resolution - 128x128.\n-2. Two datasets with a larger resolution - 224x224.\n-\n-We will apply different augmentation transforms to the larger-resolution datasets.\n-\n-The idea of FixRes is to first train a model on a smaller resolution dataset and then fine-tune\n-it on a larger resolution dataset. This simple yet effective recipe leads to non-trivial performance\n-improvements. Please refer to the [original paper](https://arxiv.org/abs/1906.06423) for\n-results.\n-\"\"\"\n-\n-# Reference: https://github.com/facebookresearch/FixRes/blob/main/transforms_v2.py.\n-\n-batch_size = 32\n-auto = tf.data.AUTOTUNE\n-smaller_size = 128\n-bigger_size = 224\n-\n-size_for_resizing = int((bigger_size / smaller_size) * bigger_size)\n-central_crop_layer = layers.CenterCrop(bigger_size, bigger_size)\n-\n-\n-def preprocess_initial(train, image_size):\n-    \"\"\"Initial preprocessing function for training on smaller resolution.\n-\n-    For training, do random_horizontal_flip -> random_crop.\n-    For validation, just resize.\n-    No color-jittering has been used.\n-    \"\"\"\n-\n-    def _pp(image, label, train):\n-        if train:\n-            channels = image.shape[-1]\n-            begin, size, _ = tf.image.sample_distorted_bounding_box(\n-                tf.shape(image),\n-                tf.zeros([0, 0, 4], tf.float32),\n-                area_range=(0.05, 1.0),\n-                min_object_covered=0,\n-                use_image_if_no_bounding_boxes=True,\n-            )\n-            image = tf.slice(image, begin, size)\n-\n-            image.set_shape([None, None, channels])\n-            image = tf.image.resize(image, [image_size, image_size])\n-            image = tf.image.random_flip_left_right(image)\n-        else:\n-            image = tf.image.resize(image, [image_size, image_size])\n-\n-        return image, label\n-\n-    return _pp\n-\n-\n-def preprocess_finetune(image, label, train):\n-    \"\"\"Preprocessing function for fine-tuning on a higher resolution.\n-\n-    For training, resize to a bigger resolution to maintain the ratio ->\n-        random_horizontal_flip -> center_crop.\n-    For validation, do the same without any horizontal flipping.\n-    No color-jittering has been used.\n-    \"\"\"\n-    image = tf.image.resize(image, [size_for_resizing, size_for_resizing])\n-    if train:\n-        image = tf.image.random_flip_left_right(image)\n-    image = central_crop_layer(image[None, ...])[0]\n-\n-    return image, label\n-\n-\n-def make_dataset(\n-    dataset: tf.data.Dataset,\n-    train: bool,\n-    image_size: int = smaller_size,\n-    fixres: bool = True,\n-    num_parallel_calls=auto,\n-):\n-    if image_size not in [smaller_size, bigger_size]:\n-        raise ValueError(f\"{image_size} resolution is not supported.\")\n-\n-    # Determine which preprocessing function we are using.\n-    if image_size == smaller_size:\n-        preprocess_func = preprocess_initial(train, image_size)\n-    elif not fixres and image_size == bigger_size:\n-        preprocess_func = preprocess_initial(train, image_size)\n-    else:\n-        preprocess_func = preprocess_finetune\n-\n-    if train:\n-        dataset = dataset.shuffle(batch_size * 10)\n-\n-    return (\n-        dataset.map(\n-            lambda x, y: preprocess_func(x, y, train),\n-            num_parallel_calls=num_parallel_calls,\n-        )\n-        .batch(batch_size)\n-        .prefetch(num_parallel_calls)\n-    )\n-\n-\n-\"\"\"\n-Notice how the augmentation transforms vary for the kind of dataset we are preparing.\n-\"\"\"\n-\n-\"\"\"\n-## Prepare datasets\n-\"\"\"\n-\n-initial_train_dataset = make_dataset(\n-    train_dataset, train=True, image_size=smaller_size\n-)\n-initial_val_dataset = make_dataset(\n-    val_dataset, train=False, image_size=smaller_size\n-)\n-\n-finetune_train_dataset = make_dataset(\n-    train_dataset, train=True, image_size=bigger_size\n-)\n-finetune_val_dataset = make_dataset(\n-    val_dataset, train=False, image_size=bigger_size\n-)\n-\n-vanilla_train_dataset = make_dataset(\n-    train_dataset, train=True, image_size=bigger_size, fixres=False\n-)\n-vanilla_val_dataset = make_dataset(\n-    val_dataset, train=False, image_size=bigger_size, fixres=False\n-)\n-\n-\"\"\"\n-## Visualize the datasets\n-\"\"\"\n-\n-\n-def visualize_dataset(batch_images):\n-    plt.figure(figsize=(10, 10))\n-    for n in range(25):\n-        ax = plt.subplot(5, 5, n + 1)\n-        plt.imshow(batch_images[n].numpy().astype(\"int\"))\n-        plt.axis(\"off\")\n-    plt.show()\n-\n-    print(f\"Batch shape: {batch_images.shape}.\")\n-\n-\n-# Smaller resolution.\n-initial_sample_images, _ = next(iter(initial_train_dataset))\n-visualize_dataset(initial_sample_images)\n-\n-# Bigger resolution, only for fine-tuning.\n-finetune_sample_images, _ = next(iter(finetune_train_dataset))\n-visualize_dataset(finetune_sample_images)\n-\n-# Bigger resolution, with the same augmentation transforms as\n-# the smaller resolution dataset.\n-vanilla_sample_images, _ = next(iter(vanilla_train_dataset))\n-visualize_dataset(vanilla_sample_images)\n-\n-\"\"\"\n-## Model training utilities\n-\n-We train multiple variants of ResNet50V2\n-([He et al.](https://arxiv.org/abs/1603.05027)):\n-\n-1. On the smaller resolution dataset (128x128). It will be trained from scratch.\n-2. Then fine-tune the model from 1 on the larger resolution (224x224) dataset.\n-3. Train another ResNet50V2 from scratch on the larger resolution dataset.\n-\n-As a reminder, the larger resolution datasets differ in terms of their augmentation\n-transforms.\n-\"\"\"\n-\n-\n-def get_training_model(num_classes=5):\n-    inputs = layers.Input((None, None, 3))\n-    resnet_base = keras.applications.ResNet50V2(\n-        include_top=False, weights=None, pooling=\"avg\"\n-    )\n-    resnet_base.trainable = True\n-\n-    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)\n-    x = resnet_base(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n-    return keras.Model(inputs, outputs)\n-\n-\n-def train_and_evaluate(\n-    model,\n-    train_ds,\n-    val_ds,\n-    epochs,\n-    learning_rate=1e-3,\n-    use_early_stopping=False,\n-):\n-    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=\"sparse_categorical_crossentropy\",\n-        metrics=[\"accuracy\"],\n-    )\n-\n-    if use_early_stopping:\n-        es_callback = keras.callbacks.EarlyStopping(patience=5)\n-        callbacks = [es_callback]\n-    else:\n-        callbacks = None\n-\n-    model.fit(\n-        train_ds,\n-        validation_data=val_ds,\n-        epochs=epochs,\n-        callbacks=callbacks,\n-    )\n-\n-    _, accuracy = model.evaluate(val_ds)\n-    print(f\"Top-1 accuracy on the validation set: {accuracy*100:.2f}%.\")\n-    return model\n-\n-\n-\"\"\"\n-## Experiment 1: Train on 128x128 and then fine-tune on 224x224\n-\"\"\"\n-\n-epochs = 30\n-\n-smaller_res_model = get_training_model()\n-smaller_res_model = train_and_evaluate(\n-    smaller_res_model, initial_train_dataset, initial_val_dataset, epochs\n-)\n-\n-\"\"\"\n-### Freeze all the layers except for the final Batch Normalization layer\n-\n-For fine-tuning, we train only two layers:\n-\n-* The final Batch Normalization ([Ioffe et al.](https://arxiv.org/abs/1502.03167)) layer.\n-* The classification layer.\n-\n-We are unfreezing the final Batch Normalization layer to compensate for the change in\n-activation statistics before the global average pooling layer. As shown in\n-[the paper](https://arxiv.org/abs/1906.06423), unfreezing the final Batch\n-Normalization layer is enough.\n-\n-For a comprehensive guide on fine-tuning models in Keras, refer to\n-[this tutorial](https://keras.io/guides/transfer_learning/).\n-\"\"\"\n-\n-for layer in smaller_res_model.layers[2].layers:\n-    layer.trainable = False\n-\n-smaller_res_model.layers[2].get_layer(\"post_bn\").trainable = True\n-\n-epochs = 10\n-\n-# Use a lower learning rate during fine-tuning.\n-bigger_res_model = train_and_evaluate(\n-    smaller_res_model,\n-    finetune_train_dataset,\n-    finetune_val_dataset,\n-    epochs,\n-    learning_rate=1e-4,\n-)\n-\n-\"\"\"\n-## Experiment 2: Train a model on 224x224 resolution from scratch\n-\n-Now, we train another model from scratch on the larger resolution dataset. Recall that\n-the augmentation transforms used in this dataset are different from before.\n-\"\"\"\n-\n-epochs = 30\n-\n-vanilla_bigger_res_model = get_training_model()\n-vanilla_bigger_res_model = train_and_evaluate(\n-    vanilla_bigger_res_model, vanilla_train_dataset, vanilla_val_dataset, epochs\n-)\n-\n-\"\"\"\n-As we can notice from the above cells, FixRes leads to a better performance. Another\n-advantage of FixRes is the improved total training time and reduction in GPU memory usage.\n-FixRes is model-agnostic, you can use it on any image classification model\n-to potentially boost performance.\n-\n-You can find more results\n-[here](https://tensorboard.dev/experiment/BQOg28w0TlmvuJYeqsVntw)\n-that were gathered by running the same code with different random seeds.\n-\"\"\"\n\n@@ -1,276 +0,0 @@\n-\"\"\"\n-Title: Gradient Centralization for Better Training Performance\n-Author: [Rishit Dagli](https://github.com/Rishit-dagli)\n-Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 06/18/21\n-Last modified: 07/25/23\n-Description: Implement Gradient Centralization to improve training performance of DNNs.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example implements [Gradient Centralization](https://arxiv.org/abs/2004.01461), a\n-new optimization technique for Deep Neural Networks by Yong et al., and demonstrates it\n-on Laurence Moroney's [Horses or Humans\n-Dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans). Gradient\n-Centralization can both speedup training process and improve the final generalization\n-performance of DNNs. It operates directly on gradients by centralizing the gradient\n-vectors to have zero mean. Gradient Centralization morever improves the Lipschitzness of\n-the loss function and its gradient so that the training process becomes more efficient\n-and stable.\n-\n-This example requires `tensorflow_datasets` which can\n-be installed with this command:\n-\n-```\n-pip install tensorflow-datasets\n-```\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-from time import time\n-\n-import keras\n-from keras import layers\n-from keras.optimizers import RMSprop\n-from keras import ops\n-\n-from tensorflow import data as tf_data\n-import tensorflow_datasets as tfds\n-\n-\n-\"\"\"\n-## Prepare the data\n-\n-For this example, we will be using the [Horses or Humans\n-dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans).\n-\"\"\"\n-\n-num_classes = 2\n-input_shape = (300, 300, 3)\n-dataset_name = \"horses_or_humans\"\n-batch_size = 128\n-AUTOTUNE = tf_data.AUTOTUNE\n-\n-(train_ds, test_ds), metadata = tfds.load(\n-    name=dataset_name,\n-    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n-    with_info=True,\n-    as_supervised=True,\n-)\n-\n-print(f\"Image shape: {metadata.features['image'].shape}\")\n-print(f\"Training images: {metadata.splits['train'].num_examples}\")\n-print(f\"Test images: {metadata.splits['test'].num_examples}\")\n-\n-\"\"\"\n-## Use Data Augmentation\n-\n-We will rescale the data to `[0, 1]` and perform simple augmentations to our data.\n-\"\"\"\n-\n-rescale = layers.Rescaling(1.0 / 255)\n-\n-data_augmentation = [\n-    layers.RandomFlip(\"horizontal_and_vertical\"),\n-    layers.RandomRotation(0.3),\n-    layers.RandomZoom(0.2),\n-]\n-\n-\n-# Helper to apply augmentation\n-def apply_aug(x):\n-    for aug in data_augmentation:\n-        x = aug(x)\n-    return x\n-\n-\n-def prepare(ds, shuffle=False, augment=False):\n-    # Rescale dataset\n-    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)\n-\n-    if shuffle:\n-        ds = ds.shuffle(1024)\n-\n-    # Batch dataset\n-    ds = ds.batch(batch_size)\n-\n-    # Use data augmentation only on the training set\n-    if augment:\n-        ds = ds.map(\n-            lambda x, y: (apply_aug(x), y),\n-            num_parallel_calls=AUTOTUNE,\n-        )\n-\n-    # Use buffered prefecting\n-    return ds.prefetch(buffer_size=AUTOTUNE)\n-\n-\n-\"\"\"\n-Rescale and augment the data\n-\"\"\"\n-\n-train_ds = prepare(train_ds, shuffle=True, augment=True)\n-test_ds = prepare(test_ds)\n-\"\"\"\n-## Define a model\n-\n-In this section we will define a Convolutional neural network.\n-\"\"\"\n-\n-model = keras.Sequential(\n-    [\n-        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),\n-        layers.MaxPooling2D(2, 2),\n-        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n-        layers.Dropout(0.5),\n-        layers.MaxPooling2D(2, 2),\n-        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n-        layers.Dropout(0.5),\n-        layers.MaxPooling2D(2, 2),\n-        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n-        layers.MaxPooling2D(2, 2),\n-        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n-        layers.MaxPooling2D(2, 2),\n-        layers.Flatten(),\n-        layers.Dropout(0.5),\n-        layers.Dense(512, activation=\"relu\"),\n-        layers.Dense(1, activation=\"sigmoid\"),\n-    ]\n-)\n-\n-\"\"\"\n-## Implement Gradient Centralization\n-\n-We will now\n-subclass the `RMSProp` optimizer class modifying the\n-`keras.optimizers.Optimizer.get_gradients()` method where we now implement Gradient\n-Centralization. On a high level the idea is that let us say we obtain our gradients\n-through back propogation for a Dense or Convolution layer we then compute the mean of the\n-column vectors of the weight matrix, and then remove the mean from each column vector.\n-\n-The experiments in [this paper](https://arxiv.org/abs/2004.01461) on various\n-applications, including general image classification, fine-grained image classification,\n-detection and segmentation and Person ReID demonstrate that GC can consistently improve\n-the performance of DNN learning.\n-\n-Also, for simplicity at the moment we are not implementing gradient cliiping functionality,\n-however this quite easy to implement.\n-\n-At the moment we are just creating a subclass for the `RMSProp` optimizer\n-however you could easily reproduce this for any other optimizer or on a custom\n-optimizer in the same way. We will be using this class in the later section when\n-we train a model with Gradient Centralization.\n-\"\"\"\n-\n-\n-class GCRMSprop(RMSprop):\n-    def get_gradients(self, loss, params):\n-        # We here just provide a modified get_gradients() function since we are\n-        # trying to just compute the centralized gradients.\n-\n-        grads = []\n-        gradients = super().get_gradients()\n-        for grad in gradients:\n-            grad_len = len(grad.shape)\n-            if grad_len > 1:\n-                axis = list(range(grad_len - 1))\n-                grad -= ops.mean(grad, axis=axis, keep_dims=True)\n-            grads.append(grad)\n-\n-        return grads\n-\n-\n-optimizer = GCRMSprop(learning_rate=1e-4)\n-\n-\"\"\"\n-## Training utilities\n-\n-We will also create a callback which allows us to easily measure the total training time\n-and the time taken for each epoch since we are interested in comparing the effect of\n-Gradient Centralization on the model we built above.\n-\"\"\"\n-\n-\n-class TimeHistory(keras.callbacks.Callback):\n-    def on_train_begin(self, logs={}):\n-        self.times = []\n-\n-    def on_epoch_begin(self, batch, logs={}):\n-        self.epoch_time_start = time()\n-\n-    def on_epoch_end(self, batch, logs={}):\n-        self.times.append(time() - self.epoch_time_start)\n-\n-\n-\"\"\"\n-## Train the model without GC\n-\n-We now train the model we built earlier without Gradient Centralization which we can\n-compare to the training performance of the model trained with Gradient Centralization.\n-\"\"\"\n-\n-time_callback_no_gc = TimeHistory()\n-model.compile(\n-    loss=\"binary_crossentropy\",\n-    optimizer=RMSprop(learning_rate=1e-4),\n-    metrics=[\"accuracy\"],\n-)\n-\n-model.summary()\n-\n-\"\"\"\n-We also save the history since we later want to compare our model trained with and not\n-trained with Gradient Centralization\n-\"\"\"\n-\n-history_no_gc = model.fit(\n-    train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc]\n-)\n-\n-\"\"\"\n-## Train the model with GC\n-\n-We will now train the same model, this time using Gradient Centralization,\n-notice our optimizer is the one using Gradient Centralization this time.\n-\"\"\"\n-\n-time_callback_gc = TimeHistory()\n-model.compile(\n-    loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n-)\n-\n-model.summary()\n-\n-history_gc = model.fit(\n-    train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc]\n-)\n-\n-\"\"\"\n-## Comparing performance\n-\"\"\"\n-\n-print(\"Not using Gradient Centralization\")\n-print(f\"Loss: {history_no_gc.history['loss'][-1]}\")\n-print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\")\n-print(f\"Training Time: {sum(time_callback_no_gc.times)}\")\n-\n-print(\"Using Gradient Centralization\")\n-print(f\"Loss: {history_gc.history['loss'][-1]}\")\n-print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\")\n-print(f\"Training Time: {sum(time_callback_gc.times)}\")\n-\n-\"\"\"\n-Readers are encouraged to try out Gradient Centralization on different datasets from\n-different domains and experiment with it's effect. You are strongly advised to check out\n-the [original paper](https://arxiv.org/abs/2004.01461) as well - the authors present\n-several studies on Gradient Centralization showing how it can improve general\n-performance, generalization, training time as well as more efficient.\n-\n-Many thanks to [Ali Mustufa Shaikh](https://github.com/ialimustufa) for reviewing this\n-implementation.\n-\"\"\"\n\n@@ -1,438 +0,0 @@\n-\"\"\"\n-Title: Image classification via fine-tuning with EfficientNet\n-Author: [Yixing Fu](https://github.com/yixingfu)\n-Date created: 2020/06/30\n-Last modified: 2023/07/10\n-Description: Use EfficientNet with weights pre-trained on imagenet for Stanford Dogs classification.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-\n-## Introduction: what is EfficientNet\n-\n-EfficientNet, first introduced in [Tan and Le, 2019](https://arxiv.org/abs/1905.11946)\n-is among the most efficient models (i.e. requiring least FLOPS for inference)\n-that reaches State-of-the-Art accuracy on both\n-imagenet and common image classification transfer learning tasks.\n-\n-The smallest base model is similar to [MnasNet](https://arxiv.org/abs/1807.11626), which\n-reached near-SOTA with a significantly smaller model. By introducing a heuristic way to\n-scale the model, EfficientNet provides a family of models (B0 to B7) that represents a\n-good combination of efficiency and accuracy on a variety of scales. Such a scaling\n-heuristics (compound-scaling, details see\n-[Tan and Le, 2019](https://arxiv.org/abs/1905.11946)) allows the\n-efficiency-oriented base model (B0) to surpass models at every scale, while avoiding\n-extensive grid-search of hyperparameters.\n-\n-A summary of the latest updates on the model is available at\n-[here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), where various\n-augmentation schemes and semi-supervised learning approaches are applied to further\n-improve the imagenet performance of the models. These extensions of the model can be used\n-by updating weights without changing model architecture.\n-\n-## B0 to B7 variants of EfficientNet\n-\n-*(This section provides some details on \"compound scaling\", and can be skipped\n-if you're only interested in using the models)*\n-\n-Based on the [original paper](https://arxiv.org/abs/1905.11946) people may have the\n-impression that EfficientNet is a continuous family of models created by arbitrarily\n-choosing scaling factor in as Eq.(3) of the paper.  However, choice of resolution,\n-depth and width are also restricted by many factors:\n-\n-- Resolution: Resolutions not divisible by 8, 16, etc. cause zero-padding near boundaries\n-of some layers which wastes computational resources. This especially applies to smaller\n-variants of the model, hence the input resolution for B0 and B1 are chosen as 224 and\n-240.\n-\n-- Depth and width: The building blocks of EfficientNet demands channel size to be\n-multiples of 8.\n-\n-- Resource limit: Memory limitation may bottleneck resolution when depth\n-and width can still increase. In such a situation, increasing depth and/or\n-width but keep resolution can still improve performance.\n-\n-As a result, the depth, width and resolution of each variant of the EfficientNet models\n-are hand-picked and proven to produce good results, though they may be significantly\n-off from the compound scaling formula.\n-Therefore, the keras implementation (detailed below) only provide these 8 models, B0 to B7,\n-instead of allowing arbitray choice of width / depth / resolution parameters.\n-\n-## Keras implementation of EfficientNet\n-\n-An implementation of EfficientNet B0 to B7 has been shipped with Keras since v2.3. To\n-use EfficientNetB0 for classifying 1000 classes of images from ImageNet, run:\n-\n-```python\n-from tensorflow.keras.applications import EfficientNetB0\n-model = EfficientNetB0(weights='imagenet')\n-```\n-\n-This model takes input images of shape `(224, 224, 3)`, and the input data should be in the\n-range `[0, 255]`. Normalization is included as part of the model.\n-\n-Because training EfficientNet on ImageNet takes a tremendous amount of resources and\n-several techniques that are not a part of the model architecture itself. Hence the Keras\n-implementation by default loads pre-trained weights obtained via training with\n-[AutoAugment](https://arxiv.org/abs/1805.09501).\n-\n-For B0 to B7 base models, the input shapes are different. Here is a list of input shape\n-expected for each model:\n-\n-| Base model | resolution|\n-|----------------|-----|\n-| EfficientNetB0 | 224 |\n-| EfficientNetB1 | 240 |\n-| EfficientNetB2 | 260 |\n-| EfficientNetB3 | 300 |\n-| EfficientNetB4 | 380 |\n-| EfficientNetB5 | 456 |\n-| EfficientNetB6 | 528 |\n-| EfficientNetB7 | 600 |\n-\n-When the model is intended for transfer learning, the Keras implementation\n-provides a option to remove the top layers:\n-```\n-model = EfficientNetB0(include_top=False, weights='imagenet')\n-```\n-This option excludes the final `Dense` layer that turns 1280 features on the penultimate\n-layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom\n-layers allows using EfficientNet as a feature extractor in a transfer learning workflow.\n-\n-Another argument in the model constructor worth noticing is `drop_connect_rate` which controls\n-the dropout rate responsible for [stochastic depth](https://arxiv.org/abs/1603.09382).\n-This parameter serves as a toggle for extra regularization in finetuning, but does not\n-affect loaded weights. For example, when stronger regularization is desired, try:\n-\n-```python\n-model = EfficientNetB0(weights='imagenet', drop_connect_rate=0.4)\n-```\n-The default value is 0.2.\n-\n-## Example: EfficientNetB0 for Stanford Dogs.\n-\n-EfficientNet is capable of a wide range of image classification tasks.\n-This makes it a good model for transfer learning.\n-As an end-to-end example, we will show using pre-trained EfficientNetB0 on\n-[Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html) dataset.\n-\n-\"\"\"\n-\n-\"\"\"\n-## Setup and data loading\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow_datasets as tfds\n-import tensorflow as tf  # For tf.data\n-import matplotlib.pyplot as plt\n-import keras\n-from keras import layers\n-from keras.applications import EfficientNetB0\n-\n-# IMG_SIZE is determined by EfficientNet model choice\n-IMG_SIZE = 224\n-BATCH_SIZE = 64\n-\n-\n-\"\"\"\n-### Loading data\n-\n-Here we load data from [tensorflow_datasets](https://www.tensorflow.org/datasets)\n-(hereafter TFDS).\n-Stanford Dogs dataset is provided in\n-TFDS as [stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs).\n-It features 20,580 images that belong to 120 classes of dog breeds\n-(12,000 for training and 8,580 for testing).\n-\n-By simply changing `dataset_name` below, you may also try this notebook for\n-other datasets in TFDS such as\n-[cifar10](https://www.tensorflow.org/datasets/catalog/cifar10),\n-[cifar100](https://www.tensorflow.org/datasets/catalog/cifar100),\n-[food101](https://www.tensorflow.org/datasets/catalog/food101),\n-etc. When the images are much smaller than the size of EfficientNet input,\n-we can simply upsample the input images. It has been shown in\n-[Tan and Le, 2019](https://arxiv.org/abs/1905.11946) that transfer learning\n-result is better for increased resolution even if input images remain small.\n-\"\"\"\n-\n-dataset_name = \"stanford_dogs\"\n-(ds_train, ds_test), ds_info = tfds.load(\n-    dataset_name, split=[\"train\", \"test\"], with_info=True, as_supervised=True\n-)\n-NUM_CLASSES = ds_info.features[\"label\"].num_classes\n-\n-\n-\"\"\"\n-When the dataset include images with various size, we need to resize them into a\n-shared size. The Stanford Dogs dataset includes only images at least 200x200\n-pixels in size. Here we resize the images to the input size needed for EfficientNet.\n-\"\"\"\n-\n-size = (IMG_SIZE, IMG_SIZE)\n-ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))\n-ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))\n-\n-\"\"\"\n-### Visualizing the data\n-\n-The following code shows the first 9 images with their labels.\n-\"\"\"\n-\n-\n-def format_label(label):\n-    string_label = label_info.int2str(label)\n-    return string_label.split(\"-\")[1]\n-\n-\n-label_info = ds_info.features[\"label\"]\n-for i, (image, label) in enumerate(ds_train.take(9)):\n-    ax = plt.subplot(3, 3, i + 1)\n-    plt.imshow(image.numpy().astype(\"uint8\"))\n-    plt.title(\"{}\".format(format_label(label)))\n-    plt.axis(\"off\")\n-\n-\n-\"\"\"\n-### Data augmentation\n-\n-We can use the preprocessing layers APIs for image augmentation.\n-\"\"\"\n-\n-img_augmentation_layers = [\n-    layers.RandomRotation(factor=0.15),\n-    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n-    layers.RandomFlip(),\n-    layers.RandomContrast(factor=0.1),\n-]\n-\n-def img_augmentation(images):\n-    for layer in img_augmentation_layers:\n-        images = layer(images)\n-    return images\n-\n-\n-\"\"\"\n-This `Sequential` model object can be used both as a part of\n-the model we later build, and as a function to preprocess\n-data before feeding into the model. Using them as function makes\n-it easy to visualize the augmented images. Here we plot 9 examples\n-of augmentation result of a given figure.\n-\"\"\"\n-\n-for image, label in ds_train.take(1):\n-    for i in range(9):\n-        ax = plt.subplot(3, 3, i + 1)\n-        aug_img = img_augmentation(np.expand_dims(image.numpy(), axis=0))\n-        aug_img = np.array(aug_img)\n-        plt.imshow(aug_img[0].astype(\"uint8\"))\n-        plt.title(\"{}\".format(format_label(label)))\n-        plt.axis(\"off\")\n-\n-\n-\"\"\"\n-### Prepare inputs\n-\n-Once we verify the input data and augmentation are working correctly,\n-we prepare dataset for training. The input data are resized to uniform\n-`IMG_SIZE`. The labels are put into one-hot\n-(a.k.a. categorical) encoding. The dataset is batched.\n-\n-Note: `prefetch` and `AUTOTUNE` may in some situation improve\n-performance, but depends on environment and the specific dataset used.\n-See this [guide](https://www.tensorflow.org/guide/data_performance)\n-for more information on data pipeline performance.\n-\"\"\"\n-\n-\n-# One-hot / categorical encoding\n-def input_preprocess_train(image, label):\n-    image = img_augmentation(image)\n-    label = tf.one_hot(label, NUM_CLASSES)\n-    return image, label\n-\n-\n-def input_preprocess_test(image, label):\n-    label = tf.one_hot(label, NUM_CLASSES)\n-    return image, label\n-\n-\n-ds_train = ds_train.map(input_preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n-ds_train = ds_train.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n-ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n-\n-ds_test = ds_test.map(input_preprocess_test, num_parallel_calls=tf.data.AUTOTUNE)\n-ds_test = ds_test.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n-\n-\n-\"\"\"\n-## Training a model from scratch\n-\n-We build an EfficientNetB0 with 120 output classes, that is initialized from scratch:\n-\n-Note: the accuracy will increase very slowly and may overfit.\n-\"\"\"\n-\n-model = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n-model.compile(\n-    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n-)\n-\n-model.summary()\n-\n-epochs = 40  # @param {type: \"slider\", min:10, max:100}\n-hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n-\n-\n-\"\"\"\n-Training the model is relatively fast. This might make it sounds easy to simply train EfficientNet on any\n-dataset wanted from scratch. However, training EfficientNet on smaller datasets,\n-especially those with lower resolution like CIFAR-100, faces the significant challenge of\n-overfitting.\n-\n-Hence training from scratch requires very careful choice of hyperparameters and is\n-difficult to find suitable regularization. It would also be much more demanding in resources.\n-Plotting the training and validation accuracy\n-makes it clear that validation accuracy stagnates at a low value.\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-\n-\n-def plot_hist(hist):\n-    plt.plot(hist.history[\"accuracy\"])\n-    plt.plot(hist.history[\"val_accuracy\"])\n-    plt.title(\"model accuracy\")\n-    plt.ylabel(\"accuracy\")\n-    plt.xlabel(\"epoch\")\n-    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n-    plt.show()\n-\n-\n-plot_hist(hist)\n-\n-\"\"\"\n-## Transfer learning from pre-trained weights\n-\n-Here we initialize the model with pre-trained ImageNet weights,\n-and we fine-tune it on our own dataset.\n-\"\"\"\n-\n-\n-def build_model(num_classes):\n-    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n-    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n-\n-    # Freeze the pretrained weights\n-    model.trainable = False\n-\n-    # Rebuild top\n-    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n-    x = layers.BatchNormalization()(x)\n-\n-    top_dropout_rate = 0.2\n-    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n-\n-    # Compile\n-    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n-    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n-    model.compile(\n-        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n-    )\n-    return model\n-\n-\n-\"\"\"\n-The first step to transfer learning is to freeze all layers and train only the top\n-layers. For this step, a relatively large learning rate (1e-2) can be used.\n-Note that validation accuracy and loss will usually be better than training\n-accuracy and loss. This is because the regularization is strong, which only\n-suppresses training-time metrics.\n-\n-Note that the convergence may take up to 50 epochs depending on choice of learning rate.\n-If image augmentation layers were not\n-applied, the validation accuracy may only reach ~60%.\n-\"\"\"\n-\n-model = build_model(num_classes=NUM_CLASSES)\n-\n-epochs = 25  # @param {type: \"slider\", min:8, max:80}\n-hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n-plot_hist(hist)\n-\n-\"\"\"\n-The second step is to unfreeze a number of layers and fit the model using smaller\n-learning rate. In this example we show unfreezing all layers, but depending on\n-specific dataset it may be desireble to only unfreeze a fraction of all layers.\n-\n-When the feature extraction with\n-pretrained model works good enough, this step would give a very limited gain on\n-validation accuracy. In our case we only see a small improvement,\n-as ImageNet pretraining already exposed the model to a good amount of dogs.\n-\n-On the other hand, when we use pretrained weights on a dataset that is more different\n-from ImageNet, this fine-tuning step can be crucial as the feature extractor also\n-needs to be adjusted by a considerable amount. Such a situation can be demonstrated\n-if choosing CIFAR-100 dataset instead, where fine-tuning boosts validation accuracy\n-by about 10% to pass 80% on `EfficientNetB0`.\n-In such a case the convergence may take more than 50 epochs.\n-\n-A side note on freezing/unfreezing models: setting `trainable` of a `Model` will\n-simultaneously set all layers belonging to the `Model` to the same `trainable`\n-attribute. Each layer is trainable only if both the layer itself and the model\n-containing it are trainable. Hence when we need to partially freeze/unfreeze\n-a model, we need to make sure the `trainable` attribute of the model is set\n-to `True`.\n-\"\"\"\n-\n-\n-def unfreeze_model(model):\n-    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n-    for layer in model.layers[-20:]:\n-        if not isinstance(layer, layers.BatchNormalization):\n-            layer.trainable = True\n-\n-    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n-    model.compile(\n-        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n-    )\n-\n-\n-unfreeze_model(model)\n-\n-epochs = 10  # @param {type: \"slider\", min:8, max:50}\n-hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test)\n-plot_hist(hist)\n-\n-\"\"\"\n-### Tips for fine tuning EfficientNet\n-\n-On unfreezing layers:\n-\n-- The `BatchNormalization` layers need to be kept frozen\n-([more details](https://keras.io/guides/transfer_learning/)).\n-If they are also turned to trainable, the\n-first epoch after unfreezing will significantly reduce accuracy.\n-- In some cases it may be beneficial to open up only a portion of layers instead of\n-unfreezing all. This will make fine tuning much faster when going to larger models like\n-B7.\n-- Each block needs to be all turned on or off. This is because the architecture includes\n-a shortcut from the first layer to the last layer for each block. Not respecting blocks\n-also significantly harms the final performance.\n-\n-Some other tips for utilizing EfficientNet:\n-\n-- Larger variants of EfficientNet do not guarantee improved performance, especially for\n-tasks with less data or fewer classes. In such a case, the larger variant of EfficientNet\n-chosen, the harder it is to tune hyperparameters.\n-- EMA (Exponential Moving Average) is very helpful in training EfficientNet from scratch,\n-but not so much for transfer learning.\n-- Do not use the RMSprop setup as in the original paper for transfer learning. The\n-momentum and learning rate are too high for transfer learning. It will easily corrupt the\n-pretrained weight and blow up the loss. A quick check is to see if loss (as categorical\n-cross entropy) is getting significantly larger than log(NUM_CLASSES) after the same\n-epoch. If so, the initial learning rate/momentum is too high.\n-- Smaller batch size benefit validation accuracy, possibly due to effectively providing\n-regularization.\n-\"\"\"\n\n@@ -1,323 +0,0 @@\n-\"\"\"\n-Title: Image classification from scratch\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2020/04/27\n-Last modified: 2023/11/09\n-Description: Training an image classifier from scratch on the Kaggle Cats vs Dogs dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to do image classification from scratch, starting from JPEG\n-image files on disk, without leveraging pre-trained weights or a pre-made Keras\n-Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary\n-classification dataset.\n-\n-We use the `image_dataset_from_directory` utility to generate the datasets, and\n-we use Keras image preprocessing layers for image standardization and data augmentation.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import numpy as np\n-import keras\n-from keras import layers\n-from tensorflow import data as tf_data\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Load the data: the Cats vs Dogs dataset\n-\n-### Raw data download\n-\n-First, let's download the 786M ZIP archive of the raw data:\n-\"\"\"\n-\n-\"\"\"shell\n-curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n-\"\"\"\n-\n-\"\"\"shell\n-unzip -q kagglecatsanddogs_5340.zip\n-ls\n-\"\"\"\n-\n-\"\"\"\n-Now we have a `PetImages` folder which contain two subfolders, `Cat` and `Dog`. Each\n-subfolder contains image files for each category.\n-\"\"\"\n-\n-\"\"\"shell\n-ls PetImages\n-\"\"\"\n-\n-\"\"\"\n-### Filter out corrupted images\n-\n-When working with lots of real-world image data, corrupted images are a common\n-occurence. Let's filter out badly-encoded images that do not feature the string \"JFIF\"\n-in their header.\n-\"\"\"\n-\n-num_skipped = 0\n-for folder_name in (\"Cat\", \"Dog\"):\n-    folder_path = os.path.join(\"PetImages\", folder_name)\n-    for fname in os.listdir(folder_path):\n-        fpath = os.path.join(folder_path, fname)\n-        try:\n-            fobj = open(fpath, \"rb\")\n-            is_jfif = b\"JFIF\" in fobj.peek(10)\n-        finally:\n-            fobj.close()\n-\n-        if not is_jfif:\n-            num_skipped += 1\n-            # Delete corrupted image\n-            os.remove(fpath)\n-\n-print(f\"Deleted {num_skipped} images.\")\n-\n-\"\"\"\n-## Generate a `Dataset`\n-\"\"\"\n-\n-image_size = (180, 180)\n-batch_size = 128\n-\n-train_ds, val_ds = keras.utils.image_dataset_from_directory(\n-    \"PetImages\",\n-    validation_split=0.2,\n-    subset=\"both\",\n-    seed=1337,\n-    image_size=image_size,\n-    batch_size=batch_size,\n-)\n-\n-\"\"\"\n-## Visualize the data\n-\n-Here are the first 9 images in the training dataset.\n-\"\"\"\n-\n-\n-plt.figure(figsize=(10, 10))\n-for images, labels in train_ds.take(1):\n-    for i in range(9):\n-        ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n-        plt.title(int(labels[i]))\n-        plt.axis(\"off\")\n-\n-\"\"\"\n-## Using image data augmentation\n-\n-When you don't have a large image dataset, it's a good practice to artificially\n-introduce sample diversity by applying random yet realistic transformations to the\n-training images, such as random horizontal flipping or small random rotations. This\n-helps expose the model to different aspects of the training data while slowing down\n-overfitting.\n-\"\"\"\n-\n-data_augmentation_layers = [\n-    layers.RandomFlip(\"horizontal\"),\n-    layers.RandomRotation(0.1),\n-]\n-\n-def data_augmentation(images):\n-    for layer in data_augmentation_layers:\n-        images = layer(images)\n-    return images\n-\n-\n-\"\"\"\n-Let's visualize what the augmented samples look like, by applying `data_augmentation`\n-repeatedly to the first few images in the dataset:\n-\"\"\"\n-\n-plt.figure(figsize=(10, 10))\n-for images, _ in train_ds.take(1):\n-    for i in range(9):\n-        augmented_images = data_augmentation(images)\n-        ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n-        plt.axis(\"off\")\n-\n-\n-\"\"\"\n-## Standardizing the data\n-\n-Our image are already in a standard size (180x180), as they are being yielded as\n-contiguous `float32` batches by our dataset. However, their RGB channel values are in\n-the `[0, 255]` range. This is not ideal for a neural network;\n-in general you should seek to make your input values small. Here, we will\n-standardize values to be in the `[0, 1]` by using a `Rescaling` layer at the start of\n-our model.\n-\"\"\"\n-\n-\"\"\"\n-## Two options to preprocess the data\n-\n-There are two ways you could be using the `data_augmentation` preprocessor:\n-\n-**Option 1: Make it part of the model**, like this:\n-\n-```python\n-inputs = keras.Input(shape=input_shape)\n-x = data_augmentation(inputs)\n-x = layers.Rescaling(1./255)(x)\n-...  # Rest of the model\n-```\n-\n-With this option, your data augmentation will happen *on device*, synchronously\n-with the rest of the model execution, meaning that it will benefit from GPU\n-acceleration.\n-\n-Note that data augmentation is inactive at test time, so the input samples will only be\n-augmented during `fit()`, not when calling `evaluate()` or `predict()`.\n-\n-If you're training on GPU, this may be a good option.\n-\n-**Option 2: apply it to the dataset**, so as to obtain a dataset that yields batches of\n-augmented images, like this:\n-\n-```python\n-augmented_train_ds = train_ds.map(\n-    lambda x, y: (data_augmentation(x, training=True), y))\n-```\n-\n-With this option, your data augmentation will happen **on CPU**, asynchronously, and will\n-be buffered before going into the model.\n-\n-If you're training on CPU, this is the better option, since it makes data augmentation\n-asynchronous and non-blocking.\n-\n-In our case, we'll go with the second option. If you're not sure\n-which one to pick, this second option (asynchronous preprocessing) is always a solid choice.\n-\"\"\"\n-\n-\"\"\"\n-## Configure the dataset for performance\n-\n-Let's apply data augmentation to our training dataset,\n-and let's make sure to use buffered prefetching so we can yield data from disk without\n-having I/O becoming blocking:\n-\"\"\"\n-\n-# Apply `data_augmentation` to the training images.\n-train_ds = train_ds.map(\n-    lambda img, label: (data_augmentation(img), label),\n-    num_parallel_calls=tf_data.AUTOTUNE,\n-)\n-# Prefetching samples in GPU memory helps maximize GPU utilization.\n-train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n-val_ds = val_ds.prefetch(tf_data.AUTOTUNE)\n-\n-\"\"\"\n-## Build a model\n-\n-We'll build a small version of the Xception network. We haven't particularly tried to\n-optimize the architecture; if you want to do a systematic search for the best model\n-configuration, consider using\n-[KerasTuner](https://github.com/keras-team/keras-tuner).\n-\n-Note that:\n-\n-- We start the model with the `data_augmentation` preprocessor, followed by a\n- `Rescaling` layer.\n-- We include a `Dropout` layer before the final classification layer.\n-\"\"\"\n-\n-\n-def make_model(input_shape, num_classes):\n-    inputs = keras.Input(shape=input_shape)\n-\n-    # Entry block\n-    x = layers.Rescaling(1.0 / 255)(inputs)\n-    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    previous_block_activation = x  # Set aside residual\n-\n-    for size in [256, 512, 728]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n-\n-        # Project residual\n-        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n-            previous_block_activation\n-        )\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    x = layers.GlobalAveragePooling2D()(x)\n-    if num_classes == 2:\n-        units = 1\n-    else:\n-        units = num_classes\n-\n-    x = layers.Dropout(0.25)(x)\n-    # We specify activation=None so as to return logits\n-    outputs = layers.Dense(units, activation=None)(x)\n-    return keras.Model(inputs, outputs)\n-\n-\n-model = make_model(input_shape=image_size + (3,), num_classes=2)\n-keras.utils.plot_model(model, show_shapes=True)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 25\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n-]\n-model.compile(\n-    optimizer=keras.optimizers.Adam(3e-4),\n-    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n-)\n-model.fit(\n-    train_ds,\n-    epochs=epochs,\n-    callbacks=callbacks,\n-    validation_data=val_ds,\n-)\n-\n-\"\"\"\n-We get to >90% validation accuracy after training for 25 epochs on the full dataset\n-(in practice, you can train for 50+ epochs before validation performance starts degrading).\n-\"\"\"\n-\n-\"\"\"\n-## Run inference on new data\n-\n-Note that data augmentation and dropout are inactive at inference time.\n-\"\"\"\n-\n-img = keras.utils.load_img(\"PetImages/Cat/6779.jpg\", target_size=image_size)\n-plt.imshow(img)\n-\n-img_array = keras.utils.img_to_array(img)\n-img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis\n-\n-predictions = model.predict(img_array)\n-score = float(keras.ops.sigmoid(predictions[0]))\n-print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")\n\n@@ -1,337 +0,0 @@\n-\"\"\"\n-Title: Image classification with Vision Transformer\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Converted to Keras 3 by: [divyasreepat](https://github.com/divyashreepathihalli), [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/01/18\n-Last modified: 2021/01/18\n-Description: Implementing the Vision Transformer (ViT) model for image classification.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n-model by Alexey Dosovitskiy et al. for image classification,\n-and demonstrates it on the CIFAR-100 dataset.\n-The ViT model applies the Transformer architecture with self-attention to sequences of\n-image patches, without using convolution layers.\n-\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\"\"\"\n-\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 256\n-num_epochs = 100\n-image_size = 72  # We'll resize input images to this size\n-patch_size = 6  # Size of the patches to be extract from the input images\n-num_patches = (image_size // patch_size) ** 2\n-projection_dim = 64\n-num_heads = 4\n-transformer_units = [\n-    projection_dim * 2,\n-    projection_dim,\n-]  # Size of the transformer layers\n-transformer_layers = 8\n-mlp_head_units = [\n-    2048,\n-    1024,\n-]  # Size of the dense layers of the final classifier\n-\n-\n-\"\"\"\n-## Use data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.Resizing(image_size, image_size),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(factor=0.02),\n-        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-# Compute the mean and the variance of the training data for normalization.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\n-\"\"\"\n-## Implement multilayer perceptron (MLP)\n-\"\"\"\n-\n-\n-def mlp(x, hidden_units, dropout_rate):\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement patch creation as a layer\n-\"\"\"\n-\n-\n-class Patches(layers.Layer):\n-    def __init__(self, patch_size):\n-        super().__init__()\n-        self.patch_size = patch_size\n-\n-    def call(self, images):\n-        input_shape = ops.shape(images)\n-        batch_size = input_shape[0]\n-        height = input_shape[1]\n-        width = input_shape[2]\n-        channels = input_shape[3]\n-        num_patches_h = height // self.patch_size\n-        num_patches_w = width // self.patch_size\n-        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n-        patches = ops.reshape(\n-            patches,\n-            (\n-                batch_size,\n-                num_patches_h * num_patches_w,\n-                self.patch_size * self.patch_size * channels,\n-            ),\n-        )\n-        return patches\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"patch_size\": self.patch_size})\n-        return config\n-\n-\n-\"\"\"\n-Let's display patches for a sample image\n-\"\"\"\n-\n-plt.figure(figsize=(4, 4))\n-image = x_train[np.random.choice(range(x_train.shape[0]))]\n-plt.imshow(image.astype(\"uint8\"))\n-plt.axis(\"off\")\n-\n-resized_image = ops.image.resize(\n-    ops.convert_to_tensor([image]), size=(image_size, image_size)\n-)\n-patches = Patches(patch_size)(resized_image)\n-print(f\"Image size: {image_size} X {image_size}\")\n-print(f\"Patch size: {patch_size} X {patch_size}\")\n-print(f\"Patches per image: {patches.shape[1]}\")\n-print(f\"Elements per patch: {patches.shape[-1]}\")\n-\n-n = int(np.sqrt(patches.shape[1]))\n-plt.figure(figsize=(4, 4))\n-for i, patch in enumerate(patches[0]):\n-    ax = plt.subplot(n, n, i + 1)\n-    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n-    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Implement the patch encoding layer\n-\n-The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n-vector of size `projection_dim`. In addition, it adds a learnable position\n-embedding to the projected vector.\n-\"\"\"\n-\n-\n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.projection = layers.Dense(units=projection_dim)\n-        self.position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=projection_dim\n-        )\n-\n-    def call(self, patch):\n-        positions = ops.expand_dims(\n-            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n-        )\n-        projected_patches = self.projection(patch)\n-        encoded = projected_patches + self.position_embedding(positions)\n-        return encoded\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"num_patches\": self.num_patches})\n-        return config\n-\n-\n-\"\"\"\n-## Build the ViT model\n-\n-The ViT model consists of multiple Transformer blocks,\n-which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n-applied to the sequence of patches. The Transformer blocks produce a\n-`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n-classifier head with softmax to produce the final class probabilities output.\n-\n-Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n-which prepends a learnable embedding to the sequence of encoded patches to serve\n-as the image representation, all the outputs of the final Transformer block are\n-reshaped with `layers.Flatten()` and used as the image\n-representation input to the classifier head.\n-Note that the `layers.GlobalAveragePooling1D` layer\n-could also be used instead to aggregate the outputs of the Transformer block,\n-especially when the number of patches and the projection dimensions are large.\n-\"\"\"\n-\n-\n-def create_vit_classifier():\n-    inputs = keras.Input(shape=input_shape)\n-    # Augment data.\n-    augmented = data_augmentation(inputs)\n-    # Create patches.\n-    patches = Patches(patch_size)(augmented)\n-    # Encode patches.\n-    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n-\n-    # Create multiple layers of the Transformer block.\n-    for _ in range(transformer_layers):\n-        # Layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-        # Skip connection 1.\n-        x2 = layers.Add()([attention_output, encoded_patches])\n-        # Layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n-        # MLP.\n-        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n-        # Skip connection 2.\n-        encoded_patches = layers.Add()([x3, x2])\n-\n-    # Create a [batch_size, projection_dim] tensor.\n-    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-    representation = layers.Flatten()(representation)\n-    representation = layers.Dropout(0.5)(representation)\n-    # Add MLP.\n-    features = mlp(\n-        representation, hidden_units=mlp_head_units, dropout_rate=0.5\n-    )\n-    # Classify outputs.\n-    logits = layers.Dense(num_classes)(features)\n-    # Create the Keras model.\n-    model = keras.Model(inputs=inputs, outputs=logits)\n-    return model\n-\n-\n-\"\"\"\n-## Compile, train, and evaluate the mode\n-\"\"\"\n-\n-\n-def run_experiment(model):\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    )\n-\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-        metrics=[\n-            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n-            keras.metrics.SparseTopKCategoricalAccuracy(\n-                5, name=\"top-5-accuracy\"\n-            ),\n-        ],\n-    )\n-\n-    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[checkpoint_callback],\n-    )\n-\n-    model.load_weights(checkpoint_filepath)\n-    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-    return history\n-\n-\n-vit_classifier = create_vit_classifier()\n-history = run_experiment(vit_classifier)\n-\n-\n-def plot_history(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_history(\"loss\")\n-plot_history(\"top-5-accuracy\")\n-\n-\n-\"\"\"\n-After 100 epochs, the ViT model achieves around 55% accuracy and\n-82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n-as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n-\n-Note that the state of the art results reported in the\n-[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n-the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n-without pre-training, you can try to train the model for more epochs, use a larger number of\n-Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n-Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n-but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n-In practice, it's recommended to fine-tune a ViT model\n-that was pre-trained using a large, high-resolution dataset.\n-\"\"\"\n\n@@ -1,692 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"\n-Author: [lukewood](https://lukewood.xyz)\n-Date created: 03/28/2023\n-Last modified: 07/25/2023\n-Description: Use KerasCV to train powerful image classifiers.\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-Classification is the process of predicting a categorical label for a given\n-input image.\n-While classification is a relatively straightforward computer vision task,\n-modern approaches still are built of several complex components.\n-Luckily, KerasCV provides APIs to construct commonly used components.\n-\n-This guide demonstrates KerasCV's modular approach to solving image\n-classification problems at three levels of complexity:\n-\n-- Inference with a pretrained classifier\n-- Fine-tuning a pretrained backbone\n-- Training a image classifier from scratch\n-\n-## Multi-Backend Support\n-\n-KerasCV's `ImageClassifier` model supports several backends like JAX, PyTorch,\n-and TensorFlow with the help of `keras`. To enable multi-backend support\n-in KerasCV, set the `KERAS_CV_MULTI_BACKEND` environment variable. We can\n-then switch between different backends by setting the `KERAS_BACKEND`\n-environment variable. Currently, `\"tensorflow\"`, `\"jax\"`, and `\"torch\"` are\n-supported.\n-\n-This demonstration uses the Jax backend.\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_CV_MULTI_BACKEND\"] = \"1\"\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"\n-\n-import json\n-import math\n-import keras_cv\n-import keras\n-from keras import ops\n-from keras import losses\n-from keras import optimizers\n-from keras.optimizers import schedules\n-from keras import metrics\n-import tensorflow as tf\n-from tensorflow import data as tf_data\n-import tensorflow_datasets as tfds\n-import numpy as np\n-\n-\"\"\"## Inference with a pretrained classifier\n-\n-Let's get started with the simplest KerasCV API: a pretrained classifier.\n-In this example, we will construct a classifier that was\n-pretrained on the ImageNet dataset.\n-We'll use this model to solve the age old \"Cat or Dog\" problem.\n-\n-The highest level module in KerasCV is a *task*. A *task* is a `keras.Model`\n-consisting of a (generally pretrained) backbone model and task-specific\n-layers. Here's an example using `keras_cv.models.ImageClassifier` with an\n-EfficientNetV2B0 Backbone.\n-\n-EfficientNetV2B0 is a great starting model when constructing an image\n-classification pipeline.\n-This architecture manages to achieve high accuracy, while using a\n-parameter count of 7M.\n-If an EfficientNetV2B0 is not powerful enough for the task you are hoping to\n-solve, be sure to check out\n-[KerasCV's other available Backbones](https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/backbones)!\n-\"\"\"\n-\n-classifier = keras_cv.models.ImageClassifier.from_preset(\n-    \"efficientnetv2_b0_imagenet_classifier\"\n-)\n-\n-\"\"\"You may notice a small deviation from the old `keras.applications` API;\n-where you would construct the class with\n-`EfficientNetV2B0(weights=\"imagenet\")`. While the old API was great for\n-classification, it did not scale effectively to other use cases that required\n-complex architectures, like object deteciton and semantic segmentation.\n-\n-Now that our classifier is built, let's apply it to this cute cat picture!\n-\"\"\"\n-\n-filepath = keras.utils.get_file(origin=\"https://i.imgur.com/9i63gLN.jpg\")\n-image = keras.utils.load_img(filepath)\n-image = np.array(image)\n-keras_cv.visualization.plot_image_gallery(\n-    image[None, ...], rows=1, cols=1, value_range=(0, 255), show=True, scale=4\n-)\n-\n-\"\"\"Next, let's get some predictions from our classifier:\"\"\"\n-\n-predictions = classifier.predict(np.expand_dims(image, axis=0))\n-\n-\"\"\"Predictions come in the form of softmax-ed category rankings.\n-We can find the index of the top classes using a simple argsort function:\n-\"\"\"\n-\n-top_classes = predictions[0].argsort(axis=-1)\n-\n-\"\"\"In order to decode the class mappings, we can construct a mapping from\n-category indices to ImageNet class names.\n-For convenience, I've stored the ImageNet class mapping in a GitHub gist.\n-Let's download and load it now.\n-\"\"\"\n-\n-classes = keras.utils.get_file(\n-    origin=\"https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json\"\n-)\n-with open(classes, \"rb\") as f:\n-    classes = json.load(f)\n-\n-\"\"\"Now we can simply look up the class names via index:\"\"\"\n-\n-top_two = [classes[str(i)] for i in top_classes[-2:]]\n-print(\"Top two classes are:\", top_two)\n-\n-\"\"\"Great!  Both of these appear to be correct!\n-However, one of the classes is \"Velvet\".\n-We're trying to classify Cats VS Dogs.\n-We don't care about the velvet blanket!\n-\n-Ideally, we'd have a classifier that only performs computation to determine if\n-an image is a cat or a dog, and has all of its resources dedicated to this\n-task. This can be solved by fine tuning our own classifier.\n-\n-# Fine tuning a pretrained classifier\n-\n-When labeled images specific to our task are available, fine-tuning a custom\n-classifier can improve performance.\n-If we want to train a Cats vs Dogs Classifier, using explicitly labeled Cat vs\n-Dog data should perform better than the generic classifier!\n-For many tasks, no relevant pretrained model\n-will be available (e.g., categorizing images specific to your application).\n-\n-First, let's get started by loading some data:\n-\"\"\"\n-\n-BATCH_SIZE = 32\n-IMAGE_SIZE = (224, 224)\n-AUTOTUNE = tf_data.AUTOTUNE\n-tfds.disable_progress_bar()\n-\n-data, dataset_info = tfds.load(\n-    \"cats_vs_dogs\", with_info=True, as_supervised=True\n-)\n-train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // BATCH_SIZE\n-train_dataset = data[\"train\"]\n-\n-num_classes = dataset_info.features[\"label\"].num_classes\n-\n-resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-encoder = keras.layers.CategoryEncoding(num_classes, \"one_hot\", dtype=\"int32\")\n-\n-\n-def preprocess_inputs(image, label):\n-    # Staticly resize images as we only iterate the dataset once.\n-    return resizing(image), encoder(label)\n-\n-\n-# Shuffle the dataset to increase diversity of batches.\n-# 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger\n-# shuffle buffers.\n-train_dataset = train_dataset.shuffle(\n-    10 * BATCH_SIZE, reshuffle_each_iteration=True\n-).map(preprocess_inputs, num_parallel_calls=AUTOTUNE)\n-train_dataset = train_dataset.batch(BATCH_SIZE)\n-\n-images = next(iter(train_dataset.take(1)))[0]\n-keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255))\n-\n-\"\"\"Meow!\n-\n-Next let's construct our model.\n-The use of imagenet in the preset name indicates that the backbone was\n-pretrained on the ImageNet dataset.\n-Pretrained backbones extract more information from our labeled examples by\n-leveraging patterns extracted from potentially much larger datasets.\n-\n-Next lets put together our classifier:\n-\"\"\"\n-\n-model = keras_cv.models.ImageClassifier.from_preset(\n-    \"efficientnetv2_b0_imagenet\", num_classes=2\n-)\n-model.compile(\n-    loss=\"categorical_crossentropy\",\n-    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n-    metrics=[\"accuracy\"],\n-)\n-\n-\"\"\"Here our classifier is just a simple `keras.Sequential`.\n-All that is left to do is call `model.fit()`:\n-\"\"\"\n-\n-model.fit(train_dataset)\n-\n-\"\"\"Let's look at how our model performs after the fine tuning:\"\"\"\n-\n-predictions = model.predict(np.expand_dims(image, axis=0))\n-\n-classes = {0: \"cat\", 1: \"dog\"}\n-print(\"Top class is:\", classes[predictions[0].argmax()])\n-\n-\"\"\"Awesome - looks like the model correctly classified the image.\n-\n-# Train a Classifier from Scratch\n-\n-Now that we've gotten our hands dirty with classification, let's take on one\n-last task: training a classification model from scratch!\n-A standard benchmark for image classification is the ImageNet dataset, however\n-due to licensing constraints we will use the CalTech 101 image classification\n-dataset in this tutorial.\n-While we use the simpler CalTech 101 dataset in this guide, the same training\n-template may be used on ImageNet to achieve near state-of-the-art scores.\n-\n-Let's start out by tackling data loading:\n-\"\"\"\n-\n-NUM_CLASSES = 101\n-# Change epochs to 100~ to fully train.\n-EPOCHS = 1\n-\n-encoder = keras.layers.CategoryEncoding(NUM_CLASSES, \"one_hot\", dtype=\"int32\")\n-\n-\n-def package_inputs(image, label):\n-    return {\"images\": image, \"labels\": encoder(label)}\n-\n-\n-train_ds, eval_ds = tfds.load(\n-    \"caltech101\", split=[\"train\", \"test\"], as_supervised=\"true\"\n-)\n-train_ds = train_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n-eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-train_ds = train_ds.shuffle(BATCH_SIZE * 16)\n-\n-\"\"\"The CalTech101 dataset has different sizes for every image, so we use the\n-`ragged_batch()` API to batch them together while maintaining each individual\n-image's shape information.\n-\"\"\"\n-\n-train_ds = train_ds.ragged_batch(BATCH_SIZE)\n-eval_ds = eval_ds.ragged_batch(BATCH_SIZE)\n-\n-batch = next(iter(train_ds.take(1)))\n-image_batch = batch[\"images\"]\n-label_batch = batch[\"labels\"]\n-\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch.to_tensor(),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"## Data Augmentation\n-\n-In our previous finetuning exmaple, we performed a static resizing operation\n-and did not utilize any image augmentation.\n-This is because a single pass over the training set was sufficient to achieve\n-decent results.\n-When training to solve a more difficult task, you'll want to include data\n-augmentation in your data pipeline.\n-\n-Data augmentation is a technique to make your model robust to changes in input\n-data such as lighting, cropping, and orientation.\n-KerasCV includes some of the most useful augmentations in the\n-`keras_cv.layers` API.\n-Creating an optimal pipeline of augmentations is an art, but in this section\n-of the guide we'll offer some tips on best practices for classification.\n-\n-One caveat to be aware of with image data augmentation is that you must be\n-careful to not shift your augmented data distribution too far from the\n-original data distribution.\n-The goal is to prevent overfitting and increase generalization,\n-but samples that lie completely out of the data distribution simply add noise\n-to the training process.\n-\n-The first augmentation we'll use is `RandomFlip`.\n-This augmentation behaves more or less how you'd expect: it either flips the\n-image or not.\n-While this augmentation is useful in CalTech101 and ImageNet, it should be\n-noted that it should not be used on tasks where the data distribution is not\n-vertical mirror invariant.\n-An example of a dataset where this occurs is MNIST hand written digits.\n-Flipping a `6` over the\n-vertical axis will make the digit appear more like a `7` than a `6`, but the\n-label will still show a `6`.\n-\"\"\"\n-\n-random_flip = keras_cv.layers.RandomFlip()\n-augmenters = [random_flip]\n-\n-image_batch = random_flip(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch.to_tensor(),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Half of the images have been flipped!\n-\n-The next augmentation we'll use is `RandomCropAndResize`.\n-This operation selects a random subset of the image, then resizes it to the\n-provided target size.\n-By using this augmentation, we force our classifier to become spatially\n-invariant.\n-Additionally, this layer accepts an `aspect_ratio_factor` which can be used to\n-distort the aspect ratio of the image.\n-While this can improve model performance, it should be used with caution.\n-It is very easy for an aspect ratio distortion to shift a sample too far from\n-the original training set's data distribution.\n-Remember - the goal of data augmentation is to produce more training samples\n-that align with the data distribution of your training set!\n-\n-`RandomCropAndResize` also can handle `tf.RaggedTensor` inputs.  In the\n-CalTech101 image dataset images come in a wide variety of sizes.\n-As such they cannot easily be batched together into a dense training batch.\n-Luckily, `RandomCropAndResize` handles the Ragged -> Dense conversion process\n-for you!\n-\n-Let's add a `RandomCropAndResize` to our set of augmentations:\n-\"\"\"\n-\n-crop_and_resize = keras_cv.layers.RandomCropAndResize(\n-    target_size=IMAGE_SIZE,\n-    crop_area_factor=(0.8, 1.0),\n-    aspect_ratio_factor=(0.9, 1.1),\n-)\n-augmenters += [crop_and_resize]\n-\n-image_batch = crop_and_resize(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Great!  We are now working with a batch of dense images.\n-Next up, lets include some spatial and color-based jitter to our training set.\n-This will allow us to produce a classifier that is robust to lighting\n-flickers, shadows, and more.\n-\n-There are limitless ways to augment an image by altering color and spatial\n-features, but perhaps the most battle tested technique is\n-[`RandAugment`](https://arxiv.org/abs/1909.13719).\n-`RandAugment` is actually a set of 10 different augmentations:\n-`AutoContrast`, `Equalize`, `Solarize`, `RandomColorJitter`, `RandomContrast`,\n-`RandomBrightness`, `ShearX`, `ShearY`, `TranslateX` and `TranslateY`.\n-At inference time, `num_augmentations` augmenters are sampled for each image,\n-and random magnitude factors are sampled for each.\n-These augmentations are then applied sequentially.\n-\n-KerasCV makes tuning these parameters easy using the `augmentations_per_image`\n-and `magnitude` parameters!\n-Let's take it for a spin:\n-\"\"\"\n-\n-rand_augment = keras_cv.layers.RandAugment(\n-    augmentations_per_image=3,\n-    magnitude=0.3,\n-    value_range=(0, 255),\n-)\n-augmenters += [rand_augment]\n-\n-image_batch = rand_augment(image_batch)\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Looks great; but we're not done yet!\n-What if an image is missing one critical feature of a class?  For example,\n-what if a leaf is blocking the view of a cat's ear, but our classifier\n-learned to classify cats simply by observing their ears?\n-\n-One easy approach to tackling this is to use `RandomCutout`, which randomly\n-strips out a sub-section of the image:\n-\"\"\"\n-\n-random_cutout = keras_cv.layers.RandomCutout(\n-    width_factor=0.4, height_factor=0.4\n-)\n-keras_cv.visualization.plot_image_gallery(\n-    random_cutout(image_batch),\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"While this tackles the problem reasonably well, it can cause the classifier\n-to develop responses to borders between features and black pixel areas caused\n-by the cutout.\n-\n-[`CutMix`](https://arxiv.org/abs/1905.04899) solves the same issue by using\n-a more complex (and more effective) technique.\n-Instead of replacing the cut-out areas with black pixels, `CutMix` replaces\n-these regions with regions of other images sampled from within your training\n-set!\n-Following this replacement, the image's classification label is updated to be\n-a blend of the original and mixed image's class label.\n-\n-What does this look like in practice?  Let's check it out:\n-\"\"\"\n-\n-cut_mix = keras_cv.layers.CutMix()\n-# CutMix needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n-\n-keras_cv.visualization.plot_image_gallery(\n-    cut_mix(inputs)[\"images\"],\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Let's hold off from adding it to our augmenter for a minute - more on that\n-soon!\n-\n-Next, let's look into `MixUp()`.\n-Unfortunately, while `MixUp()` has been empirically shown to *substantially*\n-improve both the robustness and the generalization of the trained model,\n-it is not well-understood why such improvement occurs... but\n-a little alchemy never hurt anyone!\n-\n-`MixUp()` works by sampling two images from a batch, then proceeding to\n-literally blend together their pixel intensities as well as their\n-classification labels.\n-\n-Let's see it in action:\n-\"\"\"\n-\n-mix_up = keras_cv.layers.MixUp()\n-# MixUp needs to modify both images and labels\n-inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}\n-\n-keras_cv.visualization.plot_image_gallery(\n-    mix_up(inputs)[\"images\"],\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"If you look closely, you'll see that the images have been blended together.\n-\n-Instead of applying `CutMix()` and `MixUp()` to every image, we instead pick\n-one or the other to apply to each batch.\n-This can be expressed using `keras_cv.layers.RandomChoice()`\n-\"\"\"\n-\n-cut_mix_or_mix_up = keras_cv.layers.RandomChoice(\n-    [cut_mix, mix_up], batchwise=True\n-)\n-augmenters += [cut_mix_or_mix_up]\n-\n-\"\"\"Now let's apply our final augmenter to the training data:\"\"\"\n-\n-augmenter = keras_cv.layers.Augmenter(augmenters)\n-train_ds = train_ds.map(augmenter, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-image_batch = next(iter(train_ds.take(1)))[\"images\"]\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"We also need to resize our evaluation set to get dense batches of the image\n-size expected by our model. We use the deterministic\n-`keras_cv.layers.Resizing` in this case to avoid adding noise to our\n-evaluation metric.\n-\"\"\"\n-\n-inference_resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-inference_resizing = keras_cv.layers.Resizing(\n-    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True\n-)\n-eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-image_batch = next(iter(eval_ds.take(1)))[\"images\"]\n-keras_cv.visualization.plot_image_gallery(\n-    image_batch,\n-    rows=3,\n-    cols=3,\n-    value_range=(0, 255),\n-    show=True,\n-)\n-\n-\"\"\"Finally, lets unpackage our datasets and prepare to pass them to\n-`model.fit()`, which accepts a tuple of `(images, labels)`.\n-\"\"\"\n-\n-\n-def unpackage_dict(inputs):\n-    return inputs[\"images\"], inputs[\"labels\"]\n-\n-\n-train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n-eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)\n-\n-\"\"\"Data augmentation is by far the hardest piece of training a modern\n-classifier.\n-Congratulations on making it this far!\n-\n-## Optimizer Tuning\n-\n-To achieve optimal performance, we need to use a learning rate schedule\n-instead of a single learning rate. While we won't go into detail on the\n-Cosine decay with warmup schedule used here, [you can read more about it\n-here](https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b).\n-\"\"\"\n-\n-\n-def lr_warmup_cosine_decay(\n-    global_step,\n-    warmup_steps,\n-    hold=0,\n-    total_steps=0,\n-    start_lr=0.0,\n-    target_lr=1e-2,\n-):\n-    # Cosine decay\n-    learning_rate = (\n-        0.5\n-        * target_lr\n-        * (\n-            1\n-            + ops.cos(\n-                math.pi\n-                * ops.convert_to_tensor(\n-                    global_step - warmup_steps - hold, dtype=\"float32\"\n-                )\n-                / ops.convert_to_tensor(\n-                    total_steps - warmup_steps - hold, dtype=\"float32\"\n-                )\n-            )\n-        )\n-    )\n-\n-    warmup_lr = target_lr * (global_step / warmup_steps)\n-\n-    if hold > 0:\n-        learning_rate = ops.where(\n-            global_step > warmup_steps + hold, learning_rate, target_lr\n-        )\n-\n-    learning_rate = ops.where(\n-        global_step < warmup_steps, warmup_lr, learning_rate\n-    )\n-    return learning_rate\n-\n-\n-class WarmUpCosineDecay(schedules.LearningRateSchedule):\n-    def __init__(\n-        self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2\n-    ):\n-        super().__init__()\n-        self.start_lr = start_lr\n-        self.target_lr = target_lr\n-        self.warmup_steps = warmup_steps\n-        self.total_steps = total_steps\n-        self.hold = hold\n-\n-    def __call__(self, step):\n-        lr = lr_warmup_cosine_decay(\n-            global_step=step,\n-            total_steps=self.total_steps,\n-            warmup_steps=self.warmup_steps,\n-            start_lr=self.start_lr,\n-            target_lr=self.target_lr,\n-            hold=self.hold,\n-        )\n-\n-        return ops.where(step > self.total_steps, 0.0, lr)\n-\n-\n-\"\"\"![WarmUpCosineDecay schedule](https://i.imgur.com/YCr5pII.png)\n-\n-The schedule looks a as we expect.\n-\n-Next let's construct this optimizer:\n-\"\"\"\n-\n-total_images = 9000\n-total_steps = (total_images // BATCH_SIZE) * EPOCHS\n-warmup_steps = int(0.1 * total_steps)\n-hold_steps = int(0.45 * total_steps)\n-schedule = WarmUpCosineDecay(\n-    start_lr=0.05,\n-    target_lr=1e-2,\n-    warmup_steps=warmup_steps,\n-    total_steps=total_steps,\n-    hold=hold_steps,\n-)\n-optimizer = optimizers.SGD(\n-    weight_decay=5e-4,\n-    learning_rate=schedule,\n-    momentum=0.9,\n-)\n-\n-\"\"\"At long last, we can now build our model and call `fit()`!\n-`keras_cv.models.EfficientNetV2B0Backbone()` is a convenience alias for\n-`keras_cv.models.EfficientNetV2Backbone.from_preset('efficientnetv2_b0')`.\n-Note that this preset does not come with any pretrained weights.\n-\"\"\"\n-\n-backbone = keras_cv.models.ResNet18V2Backbone()\n-model = keras.Sequential(\n-    [\n-        backbone,\n-        keras.layers.GlobalMaxPooling2D(),\n-        keras.layers.Dropout(rate=0.5),\n-        keras.layers.Dense(101, activation=\"softmax\"),\n-    ]\n-)\n-\n-\"\"\"Since the labels produced by MixUp() and CutMix() are somewhat artificial,\n-we employ label smoothing to prevent the model from overfitting to artifacts\n-of this augmentation process.\n-\"\"\"\n-\n-loss = losses.CategoricalCrossentropy(label_smoothing=0.1)\n-\n-\"\"\"Let's compile our model:\"\"\"\n-\n-model.compile(\n-    loss=loss,\n-    optimizer=optimizer,\n-    metrics=[\n-        metrics.CategoricalAccuracy(),\n-        metrics.TopKCategoricalAccuracy(k=5),\n-    ],\n-)\n-\n-\"\"\"and finally call fit().\"\"\"\n-\n-model.fit(\n-    train_ds,\n-    epochs=EPOCHS,\n-    validation_data=eval_ds,\n-)\n-\n-\"\"\"Congratulations!  You now know how to train a powerful image classifier\n-from scratch in KerasCV.\n-Depending on the availability of labeled data for your application, training\n-from scratch may or may not be more powerful than using transfer learning in\n-addition to the data augmentations discussed above. For smaller datasets,\n-pretrained models generally produce high accuracy and faster convergence.\n-\n-## Conclusions\n-\n-While image classification is perhaps the simplest problem in computer vision,\n-the modern landscape has numerous complex components.\n-Luckily, KerasCV offers robust, production-grade APIs to make assembling most\n-of these components possible in one line of code.\n-Through the use of KerasCV's `ImageClassifier` API, pretrained weights, and\n-KerasCV data augmentations you can assemble everything you need to train a\n-powerful classifier in a few hundred lines of code!\n-\n-As a follow up exercise, give the following a try:\n-\n-- Fine tune a KerasCV classifier on your own dataset\n-- Learn more about [KerasCV's data augmentations](https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/)\n-- Check out how we train our models on [ImageNet](https://github.com/keras-team/keras-cv/blob/master/examples/training/classification/imagenet/basic_training.py)\n-\"\"\"\n\n@@ -1,471 +0,0 @@\n-\"\"\"\n-Title: Keypoint Detection with Transfer Learning\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 2021/05/02\n-Last modified: 2023/07/19\n-Description: Training a keypoint detector with data augmentation and transfer learning.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-Keypoint detection consists of locating key object parts. For example, the key parts\n-of our faces include nose tips, eyebrows, eye corners, and so on. These parts help to\n-represent the underlying object in a feature-rich manner. Keypoint detection has\n-applications that include pose estimation, face detection, etc.\n-\n-In this example, we will build a keypoint detector using the\n-[StanfordExtra dataset](https://github.com/benjiebob/StanfordExtra),\n-using transfer learning. This example requires TensorFlow 2.4 or higher,\n-as well as [`imgaug`](https://imgaug.readthedocs.io/) library,\n-which can be installed using the following command:\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -q -U imgaug\n-\"\"\"\n-\n-\"\"\"\n-## Data collection\n-\"\"\"\n-\n-\"\"\"\n-The StanfordExtra dataset contains 12,000 images of dogs together with keypoints and\n-segmentation maps. It is developed from the [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/).\n-It can be downloaded with the command below:\n-\"\"\"\n-\n-\"\"\"shell\n-wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n-\"\"\"\n-\n-\"\"\"\n-Annotations are provided as a single JSON file in the StanfordExtra dataset and one needs\n-to fill [this form](https://forms.gle/sRtbicgxsWvRtRmUA) to get access to it. The\n-authors explicitly instruct users not to share the JSON file, and this example respects this wish:\n-you should obtain the JSON file yourself.\n-\n-The JSON file is expected to be locally available as `stanfordextra_v12.zip`.\n-\n-After the files are downloaded, we can extract the archives.\n-\"\"\"\n-\n-\"\"\"shell\n-tar xf images.tar\n-unzip -qq ~/stanfordextra_v12.zip\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-from keras import layers\n-import keras\n-\n-from imgaug.augmentables.kps import KeypointsOnImage\n-from imgaug.augmentables.kps import Keypoint\n-import imgaug.augmenters as iaa\n-\n-from PIL import Image\n-from sklearn.model_selection import train_test_split\n-from matplotlib import pyplot as plt\n-import pandas as pd\n-import numpy as np\n-import json\n-import os\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-IMG_SIZE = 224\n-BATCH_SIZE = 64\n-EPOCHS = 5\n-NUM_KEYPOINTS = 24 * 2  # 24 pairs each having x and y coordinates\n-\n-\"\"\"\n-## Load data\n-\n-The authors also provide a metadata file that specifies additional information about the\n-keypoints, like color information, animal pose name, etc. We will load this file in a `pandas`\n-dataframe to extract information for visualization purposes.\n-\"\"\"\n-\n-IMG_DIR = \"Images\"\n-JSON = \"StanfordExtra_V12/StanfordExtra_v12.json\"\n-KEYPOINT_DEF = \"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n-\n-# Load the ground-truth annotations.\n-with open(JSON) as infile:\n-    json_data = json.load(infile)\n-\n-# Set up a dictionary, mapping all the ground-truth information\n-# with respect to the path of the image.\n-json_dict = {i[\"img_path\"]: i for i in json_data}\n-\n-\"\"\"\n-A single entry of `json_dict` looks like the following:\n-\n-```\n-'n02085782-Japanese_spaniel/n02085782_2886.jpg':\n-{'img_bbox': [205, 20, 116, 201],\n- 'img_height': 272,\n- 'img_path': 'n02085782-Japanese_spaniel/n02085782_2886.jpg',\n- 'img_width': 350,\n- 'is_multiple_dogs': False,\n- 'joints': [[108.66666666666667, 252.0, 1],\n-            [147.66666666666666, 229.0, 1],\n-            [163.5, 208.5, 1],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [54.0, 244.0, 1],\n-            [77.33333333333333, 225.33333333333334, 1],\n-            [79.0, 196.5, 1],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [150.66666666666666, 86.66666666666667, 1],\n-            [88.66666666666667, 73.0, 1],\n-            [116.0, 106.33333333333333, 1],\n-            [109.0, 123.33333333333333, 1],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0],\n-            [0, 0, 0]],\n- 'seg': ...}\n-```\n-\"\"\"\n-\n-\"\"\"\n-In this example, the keys we are interested in are:\n-\n-* `img_path`\n-* `joints`\n-\n-There are a total of 24 entries present inside `joints`. Each entry has 3 values:\n-\n-* x-coordinate\n-* y-coordinate\n-* visibility flag of the keypoints (1 indicates visibility and 0 indicates non-visibility)\n-\n-As we can see `joints` contain multiple `[0, 0, 0]` entries which denote that those\n-keypoints were not labeled. In this example, we will consider both non-visible as well as\n-unlabeled keypoints in order to allow mini-batch learning.\n-\"\"\"\n-\n-# Load the metdata definition file and preview it.\n-keypoint_def = pd.read_csv(KEYPOINT_DEF)\n-keypoint_def.head()\n-\n-# Extract the colours and labels.\n-colours = keypoint_def[\"Hex colour\"].values.tolist()\n-colours = [\"#\" + colour for colour in colours]\n-labels = keypoint_def[\"Name\"].values.tolist()\n-\n-\n-# Utility for reading an image and for getting its annotations.\n-def get_dog(name):\n-    data = json_dict[name]\n-    img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))\n-    # If the image is RGBA convert it to RGB.\n-    if img_data.shape[-1] == 4:\n-        img_data = img_data.astype(np.uint8)\n-        img_data = Image.fromarray(img_data)\n-        img_data = np.array(img_data.convert(\"RGB\"))\n-    data[\"img_data\"] = img_data\n-\n-    return data\n-\n-\n-\"\"\"\n-## Visualize data\n-\n-Now, we write a utility function to visualize the images and their keypoints.\n-\"\"\"\n-\n-\n-# Parts of this code come from here:\n-# https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb\n-def visualize_keypoints(images, keypoints):\n-    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))\n-    [ax.axis(\"off\") for ax in np.ravel(axes)]\n-\n-    for (ax_orig, ax_all), image, current_keypoint in zip(\n-        axes, images, keypoints\n-    ):\n-        ax_orig.imshow(image)\n-        ax_all.imshow(image)\n-\n-        # If the keypoints were formed by `imgaug` then the coordinates need\n-        # to be iterated differently.\n-        if isinstance(current_keypoint, KeypointsOnImage):\n-            for idx, kp in enumerate(current_keypoint.keypoints):\n-                ax_all.scatter(\n-                    [kp.x],\n-                    [kp.y],\n-                    c=colours[idx],\n-                    marker=\"x\",\n-                    s=50,\n-                    linewidths=5,\n-                )\n-        else:\n-            current_keypoint = np.array(current_keypoint)\n-            # Since the last entry is the visibility flag, we discard it.\n-            current_keypoint = current_keypoint[:, :2]\n-            for idx, (x, y) in enumerate(current_keypoint):\n-                ax_all.scatter(\n-                    [x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5\n-                )\n-\n-    plt.tight_layout(pad=2.0)\n-    plt.show()\n-\n-\n-# Select four samples randomly for visualization.\n-samples = list(json_dict.keys())\n-num_samples = 4\n-selected_samples = np.random.choice(samples, num_samples, replace=False)\n-\n-images, keypoints = [], []\n-\n-for sample in selected_samples:\n-    data = get_dog(sample)\n-    image = data[\"img_data\"]\n-    keypoint = data[\"joints\"]\n-\n-    images.append(image)\n-    keypoints.append(keypoint)\n-\n-visualize_keypoints(images, keypoints)\n-\n-\"\"\"\n-The plots show that we have images of non-uniform sizes, which is expected in most\n-real-world scenarios. However, if we resize these images to have a uniform shape (for\n-instance (224 x 224)) their ground-truth annotations will also be affected. The same\n-applies if we apply any geometric transformation (horizontal flip, for e.g.) to an image.\n-Fortunately, `imgaug` provides utilities that can handle this issue.\n-In the next section, we will write a data generator inheriting the\n-[`keras.utils.Sequence`](https://keras.io/api/utils/python_utils/#sequence-class) class\n-that applies data augmentation on batches of data using `imgaug`.\n-\"\"\"\n-\n-\"\"\"\n-## Prepare data generator\n-\"\"\"\n-\n-\n-class KeyPointsDataset(keras.utils.PyDataset):\n-    def __init__(\n-        self, image_keys, aug, batch_size=BATCH_SIZE, train=True, **kwargs\n-    ):\n-        super().__init__(**kwargs)\n-        self.image_keys = image_keys\n-        self.aug = aug\n-        self.batch_size = batch_size\n-        self.train = train\n-        self.on_epoch_end()\n-\n-    def __len__(self):\n-        return len(self.image_keys) // self.batch_size\n-\n-    def on_epoch_end(self):\n-        self.indexes = np.arange(len(self.image_keys))\n-        if self.train:\n-            np.random.shuffle(self.indexes)\n-\n-    def __getitem__(self, index):\n-        indexes = self.indexes[\n-            index * self.batch_size : (index + 1) * self.batch_size\n-        ]\n-        image_keys_temp = [self.image_keys[k] for k in indexes]\n-        (images, keypoints) = self.__data_generation(image_keys_temp)\n-\n-        return (images, keypoints)\n-\n-    def __data_generation(self, image_keys_temp):\n-        batch_images = np.empty(\n-            (self.batch_size, IMG_SIZE, IMG_SIZE, 3), dtype=\"int\"\n-        )\n-        batch_keypoints = np.empty(\n-            (self.batch_size, 1, 1, NUM_KEYPOINTS), dtype=\"float32\"\n-        )\n-\n-        for i, key in enumerate(image_keys_temp):\n-            data = get_dog(key)\n-            current_keypoint = np.array(data[\"joints\"])[:, :2]\n-            kps = []\n-\n-            # To apply our data augmentation pipeline, we first need to\n-            # form Keypoint objects with the original coordinates.\n-            for j in range(0, len(current_keypoint)):\n-                kps.append(\n-                    Keypoint(x=current_keypoint[j][0], y=current_keypoint[j][1])\n-                )\n-\n-            # We then project the original image and its keypoint coordinates.\n-            current_image = data[\"img_data\"]\n-            kps_obj = KeypointsOnImage(kps, shape=current_image.shape)\n-\n-            # Apply the augmentation pipeline.\n-            (new_image, new_kps_obj) = self.aug(\n-                image=current_image, keypoints=kps_obj\n-            )\n-            batch_images[i,] = new_image\n-\n-            # Parse the coordinates from the new keypoint object.\n-            kp_temp = []\n-            for keypoint in new_kps_obj:\n-                kp_temp.append(np.nan_to_num(keypoint.x))\n-                kp_temp.append(np.nan_to_num(keypoint.y))\n-\n-            # More on why this reshaping later.\n-            batch_keypoints[i,] = np.array(kp_temp).reshape(1, 1, 24 * 2)\n-\n-        # Scale the coordinates to [0, 1] range.\n-        batch_keypoints = batch_keypoints / IMG_SIZE\n-\n-        return (batch_images, batch_keypoints)\n-\n-\n-\"\"\"\n-To know more about how to operate with keypoints in `imgaug` check out\n-[this document](https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html).\n-\"\"\"\n-\n-\"\"\"\n-## Define augmentation transforms\n-\"\"\"\n-\n-train_aug = iaa.Sequential(\n-    [\n-        iaa.Resize(IMG_SIZE, interpolation=\"linear\"),\n-        iaa.Fliplr(0.3),\n-        # `Sometimes()` applies a function randomly to the inputs with\n-        # a given probability (0.3, in this case).\n-        iaa.Sometimes(0.3, iaa.Affine(rotate=10, scale=(0.5, 0.7))),\n-    ]\n-)\n-\n-test_aug = iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation=\"linear\")])\n-\n-\"\"\"\n-## Create training and validation splits\n-\"\"\"\n-\n-np.random.shuffle(samples)\n-train_keys, validation_keys = (\n-    samples[int(len(samples) * 0.15) :],\n-    samples[: int(len(samples) * 0.15)],\n-)\n-\n-\n-\"\"\"\n-## Data generator investigation\n-\"\"\"\n-\n-train_dataset = KeyPointsDataset(\n-    train_keys, train_aug, workers=2, use_multiprocessing=True\n-)\n-validation_dataset = KeyPointsDataset(\n-    validation_keys, test_aug, train=False, workers=2, use_multiprocessing=True\n-)\n-\n-print(f\"Total batches in training set: {len(train_dataset)}\")\n-print(f\"Total batches in validation set: {len(validation_dataset)}\")\n-\n-sample_images, sample_keypoints = next(iter(train_dataset))\n-assert sample_keypoints.max() == 1.0\n-assert sample_keypoints.min() == 0.0\n-\n-sample_keypoints = sample_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE\n-visualize_keypoints(sample_images[:4], sample_keypoints)\n-\n-\"\"\"\n-## Model building\n-\n-The [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (on which\n-the StanfordExtra dataset is based) was built using the [ImageNet-1k dataset](http://image-net.org/).\n-So, it is likely that the models pretrained on the ImageNet-1k dataset would be useful\n-for this task. We will use a MobileNetV2 pre-trained on this dataset as a backbone to\n-extract meaningful features from the images and then pass those to a custom regression\n-head for predicting coordinates.\n-\"\"\"\n-\n-\n-def get_model():\n-    # Load the pre-trained weights of MobileNetV2 and freeze the weights\n-    backbone = keras.applications.MobileNetV2(\n-        weights=\"imagenet\",\n-        include_top=False,\n-        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n-    )\n-    backbone.trainable = False\n-\n-    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n-    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n-    x = backbone(x)\n-    x = layers.Dropout(0.3)(x)\n-    x = layers.SeparableConv2D(\n-        NUM_KEYPOINTS, kernel_size=5, strides=1, activation=\"relu\"\n-    )(x)\n-    outputs = layers.SeparableConv2D(\n-        NUM_KEYPOINTS, kernel_size=3, strides=1, activation=\"sigmoid\"\n-    )(x)\n-\n-    return keras.Model(inputs, outputs, name=\"keypoint_detector\")\n-\n-\n-\"\"\"\n-Our custom network is fully-convolutional which makes it more parameter-friendly than the\n-same version of the network having fully-connected dense layers.\n-\"\"\"\n-\n-get_model().summary()\n-\n-\"\"\"\n-Notice the output shape of the network: `(None, 1, 1, 48)`. This is why we have reshaped\n-the coordinates as: `batch_keypoints[i, :] = np.array(kp_temp).reshape(1, 1, 24 * 2)`.\n-\"\"\"\n-\n-\"\"\"\n-## Model compilation and training\n-\n-For this example, we will train the network only for five epochs.\n-\"\"\"\n-\n-model = get_model()\n-model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-4))\n-model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS)\n-\n-\"\"\"\n-## Make predictions and visualize them\n-\"\"\"\n-\n-sample_val_images, sample_val_keypoints = next(iter(validation_dataset))\n-sample_val_images = sample_val_images[:4]\n-sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE\n-predictions = model.predict(sample_val_images).reshape(-1, 24, 2) * IMG_SIZE\n-\n-# Ground-truth\n-visualize_keypoints(sample_val_images, sample_val_keypoints)\n-\n-# Predictions\n-visualize_keypoints(sample_val_images, predictions)\n-\n-\"\"\"\n-Predictions will likely improve with more training.\n-\"\"\"\n-\n-\"\"\"\n-## Going further\n-\n-* Try using other augmentation transforms from `imgaug` to investigate how that changes\n-the results.\n-* Here, we transferred the features from the pre-trained network linearly that is we did\n-not [fine-tune](https://keras.io/guides/transfer_learning/) it. You are encouraged to fine-tune it on this task and see if that\n-improves the performance. You can also try different architectures and see how they\n-affect the final performance.\n-\"\"\"\n\n@@ -1,246 +0,0 @@\n-\"\"\"\n-Title: Knowledge Distillation\n-Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n-Converted to Keras 3 by: [Md Awsafur Rahman](https://awsaf49.github.io)\n-Date created: 2020/09/01\n-Last modified: 2020/09/01\n-Description: Implementation of classical Knowledge Distillation.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction to Knowledge Distillation\n-\n-Knowledge Distillation is a procedure for model\n-compression, in which a small (student) model is trained to match a large pre-trained\n-(teacher) model. Knowledge is transferred from the teacher model to the student\n-by minimizing a loss function, aimed at matching softened teacher logits as well as\n-ground-truth labels.\n-\n-The logits are softened by applying a \"temperature\" scaling function in the softmax,\n-effectively smoothing out the probability distribution and revealing\n-inter-class relationships learned by the teacher.\n-\n-**Reference:**\n-\n-- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-import tensorflow as tf\n-import numpy as np\n-\n-\"\"\"\n-## Construct `Distiller()` class\n-\n-The custom `Distiller()` class, overrides the `Model` methods `compile`, `compute_loss`,\n-and `call`. In order to use the distiller, we need:\n-\n-- A trained teacher model\n-- A student model to train\n-- A student loss function on the difference between student predictions and ground-truth\n-- A distillation loss function, along with a `temperature`, on the difference between the\n-soft student predictions and the soft teacher labels\n-- An `alpha` factor to weight the student and distillation loss\n-- An optimizer for the student and (optional) metrics to evaluate performance\n-\n-In the `compute_loss` method, we perform a forward pass of both the teacher and student,\n-calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha`\n-and `1 - alpha`, respectively. Note: only the student weights are updated.\n-\"\"\"\n-\n-\n-class Distiller(keras.Model):\n-    def __init__(self, student, teacher):\n-        super().__init__()\n-        self.teacher = teacher\n-        self.student = student\n-\n-    def compile(\n-        self,\n-        optimizer,\n-        metrics,\n-        student_loss_fn,\n-        distillation_loss_fn,\n-        alpha=0.1,\n-        temperature=3,\n-    ):\n-        \"\"\"Configure the distiller.\n-\n-        Args:\n-            optimizer: Keras optimizer for the student weights\n-            metrics: Keras metrics for evaluation\n-            student_loss_fn: Loss function of difference between student\n-                predictions and ground-truth\n-            distillation_loss_fn: Loss function of difference between soft\n-                student predictions and soft teacher predictions\n-            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n-            temperature: Temperature for softening probability distributions.\n-                Larger temperature gives softer distributions.\n-        \"\"\"\n-        super().compile(optimizer=optimizer, metrics=metrics)\n-        self.student_loss_fn = student_loss_fn\n-        self.distillation_loss_fn = distillation_loss_fn\n-        self.alpha = alpha\n-        self.temperature = temperature\n-\n-    def compute_loss(\n-        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n-    ):\n-        teacher_pred = self.teacher(x, training=False)\n-        student_loss = self.student_loss_fn(y, y_pred)\n-\n-        distillation_loss = self.distillation_loss_fn(\n-            ops.softmax(teacher_pred / self.temperature, axis=1),\n-            ops.softmax(y_pred / self.temperature, axis=1),\n-        ) * (self.temperature**2)\n-\n-        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n-        return loss\n-\n-    def call(self, x):\n-        return self.student(x)\n-\n-\n-\"\"\"\n-## Create student and teacher models\n-\n-Initialy, we create a teacher model and a smaller student model. Both models are\n-convolutional neural networks and created using `Sequential()`,\n-but could be any Keras model.\n-\"\"\"\n-\n-# Create the teacher\n-teacher = keras.Sequential(\n-    [\n-        keras.Input(shape=(28, 28, 1)),\n-        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(negative_slope=0.2),\n-        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n-        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.Flatten(),\n-        layers.Dense(10),\n-    ],\n-    name=\"teacher\",\n-)\n-\n-# Create the student\n-student = keras.Sequential(\n-    [\n-        keras.Input(shape=(28, 28, 1)),\n-        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(negative_slope=0.2),\n-        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n-        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.Flatten(),\n-        layers.Dense(10),\n-    ],\n-    name=\"student\",\n-)\n-\n-# Clone student for later comparison\n-student_scratch = keras.models.clone_model(student)\n-\n-\"\"\"\n-## Prepare the dataset\n-\n-The dataset used for training the teacher and distilling the teacher is\n-[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for\n-any other\n-dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n-of models. Both the student and teacher are trained on the training set and evaluated on\n-the test set.\n-\"\"\"\n-\n-# Prepare the train and test dataset.\n-batch_size = 64\n-(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n-\n-# Normalize data\n-x_train = x_train.astype(\"float32\") / 255.0\n-x_train = np.reshape(x_train, (-1, 28, 28, 1))\n-\n-x_test = x_test.astype(\"float32\") / 255.0\n-x_test = np.reshape(x_test, (-1, 28, 28, 1))\n-\n-\n-\"\"\"\n-## Train the teacher\n-\n-In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\n-by training the teacher model on the training set in the usual way.\n-\"\"\"\n-\n-# Train teacher as usual\n-teacher.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-)\n-\n-# Train and evaluate teacher on data.\n-teacher.fit(x_train, y_train, epochs=5)\n-teacher.evaluate(x_test, y_test)\n-\n-\"\"\"\n-## Distill teacher to student\n-\n-We have already trained the teacher model, and we only need to initialize a\n-`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n-hyperparameters and optimizer, and distill the teacher to the student.\n-\"\"\"\n-\n-# Initialize and compile distiller\n-distiller = Distiller(student=student, teacher=teacher)\n-distiller.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(\n-        from_logits=True\n-    ),\n-    distillation_loss_fn=keras.losses.KLDivergence(),\n-    alpha=0.1,\n-    temperature=10,\n-)\n-\n-# Distill teacher to student\n-distiller.fit(x_train, y_train, epochs=3)\n-\n-# Evaluate student on test dataset\n-distiller.evaluate(x_test, y_test)\n-\n-\"\"\"\n-## Train student from scratch for comparison\n-\n-We can also train an equivalent student model from scratch without the teacher, in order\n-to evaluate the performance gain obtained by knowledge distillation.\n-\"\"\"\n-\n-# Train student as doen usually\n-student_scratch.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-)\n-\n-# Train and evaluate student trained from scratch.\n-student_scratch.fit(x_train, y_train, epochs=3)\n-student_scratch.evaluate(x_test, y_test)\n-\n-\"\"\"\n-If the teacher is trained for 5 full epochs and the student is distilled on this teacher\n-for 3 full epochs, you should in this example experience a performance boost compared to\n-training the same student model from scratch, and even compared to the teacher itself.\n-You should expect the teacher to have accuracy around 97.6%, the student trained from\n-scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove\n-or try out different seeds to use different weight initializations.\n-\"\"\"\n\n@@ -1,392 +0,0 @@\n-\"\"\"\n-Title: A walk through latent space with Stable Diffusion\n-Authors: Ian Stenbit, [fchollet](https://twitter.com/fchollet), [lukewood](https://twitter.com/luke_wood_ml)\n-Date created: 2022/09/28\n-Last modified: 2022/09/28\n-Description: Explore the latent manifold of Stable Diffusion.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Overview\n-\n-Generative image models learn a \"latent manifold\" of the visual world:\n-a low-dimensional vector space where each point maps to an image.\n-Going from such a point on the manifold back to a displayable image\n-is called \"decoding\" -- in the Stable Diffusion model, this is handled by\n-the \"decoder\" model.\n-\n-![The Stable Diffusion architecture](https://i.imgur.com/2uC8rYJ.png)\n-\n-This latent manifold of images is continuous and interpolative, meaning that:\n-\n-1. Moving a little on the manifold only changes the corresponding image a little (continuity).\n-2. For any two points A and B on the manifold (i.e. any two images), it is possible\n-to move from A to B via a path where each intermediate point is also on the manifold (i.e.\n-is also a valid image). Intermediate points would be called \"interpolations\" between\n-the two starting images.\n-\n-Stable Diffusion isn't just an image model, though, it's also a natural language model.\n-It has two latent spaces: the image representation space learned by the\n-encoder used during training, and the prompt latent space\n-which is learned using a combination of pretraining and training-time\n-fine-tuning.\n-\n-_Latent space walking_, or _latent space exploration_, is the process of\n-sampling a point in latent space and incrementally changing the latent\n-representation. Its most common application is generating animations\n-where each sampled point is fed to the decoder and is stored as a\n-frame in the final animation.\n-For high-quality latent representations, this produces coherent-looking\n-animations. These animations can provide insight into the feature map of the\n-latent space, and can ultimately lead to improvements in the training\n-process. One such GIF is displayed below:\n-\n-![Panda to Plane](/img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif)\n-\n-In this guide, we will show how to take advantage of the Stable Diffusion API\n-in KerasCV to perform prompt interpolation and circular walks through\n-Stable Diffusion's visual latent manifold, as well as through\n-the text encoder's latent manifold.\n-\n-This guide assumes the reader has a\n-high-level understanding of Stable Diffusion.\n-If you haven't already, you should start\n-by reading the [Stable Diffusion Tutorial](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/).\n-\n-To start, we import KerasCV and load up a Stable Diffusion model using the\n-optimizations discussed in the tutorial\n-[Generate images with Stable Diffusion](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/).\n-Note that if you are running with a M1 Mac GPU you should not enable mixed precision.\n-\"\"\"\n-\n-\"\"\"shell\n-pip install keras-cv --upgrade --quiet\n-\"\"\"\n-\n-import keras_cv\n-import keras\n-import matplotlib.pyplot as plt\n-from keras import ops\n-import numpy as np\n-import math\n-from PIL import Image\n-\n-# Enable mixed precision\n-# (only do this if you have a recent NVIDIA GPU)\n-keras.mixed_precision.set_global_policy(\"mixed_float16\")\n-\n-# Instantiate the Stable Diffusion model\n-model = keras_cv.models.StableDiffusion(jit_compile=True)\n-\n-\"\"\"\n-## Interpolating between text prompts\n-\n-In Stable Diffusion, a text prompt is first encoded into a vector,\n-and that encoding is used to guide the diffusion process.\n-The latent encoding vector has shape\n-77x768 (that's huge!), and when we give Stable Diffusion a text prompt, we're\n-generating images from just one such point on the latent manifold.\n-\n-To explore more of this manifold, we can interpolate between two text encodings\n-and generate images at those interpolated points:\n-\"\"\"\n-\n-prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n-prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n-interpolation_steps = 5\n-\n-encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n-encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n-\n-interpolated_encodings = ops.linspace(\n-    encoding_1, encoding_2, interpolation_steps\n-)\n-\n-# Show the size of the latent manifold\n-print(f\"Encoding shape: {encoding_1.shape}\")\n-\n-\"\"\"\n-Once we've interpolated the encodings, we can generate images from each point.\n-Note that in order to maintain some stability between the resulting images we\n-keep the diffusion noise constant between images.\n-\"\"\"\n-\n-seed = 12345\n-noise = keras.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n-\n-images = model.generate_image(\n-    interpolated_encodings,\n-    batch_size=interpolation_steps,\n-    diffusion_noise=noise,\n-)\n-\n-\"\"\"\n-Now that we've generated some interpolated images, let's take a look at them!\n-\n-Throughout this tutorial, we're going to export sequences of images as gifs so\n-that they can be easily viewed with some temporal context. For sequences of\n-images where the first and last images don't match conceptually, we rubber-band\n-the gif.\n-\n-If you're running in Colab, you can view your own GIFs by running:\n-\n-```\n-from IPython.display import Image as IImage\n-IImage(\"doggo-and-fruit-5.gif\")\n-```\n-\"\"\"\n-\n-\n-def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n-    if rubber_band:\n-        images += images[2:-1][::-1]\n-    images[0].save(\n-        filename,\n-        save_all=True,\n-        append_images=images[1:],\n-        duration=1000 // frames_per_second,\n-        loop=0,\n-    )\n-\n-\n-export_as_gif(\n-    \"doggo-and-fruit-5.gif\",\n-    [Image.fromarray(img) for img in images],\n-    frames_per_second=2,\n-    rubber_band=True,\n-)\n-\n-\"\"\"\n-![Dog to Fruit 5](https://i.imgur.com/4ZCxZY4.gif)\n-\n-The results may seem surprising. Generally, interpolating between prompts\n-produces coherent looking images, and often demonstrates a progressive concept\n-shift between the contents of the two prompts. This is indicative of a high\n-quality representation space, that closely mirrors the natural structure\n-of the visual world.\n-\n-To best visualize this, we should do a much more fine-grained interpolation,\n-using hundreds of steps. In order to keep batch size small (so that we don't\n-OOM our GPU), this requires manually batching our interpolated\n-encodings.\n-\"\"\"\n-\n-interpolation_steps = 150\n-batch_size = 3\n-batches = interpolation_steps // batch_size\n-\n-interpolated_encodings = ops.linspace(\n-    encoding_1, encoding_2, interpolation_steps\n-)\n-batched_encodings = ops.split(interpolated_encodings, batches)\n-\n-images = []\n-for batch in range(batches):\n-    images += [\n-        Image.fromarray(img)\n-        for img in model.generate_image(\n-            batched_encodings[batch],\n-            batch_size=batch_size,\n-            num_steps=25,\n-            diffusion_noise=noise,\n-        )\n-    ]\n-\n-export_as_gif(\"doggo-and-fruit-150.gif\", images, rubber_band=True)\n-\n-\"\"\"\n-![Dog to Fruit 150](/img/examples/generative/random_walks_with_stable_diffusion/dog2fruit150.gif)\n-\n-The resulting gif shows a much clearer and more coherent shift between the two\n-prompts. Try out some prompts of your own and experiment!\n-\n-We can even extend this concept for more than one image. For example, we can\n-interpolate between four prompts:\n-\"\"\"\n-\n-prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n-prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n-prompt_3 = \"The eiffel tower in the style of starry night\"\n-prompt_4 = \"An architectural sketch of a skyscraper\"\n-\n-interpolation_steps = 6\n-batch_size = 3\n-batches = (interpolation_steps**2) // batch_size\n-\n-encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n-encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n-encoding_3 = ops.squeeze(model.encode_text(prompt_3))\n-encoding_4 = ops.squeeze(model.encode_text(prompt_4))\n-\n-interpolated_encodings = ops.linspace(\n-    ops.linspace(encoding_1, encoding_2, interpolation_steps),\n-    ops.linspace(encoding_3, encoding_4, interpolation_steps),\n-    interpolation_steps,\n-)\n-interpolated_encodings = ops.reshape(\n-    interpolated_encodings, (interpolation_steps**2, 77, 768)\n-)\n-batched_encodings = ops.split(interpolated_encodings, batches)\n-\n-images = []\n-for batch in range(batches):\n-    images.append(\n-        model.generate_image(\n-            batched_encodings[batch],\n-            batch_size=batch_size,\n-            diffusion_noise=noise,\n-        )\n-    )\n-\n-\n-def plot_grid(\n-    images,\n-    path,\n-    grid_size,\n-    scale=2,\n-):\n-    fig = plt.figure(figsize=(grid_size * scale, grid_size * scale))\n-    fig.tight_layout()\n-    plt.subplots_adjust(wspace=0, hspace=0)\n-    plt.margins(x=0, y=0)\n-    plt.axis(\"off\")\n-    images = images.astype(int)\n-    for row in range(grid_size):\n-        for col in range(grid_size):\n-            index = row * grid_size + col\n-            plt.subplot(grid_size, grid_size, index + 1)\n-            plt.imshow(images[index].astype(\"uint8\"))\n-            plt.axis(\"off\")\n-            plt.margins(x=0, y=0)\n-    plt.savefig(\n-        fname=path,\n-        pad_inches=0,\n-        bbox_inches=\"tight\",\n-        transparent=False,\n-        dpi=60,\n-    )\n-\n-\n-images = np.concatenate(images)\n-plot_grid(images, \"4-way-interpolation.jpg\", interpolation_steps)\n-\n-\"\"\"\n-We can also interpolate while allowing diffusion noise to vary by dropping\n-the `diffusion_noise` parameter:\n-\"\"\"\n-\n-images = []\n-for batch in range(batches):\n-    images.append(\n-        model.generate_image(batched_encodings[batch], batch_size=batch_size)\n-    )\n-\n-images = np.concatenate(images)\n-plot_grid(images, \"4-way-interpolation-varying-noise.jpg\", interpolation_steps)\n-\n-\"\"\"\n-Next up -- let's go for some walks!\n-\n-## A walk around a text prompt\n-\n-Our next experiment will be to go for a walk around the latent manifold\n-starting from a point produced by a particular prompt.\n-\"\"\"\n-\n-walk_steps = 150\n-batch_size = 3\n-batches = walk_steps // batch_size\n-step_size = 0.005\n-\n-encoding = ops.squeeze(\n-    model.encode_text(\"The Eiffel Tower in the style of starry night\")\n-)\n-# Note that (77, 768) is the shape of the text encoding.\n-delta = ops.ones_like(encoding) * step_size\n-\n-walked_encodings = []\n-for step_index in range(walk_steps):\n-    walked_encodings.append(encoding)\n-    encoding += delta\n-walked_encodings = ops.stack(walked_encodings)\n-batched_encodings = ops.split(walked_encodings, batches)\n-\n-images = []\n-for batch in range(batches):\n-    images += [\n-        Image.fromarray(img)\n-        for img in model.generate_image(\n-            batched_encodings[batch],\n-            batch_size=batch_size,\n-            num_steps=25,\n-            diffusion_noise=noise,\n-        )\n-    ]\n-\n-export_as_gif(\"eiffel-tower-starry-night.gif\", images, rubber_band=True)\n-\n-\"\"\"\n-![Eiffel tower walk gif](https://i.imgur.com/9MMYtal.gif)\n-\n-Perhaps unsurprisingly, walking too far from the encoder's latent manifold\n-produces images that look incoherent. Try it for yourself by setting\n-your own prompt, and adjusting `step_size` to increase or decrease the magnitude\n-of the walk. Note that when the magnitude of the walk gets large, the walk often\n-leads into areas which produce extremely noisy images.\n-\n-## A circular walk through the diffusion noise space for a single prompt\n-\n-Our final experiment is to stick to one prompt and explore the variety of images\n-that the diffusion model can produce from that prompt. We do this by controlling\n-the noise that is used to seed the diffusion process.\n-\n-We create two noise components, `x` and `y`, and do a walk from 0 to 2π, summing\n-the cosine of our `x` component and the sin of our `y` component to produce noise.\n-Using this approach, the end of our walk arrives at the same noise inputs where\n-we began our walk, so we get a \"loopable\" result!\n-\"\"\"\n-\n-prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n-encoding = ops.squeeze(model.encode_text(prompt))\n-walk_steps = 150\n-batch_size = 3\n-batches = walk_steps // batch_size\n-\n-walk_noise_x = keras.random.normal(noise.shape, dtype=\"float64\")\n-walk_noise_y = keras.random.normal(noise.shape, dtype=\"float64\")\n-\n-walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi)\n-walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi)\n-noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0)\n-noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0)\n-noise = ops.add(noise_x, noise_y)\n-batched_noise = ops.split(noise, batches)\n-\n-images = []\n-for batch in range(batches):\n-    images += [\n-        Image.fromarray(img)\n-        for img in model.generate_image(\n-            encoding,\n-            batch_size=batch_size,\n-            num_steps=25,\n-            diffusion_noise=batched_noise[batch],\n-        )\n-    ]\n-\n-export_as_gif(\"cows.gif\", images)\n-\n-\"\"\"\n-![Happy Cows](/img/examples/generative/random_walks_with_stable_diffusion/happycows.gif)\n-\n-Experiment with your own prompts and with different values of\n-`unconditional_guidance_scale`!\n-\n-## Conclusion\n-\n-Stable Diffusion offers a lot more than just single text-to-image generation.\n-Exploring the latent manifold of the text encoder and the noise space of the\n-diffusion model are two fun ways to experience the power of this model, and\n-KerasCV makes it easy!\n-\"\"\"\n\n@@ -1,335 +0,0 @@\n-\"\"\"\n-Title: Learning to Resize in Computer Vision\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/04/30\n-Last modified: 2023/07/26\n-Description: How to optimally learn representations of images for a given resolution.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-It is a common belief that if we constrain vision models to perceive things as humans do,\n-their performance can be improved. For example, in [this work](https://arxiv.org/abs/1811.12231),\n-Geirhos et al. showed that the vision models pre-trained on the ImageNet-1k dataset are\n-biased towards texture, whereas human beings mostly use the shape descriptor to develop a\n-common perception. But does this belief always apply, especially when it comes to improving\n-the performance of vision models?\n-\n-It turns out it may not always be the case. When training vision models, it is common to\n-resize images to a lower dimension ((224 x 224), (299 x 299), etc.) to allow mini-batch\n-learning and also to keep up the compute limitations.  We generally make use of image\n-resizing methods like **bilinear interpolation** for this step and the resized images do\n-not lose much of their perceptual character to the human eyes. In\n-[Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950v1), Talebi et al. show\n-that if we try to optimize the perceptual quality of the images for the vision models\n-rather than the human eyes, their performance can further be improved. They investigate\n-the following question:\n-\n-**For a given image resolution and a model, how to best resize the given images?**\n-\n-As shown in the paper, this idea helps to consistently improve the performance of the\n-common vision models (pre-trained on ImageNet-1k) like DenseNet-121, ResNet-50,\n-MobileNetV2, and EfficientNets. In this example, we will implement the learnable image\n-resizing module as proposed in the paper and demonstrate that on the\n-[Cats and Dogs dataset](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n-using the [DenseNet-121](https://arxiv.org/abs/1608.06993) architecture.\n-\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-from keras import layers\n-import keras\n-from keras import ops\n-\n-from tensorflow import data as tf_data\n-from tensorflow import image as tf_image\n-from tensorflow import one_hot as tf_one_hot\n-\n-import tensorflow_datasets as tfds\n-\n-tfds.disable_progress_bar()\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-\"\"\"\n-In order to facilitate mini-batch learning, we need to have a fixed shape for the images\n-inside a given batch. This is why an initial resizing is required. We first resize all\n-the images to (300 x 300) shape and then learn their optimal representation for the\n-(150 x 150) resolution.\n-\"\"\"\n-\n-INP_SIZE = (300, 300)\n-TARGET_SIZE = (150, 150)\n-INTERPOLATION = \"bilinear\"\n-\n-AUTO = tf_data.AUTOTUNE\n-BATCH_SIZE = 50\n-EPOCHS = 5\n-\n-\"\"\"\n-In this example, we will use the bilinear interpolation but the learnable image resizer\n-module is not dependent on any specific interpolation method. We can also use others,\n-such as bicubic.\n-\"\"\"\n-\n-\"\"\"\n-## Load and prepare the dataset\n-\n-For this example, we will only use 40% of the total training dataset.\n-\"\"\"\n-\n-train_ds, validation_ds = tfds.load(\n-    \"cats_vs_dogs\",\n-    # Reserve 10% for validation\n-    split=[\"train[:40%]\", \"train[40%:50%]\"],\n-    as_supervised=True,\n-)\n-\n-\n-def preprocess_dataset(image, label):\n-    image = tf_image.resize(image, (INP_SIZE[0], INP_SIZE[1]))\n-    label = tf_one_hot(label, depth=2)\n-    return (image, label)\n-\n-\n-train_ds = (\n-    train_ds.shuffle(BATCH_SIZE * 100)\n-    .map(preprocess_dataset, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-validation_ds = (\n-    validation_ds.map(preprocess_dataset, num_parallel_calls=AUTO)\n-    .batch(BATCH_SIZE)\n-    .prefetch(AUTO)\n-)\n-\n-\"\"\"\n-## Define the learnable resizer utilities\n-\n-The figure below (courtesy: [Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950v1))\n-presents the structure of the learnable resizing module:\n-\n-![](https://i.ibb.co/gJYtSs0/image.png)\n-\"\"\"\n-\n-\n-def conv_block(\n-    x, filters, kernel_size, strides, activation=layers.LeakyReLU(0.2)\n-):\n-    x = layers.Conv2D(\n-        filters, kernel_size, strides, padding=\"same\", use_bias=False\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-    if activation:\n-        x = activation(x)\n-    return x\n-\n-\n-def res_block(x):\n-    inputs = x\n-    x = conv_block(x, 16, 3, 1)\n-    x = conv_block(x, 16, 3, 1, activation=None)\n-    return layers.Add()([inputs, x])\n-\n-\n-def get_learnable_resizer(\n-    filters=16, num_res_blocks=1, interpolation=INTERPOLATION\n-):\n-    inputs = layers.Input(shape=[None, None, 3])\n-\n-    # First, perform naive resizing.\n-    naive_resize = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(\n-        inputs\n-    )\n-\n-    # First convolution block without batch normalization.\n-    x = layers.Conv2D(\n-        filters=filters, kernel_size=7, strides=1, padding=\"same\"\n-    )(inputs)\n-    x = layers.LeakyReLU(0.2)(x)\n-\n-    # Second convolution block with batch normalization.\n-    x = layers.Conv2D(\n-        filters=filters, kernel_size=1, strides=1, padding=\"same\"\n-    )(x)\n-    x = layers.LeakyReLU(0.2)(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    # Intermediate resizing as a bottleneck.\n-    bottleneck = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(x)\n-\n-    # Residual passes.\n-    for _ in range(num_res_blocks):\n-        x = res_block(bottleneck)\n-\n-    # Projection.\n-    x = layers.Conv2D(\n-        filters=filters,\n-        kernel_size=3,\n-        strides=1,\n-        padding=\"same\",\n-        use_bias=False,\n-    )(x)\n-    x = layers.BatchNormalization()(x)\n-\n-    # Skip connection.\n-    x = layers.Add()([bottleneck, x])\n-\n-    # Final resized image.\n-    x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding=\"same\")(x)\n-    final_resize = layers.Add()([naive_resize, x])\n-\n-    return keras.Model(inputs, final_resize, name=\"learnable_resizer\")\n-\n-\n-learnable_resizer = get_learnable_resizer()\n-\n-\"\"\"\n-## Visualize the outputs of the learnable resizing module\n-\n-Here, we visualize how the resized images would look like after being passed through the\n-random weights of the resizer.\n-\"\"\"\n-\n-sample_images, _ = next(iter(train_ds))\n-\n-get_np = lambda image: ops.convert_to_numpy(\n-    ops.squeeze(image)\n-)  # Helper to convert image from any backend to numpy\n-\n-plt.figure(figsize=(16, 10))\n-for i, image in enumerate(sample_images[:6]):\n-    image = image / 255\n-\n-    ax = plt.subplot(3, 4, 2 * i + 1)\n-    plt.title(\"Input Image\")\n-    plt.imshow(image.numpy().squeeze())\n-    plt.axis(\"off\")\n-\n-    ax = plt.subplot(3, 4, 2 * i + 2)\n-    resized_image = learnable_resizer(image[None, ...])\n-    plt.title(\"Resized Image\")\n-    plt.imshow(get_np(resized_image))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Model building utility\n-\"\"\"\n-\n-\n-def get_model():\n-    backbone = keras.applications.DenseNet121(\n-        weights=None,\n-        include_top=True,\n-        classes=2,\n-        input_shape=((TARGET_SIZE[0], TARGET_SIZE[1], 3)),\n-    )\n-    backbone.trainable = True\n-\n-    inputs = layers.Input((INP_SIZE[0], INP_SIZE[1], 3))\n-    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n-    x = learnable_resizer(x)\n-    outputs = backbone(x)\n-\n-    return keras.Model(inputs, outputs)\n-\n-\n-\"\"\"\n-The structure of the learnable image resizer module allows for flexible integrations with\n-different vision models.\n-\"\"\"\n-\n-\"\"\"\n-## Compile and train our model with learnable resizer\n-\"\"\"\n-\n-model = get_model()\n-model.compile(\n-    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n-    optimizer=\"sgd\",\n-    metrics=[\"accuracy\"],\n-)\n-model.fit(train_ds, validation_data=validation_ds, epochs=EPOCHS)\n-\n-\"\"\"\n-## Visualize the outputs of the trained visualizer\n-\"\"\"\n-\n-plt.figure(figsize=(16, 10))\n-for i, image in enumerate(sample_images[:6]):\n-    image = image / 255\n-\n-    ax = plt.subplot(3, 4, 2 * i + 1)\n-    plt.title(\"Input Image\")\n-    plt.imshow(image.numpy().squeeze())\n-    plt.axis(\"off\")\n-\n-    ax = plt.subplot(3, 4, 2 * i + 2)\n-    resized_image = learnable_resizer(image[None, ...])\n-    plt.title(\"Resized Image\")\n-    plt.imshow(get_np(resized_image) / 10)\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-The plot shows that the visuals of the images have improved with training. The following\n-table shows the benefits of using the resizing module in comparison to using the bilinear\n-interpolation:\n-\n-|           Model           \t| Number of  parameters (Million) \t| Top-1 accuracy \t|\n-|:-------------------------:\t|:-------------------------------:\t|:--------------:\t|\n-|   With the learnable resizer  \t|             7.051717            \t|      67.67%     \t|\n-| Without the learnable resizer \t|             7.039554            \t|      60.19%      \t|\n-\n-For more details, you can check out [this repository](https://github.com/sayakpaul/Learnable-Image-Resizing).\n-Note the above-reported models were trained for 10 epochs on 90% of the training set of\n-Cats and Dogs unlike this example. Also, note that the increase in the number of\n-parameters due to the resizing module is very negligible. To ensure that the improvement\n-in the performance is not due to stochasticity, the models were trained using the same\n-initial random weights.\n-\n-Now, a question worth asking here is -  _isn't the improved accuracy simply a consequence\n-of adding more layers (the resizer is a mini network after all) to the model, compared to\n-the baseline?_\n-\n-To show that it is not the case, the authors conduct the following experiment:\n-\n-* Take a pre-trained model trained some size, say (224 x 224).\n-\n-* Now, first, use it to infer predictions on images resized to a lower resolution. Record\n-the performance.\n-\n-* For the second experiment, plug in the resizer module at the top of the pre-trained\n-model and warm-start the training. Record the performance.\n-\n-Now, the authors argue that using the second option is better because it helps the model\n-learn how to adjust the representations better with respect to the given resolution.\n-Since the results purely are empirical, a few more experiments such as analyzing the\n-cross-channel interaction would have been even better. It is worth noting that elements\n-like [Squeeze and Excitation (SE) blocks](https://arxiv.org/abs/1709.01507), [Global Context (GC) blocks](https://arxiv.org/pdf/1904.11492) also add a few\n-parameters to an existing network but they are known to help a network process\n-information in systematic ways to improve the overall performance.\n-\"\"\"\n-\n-\"\"\"\n-## Notes\n-\n-* To impose shape bias inside the vision models, Geirhos et al. trained them with a\n-combination of natural and stylized images. It might be interesting to investigate if\n-this learnable resizing module could achieve something similar as the outputs seem to\n-discard the texture information.\n-\n-* The resizer module can handle arbitrary resolutions and aspect ratios which is very\n-important for tasks like object detection and segmentation.\n-\n-* There is another closely related topic on ***adaptive image resizing*** that attempts\n-to resize images/feature maps adaptively during training. [EfficientV2](https://arxiv.org/pdf/2104.00298)\n-uses this idea.\n-\"\"\"\n\n@@ -1,491 +0,0 @@\n-\"\"\"\n-Title: Image classification with modern MLP models\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Converted to Keras 3 by: [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/), [divyasreepat](https://github.com/divyashreepathihalli)\n-Date created: 2021/05/30\n-Last modified: 2023/08/03\n-Description: Implementing the MLP-Mixer, FNet, and gMLP models for CIFAR-100 image classification.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example implements three modern attention-free, multi-layer perceptron (MLP) based models for image\n-classification, demonstrated on the CIFAR-100 dataset:\n-\n-1. The [MLP-Mixer](https://arxiv.org/abs/2105.01601) model, by Ilya Tolstikhin et al., based on two types of MLPs.\n-3. The [FNet](https://arxiv.org/abs/2105.03824) model, by James Lee-Thorp et al., based on unparameterized\n-Fourier Transform.\n-2. The [gMLP](https://arxiv.org/abs/2105.08050) model, by Hanxiao Liu et al., based on MLP with gating.\n-\n-The purpose of the example is not to compare between these models, as they might perform differently on\n-different datasets with well-tuned hyperparameters. Rather, it is to show simple implementations of their\n-main building blocks.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\"\"\"\n-\n-weight_decay = 0.0001\n-batch_size = 128\n-num_epochs = 50\n-dropout_rate = 0.2\n-image_size = 64  # We'll resize input images to this size.\n-patch_size = 8  # Size of the patches to be extracted from the input images.\n-num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n-embedding_dim = 256  # Number of hidden units.\n-num_blocks = 4  # Number of blocks.\n-\n-print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n-print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n-print(f\"Patches per image: {num_patches}\")\n-print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n-\n-\"\"\"\n-## Build a classification model\n-\n-We implement a method that builds a classifier given the processing blocks.\n-\"\"\"\n-\n-\n-def build_classifier(blocks, positional_encoding=False):\n-    inputs = layers.Input(shape=input_shape)\n-    # Augment data.\n-    augmented = data_augmentation(inputs)\n-    # Create patches.\n-    patches = Patches(patch_size)(augmented)\n-    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n-    x = layers.Dense(units=embedding_dim)(patches)\n-    if positional_encoding:\n-        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n-    # Process x using the module blocks.\n-    x = blocks(x)\n-    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n-    representation = layers.GlobalAveragePooling1D()(x)\n-    # Apply dropout.\n-    representation = layers.Dropout(rate=dropout_rate)(representation)\n-    # Compute logits outputs.\n-    logits = layers.Dense(num_classes)(representation)\n-    # Create the Keras model.\n-    return keras.Model(inputs=inputs, outputs=logits)\n-\n-\n-\"\"\"\n-## Define an experiment\n-\n-We implement a utility function to compile, train, and evaluate a given model.\n-\"\"\"\n-\n-\n-def run_experiment(model):\n-    # Create Adam optimizer with weight decay.\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate,\n-        weight_decay=weight_decay,\n-    )\n-    # Compile the model.\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-        metrics=[\n-            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n-            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n-        ],\n-    )\n-    # Create a learning rate scheduler callback.\n-    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n-        monitor=\"val_loss\", factor=0.5, patience=5\n-    )\n-    # Create an early stopping callback.\n-    early_stopping = keras.callbacks.EarlyStopping(\n-        monitor=\"val_loss\", patience=10, restore_best_weights=True\n-    )\n-    # Fit the model.\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[early_stopping, reduce_lr],\n-    )\n-\n-    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-    # Return history to plot learning curves.\n-    return history\n-\n-\n-\"\"\"\n-## Use data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.Resizing(image_size, image_size),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-# Compute the mean and the variance of the training data for normalization.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\n-\"\"\"\n-## Implement patch extraction as a layer\n-\"\"\"\n-\n-\n-class Patches(layers.Layer):\n-    def __init__(self, patch_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.patch_size = patch_size\n-\n-    def call(self, x):\n-        patches = keras.ops.image.extract_patches(x, self.patch_size)\n-        batch_size = keras.ops.shape(patches)[0]\n-        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n-        patch_dim = keras.ops.shape(patches)[3]\n-        out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n-        return out\n-\n-\n-\"\"\"\n-## Implement position embedding as a layer\n-\"\"\"\n-\n-\n-class PositionEmbedding(keras.layers.Layer):\n-    def __init__(\n-        self,\n-        sequence_length,\n-        initializer=\"glorot_uniform\",\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        if sequence_length is None:\n-            raise ValueError(\n-                \"`sequence_length` must be an Integer, received `None`.\"\n-            )\n-        self.sequence_length = int(sequence_length)\n-        self.initializer = keras.initializers.get(initializer)\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"sequence_length\": self.sequence_length,\n-                \"initializer\": keras.initializers.serialize(self.initializer),\n-            }\n-        )\n-        return config\n-\n-    def build(self, input_shape):\n-        feature_size = input_shape[-1]\n-        self.position_embeddings = self.add_weight(\n-            name=\"embeddings\",\n-            shape=[self.sequence_length, feature_size],\n-            initializer=self.initializer,\n-            trainable=True,\n-        )\n-\n-        super().build(input_shape)\n-\n-    def call(self, inputs, start_index=0):\n-        shape = keras.ops.shape(inputs)\n-        feature_length = shape[-1]\n-        sequence_length = shape[-2]\n-        # trim to match the length of the input sequence, which might be less\n-        # than the sequence_length of the layer.\n-        position_embeddings = keras.ops.convert_to_tensor(\n-            self.position_embeddings\n-        )\n-        position_embeddings = keras.ops.slice(\n-            position_embeddings,\n-            (start_index, 0),\n-            (sequence_length, feature_length),\n-        )\n-        return keras.ops.broadcast_to(position_embeddings, shape)\n-\n-    def compute_output_shape(self, input_shape):\n-        return input_shape\n-\n-\n-\"\"\"\n-## The MLP-Mixer model\n-\n-The MLP-Mixer is an architecture based exclusively on\n-multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n-\n-1. One applied independently to image patches, which mixes the per-location features.\n-2. The other applied across patches (along channels), which mixes spatial information.\n-\n-This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n-such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n-instead of batch normalization.\n-\"\"\"\n-\n-\"\"\"\n-### Implement the MLP-Mixer module\n-\"\"\"\n-\n-\n-class MLPMixerLayer(layers.Layer):\n-    def __init__(\n-        self, num_patches, hidden_units, dropout_rate, *args, **kwargs\n-    ):\n-        super().__init__(*args, **kwargs)\n-\n-        self.mlp1 = keras.Sequential(\n-            [\n-                layers.Dense(units=num_patches, activation=\"gelu\"),\n-                layers.Dense(units=num_patches),\n-                layers.Dropout(rate=dropout_rate),\n-            ]\n-        )\n-        self.mlp2 = keras.Sequential(\n-            [\n-                layers.Dense(units=num_patches, activation=\"gelu\"),\n-                layers.Dense(units=hidden_units),\n-                layers.Dropout(rate=dropout_rate),\n-            ]\n-        )\n-        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n-\n-    def build(self, input_shape):\n-        return super().build(input_shape)\n-\n-    def call(self, inputs):\n-        # Apply layer normalization.\n-        x = self.normalize(inputs)\n-        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n-        x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n-        # Apply mlp1 on each channel independently.\n-        mlp1_outputs = self.mlp1(x_channels)\n-        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n-        mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n-        # Add skip connection.\n-        x = mlp1_outputs + inputs\n-        # Apply layer normalization.\n-        x_patches = self.normalize(x)\n-        # Apply mlp2 on each patch independtenly.\n-        mlp2_outputs = self.mlp2(x_patches)\n-        # Add skip connection.\n-        x = x + mlp2_outputs\n-        return x\n-\n-\n-\"\"\"\n-### Build, train, and evaluate the MLP-Mixer model\n-\n-Note that training the model with the current settings on a V100 GPUs\n-takes around 8 seconds per epoch.\n-\"\"\"\n-\n-mlpmixer_blocks = keras.Sequential(\n-    [\n-        MLPMixerLayer(num_patches, embedding_dim, dropout_rate)\n-        for _ in range(num_blocks)\n-    ]\n-)\n-learning_rate = 0.005\n-mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n-history = run_experiment(mlpmixer_classifier)\n-\n-\"\"\"\n-The MLP-Mixer model tends to have much less number of parameters compared\n-to convolutional and transformer-based models, which leads to less training and\n-serving computational cost.\n-\n-As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n-when pre-trained on large datasets, or with modern regularization schemes,\n-the MLP-Mixer attains competitive scores to state-of-the-art models.\n-You can obtain better results by increasing the embedding dimensions,\n-increasing the number of mixer blocks, and training the model for longer.\n-You may also try to increase the size of the input images and use different patch sizes.\n-\"\"\"\n-\n-\"\"\"\n-## The FNet model\n-\n-The FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer\n-in the Transformer block with a parameter-free 2D Fourier transformation layer:\n-\n-1. One 1D Fourier Transform is applied along the patches.\n-2. One 1D Fourier Transform is applied along the channels.\n-\"\"\"\n-\n-\"\"\"\n-### Implement the FNet module\n-\"\"\"\n-\n-\n-class FNetLayer(layers.Layer):\n-    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        self.ffn = keras.Sequential(\n-            [\n-                layers.Dense(units=embedding_dim, activation=\"gelu\"),\n-                layers.Dropout(rate=dropout_rate),\n-                layers.Dense(units=embedding_dim),\n-            ]\n-        )\n-\n-        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n-        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n-\n-    def call(self, inputs):\n-        # Apply fourier transformations.\n-        real_part = inputs\n-        im_part = keras.ops.zeros_like(inputs)\n-        x = keras.ops.fft2((real_part, im_part))[0]\n-        # Add skip connection.\n-        x = x + inputs\n-        # Apply layer normalization.\n-        x = self.normalize1(x)\n-        # Apply Feedfowrad network.\n-        x_ffn = self.ffn(x)\n-        # Add skip connection.\n-        x = x + x_ffn\n-        # Apply layer normalization.\n-        return self.normalize2(x)\n-\n-\n-\"\"\"\n-### Build, train, and evaluate the FNet model\n-\n-Note that training the model with the current settings on a V100 GPUs\n-takes around 8 seconds per epoch.\n-\"\"\"\n-\n-fnet_blocks = keras.Sequential(\n-    [FNetLayer(embedding_dim, dropout_rate) for _ in range(num_blocks)]\n-)\n-learning_rate = 0.001\n-fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n-history = run_experiment(fnet_classifier)\n-\n-\"\"\"\n-As shown in the [FNet](https://arxiv.org/abs/2105.03824) paper,\n-better results can be achieved by increasing the embedding dimensions,\n-increasing the number of FNet blocks, and training the model for longer.\n-You may also try to increase the size of the input images and use different patch sizes.\n-The FNet scales very efficiently to long inputs, runs much faster than attention-based\n-Transformer models, and produces competitive accuracy results.\n-\"\"\"\n-\n-\"\"\"\n-## The gMLP model\n-\n-The gMLP is a MLP architecture that features a Spatial Gating Unit (SGU).\n-The SGU enables cross-patch interactions across the spatial (channel) dimension, by:\n-\n-1. Transforming the input spatially by applying linear projection across patches (along channels).\n-2. Applying element-wise multiplication of the input and its spatial transformation.\n-\"\"\"\n-\n-\"\"\"\n-### Implement the gMLP module\n-\"\"\"\n-\n-\n-class gMLPLayer(layers.Layer):\n-    def __init__(\n-        self, num_patches, embedding_dim, dropout_rate, *args, **kwargs\n-    ):\n-        super().__init__(*args, **kwargs)\n-\n-        self.channel_projection1 = keras.Sequential(\n-            [\n-                layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),\n-                layers.Dropout(rate=dropout_rate),\n-            ]\n-        )\n-\n-        self.channel_projection2 = layers.Dense(units=embedding_dim)\n-\n-        self.spatial_projection = layers.Dense(\n-            units=num_patches, bias_initializer=\"Ones\"\n-        )\n-\n-        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n-        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n-\n-    def spatial_gating_unit(self, x):\n-        # Split x along the channel dimensions.\n-        # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].\n-        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n-        # Apply layer normalization.\n-        v = self.normalize2(v)\n-        # Apply spatial projection.\n-        v_channels = keras.ops.transpose(v, axes=(0, 2, 1))\n-        v_projected = self.spatial_projection(v_channels)\n-        v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))\n-        # Apply element-wise multiplication.\n-        return u * v_projected\n-\n-    def call(self, inputs):\n-        # Apply layer normalization.\n-        x = self.normalize1(inputs)\n-        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n-        x_projected = self.channel_projection1(x)\n-        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n-        x_spatial = self.spatial_gating_unit(x_projected)\n-        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n-        x_projected = self.channel_projection2(x_spatial)\n-        # Add skip connection.\n-        return x + x_projected\n-\n-\n-\"\"\"\n-### Build, train, and evaluate the gMLP model\n-\n-Note that training the model with the current settings on a V100 GPUs\n-takes around 9 seconds per epoch.\n-\"\"\"\n-\n-gmlp_blocks = keras.Sequential(\n-    [\n-        gMLPLayer(num_patches, embedding_dim, dropout_rate)\n-        for _ in range(num_blocks)\n-    ]\n-)\n-learning_rate = 0.003\n-gmlp_classifier = build_classifier(gmlp_blocks)\n-history = run_experiment(gmlp_classifier)\n-\n-\"\"\"\n-As shown in the [gMLP](https://arxiv.org/abs/2105.08050) paper,\n-better results can be achieved by increasing the embedding dimensions,\n-increasing the number of gMLP blocks, and training the model for longer.\n-You may also try to increase the size of the input images and use different patch sizes.\n-Note that, the paper used advanced regularization strategies, such as MixUp and CutMix,\n-as well as AutoAugment.\n-\"\"\"\n\n@@ -1,84 +0,0 @@\n-\"\"\"\n-Title: Simple MNIST convnet\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2015/06/19\n-Last modified: 2020/04/21\n-Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-# Model / data parameters\n-num_classes = 10\n-input_shape = (28, 28, 1)\n-\n-# Load the data and split it between train and test sets\n-(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n-\n-# Scale images to the [0, 1] range\n-x_train = x_train.astype(\"float32\") / 255\n-x_test = x_test.astype(\"float32\") / 255\n-# Make sure images have shape (28, 28, 1)\n-x_train = np.expand_dims(x_train, -1)\n-x_test = np.expand_dims(x_test, -1)\n-print(\"x_train shape:\", x_train.shape)\n-print(x_train.shape[0], \"train samples\")\n-print(x_test.shape[0], \"test samples\")\n-\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-\"\"\"\n-## Build the model\n-\"\"\"\n-\n-model = keras.Sequential(\n-    [\n-        keras.Input(shape=input_shape),\n-        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n-        layers.MaxPooling2D(pool_size=(2, 2)),\n-        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n-        layers.MaxPooling2D(pool_size=(2, 2)),\n-        layers.Flatten(),\n-        layers.Dropout(0.5),\n-        layers.Dense(num_classes, activation=\"softmax\"),\n-    ]\n-)\n-\n-model.summary()\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-batch_size = 128\n-epochs = 15\n-\n-model.compile(\n-    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n-)\n-\n-model.fit(\n-    x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1\n-)\n-\n-\"\"\"\n-## Evaluate the trained model\n-\"\"\"\n-\n-score = model.evaluate(x_test, y_test, verbose=0)\n-print(\"Test loss:\", score[0])\n-print(\"Test accuracy:\", score[1])\n\n@@ -1,539 +0,0 @@\n-\"\"\"\n-Title: Object detection with Vision Transformers\n-Author: [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)\n-Date created: 2022/03/27\n-Last modified: 2022/03/27\n-Description: A simple Keras implementation of object detection using Vision Transformers.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The article\n-[Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n-architecture by Alexey Dosovitskiy et al.\n-demonstrates that a pure transformer applied directly to sequences of image\n-patches can perform well on object detection tasks.\n-\n-In this Keras example, we implement an object detection ViT\n-and we train it on the\n-[Caltech 101 dataset](http://www.vision.caltech.edu/datasets/)\n-to detect an airplane in the given image.\n-\n-This example requires TensorFlow 2.4 or higher, and\n-[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n-from which we import the `AdamW` optimizer.\n-\n-TensorFlow Addons can be installed via the following command:\n-\n-```\n-pip install -U git+https://github.com/keras-team/keras\n-```\n-\"\"\"\n-\n-\"\"\"\n-## Imports and setup\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n-\n-\n-import numpy as np\n-import keras\n-from keras import layers\n-from keras import ops\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import cv2\n-import os\n-import scipy.io\n-import shutil\n-\n-\"\"\"\n-## Prepare dataset\n-\n-We use the [Caltech 101 Dataset](https://data.caltech.edu/records/mzrjq-6wc02).\n-\"\"\"\n-\n-# Path to images and annotations\n-path_images = \"/101_ObjectCategories/airplanes/\"\n-path_annot = \"/Annotations/Airplanes_Side_2/\"\n-\n-path_to_downloaded_file = keras.utils.get_file(\n-    fname=\"caltech_101_zipped\",\n-    origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n-    extract=True,\n-    archive_format=\"zip\",  # downloaded file format\n-    cache_dir=\"/\",  # cache and extract in current directory\n-)\n-\n-# Extracting tar files found inside main zip file\n-shutil.unpack_archive(\"/datasets/caltech-101/101_ObjectCategories.tar.gz\", \"/\")\n-shutil.unpack_archive(\"/datasets/caltech-101/Annotations.tar\", \"/\")\n-\n-# list of paths to images and annotations\n-image_paths = [\n-    f\n-    for f in os.listdir(path_images)\n-    if os.path.isfile(os.path.join(path_images, f))\n-]\n-annot_paths = [\n-    f\n-    for f in os.listdir(path_annot)\n-    if os.path.isfile(os.path.join(path_annot, f))\n-]\n-\n-image_paths.sort()\n-annot_paths.sort()\n-\n-image_size = 224  # resize input images to this size\n-\n-images, targets = [], []\n-\n-# loop over the annotations and images, preprocess them and store in lists\n-for i in range(0, len(annot_paths)):\n-    # Access bounding box coordinates\n-    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n-\n-    top_left_x, top_left_y = annot[2], annot[0]\n-    bottom_right_x, bottom_right_y = annot[3], annot[1]\n-\n-    image = keras.utils.load_img(\n-        path_images + image_paths[i],\n-    )\n-    (w, h) = image.size[:2]\n-\n-    # resize train set images\n-    if i < int(len(annot_paths) * 0.8):\n-        # resize image if it is for training dataset\n-        image = image.resize((image_size, image_size))\n-\n-    # convert image to array and append to list\n-    images.append(keras.utils.img_to_array(image))\n-\n-    # apply relative scaling to bounding boxes as per given image and append to list\n-    targets.append(\n-        (\n-            float(top_left_x) / w,\n-            float(top_left_y) / h,\n-            float(bottom_right_x) / w,\n-            float(bottom_right_y) / h,\n-        )\n-    )\n-\n-# Convert the list to numpy array, split to train and test dataset\n-(x_train), (y_train) = (\n-    np.asarray(images[: int(len(images) * 0.8)]),\n-    np.asarray(targets[: int(len(targets) * 0.8)]),\n-)\n-(x_test), (y_test) = (\n-    np.asarray(images[int(len(images) * 0.8) :]),\n-    np.asarray(targets[int(len(targets) * 0.8) :]),\n-)\n-\n-\"\"\"\n-## Implement multilayer-perceptron (MLP)\n-\n-We use the code from the Keras example\n-[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n-as a reference.\n-\"\"\"\n-\n-\n-def mlp(x, hidden_units, dropout_rate):\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the patch creation layer\n-\"\"\"\n-\n-\n-class Patches(layers.Layer):\n-    def __init__(self, patch_size):\n-        super().__init__()\n-        self.patch_size = patch_size\n-\n-    def call(self, images):\n-        input_shape = ops.shape(images)\n-        batch_size = input_shape[0]\n-        height = input_shape[1]\n-        width = input_shape[2]\n-        channels = input_shape[3]\n-        num_patches_h = height // self.patch_size\n-        num_patches_w = width // self.patch_size\n-        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n-        patches = ops.reshape(\n-            patches,\n-            (\n-                batch_size,\n-                num_patches_h * num_patches_w,\n-                self.patch_size * self.patch_size * channels,\n-            ),\n-        )\n-        return patches\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"patch_size\": self.patch_size})\n-        return config\n-\n-\n-\"\"\"\n-## Display patches for an input image\n-\"\"\"\n-\n-patch_size = 32  # Size of the patches to be extracted from the input images\n-\n-plt.figure(figsize=(4, 4))\n-plt.imshow(x_train[0].astype(\"uint8\"))\n-plt.axis(\"off\")\n-\n-patches = Patches(patch_size)(np.expand_dims(x_train[0], axis=0))\n-print(f\"Image size: {image_size} X {image_size}\")\n-print(f\"Patch size: {patch_size} X {patch_size}\")\n-print(\n-    f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\"\n-)\n-\n-\n-n = int(np.sqrt(patches.shape[1]))\n-plt.figure(figsize=(4, 4))\n-for i, patch in enumerate(patches[0]):\n-    ax = plt.subplot(n, n, i + 1)\n-    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n-    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Implement the patch encoding layer\n-\n-The `PatchEncoder` layer linearly transforms a patch by projecting it into a\n-vector of size `projection_dim`. It also adds a learnable position\n-embedding to the projected vector.\n-\"\"\"\n-\n-\n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.projection = layers.Dense(units=projection_dim)\n-        self.position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=projection_dim\n-        )\n-\n-    # Override function to avoid error while saving model\n-    def get_config(self):\n-        config = super().get_config().copy()\n-        config.update(\n-            {\n-                \"input_shape\": input_shape,\n-                \"patch_size\": patch_size,\n-                \"num_patches\": num_patches,\n-                \"projection_dim\": projection_dim,\n-                \"num_heads\": num_heads,\n-                \"transformer_units\": transformer_units,\n-                \"transformer_layers\": transformer_layers,\n-                \"mlp_head_units\": mlp_head_units,\n-            }\n-        )\n-        return config\n-\n-    def call(self, patch):\n-        positions = ops.expand_dims(\n-            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n-        )\n-        projected_patches = self.projection(patch)\n-        encoded = projected_patches + self.position_embedding(positions)\n-        return encoded\n-\n-\n-\"\"\"\n-## Build the ViT model\n-\n-The ViT model has multiple Transformer blocks.\n-The `MultiHeadAttention` layer is used for self-attention,\n-applied to the sequence of image patches. The encoded patches (skip connection)\n-and self-attention layer outputs are normalized and fed into a\n-multilayer perceptron (MLP).\n-The model outputs four dimensions representing\n-the bounding box coordinates of an object.\n-\"\"\"\n-\n-\n-def create_vit_object_detector(\n-    input_shape,\n-    patch_size,\n-    num_patches,\n-    projection_dim,\n-    num_heads,\n-    transformer_units,\n-    transformer_layers,\n-    mlp_head_units,\n-):\n-    inputs = keras.Input(shape=input_shape)\n-    # Create patches\n-    patches = Patches(patch_size)(inputs)\n-    # Encode patches\n-    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n-\n-    # Create multiple layers of the Transformer block.\n-    for _ in range(transformer_layers):\n-        # Layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-        # Skip connection 1.\n-        x2 = layers.Add()([attention_output, encoded_patches])\n-        # Layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n-        # MLP\n-        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n-        # Skip connection 2.\n-        encoded_patches = layers.Add()([x3, x2])\n-\n-    # Create a [batch_size, projection_dim] tensor.\n-    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-    representation = layers.Flatten()(representation)\n-    representation = layers.Dropout(0.3)(representation)\n-    # Add MLP.\n-    features = mlp(\n-        representation, hidden_units=mlp_head_units, dropout_rate=0.3\n-    )\n-\n-    bounding_box = layers.Dense(4)(\n-        features\n-    )  # Final four neurons that output bounding box\n-\n-    # return Keras model.\n-    return keras.Model(inputs=inputs, outputs=bounding_box)\n-\n-\n-\"\"\"\n-## Run the experiment\n-\"\"\"\n-\n-\n-def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    )\n-\n-    # Compile model.\n-    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n-\n-    checkpoint_filepath = \"vit_object_detector.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_loss\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[\n-            checkpoint_callback,\n-            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n-        ],\n-    )\n-\n-    return history\n-\n-\n-input_shape = (image_size, image_size, 3)  # input image shape\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 32\n-num_epochs = 15\n-num_patches = (image_size // patch_size) ** 2\n-projection_dim = 64\n-num_heads = 4\n-# Size of the transformer layers\n-transformer_units = [\n-    projection_dim * 2,\n-    projection_dim,\n-]\n-transformer_layers = 4\n-mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n-\n-\n-history = []\n-num_patches = (image_size // patch_size) ** 2\n-\n-vit_object_detector = create_vit_object_detector(\n-    input_shape,\n-    patch_size,\n-    num_patches,\n-    projection_dim,\n-    num_heads,\n-    transformer_units,\n-    transformer_layers,\n-    mlp_head_units,\n-)\n-\n-# Train model\n-history = run_experiment(\n-    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n-)\n-\n-\n-def plot_history(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_history(\"loss\")\n-\n-\n-\"\"\"\n-## Evaluate the model\n-\"\"\"\n-\n-import matplotlib.patches as patches\n-\n-# Saves the model in current path\n-vit_object_detector.save(\"vit_object_detector.keras\")\n-\n-\n-# To calculate IoU (intersection over union, given two bounding boxes)\n-def bounding_box_intersection_over_union(box_predicted, box_truth):\n-    # get (x, y) coordinates of intersection of bounding boxes\n-    top_x_intersect = max(box_predicted[0], box_truth[0])\n-    top_y_intersect = max(box_predicted[1], box_truth[1])\n-    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n-    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n-\n-    # calculate area of the intersection bb (bounding box)\n-    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n-        0, bottom_y_intersect - top_y_intersect + 1\n-    )\n-\n-    # calculate area of the prediction bb and ground-truth bb\n-    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n-        box_predicted[3] - box_predicted[1] + 1\n-    )\n-    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n-        box_truth[3] - box_truth[1] + 1\n-    )\n-\n-    # calculate intersection over union by taking intersection\n-    # area and dividing it by the sum of predicted bb and ground truth\n-    # bb areas subtracted by  the interesection area\n-\n-    # return ioU\n-    return intersection_area / float(\n-        box_predicted_area + box_truth_area - intersection_area\n-    )\n-\n-\n-i, mean_iou = 0, 0\n-\n-# Compare results for 10 images in the test set\n-for input_image in x_test[:10]:\n-    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n-    im = input_image\n-\n-    # Display the image\n-    ax1.imshow(im.astype(\"uint8\"))\n-    ax2.imshow(im.astype(\"uint8\"))\n-\n-    input_image = cv2.resize(\n-        input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n-    )\n-    input_image = np.expand_dims(input_image, axis=0)\n-    preds = vit_object_detector.predict(input_image)[0]\n-\n-    (h, w) = (im).shape[0:2]\n-\n-    top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n-\n-    bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n-\n-    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n-    # Create the bounding box\n-    rect = patches.Rectangle(\n-        (top_left_x, top_left_y),\n-        bottom_right_x - top_left_x,\n-        bottom_right_y - top_left_y,\n-        facecolor=\"none\",\n-        edgecolor=\"red\",\n-        linewidth=1,\n-    )\n-    # Add the bounding box to the image\n-    ax1.add_patch(rect)\n-    ax1.set_xlabel(\n-        \"Predicted: \"\n-        + str(top_left_x)\n-        + \", \"\n-        + str(top_left_y)\n-        + \", \"\n-        + str(bottom_right_x)\n-        + \", \"\n-        + str(bottom_right_y)\n-    )\n-\n-    top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n-\n-    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(\n-        y_test[i][3] * h\n-    )\n-\n-    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n-\n-    mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n-    # Create the bounding box\n-    rect = patches.Rectangle(\n-        (top_left_x, top_left_y),\n-        bottom_right_x - top_left_x,\n-        bottom_right_y - top_left_y,\n-        facecolor=\"none\",\n-        edgecolor=\"red\",\n-        linewidth=1,\n-    )\n-    # Add the bounding box to the image\n-    ax2.add_patch(rect)\n-    ax2.set_xlabel(\n-        \"Target: \"\n-        + str(top_left_x)\n-        + \", \"\n-        + str(top_left_y)\n-        + \", \"\n-        + str(bottom_right_x)\n-        + \", \"\n-        + str(bottom_right_y)\n-        + \"\\n\"\n-        + \"IoU\"\n-        + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n-    )\n-    i = i + 1\n-\n-print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n-plt.show()\n-\n-\"\"\"\n-This example demonstrates that a pure Transformer can be trained\n-to predict the bounding boxes of an object in a given image,\n-thus extending the use of Transformers to object detection tasks.\n-The model can be improved further by tuning hyper-parameters and pre-training.\n-\"\"\"\n\n@@ -1,272 +0,0 @@\n-\"\"\"\n-Title: Image segmentation with a U-Net-like architecture\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2019/03/20\n-Last modified: 2020/04/20\n-Description: Image segmentation model trained from scratch on the Oxford Pets dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Download the data\n-\"\"\"\n-\n-\"\"\"shell\n-!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n-!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n-\n-curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n-curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n-\n-tar -xf images.tar.gz\n-tar -xf annotations.tar.gz\n-\"\"\"\n-\n-\"\"\"\n-## Prepare paths of input images and target segmentation masks\n-\"\"\"\n-\n-import os\n-\n-input_dir = \"images/\"\n-target_dir = \"annotations/trimaps/\"\n-img_size = (160, 160)\n-num_classes = 3\n-batch_size = 32\n-\n-input_img_paths = sorted(\n-    [\n-        os.path.join(input_dir, fname)\n-        for fname in os.listdir(input_dir)\n-        if fname.endswith(\".jpg\")\n-    ]\n-)\n-target_img_paths = sorted(\n-    [\n-        os.path.join(target_dir, fname)\n-        for fname in os.listdir(target_dir)\n-        if fname.endswith(\".png\") and not fname.startswith(\".\")\n-    ]\n-)\n-\n-print(\"Number of samples:\", len(input_img_paths))\n-\n-for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n-    print(input_path, \"|\", target_path)\n-\n-\"\"\"\n-## What does one input image and corresponding segmentation mask look like?\n-\"\"\"\n-\n-from IPython.display import Image, display\n-from keras.utils import load_img\n-from PIL import ImageOps\n-\n-# Display input image #7\n-display(Image(filename=input_img_paths[9]))\n-\n-# Display auto-contrast version of corresponding target (per-pixel categories)\n-img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n-display(img)\n-\n-\"\"\"\n-## Prepare dataset to load & vectorize batches of data\n-\"\"\"\n-\n-import keras\n-import numpy as np\n-from tensorflow import data as tf_data\n-from tensorflow import image as tf_image\n-from tensorflow import io as tf_io\n-\n-\n-def get_dataset(\n-    batch_size,\n-    img_size,\n-    input_img_paths,\n-    target_img_paths,\n-    max_dataset_len=None,\n-):\n-    \"\"\"Returns a TF Dataset.\"\"\"\n-\n-    def load_img_masks(input_img_path, target_img_path):\n-        input_img = tf_io.read_file(input_img_path)\n-        input_img = tf_io.decode_png(input_img, channels=3)\n-        input_img = tf_image.resize(input_img, img_size)\n-        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n-\n-        target_img = tf_io.read_file(target_img_path)\n-        target_img = tf_io.decode_png(target_img, channels=1)\n-        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n-        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n-\n-        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n-        target_img -= 1\n-        return input_img, target_img\n-\n-    # For faster debugging, limit the size of data\n-    if max_dataset_len:\n-        input_img_paths = input_img_paths[:max_dataset_len]\n-        target_img_paths = target_img_paths[:max_dataset_len]\n-    dataset = tf_data.Dataset.from_tensor_slices(\n-        (input_img_paths, target_img_paths)\n-    )\n-    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n-    return dataset.batch(batch_size)\n-\n-\n-\"\"\"\n-## Prepare U-Net Xception-style model\n-\"\"\"\n-\n-from keras import layers\n-\n-\n-def get_model(img_size, num_classes):\n-    inputs = keras.Input(shape=img_size + (3,))\n-\n-    ### [First half of the network: downsampling inputs] ###\n-\n-    # Entry block\n-    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    previous_block_activation = x  # Set aside residual\n-\n-    # Blocks 1, 2, 3 are identical apart from the feature depth.\n-    for filters in [64, 128, 256]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n-\n-        # Project residual\n-        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n-            previous_block_activation\n-        )\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    ### [Second half of the network: upsampling inputs] ###\n-\n-    for filters in [256, 128, 64, 32]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.UpSampling2D(2)(x)\n-\n-        # Project residual\n-        residual = layers.UpSampling2D(2)(previous_block_activation)\n-        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    # Add a per-pixel classification layer\n-    outputs = layers.Conv2D(\n-        num_classes, 3, activation=\"softmax\", padding=\"same\"\n-    )(x)\n-\n-    # Define the model\n-    model = keras.Model(inputs, outputs)\n-    return model\n-\n-\n-# Build model\n-model = get_model(img_size, num_classes)\n-model.summary()\n-\n-\"\"\"\n-## Set aside a validation split\n-\"\"\"\n-\n-import random\n-\n-# Split our img paths into a training and a validation set\n-val_samples = 1000\n-random.Random(1337).shuffle(input_img_paths)\n-random.Random(1337).shuffle(target_img_paths)\n-train_input_img_paths = input_img_paths[:-val_samples]\n-train_target_img_paths = target_img_paths[:-val_samples]\n-val_input_img_paths = input_img_paths[-val_samples:]\n-val_target_img_paths = target_img_paths[-val_samples:]\n-\n-# Instantiate dataset for each split\n-# Limit input files in `max_dataset_len` for faster epoch training time.\n-# Remove the `max_dataset_len` arg when running with full dataset.\n-train_dataset = get_dataset(\n-    batch_size,\n-    img_size,\n-    train_input_img_paths,\n-    train_target_img_paths,\n-    max_dataset_len=1000,\n-)\n-valid_dataset = get_dataset(\n-    batch_size, img_size, val_input_img_paths, val_target_img_paths\n-)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-# Configure the model for training.\n-# We use the \"sparse\" version of categorical_crossentropy\n-# because our target data is integers.\n-model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\n-        \"oxford_segmentation.keras\", save_best_only=True\n-    )\n-]\n-\n-# Train the model, doing validation at the end of each epoch.\n-epochs = 15\n-model.fit(\n-    train_dataset,\n-    epochs=epochs,\n-    validation_data=valid_dataset,\n-    callbacks=callbacks,\n-)\n-\n-\"\"\"\n-## Visualize predictions\n-\"\"\"\n-\n-# Generate predictions for all images in the validation set\n-\n-val_dataset = get_dataset(\n-    batch_size, img_size, val_input_img_paths, val_target_img_paths\n-)\n-val_preds = model.predict(val_dataset)\n-\n-\n-def display_mask(i):\n-    \"\"\"Quick utility to display a model's prediction.\"\"\"\n-    mask = np.argmax(val_preds[i], axis=-1)\n-    mask = np.expand_dims(mask, axis=-1)\n-    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n-    display(img)\n-\n-\n-# Display results for validation image #10\n-i = 10\n-\n-# Display input image\n-display(Image(filename=val_input_img_paths[i]))\n-\n-# Display ground-truth target mask\n-img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n-display(img)\n-\n-# Display mask predicted by our model\n-display_mask(i)  # Note that the model only sees inputs at 150x150.\n\n@@ -1,605 +0,0 @@\n-\"\"\"\n-Title: Point cloud segmentation with PointNet\n-Author: [Soumik Rakshit](https://github.com/soumik12345), [Sayak Paul](https://github.com/sayakpaul)\n-Date created: 2020/10/23\n-Last modified: 2020/10/24\n-Description: Implementation of a PointNet-based model for segmenting point clouds.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-A \"point cloud\" is an important type of data structure for storing geometric shape data.\n-Due to its irregular format, it's often transformed into\n-regular 3D voxel grids or collections of images before being used in deep learning applications,\n-a step which makes the data unnecessarily large.\n-The PointNet family of models solves this problem by directly consuming point clouds, respecting\n-the permutation-invariance property of the point data. The PointNet family of\n-models provides a simple, unified architecture\n-for applications ranging from **object classification**, **part segmentation**, to\n-**scene semantic parsing**.\n-\n-In this example, we demonstrate the implementation of the PointNet architecture\n-for shape segmentation.\n-\n-### References\n-\n-- [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)\n-- [Point cloud classification with PointNet](https://keras.io/examples/vision/pointnet/)\n-- [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)\n-\"\"\"\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import os\n-import json\n-import random\n-import numpy as np\n-import pandas as pd\n-from tqdm import tqdm\n-from glob import glob\n-\n-import tensorflow as tf  # For tf.data\n-import keras\n-from keras import layers\n-\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Downloading Dataset\n-\n-The [ShapeNet dataset](https://shapenet.org/) is an ongoing effort to establish a richly-annotated,\n-large-scale dataset of 3D shapes. **ShapeNetCore** is a subset of the full ShapeNet\n-dataset with clean single 3D models and manually verified category and alignment\n-annotations. It covers 55 common object categories, with about 51,300 unique 3D models.\n-\n-For this example, we use one of the 12 object categories of\n-[PASCAL 3D+](http://cvgl.stanford.edu/projects/pascal3d.html),\n-included as part of the ShapenetCore dataset.\n-\"\"\"\n-\n-dataset_url = \"https://git.io/JiY4i\"\n-\n-dataset_path = keras.utils.get_file(\n-    fname=\"shapenet.zip\",\n-    origin=dataset_url,\n-    cache_subdir=\"datasets\",\n-    hash_algorithm=\"auto\",\n-    extract=True,\n-    archive_format=\"auto\",\n-    cache_dir=\"datasets\",\n-)\n-\n-\"\"\"\n-## Loading the dataset\n-\n-We parse the dataset metadata in order to easily map model categories to their\n-respective directories and segmentation classes to colors for the purpose of\n-visualization.\n-\"\"\"\n-\n-with open(\"/tmp/.keras/datasets/PartAnnotation/metadata.json\") as json_file:\n-    metadata = json.load(json_file)\n-\n-print(metadata)\n-\n-\"\"\"\n-In this example, we train PointNet to segment the parts of an `Airplane` model.\n-\"\"\"\n-\n-points_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points\".format(\n-    metadata[\"Airplane\"][\"directory\"]\n-)\n-labels_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points_label\".format(\n-    metadata[\"Airplane\"][\"directory\"]\n-)\n-LABELS = metadata[\"Airplane\"][\"lables\"]\n-COLORS = metadata[\"Airplane\"][\"colors\"]\n-\n-VAL_SPLIT = 0.2\n-NUM_SAMPLE_POINTS = 1024\n-BATCH_SIZE = 32\n-EPOCHS = 60\n-INITIAL_LR = 1e-3\n-\n-\"\"\"\n-## Structuring the dataset\n-\n-We generate the following in-memory data structures from the Airplane point clouds and\n-their labels:\n-\n-- `point_clouds` is a list of `np.array` objects that represent the point cloud data in\n-the form of x, y and z coordinates. Axis 0 represents the number of points in the\n-point cloud, while axis 1 represents the coordinates. `all_labels` is the list\n-that represents the label of each coordinate as a string (needed mainly for\n-visualization purposes).\n-- `test_point_clouds` is in the same format as `point_clouds`, but doesn't have\n-corresponding the labels of the point clouds.\n-- `all_labels` is a list of `np.array` objects that represent the point cloud labels\n-for each coordinate, corresponding to the `point_clouds` list.\n-- `point_cloud_labels` is a list of `np.array` objects that represent the point cloud\n-labels for each coordinate in one-hot encoded form, corresponding to the `point_clouds`\n-list.\n-\"\"\"\n-\n-point_clouds, test_point_clouds = [], []\n-point_cloud_labels, all_labels = [], []\n-\n-points_files = glob(os.path.join(points_dir, \"*.pts\"))\n-for point_file in tqdm(points_files):\n-    point_cloud = np.loadtxt(point_file)\n-    if point_cloud.shape[0] < NUM_SAMPLE_POINTS:\n-        continue\n-\n-    # Get the file-id of the current point cloud for parsing its\n-    # labels.\n-    file_id = point_file.split(\"/\")[-1].split(\".\")[0]\n-    label_data, num_labels = {}, 0\n-    for label in LABELS:\n-        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n-        if os.path.exists(label_file):\n-            label_data[label] = np.loadtxt(label_file).astype(\"float32\")\n-            num_labels = len(label_data[label])\n-\n-    # Point clouds having labels will be our training samples.\n-    try:\n-        label_map = [\"none\"] * num_labels\n-        for label in LABELS:\n-            for i, data in enumerate(label_data[label]):\n-                label_map[i] = label if data == 1 else label_map[i]\n-        label_data = [\n-            LABELS.index(label) if label != \"none\" else len(LABELS)\n-            for label in label_map\n-        ]\n-        # Apply one-hot encoding to the dense label representation.\n-        label_data = keras.utils.to_categorical(label_data, num_classes=len(LABELS) + 1)\n-\n-        point_clouds.append(point_cloud)\n-        point_cloud_labels.append(label_data)\n-        all_labels.append(label_map)\n-    except KeyError:\n-        test_point_clouds.append(point_cloud)\n-\n-\"\"\"\n-Next, we take a look at some samples from the in-memory arrays we just generated:\n-\"\"\"\n-\n-for _ in range(5):\n-    i = random.randint(0, len(point_clouds) - 1)\n-    print(f\"point_clouds[{i}].shape:\", point_clouds[0].shape)\n-    print(f\"point_cloud_labels[{i}].shape:\", point_cloud_labels[0].shape)\n-    for j in range(5):\n-        print(\n-            f\"all_labels[{i}][{j}]:\",\n-            all_labels[i][j],\n-            f\"\\tpoint_cloud_labels[{i}][{j}]:\",\n-            point_cloud_labels[i][j],\n-            \"\\n\",\n-        )\n-\n-\"\"\"\n-Now, let's visualize some of the point clouds along with their labels.\n-\"\"\"\n-\n-\n-def visualize_data(point_cloud, labels):\n-    df = pd.DataFrame(\n-        data={\n-            \"x\": point_cloud[:, 0],\n-            \"y\": point_cloud[:, 1],\n-            \"z\": point_cloud[:, 2],\n-            \"label\": labels,\n-        }\n-    )\n-    fig = plt.figure(figsize=(15, 10))\n-    ax = plt.axes(projection=\"3d\")\n-    for index, label in enumerate(LABELS):\n-        c_df = df[df[\"label\"] == label]\n-        try:\n-            ax.scatter(\n-                c_df[\"x\"], c_df[\"y\"], c_df[\"z\"], label=label, alpha=0.5, c=COLORS[index]\n-            )\n-        except IndexError:\n-            pass\n-    ax.legend()\n-    plt.show()\n-\n-\n-visualize_data(point_clouds[0], all_labels[0])\n-visualize_data(point_clouds[300], all_labels[300])\n-\n-\"\"\"\n-### Preprocessing\n-\n-Note that all the point clouds that we have loaded consist of a variable number of points,\n-which makes it difficult for us to batch them together. In order to overcome this problem, we\n-randomly sample a fixed number of points from each point cloud. We also normalize the\n-point clouds in order to make the data scale-invariant.\n-\"\"\"\n-\n-for index in tqdm(range(len(point_clouds))):\n-    current_point_cloud = point_clouds[index]\n-    current_label_cloud = point_cloud_labels[index]\n-    current_labels = all_labels[index]\n-    num_points = len(current_point_cloud)\n-    # Randomly sampling respective indices.\n-    sampled_indices = random.sample(list(range(num_points)), NUM_SAMPLE_POINTS)\n-    # Sampling points corresponding to sampled indices.\n-    sampled_point_cloud = np.array([current_point_cloud[i] for i in sampled_indices])\n-    # Sampling corresponding one-hot encoded labels.\n-    sampled_label_cloud = np.array([current_label_cloud[i] for i in sampled_indices])\n-    # Sampling corresponding labels for visualization.\n-    sampled_labels = np.array([current_labels[i] for i in sampled_indices])\n-    # Normalizing sampled point cloud.\n-    norm_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n-    norm_point_cloud /= np.max(np.linalg.norm(norm_point_cloud, axis=1))\n-    point_clouds[index] = norm_point_cloud\n-    point_cloud_labels[index] = sampled_label_cloud\n-    all_labels[index] = sampled_labels\n-\n-\"\"\"\n-Let's visualize the sampled and normalized point clouds along with their corresponding\n-labels.\n-\"\"\"\n-\n-visualize_data(point_clouds[0], all_labels[0])\n-visualize_data(point_clouds[300], all_labels[300])\n-\n-\"\"\"\n-### Creating TensorFlow datasets\n-\n-We create `tf.data.Dataset` objects for the training and validation data.\n-We also augment the training point clouds by applying random jitter to them.\n-\"\"\"\n-\n-\n-def load_data(point_cloud_batch, label_cloud_batch):\n-    point_cloud_batch.set_shape([NUM_SAMPLE_POINTS, 3])\n-    label_cloud_batch.set_shape([NUM_SAMPLE_POINTS, len(LABELS) + 1])\n-    return point_cloud_batch, label_cloud_batch\n-\n-\n-def augment(point_cloud_batch, label_cloud_batch):\n-    noise = tf.random.uniform(\n-        tf.shape(label_cloud_batch), -0.001, 0.001, dtype=tf.float64\n-    )\n-    point_cloud_batch += noise[:, :, :3]\n-    return point_cloud_batch, label_cloud_batch\n-\n-\n-def generate_dataset(point_clouds, label_clouds, is_training=True):\n-    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n-    dataset = dataset.shuffle(BATCH_SIZE * 100) if is_training else dataset\n-    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n-    dataset = dataset.batch(batch_size=BATCH_SIZE)\n-    dataset = (\n-        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n-        if is_training\n-        else dataset\n-    )\n-    return dataset\n-\n-\n-split_index = int(len(point_clouds) * (1 - VAL_SPLIT))\n-train_point_clouds = point_clouds[:split_index]\n-train_label_cloud = point_cloud_labels[:split_index]\n-total_training_examples = len(train_point_clouds)\n-\n-val_point_clouds = point_clouds[split_index:]\n-val_label_cloud = point_cloud_labels[split_index:]\n-\n-print(\"Num train point clouds:\", len(train_point_clouds))\n-print(\"Num train point cloud labels:\", len(train_label_cloud))\n-print(\"Num val point clouds:\", len(val_point_clouds))\n-print(\"Num val point cloud labels:\", len(val_label_cloud))\n-\n-train_dataset = generate_dataset(train_point_clouds, train_label_cloud)\n-val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False)\n-\n-print(\"Train Dataset:\", train_dataset)\n-print(\"Validation Dataset:\", val_dataset)\n-\n-\"\"\"\n-## PointNet model\n-\n-The figure below depicts the internals of the PointNet model family:\n-\n-![](https://i.imgur.com/qFLNw5L.png)\n-\n-Given that PointNet is meant to consume an ***unordered set*** of coordinates as its input data,\n-its architecture needs to match the following characteristic properties\n-of point cloud data:\n-\n-### Permutation invariance\n-\n-Given the unstructured nature of point cloud data, a scan made up of `n` points has `n!`\n-permutations. The subsequent data processing must be invariant to the different\n-representations. In order to make PointNet invariant to input permutations, we use a\n-symmetric function (such as max-pooling) once the `n` input points are mapped to\n-higher-dimensional space. The result is a **global feature vector** that aims to capture\n-an aggregate signature of the `n` input points. The global feature vector is used alongside\n-local point features for segmentation.\n-\n-![](https://i.imgur.com/0mrvvjb.png)\n-\n-### Transformation invariance\n-\n-Segmentation outputs should be unchanged if the object undergoes certain transformations,\n-such as translation or scaling. For a given input point cloud, we apply an appropriate\n-rigid or affine transformation to achieve pose normalization. Because each of the `n` input\n-points are represented as a vector and are mapped to the embedding spaces independently,\n-applying a geometric transformation simply amounts to matrix multiplying each point with\n-a transformation matrix. This is motivated by the concept of\n-[Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n-\n-The operations comprising the T-Net are motivated by the higher-level architecture of\n-PointNet. MLPs (or fully-connected layers) are used to map the input points independently\n-and identically to a higher-dimensional space; max-pooling is used to encode a global\n-feature vector whose dimensionality is then reduced with fully-connected layers. The\n-input-dependent features at the final fully-connected layer are then combined with\n-globally trainable weights and biases, resulting in a 3-by-3 transformation matrix.\n-\n-![](https://i.imgur.com/aEj3GYi.png)\n-\n-### Point interactions\n-\n-The interaction between neighboring points often carries useful information (i.e., a\n-single point should not be treated in isolation). Whereas classification need only make\n-use of global features, segmentation must be able to leverage local point features along\n-with global point features.\n-\n-\n-**Note**: The figures presented in this section have been taken from the\n-[original paper](https://arxiv.org/abs/1612.00593).\n-\"\"\"\n-\n-\"\"\"\n-Now that we know the pieces that compose the PointNet model, we can implement the model.\n-We start by implementing the basic blocks i.e., the convolutional block and the multi-layer\n-perceptron block.\n-\"\"\"\n-\n-\n-def conv_block(x, filters, name):\n-    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=f\"{name}_conv\")(x)\n-    x = layers.BatchNormalization(name=f\"{name}_batch_norm\")(x)\n-    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n-\n-\n-def mlp_block(x, filters, name):\n-    x = layers.Dense(filters, name=f\"{name}_dense\")(x)\n-    x = layers.BatchNormalization(name=f\"{name}_batch_norm\")(x)\n-    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n-\n-\n-\"\"\"\n-We implement a regularizer (taken from\n-[this example](https://keras.io/examples/vision/pointnet/#build-a-model))\n-to enforce orthogonality in the feature space. This is needed to ensure\n-that the magnitudes of the transformed features do not vary too much.\n-\"\"\"\n-\n-\n-class OrthogonalRegularizer(keras.regularizers.Regularizer):\n-    \"\"\"Reference: https://keras.io/examples/vision/pointnet/#build-a-model\"\"\"\n-\n-    def __init__(self, num_features, l2reg=0.001):\n-        self.num_features = num_features\n-        self.l2reg = l2reg\n-        self.identity = keras.ops.eye(num_features)\n-\n-    def __call__(self, x):\n-        x = keras.ops.reshape(x, (-1, self.num_features, self.num_features))\n-        xxt = keras.ops.tensordot(x, x, axes=(2, 2))\n-        xxt = keras.ops.reshape(xxt, (-1, self.num_features, self.num_features))\n-        return keras.ops.sum(self.l2reg * keras.ops.square(xxt - self.identity))\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg})\n-        return config\n-\n-\n-\"\"\"\n-The next piece is the transformation network which we explained earlier.\n-\"\"\"\n-\n-\n-def transformation_net(inputs, num_features, name):\n-    \"\"\"\n-    Reference: https://keras.io/examples/vision/pointnet/#build-a-model.\n-\n-    The `filters` values come from the original paper:\n-    https://arxiv.org/abs/1612.00593.\n-    \"\"\"\n-    x = conv_block(inputs, filters=64, name=f\"{name}_1\")\n-    x = conv_block(x, filters=128, name=f\"{name}_2\")\n-    x = conv_block(x, filters=1024, name=f\"{name}_3\")\n-    x = layers.GlobalMaxPooling1D()(x)\n-    x = mlp_block(x, filters=512, name=f\"{name}_1_1\")\n-    x = mlp_block(x, filters=256, name=f\"{name}_2_1\")\n-    return layers.Dense(\n-        num_features * num_features,\n-        kernel_initializer=\"zeros\",\n-        bias_initializer=keras.initializers.Constant(np.eye(num_features).flatten()),\n-        activity_regularizer=OrthogonalRegularizer(num_features),\n-        name=f\"{name}_final\",\n-    )(x)\n-\n-\n-def transformation_block(inputs, num_features, name):\n-    transformed_features = transformation_net(inputs, num_features, name=name)\n-    transformed_features = layers.Reshape((num_features, num_features))(\n-        transformed_features\n-    )\n-    return layers.Dot(axes=(2, 1), name=f\"{name}_mm\")([inputs, transformed_features])\n-\n-\n-\"\"\"\n-Finally, we piece the above blocks together and implement the segmentation model.\n-\"\"\"\n-\n-\n-def get_shape_segmentation_model(num_points, num_classes):\n-    input_points = keras.Input(shape=(None, 3))\n-\n-    # PointNet Classification Network.\n-    transformed_inputs = transformation_block(\n-        input_points, num_features=3, name=\"input_transformation_block\"\n-    )\n-    features_64 = conv_block(transformed_inputs, filters=64, name=\"features_64\")\n-    features_128_1 = conv_block(features_64, filters=128, name=\"features_128_1\")\n-    features_128_2 = conv_block(features_128_1, filters=128, name=\"features_128_2\")\n-    transformed_features = transformation_block(\n-        features_128_2, num_features=128, name=\"transformed_features\"\n-    )\n-    features_512 = conv_block(transformed_features, filters=512, name=\"features_512\")\n-    features_2048 = conv_block(features_512, filters=2048, name=\"pre_maxpool_block\")\n-    global_features = layers.MaxPool1D(pool_size=num_points, name=\"global_features\")(\n-        features_2048\n-    )\n-    global_features = keras.ops.tile(global_features, [1, num_points, 1])\n-\n-    # Segmentation head.\n-    segmentation_input = layers.Concatenate(name=\"segmentation_input\")(\n-        [\n-            features_64,\n-            features_128_1,\n-            features_128_2,\n-            transformed_features,\n-            features_512,\n-            global_features,\n-        ]\n-    )\n-    segmentation_features = conv_block(\n-        segmentation_input, filters=128, name=\"segmentation_features\"\n-    )\n-    outputs = layers.Conv1D(\n-        num_classes, kernel_size=1, activation=\"softmax\", name=\"segmentation_head\"\n-    )(segmentation_features)\n-    return keras.Model(input_points, outputs)\n-\n-\n-\"\"\"\n-## Instantiate the model\n-\"\"\"\n-\n-x, y = next(iter(train_dataset))\n-\n-num_points = x.shape[1]\n-num_classes = y.shape[-1]\n-\n-segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n-segmentation_model.summary()\n-\n-\"\"\"\n-## Training\n-\n-For the training the authors recommend using a learning rate schedule that decays the\n-initial learning rate by half every 20 epochs. In this example, we use 5 epochs.\n-\"\"\"\n-\n-steps_per_epoch = total_training_examples // BATCH_SIZE\n-total_training_steps = steps_per_epoch * EPOCHS\n-print(f\"Steps per epoch: {steps_per_epoch}.\")\n-print(f\"Total training steps: {total_training_steps}.\")\n-\n-lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n-    initial_learning_rate=0.003, decay_steps=steps_per_epoch * 5, decay_rate=0.5, staircase=True\n-)\n-\n-steps = range(total_training_steps)\n-lrs = [lr_schedule(step) for step in steps]\n-\n-plt.plot(lrs)\n-plt.xlabel(\"Steps\")\n-plt.ylabel(\"Learning Rate\")\n-plt.show()\n-\n-\"\"\"\n-Finally, we implement a utility for running our experiments and launch model training.\n-\"\"\"\n-\n-\n-def run_experiment(epochs):\n-    segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n-    segmentation_model.compile(\n-        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n-        loss=keras.losses.CategoricalCrossentropy(),\n-        metrics=[\"accuracy\"],\n-    )\n-\n-    checkpoint_filepath = \"checkpoint.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_loss\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    history = segmentation_model.fit(\n-        train_dataset,\n-        validation_data=val_dataset,\n-        epochs=epochs,\n-        callbacks=[checkpoint_callback],\n-    )\n-\n-    segmentation_model.load_weights(checkpoint_filepath)\n-    return segmentation_model, history\n-\n-\n-segmentation_model, history = run_experiment(epochs=EPOCHS)\n-\n-\"\"\"\n-## Visualize the training landscape\n-\"\"\"\n-\n-\n-def plot_result(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_result(\"loss\")\n-plot_result(\"accuracy\")\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-validation_batch = next(iter(val_dataset))\n-val_predictions = segmentation_model.predict(validation_batch[0])\n-print(f\"Validation prediction shape: {val_predictions.shape}\")\n-\n-\n-def visualize_single_point_cloud(point_clouds, label_clouds, idx):\n-    label_map = LABELS + [\"none\"]\n-    point_cloud = point_clouds[idx]\n-    label_cloud = label_clouds[idx]\n-    visualize_data(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])\n-\n-\n-idx = np.random.choice(len(validation_batch[0]))\n-print(f\"Index selected: {idx}\")\n-\n-# Plotting with ground-truth.\n-visualize_single_point_cloud(validation_batch[0], validation_batch[1], idx)\n-\n-# Plotting with predicted labels.\n-visualize_single_point_cloud(validation_batch[0], val_predictions, idx)\n-\n-\"\"\"\n-## Final notes\n-\n-If you are interested in learning more about this topic, you may find\n-[this repository](https://github.com/soumik12345/point-cloud-segmentation)\n-useful.\n-\"\"\"\n\n@@ -1,423 +0,0 @@\n-\"\"\"\n-Title: Image similarity estimation using a Siamese Network with a contrastive loss\n-Author: Mehdi\n-Date created: 2021/05/06\n-Last modified: 2022/09/10\n-Description: Similarity learning using a siamese network trained with a contrastive loss.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-[Siamese Networks](https://en.wikipedia.org/wiki/Siamese_neural_network)\n-are neural networks which share weights between two or more sister networks,\n-each producing embedding vectors of its respective inputs.\n-\n-In supervised similarity learning, the networks are then trained to maximize the\n-contrast (distance) between embeddings of inputs of different classes, while minimizing the distance between\n-embeddings of similar classes, resulting in embedding spaces that reflect\n-the class segmentation of the training inputs.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import random\n-import numpy as np\n-import keras\n-from keras import ops\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Hyperparameters\n-\"\"\"\n-\n-epochs = 10\n-batch_size = 16\n-margin = 1  # Margin for contrastive loss.\n-\n-\"\"\"\n-## Load the MNIST dataset\n-\"\"\"\n-(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.mnist.load_data()\n-\n-# Change the data type to a floating point format\n-x_train_val = x_train_val.astype(\"float32\")\n-x_test = x_test.astype(\"float32\")\n-\n-\n-\"\"\"\n-## Define training and validation sets\n-\"\"\"\n-\n-# Keep 50% of train_val  in validation set\n-x_train, x_val = x_train_val[:30000], x_train_val[30000:]\n-y_train, y_val = y_train_val[:30000], y_train_val[30000:]\n-del x_train_val, y_train_val\n-\n-\n-\"\"\"\n-## Create pairs of images\n-\n-We will train the model to differentiate between digits of different classes. For\n-example, digit `0` needs to be differentiated from the rest of the\n-digits (`1` through `9`), digit `1` - from `0` and `2` through `9`, and so on.\n-To carry this out, we will select N random images from class A (for example,\n-for digit `0`) and pair them with N random images from another class B\n-(for example, for digit `1`). Then, we can repeat this process for all classes\n-of digits (until digit `9`). Once we have paired digit `0` with other digits,\n-we can repeat this process for the remaining classes for the rest of the digits\n-(from `1` until `9`).\n-\"\"\"\n-\n-\n-def make_pairs(x, y):\n-    \"\"\"Creates a tuple containing image pairs with corresponding label.\n-\n-    Arguments:\n-        x: List containing images, each index in this list corresponds to one image.\n-        y: List containing labels, each label with datatype of `int`.\n-\n-    Returns:\n-        Tuple containing two numpy arrays as (pairs_of_samples, labels),\n-        where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n-        labels are a binary array of shape (2len(x)).\n-    \"\"\"\n-\n-    num_classes = max(y) + 1\n-    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n-\n-    pairs = []\n-    labels = []\n-\n-    for idx1 in range(len(x)):\n-        # add a matching example\n-        x1 = x[idx1]\n-        label1 = y[idx1]\n-        idx2 = random.choice(digit_indices[label1])\n-        x2 = x[idx2]\n-\n-        pairs += [[x1, x2]]\n-        labels += [0]\n-\n-        # add a non-matching example\n-        label2 = random.randint(0, num_classes - 1)\n-        while label2 == label1:\n-            label2 = random.randint(0, num_classes - 1)\n-\n-        idx2 = random.choice(digit_indices[label2])\n-        x2 = x[idx2]\n-\n-        pairs += [[x1, x2]]\n-        labels += [1]\n-\n-    return np.array(pairs), np.array(labels).astype(\"float32\")\n-\n-\n-# make train pairs\n-pairs_train, labels_train = make_pairs(x_train, y_train)\n-\n-# make validation pairs\n-pairs_val, labels_val = make_pairs(x_val, y_val)\n-\n-# make test pairs\n-pairs_test, labels_test = make_pairs(x_test, y_test)\n-\n-\"\"\"\n-We get:\n-\n-**pairs_train.shape = (60000, 2, 28, 28)**\n-\n-- We have 60,000 pairs\n-- Each pair contains 2 images\n-- Each image has shape `(28, 28)`\n-\"\"\"\n-\n-\"\"\"\n-Split the training pairs\n-\"\"\"\n-\n-x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28)\n-x_train_2 = pairs_train[:, 1]\n-\n-\"\"\"\n-Split the validation pairs\n-\"\"\"\n-\n-x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28)\n-x_val_2 = pairs_val[:, 1]\n-\n-\"\"\"\n-Split the test pairs\n-\"\"\"\n-\n-x_test_1 = pairs_test[:, 0]  # x_test_1.shape = (20000, 28, 28)\n-x_test_2 = pairs_test[:, 1]\n-\n-\n-\"\"\"\n-## Visualize pairs and their labels\n-\"\"\"\n-\n-\n-def visualize(\n-    pairs, labels, to_show=6, num_col=3, predictions=None, test=False\n-):\n-    \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n-\n-    Arguments:\n-        pairs: Numpy Array, of pairs to visualize, having shape\n-               (Number of pairs, 2, 28, 28).\n-        to_show: Int, number of examples to visualize (default is 6)\n-                `to_show` must be an integral multiple of `num_col`.\n-                 Otherwise it will be trimmed if it is greater than num_col,\n-                 and incremented if if it is less then num_col.\n-        num_col: Int, number of images in one row - (default is 3)\n-                 For test and train respectively, it should not exceed 3 and 7.\n-        predictions: Numpy Array of predictions with shape (to_show, 1) -\n-                     (default is None)\n-                     Must be passed when test=True.\n-        test: Boolean telling whether the dataset being visualized is\n-              train dataset or test dataset - (default False).\n-\n-    Returns:\n-        None.\n-    \"\"\"\n-\n-    # Define num_row\n-    # If to_show % num_col != 0\n-    #    trim to_show,\n-    #       to trim to_show limit num_row to the point where\n-    #       to_show % num_col == 0\n-    #\n-    # If to_show//num_col == 0\n-    #    then it means num_col is greater then to_show\n-    #    increment to_show\n-    #       to increment to_show set num_row to 1\n-    num_row = to_show // num_col if to_show // num_col != 0 else 1\n-\n-    # `to_show` must be an integral multiple of `num_col`\n-    #  we found num_row and we have num_col\n-    #  to increment or decrement to_show\n-    #  to make it integral multiple of `num_col`\n-    #  simply set it equal to num_row * num_col\n-    to_show = num_row * num_col\n-\n-    # Plot the images\n-    fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))\n-    for i in range(to_show):\n-        # If the number of rows is 1, the axes array is one-dimensional\n-        if num_row == 1:\n-            ax = axes[i % num_col]\n-        else:\n-            ax = axes[i // num_col, i % num_col]\n-\n-        ax.imshow(\n-            ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\"\n-        )\n-        ax.set_axis_off()\n-        if test:\n-            ax.set_title(\n-                \"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0])\n-            )\n-        else:\n-            ax.set_title(\"Label: {}\".format(labels[i]))\n-    if test:\n-        plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)\n-    else:\n-        plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n-    plt.show()\n-\n-\n-\"\"\"\n-Inspect training pairs\n-\"\"\"\n-\n-visualize(pairs_train[:-1], labels_train[:-1], to_show=4, num_col=4)\n-\n-\"\"\"\n-Inspect validation pairs\n-\"\"\"\n-\n-visualize(pairs_val[:-1], labels_val[:-1], to_show=4, num_col=4)\n-\n-\"\"\"\n-Inspect test pairs\n-\"\"\"\n-\n-visualize(pairs_test[:-1], labels_test[:-1], to_show=4, num_col=4)\n-\n-\"\"\"\n-## Define the model\n-\n-There are two input layers, each leading to its own network, which\n-produces embeddings. A `Lambda` layer then merges them using an\n-[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) and the\n-merged output is fed to the final network.\n-\"\"\"\n-\n-\n-# Provided two tensors t1 and t2\n-# Euclidean distance = sqrt(sum(square(t1-t2)))\n-def euclidean_distance(vects):\n-    \"\"\"Find the Euclidean distance between two vectors.\n-\n-    Arguments:\n-        vects: List containing two tensors of same length.\n-\n-    Returns:\n-        Tensor containing euclidean distance\n-        (as floating point value) between vectors.\n-    \"\"\"\n-\n-    x, y = vects\n-    sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)\n-    return ops.sqrt(ops.maximum(sum_square, keras.backend.epsilon()))\n-\n-\n-input = keras.layers.Input((28, 28, 1))\n-x = keras.layers.BatchNormalization()(input)\n-x = keras.layers.Conv2D(4, (5, 5), activation=\"tanh\")(x)\n-x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n-x = keras.layers.Conv2D(16, (5, 5), activation=\"tanh\")(x)\n-x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n-x = keras.layers.Flatten()(x)\n-\n-x = keras.layers.BatchNormalization()(x)\n-x = keras.layers.Dense(10, activation=\"tanh\")(x)\n-embedding_network = keras.Model(input, x)\n-\n-\n-input_1 = keras.layers.Input((28, 28, 1))\n-input_2 = keras.layers.Input((28, 28, 1))\n-\n-# As mentioned above, Siamese Network share weights between\n-# tower networks (sister networks). To allow this, we will use\n-# same embedding network for both tower networks.\n-tower_1 = embedding_network(input_1)\n-tower_2 = embedding_network(input_2)\n-\n-merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))(\n-    [tower_1, tower_2]\n-)\n-normal_layer = keras.layers.BatchNormalization()(merge_layer)\n-output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n-siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n-\n-\n-\"\"\"\n-## Define the contrastive Loss\n-\"\"\"\n-\n-\n-def loss(margin=1):\n-    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n-\n-    Arguments:\n-        margin: Integer, defines the baseline for distance for which pairs\n-                should be classified as dissimilar. - (default is 1).\n-\n-    Returns:\n-        'contrastive_loss' function with data ('margin') attached.\n-    \"\"\"\n-\n-    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n-    #                         true_value * square( max(margin-prediction, 0) ))\n-    def contrastive_loss(y_true, y_pred):\n-        \"\"\"Calculates the contrastive loss.\n-\n-        Arguments:\n-            y_true: List of labels, each label is of type float32.\n-            y_pred: List of predictions of same length as of y_true,\n-                    each label is of type float32.\n-\n-        Returns:\n-            A tensor containing contrastive loss as floating point value.\n-        \"\"\"\n-\n-        square_pred = ops.square(y_pred)\n-        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n-        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n-\n-    return contrastive_loss\n-\n-\n-\"\"\"\n-## Compile the model with the contrastive loss\n-\"\"\"\n-\n-siamese.compile(\n-    loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"]\n-)\n-siamese.summary()\n-\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-history = siamese.fit(\n-    [x_train_1, x_train_2],\n-    labels_train,\n-    validation_data=([x_val_1, x_val_2], labels_val),\n-    batch_size=batch_size,\n-    epochs=epochs,\n-)\n-\n-\"\"\"\n-## Visualize results\n-\"\"\"\n-\n-\n-def plt_metric(history, metric, title, has_valid=True):\n-    \"\"\"Plots the given 'metric' from 'history'.\n-\n-    Arguments:\n-        history: history attribute of History object returned from Model.fit.\n-        metric: Metric to plot, a string value present as key in 'history'.\n-        title: A string to be used as title of plot.\n-        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n-\n-    Returns:\n-        None.\n-    \"\"\"\n-    plt.plot(history[metric])\n-    if has_valid:\n-        plt.plot(history[\"val_\" + metric])\n-        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n-    plt.title(title)\n-    plt.ylabel(metric)\n-    plt.xlabel(\"epoch\")\n-    plt.show()\n-\n-\n-# Plot the accuracy\n-plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n-\n-# Plot the contrastive loss\n-plt_metric(history=history.history, metric=\"loss\", title=\"Contrastive Loss\")\n-\n-\"\"\"\n-## Evaluate the model\n-\"\"\"\n-\n-results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n-print(\"test loss, test acc:\", results)\n-\n-\"\"\"\n-## Visualize the predictions\n-\"\"\"\n-\n-predictions = siamese.predict([x_test_1, x_test_2])\n-visualize(\n-    pairs_test, labels_test, to_show=3, predictions=predictions, test=True\n-)\n-\n-\"\"\"\n-**Example available on HuggingFace**\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Siamese%20Network-black.svg)](https://huggingface.co/keras-io/siamese-contrastive) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Siamese%20Network-black.svg)](https://huggingface.co/spaces/keras-io/siamese-contrastive) |\n-\"\"\"\n\n@@ -1,407 +0,0 @@\n-\"\"\"\n-Title: Image Super-Resolution using an Efficient Sub-Pixel CNN\n-Author: [Xingyu Long](https://github.com/xingyu-long)\n-Date created: 2020/07/28\n-Last modified: 2020/08/27\n-Description: Implementing Super-Resolution using Efficient sub-pixel model on BSDS500.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Md Awsfalur Rahman](https://awsaf49.github.io)\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-ESPCN (Efficient Sub-Pixel CNN), proposed by [Shi, 2016](https://arxiv.org/abs/1609.05158)\n-is a model that reconstructs a high-resolution version of an image given a low-resolution\n-version.\n-It leverages efficient \"sub-pixel convolution\" layers, which learns an array of\n-image upscaling filters.\n-\n-In this code example, we will implement the model from the paper and train it on a small\n-dataset,\n-[BSDS500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html).\n-[BSDS500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-from keras.utils import load_img\n-from keras.utils import array_to_img\n-from keras.utils import img_to_array\n-from keras.preprocessing import image_dataset_from_directory\n-import tensorflow as tf  #  only for data preprocessing\n-\n-import os\n-import math\n-import numpy as np\n-\n-from IPython.display import display\n-\n-\"\"\"\n-## Load data: BSDS500 dataset\n-\n-### Download dataset\n-\n-We use the built-in `keras.utils.get_file` utility to retrieve the dataset.\n-\"\"\"\n-\n-dataset_url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\"\n-data_dir = keras.utils.get_file(origin=dataset_url, fname=\"BSR\", untar=True)\n-root_dir = os.path.join(data_dir, \"BSDS500/data\")\n-\n-\"\"\"\n-We create training and validation datasets via `image_dataset_from_directory`.\n-\"\"\"\n-\n-crop_size = 300\n-upscale_factor = 3\n-input_size = crop_size // upscale_factor\n-batch_size = 8\n-\n-train_ds = image_dataset_from_directory(\n-    root_dir,\n-    batch_size=batch_size,\n-    image_size=(crop_size, crop_size),\n-    validation_split=0.2,\n-    subset=\"training\",\n-    seed=1337,\n-    label_mode=None,\n-)\n-\n-valid_ds = image_dataset_from_directory(\n-    root_dir,\n-    batch_size=batch_size,\n-    image_size=(crop_size, crop_size),\n-    validation_split=0.2,\n-    subset=\"validation\",\n-    seed=1337,\n-    label_mode=None,\n-)\n-\n-\"\"\"\n-We rescale the images to take values in the range [0, 1].\n-\"\"\"\n-\n-\n-def scaling(input_image):\n-    input_image = input_image / 255.0\n-    return input_image\n-\n-\n-# Scale from (0, 255) to (0, 1)\n-train_ds = train_ds.map(scaling)\n-valid_ds = valid_ds.map(scaling)\n-\n-\"\"\"\n-Let's visualize a few sample images:\n-\"\"\"\n-\n-for batch in train_ds.take(1):\n-    for img in batch:\n-        display(array_to_img(img))\n-\n-\"\"\"\n-We prepare a dataset of test image paths that we will use for\n-visual evaluation at the end of this example.\n-\"\"\"\n-\n-dataset = os.path.join(root_dir, \"images\")\n-test_path = os.path.join(dataset, \"test\")\n-\n-test_img_paths = sorted(\n-    [\n-        os.path.join(test_path, fname)\n-        for fname in os.listdir(test_path)\n-        if fname.endswith(\".jpg\")\n-    ]\n-)\n-\n-\"\"\"\n-## Crop and resize images\n-\n-Let's process image data.\n-First, we convert our images from the RGB color space to the\n-[YUV colour space](https://en.wikipedia.org/wiki/YUV).\n-\n-For the input data (low-resolution images),\n-we crop the image, retrieve the `y` channel (luninance),\n-and resize it with the `area` method (use `BICUBIC` if you use PIL).\n-We only consider the luminance channel\n-in the YUV color space because humans are more sensitive to\n-luminance change.\n-\n-For the target data (high-resolution images), we just crop the image\n-and retrieve the `y` channel.\n-\"\"\"\n-\n-# Use TF Ops to process.\n-def process_input(input, input_size, upscale_factor):\n-    input = tf.image.rgb_to_yuv(input)\n-    last_dimension_axis = len(input.shape) - 1\n-    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n-    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n-\n-\n-def process_target(input):\n-    input = tf.image.rgb_to_yuv(input)\n-    last_dimension_axis = len(input.shape) - 1\n-    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n-    return y\n-\n-\n-train_ds = train_ds.map(\n-    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n-)\n-train_ds = train_ds.prefetch(buffer_size=32)\n-\n-valid_ds = valid_ds.map(\n-    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n-)\n-valid_ds = valid_ds.prefetch(buffer_size=32)\n-\n-\"\"\"\n-Let's take a look at the input and target data.\n-\"\"\"\n-\n-for batch in train_ds.take(1):\n-    for img in batch[0]:\n-        display(array_to_img(img))\n-    for img in batch[1]:\n-        display(array_to_img(img))\n-\n-\"\"\"\n-## Build a model\n-\n-Compared to the paper, we add one more layer and we use the `relu` activation function\n-instead of `tanh`.\n-It achieves better performance even though we train the model for fewer epochs.\n-\"\"\"\n-\n-\n-class DepthToSpace(layers.Layer):\n-    def __init__(self, block_size):\n-        super().__init__()\n-        self.block_size = block_size\n-\n-    def call(self, input):\n-        batch, height, width, depth = ops.shape(input)\n-        depth = depth // (self.block_size**2)\n-\n-        x = ops.reshape(\n-            input, [batch, height, width, self.block_size, self.block_size, depth]\n-        )\n-        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n-        x = ops.reshape(\n-            x, [batch, height * self.block_size, width * self.block_size, depth]\n-        )\n-        return x\n-\n-\n-def get_model(upscale_factor=3, channels=1):\n-    conv_args = {\n-        \"activation\": \"relu\",\n-        \"kernel_initializer\": \"orthogonal\",\n-        \"padding\": \"same\",\n-    }\n-    inputs = keras.Input(shape=(None, None, channels))\n-    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n-    x = layers.Conv2D(64, 3, **conv_args)(x)\n-    x = layers.Conv2D(32, 3, **conv_args)(x)\n-    x = layers.Conv2D(channels * (upscale_factor**2), 3, **conv_args)(x)\n-    outputs = DepthToSpace(upscale_factor)(x)\n-\n-    return keras.Model(inputs, outputs)\n-\n-\n-\"\"\"\n-## Define utility functions\n-\n-We need to define several utility functions to monitor our results:\n-\n-- `plot_results` to plot an save an image.\n-- `get_lowres_image` to convert an image to its low-resolution version.\n-- `upscale_image` to turn a low-resolution image to\n-a high-resolution version reconstructed by the model.\n-In this function, we use the `y` channel from the YUV color space\n-as input to the model and then combine the output with the\n-other channels to obtain an RGB image.\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n-from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n-import PIL\n-\n-\n-def plot_results(img, prefix, title):\n-    \"\"\"Plot the result with zoom-in area.\"\"\"\n-    img_array = img_to_array(img)\n-    img_array = img_array.astype(\"float32\") / 255.0\n-\n-    # Create a new figure with a default 111 subplot.\n-    fig, ax = plt.subplots()\n-    im = ax.imshow(img_array[::-1], origin=\"lower\")\n-\n-    plt.title(title)\n-    # zoom-factor: 2.0, location: upper-left\n-    axins = zoomed_inset_axes(ax, 2, loc=2)\n-    axins.imshow(img_array[::-1], origin=\"lower\")\n-\n-    # Specify the limits.\n-    x1, x2, y1, y2 = 200, 300, 100, 200\n-    # Apply the x-limits.\n-    axins.set_xlim(x1, x2)\n-    # Apply the y-limits.\n-    axins.set_ylim(y1, y2)\n-\n-    plt.yticks(visible=False)\n-    plt.xticks(visible=False)\n-\n-    # Make the line.\n-    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n-    plt.savefig(str(prefix) + \"-\" + title + \".png\")\n-    plt.show()\n-\n-\n-def get_lowres_image(img, upscale_factor):\n-    \"\"\"Return low-resolution image to use as model input.\"\"\"\n-    return img.resize(\n-        (img.size[0] // upscale_factor, img.size[1] // upscale_factor),\n-        PIL.Image.BICUBIC,\n-    )\n-\n-\n-def upscale_image(model, img):\n-    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n-    ycbcr = img.convert(\"YCbCr\")\n-    y, cb, cr = ycbcr.split()\n-    y = img_to_array(y)\n-    y = y.astype(\"float32\") / 255.0\n-\n-    input = np.expand_dims(y, axis=0)\n-    out = model.predict(input)\n-\n-    out_img_y = out[0]\n-    out_img_y *= 255.0\n-\n-    # Restore the image in RGB color space.\n-    out_img_y = out_img_y.clip(0, 255)\n-    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n-    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n-    out_img_cb = cb.resize(out_img_y.size, PIL.Image.BICUBIC)\n-    out_img_cr = cr.resize(out_img_y.size, PIL.Image.BICUBIC)\n-    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n-        \"RGB\"\n-    )\n-    return out_img\n-\n-\n-\"\"\"\n-## Define callbacks to monitor training\n-\n-The `ESPCNCallback` object will compute and display\n-the [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) metric.\n-This is the main metric we use to evaluate super-resolution performance.\n-\"\"\"\n-\n-\n-class ESPCNCallback(keras.callbacks.Callback):\n-    def __init__(self):\n-        super().__init__()\n-        self.test_img = get_lowres_image(load_img(test_img_paths[0]), upscale_factor)\n-\n-    # Store PSNR value in each epoch.\n-    def on_epoch_begin(self, epoch, logs=None):\n-        self.psnr = []\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n-        if epoch % 20 == 0:\n-            prediction = upscale_image(self.model, self.test_img)\n-            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")\n-\n-    def on_test_batch_end(self, batch, logs=None):\n-        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n-\n-\n-\"\"\"\n-Define `ModelCheckpoint` and `EarlyStopping` callbacks.\n-\"\"\"\n-\n-early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n-\n-checkpoint_filepath = \"/tmp/checkpoint.keras\"\n-\n-model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-    filepath=checkpoint_filepath,\n-    save_weights_only=False,\n-    monitor=\"loss\",\n-    mode=\"min\",\n-    save_best_only=True,\n-)\n-\n-model = get_model(upscale_factor=upscale_factor, channels=1)\n-model.summary()\n-\n-callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback]\n-loss_fn = keras.losses.MeanSquaredError()\n-optimizer = keras.optimizers.Adam(learning_rate=0.001)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-epochs = 100\n-\n-model.compile(\n-    optimizer=optimizer,\n-    loss=loss_fn,\n-)\n-\n-model.fit(\n-    train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, verbose=2\n-)\n-\n-# The model weights (that are considered the best) are loaded into the model.\n-model.load_weights(checkpoint_filepath)\n-\n-\"\"\"\n-## Run model prediction and plot the results\n-\n-Let's compute the reconstructed version of a few images and save the results.\n-\"\"\"\n-\n-total_bicubic_psnr = 0.0\n-total_test_psnr = 0.0\n-\n-for index, test_img_path in enumerate(test_img_paths[50:60]):\n-    img = load_img(test_img_path)\n-    lowres_input = get_lowres_image(img, upscale_factor)\n-    w = lowres_input.size[0] * upscale_factor\n-    h = lowres_input.size[1] * upscale_factor\n-    highres_img = img.resize((w, h))\n-    prediction = upscale_image(model, lowres_input)\n-    lowres_img = lowres_input.resize((w, h))\n-    lowres_img_arr = img_to_array(lowres_img)\n-    highres_img_arr = img_to_array(highres_img)\n-    predict_img_arr = img_to_array(prediction)\n-    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n-    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n-\n-    total_bicubic_psnr += bicubic_psnr\n-    total_test_psnr += test_psnr\n-\n-    print(\n-        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n-    )\n-    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n-    plot_results(lowres_img, index, \"lowres\")\n-    plot_results(highres_img, index, \"highres\")\n-    plot_results(prediction, index, \"prediction\")\n-\n-print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n-print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))\n\n@@ -1,274 +0,0 @@\n-\"\"\"\n-Title: Supervised Contrastive Learning\n-Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Date created: 2020/11/30\n-Last modified: 2020/11/30\n-Description: Using supervised contrastive learning for image classification.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n-(Prannay Khosla et al.) is a training methodology that outperforms\n-supervised training with crossentropy on classification tasks.\n-\n-Essentially, training an image classification model with Supervised Contrastive\n-Learning is performed in two phases:\n-\n-1. Training an encoder to learn to produce vector representations of input images such\n-that representations of images in the same class will be more similar compared to\n-representations of images in different classes.\n-2. Training a classifier on top of the frozen encoder.\n-\"\"\"\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras\n-from keras import ops\n-from keras import layers\n-from keras.applications.resnet_v2 import ResNet50V2\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 10\n-input_shape = (32, 32, 3)\n-\n-# Load the train and test data splits\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-\n-# Display shapes of train and test datasets\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\n-\"\"\"\n-## Using image data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(0.02),\n-    ]\n-)\n-\n-# Setting the state of the normalization layer.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\"\"\"\n-## Build the encoder model\n-\n-The encoder model takes the image as input and turns it into a 2048-dimensional\n-feature vector.\n-\"\"\"\n-\n-\n-def create_encoder():\n-    resnet = ResNet50V2(\n-        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n-    )\n-\n-    inputs = keras.Input(shape=input_shape)\n-    augmented = data_augmentation(inputs)\n-    outputs = resnet(augmented)\n-    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n-    return model\n-\n-\n-encoder = create_encoder()\n-encoder.summary()\n-\n-learning_rate = 0.001\n-batch_size = 265\n-hidden_units = 512\n-projection_units = 128\n-num_epochs = 50\n-dropout_rate = 0.5\n-temperature = 0.05\n-\n-\"\"\"\n-## Build the classification model\n-\n-The classification model adds a fully-connected layer on top of the encoder,\n-plus a softmax layer with the target classes.\n-\"\"\"\n-\n-\n-def create_classifier(encoder, trainable=True):\n-    for layer in encoder.layers:\n-        layer.trainable = trainable\n-\n-    inputs = keras.Input(shape=input_shape)\n-    features = encoder(inputs)\n-    features = layers.Dropout(dropout_rate)(features)\n-    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n-    features = layers.Dropout(dropout_rate)(features)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n-\n-    model = keras.Model(\n-        inputs=inputs, outputs=outputs, name=\"cifar10-classifier\"\n-    )\n-    model.compile(\n-        optimizer=keras.optimizers.Adam(learning_rate),\n-        loss=keras.losses.SparseCategoricalCrossentropy(),\n-        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-    )\n-    return model\n-\n-\n-\"\"\"\n-## Define npairs loss function\n-\"\"\"\n-\n-\n-def npairs_loss(y_true, y_pred):\n-    \"\"\"Computes the npairs loss between `y_true` and `y_pred`.\n-\n-    Npairs loss expects paired data where a pair is composed of samples from\n-    the same labels and each pairs in the minibatch have different labels.\n-    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n-    as logits and the remapped multi-class labels, `y_true`, as labels.\n-\n-\n-    See:\n-    http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n-\n-    Args:\n-      y_true: Ground truth values, of shape `[batch_size]` of multi-class\n-        labels.\n-      y_pred: Predicted values of shape `[batch_size, batch_size]` of\n-        similarity matrix between embedding matrices.\n-\n-    Returns:\n-      npairs_loss: float scalar.\n-    \"\"\"\n-    y_pred = ops.cast(y_pred, \"float32\")\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-    y_true = ops.cast(ops.equal(y_true, ops.transpose(y_true)), y_pred.dtype)\n-    y_true /= ops.sum(y_true, 1, keepdims=True)\n-    loss = ops.categorical_crossentropy(y_true, y_pred, from_logits=True)\n-\n-    return ops.mean(loss)\n-\n-\n-\"\"\"\n-## Experiment 1: Train the baseline classification model\n-\n-In this experiment, a baseline classifier is trained as usual, i.e., the\n-encoder and the classifier parts are trained together as a single model\n-to minimize the crossentropy loss.\n-\"\"\"\n-\n-encoder = create_encoder()\n-classifier = create_classifier(encoder)\n-classifier.summary()\n-\n-history = classifier.fit(\n-    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n-)\n-\n-accuracy = classifier.evaluate(x_test, y_test)[1]\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-\n-\"\"\"\n-## Experiment 2: Use supervised contrastive learning\n-\n-In this experiment, the model is trained in two phases. In the first phase,\n-the encoder is pretrained to optimize the supervised contrastive loss,\n-described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n-\n-In the second phase, the classifier is trained using the trained encoder with\n-its weights freezed; only the weights of fully-connected layers with the\n-softmax are optimized.\n-\n-### 1. Supervised contrastive learning loss function\n-\"\"\"\n-\n-\n-class SupervisedContrastiveLoss(keras.losses.Loss):\n-    def __init__(self, temperature=1, name=None):\n-        super().__init__(name=name)\n-        self.temperature = temperature\n-\n-    def __call__(self, labels, feature_vectors, sample_weight=None):\n-        # Normalize feature vectors\n-        feature_vectors_normalized = keras.utils.normalize(\n-            feature_vectors, axis=1, order=2\n-        )\n-        # Compute logits\n-        logits = ops.divide(\n-            ops.matmul(\n-                feature_vectors_normalized,\n-                ops.transpose(feature_vectors_normalized),\n-            ),\n-            self.temperature,\n-        )\n-        return npairs_loss(ops.squeeze(labels), logits)\n-\n-\n-def add_projection_head(encoder):\n-    inputs = keras.Input(shape=input_shape)\n-    features = encoder(inputs)\n-    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n-    model = keras.Model(\n-        inputs=inputs,\n-        outputs=outputs,\n-        name=\"cifar-encoder_with_projection-head\",\n-    )\n-    return model\n-\n-\n-\"\"\"\n-### 2. Pretrain the encoder\n-\"\"\"\n-\n-encoder = create_encoder()\n-\n-encoder_with_projection_head = add_projection_head(encoder)\n-encoder_with_projection_head.compile(\n-    optimizer=keras.optimizers.Adam(learning_rate),\n-    loss=SupervisedContrastiveLoss(temperature),\n-)\n-\n-encoder_with_projection_head.summary()\n-\n-history = encoder_with_projection_head.fit(\n-    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n-)\n-\n-\"\"\"\n-### 3. Train the classifier with the frozen encoder\n-\"\"\"\n-\n-classifier = create_classifier(encoder, trainable=False)\n-\n-history = classifier.fit(\n-    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n-)\n-\n-accuracy = classifier.evaluate(x_test, y_test)[1]\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-\"\"\"\n-We get to an improved test accuracy.\n-\"\"\"\n-\n-\"\"\"\n-## Conclusion\n-\n-As shown in the experiments, using the supervised contrastive learning technique\n-outperformed the conventional technique in terms of the test accuracy. Note that\n-the same training budget (i.e., number of epochs) was given to each technique.\n-Supervised contrastive learning pays off when the encoder involves a complex\n-architecture, like ResNet, and multi-class problems with many labels.\n-In addition, large batch sizes and multi-layer projection heads\n-improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n-paper for more details.\n-\n-\"\"\"\n\n@@ -1,529 +0,0 @@\n-\"\"\"\n-Title: Learning to tokenize in Vision Transformers\n-Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n-Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 2021/12/10\n-Last modified: 2023/08/14\n-Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-Vision Transformers ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)) and many\n-other Transformer-based architectures ([Liu et al.](https://arxiv.org/abs/2103.14030),\n-[Yuan et al.](https://arxiv.org/abs/2101.11986), etc.) have shown strong results in\n-image recognition. The following provides a brief overview of the components involved in the\n-Vision Transformer architecture for image classification:\n-\n-* Extract small patches from input images.\n-* Linearly project those patches.\n-* Add positional embeddings to these linear projections.\n-* Run these projections through a series of Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n-blocks.\n-* Finally, take the representation from the final Transformer block and add a\n-classification head.\n-\n-If we take 224x224 images and extract 16x16 patches, we get a total of 196 patches (also\n-called tokens) for each image. The number of patches increases as we increase the\n-resolution, leading to higher memory footprint. Could we use a reduced\n-number of patches without having to compromise performance?\n-Ryoo et al. investigate this question in\n-[TokenLearner: Adaptive Space-Time Tokenization for Videos](https://openreview.net/forum?id=z-l1kpDXs88).\n-They introduce a novel module called **TokenLearner** that can help reduce the number\n-of patches used by a Vision Transformer (ViT) in an adaptive manner. With TokenLearner\n-incorporated in the standard ViT architecture, they are able to reduce the amount of\n-compute (measured in FLOPS) used by the model.\n-\n-In this example, we implement the TokenLearner module and demonstrate its\n-performance with a mini ViT and the CIFAR-10 dataset. We make use of the following\n-references:\n-\n-* [Official TokenLearner code](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py)\n-* [Image Classification with ViTs on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n-* [TokenLearner slides from NeurIPS 2021](https://nips.cc/media/neurips-2021/Slides/26578.pdf)\n-\"\"\"\n-\n-\n-\"\"\"\n-## Imports\n-\"\"\"\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-from tensorflow import data as tf_data\n-\n-\n-from datetime import datetime\n-import matplotlib.pyplot as plt\n-import numpy as np\n-\n-import math\n-\n-\"\"\"\n-## Hyperparameters\n-\n-Please feel free to change the hyperparameters and check your results. The best way to\n-develop intuition about the architecture is to experiment with it.\n-\"\"\"\n-\n-# DATA\n-BATCH_SIZE = 256\n-AUTO = tf_data.AUTOTUNE\n-INPUT_SHAPE = (32, 32, 3)\n-NUM_CLASSES = 10\n-\n-# OPTIMIZER\n-LEARNING_RATE = 1e-3\n-WEIGHT_DECAY = 1e-4\n-\n-# TRAINING\n-EPOCHS = 20\n-\n-# AUGMENTATION\n-IMAGE_SIZE = 48  # We will resize input images to this size.\n-PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n-NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n-\n-# ViT ARCHITECTURE\n-LAYER_NORM_EPS = 1e-6\n-PROJECTION_DIM = 128\n-NUM_HEADS = 4\n-NUM_LAYERS = 4\n-MLP_UNITS = [\n-    PROJECTION_DIM * 2,\n-    PROJECTION_DIM,\n-]\n-\n-# TOKENLEARNER\n-NUM_TOKENS = 4\n-\n-\"\"\"\n-## Load and prepare the CIFAR-10 dataset\n-\"\"\"\n-\n-# Load the CIFAR-10 dataset.\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n-(x_train, y_train), (x_val, y_val) = (\n-    (x_train[:40000], y_train[:40000]),\n-    (x_train[40000:], y_train[40000:]),\n-)\n-print(f\"Training samples: {len(x_train)}\")\n-print(f\"Validation samples: {len(x_val)}\")\n-print(f\"Testing samples: {len(x_test)}\")\n-\n-# Convert to tf.data.Dataset objects.\n-train_ds = tf_data.Dataset.from_tensor_slices((x_train, y_train))\n-train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)\n-\n-val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val))\n-val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n-\n-test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))\n-test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n-\n-\"\"\"\n-## Data augmentation\n-\n-The augmentation pipeline consists of:\n-\n-- Rescaling\n-- Resizing\n-- Random cropping (fixed-sized or random sized)\n-- Random horizontal flipping\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Rescaling(1 / 255.0),\n-        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n-        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n-        layers.RandomFlip(\"horizontal\"),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-\n-\"\"\"\n-Note that image data augmentation layers do not apply data transformations at inference time.\n-This means that when these layers are called with `training=False` they behave differently. Refer\n-[to the documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) for more\n-details.\n-\"\"\"\n-\n-\"\"\"\n-## Positional embedding module\n-\n-A [Transformer](https://arxiv.org/abs/1706.03762) architecture consists of **multi-head\n-self attention** layers and **fully-connected feed forward** networks (MLP) as the main\n-components. Both these components are _permutation invariant_: they're not aware of\n-feature order.\n-\n-To overcome this problem we inject tokens with positional information. The\n-`position_embedding` function adds this positional information to the linearly projected\n-tokens.\n-\"\"\"\n-\n-\n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=projection_dim\n-        )\n-\n-    def call(self, patch):\n-        positions = ops.expand_dims(\n-            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n-        )\n-        encoded = patch + self.position_embedding(positions)\n-        return encoded\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"num_patches\": self.num_patches})\n-        return config\n-\n-\n-\"\"\"\n-## MLP block for Transformer\n-\n-This serves as the Fully Connected Feed Forward block for our Transformer.\n-\"\"\"\n-\n-\n-def mlp(x, dropout_rate, hidden_units):\n-    # Iterate over the hidden units and\n-    # add Dense => Dropout.\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=ops.gelu)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## TokenLearner module\n-\n-The following figure presents a pictorial overview of the module\n-([source](https://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html)).\n-\n-![TokenLearner module GIF](https://blogger.googleusercontent.com/img/a/AVvXsEiylT3_nmd9-tzTnz3g3Vb4eTn-L5sOwtGJOad6t2we7FsjXSpbLDpuPrlInAhtE5hGCA_PfYTJtrIOKfLYLYGcYXVh1Ksfh_C1ZC-C8gw6GKtvrQesKoMrEA_LU_Gd5srl5-3iZDgJc1iyCELoXtfuIXKJ2ADDHOBaUjhU8lXTVdr2E7bCVaFgVHHkmA=w640-h208)\n-\n-The TokenLearner module takes as input an image-shaped tensor. It then passes it through\n-multiple single-channel convolutional layers extracting different spatial attention maps\n-focusing on different parts of the input. These attention maps are then element-wise\n-multiplied to the input and result is aggregated with pooling. This pooled output can be\n-trated as a summary of the input and has much lesser number of patches (8, for example)\n-than the original one (196, for example).\n-\n-Using multiple convolution layers helps with expressivity. Imposing a form of spatial\n-attention helps retain relevant information from the inputs. Both of these components are\n-crucial to make TokenLearner work, especially when we are significantly reducing the number of patches.\n-\"\"\"\n-\n-\n-def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n-    # Layer normalize the inputs.\n-    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(\n-        inputs\n-    )  # (B, H, W, C)\n-\n-    # Applying Conv2D => Reshape => Permute\n-    # The reshape and permute is done to help with the next steps of\n-    # multiplication and Global Average Pooling.\n-    attention_maps = keras.Sequential(\n-        [\n-            # 3 layers of conv with gelu activation as suggested\n-            # in the paper.\n-            layers.Conv2D(\n-                filters=number_of_tokens,\n-                kernel_size=(3, 3),\n-                activation=ops.gelu,\n-                padding=\"same\",\n-                use_bias=False,\n-            ),\n-            layers.Conv2D(\n-                filters=number_of_tokens,\n-                kernel_size=(3, 3),\n-                activation=ops.gelu,\n-                padding=\"same\",\n-                use_bias=False,\n-            ),\n-            layers.Conv2D(\n-                filters=number_of_tokens,\n-                kernel_size=(3, 3),\n-                activation=ops.gelu,\n-                padding=\"same\",\n-                use_bias=False,\n-            ),\n-            # This conv layer will generate the attention maps\n-            layers.Conv2D(\n-                filters=number_of_tokens,\n-                kernel_size=(3, 3),\n-                activation=\"sigmoid\",  # Note sigmoid for [0, 1] output\n-                padding=\"same\",\n-                use_bias=False,\n-            ),\n-            # Reshape and Permute\n-            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n-            layers.Permute((2, 1)),\n-        ]\n-    )(\n-        x\n-    )  # (B, num_of_tokens, H*W)\n-\n-    # Reshape the input to align it with the output of the conv block.\n-    num_filters = inputs.shape[-1]\n-    inputs = layers.Reshape((1, -1, num_filters))(\n-        inputs\n-    )  # inputs == (B, 1, H*W, C)\n-\n-    # Element-Wise multiplication of the attention maps and the inputs\n-    attended_inputs = (\n-        ops.expand_dims(attention_maps, axis=-1) * inputs\n-    )  # (B, num_tokens, H*W, C)\n-\n-    # Global average pooling the element wise multiplication result.\n-    outputs = ops.mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n-    return outputs\n-\n-\n-\"\"\"\n-## Transformer block\n-\"\"\"\n-\n-\n-def transformer(encoded_patches):\n-    # Layer normalization 1.\n-    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n-\n-    # Multi Head Self Attention layer 1.\n-    attention_output = layers.MultiHeadAttention(\n-        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n-    )(x1, x1)\n-\n-    # Skip connection 1.\n-    x2 = layers.Add()([attention_output, encoded_patches])\n-\n-    # Layer normalization 2.\n-    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n-\n-    # MLP layer 1.\n-    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)\n-\n-    # Skip connection 2.\n-    encoded_patches = layers.Add()([x4, x2])\n-    return encoded_patches\n-\n-\n-\"\"\"\n-## ViT model with the TokenLearner module\n-\"\"\"\n-\n-\n-def create_vit_classifier(\n-    use_token_learner=True, token_learner_units=NUM_TOKENS\n-):\n-    inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n-\n-    # Augment data.\n-    augmented = data_augmentation(inputs)\n-\n-    # Create patches and project the pathces.\n-    projected_patches = layers.Conv2D(\n-        filters=PROJECTION_DIM,\n-        kernel_size=(PATCH_SIZE, PATCH_SIZE),\n-        strides=(PATCH_SIZE, PATCH_SIZE),\n-        padding=\"VALID\",\n-    )(augmented)\n-    _, h, w, c = projected_patches.shape\n-    projected_patches = layers.Reshape((h * w, c))(\n-        projected_patches\n-    )  # (B, number_patches, projection_dim)\n-\n-    # Add positional embeddings to the projected patches.\n-    encoded_patches = PatchEncoder(\n-        num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n-    )(\n-        projected_patches\n-    )  # (B, number_patches, projection_dim)\n-    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n-\n-    # Iterate over the number of layers and stack up blocks of\n-    # Transformer.\n-    for i in range(NUM_LAYERS):\n-        # Add a Transformer block.\n-        encoded_patches = transformer(encoded_patches)\n-\n-        # Add TokenLearner layer in the middle of the\n-        # architecture. The paper suggests that anywhere\n-        # between 1/2 or 3/4 will work well.\n-        if use_token_learner and i == NUM_LAYERS // 2:\n-            _, hh, c = encoded_patches.shape\n-            h = int(math.sqrt(hh))\n-            encoded_patches = layers.Reshape((h, h, c))(\n-                encoded_patches\n-            )  # (B, h, h, projection_dim)\n-            encoded_patches = token_learner(\n-                encoded_patches, token_learner_units\n-            )  # (B, num_tokens, c)\n-\n-    # Layer normalization and Global average pooling.\n-    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(\n-        encoded_patches\n-    )\n-    representation = layers.GlobalAvgPool1D()(representation)\n-\n-    # Classify outputs.\n-    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n-\n-    # Create the Keras model.\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-\"\"\"\n-As shown in the [TokenLearner paper](https://openreview.net/forum?id=z-l1kpDXs88), it is\n-almost always advantageous to include the TokenLearner module in the middle of the\n-network.\n-\"\"\"\n-\n-\"\"\"\n-## Training utility\n-\"\"\"\n-\n-\n-def run_experiment(model):\n-    # Initialize the AdamW optimizer.\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n-    )\n-\n-    # Compile the model with the optimizer, loss function\n-    # and the metrics.\n-    model.compile(\n-        optimizer=optimizer,\n-        loss=\"sparse_categorical_crossentropy\",\n-        metrics=[\n-            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n-            keras.metrics.SparseTopKCategoricalAccuracy(\n-                5, name=\"top-5-accuracy\"\n-            ),\n-        ],\n-    )\n-\n-    # Define callbacks\n-    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_accuracy\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    # Train the model.\n-    _ = model.fit(\n-        train_ds,\n-        epochs=EPOCHS,\n-        validation_data=val_ds,\n-        callbacks=[checkpoint_callback],\n-    )\n-\n-    model.load_weights(checkpoint_filepath)\n-    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-\n-\"\"\"\n-## Train and evaluate a ViT with TokenLearner\n-\"\"\"\n-\n-vit_token_learner = create_vit_classifier()\n-run_experiment(vit_token_learner)\n-\n-\"\"\"\n-## Results\n-\n-We experimented with and without the TokenLearner inside the mini ViT we implemented\n-(with the same hyperparameters presented in this example). Here are our results:\n-\n-| **TokenLearner** | **# tokens in<br> TokenLearner** | **Top-1 Acc<br>(Averaged across 5 runs)** | **GFLOPs** | **TensorBoard** |\n-|:---:|:---:|:---:|:---:|:---:|\n-| N | - | 56.112% | 0.0184 | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n-| Y | 8 | **56.55%** | **0.0153** | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n-| N | - | 56.37% | 0.0184 | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n-| Y | 4 | **56.4980%** | **0.0147** | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n-| N | - (# Transformer layers: 8) | 55.36% | 0.0359 | [Link](https://tensorboard.dev/experiment/sepBK5zNSaOtdCeEG6SV9w/) |\n-\n-TokenLearner is able to consistently outperform our mini ViT without the module. It is\n-also interesting to notice that it was also able to outperform a deeper version of our\n-mini ViT (with 8 layers). The authors also report similar observations in the paper and\n-they attribute this to the adaptiveness of TokenLearner.\n-\n-One should also note that the FLOPs count **decreases** considerably with the addition of\n-the TokenLearner module. With less FLOPs count the TokenLearner module is able to\n-deliver better results. This aligns very well with the authors' findings.\n-\n-Additionally, the authors [introduced](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py#L104)\n-a newer version of the TokenLearner for smaller training data regimes. Quoting the authors:\n-\n-> Instead of using 4 conv. layers with small channels to implement spatial attention,\n-  this version uses 2 grouped conv. layers with more channels. It also uses softmax\n-  instead of sigmoid. We confirmed that this version works better when having limited\n-  training data, such as training with ImageNet1K from scratch.\n-\n-We experimented with this module and in the following table we summarize the results:\n-\n-| **# Groups** | **# Tokens** | **Top-1 Acc** | **GFLOPs** | **TensorBoard** |\n-|:---:|:---:|:---:|:---:|:---:|\n-| 4 | 4 | 54.638% | 0.0149 | [Link](https://tensorboard.dev/experiment/KmfkGqAGQjikEw85phySmw/) |\n-| 8 | 8 | 54.898% | 0.0146 | [Link](https://tensorboard.dev/experiment/0PpgYOq9RFWV9njX6NJQ2w/) |\n-| 4 | 8 | 55.196% | 0.0149 | [Link](https://tensorboard.dev/experiment/WUkrHbZASdu3zrfmY4ETZg/) |\n-\n-Please note that we used the same hyperparameters presented in this example. Our\n-implementation is available\n-[in this notebook](https://github.com/ariG23498/TokenLearner/blob/master/TokenLearner-V1.1.ipynb).\n-We acknowledge that the results with this new TokenLearner module are slightly off\n-than expected and this might mitigate with hyperparameter tuning.\n-\n-*Note*: To compute the FLOPs of our models we used\n-[this utility](https://github.com/AdityaKane2001/regnety/blob/main/regnety/utils/model_utils.py#L27)\n-from [this repository](https://github.com/AdityaKane2001/regnety).\n-\"\"\"\n-\n-\"\"\"\n-## Number of parameters\n-\n-You may have noticed that adding the TokenLearner module increases the number of\n-parameters of the base network. But that does not mean it is less efficient as shown by\n-[Dehghani et al.](https://arxiv.org/abs/2110.12894). Similar findings were reported\n-by [Bello et al.](https://arxiv.org/abs/2103.07579) as well. The TokenLearner module\n-helps reducing the FLOPS in the overall network thereby helping to reduce the memory\n-footprint.\n-\"\"\"\n-\n-\"\"\"\n-## Final notes\n-\n-* TokenFuser: The authors of the paper also propose another module named TokenFuser. This\n-module helps in remapping the representation of the TokenLearner output back to its\n-original spatial resolution. To reuse the TokenLearner in the ViT architecture, the\n-TokenFuser is a must. We first learn the tokens from the TokenLearner, build a\n-representation of the tokens from a Transformer layer and then remap the representation\n-into the original spatial resolution, so that it can again be consumed by a TokenLearner.\n-Note here that you can only use the TokenLearner module once in entire ViT model if not\n-paired with the TokenFuser.\n-* Use of these modules for video: The authors also suggest that TokenFuser goes really\n-well with Vision Transformers for Videos ([Arnab et al.](https://arxiv.org/abs/2103.15691)).\n-\n-We are grateful to [JarvisLabs](https://jarvislabs.ai/) and\n-[Google Developers Experts](https://developers.google.com/programs/experts/)\n-program for helping with GPU credits. Also, we are thankful to Michael Ryoo (first\n-author of TokenLearner) for fruitful discussions.\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/🤗%20Model-TokenLearner-black.svg)](https://huggingface.co/keras-io/learning_to_tokenize_in_ViT) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-TokenLearner-black.svg)](https://huggingface.co/spaces/keras-io/token_learner) |\n-\"\"\"\n\n@@ -1,420 +0,0 @@\n-\"\"\"\n-Title: Video Classification with Transformers\n-Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Date created: 2021/06/08\n-Last modified: 2023/22/07\n-Description: Training a video classifier with hybrid transformers.\n-Accelerator: GPU\n-Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n-\"\"\"\n-\"\"\"\n-This example is a follow-up to the\n-[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\n-example. This time, we will be using a Transformer-based model\n-([Vaswani et al.](https://arxiv.org/abs/1706.03762)) to classify videos. You can follow\n-[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11)\n-in case you need an introduction to Transformers (with code). After reading this\n-example, you will know how to develop hybrid Transformer-based models for video\n-classification that operate on CNN feature maps.\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -q git+https://github.com/keras-team/keras\n-pip install -q git+https://github.com/tensorflow/docs\n-\"\"\"\n-\n-\"\"\"\n-## Data collection\n-\n-As done in the [predecessor](https://keras.io/examples/vision/video_classification/) to\n-this example, we will be using a subsampled version of the\n-[UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php),\n-a well-known benchmark dataset. In case you want to operate on a larger subsample or\n-even the entire dataset, please refer to\n-[this notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb).\n-\"\"\"\n-\n-\"\"\"shell\n-wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n-tar -xf ucf101_top5.tar.gz\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import keras\n-from keras import layers\n-from keras.applications.densenet import DenseNet121\n-\n-from tensorflow_docs.vis import embed\n-\n-import matplotlib.pyplot as plt\n-import pandas as pd\n-import numpy as np\n-import imageio\n-import cv2\n-\n-\"\"\"\n-## Define hyperparameters\n-\"\"\"\n-\n-MAX_SEQ_LENGTH = 20\n-NUM_FEATURES = 1024\n-IMG_SIZE = 128\n-\n-EPOCHS = 5\n-\n-\"\"\"\n-## Data preparation\n-\n-We will mostly be following the same data preparation steps in this example, except for\n-the following changes:\n-\n-* We reduce the image size to 128x128 instead of 224x224 to speed up computation.\n-* Instead of using a pre-trained [InceptionV3](https://arxiv.org/abs/1512.00567) network,\n-we use a pre-trained\n-[DenseNet121](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n-for feature extraction.\n-* We directly pad shorter videos to length `MAX_SEQ_LENGTH`.\n-\n-First, let's load up the\n-[DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n-\"\"\"\n-\n-train_df = pd.read_csv(\"train.csv\")\n-test_df = pd.read_csv(\"test.csv\")\n-\n-print(f\"Total videos for training: {len(train_df)}\")\n-print(f\"Total videos for testing: {len(test_df)}\")\n-\n-center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n-\n-\n-def crop_center(frame):\n-    cropped = center_crop_layer(frame[None, ...])\n-    cropped = keras.ops.convert_to_numpy(cropped)\n-    cropped = keras.ops.squeeze(cropped)\n-    return cropped\n-\n-\n-# Following method is modified from this tutorial:\n-# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n-def load_video(path, max_frames=0, offload_to_cpu=False):\n-    cap = cv2.VideoCapture(path)\n-    frames = []\n-    try:\n-        while True:\n-            ret, frame = cap.read()\n-            if not ret:\n-                break\n-            frame = frame[:, :, [2, 1, 0]]\n-            frame = crop_center(frame)\n-            if offload_to_cpu and keras.backend.backend() == \"torch\":\n-                frame = frame.to(\"cpu\")\n-            frames.append(frame)\n-\n-            if len(frames) == max_frames:\n-                break\n-    finally:\n-        cap.release()\n-    if offload_to_cpu and keras.backend.backend() == \"torch\":\n-        return np.array([frame.to(\"cpu\").numpy() for frame in frames])\n-    return np.array(frames)\n-\n-\n-def build_feature_extractor():\n-    feature_extractor = DenseNet121(\n-        weights=\"imagenet\",\n-        include_top=False,\n-        pooling=\"avg\",\n-        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n-    )\n-    preprocess_input = keras.applications.densenet.preprocess_input\n-\n-    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n-    preprocessed = preprocess_input(inputs)\n-\n-    outputs = feature_extractor(preprocessed)\n-    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n-\n-\n-feature_extractor = build_feature_extractor()\n-\n-\n-# Label preprocessing with StringLookup.\n-label_processor = keras.layers.StringLookup(\n-    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n-)\n-print(label_processor.get_vocabulary())\n-\n-\n-def prepare_all_videos(df, root_dir):\n-    num_samples = len(df)\n-    video_paths = df[\"video_name\"].values.tolist()\n-    labels = df[\"tag\"].values\n-    labels = label_processor(labels[..., None]).numpy()\n-\n-    # `frame_features` are what we will feed to our sequence model.\n-    frame_features = np.zeros(\n-        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n-    )\n-\n-    # For each video.\n-    for idx, path in enumerate(video_paths):\n-        # Gather all its frames and add a batch dimension.\n-        frames = load_video(os.path.join(root_dir, path))\n-\n-        # Pad shorter videos.\n-        if len(frames) < MAX_SEQ_LENGTH:\n-            diff = MAX_SEQ_LENGTH - len(frames)\n-            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n-            frames = np.concatenate(frames, padding)\n-\n-        frames = frames[None, ...]\n-\n-        # Initialize placeholder to store the features of the current video.\n-        temp_frame_features = np.zeros(\n-            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n-        )\n-\n-        # Extract features from the frames of the current video.\n-        for i, batch in enumerate(frames):\n-            video_length = batch.shape[0]\n-            length = min(MAX_SEQ_LENGTH, video_length)\n-            for j in range(length):\n-                if np.mean(batch[j, :]) > 0.0:\n-                    temp_frame_features[i, j, :] = feature_extractor.predict(\n-                        batch[None, j, :]\n-                    )\n-\n-                else:\n-                    temp_frame_features[i, j, :] = 0.0\n-\n-        frame_features[idx,] = temp_frame_features.squeeze()\n-\n-    return frame_features, labels\n-\n-\n-\"\"\"\n-Calling `prepare_all_videos()` on `train_df` and `test_df` takes ~20 minutes to\n-complete. For this reason, to save time, here we download already preprocessed NumPy arrays:\n-\"\"\"\n-\n-\"\"\"shell\n-!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n-!tar -xf top5_data_prepared.tar.gz\n-\"\"\"\n-\n-train_data, train_labels = np.load(\"train_data.npy\"), np.load(\n-    \"train_labels.npy\"\n-)\n-test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n-\n-print(f\"Frame features in train set: {train_data.shape}\")\n-\n-\"\"\"\n-## Building the Transformer-based model\n-\n-We will be building on top of the code shared in\n-[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11) of\n-[Deep Learning with Python (Second ed.)](https://www.manning.com/books/deep-learning-with-python)\n-by François Chollet.\n-\n-First, self-attention layers that form the basic blocks of a Transformer are\n-order-agnostic. Since videos are ordered sequences of frames, we need our\n-Transformer model to take into account order information.\n-We do this via **positional encoding**.\n-We simply embed the positions of the frames present inside videos with an\n-[`Embedding` layer](https://keras.io/api/layers/core_layers/embedding). We then\n-add these positional embeddings to the precomputed CNN feature maps.\n-\"\"\"\n-\n-\n-class PositionalEmbedding(layers.Layer):\n-    def __init__(self, sequence_length, output_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.position_embeddings = layers.Embedding(\n-            input_dim=sequence_length, output_dim=output_dim\n-        )\n-        self.sequence_length = sequence_length\n-        self.output_dim = output_dim\n-\n-    def build(self, input_shape):\n-        self.position_embeddings.build(input_shape)\n-\n-    def call(self, inputs):\n-        # The inputs are of shape: `(batch_size, frames, num_features)`\n-        inputs = keras.ops.cast(inputs, self.compute_dtype)\n-        length = keras.ops.shape(inputs)[1]\n-        positions = keras.ops.arange(start=0, stop=length, step=1)\n-        embedded_positions = self.position_embeddings(positions)\n-        return inputs + embedded_positions\n-\n-\n-\"\"\"\n-Now, we can create a subclassed layer for the Transformer.\n-\"\"\"\n-\n-\n-class TransformerEncoder(layers.Layer):\n-    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n-        super().__init__(**kwargs)\n-        self.embed_dim = embed_dim\n-        self.dense_dim = dense_dim\n-        self.num_heads = num_heads\n-        self.attention = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n-        )\n-        self.dense_proj = keras.Sequential(\n-            [\n-                layers.Dense(dense_dim, activation=keras.activations.gelu),\n-                layers.Dense(embed_dim),\n-            ]\n-        )\n-        self.layernorm_1 = layers.LayerNormalization()\n-        self.layernorm_2 = layers.LayerNormalization()\n-\n-    def call(self, inputs, mask=None):\n-        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n-        proj_input = self.layernorm_1(inputs + attention_output)\n-        proj_output = self.dense_proj(proj_input)\n-        return self.layernorm_2(proj_input + proj_output)\n-\n-\n-\"\"\"\n-## Utility functions for training\n-\"\"\"\n-\n-\n-def get_compiled_model(shape):\n-    sequence_length = MAX_SEQ_LENGTH\n-    embed_dim = NUM_FEATURES\n-    dense_dim = 4\n-    num_heads = 1\n-    classes = len(label_processor.get_vocabulary())\n-\n-    inputs = keras.Input(shape=shape)\n-    x = PositionalEmbedding(\n-        sequence_length, embed_dim, name=\"frame_position_embedding\"\n-    )(inputs)\n-    x = TransformerEncoder(\n-        embed_dim, dense_dim, num_heads, name=\"transformer_layer\"\n-    )(x)\n-    x = layers.GlobalMaxPooling1D()(x)\n-    x = layers.Dropout(0.5)(x)\n-    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n-    model = keras.Model(inputs, outputs)\n-\n-    model.compile(\n-        optimizer=\"adam\",\n-        loss=\"sparse_categorical_crossentropy\",\n-        metrics=[\"accuracy\"],\n-    )\n-    return model\n-\n-\n-def run_experiment():\n-    filepath = \"/tmp/video_classifier.weights.h5\"\n-    checkpoint = keras.callbacks.ModelCheckpoint(\n-        filepath, save_weights_only=True, save_best_only=True, verbose=1\n-    )\n-\n-    model = get_compiled_model(train_data.shape[1:])\n-    history = model.fit(\n-        train_data,\n-        train_labels,\n-        validation_split=0.15,\n-        epochs=EPOCHS,\n-        callbacks=[checkpoint],\n-    )\n-\n-    model.load_weights(filepath)\n-    _, accuracy = model.evaluate(test_data, test_labels)\n-    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-\n-    return model\n-\n-\n-\"\"\"\n-## Model training and inference\n-\"\"\"\n-\n-trained_model = run_experiment()\n-\n-\"\"\"\n-**Note**: This model has ~4.23 Million parameters, which is way more than the sequence\n-model (99918 parameters) we used in the prequel of this example.  This kind of\n-Transformer model works best with a larger dataset and a longer pre-training schedule.\n-\"\"\"\n-\n-\n-def prepare_single_video(frames):\n-    frame_features = np.zeros(\n-        shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n-    )\n-\n-    # Pad shorter videos.\n-    if len(frames) < MAX_SEQ_LENGTH:\n-        diff = MAX_SEQ_LENGTH - len(frames)\n-        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n-        frames = np.concatenate(frames, padding)\n-\n-    frames = frames[None, ...]\n-\n-    # Extract features from the frames of the current video.\n-    for i, batch in enumerate(frames):\n-        video_length = batch.shape[0]\n-        length = min(MAX_SEQ_LENGTH, video_length)\n-        for j in range(length):\n-            if np.mean(batch[j, :]) > 0.0:\n-                frame_features[i, j, :] = feature_extractor.predict(\n-                    batch[None, j, :]\n-                )\n-            else:\n-                frame_features[i, j, :] = 0.0\n-\n-    return frame_features\n-\n-\n-def predict_action(path):\n-    class_vocab = label_processor.get_vocabulary()\n-\n-    frames = load_video(os.path.join(\"test\", path), offload_to_cpu=True)\n-    frame_features = prepare_single_video(frames)\n-    probabilities = trained_model.predict(frame_features)[0]\n-\n-    plot_x_axis, plot_y_axis = [], []\n-\n-    for i in np.argsort(probabilities)[::-1]:\n-        plot_x_axis.append(class_vocab[i])\n-        plot_y_axis.append(probabilities[i])\n-        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n-\n-    plt.bar(plot_x_axis, plot_y_axis, label=plot_x_axis)\n-    plt.xlabel(\"class_label\")\n-    plt.xlabel(\"Probability\")\n-    plt.show()\n-\n-    return frames\n-\n-\n-# This utility is for visualization.\n-# Referenced from:\n-# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n-def to_gif(images):\n-    converted_images = images.astype(np.uint8)\n-    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n-    return embed.embed_file(\"animation.gif\")\n-\n-\n-test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n-print(f\"Test video path: {test_video}\")\n-test_frames = predict_action(test_video)\n-to_gif(test_frames[:MAX_SEQ_LENGTH])\n-\n-\"\"\"\n-The performance of our model is far from optimal, because it was trained on a\n-small dataset.\n-\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
