{"custom_id": "keras#636337fd06cbac939e774cee26c6647fc4823e6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 58 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,58 @@\n+\"\"\"\n+Title: Bidirectional LSTM on IMDB\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/05/03\n+Last modified: 2020/05/03\n+Description: Train a 2-layer bidirectional LSTM on the IMDB movie review sentiment classification dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+\n+max_features = 20000  # Only consider the top 20k words\n+maxlen = 200  # Only consider the first 200 words of each movie review\n+\n+\"\"\"\n+## Build the model\n+\"\"\"\n+\n+# Input for variable-length sequences of integers\n+inputs = keras.Input(shape=(None,), dtype=\"int32\")\n+# Embed each integer in a 128-dimensional vector\n+x = layers.Embedding(max_features, 128)(inputs)\n+# Add 2 bidirectional LSTMs\n+x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n+x = layers.Bidirectional(layers.LSTM(64))(x)\n+# Add a classifier\n+outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n+model = keras.Model(inputs, outputs)\n+model.summary()\n+\n+\"\"\"\n+## Load the IMDB movie review sentiment data\n+\"\"\"\n+\n+(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n+    num_words=max_features\n+)\n+print(len(x_train), \"Training sequences\")\n+print(len(x_val), \"Validation sequences\")\n+# Use pad_sequence to standardize sequence length:\n+# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n+x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n+x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n+\n+\"\"\"\n+## Train and evaluate the model\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/bidirectional-lstm-imdb)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/bidirectional_lstm_imdb).\n+\"\"\"\n+\n+model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n+model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0490665dc93079ae044934bdf9ec0d2ba787cde6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 16 | Churn Cumulative: 1039 | Contributors (this commit): 9 | Commits (past 90d): 36 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,6 @@ from keras_core.backend import convert_to_tensor\n from keras_core.backend import is_tensor\n from keras_core.backend import name_scope\n from keras_core.backend import random\n-from keras_core.backend import shape\n from keras_core.operations import image\n from keras_core.operations import operation_utils\n from keras_core.operations.core import *  # noqa: F403\n\n@@ -222,3 +222,11 @@ def stop_gradient(variable):\n     >>> var = keras_core.operations.stop_gradient(var)\n     \"\"\"\n     return backend.core.stop_gradient(variable)\n+\n+\n+@keras_core_export(\"keras_core.operations.shape\")\n+def shape(x):\n+    \"\"\"Gets the shape of the tensor input.\"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return x.shape\n+    return backend.core.shape(x)\n\n@@ -184,3 +184,10 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         model.fit(x, y, epochs=1, batch_size=2)\n         self.assertEqual(model.layers[0].w.numpy(), 0.0)\n         self.assertNotEqual(model.layers[0].b.numpy(), 0.0)\n+\n+    def test_shape(self):\n+        x = np.ones((2, 3, 7, 1))\n+        self.assertAllEqual(core.shape(x), (2, 3, 7, 1))\n+\n+        x = KerasTensor((None, 3, None, 1))\n+        self.assertAllEqual(core.shape(x), (None, 3, None, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5c4c527d369d23c151a28d9c10fa51528daedf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 616 | Contributors (this commit): 6 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -362,9 +362,10 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n+    size_x = size(x)\n     tf.assert_greater(\n-        size(x),\n-        tf.constant(0, dtype=tf.int64),\n+        size_x,\n+        tf.constant(0, dtype=size_x.dtype),\n         message=\"Cannot compute the min of an empty tensor.\",\n     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61b76f1655f88a5cfa83b4096345b3f7f9dcb7df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1589 | Lines Deleted: 2 | Files Changed: 10 | Hunks: 16 | Methods Changed: 60 | Complexity Δ (Sum/Max): 74/16 | Churn Δ: 1591 | Churn Cumulative: 2532 | Contributors (this commit): 1 | Commits (past 90d): 19 | Contributors (cumulative): 10 | DMM Complexity: 0.9609175870858114\n\nDIFF:\n@@ -60,6 +60,46 @@ def benchmark_prelu(\n     )\n \n \n+def benchmark_relu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ReLU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_leaky_relu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"LeakyReLU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n def benchmark_softmax(\n     num_samples,\n     batch_size,\n@@ -82,6 +122,8 @@ def benchmark_softmax(\n \n BENCHMARK_NAMES = {\n     \"benchmark_elu\": benchmark_elu,\n+    \"benchmark_relu\": benchmark_relu,\n+    \"benchmark_leaky_relu\": benchmark_leaky_relu,\n     \"benchmark_prelu\": benchmark_prelu,\n     \"benchmark_softmax\": benchmark_softmax,\n }\n\n@@ -163,7 +163,9 @@ class LayerBenchmark:\n             outputs = self._tf_keras_layer(inputs)\n         return tf.keras.Model(inputs=inputs, outputs=outputs)\n \n-    def benchmark_predict(self, num_samples, batch_size):\n+    def benchmark_predict(self, num_samples, batch_size, data=None):\n+        if data is None:\n+            # Generate default data if not provided.\n             if isinstance(self.input_shape[0], (tuple, list)):\n                 # The layer has multiple inputs.\n                 data = []\n@@ -207,7 +209,9 @@ class LayerBenchmark:\n             f\"{tf_keras_throughput:.2f} samples/sec.\"\n         )\n \n-    def benchmark_train(self, num_samples, batch_size):\n+    def benchmark_train(self, num_samples, batch_size, data=None, label=None):\n+        if data is None:\n+            # Generate default data if not provided.\n             if isinstance(self.input_shape[0], (tuple, list)):\n                 # The layer has multiple inputs.\n                 data = []\n@@ -217,6 +221,9 @@ class LayerBenchmark:\n             else:\n                 data_shape = [num_samples] + list(self.input_shape)\n                 data = [np.random.normal(size=data_shape)]\n+\n+        if label is None:\n+            # Generate default label if not provided.\n             if self.flat_call_inputs:\n                 # Scale by a small factor to avoid zero gradients.\n                 label = np.array(self._keras_core_layer(*data)) * 1.001\n\n@@ -161,6 +161,64 @@ def benchmark_depthwise_conv2D(\n     )\n \n \n+def benchmark_separable_conv1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SeparableConv1D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+        \"filters\": 3,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_separable_conv2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SeparableConv2D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+        \"filters\": 3,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[128, 128, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n def benchmark_conv1D_transpose(\n     num_samples,\n     batch_size,\n@@ -251,6 +309,8 @@ BENCHMARK_NAMES = {\n     \"benchmark_conv3D\": benchmark_conv3D,\n     \"benchmark_depthwise_conv1D\": benchmark_depthwise_conv1D,\n     \"benchmark_depthwise_conv2D\": benchmark_depthwise_conv2D,\n+    \"benchmark_separable_conv1D\": benchmark_separable_conv1D,\n+    \"benchmark_separable_conv2D\": benchmark_separable_conv2D,\n     \"benchmark_conv1D_transpose\": benchmark_conv1D_transpose,\n     \"benchmark_conv2D_transpose\": benchmark_conv2D_transpose,\n     \"benchmark_conv3D_transpose\": benchmark_conv3D_transpose,\n\n@@ -12,6 +12,7 @@ python3 -m benchmarks.layer_benchmark.core_benchmark \\\n ```\n \"\"\"\n \n+import numpy as np\n from absl import app\n from absl import flags\n \n@@ -73,6 +74,37 @@ def benchmark_einsum_dense(\n     )\n \n \n+def benchmark_embedding(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Embedding\"\n+    init_args = {\n+        \"input_dim\": 30,\n+        \"output_shape\": 128,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 32],\n+        jit_compile=jit_compile,\n+    )\n+\n+    data = np.random.randint(30, size=(num_samples, 32))\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+        data=data,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+        data=data,\n+    )\n+\n+\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n\n@@ -0,0 +1,224 @@\n+\"\"\" Benchmark merge layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n+    --benchmark_name=benchmark_add \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_add(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Add\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Average\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_concatenate(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Concatenate\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_dot(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Dot\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 32], [32, 64]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_maximum(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Maximum\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_minimum(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Minimum\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_multiply(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Multiply\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 32]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_subtract(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Subtract\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_add\": benchmark_add,\n+    \"benchmark_average\": benchmark_average,\n+    \"benchmark_concatenate\": benchmark_concatenate,\n+    \"benchmark_dot\": benchmark_dot,\n+    \"benchmark_maximum\": benchmark_maximum,\n+    \"benchmark_minimum\": benchmark_minimum,\n+    \"benchmark_multiply\": benchmark_multiply,\n+    \"benchmark_subtract\": benchmark_subtract,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -97,10 +97,31 @@ def benchmark_layer_normalization(\n     )\n \n \n+def benchmark_unit_normalization(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UnitNormalization\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n     \"benchmark_group_normalization\": benchmark_group_normalization,\n     \"benchmark_layer_normalization\": benchmark_layer_normalization,\n+    \"benchmark_unit_normalization\": benchmark_unit_normalization,\n }\n \n \n\n@@ -0,0 +1,373 @@\n+\"\"\" Benchmark pooling layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.pooling_benchmark \\\n+    --benchmark_name=benchmark_max_pooling1d \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_average_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling1D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling2D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling3D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling1D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling2D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling3D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_average_pooling1d\": benchmark_average_pooling1d,\n+    \"benchmark_average_pooling2d\": benchmark_average_pooling2d,\n+    \"benchmark_average_pooling3d\": benchmark_average_pooling3d,\n+    \"benchmark_max_pooling1d\": benchmark_max_pooling1d,\n+    \"benchmark_max_pooling2d\": benchmark_max_pooling2d,\n+    \"benchmark_max_pooling3d\": benchmark_max_pooling3d,\n+    \"benchmark_global_average_pooling1d\": benchmark_global_average_pooling1d,\n+    \"benchmark_global_average_pooling2d\": benchmark_global_average_pooling2d,\n+    \"benchmark_global_average_pooling3d\": benchmark_global_average_pooling3d,\n+    \"benchmark_global_max_pooling1d\": benchmark_global_max_pooling1d,\n+    \"benchmark_global_max_pooling2d\": benchmark_global_max_pooling2d,\n+    \"benchmark_global_max_pooling3d\": benchmark_global_max_pooling3d,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,216 @@\n+\"\"\" Benchmark regularization layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.regularization_benchmark \\\n+    --benchmark_name=benchmark_dropout\\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_dropout(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Dropout\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gaussian_dropout(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GaussionDropout\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gaussian_noise(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GaussionNoise\"\n+    init_args = {\n+        \"stddev\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout1D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout2D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout3D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout3D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_dropout\": benchmark_dropout,\n+    \"benchmark_gaussian_dropout\": benchmark_gaussian_dropout,\n+    \"benchmark_gaussian_noise\": benchmark_gaussian_noise,\n+    \"benchmark_spatial_dropout1D\": benchmark_spatial_dropout1D,\n+    \"benchmark_spatial_dropout2D\": benchmark_spatial_dropout2D,\n+    \"benchmark_spatial_dropout3D\": benchmark_spatial_dropout3D,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,336 @@\n+\"\"\" Benchmark reshaping layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.reshaping_benchmark \\\n+    --benchmark_name=benchmark_cropping2d \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_cropping1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_cropping2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_cropping3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_flatten(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Flatten\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_permute(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Permute\"\n+    init_args = {\n+        \"dim\": (2, 1),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_cropping1d\": benchmark_cropping1d,\n+    \"benchmark_cropping2d\": benchmark_cropping2d,\n+    \"benchmark_cropping3d\": benchmark_cropping3d,\n+    \"benchmark_flatten\": benchmark_flatten,\n+    \"benchmark_permute\": benchmark_permute,\n+    \"benchmark_up_sampling1d\": benchmark_up_sampling1d,\n+    \"benchmark_up_sampling2d\": benchmark_up_sampling2d,\n+    \"benchmark_up_sampling3d\": benchmark_up_sampling3d,\n+    \"benchmark_zero_padding1d\": benchmark_zero_padding1d,\n+    \"benchmark_zero_padding2d\": benchmark_zero_padding2d,\n+    \"benchmark_zero_padding3d\": benchmark_zero_padding3d,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,276 @@\n+\"\"\" Benchmark rnn layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n+    --benchmark_name=benchmark_lstm \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+import keras_core\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_conv_lstm1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM1D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv_lstm2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM2D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv_lstm3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM3D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[16, 32, 32, 16, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gru(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GRU\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_lstm(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"LSTM\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_simple_rnn(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SimpleRNN\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_bidirectional(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Bidirectional\"\n+    init_args = {\n+        \"layer\": keras_core.layers.LSTM(32),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_time_distributed(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"TimeDistributed\"\n+    init_args = {\n+        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[10, 128, 128, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_conv_lstm1d\": benchmark_conv_lstm1d,\n+    \"benchmark_conv_lstm2d\": benchmark_conv_lstm2d,\n+    \"benchmark_conv_lstm3d\": benchmark_conv_lstm3d,\n+    \"benchmark_gru\": benchmark_gru,\n+    \"benchmark_lstm\": benchmark_lstm,\n+    \"benchmark_simple_rnn\": benchmark_simple_rnn,\n+    \"benchmark_bidirectional\": benchmark_bidirectional,\n+    \"benchmark_time_distributed\": benchmark_time_distributed,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fabbf818802e555b4709ae71c322cca8950c7d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1970 | Files Changed: 14 | Hunks: 21 | Methods Changed: 85 | Complexity Δ (Sum/Max): -108/0 | Churn Δ: 1974 | Churn Cumulative: 5816 | Contributors (this commit): 9 | Commits (past 90d): 78 | Contributors (cumulative): 37 | DMM Complexity: 0.0336996336996337\n\nDIFF:\n@@ -60,46 +60,6 @@ def benchmark_prelu(\n     )\n \n \n-def benchmark_relu(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ReLU\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_leaky_relu(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"LeakyReLU\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n def benchmark_softmax(\n     num_samples,\n     batch_size,\n@@ -122,8 +82,6 @@ def benchmark_softmax(\n \n BENCHMARK_NAMES = {\n     \"benchmark_elu\": benchmark_elu,\n-    \"benchmark_relu\": benchmark_relu,\n-    \"benchmark_leaky_relu\": benchmark_leaky_relu,\n     \"benchmark_prelu\": benchmark_prelu,\n     \"benchmark_softmax\": benchmark_softmax,\n }\n\n@@ -163,9 +163,7 @@ class LayerBenchmark:\n             outputs = self._tf_keras_layer(inputs)\n         return tf.keras.Model(inputs=inputs, outputs=outputs)\n \n-    def benchmark_predict(self, num_samples, batch_size, data=None):\n-        if data is None:\n-            # Generate default data if not provided.\n+    def benchmark_predict(self, num_samples, batch_size):\n         if isinstance(self.input_shape[0], (tuple, list)):\n             # The layer has multiple inputs.\n             data = []\n@@ -209,9 +207,7 @@ class LayerBenchmark:\n             f\"{tf_keras_throughput:.2f} samples/sec.\"\n         )\n \n-    def benchmark_train(self, num_samples, batch_size, data=None, label=None):\n-        if data is None:\n-            # Generate default data if not provided.\n+    def benchmark_train(self, num_samples, batch_size):\n         if isinstance(self.input_shape[0], (tuple, list)):\n             # The layer has multiple inputs.\n             data = []\n@@ -221,9 +217,6 @@ class LayerBenchmark:\n         else:\n             data_shape = [num_samples] + list(self.input_shape)\n             data = [np.random.normal(size=data_shape)]\n-\n-        if label is None:\n-            # Generate default label if not provided.\n         if self.flat_call_inputs:\n             # Scale by a small factor to avoid zero gradients.\n             label = np.array(self._keras_core_layer(*data)) * 1.001\n\n@@ -161,64 +161,6 @@ def benchmark_depthwise_conv2D(\n     )\n \n \n-def benchmark_separable_conv1D(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SeparableConv1D\"\n-    init_args = {\n-        \"kernel_size\": 16,\n-        \"depth_multiplier\": 2,\n-        \"filters\": 3,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_separable_conv2D(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SeparableConv2D\"\n-    init_args = {\n-        \"kernel_size\": 16,\n-        \"depth_multiplier\": 2,\n-        \"filters\": 3,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[128, 128, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n def benchmark_conv1D_transpose(\n     num_samples,\n     batch_size,\n@@ -309,8 +251,6 @@ BENCHMARK_NAMES = {\n     \"benchmark_conv3D\": benchmark_conv3D,\n     \"benchmark_depthwise_conv1D\": benchmark_depthwise_conv1D,\n     \"benchmark_depthwise_conv2D\": benchmark_depthwise_conv2D,\n-    \"benchmark_separable_conv1D\": benchmark_separable_conv1D,\n-    \"benchmark_separable_conv2D\": benchmark_separable_conv2D,\n     \"benchmark_conv1D_transpose\": benchmark_conv1D_transpose,\n     \"benchmark_conv2D_transpose\": benchmark_conv2D_transpose,\n     \"benchmark_conv3D_transpose\": benchmark_conv3D_transpose,\n\n@@ -12,7 +12,6 @@ python3 -m benchmarks.layer_benchmark.core_benchmark \\\n ```\n \"\"\"\n \n-import numpy as np\n from absl import app\n from absl import flags\n \n@@ -74,37 +73,6 @@ def benchmark_einsum_dense(\n     )\n \n \n-def benchmark_embedding(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Embedding\"\n-    init_args = {\n-        \"input_dim\": 30,\n-        \"output_shape\": 128,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[64, 32],\n-        jit_compile=jit_compile,\n-    )\n-\n-    data = np.random.randint(30, size=(num_samples, 32))\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-        data=data,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-        data=data,\n-    )\n-\n-\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n\n@@ -1,224 +0,0 @@\n-\"\"\" Benchmark merge layers.\n-\n-To run benchmarks, see the following command for an example, please change the\n-flag to your custom value:\n-\n-```\n-python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n-    --benchmark_name=benchmark_add \\\n-    --num_samples=8192 \\\n-    --batch_size=1024 \\\n-    --jit_compile=True\n-```\n-\"\"\"\n-\n-from absl import app\n-from absl import flags\n-\n-from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n-\n-FLAGS = flags.FLAGS\n-\n-\n-def benchmark_add(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Add\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_average(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Average\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_concatenate(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Concatenate\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_dot(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Dot\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 32], [32, 64]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_maximum(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Maximum\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_minimum(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Minimum\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_multiply(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Multiply\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 32]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_subtract(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Subtract\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[[256, 256], [256, 256]],\n-        flat_call_inputs=False,\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-BENCHMARK_NAMES = {\n-    \"benchmark_add\": benchmark_add,\n-    \"benchmark_average\": benchmark_average,\n-    \"benchmark_concatenate\": benchmark_concatenate,\n-    \"benchmark_dot\": benchmark_dot,\n-    \"benchmark_maximum\": benchmark_maximum,\n-    \"benchmark_minimum\": benchmark_minimum,\n-    \"benchmark_multiply\": benchmark_multiply,\n-    \"benchmark_subtract\": benchmark_subtract,\n-}\n-\n-\n-def main(_):\n-    benchmark_name = FLAGS.benchmark_name\n-    num_samples = FLAGS.num_samples\n-    batch_size = FLAGS.batch_size\n-    jit_compile = FLAGS.jit_compile\n-\n-    if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n-            benchmark_fn(num_samples, batch_size, jit_compile)\n-        return\n-\n-    if benchmark_name not in BENCHMARK_NAMES:\n-        raise ValueError(\n-            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n-            f\"be one of {BENCHMARK_NAMES.keys()}\"\n-        )\n-    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n-    benchmark_fn(num_samples, batch_size, jit_compile)\n-\n-\n-if __name__ == \"__main__\":\n-    app.run(main)\n\n@@ -97,31 +97,10 @@ def benchmark_layer_normalization(\n     )\n \n \n-def benchmark_unit_normalization(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"UnitNormalization\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n     \"benchmark_group_normalization\": benchmark_group_normalization,\n     \"benchmark_layer_normalization\": benchmark_layer_normalization,\n-    \"benchmark_unit_normalization\": benchmark_unit_normalization,\n }\n \n \n\n@@ -1,373 +0,0 @@\n-\"\"\" Benchmark pooling layers.\n-\n-To run benchmarks, see the following command for an example, please change the\n-flag to your custom value:\n-\n-```\n-python3 -m benchmarks.layer_benchmark.pooling_benchmark \\\n-    --benchmark_name=benchmark_max_pooling1d \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n-    --jit_compile=True\n-```\n-\"\"\"\n-\n-\n-from absl import app\n-from absl import flags\n-\n-from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n-\n-FLAGS = flags.FLAGS\n-\n-\n-def benchmark_average_pooling1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"AveragePooling1D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_average_pooling2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"AveragePooling2D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_average_pooling3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"AveragePooling3D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[64, 64, 64, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_max_pooling1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"MaxPooling1D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_max_pooling2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"MaxPooling2D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_max_pooling3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"MaxPooling3D\"\n-    init_args = {\n-        \"pool_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[64, 64, 64, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_average_pooling1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalAveragePooling1D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_average_pooling2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalAveragePooling2D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_average_pooling3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalAveragePooling3D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[64, 64, 64, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_max_pooling1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalMaxPooling1D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_max_pooling2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalMaxPooling2D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_global_max_pooling3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GlobalMaxPooling3D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[64, 64, 64, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-BENCHMARK_NAMES = {\n-    \"benchmark_average_pooling1d\": benchmark_average_pooling1d,\n-    \"benchmark_average_pooling2d\": benchmark_average_pooling2d,\n-    \"benchmark_average_pooling3d\": benchmark_average_pooling3d,\n-    \"benchmark_max_pooling1d\": benchmark_max_pooling1d,\n-    \"benchmark_max_pooling2d\": benchmark_max_pooling2d,\n-    \"benchmark_max_pooling3d\": benchmark_max_pooling3d,\n-    \"benchmark_global_average_pooling1d\": benchmark_global_average_pooling1d,\n-    \"benchmark_global_average_pooling2d\": benchmark_global_average_pooling2d,\n-    \"benchmark_global_average_pooling3d\": benchmark_global_average_pooling3d,\n-    \"benchmark_global_max_pooling1d\": benchmark_global_max_pooling1d,\n-    \"benchmark_global_max_pooling2d\": benchmark_global_max_pooling2d,\n-    \"benchmark_global_max_pooling3d\": benchmark_global_max_pooling3d,\n-}\n-\n-\n-def main(_):\n-    benchmark_name = FLAGS.benchmark_name\n-    num_samples = FLAGS.num_samples\n-    batch_size = FLAGS.batch_size\n-    jit_compile = FLAGS.jit_compile\n-\n-    if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n-            benchmark_fn(num_samples, batch_size, jit_compile)\n-        return\n-\n-    if benchmark_name not in BENCHMARK_NAMES:\n-        raise ValueError(\n-            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n-            f\"be one of {BENCHMARK_NAMES.keys()}\"\n-        )\n-    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n-    benchmark_fn(num_samples, batch_size, jit_compile)\n-\n-\n-if __name__ == \"__main__\":\n-    app.run(main)\n\n@@ -1,216 +0,0 @@\n-\"\"\" Benchmark regularization layers.\n-\n-To run benchmarks, see the following command for an example, please change the\n-flag to your custom value:\n-\n-```\n-python3 -m benchmarks.layer_benchmark.regularization_benchmark \\\n-    --benchmark_name=benchmark_dropout\\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n-    --jit_compile=True\n-```\n-\"\"\"\n-\n-from absl import app\n-from absl import flags\n-\n-from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n-\n-FLAGS = flags.FLAGS\n-\n-\n-def benchmark_dropout(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Dropout\"\n-    init_args = {\n-        \"rate\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_gaussian_dropout(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GaussionDropout\"\n-    init_args = {\n-        \"rate\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_gaussian_noise(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GaussionNoise\"\n-    init_args = {\n-        \"stddev\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 4],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_spatial_dropout1D(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SpatialDropout1D\"\n-    init_args = {\n-        \"rate\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_spatial_dropout2D(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SpatialDropout2D\"\n-    init_args = {\n-        \"rate\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_spatial_dropout3D(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SpatialDropout3D\"\n-    init_args = {\n-        \"rate\": 0.5,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 32, 32, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-BENCHMARK_NAMES = {\n-    \"benchmark_dropout\": benchmark_dropout,\n-    \"benchmark_gaussian_dropout\": benchmark_gaussian_dropout,\n-    \"benchmark_gaussian_noise\": benchmark_gaussian_noise,\n-    \"benchmark_spatial_dropout1D\": benchmark_spatial_dropout1D,\n-    \"benchmark_spatial_dropout2D\": benchmark_spatial_dropout2D,\n-    \"benchmark_spatial_dropout3D\": benchmark_spatial_dropout3D,\n-}\n-\n-\n-def main(_):\n-    benchmark_name = FLAGS.benchmark_name\n-    num_samples = FLAGS.num_samples\n-    batch_size = FLAGS.batch_size\n-    jit_compile = FLAGS.jit_compile\n-\n-    if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n-            benchmark_fn(num_samples, batch_size, jit_compile)\n-        return\n-\n-    if benchmark_name not in BENCHMARK_NAMES:\n-        raise ValueError(\n-            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n-            f\"be one of {BENCHMARK_NAMES.keys()}\"\n-        )\n-    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n-    benchmark_fn(num_samples, batch_size, jit_compile)\n-\n-\n-if __name__ == \"__main__\":\n-    app.run(main)\n\n@@ -1,336 +0,0 @@\n-\"\"\" Benchmark reshaping layers.\n-\n-To run benchmarks, see the following command for an example, please change the\n-flag to your custom value:\n-\n-```\n-python3 -m benchmarks.layer_benchmark.reshaping_benchmark \\\n-    --benchmark_name=benchmark_cropping2d \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n-    --jit_compile=True\n-```\n-\"\"\"\n-\n-from absl import app\n-from absl import flags\n-\n-from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n-\n-FLAGS = flags.FLAGS\n-\n-\n-def benchmark_cropping1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Cropping1D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_cropping2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Cropping2D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_cropping3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Cropping3D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 32, 32, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_flatten(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Flatten\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_permute(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Permute\"\n-    init_args = {\n-        \"dim\": (2, 1),\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_up_sampling1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"UpSampling1D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_up_sampling2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"UpSampling2D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_up_sampling3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"UpSampling3D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 32, 32, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_zero_padding1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ZeroPadding1D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_zero_padding2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ZeroPadding2D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[256, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_zero_padding3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ZeroPadding3D\"\n-    init_args = {}\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 32, 32, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-BENCHMARK_NAMES = {\n-    \"benchmark_cropping1d\": benchmark_cropping1d,\n-    \"benchmark_cropping2d\": benchmark_cropping2d,\n-    \"benchmark_cropping3d\": benchmark_cropping3d,\n-    \"benchmark_flatten\": benchmark_flatten,\n-    \"benchmark_permute\": benchmark_permute,\n-    \"benchmark_up_sampling1d\": benchmark_up_sampling1d,\n-    \"benchmark_up_sampling2d\": benchmark_up_sampling2d,\n-    \"benchmark_up_sampling3d\": benchmark_up_sampling3d,\n-    \"benchmark_zero_padding1d\": benchmark_zero_padding1d,\n-    \"benchmark_zero_padding2d\": benchmark_zero_padding2d,\n-    \"benchmark_zero_padding3d\": benchmark_zero_padding3d,\n-}\n-\n-\n-def main(_):\n-    benchmark_name = FLAGS.benchmark_name\n-    num_samples = FLAGS.num_samples\n-    batch_size = FLAGS.batch_size\n-    jit_compile = FLAGS.jit_compile\n-\n-    if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n-            benchmark_fn(num_samples, batch_size, jit_compile)\n-        return\n-\n-    if benchmark_name not in BENCHMARK_NAMES:\n-        raise ValueError(\n-            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n-            f\"be one of {BENCHMARK_NAMES.keys()}\"\n-        )\n-    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n-    benchmark_fn(num_samples, batch_size, jit_compile)\n-\n-\n-if __name__ == \"__main__\":\n-    app.run(main)\n\n@@ -1,276 +0,0 @@\n-\"\"\" Benchmark rnn layers.\n-\n-To run benchmarks, see the following command for an example, please change the\n-flag to your custom value:\n-\n-```\n-python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n-    --benchmark_name=benchmark_lstm \\\n-    --num_samples=2048 \\\n-    --batch_size=256 \\\n-    --jit_compile=True\n-```\n-\"\"\"\n-\n-from absl import app\n-from absl import flags\n-\n-import keras_core\n-from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n-\n-FLAGS = flags.FLAGS\n-\n-\n-def benchmark_conv_lstm1d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ConvLSTM1D\"\n-    init_args = {\n-        \"filters\": 16,\n-        \"kernel_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 256, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_conv_lstm2d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ConvLSTM2D\"\n-    init_args = {\n-        \"filters\": 16,\n-        \"kernel_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 64, 64, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_conv_lstm3d(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"ConvLSTM3D\"\n-    init_args = {\n-        \"filters\": 16,\n-        \"kernel_size\": 2,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[16, 32, 32, 16, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_gru(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"GRU\"\n-    init_args = {\n-        \"units\": 32,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_lstm(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"LSTM\"\n-    init_args = {\n-        \"units\": 32,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_simple_rnn(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"SimpleRNN\"\n-    init_args = {\n-        \"units\": 32,\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_bidirectional(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"Bidirectional\"\n-    init_args = {\n-        \"layer\": keras_core.layers.LSTM(32),\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[32, 256],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-def benchmark_time_distributed(\n-    num_samples,\n-    batch_size,\n-    jit_compile=True,\n-):\n-    layer_name = \"TimeDistributed\"\n-    init_args = {\n-        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n-    }\n-    benchmark = LayerBenchmark(\n-        layer_name,\n-        init_args,\n-        input_shape=[10, 128, 128, 3],\n-        jit_compile=jit_compile,\n-    )\n-\n-    benchmark.benchmark_predict(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-    benchmark.benchmark_train(\n-        num_samples=num_samples,\n-        batch_size=batch_size,\n-    )\n-\n-\n-BENCHMARK_NAMES = {\n-    \"benchmark_conv_lstm1d\": benchmark_conv_lstm1d,\n-    \"benchmark_conv_lstm2d\": benchmark_conv_lstm2d,\n-    \"benchmark_conv_lstm3d\": benchmark_conv_lstm3d,\n-    \"benchmark_gru\": benchmark_gru,\n-    \"benchmark_lstm\": benchmark_lstm,\n-    \"benchmark_simple_rnn\": benchmark_simple_rnn,\n-    \"benchmark_bidirectional\": benchmark_bidirectional,\n-    \"benchmark_time_distributed\": benchmark_time_distributed,\n-}\n-\n-\n-def main(_):\n-    benchmark_name = FLAGS.benchmark_name\n-    num_samples = FLAGS.num_samples\n-    batch_size = FLAGS.batch_size\n-    jit_compile = FLAGS.jit_compile\n-\n-    if benchmark_name is None:\n-        for name, benchmark_fn in BENCHMARK_NAMES:\n-            benchmark_fn(num_samples, batch_size, jit_compile)\n-        return\n-\n-    if benchmark_name not in BENCHMARK_NAMES:\n-        raise ValueError(\n-            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n-            f\"be one of {BENCHMARK_NAMES.keys()}\"\n-        )\n-    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n-    benchmark_fn(num_samples, batch_size, jit_compile)\n-\n-\n-if __name__ == \"__main__\":\n-    app.run(main)\n\n@@ -1,105 +0,0 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core.backend import KerasTensor\n-from keras_core.layers.layer import Layer\n-from keras_core.operations import numpy as knp\n-from keras_core.operations.function import Function\n-\n-\n-class MiniDense(Layer):\n-    def __init__(self, units, name=None):\n-        super().__init__(name=name)\n-        self.units = units\n-\n-    def build(self, input_shape):\n-        input_dim = input_shape[-1]\n-        w_shape = (input_dim, self.units)\n-        w_value = initializers.GlorotUniform()(w_shape)\n-        self.w = backend.Variable(w_value)\n-\n-        b_shape = (self.units,)\n-        b_value = initializers.Zeros()(b_shape)\n-        self.b = backend.Variable(b_value)\n-\n-    def call(self, inputs):\n-        return knp.matmul(inputs, self.w) + self.b\n-\n-\n-class MiniDropout(Layer):\n-    def __init__(self, rate, name=None):\n-        super().__init__(name=name)\n-        self.rate = rate\n-        self.seed_generator = backend.random.SeedGenerator(1337)\n-\n-    def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n-\n-\n-class MiniBatchNorm(Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.epsilon = 1e-5\n-        self.momentum = 0.99\n-\n-    def build(self, input_shape):\n-        shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n-        )\n-        self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape), trainable=False\n-        )\n-        self.beta = backend.Variable(initializers.Zeros()(shape))\n-        self.gamma = backend.Variable(initializers.Ones()(shape))\n-\n-    def call(self, inputs, training=False):\n-        if training:\n-            mean = knp.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = knp.var(inputs, axis=(0,))\n-            outputs = (inputs - mean) / (variance + self.epsilon)\n-            self.variance.assign(\n-                self.variance * self.momentum + variance * (1.0 - self.momentum)\n-            )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n-        else:\n-            outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n-        outputs *= self.gamma\n-        outputs += self.beta\n-        return outputs\n-\n-\n-# Eager call\n-layer = MiniDense(5)\n-x = knp.zeros((3, 4))\n-y = layer(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-assert y.shape == (3, 5)\n-assert layer.built\n-print(layer.variables)\n-assert len(layer.variables) == 2\n-\n-# Symbolic call\n-x = KerasTensor((3, 4))\n-layer = MiniDense(5)\n-y = layer(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-assert y.shape == (3, 5)\n-assert layer.built\n-assert len(layer.variables) == 2\n-\n-# Symbolic graph building\n-x = KerasTensor((3, 4))\n-y = MiniDense(5)(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-fn = Function(inputs=x, outputs=y)\n-y_val = fn(knp.ones((3, 4)))\n-assert y_val.shape == (3, 5)\n-\n-print(y_val)\n\n@@ -1,168 +0,0 @@\n-import jax\n-import numpy as np\n-\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import operations as ops\n-from keras_core.layers.layer import Layer\n-from keras_core.optimizers import SGD\n-\n-\n-class MiniDense(Layer):\n-    def __init__(self, units, name=None):\n-        super().__init__(name=name)\n-        self.units = units\n-\n-    def build(self, input_shape):\n-        input_dim = input_shape[-1]\n-        w_shape = (input_dim, self.units)\n-        w_value = initializers.GlorotUniform()(w_shape)\n-        self.w = backend.Variable(w_value, name=\"kernel\")\n-\n-        b_shape = (self.units,)\n-        b_value = initializers.Zeros()(b_shape)\n-        self.b = backend.Variable(b_value, name=\"bias\")\n-\n-    def call(self, inputs):\n-        return ops.matmul(inputs, self.w) + self.b\n-\n-\n-class MiniDropout(Layer):\n-    def __init__(self, rate, name=None):\n-        super().__init__(name=name)\n-        self.rate = rate\n-        self.seed_generator = backend.random.SeedGenerator(1337)\n-\n-    def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n-\n-\n-class MiniBatchNorm(Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.epsilon = 1e-5\n-        self.momentum = 0.99\n-\n-    def build(self, input_shape):\n-        shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False, name=\"mean\"\n-        )\n-        self.variance = backend.Variable(\n-            initializers.Ones()(shape),\n-            trainable=False,\n-            name=\"variance\",\n-        )\n-        self.beta = backend.Variable(initializers.Zeros()(shape))\n-        self.gamma = backend.Variable(initializers.Ones()(shape))\n-\n-    def call(self, inputs, training=False):\n-        if training:\n-            mean = ops.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = ops.var(inputs, axis=(0,))\n-            outputs = (inputs - mean) / (variance + self.epsilon)\n-            self.variance.assign(\n-                self.variance * self.momentum + variance * (1.0 - self.momentum)\n-            )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n-        else:\n-            outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n-        outputs *= self.gamma\n-        outputs += self.beta\n-        return outputs\n-\n-\n-class MyModel(Layer):\n-    def __init__(self, units, num_classes):\n-        super().__init__()\n-        self.dense1 = MiniDense(units)\n-        # self.bn = MiniBatchNorm()\n-        self.dropout = MiniDropout(0.5)\n-        self.dense2 = MiniDense(num_classes)\n-\n-    def call(self, x):\n-        x = self.dense1(x)\n-        # x = self.bn(x)\n-        x = self.dropout(x)\n-        return self.dense2(x)\n-\n-\n-def Dataset():\n-    for _ in range(10):\n-        yield (np.random.random((8, 4)), np.random.random((8, 2)))\n-\n-\n-def loss_fn(y_true, y_pred):\n-    return ops.sum((y_true - y_pred) ** 2)\n-\n-\n-optimizer = SGD()\n-model = MyModel(8, 2)\n-dataset = Dataset()\n-\n-# Build model\n-x = ops.convert_to_tensor(np.random.random((8, 4)))\n-model(x)\n-# Build optimizer\n-optimizer.build(model.trainable_variables)\n-\n-\n-################################\n-## Currently operational workflow\n-\n-\n-def compute_loss_and_updates(\n-    trainable_variables, non_trainable_variables, x, y\n-):\n-    y_pred, non_trainable_variables = model.stateless_call(\n-        trainable_variables, non_trainable_variables, x\n-    )\n-\n-    loss = loss_fn(y, y_pred)\n-    return loss, non_trainable_variables\n-\n-\n-grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n-\n-\n-@jax.jit\n-def train_step(\n-    trainable_variables, non_trainable_variables, optimizer_variables, x, y\n-):\n-    (loss, non_trainable_variables), grads = grad_fn(\n-        trainable_variables, non_trainable_variables, x, y\n-    )\n-    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, trainable_variables, optimizer_variables\n-    )\n-    return trainable_variables, non_trainable_variables, optimizer_variables\n-\n-\n-# Training loop\n-trainable_variables = model.trainable_variables\n-non_trainable_variables = model.non_trainable_variables\n-optimizer_variables = optimizer.variables\n-for x, y in dataset:\n-    (\n-        trainable_variables,\n-        non_trainable_variables,\n-        optimizer_variables,\n-    ) = train_step(\n-        trainable_variables, non_trainable_variables, optimizer_variables, x, y\n-    )\n-\n-# Post-processing model state update\n-for variable, value in zip(model.trainable_variables, trainable_variables):\n-    print(variable.name, np.sum(np.abs(variable - value)))\n-    variable.assign(value)\n-for variable, value in zip(\n-    model.non_trainable_variables, non_trainable_variables\n-):\n-    print(variable.name, np.sum(np.abs(variable - value)))\n-    variable.assign(value)\n-\n-print(\"Updated values\")\n\n@@ -362,10 +362,9 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n-    size_x = size(x)\n     tf.assert_greater(\n-        size_x,\n-        tf.constant(0, dtype=size_x.dtype),\n+        size(x),\n+        tf.constant(0, dtype=tf.int64),\n         message=\"Cannot compute the min of an empty tensor.\",\n     )\n \n\n@@ -1,105 +0,0 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core.backend import KerasTensor\n-from keras_core.layers.layer import Layer\n-from keras_core.operations import numpy as knp\n-from keras_core.operations.function import Function\n-\n-\n-class MiniDense(Layer):\n-    def __init__(self, units, name=None):\n-        super().__init__(name=name)\n-        self.units = units\n-\n-    def build(self, input_shape):\n-        input_dim = input_shape[-1]\n-        w_shape = (input_dim, self.units)\n-        w_value = initializers.GlorotUniform()(w_shape)\n-        self.w = backend.Variable(w_value)\n-\n-        b_shape = (self.units,)\n-        b_value = initializers.Zeros()(b_shape)\n-        self.b = backend.Variable(b_value)\n-\n-    def call(self, inputs):\n-        return knp.matmul(inputs, self.w) + self.b\n-\n-\n-class MiniDropout(Layer):\n-    def __init__(self, rate, name=None):\n-        super().__init__(name=name)\n-        self.rate = rate\n-        self.seed_generator = backend.random.SeedGenerator(1337)\n-\n-    def call(self, inputs):\n-        return backend.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n-\n-\n-class MiniBatchNorm(Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.epsilon = 1e-5\n-        self.momentum = 0.99\n-\n-    def build(self, input_shape):\n-        shape = (input_shape[-1],)\n-        self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n-        )\n-        self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape), trainable=False\n-        )\n-        self.beta = backend.Variable(initializers.Zeros()(shape))\n-        self.gamma = backend.Variable(initializers.Ones()(shape))\n-\n-    def call(self, inputs, training=False):\n-        if training:\n-            mean = knp.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = knp.var(inputs, axis=(0,))\n-            outputs = (inputs - mean) / (variance + self.epsilon)\n-            self.variance.assign(\n-                self.variance * self.momentum + variance * (1.0 - self.momentum)\n-            )\n-            self.mean.assign(\n-                self.mean * self.momentum + mean * (1.0 - self.momentum)\n-            )\n-        else:\n-            outputs = (inputs - self.mean) / (self.variance + self.epsilon)\n-        outputs *= self.gamma\n-        outputs += self.beta\n-        return outputs\n-\n-\n-# Eager call\n-layer = MiniDense(5)\n-x = knp.zeros((3, 4))\n-y = layer(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-assert y.shape == (3, 5)\n-assert layer.built\n-print(layer.variables)\n-assert len(layer.variables) == 2\n-\n-# Symbolic call\n-x = KerasTensor((None, 4))\n-layer = MiniDense(5)\n-y = layer(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-assert y.shape == (None, 5)\n-assert layer.built\n-assert len(layer.variables) == 2\n-\n-# Symbolic graph building\n-x = KerasTensor((None, 4))\n-y = MiniDense(5)(x)\n-y = MiniBatchNorm()(y, training=True)\n-y = MiniDropout(0.5)(y)\n-fn = Function(inputs=x, outputs=y)\n-y_val = fn(knp.zeros((3, 4)))\n-assert y_val.shape == (3, 5)\n-\n-print(y_val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
