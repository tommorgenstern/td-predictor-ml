{"custom_id": "scrapy#613fd41f44d1455f9c9369087958674f3fdfcc8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 290 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -109,7 +109,7 @@ class CrawlerRun(object):\n         self.reqreached = []\n         self.itemerror = []\n         self.itemresp = []\n-        self.bytes = defaultdict(lambda: b\"\")\n+        self.bytes = defaultdict(lambda: list())\n         self.signals_caught = {}\n         self.spider_class = spider_class\n \n@@ -160,7 +160,7 @@ class CrawlerRun(object):\n         self.itemresp.append((item, response))\n \n     def bytes_received(self, data, request, spider):\n-        self.bytes[request] += data\n+        self.bytes[request].append(data)\n \n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n@@ -274,17 +274,18 @@ class EngineTest(unittest.TestCase):\n     def _assert_bytes_received(self):\n         self.assertEqual(8, len(self.run.bytes))\n         for request, data in self.run.bytes.items():\n+            joined_data = b\"\".join(data)\n             if self.run.getpath(request.url) == \"/\":\n-                self.assertEqual(data, get_testdata(\"test_site\", \"index.html\"))\n+                self.assertEqual(joined_data, get_testdata(\"test_site\", \"index.html\"))\n             elif self.run.getpath(request.url) == \"/item1.html\":\n-                self.assertEqual(data, get_testdata(\"test_site\", \"item1.html\"))\n+                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item1.html\"))\n             elif self.run.getpath(request.url) == \"/item2.html\":\n-                self.assertEqual(data, get_testdata(\"test_site\", \"item2.html\"))\n+                self.assertEqual(joined_data, get_testdata(\"test_site\", \"item2.html\"))\n             elif self.run.getpath(request.url) == \"/redirected\":\n-                self.assertEqual(data, b\"Redirected here\")\n+                self.assertEqual(joined_data, b\"Redirected here\")\n             elif self.run.getpath(request.url) == '/redirect':\n                 self.assertEqual(\n-                    data,\n+                    joined_data,\n                     b\"\\n<html>\\n\"\n                     b\"    <head>\\n\"\n                     b\"        <meta http-equiv=\\\"refresh\\\" content=\\\"0;URL=/redirected\\\">\\n\"\n@@ -296,7 +297,7 @@ class EngineTest(unittest.TestCase):\n                 )\n             elif self.run.getpath(request.url) == \"/tem999.html\":\n                 self.assertEqual(\n-                    data,\n+                    joined_data,\n                     b\"\\n<html>\\n\"\n                     b\"  <head><title>404 - No Such Resource</title></head>\\n\"\n                     b\"  <body>\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad4477d335bee8b10bc3bbca969defddd9b316f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 496 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,7 +107,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     if isinstance(exception_result, Failure):\n                         raise\n                     return exception_result\n-                else:\n                 if _isiterable(result):\n                     result = _evaluate_iterable(result, method_index+1, recovered)\n                 else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3b168948cb533427eceb68cf84bc8542732848b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 63 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -84,6 +84,6 @@ class RetryMiddleware(object):\n             return retryreq\n         else:\n             stats.inc_value('retry/max_reached')\n-            logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n+            logger.error(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                          {'request': request, 'retries': retries, 'reason': reason},\n                          extra={'spider': spider})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c9643d38cc076c4d2032efd994fda4cfcc9f88a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 312 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -12,6 +12,7 @@ module with the ``runserver`` argument::\n \n import os\n import re\n+import string\n import sys\n from collections import defaultdict\n from urllib.parse import urlparse\n@@ -90,6 +91,7 @@ def start_test_site(debug=False):\n     r = static.File(root_dir)\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n+    r.putChild(b\"random\", static.Data(string.ascii_letters.encode(\"utf8\") * 2**14, \"text/plain\"))\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n@@ -117,8 +119,12 @@ class CrawlerRun(object):\n         self.port = start_test_site()\n         self.portno = self.port.getHost().port\n \n-        start_urls = [self.geturl(\"/\"), self.geturl(\"/redirect\"),\n-                      self.geturl(\"/redirect\")]  # a duplicate\n+        start_urls = [\n+            self.geturl(\"/\"),\n+            self.geturl(\"/redirect\"),\n+            self.geturl(\"/redirect\"),  # duplicate\n+            self.geturl(\"/random\"),\n+        ]\n \n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n@@ -190,7 +196,7 @@ class EngineTest(unittest.TestCase):\n             self.run = CrawlerRun(spider)\n             yield self.run.run()\n             self._assert_visited_urls()\n-            self._assert_scheduled_requests(urls_to_visit=8)\n+            self._assert_scheduled_requests(urls_to_visit=9)\n             self._assert_downloaded_responses()\n             self._assert_scraped_items()\n             self._assert_signals_caught()\n@@ -200,7 +206,7 @@ class EngineTest(unittest.TestCase):\n     def test_crawler_dupefilter(self):\n         self.run = CrawlerRun(TestDupeFilterSpider)\n         yield self.run.run()\n-        self._assert_scheduled_requests(urls_to_visit=7)\n+        self._assert_scheduled_requests(urls_to_visit=8)\n         self._assert_dropped_requests()\n \n     @defer.inlineCallbacks\n@@ -237,8 +243,8 @@ class EngineTest(unittest.TestCase):\n \n     def _assert_downloaded_responses(self):\n         # response tests\n-        self.assertEqual(8, len(self.run.respplug))\n-        self.assertEqual(8, len(self.run.reqreached))\n+        self.assertEqual(9, len(self.run.respplug))\n+        self.assertEqual(9, len(self.run.reqreached))\n \n         for response, _ in self.run.respplug:\n             if self.run.getpath(response.url) == '/item999.html':\n@@ -272,7 +278,7 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('200', item['price'])\n \n     def _assert_bytes_received(self):\n-        self.assertEqual(8, len(self.run.bytes))\n+        self.assertEqual(9, len(self.run.bytes))\n         for request, data in self.run.bytes.items():\n             joined_data = b\"\".join(data)\n             if self.run.getpath(request.url) == \"/\":\n@@ -306,6 +312,8 @@ class EngineTest(unittest.TestCase):\n                     b\"  </body>\\n\"\n                     b\"</html>\\n\"\n                 )\n+            elif self.run.getpath(request.url) == \"/random\":\n+                self.assertTrue(len(data) > 1)  # signal was fired multiple times\n \n     def _assert_signals_caught(self):\n         assert signals.engine_started in self.run.signals_caught\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a499f38b14d16338d20084c0dcb24528a1f1f22f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1892 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -268,7 +268,7 @@ class ScrapyProxyAgent(Agent):\n         )\n \n \n-class ScrapyAgent(object):\n+class ScrapyAgent:\n \n     _Agent = Agent\n     _ProxyAgent = ScrapyProxyAgent\n@@ -438,7 +438,7 @@ class ScrapyAgent(object):\n \n \n @implementer(IBodyProducer)\n-class _RequestBodyProducer(object):\n+class _RequestBodyProducer:\n \n     def __init__(self, body):\n         self.body = body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f02a8dccb95373f22bac18c08d9fda8169dcb02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 16 | Methods Changed: 17 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 4026 | Contributors (this commit): 33 | Commits (past 90d): 49 | Contributors (cumulative): 64 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -32,8 +32,9 @@ logger = logging.getLogger(__name__)\n class HTTP11DownloadHandler:\n     lazy = False\n \n-    def __init__(self, settings, crawler=None):\n+    def __init__(self, settings, crawler=None, source=\"http11\"):\n         self.crawler = crawler\n+        self.source = source\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n@@ -67,8 +68,8 @@ class HTTP11DownloadHandler:\n         self._disconnect_timeout = 1\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler.settings, crawler)\n+    def from_crawler(cls, crawler, **kwargs):\n+        return cls(crawler.settings, crawler, **kwargs)\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -79,6 +80,7 @@ class HTTP11DownloadHandler:\n             warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n             fail_on_dataloss=self._fail_on_dataloss,\n             crawler=self.crawler,\n+            source=self.source,\n         )\n         return agent.download_request(request)\n \n@@ -275,7 +277,7 @@ class ScrapyAgent:\n     _TunnelingAgent = TunnelingAgent\n \n     def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n-                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None):\n+                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None, source=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n@@ -285,6 +287,7 @@ class ScrapyAgent:\n         self._fail_on_dataloss = fail_on_dataloss\n         self._txresponse = None\n         self._crawler = crawler\n+        self._source = source\n \n     def _get_agent(self, request, timeout):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n@@ -421,6 +424,7 @@ class ScrapyAgent:\n                 warnsize,\n                 fail_on_dataloss,\n                 self._crawler,\n+                self._source,\n             )\n         )\n \n@@ -457,7 +461,9 @@ class _RequestBodyProducer:\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler):\n+    def __init__(\n+        self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler, source\n+    ):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n@@ -469,6 +475,7 @@ class _ResponseReader(protocol.Protocol):\n         self._reached_warnsize = False\n         self._bytes_received = 0\n         self._crawler = crawler\n+        self._source = source\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n@@ -483,6 +490,7 @@ class _ResponseReader(protocol.Protocol):\n             data=bodyBytes,\n             request=self._request,\n             spider=self._crawler.spider,\n+            source=self._source,\n         )\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n\n@@ -73,6 +73,7 @@ class S3DownloadHandler:\n             objcls=httpdownloadhandler,\n             settings=settings,\n             crawler=crawler,\n+            source=\"s3\",\n         )\n         self._download_http = _http_handler.download_request\n \n\n@@ -730,6 +730,9 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n \n class HttpDownloadHandlerMock:\n \n+    def __init__(self, *args, **kwargs):\n+        pass\n+\n     def download_request(self, request, spider):\n         return request\n \n\n@@ -112,6 +112,7 @@ class CrawlerRun(object):\n         self.itemerror = []\n         self.itemresp = []\n         self.bytes = defaultdict(lambda: list())\n+        self.bytes_source = set()\n         self.signals_caught = {}\n         self.spider_class = spider_class\n \n@@ -165,8 +166,9 @@ class CrawlerRun(object):\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n-    def bytes_received(self, data, request, spider):\n+    def bytes_received(self, data, request, spider, source):\n         self.bytes[request].append(data)\n+        self.bytes_source.add(source)\n \n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n@@ -279,6 +281,7 @@ class EngineTest(unittest.TestCase):\n \n     def _assert_bytes_received(self):\n         self.assertEqual(9, len(self.run.bytes))\n+        self.assertEqual(self.run.bytes_source, set([\"http11\"]))\n         for request, data in self.run.bytes.items():\n             joined_data = b\"\".join(data)\n             if self.run.getpath(request.url) == \"/\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc825c21deaa56875f2abf1b30b53abb60c566c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 30 | Churn Cumulative: 952 | Contributors (this commit): 13 | Commits (past 90d): 15 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,6 +106,17 @@ class AsyncDefAsyncioSpider(SimpleSpider):\n         self.logger.info(\"Got response %d\" % status)\n \n \n+class AsyncDefAsyncioReturnSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_return'\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.info(\"Got response %d\" % status)\n+        return [{'id': 1}, {'id': 2}]\n+\n+\n class ItemSpider(FollowAllSpider):\n \n     name = 'item'\n\n@@ -6,13 +6,14 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n+from scrapy import signals\n from scrapy.crawler import CrawlerRunner\n from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n                            SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback,\n-                           AsyncDefSpider, AsyncDefAsyncioSpider)\n+                           AsyncDefSpider, AsyncDefAsyncioSpider, AsyncDefAsyncioReturnSpider)\n \n \n class CrawlTestCase(TestCase):\n@@ -326,3 +327,19 @@ with multiples lines\n         with LogCapture() as log:\n             yield runner.join()\n         self.assertIn(\"Got response 200\", str(log))\n+\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse_list(self):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioReturnSpider)\n+        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        self.assertIn(\"Got response 200\", str(log))\n+        self.assertIn({'id': 1}, items)\n+        self.assertIn({'id': 2}, items)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2ae380efcaa5a3419a4f6a35541ae0fb71a2e7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 330 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class CachingThreadedResolver(ThreadedResolver):\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n \n-    def install_on_reactor(self,):\n+    def install_on_reactor(self):\n         self.reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n\n@@ -21,7 +21,7 @@ class MockThreadedResolver(ThreadedResolver):\n     def from_crawler(cls, crawler, reactor):\n         return cls(reactor)\n \n-    def install_on_reactor(self,):\n+    def install_on_reactor(self):\n         self.reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb8f7dc609382153df79774ad9d8f6d33d064279", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 70 | Churn Cumulative: 347 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import argparse\n import json\n import os\n import random\n@@ -6,18 +7,19 @@ from subprocess import Popen, PIPE\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n-from twisted.web.server import Site, NOT_DONE_YET\n-from twisted.web.resource import Resource\n+from twisted.internet import defer, reactor, ssl\n+from twisted.internet.task import deferLater\n+from twisted.names import dns, error\n+from twisted.names.server import DNSServerFactory\n+from twisted.web.resource import EncodingResourceWrapper, Resource\n+from twisted.web.server import GzipEncoderFactory, NOT_DONE_YET, Site\n from twisted.web.static import File\n from twisted.web.test.test_webclient import PayloadResource\n-from twisted.web.server import GzipEncoderFactory\n-from twisted.web.resource import EncodingResourceWrapper\n from twisted.web.util import redirectTo\n-from twisted.internet import reactor, ssl\n-from twisted.internet.task import deferLater\n \n from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.ssl import SSL_OP_NO_TLSv1_3\n+from scrapy.utils.test import get_testenv\n \n \n def getarg(request, name, default=None, type=None):\n@@ -198,12 +200,10 @@ class Root(Resource):\n         return b'Scrapy mock HTTP server\\n'\n \n \n-class MockServer():\n+class MockServer:\n \n     def __enter__(self):\n-        from scrapy.utils.test import get_testenv\n-\n-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver'],\n+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'http'],\n                           stdout=PIPE, env=get_testenv())\n         http_address = self.proc.stdout.readline().strip().decode('ascii')\n         https_address = self.proc.stdout.readline().strip().decode('ascii')\n@@ -224,6 +224,37 @@ class MockServer():\n         return host + path\n \n \n+class MockDNSResolver:\n+    \"\"\"\n+    Implements twisted.internet.interfaces.IResolver partially\n+    \"\"\"\n+\n+    def _resolve(self, name):\n+        record = dns.Record_A(address=b\"127.0.0.1\")\n+        answer = dns.RRHeader(name=name, payload=record)\n+        return [answer], [], []\n+\n+    def query(self, query, timeout=None):\n+        if query.type == dns.A:\n+            return defer.succeed(self._resolve(query.name.name))\n+        return defer.fail(error.DomainError())\n+\n+    def lookupAllRecords(self, name, timeout=None):\n+        return defer.succeed(self._resolve(name))\n+\n+\n+class MockDNSServer():\n+\n+    def __enter__(self):\n+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', 'dns'],\n+                          stdout=PIPE, env=get_testenv())\n+        return self\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.proc.kill()\n+        self.proc.communicate()\n+\n+\n def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n     factory = ssl.DefaultOpenSSLContextFactory(\n          os.path.join(os.path.dirname(__file__), keyfile),\n@@ -238,6 +269,11 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n \n \n if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"-t\", \"--type\", type=str, choices=(\"http\", \"dns\"), default=\"http\")\n+    args = parser.parse_args()\n+\n+    if args.type == \"http\":\n         root = Root()\n         factory = Site(root)\n         httpPort = reactor.listenTCP(0, factory)\n@@ -247,10 +283,20 @@ if __name__ == \"__main__\":\n         def print_listening():\n             httpHost = httpPort.getHost()\n             httpsHost = httpsPort.getHost()\n-        httpAddress = 'http://%s:%d' % (httpHost.host, httpHost.port)\n-        httpsAddress = 'https://%s:%d' % (httpsHost.host, httpsHost.port)\n+            httpAddress = \"http://%s:%d\" % (httpHost.host, httpHost.port)\n+            httpsAddress = \"https://%s:%d\" % (httpsHost.host, httpsHost.port)\n             print(httpAddress)\n             print(httpsAddress)\n \n+    elif args.type == \"dns\":\n+        clients = [MockDNSResolver()]\n+        factory = DNSServerFactory(clients=clients)\n+        protocol = dns.DNSDatagramProtocol(controller=factory)\n+        reactor.listenUDP(10053, protocol)\n+        reactor.listenTCP(10053, factory)\n+\n+        def print_listening():\n+            print(\"DNS server running on port 10053\")\n+\n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0ef8ad2d6f958de6ce04cd7756e142efeb1a6a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 62 | Files Changed: 4 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 119 | Churn Cumulative: 1164 | Contributors (this commit): 14 | Commits (past 90d): 20 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,51 +0,0 @@\n-from urllib.parse import urlparse\n-\n-from twisted.internet import defer\n-from twisted.internet.base import ThreadedResolver\n-from twisted.internet.interfaces import IResolverSimple\n-from zope.interface.declarations import implementer\n-\n-from scrapy import Spider, Request\n-from scrapy.crawler import CrawlerProcess\n-\n-from tests.mockserver import MockServer\n-\n-\n-@implementer(IResolverSimple)\n-class MockThreadedResolver(ThreadedResolver):\n-    \"\"\"\n-    Resolves all names to localhost\n-    \"\"\"\n-\n-    @classmethod\n-    def from_crawler(cls, crawler, reactor):\n-        return cls(reactor)\n-\n-    def install_on_reactor(self):\n-        self.reactor.installResolver(self)\n-\n-    def getHostByName(self, name, timeout=None):\n-        return defer.succeed(\"127.0.0.1\")\n-\n-\n-class LocalhostSpider(Spider):\n-    name = \"localhost_spider\"\n-\n-    def start_requests(self):\n-        yield Request(self.url)\n-\n-    def parse(self, response):\n-        netloc = urlparse(response.url).netloc\n-        self.logger.info(\"Host: %s\" % netloc.split(\":\")[0])\n-        self.logger.info(\"Type: %s\" % type(response.ip_address))\n-        self.logger.info(\"IP address: %s\" % response.ip_address)\n-\n-\n-with MockServer() as mockserver:\n-    settings = {\"DNS_RESOLVER\": __name__ + \".MockThreadedResolver\"}\n-    process = CrawlerProcess(settings)\n-\n-    port = urlparse(mockserver.http_address).port\n-    url = \"http://not.a.real.domain:{port}/echo?body=test\".format(port=port)\n-    process.crawl(LocalhostSpider, url=url)\n-    process.start()\n\n@@ -0,0 +1,37 @@\n+from urllib.parse import urlparse\n+\n+from twisted.internet import reactor\n+from twisted.names.client import createResolver\n+\n+from scrapy import Spider, Request\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+from tests.mockserver import MockServer, MockDNSServer\n+\n+\n+class LocalhostSpider(Spider):\n+    name = \"localhost_spider\"\n+\n+    def start_requests(self):\n+        yield Request(self.url)\n+\n+    def parse(self, response):\n+        netloc = urlparse(response.url).netloc\n+        self.logger.info(\"Host: %s\" % netloc.split(\":\")[0])\n+        self.logger.info(\"Type: %s\" % type(response.ip_address))\n+        self.logger.info(\"IP address: %s\" % response.ip_address)\n+\n+\n+with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:\n+    port = urlparse(mock_http_server.http_address).port\n+    url = \"http://not.a.real.domain:{port}/echo\".format(port=port)\n+\n+    servers = [(mock_dns_server.host, mock_dns_server.port)]\n+    reactor.installResolver(createResolver(servers=servers))\n+\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    d = runner.crawl(LocalhostSpider, url=url)\n+    d.addBoth(lambda _: reactor.stop())\n+    reactor.run()\n\n@@ -246,8 +246,11 @@ class MockDNSResolver:\n class MockDNSServer():\n \n     def __enter__(self):\n-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', 'dns'],\n+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],\n                           stdout=PIPE, env=get_testenv())\n+        host, port = self.proc.stdout.readline().strip().decode('ascii').split(\":\")\n+        self.host = host\n+        self.port = int(port)\n         return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n@@ -292,11 +295,11 @@ if __name__ == \"__main__\":\n         clients = [MockDNSResolver()]\n         factory = DNSServerFactory(clients=clients)\n         protocol = dns.DNSDatagramProtocol(controller=factory)\n-        reactor.listenUDP(10053, protocol)\n-        reactor.listenTCP(10053, factory)\n+        listener = reactor.listenUDP(0, protocol)\n \n         def print_listening():\n-            print(\"DNS server running on port 10053\")\n+            host = listener.getHost()\n+            print(\"%s:%s\" % (host.host, host.port))\n \n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n@@ -281,9 +281,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n             self.assertNotIn(\"Asyncio reactor is installed\", str(log))\n \n \n-class CrawlerProcessSubprocess(unittest.TestCase):\n-    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerProcess')\n-\n+class ScriptRunnerMixin:\n     def run_script(self, script_name):\n         script_path = os.path.join(self.script_dir, script_name)\n         args = (sys.executable, script_path)\n@@ -292,6 +290,10 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n         stdout, stderr = p.communicate()\n         return stderr.decode('utf-8')\n \n+\n+class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n+    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerProcess')\n+\n     def test_simple(self):\n         log = self.run_script('simple.py')\n         self.assertIn('Spider closed (finished)', log)\n@@ -325,9 +327,13 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n         ]))\n \n+\n+class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n+    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerRunner')\n+\n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n-        self.assertIn(\"Spider closed (finished)\", log)\n-        self.assertIn(\"Host: not.a.real.domain\", log)\n-        self.assertIn(\"Type: <class 'ipaddress.IPv4Address'>\", log)\n-        self.assertIn(\"IP address: 127.0.0.1\", log)\n+        self.assertIn(\"INFO: Spider closed (finished)\", log)\n+        self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n+        self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n+        self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13670f0397ba8dcec3dceb1852bad5751406d19d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 167 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,8 @@ collect_ignore = [\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n-    *_py_files(\"tests/CrawlerProcess\")\n+    *_py_files(\"tests/CrawlerProcess\"),\n+    *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n for line in open('tests/ignores.txt'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad70497416527c3d882a64f7803e73155f3fa1da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 360 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -243,7 +243,7 @@ class MockDNSResolver:\n         return defer.succeed(self._resolve(name))\n \n \n-class MockDNSServer():\n+class MockDNSServer:\n \n     def __enter__(self):\n         self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbea370c58c1d82b52fd9c1f7d3a6cee94477c7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 500 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -59,12 +59,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n-        def _evaluate_iterable(iterable, method_index, recover_to):\n+        def _evaluate_iterable(iterable, exception_processor_index, recover_to):\n             try:\n                 for r in iterable:\n                     yield r\n             except Exception as ex:\n-                exception_result = process_spider_exception(Failure(ex), method_index)\n+                exception_result = process_spider_exception(Failure(ex), exception_processor_index)\n                 if isinstance(exception_result, Failure):\n                     raise\n                 recover_to.extend(exception_result)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#898bc00811aac9d3e38d1863b95a10c2e8effb02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 50 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -14,6 +14,7 @@ spider_error = object()\n request_scheduled = object()\n request_dropped = object()\n request_reached_downloader = object()\n+request_left_downloader = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae04174884eeb777d7b3caceed52bf522944ceb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 790 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -181,6 +181,9 @@ class Downloader(object):\n         def finish_transferring(_):\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n+            self.signals.send_catch_log(signal=signals.request_left_downloader,\n+                                        request=request,\n+                                        spider=spider)\n             return _\n \n         return dfd.addBoth(finish_transferring)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9916f6e556f9d4a41ea86d4a73687af1a40e43ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 59 | Churn Cumulative: 177 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,59 @@\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+from scrapy.signals import request_left_downloader\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer\n+\n+class SignalCatcherSpider(Spider):\n+    name = 'signal_catcher'\n+\n+    def __init__(self, crawler, url, *args, **kwargs):\n+        super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n+        crawler.signals.connect(self.on_response_download,\n+                                signal=request_left_downloader)\n+        self.catched_times = 0\n+        self.start_urls = [url]\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = cls(crawler, *args, **kwargs)\n+        return spider\n+\n+    def on_response_download(self, request, spider):\n+        self.catched_times = self.catched_times + 1\n+\n+\n+class TestCatching(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_success(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_timeout(self):\n+        crawler = get_crawler(SignalCatcherSpider,\n+                              {'DOWNLOAD_TIMEOUT': 0.1})\n+        yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_disconnect(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl(self.mockserver.url(\"/drop\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_noconnect(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')\n+        self.assertEqual(crawler.spider.catched_times, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3769f75386104c1a3072894b302d3c3239ff8c37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 178 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n+\n class SignalCatcherSpider(Spider):\n     name = 'signal_catcher'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#576663e5a778646aa4ef870641fa677ea94d21f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 807 | Contributors (this commit): 55 | Commits (past 90d): 7 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -225,7 +225,7 @@ MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_WARNING_MB = 0\n \n METAREFRESH_ENABLED = True\n-METAREFRESH_IGNORE_TAGS = ['script', 'noscript']\n+METAREFRESH_IGNORE_TAGS = []\n METAREFRESH_MAXDELAY = 100\n \n NEWSPIDER_MODULE = ''\n\n@@ -300,19 +300,21 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         body = ('''<noscript><meta http-equiv=\"refresh\" '''\n                 '''content=\"0;URL='http://example.org/newpage'\"></noscript>''')\n         rsp = HtmlResponse(req.url, body=body.encode())\n-        response = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(response, Response)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, 'http://example.org/newpage')\n \n-    def test_ignore_tags_empty_list(self):\n-        crawler = get_crawler(Spider, {'METAREFRESH_IGNORE_TAGS': []})\n+    def test_ignore_tags_1_x_list(self):\n+        \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n+        settings = {'METAREFRESH_IGNORE_TAGS': ['script', 'noscript']}\n+        crawler = get_crawler(Spider, settings)\n         mw = MetaRefreshMiddleware.from_crawler(crawler)\n         req = Request(url='http://example.org')\n         body = ('''<noscript><meta http-equiv=\"refresh\" '''\n                 '''content=\"0;URL='http://example.org/newpage'\"></noscript>''')\n         rsp = HtmlResponse(req.url, body=body.encode())\n-        req2 = mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, 'http://example.org/newpage')\n+        response = mw.process_response(req, rsp, self.spider)\n+        assert isinstance(response, Response)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a91a5427df4846ed9fa11612cfeb9e31f34a1c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 190 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ class SignalCatcherSpider(Spider):\n         super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n         crawler.signals.connect(self.on_response_download,\n                                 signal=request_left_downloader)\n-        self.catched_times = 0\n+        self.caught_times = 0\n         self.start_urls = [url]\n \n     @classmethod\n@@ -22,7 +22,7 @@ class SignalCatcherSpider(Spider):\n         return spider\n \n     def on_response_download(self, request, spider):\n-        self.catched_times = self.catched_times + 1\n+        self.caught_times = self.caught_times + 1\n \n \n class TestCatching(TestCase):\n@@ -38,23 +38,23 @@ class TestCatching(TestCase):\n     def test_success(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n+        self.assertEqual(crawler.spider.caught_times, 1)\n \n     @defer.inlineCallbacks\n     def test_timeout(self):\n         crawler = get_crawler(SignalCatcherSpider,\n                               {'DOWNLOAD_TIMEOUT': 0.1})\n         yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n+        self.assertEqual(crawler.spider.caught_times, 1)\n \n     @defer.inlineCallbacks\n     def test_disconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/drop\"))\n-        self.assertEqual(crawler.spider.catched_times, 1)\n+        self.assertEqual(crawler.spider.caught_times, 1)\n \n     @defer.inlineCallbacks\n     def test_noconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')\n-        self.assertEqual(crawler.spider.catched_times, 1)\n+        self.assertEqual(crawler.spider.caught_times, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4be19e443e9c101a248c21509ae8000ce500d51a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 194 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ class SignalCatcherSpider(Spider):\n \n     def __init__(self, crawler, url, *args, **kwargs):\n         super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n-        crawler.signals.connect(self.on_response_download,\n+        crawler.signals.connect(self.on_request_left,\n                                 signal=request_left_downloader)\n         self.caught_times = 0\n         self.start_urls = [url]\n@@ -21,7 +21,7 @@ class SignalCatcherSpider(Spider):\n         spider = cls(crawler, *args, **kwargs)\n         return spider\n \n-    def on_response_download(self, request, spider):\n+    def on_request_left(self, request, spider):\n         self.caught_times = self.caught_times + 1\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#489ffcda5143a2ef28d4cbcf5418babd963f2b0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 39 | Churn Cumulative: 39 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,39 @@\n+from twisted.internet import defer\n+from twisted.trial import unittest\n+\n+from scrapy import signals, Request, Spider\n+from scrapy.utils.test import get_crawler\n+\n+from tests.mockserver import MockServer\n+\n+\n+class ItemSpider(Spider):\n+    name = 'itemspider'\n+\n+    def start_requests(self):\n+        for _ in range(10):\n+            yield Request(self.mockserver.url('/status?n=200'),\n+                          dont_filter=True)\n+\n+    def parse(self, response):\n+        return {'field': 42}\n+\n+\n+class AsyncSignalTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self.items = []\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    async def _on_item_scraped(self, item):\n+        self.items.append(item)\n+\n+    @defer.inlineCallbacks\n+    def test_simple_pipeline(self):\n+        crawler = get_crawler(ItemSpider)\n+        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35dafef7f106dbf1c022d997b4e29e3eee84de7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 60 | Files Changed: 14 | Hunks: 44 | Methods Changed: 21 | Complexity Δ (Sum/Max): -1/5 | Churn Δ: 196 | Churn Cumulative: 4801 | Contributors (this commit): 69 | Commits (past 90d): 71 | Contributors (cumulative): 141 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,6 @@ from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_reactor_installed\n from scrapy.utils.log import (\n     configure_logging,\n     get_scrapy_root_handler,\n@@ -23,6 +22,7 @@ from scrapy.utils.log import (\n )\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n+from scrapy.utils.reactor import install_reactor, verify_installed_reactor\n \n \n logger = logging.getLogger(__name__)\n@@ -138,7 +138,7 @@ class CrawlerRunner:\n         self._crawlers = set()\n         self._active = set()\n         self.bootstrap_failed = False\n-        self._handle_asyncio_reactor()\n+        self._handle_twisted_reactor()\n \n     @property\n     def spiders(self):\n@@ -232,10 +232,9 @@ class CrawlerRunner:\n         while self._active:\n             yield defer.DeferredList(self._active)\n \n-    def _handle_asyncio_reactor(self):\n-        if self.settings.getbool('ASYNCIO_REACTOR') and not is_asyncio_reactor_installed():\n-            raise Exception(\"ASYNCIO_REACTOR is on but the Twisted asyncio \"\n-                            \"reactor is not installed.\")\n+    def _handle_twisted_reactor(self):\n+        if self.settings.get(\"TWISTED_REACTOR\"):\n+            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n \n \n class CrawlerProcess(CrawlerRunner):\n@@ -324,10 +323,10 @@ class CrawlerProcess(CrawlerRunner):\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n             pass\n \n-    def _handle_asyncio_reactor(self):\n-        if self.settings.getbool('ASYNCIO_REACTOR'):\n-            install_asyncio_reactor()\n-        super()._handle_asyncio_reactor()\n+    def _handle_twisted_reactor(self):\n+        if self.settings.get(\"TWISTED_REACTOR\"):\n+            install_reactor(self.settings[\"TWISTED_REACTOR\"])\n+        super()._handle_twisted_reactor()\n \n \n def _get_spider_loader(settings):\n\n@@ -19,8 +19,6 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n-ASYNCIO_REACTOR = False\n-\n AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n AUTOTHROTTLE_MAX_DELAY = 60.0\n@@ -291,6 +289,8 @@ TELNETCONSOLE_HOST = '127.0.0.1'\n TELNETCONSOLE_USERNAME = 'scrapy'\n TELNETCONSOLE_PASSWORD = None\n \n+TWISTED_REACTOR = None\n+\n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n     'scrapy.contracts.default.UrlContract': 1,\n\n@@ -1,17 +0,0 @@\n-import asyncio\n-from contextlib import suppress\n-\n-from twisted.internet import asyncioreactor\n-from twisted.internet.error import ReactorAlreadyInstalledError\n-\n-\n-def install_asyncio_reactor():\n-    \"\"\" Tries to install AsyncioSelectorReactor\n-    \"\"\"\n-    with suppress(ReactorAlreadyInstalledError):\n-        asyncioreactor.install(asyncio.get_event_loop())\n-\n-\n-def is_asyncio_reactor_installed():\n-    from twisted.internet import reactor\n-    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n@@ -2,14 +2,14 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n import asyncio\n-from functools import wraps\n import inspect\n+from functools import wraps\n \n from twisted.internet import defer, task\n from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.asyncio import is_asyncio_reactor_installed\n+from scrapy.utils.reactor import is_asyncio_reactor_installed\n \n \n def defer_fail(_failure):\n\n@@ -1,17 +1,16 @@\n # -*- coding: utf-8 -*-\n \n-import sys\n import logging\n+import sys\n import warnings\n from logging.config import dictConfig\n \n-from twisted.python.failure import Failure\n from twisted.python import log as twisted_log\n+from twisted.python.failure import Failure\n \n import scrapy\n-from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncio import is_asyncio_reactor_installed\n+from scrapy.settings import Settings\n from scrapy.utils.versions import scrapy_components_versions\n \n \n@@ -149,8 +148,8 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n-    if is_asyncio_reactor_installed():\n-        logger.debug(\"Asyncio reactor is installed\")\n+    from twisted.internet import reactor\n+    logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n \n \n class StreamLogger(object):\n\n@@ -1,4 +1,9 @@\n-from twisted.internet import error\n+import asyncio\n+from contextlib import suppress\n+\n+from twisted.internet import asyncioreactor, error\n+\n+from scrapy.utils.misc import load_object\n \n \n def listen_tcp(portrange, host, factory):\n@@ -42,3 +47,31 @@ class CallLaterOnce(object):\n     def __call__(self):\n         self._call = None\n         return self._func(*self._a, **self._kw)\n+\n+\n+def install_reactor(reactor_path):\n+    reactor_class = load_object(reactor_path)\n+    if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n+        with suppress(error.ReactorAlreadyInstalledError):\n+            asyncioreactor.install(asyncio.get_event_loop())\n+    else:\n+        *module, _ = reactor_path.split(\".\")\n+        installer_path = module + [\"install\"]\n+        installer = load_object(\".\".join(installer_path))\n+        with suppress(error.ReactorAlreadyInstalledError):\n+            installer()\n+\n+\n+def verify_installed_reactor(reactor_path):\n+    from twisted.internet import reactor\n+    reactor_class = load_object(reactor_path)\n+    if not isinstance(reactor, reactor_class):\n+        msg = \"The installed reactor ({}.{}) does not match the requested one ({})\".format(\n+            reactor.__module__, reactor.__class__.__name__, reactor_path\n+        )\n+        raise Exception(msg)\n+\n+\n+def is_asyncio_reactor_installed():\n+    from twisted.internet import reactor\n+    return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n@@ -10,8 +10,7 @@ class NoRequestsSpider(scrapy.Spider):\n \n \n process = CrawlerProcess(settings={\n-    'ASYNCIO_REACTOR': True,\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n })\n-\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -15,8 +15,7 @@ class NoRequestsSpider(scrapy.Spider):\n \n \n process = CrawlerProcess(settings={\n-    'ASYNCIO_REACTOR': True,\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n })\n-\n process.crawl(NoRequestsSpider)\n process.start()\n\n@@ -0,0 +1,13 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = 'asyncio_reactor'\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+})\n+process.crawl(AsyncioReactorSpider)\n+process.start()\n\n@@ -0,0 +1,13 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class PollReactorSpider(scrapy.Spider):\n+    name = 'poll_reactor'\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n+})\n+process.crawl(PollReactorSpider)\n+process.start()\n\n@@ -0,0 +1,13 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class SelectReactorSpider(scrapy.Spider):\n+    name = 'epoll_reactor'\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n+})\n+process.crawl(SelectReactorSpider)\n+process.start()\n\n@@ -296,12 +296,14 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"badspider.py\", log)\n \n     def test_asyncio_enabled_true(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_REACTOR=True'])\n-        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n+        ])\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     def test_asyncio_enabled_false(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_REACTOR=False'])\n-        self.assertNotIn(\"DEBUG: Asyncio reactor is installed\", log)\n+        log = self.get_log(self.debug_log_spider, args=[])\n+        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n \n class BenchCommandTest(CommandTest):\n\n@@ -254,30 +254,38 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     def test_crawler_runner_asyncio_enabled_true(self):\n         if self.reactor_pytest == 'asyncio':\n-            runner = CrawlerRunner(settings={'ASYNCIO_REACTOR': True})\n+            runner = CrawlerRunner(settings={\n+                \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            })\n         else:\n-            msg = \"ASYNCIO_REACTOR is on but the Twisted asyncio reactor is not installed\"\n+            msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n-                runner = CrawlerRunner(settings={'ASYNCIO_REACTOR': True})\n+                runner = CrawlerRunner(settings={\n+                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                })\n \n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n             if self.reactor_pytest == 'asyncio':\n-                runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': True})\n+                runner = CrawlerProcess(settings={\n+                    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                })\n                 yield runner.crawl(NoRequestsSpider)\n-                self.assertIn(\"Asyncio reactor is installed\", str(log))\n+                self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", str(log))\n             else:\n-                msg = \"ASYNCIO_REACTOR is on but the Twisted asyncio reactor is not installed\"\n+                msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n                 with self.assertRaisesRegex(Exception, msg):\n-                    runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': True})\n+                    runner = CrawlerProcess(settings={\n+                        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+                    })\n \n     @defer.inlineCallbacks\n     def test_crawler_process_asyncio_enabled_false(self):\n-        runner = CrawlerProcess(settings={'ASYNCIO_REACTOR': False})\n+        runner = CrawlerProcess(settings={\"TWISTED_REACTOR\": None})\n         with LogCapture(level=logging.DEBUG) as log:\n             yield runner.crawl(NoRequestsSpider)\n-            self.assertNotIn(\"Asyncio reactor is installed\", str(log))\n+            self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", str(log))\n \n \n class CrawlerProcessSubprocess(unittest.TestCase):\n@@ -294,17 +302,17 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_simple(self):\n         log = self.run_script('simple.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertNotIn(\"DEBUG: Asyncio reactor is installed\", log)\n+        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script('asyncio_enabled_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertIn(\"DEBUG: Asyncio reactor is installed\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     def test_ipv6_default_name_resolver(self):\n         log = self.run_script('default_name_resolver.py')\n@@ -323,3 +331,18 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n             \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n         ]))\n+\n+    def test_reactor_select(self):\n+        log = self.run_script(\"twisted_reactor_select.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n+\n+    def test_reactor_poll(self):\n+        log = self.run_script(\"twisted_reactor_poll.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n+\n+    def test_reactor_asyncio(self):\n+        log = self.run_script(\"twisted_reactor_asyncio.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from pytest import mark\n \n-from scrapy.utils.asyncio import is_asyncio_reactor_installed, install_asyncio_reactor\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, install_reactor\n \n \n @mark.usefixtures('reactor_pytest')\n@@ -14,4 +14,4 @@ class AsyncioTest(TestCase):\n \n     def test_install_asyncio_reactor(self):\n         # this should do nothing\n-        install_asyncio_reactor()\n+        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
