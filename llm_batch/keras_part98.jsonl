{"custom_id": "keras#b0d52d930aa00b7d0876abffe38e33d8a307b117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 968 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ from __future__ import absolute_import\n import copy\n import inspect\n import types\n+import numpy\n \n from ..utils.np_utils import to_categorical\n from ..models import Sequential\n@@ -202,9 +203,17 @@ class KerasClassifier(BaseWrapper):\n         # Returns\n             proba: array-like, shape `(n_samples, n_outputs)`\n                 Class probability estimates.\n+                In the case of binary classification (i.e. 1 output of 0 or 1)\n+                 will return '(n_samples, 2)'\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n-        return self.model.predict_proba(X, **kwargs)\n+        probs = self.model.predict_proba(X, **kwargs)\n+\n+        # check if binary classification\n+        if probs.shape[1] == 1:\n+            # first column is probability of class 0 and second is of class 1\n+            probs = numpy.hstack([1 - probs, probs])\n+        return probs\n \n     def score(self, X, y, **kwargs):\n         '''Returns the mean accuracy on the given test data and labels.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25c10af59694c6a61778f9111e3ca97dfd8971b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 1874 | Contributors (this commit): 10 | Commits (past 90d): 36 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -915,12 +915,16 @@ class Model(Container):\n             raise Exception('You must compile a model before training/testing.'\n                             ' Use `model.compile(optimizer, loss)`.')\n \n+        # if using sparse cce replace last dim of output shapes with 1\n+        output_shapes = self.internal_output_shapes\n+        if self.loss == \"sparse_categorical_crossentropy\":\n+            output_shapes = [s[:-1] + (1,) for s in output_shapes]\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes,\n                                    check_batch_dim=False,\n                                    exception_prefix='model input')\n         y = standardize_input_data(y, self.output_names,\n-                                   self.internal_output_shapes,\n+                                   output_shapes,\n                                    check_batch_dim=False,\n                                    exception_prefix='model target')\n         sample_weights = standardize_sample_weights(sample_weight,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7e39347b99dd5489b089ffac8a525db0ff5cc03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 616 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class BatchNormalization(Layer):\n \n     # Arguments\n         epsilon: small float > 0. Fuzz parameter.\n-        mode: integer, 0 or 1.\n+        mode: integer, 0, 1 or 2.\n             - 0: feature-wise normalization.\n                 Each feature map in the input will\n                 be normalized separately. The axis on which\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6ecab58cb9bcd4da5836d540e2f83617239c352", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2838 | Contributors (this commit): 15 | Commits (past 90d): 43 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1409,7 +1409,8 @@ def merge(inputs, mode='sum', concat_axis=-1,\n         output_shape: shape tuple (tuple of integers), or lambda/function\n             to compute output_shape (only if merge mode is a lambda/function).\n             If the latter case, it should take as input a list of shape tuples\n-            (1:1 mapping to input tensors) and return a single shape tuple.\n+            (1:1 mapping to input tensors) and return a single shape tuple, including the\n+            batch size (same convention as the `get_output_shape_for` method of layers).\n         node_indices: optional list of integers containing\n             the output node index for each input layer\n             (in case some input layers have multiple output nodes).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8d605db55a1b7b64d9e10b961ac5cc0674cdaa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1188 | Contributors (this commit): 13 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -543,7 +543,7 @@ class DirectoryIterator(Iterator):\n             for fname in os.listdir(subpath):\n                 is_valid = False\n                 for extension in white_list_formats:\n-                    if fname.endswith('.' + extension):\n+                    if fname.lower().endswith('.' + extension):\n                         is_valid = True\n                         break\n                 if is_valid:\n@@ -559,7 +559,7 @@ class DirectoryIterator(Iterator):\n             for fname in os.listdir(subpath):\n                 is_valid = False\n                 for extension in white_list_formats:\n-                    if fname.endswith('.' + extension):\n+                    if fname.lower().endswith('.' + extension):\n                         is_valid = True\n                         break\n                 if is_valid:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#825beb42c4dbf55ecebfb3021f3b59133e52bb5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 1885 | Contributors (this commit): 11 | Commits (past 90d): 37 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1044,6 +1044,17 @@ class Model(Container):\n \n         # prepare display labels\n         out_labels = self.metrics_names\n+\n+        # rename duplicated metrics name\n+        new_out_labels = []\n+        for i, label in enumerate(out_labels):\n+            new_label = label\n+            if out_labels.count(label) > 1:\n+                dup_idx = out_labels[:i].count(label)\n+                new_label += str(dup_idx + 1)\n+            new_out_labels.append(new_label)\n+        out_labels = new_out_labels\n+\n         if do_validation:\n             callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4b3a052a4f49f6ba5cdd93e983de45df595bc12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 2872 | Contributors (this commit): 16 | Commits (past 90d): 46 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -1046,14 +1046,15 @@ class Model(Container):\n         out_labels = self.metrics_names\n \n         # rename duplicated metrics name\n-        new_out_labels = []\n+        # (can happen with an output layer shared among multiple dataflows)\n+        deduped_out_labels = []\n         for i, label in enumerate(out_labels):\n             new_label = label\n             if out_labels.count(label) > 1:\n                 dup_idx = out_labels[:i].count(label)\n-                new_label += str(dup_idx + 1)\n-            new_out_labels.append(new_label)\n-        out_labels = new_out_labels\n+                new_label += '_' + str(dup_idx + 1)\n+            deduped_out_labels.append(new_label)\n+        out_labels = deduped_out_labels\n \n         if do_validation:\n             callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n\n@@ -2,7 +2,7 @@ from __future__ import absolute_import\n import copy\n import inspect\n import types\n-import numpy\n+import numpy as np\n \n from ..utils.np_utils import to_categorical\n from ..models import Sequential\n@@ -203,8 +203,10 @@ class KerasClassifier(BaseWrapper):\n         # Returns\n             proba: array-like, shape `(n_samples, n_outputs)`\n                 Class probability estimates.\n-                In the case of binary classification (i.e. 1 output of 0 or 1)\n-                 will return '(n_samples, 2)'\n+                In the case of binary classification,\n+                tp match the scikit-learn API,\n+                will return an array of shape '(n_samples, 2)'\n+                (instead of `(n_sample, 1)` as in Keras).\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n         probs = self.model.predict_proba(X, **kwargs)\n@@ -212,7 +214,7 @@ class KerasClassifier(BaseWrapper):\n         # check if binary classification\n         if probs.shape[1] == 1:\n             # first column is probability of class 0 and second is of class 1\n-            probs = numpy.hstack([1 - probs, probs])\n+            probs = np.hstack([1 - probs, probs])\n         return probs\n \n     def score(self, X, y, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e0c8cf25bad948bb5886ff25b1d4c7781ab51c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 51 | Churn Cumulative: 589 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from __future__ import absolute_import\n+import numpy as np\n from . import backend as K\n \n \n@@ -16,6 +17,47 @@ class Regularizer(object):\n         return {'name': self.__class__.__name__}\n \n \n+class EigenvalueRegularizer(Regularizer):\n+    '''This takes a constant that controls the \n+    regularization by Eigenvalue Decay on the \n+    current layer and outputs the regularized \n+    loss (evaluated on the training data) and \n+    the original loss (evaluated on the \n+    validation data).\n+    '''\n+    def __init__(self, k):\n+        self.k = k\n+        self.uses_learning_phase = True\n+\n+    def set_param(self, p):\n+        self.p = p\n+\n+    def __call__(self, loss):\n+        power = 9  # number of iterations of the power method\n+        W = self.p\n+        if K.ndim(W) > 2:\n+            raise Exception('Eigenvalue Decay regularizer '\n+                            'is only available for dense '\n+                            'and embedding layers.')\n+        WW = K.dot(K.transpose(W), W)\n+        dim1, dim2 = K.eval(K.shape(WW))  # number of neurons in the layer\n+        k = self.k\n+        \n+        # power method for approximating the dominant eigenvector:\n+        o = K.ones([dim1, 1])  # initial values for the dominant eigenvector\n+        domin_eigenvect = K.dot(WW, o)\n+        for n in range(power - 1):\n+            domin_eigenvect = K.dot(WW, domin_eigenvect)    \n+        \n+        WWd = K.dot(WW, domin_eigenvect)\n+        \n+        # the corresponding dominant eigenvalue:\n+        domin_eigenval = K.dot(K.transpose(WWd), domin_eigenvect) / K.dot(K.transpose(domin_eigenvect), domin_eigenvect)\n+        regularized_loss = loss + (domin_eigenval ** 0.5) * self.k  # multiplied by the given regularization gain\n+\n+        return K.in_train_phase(regularized_loss[0, 0], loss)\n+\n+\n class WeightRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n\n@@ -51,6 +51,15 @@ def create_model(weight_reg=None, activity_reg=None):\n     return model\n \n \n+def test_Eigenvalue_reg():\n+    (X_train, Y_train), (X_test, Y_test), test_ids = get_data()\n+    reg = regularizers.EigenvalueRegularizer(0.01)\n+    model = create_model(weight_reg=reg)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+    \n+    \n def test_W_reg():\n     (X_train, Y_train), (X_test, Y_test), test_ids = get_data()\n     for reg in [regularizers.l1(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab4bf447c658636cc2f6ae53cf9f0e639b6a4595", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 30 | Churn Cumulative: 2431 | Contributors (this commit): 29 | Commits (past 90d): 39 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -389,8 +389,11 @@ def expand_dims(x, dim=-1):\n def squeeze(x, axis):\n     '''Remove a 1-dimension from the tensor at index \"axis\".\n     '''\n-    x = T.addbroadcast(x, axis)\n-    return T.squeeze(x)\n+    broadcastable = x.broadcastable[:axis] + x.broadcastable[axis+1:]\n+    x = T.patternbroadcast(x, [i == axis for i in range(x.type.ndim)])\n+    x = T.squeeze(x)\n+    x = T.patternbroadcast(x, broadcastable)\n+    return x\n \n \n def temporal_padding(x, padding=1):\n\n@@ -38,6 +38,26 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     assert zth.shape == ztf.shape\n     assert_allclose(zth, ztf, atol=1e-05)\n \n+def check_composed_tensor_operations(first_function_name, first_function_args,\n+                                     second_function_name, second_function_args,\n+                                     input_shape):\n+    ''' Creates a random tensor t0 with shape input_shape and compute\n+                 t1 = first_function_name(t0, **first_function_args)\n+                 t2 = second_function_name(t1, **second_function_args)\n+        with both Theano and TensorFlow backends and ensures the answers match.\n+    '''\n+    val = np.random.random(input_shape) - 0.5\n+    xth = KTH.variable(val)\n+    xtf = KTF.variable(val)\n+\n+    yth = getattr(KTH, first_function_name)(xth, **first_function_args)\n+    ytf = getattr(KTF, first_function_name)(xtf, **first_function_args)\n+\n+    zth = KTH.eval(getattr(KTH, second_function_name)(yth, **second_function_args))\n+    ztf = KTF.eval(getattr(KTF, second_function_name)(ytf, **second_function_args))\n+\n+    assert zth.shape == ztf.shape\n+    assert_allclose(zth, ztf, atol=1e-05)   \n \n class TestBackend(object):\n \n@@ -70,6 +90,9 @@ class TestBackend(object):\n         check_single_tensor_operation('expand_dims', (4, 3), dim=-1)\n         check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n         check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n+        check_composed_tensor_operations('reshape', {'shape':(4,3,1,1)}, \n+                                         'squeeze', {'axis':2}, \n+                                         (4, 3, 1, 1))\n \n     def test_repeat_elements(self):\n         reps = 3\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e49ba233f994ae349609d0c72b2defaaf8b7991c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3547 | Contributors (this commit): 18 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -164,9 +164,9 @@ class Convolution1D(Layer):\n                           dim_ordering='th')\n         if self.bias:\n             output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n-        output = self.activation(output)\n         output = K.squeeze(output, 3)  # remove the dummy 3rd dimension\n         output = K.permute_dimensions(output, (0, 2, 1))\n+        output = self.activation(output)\n         return output\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4c2d8bbd4e387bcb65129ce26fb8770c10f528f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 88 | Churn Cumulative: 1292 | Contributors (this commit): 15 | Commits (past 90d): 11 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -415,6 +415,87 @@ class Adamax(Optimizer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class Nadam(Optimizer):\n+    '''\n+    Nesterov Adam optimizer: Adam ~ RMSProp + momentum, Nadam ~ RMSProp + NAG\n+\n+    Default parameters follow those provided in the paper.\n+\n+    Hard-coded values for warming momentum schedule calculation\n+    (used in schedule_decay, momentum_cache_t, momentum_cache_t_1, lines 456-458)\n+    are given in [1] with reference in [2] (p.4 eq.5) and strongly motivated\n+    to keep these values hard-coded and constant.\n+\n+    # Arguments\n+        lr: float >= 0. Learning rate.\n+        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        epsilon: float >= 0. Fuzz factor.\n+\n+    # References\n+    [1] Nadam report - http://cs229.stanford.edu/proj2015/054_report.pdf\n+    [2] On the importance of initialization and momentum in deep learning -\n+        http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n+    '''\n+    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+                 epsilon=1e-8, **kwargs):\n+        super(Nadam, self).__init__(**kwargs)\n+        self.__dict__.update(locals())\n+        self.iterations = K.variable(0.)\n+        self.m_schedule = K.variable(1.)\n+        self.lr = K.variable(lr)\n+        self.beta_1 = K.variable(beta_1)\n+        self.beta_2 = K.variable(beta_2)\n+\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n+        self.updates = [(self.iterations, self.iterations + 1)]\n+\n+        t = self.iterations + 1\n+\n+        # Due to the recommendations in [2], i.e. warming momentum schedule\n+        schedule_decay = 0.004  # Exactly given in [1] and [2]\n+        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * schedule_decay)))\n+        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * schedule_decay)))\n+        m_schedule_new = self.m_schedule * momentum_cache_t\n+        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n+        self.updates.append((self.m_schedule, m_schedule_new))\n+\n+        ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        vs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+\n+        self.weights = ms + vs\n+\n+        for p, g, m, v in zip(params, grads, ms, vs):\n+            # the following equations given in [1]\n+            g_prime = g / (1. - m_schedule_new)\n+            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n+            m_t_prime = m_t / (1. - m_schedule_next)\n+            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n+            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n+            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n+\n+            self.updates.append((m, m_t))\n+            self.updates.append((v, v_t))\n+\n+            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n+            new_p = p_t\n+\n+            # apply constraints\n+            if p in constraints:\n+                c = constraints[p]\n+                new_p = c(new_p)\n+            self.updates.append((p, new_p))\n+        return self.updates\n+\n+    def get_config(self):\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'beta_1': float(K.get_value(self.beta_1)),\n+                  'beta_2': float(K.get_value(self.beta_2)),\n+                  'epsilon': self.epsilon}\n+        base_config = super(Nadam, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n # aliases\n sgd = SGD\n rmsprop = RMSprop\n@@ -422,6 +503,7 @@ adagrad = Adagrad\n adadelta = Adadelta\n adam = Adam\n adamax = Adamax\n+nadam = Nadam\n \n \n def get(identifier, kwargs=None):\n\n@@ -2,7 +2,7 @@ from __future__ import print_function\n import pytest\n \n from keras.utils.test_utils import get_test_data\n-from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax\n+from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.utils.np_utils import to_categorical\n@@ -63,5 +63,9 @@ def test_adamax():\n     _test_optimizer(Adamax())\n \n \n+def test_nadam():\n+    _test_optimizer(Nadam())\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c30432a665e23d598d2cec11094b0cc3cac2f46d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 1278 | Contributors (this commit): 14 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -417,14 +417,12 @@ class Adamax(Optimizer):\n \n class Nadam(Optimizer):\n     '''\n-    Nesterov Adam optimizer: Adam ~ RMSProp + momentum, Nadam ~ RMSProp + NAG\n+    Nesterov Adam optimizer: Much like Adam is essentially RMSprop with momentum,\n+    Nadam is Adam RMSprop with Nesterov momentum.\n \n     Default parameters follow those provided in the paper.\n-\n-    Hard-coded values for warming momentum schedule calculation\n-    (used in schedule_decay, momentum_cache_t, momentum_cache_t_1, lines 456-458)\n-    are given in [1] with reference in [2] (p.4 eq.5) and strongly motivated\n-    to keep these values hard-coded and constant.\n+    It is recommended to leave the parameters of this optimizer\n+    at their default values.\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n@@ -437,7 +435,7 @@ class Nadam(Optimizer):\n             http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n     '''\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, **kwargs):\n+                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n@@ -445,6 +443,7 @@ class Nadam(Optimizer):\n         self.lr = K.variable(lr)\n         self.beta_1 = K.variable(beta_1)\n         self.beta_2 = K.variable(beta_2)\n+        self.schedule_decay = schedule_decay\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n@@ -453,9 +452,8 @@ class Nadam(Optimizer):\n         t = self.iterations + 1\n \n         # Due to the recommendations in [2], i.e. warming momentum schedule\n-        schedule_decay = 0.004  # Exactly given in [1] and [2]\n-        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * schedule_decay)))\n-        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * schedule_decay)))\n+        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(0.96, t * self.schedule_decay)))\n+        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(0.96, (t + 1) * self.schedule_decay)))\n         m_schedule_new = self.m_schedule * momentum_cache_t\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n@@ -491,7 +489,8 @@ class Nadam(Optimizer):\n         config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n-                  'epsilon': self.epsilon}\n+                  'epsilon': self.epsilon,\n+                  'schedule_decay': self.schedule_decay}\n         base_config = super(Nadam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f8e4574dc1d807fe98553369499da892077f3d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 7615 | Contributors (this commit): 45 | Commits (past 90d): 24 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -730,13 +730,16 @@ class Sequential(Model):\n         return copy.deepcopy(config)\n \n     @classmethod\n-    def from_config(cls, config, layer_cache={}):\n+    def from_config(cls, config, layer_cache=None):\n         '''Supports legacy formats\n         '''\n         from keras.utils.layer_utils import layer_from_config\n         from keras.layers import Merge\n         assert type(config) is list\n \n+        if not layer_cache:\n+            layer_cache = {}\n+\n         def normalize_legacy_config(conf):\n             if 'class_name' not in conf:\n                 class_name = conf['name']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b83a1b1ac69d3bb3c2ac55e61878df1fd392768", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 295 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -189,7 +189,7 @@ def eval_loss_and_grads(x):\n class Evaluator(object):\n     def __init__(self):\n         self.loss_value = None\n-        self.grads_values = None\n+        self.grad_values = None\n \n     def loss(self, x):\n         assert self.loss_value is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7397f4b0d0d4ffd0685a2a9c84d407b348199f5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 4026 | Contributors (this commit): 38 | Commits (past 90d): 67 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -314,17 +314,23 @@ def prod(x, axis=None, keepdims=False):\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n-def std(x, axis=None, keepdims=False):\n-    '''Standard deviation of a tensor, alongside the specificied axis.\n+def var(x, axis=None, keepdims=False):\n+    '''Variance of a tensor, alongside the specified axis.\n     '''\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, _FLOATX)\n     m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=True)\n     devs_squared = tf.square(x - m)\n-    return tf.sqrt(tf.reduce_mean(devs_squared,\n+    return tf.reduce_mean(devs_squared,\n                           reduction_indices=axis,\n-                                  keep_dims=keepdims))\n+                          keep_dims=keepdims)\n+\n+\n+def std(x, axis=None, keepdims=False):\n+    '''Standard deviation of a tensor, alongside the specified axis.\n+    '''\n+    return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n \n \n def mean(x, axis=None, keepdims=False):\n\n@@ -200,6 +200,10 @@ def std(x, axis=None, keepdims=False):\n     return T.std(x, axis=axis, keepdims=keepdims)\n \n \n+def var(x, axis=None, keepdims=False):\n+    return T.var(x, axis=axis, keepdims=keepdims)\n+\n+\n def any(x, axis=None, keepdims=False):\n     '''Bitwise reduction (logical OR).\n     '''\n\n@@ -139,7 +139,7 @@ class BatchNormalization(Layer):\n         elif self.mode == 1:\n             # sample-wise normalization\n             m = K.mean(x, axis=-1, keepdims=True)\n-            std = K.std(x, axis=-1, keepdims=True)\n+            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n             x_normed = (x - m) / (std + self.epsilon)\n             out = self.gamma * x_normed + self.beta\n         return out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9aee4126e3c72a49f9ab2af86f053326ac0a4e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 1899 | Contributors (this commit): 11 | Commits (past 90d): 39 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -235,7 +235,10 @@ def collect_trainable_weights(layer):\n     if not trainable:\n         return []\n     weights = []\n-    if layer.__class__.__name__ in ['Sequential', 'Model']:\n+    if layer.__class__.__name__ == 'Sequential':\n+        for sublayer in layer.flattened_layers:\n+            weights += collect_trainable_weights(sublayer)\n+    elif layer.__class__.__name__ == 'Model':\n         for sublayer in layer.layers:\n             weights += collect_trainable_weights(sublayer)\n     elif layer.__class__.__name__ == 'Graph':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53aaa842ed4cb3b8619032eb6c5deedef32fdbc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 208 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -206,8 +206,10 @@ class Tokenizer(object):\n                 elif mode == 'binary':\n                     X[i][j] = 1\n                 elif mode == 'tfidf':\n-                    tf = np.log(c / len(seq))\n-                    df = (1 + np.log(1 + self.index_docs.get(j, 0) / (1 + self.document_count)))\n+                    # Use weighting scheme 2 in\n+                    #   https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n+                    tf = 1 + np.log(c)\n+                    df = np.log(1 + self.index_docs.get(j, 0) / (1 + self.document_count))\n                     X[i][j] = tf / df\n                 else:\n                     raise Exception('Unknown vectorization mode: ' + str(mode))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b122ba25f79ab0a43ff8d78f65cfba36a7bcee5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 1922 | Contributors (this commit): 11 | Commits (past 90d): 40 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,8 @@ from ..utils.generic_utils import Progbar\n from .. import callbacks as cbks\n \n \n-def standardize_input_data(data, names, shapes=None, check_batch_dim=True,\n+def standardize_input_data(data, names, shapes=None,\n+                           check_batch_dim=True,\n                            exception_prefix=''):\n     '''Users may pass data as a list of arrays, dictionary of arrays,\n     or as a single array. We normalize this to an ordered list of\n@@ -84,6 +85,8 @@ def standardize_input_data(data, names, shapes=None, check_batch_dim=True,\n     # check shapes compatibility\n     if shapes:\n         for i in range(len(names)):\n+            if shapes[i] is None:\n+                continue\n             array = arrays[i]\n             if len(array.shape) != len(shapes[i]):\n                 raise Exception('Error when checking ' + exception_prefix +\n@@ -918,10 +921,14 @@ class Model(Container):\n             raise Exception('You must compile a model before training/testing.'\n                             ' Use `model.compile(optimizer, loss)`.')\n \n-        # if using sparse cce replace last dim of output shapes with 1\n-        output_shapes = self.internal_output_shapes\n-        if self.loss == \"sparse_categorical_crossentropy\":\n-            output_shapes = [s[:-1] + (1,) for s in output_shapes]\n+        output_shapes = []\n+        for output_shape, loss_fn in zip(self.internal_output_shapes, self.loss_functions):\n+            if loss_fn.__name__ == 'sparse_categorical_crossentropy':\n+                output_shapes.append(output_shape[:-1] + (1,))\n+            elif getattr(objectives, loss_fn.__name__, None) is None:\n+                output_shapes.append(None)\n+            else:\n+                output_shapes.append(output_shape)\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes,\n                                    check_batch_dim=False,\n@@ -1401,7 +1408,7 @@ class Model(Container):\n                     outs = self.train_on_batch(x, y,\n                                                sample_weight=sample_weight,\n                                                class_weight=class_weight)\n-                except Exception as e:\n+                except:\n                     _stop.set()\n                     raise\n \n@@ -1503,7 +1510,7 @@ class Model(Container):\n                                 'or (x, y). Found: ' + str(generator_output))\n             try:\n                 outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n-            except Exception as e:\n+            except:\n                 _stop.set()\n                 raise\n \n@@ -1575,7 +1582,7 @@ class Model(Container):\n \n             try:\n                 outs = self.predict_on_batch(x)\n-            except Exception as e:\n+            except:\n                 _stop.set()\n                 raise\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c53c64d7fa8d80990d18d662093227f2fbe55ed7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 210 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from ..utils.data_utils import get_file\n from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n+import sys\n \n \n def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0,\n@@ -64,4 +65,11 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0,\n def get_word_index(path=\"reuters_word_index.pkl\"):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n     f = open(path, 'rb')\n-    return cPickle.load(f)\n+\n+    if sys.version_info < (3,):\n+        data = cPickle.load(f)\n+    else:\n+        data = cPickle.load(f, encoding=\"latin1\")\n+\n+    f.close()\n+    return data\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#936360020cbd9e9e174c47554c49bfc9c96898f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 212 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -209,8 +209,8 @@ class Tokenizer(object):\n                     # Use weighting scheme 2 in\n                     #   https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n                     tf = 1 + np.log(c)\n-                    df = np.log(1 + self.index_docs.get(j, 0) / (1 + self.document_count))\n-                    X[i][j] = tf / df\n+                    idf = np.log(1 + self.document_count / (1 + self.index_docs.get(j, 0)))\n+                    X[i][j] = tf * idf\n                 else:\n                     raise Exception('Unknown vectorization mode: ' + str(mode))\n         return X\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10368d867fa25f5fee957378911a2888e85811ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 213 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@\n from a fast Cython rewrite.\n '''\n from __future__ import absolute_import\n+from __future__ import division\n \n import string\n import sys\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f42160021872c8d327336995e877f5d63fcf7038", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1568 | Contributors (this commit): 19 | Commits (past 90d): 9 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -246,7 +246,7 @@ class ModelCheckpoint(Callback):\n     def __init__(self, filepath, monitor='val_loss', verbose=0,\n                  save_best_only=False, mode='auto'):\n \n-        super(Callback, self).__init__()\n+        super(ModelCheckpoint, self).__init__()\n         self.monitor = monitor\n         self.verbose = verbose\n         self.filepath = filepath\n@@ -313,7 +313,7 @@ class EarlyStopping(Callback):\n             monitored has stopped increasing.\n     '''\n     def __init__(self, monitor='val_loss', patience=0, verbose=0, mode='auto'):\n-        super(Callback, self).__init__()\n+        super(EarlyStopping, self).__init__()\n \n         self.monitor = monitor\n         self.patience = patience\n@@ -369,6 +369,7 @@ class RemoteMonitor(Callback):\n             of event data.\n     '''\n     def __init__(self, root='http://localhost:9000'):\n+        super(RemoteMonitor, self).__init__()\n         self.root = root\n \n     def on_epoch_end(self, epoch, logs={}):\n@@ -435,7 +436,7 @@ class TensorBoard(Callback):\n     '''\n \n     def __init__(self, log_dir='./logs', histogram_freq=0, write_graph=True):\n-        super(Callback, self).__init__()\n+        super(TensorBoard, self).__init__()\n         if K._BACKEND != 'tensorflow':\n             raise Exception('TensorBoard callback only works '\n                             'with the TensorFlow backend.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a13a35fe523f79f85d7c540cfff05026f149eccd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2844 | Contributors (this commit): 15 | Commits (past 90d): 43 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1342,9 +1342,9 @@ class Merge(Layer):\n \n         if isinstance(self._output_shape, python_types.LambdaType):\n             if py3:\n-                output_shape = marshal.dumps(self._output_shape.__code__)\n+                output_shape = marshal.dumps(self._output_shape.__code__).decode('raw_unicode_escape')\n             else:\n-                output_shape = marshal.dumps(self._output_shape.func_code)\n+                output_shape = marshal.dumps(self._output_shape.func_code).decode('raw_unicode_escape')\n             output_shape_type = 'lambda'\n         elif callable(self._output_shape):\n             output_shape = self._output_shape.__name__\n@@ -1376,7 +1376,7 @@ class Merge(Layer):\n         if output_shape_type == 'function':\n             output_shape = globals()[config['output_shape']]\n         elif output_shape_type == 'lambda':\n-            output_shape = marshal.loads(config['output_shape'])\n+            output_shape = marshal.loads(config['output_shape'].encode('raw_unicode_escape'))\n             output_shape = python_types.FunctionType(output_shape, globals())\n         else:\n             output_shape = config['output_shape']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e37df7ca853431f0579d66307ba3f7e4d7cdd20b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10431 | Contributors (this commit): 48 | Commits (past 90d): 19 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -460,9 +460,9 @@ class Lambda(Layer):\n \n         if isinstance(self._output_shape, python_types.LambdaType):\n             if py3:\n-                output_shape = marshal.dumps(self._output_shape.__code__)\n+                output_shape = marshal.dumps(self._output_shape.__code__).decode('raw_unicode_escape')\n             else:\n-                output_shape = marshal.dumps(self._output_shape.func_code)\n+                output_shape = marshal.dumps(self._output_shape.func_code).decode('raw_unicode_escape')\n             output_shape_type = 'lambda'\n         elif callable(self._output_shape):\n             output_shape = self._output_shape.__name__\n@@ -494,7 +494,7 @@ class Lambda(Layer):\n         if output_shape_type == 'function':\n             output_shape = globals()[config['output_shape']]\n         elif output_shape_type == 'lambda':\n-            output_shape = marshal.loads(config['output_shape'])\n+            output_shape = marshal.loads(config['output_shape'].encode('raw_unicode_escape'))\n             output_shape = python_types.FunctionType(output_shape, globals())\n         else:\n             output_shape = config['output_shape']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60e0c96f6c4ac60f174b733b94342939e086d214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1924 | Contributors (this commit): 12 | Commits (past 90d): 40 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ def standardize_input_data(data, names, shapes=None,\n                     raise Exception('Error when checking ' + exception_prefix +\n                                     ': you are passing a list as '\n                                     'input to your model, '\n-                                    'but the model expects a '\n+                                    'but the model expects '\n                                     'a list of ' + str(len(names)) +\n                                     ' Numpy arrays instead. '\n                                     'The list you passed was: ' +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35134724673402593aed45c96368b976deb83ddf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 29 | Churn Cumulative: 1984 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -327,17 +327,17 @@ class EarlyStopping(Callback):\n \n         if mode == 'min':\n             self.monitor_op = np.less\n-            self.best = np.Inf\n         elif mode == 'max':\n             self.monitor_op = np.greater\n-            self.best = -np.Inf\n         else:\n             if 'acc' in self.monitor:\n                 self.monitor_op = np.greater\n-                self.best = -np.Inf\n             else:\n                 self.monitor_op = np.less\n-                self.best = np.Inf\n+\n+    def on_train_begin(self, logs={}):\n+        self.wait = 0       # Allow instances to be re-used\n+        self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n \n     def on_epoch_end(self, epoch, logs={}):\n         current = logs.get(self.monitor)\n\n@@ -105,6 +105,27 @@ def test_EarlyStopping():\n                         validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=20)\n \n \n+def test_EarlyStopping_reuse():\n+    patience = 3\n+    data = np.random.random((100, 1))\n+    labels = np.where(data > 0.5, 1, 0)\n+    model = Sequential((\n+        Dense(1, input_dim=1, activation='relu'),\n+        Dense(1, activation='sigmoid'),\n+    ))\n+    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n+    stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n+    weights = model.get_weights()\n+\n+    hist = model.fit(data, labels, callbacks=[stopper])\n+    assert len(hist.epoch) >= patience\n+\n+    # This should allow training to go for at least `patience` epochs\n+    model.set_weights(weights)\n+    hist = model.fit(data, labels, callbacks=[stopper])\n+    assert len(hist.epoch) >= patience\n+\n+\n def test_LearningRateScheduler():\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n                                                          nb_test=test_samples,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
