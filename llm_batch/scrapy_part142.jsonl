{"custom_id": "scrapy#1e12c92b8f4114b75bac7bed714d2e59e12b701d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 231 | Lines Deleted: 205 | Files Changed: 33 | Hunks: 165 | Methods Changed: 107 | Complexity Δ (Sum/Max): 14/6 | Churn Δ: 436 | Churn Cumulative: 8383 | Contributors (this commit): 14 | Commits (past 90d): 68 | Contributors (cumulative): 73 | DMM Complexity: 0.9629629629629629\n\nDIFF:\n@@ -8,7 +8,6 @@ from collections import defaultdict\n \n from twisted.internet import reactor\n from twisted.python import log as txlog\n-from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals, log\n \n@@ -29,12 +28,12 @@ class CloseSpider(object):\n         if self.errorcount:\n             txlog.addObserver(self.catch_log)\n         if self.pagecount:\n-            dispatcher.connect(self.page_count, signal=signals.response_received)\n+            crawler.signals.connect(self.page_count, signal=signals.response_received)\n         if self.timeout:\n-            dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n         if self.itemcount:\n-            dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n+        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -3,30 +3,33 @@ Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n import datetime\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n from scrapy import signals\n-from scrapy.stats import stats\n \n class CoreStats(object):\n \n-    def __init__(self):\n-        dispatcher.connect(self.stats_spider_opened, signal=signals.stats_spider_opened)\n-        dispatcher.connect(self.stats_spider_closing, signal=signals.stats_spider_closing)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n+    def __init__(self, stats):\n+        self.stats = stats\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        o = cls(crawler.stats)\n+        crawler.signals.connect(o.stats_spider_opened, signal=signals.stats_spider_opened)\n+        crawler.signals.connect(o.stats_spider_closing, signal=signals.stats_spider_closing)\n+        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n+        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n+        return o\n \n     def stats_spider_opened(self, spider):\n-        stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n+        self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n \n     def stats_spider_closing(self, spider, reason):\n-        stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n-        stats.set_value('finish_reason', reason, spider=spider)\n+        self.stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n+        self.stats.set_value('finish_reason', reason, spider=spider)\n \n     def item_scraped(self, item, spider):\n-        stats.inc_value('item_scraped_count', spider=spider)\n+        self.stats.inc_value('item_scraped_count', spider=spider)\n \n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n-        stats.inc_value('item_dropped_count', spider=spider)\n-        stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)\n+        self.stats.inc_value('item_dropped_count', spider=spider)\n+        self.stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)\n\n@@ -5,11 +5,9 @@ import cPickle as pickle\n \n from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.stats import stats\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n@@ -19,19 +17,21 @@ from scrapy.utils.project import data_path\n \n class HttpCacheMiddleware(object):\n \n-    def __init__(self, settings):\n+    def __init__(self, settings, stats):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+        self.stats = stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings)\n+        o = cls(crawler.settings, crawler.stats)\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n+        return o\n \n     def spider_opened(self, spider):\n         self.storage.open_spider(spider)\n@@ -45,10 +45,10 @@ class HttpCacheMiddleware(object):\n         response = self.storage.retrieve_response(spider, request)\n         if response and self.is_cacheable_response(response):\n             response.flags.append('cached')\n-            stats.inc_value('httpcache/hit', spider=spider)\n+            self.stats.inc_value('httpcache/hit', spider=spider)\n             return response\n \n-        stats.inc_value('httpcache/miss', spider=spider)\n+        self.stats.inc_value('httpcache/miss', spider=spider)\n         if self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n@@ -57,7 +57,7 @@ class HttpCacheMiddleware(object):\n             and self.is_cacheable_response(response)\n             and 'cached' not in response.flags):\n             self.storage.store_response(spider, request, response)\n-            stats.inc_value('httpcache/store', spider=spider)\n+            self.stats.inc_value('httpcache/store', spider=spider)\n         return response\n \n     def is_cacheable_response(self, response):\n\n@@ -6,8 +6,6 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n import robotparser\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n from scrapy import signals, log\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n@@ -24,8 +22,8 @@ class RobotsTxtMiddleware(object):\n         self._parsers = {}\n         self._spider_netlocs = {}\n         self._useragents = {}\n-        dispatcher.connect(self.spider_opened, signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signals.spider_closed)\n+        crawler.signals.connect(self.spider_opened, signals.spider_opened)\n+        crawler.signals.connect(self.spider_closed, signals.spider_closed)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -1,30 +1,32 @@\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.request import request_httprepr\n from scrapy.utils.response import response_httprepr\n-from scrapy.stats import stats\n \n class DownloaderStats(object):\n \n+    def __init__(self, stats):\n+        self.stats = stats\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('DOWNLOADER_STATS'):\n             raise NotConfigured\n-        return cls()\n+        return cls(crawler.stats)\n \n     def process_request(self, request, spider):\n-        stats.inc_value('downloader/request_count', spider=spider)\n-        stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n+        self.stats.inc_value('downloader/request_count', spider=spider)\n+        self.stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n         reqlen = len(request_httprepr(request))\n-        stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n+        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n \n     def process_response(self, request, response, spider):\n-        stats.inc_value('downloader/response_count', spider=spider)\n-        stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n+        self.stats.inc_value('downloader/response_count', spider=spider)\n+        self.stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n         reslen = len(response_httprepr(response))\n-        stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n+        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n         return response\n \n     def process_exception(self, request, exception, spider):\n         ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n-        stats.inc_value('downloader/exception_count', spider=spider)\n-        stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n+        self.stats.inc_value('downloader/exception_count', spider=spider)\n+        self.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n\n@@ -15,7 +15,6 @@ from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n from scrapy import log, signals\n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n@@ -149,9 +148,14 @@ class FeedExporter(object):\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n         self.slots = {}\n-        dispatcher.connect(self.open_spider, signals.spider_opened)\n-        dispatcher.connect(self.close_spider, signals.spider_closed)\n-        dispatcher.connect(self.item_scraped, signals.item_scraped)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        o = cls()\n+        crawler.signals.connect(o.open_spider, signals.spider_opened)\n+        crawler.signals.connect(o.close_spider, signals.spider_closed)\n+        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n+        return o\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -1,6 +1,5 @@\n from twisted.internet import task\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy import log, signals\n \n@@ -19,12 +18,20 @@ class LogStats(object):\n         self.interval = interval\n         self.slots = {}\n         self.multiplier = 60.0 / self.interval\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.response_received, signal=signals.response_received)\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        interval = settings.getfloat('LOGSTATS_INTERVAL')\n+        if not interval:\n+            raise NotConfigured\n+        o = cls(interval)\n+        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n+        crawler.signals.connect(o.response_received, signal=signals.response_received)\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n+        crawler.signals.connect(o.engine_started, signal=signals.engine_started)\n+        crawler.signals.connect(o.engine_stopped, signal=signals.engine_stopped)\n+        return o\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -6,30 +6,29 @@ See documentation in docs/topics/extensions.rst\n \n import gc\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n-from scrapy.stats import stats\n from scrapy.utils.trackref import live_refs\n \n class MemoryDebugger(object):\n \n-    def __init__(self, trackrefs=False):\n+    def __init__(self, stats, trackrefs=False):\n         try:\n             import libxml2\n             self.libxml2 = libxml2\n         except ImportError:\n             self.libxml2 = None\n+        self.stats = stats\n         self.trackrefs = trackrefs\n-        dispatcher.connect(self.engine_started, signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n             raise NotConfigured\n-        return cls(crawler.settings.getbool('TRACK_REFS'))\n+        o = cls(crawler.stats, crawler.settings.getbool('TRACK_REFS'))\n+        crawler.signals.connect(o.engine_started, signals.engine_started)\n+        crawler.signals.connect(o.engine_stopped, signals.engine_stopped)\n+        return o\n \n     def engine_started(self):\n         if self.libxml2:\n@@ -38,11 +37,11 @@ class MemoryDebugger(object):\n     def engine_stopped(self):\n         if self.libxml2:\n             self.libxml2.cleanupParser()\n-            stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))\n+            self.stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))\n         gc.collect()\n-        stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n+        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n         if self.trackrefs:\n             for cls, wdict in live_refs.iteritems():\n                 if not wdict:\n                     continue\n-                stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))\n+                self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))\n\n@@ -9,12 +9,9 @@ from pprint import pformat\n \n from twisted.internet import task\n \n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy import signals\n-from scrapy import log\n+from scrapy import signals, log\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.stats import stats\n from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n from scrapy.utils.engine import get_engine_status\n \n@@ -33,8 +30,8 @@ class MemoryUsage(object):\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n         self.mail = MailSender()\n-        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n+        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -44,7 +41,7 @@ class MemoryUsage(object):\n         return get_vmvalue_from_procfs('VmRSS')\n \n     def engine_started(self):\n-        stats.set_value('memusage/startup', self.get_virtual_size())\n+        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n         self.tasks = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n@@ -64,18 +61,18 @@ class MemoryUsage(object):\n                 tsk.stop()\n \n     def update(self):\n-        stats.max_value('memusage/max', self.get_virtual_size())\n+        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())\n \n     def _check_limit(self):\n         if self.get_virtual_size() > self.limit:\n-            stats.set_value('memusage/limit_reached', 1)\n+            self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit/1024/1024\n             log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n             if self.notify_mails:\n                 subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n-                stats.set_value('memusage/limit_notified', 1)\n+                self.crawler.stats.set_value('memusage/limit_notified', 1)\n             open_spiders = self.crawler.engine.open_spiders\n             if open_spiders:\n                 for spider in open_spiders:\n@@ -87,18 +84,19 @@ class MemoryUsage(object):\n         if self.warned: # warn only once\n             return\n         if self.get_virtual_size() > self.warning:\n-            stats.set_value('memusage/warning_reached', 1)\n+            self.crawler.stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning/1024/1024\n             log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n             if self.notify_mails:\n                 subj = \"%s warning: memory usage reached %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n-                stats.set_value('memusage/warning_notified', 1)\n+                self.crawler.stats.set_value('memusage/warning_notified', 1)\n             self.warned = True\n \n     def _send_report(self, rcpts, subject):\n         \"\"\"send notification mail with some additional useful info\"\"\"\n+        stats = self.crawler.stats\n         s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n         s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n         s += \"Current memory usage           : %dM\\r\\n\" % (self.get_virtual_size()/1024/1024)\n\n@@ -15,12 +15,9 @@ from collections import defaultdict\n from twisted.internet import defer, threads\n from PIL import Image\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy import log\n-from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n from scrapy.http import Request\n-from scrapy import signals\n from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest\n from scrapy.contrib.pipeline.media import MediaPipeline\n \n@@ -40,10 +37,6 @@ class FSImagesStore(object):\n         self.basedir = basedir\n         self._mkdir(self.basedir)\n         self.created_directories = defaultdict(set)\n-        dispatcher.connect(self.spider_closed, signals.spider_closed)\n-\n-    def spider_closed(self, spider):\n-        self.created_directories.pop(spider.name, None)\n \n     def persist_image(self, key, image, buf, info):\n         absolute_path = self._get_filesystem_path(key)\n@@ -274,8 +267,8 @@ class ImagesPipeline(MediaPipeline):\n             yield thumb_key, thumb_image, thumb_buf\n \n     def inc_stats(self, spider, status):\n-        stats.inc_value('image_count', spider=spider)\n-        stats.inc_value('image_status_count/%s' % status, spider=spider)\n+        spider.crawler.stats.inc_value('image_count', spider=spider)\n+        spider.crawler.stats.inc_value('image_status_count/%s' % status, spider=spider)\n \n     def convert_image(self, image, size=None):\n         if image.format == 'PNG' and image.mode == 'RGBA':\n\n@@ -19,16 +19,13 @@ class DepthMiddleware(object):\n         self.prio = prio\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_crawler(cls, crawler):\n+        settings = crawler.settings\n         maxdepth = settings.getint('DEPTH_LIMIT')\n         usestats = settings.getbool('DEPTH_STATS')\n         verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n         prio = settings.getint('DEPTH_PRIORITY')\n-        if usestats:\n-            from scrapy.stats import stats\n-        else:\n-            stats = None\n-        return cls(maxdepth, stats, verbose, prio)\n+        return cls(maxdepth, crawler.stats, verbose, prio)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n\n@@ -6,7 +6,6 @@ See documentation in docs/topics/spider-middleware.rst\n \n import re\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -17,8 +16,13 @@ class OffsiteMiddleware(object):\n     def __init__(self):\n         self.host_regexes = {}\n         self.domains_seen = {}\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        o = cls()\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n+        return o\n \n     def process_spider_output(self, response, result, spider):\n         for x in result:\n\n@@ -1,7 +1,6 @@\n import os, cPickle as pickle\n \n from scrapy import signals\n-from scrapy.xlib.pydispatch import dispatcher\n \n class SpiderState(object):\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n@@ -12,8 +11,8 @@ class SpiderState(object):\n     @classmethod\n     def from_crawler(cls, crawler):\n         obj = cls(crawler.settings.get('JOBDIR'))\n-        dispatcher.connect(obj.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(obj.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n+        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n         return obj\n \n     def spider_closed(self, spider):\n\n@@ -4,30 +4,29 @@ StatsMailer extension sends an email when a spider finishes scraping.\n Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \"\"\"\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.stats import stats\n from scrapy import signals\n from scrapy.mail import MailSender\n from scrapy.exceptions import NotConfigured\n \n class StatsMailer(object):\n \n-    def __init__(self, recipients):\n+    def __init__(self, stats, recipients):\n+        self.stats = stats\n         self.recipients = recipients\n-        if not self.recipients:\n-            raise NotConfigured\n-        dispatcher.connect(self.stats_spider_closed, signal=signals.stats_spider_closed)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n-        return cls(recipients)\n+        if not recipients:\n+            raise NotConfigured\n+        o = cls(crawler.stats, recipients)\n+        crawler.connect(o.stats_spider_closed, signal=signals.stats_spider_closed)\n+        return o\n         \n     def stats_spider_closed(self, spider, spider_stats):\n         mail = MailSender()\n         body = \"Global stats\\n\\n\"\n-        body += \"\\n\".join(\"%-50s : %s\" % i for i in stats.get_stats().items())\n+        body += \"\\n\".join(\"%-50s : %s\" % i for i in self.stats.get_stats().items())\n         body += \"\\n\\n%s stats\\n\\n\" % spider.name\n         body += \"\\n\".join(\"%-50s : %s\" % i for i in spider_stats.items())\n         mail.send(self.recipients, \"Scrapy stats for: %s\" % spider.name, body)\n\n@@ -1,4 +1,3 @@\n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n from scrapy.utils.httpobj import urlparse_cached\n@@ -63,8 +62,8 @@ class AutoThrottle(object):\n         if not settings.getbool('AUTOTHROTTLE_ENABLED'):\n             raise NotConfigured\n         self.crawler = crawler\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.response_received, signal=signals.response_received)\n+        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(self.response_received, signal=signals.response_received)\n         self.START_DELAY = settings.getfloat(\"AUTOTHROTTLE_START_DELAY\", 5.0)\n         self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n         self.MAX_CONCURRENCY = settings.getint(\"AUTOTHROTTLE_MAX_CONCURRENCY\", 8)\n\n@@ -1,9 +1,8 @@\n from scrapy.webservice import JsonRpcResource\n-from scrapy.stats import stats\n \n class StatsResource(JsonRpcResource):\n \n     ws_name = 'stats'\n \n     def __init__(self, crawler):\n-        JsonRpcResource.__init__(self, crawler, stats)\n+        JsonRpcResource.__init__(self, crawler, crawler.stats)\n\n@@ -8,7 +8,6 @@ from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.signal import send_catch_log\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -68,6 +67,7 @@ class Downloader(object):\n \n     def __init__(self, crawler):\n         self.settings = crawler.settings\n+        self.signals = crawler.signals\n         self.slots = {}\n         self.active = set()\n         self.handlers = DownloadHandlers()\n@@ -114,7 +114,7 @@ class Downloader(object):\n \n     def _enqueue_request(self, request, spider, slot):\n         def _downloaded(response):\n-            send_catch_log(signal=signals.response_downloaded, \\\n+            self.signals.send_catch_log(signal=signals.response_downloaded, \\\n                     response=response, request=request, spider=spider)\n             return response\n \n\n@@ -11,13 +11,11 @@ from twisted.internet import defer\n from twisted.python.failure import Failure\n \n from scrapy import log, signals\n-from scrapy.stats import stats\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n-from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.reactor import CallLaterOnce\n \n \n@@ -53,7 +51,9 @@ class Slot(object):\n class ExecutionEngine(object):\n \n     def __init__(self, crawler, spider_closed_callback):\n+        self.crawler = crawler\n         self.settings = crawler.settings\n+        self.signals = crawler.signals\n         self.slots = {}\n         self.running = False\n         self.paused = False\n@@ -71,7 +71,7 @@ class ExecutionEngine(object):\n         \"\"\"Start the execution engine\"\"\"\n         assert not self.running, \"Engine already running\"\n         self.start_time = time()\n-        yield send_catch_log_deferred(signal=signals.engine_started)\n+        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n \n     def stop(self):\n@@ -195,7 +195,7 @@ class ExecutionEngine(object):\n                 response.request = request # tie request to response received\n                 log.msg(log.formatter.crawled(request, response, spider), \\\n                     level=log.DEBUG, spider=spider)\n-                send_catch_log(signal=signals.response_received, \\\n+                self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n             return response\n \n@@ -218,13 +218,13 @@ class ExecutionEngine(object):\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n         nextcall = CallLaterOnce(self._next_request, spider)\n-        scheduler = self.scheduler_cls.from_settings(self.settings)\n+        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n         slot = Slot(start_requests or (), close_if_idle, nextcall, scheduler)\n         self.slots[spider] = slot\n         yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n-        stats.open_spider(spider)\n-        yield send_catch_log_deferred(signals.spider_opened, spider=spider)\n+        self.crawler.stats.open_spider(spider)\n+        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n         slot.nextcall.schedule()\n \n     def _spider_idle(self, spider):\n@@ -235,7 +235,7 @@ class ExecutionEngine(object):\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        res = send_catch_log(signal=signals.spider_idle, \\\n+        res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                 for _, x in res):\n@@ -261,11 +261,11 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: slot.scheduler.close(reason))\n         dfd.addErrback(log.err, spider=spider)\n \n-        dfd.addBoth(lambda _: send_catch_log_deferred(signal=signals.spider_closed, \\\n+        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, \\\n             spider=spider, reason=reason))\n         dfd.addErrback(log.err, spider=spider)\n \n-        dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n+        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log.err, spider=spider)\n \n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n@@ -284,5 +284,5 @@ class ExecutionEngine(object):\n \n     @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n-        yield send_catch_log_deferred(signal=signals.engine_stopped)\n-        yield stats.engine_stopped()\n+        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n+        yield self.crawler.stats.engine_stopped()\n\n@@ -6,26 +6,27 @@ from scrapy.utils.pqueue import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n-from scrapy.stats import stats\n from scrapy import log\n \n class Scheduler(object):\n \n-    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False):\n+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None):\n         self.df = dupefilter\n         self.dqdir = self._dqdir(jobdir)\n         self.dqclass = dqclass\n         self.mqclass = mqclass\n         self.logunser = logunser\n+        self.stats = stats\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_crawler(cls, crawler):\n+        settings = crawler.settings\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n         dupefilter = dupefilter_cls.from_settings(settings)\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n         logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')\n-        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser)\n+        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser, crawler.stats)\n \n     def has_pending_requests(self):\n         return len(self) > 0\n@@ -67,11 +68,13 @@ class Scheduler(object):\n                     (request, str(e)), level=log.ERROR, spider=self.spider)\n             return\n         else:\n-            stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n+            if self.stats:\n+                self.stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n             return True\n \n     def _mqpush(self, request):\n-        stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n+        if self.stats:\n+            self.stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n         self.mqs.push(request, -request.priority)\n \n     def _dqpop(self):\n\n@@ -9,14 +9,12 @@ from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n-from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.exceptions import CloseSpider, DropItem\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy import log\n-from scrapy.stats import stats\n \n \n class Slot(object):\n@@ -68,6 +66,7 @@ class Scraper(object):\n         self.itemproc = itemproc_cls.from_crawler(crawler)\n         self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n         self.crawler = crawler\n+        self.signals = crawler.signals\n \n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n@@ -146,9 +145,9 @@ class Scraper(object):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n         log.err(_failure, \"Spider error processing %s\" % request, spider=spider)\n-        send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n+        self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n             spider=spider)\n-        stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n+        self.crawler.stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n             spider=spider)\n \n     def handle_spider_output(self, result, request, response, spider):\n@@ -164,7 +163,7 @@ class Scraper(object):\n         from the given spider\n         \"\"\"\n         if isinstance(output, Request):\n-            send_catch_log(signal=signals.request_received, request=output, \\\n+            self.signals.send_catch_log(signal=signals.request_received, request=output, \\\n                 spider=spider)\n             self.crawler.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n@@ -199,13 +198,13 @@ class Scraper(object):\n             if isinstance(ex, DropItem):\n                 log.msg(log.formatter.dropped(item, ex, response, spider), \\\n                     level=log.WARNING, spider=spider)\n-                return send_catch_log_deferred(signal=signals.item_dropped, \\\n+                return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             log.msg(log.formatter.scraped(output, response, spider), \\\n                 log.DEBUG, spider=spider)\n-            return send_catch_log_deferred(signal=signals.item_scraped, \\\n+            return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, response=response, spider=spider)\n \n\n@@ -2,10 +2,10 @@ import signal\n \n from twisted.internet import reactor, defer\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n from scrapy.extension import ExtensionManager\n+from scrapy.signalmanager import SignalManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy import log, signals\n@@ -16,6 +16,8 @@ class Crawler(object):\n     def __init__(self, settings):\n         self.configured = False\n         self.settings = settings\n+        self.signals = SignalManager(self)\n+        self.stats = load_object(settings['STATS_CLASS'])(self)\n \n     def install(self):\n         import scrapy.project\n@@ -65,7 +67,7 @@ class CrawlerProcess(Crawler):\n \n     def __init__(self, *a, **kw):\n         super(CrawlerProcess, self).__init__(*a, **kw)\n-        dispatcher.connect(self.stop, signals.engine_stopped)\n+        self.signals.connect(self.stop, signals.engine_stopped)\n         install_shutdown_handlers(self._signal_shutdown)\n \n     def start(self):\n\n@@ -17,7 +17,6 @@ from twisted.mail.smtp import ESMTPSenderFactory\n from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n-from scrapy.utils.signal import send_catch_log\n \n \n # signal sent when message is sent\n@@ -28,13 +27,14 @@ mail_sent = object()\n class MailSender(object):\n \n     def __init__(self, smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, \\\n-            smtpport=None, debug=False):\n+            smtpport=None, debug=False, crawler=None):\n         self.smtphost = smtphost or settings['MAIL_HOST']\n         self.smtpport = smtpport or settings.getint('MAIL_PORT')\n         self.smtpuser = smtpuser or settings['MAIL_USER']\n         self.smtppass = smtppass or settings['MAIL_PASS']\n         self.mailfrom = mailfrom or settings['MAIL_FROM']\n         self.debug = debug\n+        self.signals = crawler.signals if crawler else None\n \n         if not self.smtphost or not self.mailfrom:\n             raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n@@ -65,7 +65,8 @@ class MailSender(object):\n         else:\n             msg.set_payload(body)\n \n-        send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,\n+        if self.signals:\n+            self.signals.send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,\n                        cc=cc, attach=attachs, msg=msg)\n \n         if self.debug:\n\n@@ -0,0 +1,27 @@\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.utils import signal\n+\n+class SignalManager(object):\n+\n+    def __init__(self, sender=dispatcher.Anonymous):\n+        self.sender = sender\n+\n+    def connect(self, *a, **kw):\n+        kw.setdefault('sender', self.sender)\n+        return dispatcher.connect(*a, **kw)\n+\n+    def disconnect(self, *a, **kw):\n+        kw.setdefault('sender', self.sender)\n+        return dispatcher.disconnect(*a, **kw)\n+\n+    def send_catch_log(self, *a, **kw):\n+        kw.setdefault('sender', self.sender)\n+        return signal.send_catch_log(*a, **kw)\n+\n+    def send_catch_log_deferred(self, *a, **kw):\n+        kw.setdefault('sender', self.sender)\n+        return signal.send_catch_log_deferred(*a, **kw)\n+\n+    def disconnect_all(self, *a, **kw):\n+        kw.setdefault('sender', self.sender)\n+        return signal.disconnect_all(*a, **kw)\n\n@@ -9,7 +9,6 @@ from scrapy import signals\n from scrapy.interfaces import ISpiderManager\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n-from scrapy.xlib.pydispatch import dispatcher\n \n \n class SpiderManager(object):\n@@ -22,7 +21,6 @@ class SpiderManager(object):\n         for name in self.spider_modules:\n             for module in walk_modules(name):\n                 self._load_spiders(module)\n-        dispatcher.connect(self.close_spider, signals.spider_closed)\n \n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n@@ -34,7 +32,9 @@ class SpiderManager(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls.from_settings(crawler.settings)\n+        sm = cls.from_settings(crawler.settings)\n+        crawler.signals.connect(sm.close_spider, signals.spider_closed)\n+        return sm\n \n     def create(self, spider_name, **spider_kwargs):\n         try:\n\n@@ -1,9 +1,7 @@\n-from scrapy.statscol import DummyStatsCollector\n-from scrapy.conf import settings\n-from scrapy.utils.misc import load_object\n+from scrapy.project import crawler\n+stats = crawler.stats\n \n-# if stats are disabled use a DummyStatsCollector to improve performance\n-if settings.getbool('STATS_ENABLED'):\n-    stats = load_object(settings['STATS_CLASS'])()\n-else:\n-    stats = DummyStatsCollector()\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.stats` is deprecated, use `crawler.stats` attribute instead\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -3,20 +3,16 @@ Scrapy extension for collecting scraping stats\n \"\"\"\n import pprint\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n     stats_spider_closed\n-from scrapy.utils.signal import send_catch_log\n-from scrapy import signals\n from scrapy import log\n-from scrapy.conf import settings\n \n class StatsCollector(object):\n \n-    def __init__(self):\n-        self._dump = settings.getbool('STATS_DUMP')\n+    def __init__(self, crawler):\n+        self._dump = crawler.settings.getbool('STATS_DUMP')\n         self._stats = {None: {}} # None is for global stats\n+        self._signals = crawler.signals\n \n     def get_value(self, key, default=None, spider=None):\n         return self._stats[spider].get(key, default)\n@@ -50,12 +46,12 @@ class StatsCollector(object):\n \n     def open_spider(self, spider):\n         self._stats[spider] = {}\n-        send_catch_log(stats_spider_opened, spider=spider)\n+        self._signals.send_catch_log(stats_spider_opened, spider=spider)\n \n     def close_spider(self, spider, reason):\n-        send_catch_log(stats_spider_closing, spider=spider, reason=reason)\n+        self._signals.send_catch_log(stats_spider_closing, spider=spider, reason=reason)\n         stats = self._stats.pop(spider)\n-        send_catch_log(stats_spider_closed, spider=spider, reason=reason, \\\n+        self._signals.send_catch_log(stats_spider_closed, spider=spider, reason=reason, \\\n             spider_stats=stats)\n         if self._dump:\n             log.msg(\"Dumping spider stats:\\n\" + pprint.pformat(stats), \\\n@@ -73,8 +69,8 @@ class StatsCollector(object):\n \n class MemoryStatsCollector(StatsCollector):\n \n-    def __init__(self):\n-        super(MemoryStatsCollector, self).__init__()\n+    def __init__(self, crawler):\n+        super(MemoryStatsCollector, self).__init__(crawler)\n         self.spider_stats = {}\n \n     def _persist_stats(self, stats, spider=None):\n\n@@ -10,11 +10,8 @@ from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n from twisted.internet import protocol\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n-from scrapy.stats import stats\n from scrapy import log, signals\n-from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n@@ -39,8 +36,8 @@ class TelnetConsole(protocol.ServerFactory):\n         self.noisy = False\n         self.portrange = map(int, crawler.settings.getlist('TELNETCONSOLE_PORT'))\n         self.host = crawler.settings['TELNETCONSOLE_HOST']\n-        dispatcher.connect(self.start_listening, signals.engine_started)\n-        dispatcher.connect(self.stop_listening, signals.engine_stopped)\n+        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n+        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -70,7 +67,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'slot': slot,\n             'manager': self.crawler,\n             'extensions': self.crawler.extensions,\n-            'stats': stats,\n+            'stats': self.crawler.stats,\n             'spiders': self.crawler.spiders,\n             'settings': self.crawler.settings,\n             'est': lambda: print_engine_status(self.crawler.engine),\n@@ -80,5 +77,5 @@ class TelnetConsole(protocol.ServerFactory):\n             'help': \"This is Scrapy telnet console. For more info see: \" \\\n                 \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n         }\n-        send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n+        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n         return telnet_vars\n\n@@ -3,9 +3,9 @@ import unittest, tempfile, shutil, time\n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spider import BaseSpider\n from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware\n-from scrapy.stats import stats\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n+from scrapy.utils.test import get_crawler\n \n \n class HttpCacheMiddlewareTest(unittest.TestCase):\n@@ -13,14 +13,15 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     storage_class = FilesystemCacheStorage\n \n     def setUp(self):\n+        self.crawler = get_crawler()\n         self.spider = BaseSpider('example.com')\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com', headers={'User-Agent': 'test'})\n         self.response = Response('http://www.example.com', headers={'Content-Type': 'text/html'}, body='test body', status=202)\n-        stats.open_spider(self.spider)\n+        self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n-        stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, '')\n         shutil.rmtree(self.tmpdir)\n \n     def _get_settings(self, **new_settings):\n@@ -37,7 +38,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         return self.storage_class(self._get_settings(**new_settings))\n \n     def _get_middleware(self, **new_settings):\n-        return HttpCacheMiddleware(self._get_settings(**new_settings))\n+        return HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)\n \n     def test_storage(self):\n         storage = self._get_storage()\n@@ -58,7 +59,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         assert storage.retrieve_response(self.spider, self.request)\n \n     def test_middleware(self):\n-        mw = HttpCacheMiddleware(self._get_settings())\n+        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)\n         assert mw.process_request(self.request, self.spider) is None\n         mw.process_response(self.request, self.response, self.spider)\n         response = mw.process_request(self.request, self.spider)\n@@ -67,7 +68,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         assert 'cached' in response.flags\n \n     def test_different_request_response_urls(self):\n-        mw = HttpCacheMiddleware(self._get_settings())\n+        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)\n         req = Request('http://host.com/path')\n         res = Response('http://host2.net/test.html')\n         assert mw.process_request(req, self.spider) is None\n\n@@ -3,35 +3,36 @@ from unittest import TestCase\n from scrapy.contrib.downloadermiddleware.stats import DownloaderStats\n from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n-from scrapy.stats import stats\n+from scrapy.utils.test import get_crawler\n \n \n class TestDownloaderStats(TestCase):\n \n     def setUp(self):\n+        self.crawler = get_crawler()\n         self.spider = BaseSpider('scrapytest.org')\n-        self.mw = DownloaderStats()\n+        self.mw = DownloaderStats(self.crawler.stats)\n \n-        stats.open_spider(self.spider)\n+        self.crawler.stats.open_spider(self.spider)\n \n         self.req = Request('http://scrapytest.org')\n         self.res = Response('scrapytest.org', status=400)\n \n     def test_process_request(self):\n         self.mw.process_request(self.req, self.spider)\n-        self.assertEqual(stats.get_value('downloader/request_count', \\\n+        self.assertEqual(self.crawler.stats.get_value('downloader/request_count', \\\n             spider=self.spider), 1)\n         \n     def test_process_response(self):\n         self.mw.process_response(self.req, self.res, self.spider)\n-        self.assertEqual(stats.get_value('downloader/response_count', \\\n+        self.assertEqual(self.crawler.stats.get_value('downloader/response_count', \\\n             spider=self.spider), 1)\n \n     def test_process_exception(self):\n         self.mw.process_exception(self.req, Exception(), self.spider)\n-        self.assertEqual(stats.get_value('downloader/exception_count', \\\n+        self.assertEqual(self.crawler.stats.get_value('downloader/exception_count', \\\n             spider=self.spider), 1)\n \n     def tearDown(self):\n-        stats.close_spider(self.spider, '')\n+        self.crawler.stats.close_spider(self.spider, '')\n \n\n@@ -1,22 +1,22 @@\n from cStringIO import StringIO\n import unittest\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n from scrapy.mail import MailSender, mail_sent\n+from scrapy.utils.test import get_crawler\n \n \n class MailSenderTest(unittest.TestCase):\n \n     def setUp(self):\n         self.catched_msg = None\n-        dispatcher.connect(self._catch_mail_sent, signal=mail_sent)\n+        self.crawler = get_crawler()\n+        self.crawler.signals.connect(self._catch_mail_sent, signal=mail_sent)\n \n     def tearDown(self):\n-        dispatcher.disconnect(self._catch_mail_sent, signal=mail_sent)\n+        self.crawler.signals.disconnect(self._catch_mail_sent, signal=mail_sent)\n \n     def test_send(self):\n-        mailsender = MailSender(debug=True)\n+        mailsender = MailSender(debug=True, crawler=self.crawler)\n         mailsender.send(to=['test@scrapy.org'], subject='subject', body='body')\n \n         assert self.catched_msg\n@@ -36,7 +36,7 @@ class MailSenderTest(unittest.TestCase):\n         attach.seek(0)\n         attachs = [('attachment', 'text/plain', attach)]\n \n-        mailsender = MailSender(debug=True)\n+        mailsender = MailSender(debug=True, crawler=self.crawler)\n         mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n                        attachs=attachs)\n \n\n@@ -4,6 +4,7 @@ from scrapy.contrib.spidermiddleware.depth import DepthMiddleware\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n from scrapy.statscol import StatsCollector\n+from scrapy.utils.test import get_crawler\n \n \n class TestDepthMiddleware(TestCase):\n@@ -11,7 +12,7 @@ class TestDepthMiddleware(TestCase):\n     def setUp(self):\n         self.spider = BaseSpider('scrapytest.org')\n \n-        self.stats = StatsCollector()\n+        self.stats = StatsCollector(get_crawler())\n         self.stats.open_spider(self.spider)\n \n         self.mw = DepthMiddleware(1, self.stats, True)\n\n@@ -1,18 +1,19 @@\n import unittest \n \n from scrapy.spider import BaseSpider\n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.statscol import StatsCollector, DummyStatsCollector\n from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n     stats_spider_closed\n+from scrapy.utils.test import get_crawler\n \n class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n+        self.crawler = get_crawler()\n         self.spider = BaseSpider('foo')\n \n     def test_collector(self):\n-        stats = StatsCollector()\n+        stats = StatsCollector(self.crawler)\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_value('anything'), None)\n         self.assertEqual(stats.get_value('anything', 'default'), 'default')\n@@ -39,7 +40,7 @@ class StatsCollectorTest(unittest.TestCase):\n         self.assertEqual(stats.get_value('test4'), 7)\n \n     def test_dummy_collector(self):\n-        stats = DummyStatsCollector()\n+        stats = DummyStatsCollector(self.crawler)\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_value('anything'), None)\n         self.assertEqual(stats.get_value('anything', 'default'), 'default')\n@@ -70,11 +71,11 @@ class StatsCollectorTest(unittest.TestCase):\n             assert spider_stats == {'test': 1}\n             signals_catched.add(stats_spider_closed)\n \n-        dispatcher.connect(spider_opened, signal=stats_spider_opened)\n-        dispatcher.connect(spider_closing, signal=stats_spider_closing)\n-        dispatcher.connect(spider_closed, signal=stats_spider_closed)\n+        self.crawler.signals.connect(spider_opened, signal=stats_spider_opened)\n+        self.crawler.signals.connect(spider_closing, signal=stats_spider_closing)\n+        self.crawler.signals.connect(spider_closed, signal=stats_spider_closed)\n \n-        stats = StatsCollector()\n+        stats = StatsCollector(self.crawler)\n         stats.open_spider(self.spider)\n         stats.set_value('test', 1, spider=self.spider)\n         self.assertEqual([(self.spider, {'test': 1})], list(stats.iter_spider_stats()))\n@@ -83,9 +84,9 @@ class StatsCollectorTest(unittest.TestCase):\n         assert stats_spider_closing in signals_catched\n         assert stats_spider_closed in signals_catched\n \n-        dispatcher.disconnect(spider_opened, signal=stats_spider_opened)\n-        dispatcher.disconnect(spider_closing, signal=stats_spider_closing)\n-        dispatcher.disconnect(spider_closed, signal=stats_spider_closed)\n+        self.crawler.signals.disconnect(spider_opened, signal=stats_spider_opened)\n+        self.crawler.signals.disconnect(spider_closing, signal=stats_spider_closing)\n+        self.crawler.signals.disconnect(spider_closed, signal=stats_spider_closed)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -6,7 +6,6 @@ See docs/topics/webservice.rst\n \n from twisted.web import server, error\n \n-from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy import log, signals\n from scrapy.utils.jsonrpc import jsonrpc_server_call\n@@ -80,8 +79,8 @@ class WebService(server.Site):\n             root.putChild(res.ws_name, res)\n         server.Site.__init__(self, root, logPath=logfile)\n         self.noisy = False\n-        dispatcher.connect(self.start_listening, signals.engine_started)\n-        dispatcher.connect(self.stop_listening, signals.engine_stopped)\n+        crawler.signals.connect(self.start_listening, signals.engine_started)\n+        crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
