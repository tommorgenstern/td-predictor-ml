{"custom_id": "scrapy#b06a670777058f19bb249a1d192a2cea27ea0475", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1379 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -64,11 +64,11 @@ class Crawler(object):\n         self.crawling = True\n \n         try:\n+            self.spider = self._create_spider(*args, **kwargs)\n             self.spidercls.update_settings(self.settings)\n             self.settings.freeze()\n             self.extensions = ExtensionManager.from_crawler(self)\n \n-            self.spider = self._create_spider(*args, **kwargs)\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n             yield self.engine.open_spider(self.spider, start_requests)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86c74ce53e1fca2174bf38ed75399ba298df16a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1381 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class Crawler(object):\n \n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n-            self.spidercls.update_settings(self.settings)\n+            self.spider.update_settings(self.settings)\n             self.settings.freeze()\n             self.extensions = ExtensionManager.from_crawler(self)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd45b31fe42635ff43df62c667f1ace5f9169737", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 528 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,7 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n-from urlparse import urlsplit, urlunsplit\n+import re\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                     unquote)\n@@ -115,16 +115,10 @@ def escape_ajax(url):\n \n def add_http_if_no_scheme(url):\n     \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n-    parts = urlsplit(url)\n-    scheme = parts.scheme or \"http\"\n-    if parts.netloc:\n-        netloc = parts.netloc\n-        path = parts.path\n-    else:\n-        path_parts = url.split(\"/\", 1)\n-        netloc = path_parts[0]\n-        path = path_parts[1] if len(path_parts) > 1 else \"/\"\n+    match = re.match(r\"^\\w+://\", url, flags=re.I)\n+    parts = urlparse(url)\n+    if not match:\n+        scheme = \"http:\" if parts.netloc else \"http://\"\n+        url = scheme + url\n \n-    return urlunsplit((\n-        scheme, netloc, path, parts.query, parts.fragment\n-    ))\n+    return url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97b51ea33b6b96241ef87861619e6820ee9e765c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 180 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -4,8 +4,9 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n+import re\n+from six.moves.urllib.parse import urlparse, urlunparse\n from threading import Thread\n-import urlparse\n from w3lib.url import any_to_uri\n \n from scrapy.commands import ScrapyCommand\n@@ -45,7 +46,7 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         url = args[0] if args else None\n         if url:\n-            parts = urlparse.urlsplit(url)\n+            parts = urlparse(url)\n             if not parts.scheme:\n                 if \".\" not in parts.path.split(\"/\", 1)[0]:\n                     url = any_to_uri(url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c68c95cadbf45e11657b00058ff29921c27bffa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 1413 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,17 +31,6 @@ class Crawler(object):\n         self.settings = settings.copy()\n \n         self.signals = SignalManager(self)\n-        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n-\n-        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n-        logging.root.addHandler(handler)\n-        # lambda is assigned to Crawler attribute because this way it is not\n-        # garbage collected after leaving __init__ scope\n-        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n-        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n-\n-        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n-        self.logformatter = lf_cls.from_crawler(self)\n \n         self.crawling = False\n         self.spider = None\n@@ -74,6 +63,20 @@ class Crawler(object):\n             self.spider = self._create_spider(*args, **kwargs)\n             self.spider.update_settings(self.settings)\n             self.settings.freeze()\n+\n+            self.stats = load_object(self.settings['STATS_CLASS'])(self)\n+\n+            handler = LogCounterHandler(self,\n+                                        level=self.settings.get('LOG_LEVEL'))\n+            logging.root.addHandler(handler)\n+            # lambda is assigned to Crawler attribute because this way it is not\n+            # garbage collected after leaving __init__ scope\n+            self.__remove_handler = lambda: logging.root.removeHandler(handler)\n+            self.signals.connect(self.__remove_handler, signals.engine_stopped)\n+\n+            lf_cls = load_object(self.settings['LOG_FORMATTER'])\n+            self.logformatter = lf_cls.from_crawler(self)\n+\n             self.extensions = ExtensionManager.from_crawler(self)\n \n             self.engine = self._create_engine()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aafb31d6fb86cb05ae1945685adbb615566068c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 1438 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,6 +31,17 @@ class Crawler(object):\n         self.settings = settings.copy()\n \n         self.signals = SignalManager(self)\n+        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n+\n+        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n+        logging.root.addHandler(handler)\n+        # lambda is assigned to Crawler attribute because this way it is not\n+        # garbage collected after leaving __init__ scope\n+        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n+        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n+\n+        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n+        self.logformatter = lf_cls.from_crawler(self)\n \n         self.crawling = False\n         self.spider = None\n@@ -63,20 +74,6 @@ class Crawler(object):\n             self.spider = self._create_spider(*args, **kwargs)\n             self.spider.update_settings(self.settings)\n             self.settings.freeze()\n-\n-            self.stats = load_object(self.settings['STATS_CLASS'])(self)\n-\n-            handler = LogCounterHandler(self,\n-                                        level=self.settings.get('LOG_LEVEL'))\n-            logging.root.addHandler(handler)\n-            # lambda is assigned to Crawler attribute because this way it is not\n-            # garbage collected after leaving __init__ scope\n-            self.__remove_handler = lambda: logging.root.removeHandler(handler)\n-            self.signals.connect(self.__remove_handler, signals.engine_stopped)\n-\n-            lf_cls = load_object(self.settings['LOG_FORMATTER'])\n-            self.logformatter = lf_cls.from_crawler(self)\n-\n             self.extensions = ExtensionManager.from_crawler(self)\n \n             self.engine = self._create_engine()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc2639731953fb35fd52c374215ae40f5db80665", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 1445 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,13 +64,6 @@ class Crawler(object):\n         self.crawling = True\n \n         try:\n-            # Support multiple calls to crawl()\n-            if self.settings.frozen:\n-                # Dirty hack, this should probably be more like\n-                # self.settings = self.settings.mutable_copy()\n-                # or maybe\n-                # self.settings.unfreeze()\n-                self.settings.frozen = False\n             self.spider = self._create_spider(*args, **kwargs)\n             self.spider.update_settings(self.settings)\n             self.settings.freeze()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#380f76d35fa0350a37f17fbb2e2065f1e527cadd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 479 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,14 +61,16 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n-        crawler = CrawlerRunner({\"DOWNLOAD_TIMEOUT\": 0.35}).create_crawler(DelaySpider)\n-        yield crawler.crawl(n=0.5)\n+        runner = CrawlerRunner({\"DOWNLOAD_TIMEOUT\": 0.35})\n+        crawler = runner.create_crawler(DelaySpider)\n+        yield runner.crawl(crawler, n=0.5)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n         self.assertTrue(crawler.spider.t2_err > 0)\n         self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n         # server hangs after receiving response headers\n-        yield crawler.crawl(n=0.5, b=1)\n+        crawler = runner.create_crawler(DelaySpider)\n+        yield runner.crawl(crawler, n=0.5, b=1)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n         self.assertTrue(crawler.spider.t2_err > 0)\n@@ -131,11 +133,15 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)\n-        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3)\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(DuplicateStartRequestsSpider)\n+        yield runner.crawl(crawler, dont_filter=True, distinct_urls=2,\n+                           dupe_factor=3)\n         self.assertEqual(crawler.spider.visited, 6)\n \n-        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4)\n+        crawler = runner.create_crawler(DuplicateStartRequestsSpider)\n+        yield runner.crawl(crawler, dont_filter=False, distinct_urls=3,\n+                           dupe_factor=4)\n         self.assertEqual(crawler.spider.visited, 3)\n \n     @defer.inlineCallbacks\n\n@@ -308,6 +308,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n             # download_maxsize < 100, hence the CancelledError\n             self.assertIsInstance(failure.value, defer.CancelledError)\n \n+            crawler = get_crawler(SingleRequestSpider)\n             request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n             request = request.replace(url='http://localhost:8998/xpayload')\n             yield crawler.crawl(seed=request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daec0457110e188a131271f9fce86ecb1dbd14c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 64 | Churn Cumulative: 575 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,10 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n-from scrapy.crawler import CrawlerRunner\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.extensions.throttle import AutoThrottle\n+from scrapy.settings import Settings\n+from scrapy.utils.spider import DefaultSpider\n from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n@@ -272,3 +275,30 @@ with multiples lines\n \n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_populate_spider_settings(self):\n+        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider',\n+                           'AUTOTHROTTLE_ENABLED': True}\n+        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n+\n+        class CustomSettingsSpider(DefaultSpider):\n+            custom_settings = spider_settings\n+\n+            def parse(self, response):\n+                return\n+\n+        settings = Settings()\n+        settings.setdict(project_settings, priority='project')\n+        crawler = Crawler(CustomSettingsSpider, settings)\n+        yield crawler.crawl()\n+\n+        self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n+        self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n+        self.assertEqual(crawler.settings.get('TEST3'), 'project')\n+\n+        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n+        self.assertIn(AutoThrottle, enabled_exts)\n+\n+        self.assertFalse(settings.frozen)\n+        self.assertTrue(crawler.settings.frozen)\n\n@@ -7,7 +7,6 @@ from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n-from scrapy.extensions.throttle import AutoThrottle\n \n \n class BaseCrawlerTest(unittest.TestCase):\n@@ -33,24 +32,6 @@ class CrawlerTestCase(BaseCrawlerTest):\n             self.crawler.spiders\n             self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n \n-    def test_populate_spidercls_settings(self):\n-        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n-        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n-\n-        class CustomSettingsSpider(DefaultSpider):\n-            custom_settings = spider_settings\n-\n-        settings = Settings()\n-        settings.setdict(project_settings, priority='project')\n-        crawler = Crawler(CustomSettingsSpider, settings)\n-\n-        self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n-        self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n-        self.assertEqual(crawler.settings.get('TEST3'), 'project')\n-\n-        self.assertFalse(settings.frozen)\n-        self.assertTrue(crawler.settings.frozen)\n-\n     def test_crawler_accepts_dict(self):\n         crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n         self.assertEqual(crawler.settings['foo'], 'bar')\n@@ -61,19 +42,6 @@ class CrawlerTestCase(BaseCrawlerTest):\n         self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n \n \n-class SpiderSettingsTestCase(unittest.TestCase):\n-    def test_spider_custom_settings(self):\n-        class MySpider(scrapy.Spider):\n-            name = 'spider'\n-            custom_settings = {\n-                'AUTOTHROTTLE_ENABLED': True\n-            }\n-\n-        crawler = Crawler(MySpider, {})\n-        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n-        self.assertIn(AutoThrottle, enabled_exts)\n-\n-\n class SpiderLoaderWithWrongInterface(object):\n \n     def unneeded_method(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2629997a2f620c54ab5b052947a0132a52042984", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 172 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,9 +75,8 @@ class Spider(object_ref):\n     def parse(self, response):\n         raise NotImplementedError\n \n-    @classmethod\n-    def update_settings(cls, settings):\n-        settings.setdict(cls.custom_settings or {}, priority='spider')\n+    def update_settings(self, settings):\n+        settings.setdict(self.custom_settings or {}, priority='spider')\n \n     @classmethod\n     def handles_request(cls, request):\n\n@@ -98,11 +98,17 @@ class SpiderTest(unittest.TestCase):\n         self.spider_class.custom_settings = spider_settings\n         settings = Settings(project_settings, priority='project')\n \n-        self.spider_class.update_settings(settings)\n+        spider = self.spider_class('example.com')\n+        spider.update_settings(settings)\n         self.assertEqual(settings.get('TEST1'), 'spider')\n         self.assertEqual(settings.get('TEST2'), 'spider')\n         self.assertEqual(settings.get('TEST3'), 'project')\n \n+        spider_instance_settings = {'TEST1': 'spider_instance'}\n+        spider.custom_settings = spider_instance_settings\n+        spider.update_settings(settings)\n+        self.assertEqual(settings.get('TEST1'), 'spider_instance')\n+\n     def test_logger(self):\n         spider = self.spider_class('example.com')\n         with LogCapture() as l:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e66f64989409ddf9117f854934d52b2772aace74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 426 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,8 @@ DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n \n-DOWNLOAD_HANDLERS = {\n+DOWNLOAD_HANDLERS = {}\n+DOWNLOAD_HANDLERS_BASE = {\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n     'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n@@ -81,7 +82,9 @@ DOWNLOADER = 'scrapy.core.downloader.Downloader'\n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n \n-DOWNLOADER_MIDDLEWARES = {\n+DOWNLOADER_MIDDLEWARES = {}\n+\n+DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n     'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n@@ -113,7 +116,9 @@ except KeyError:\n     else:\n         EDITOR = 'vi'\n \n-EXTENSIONS = {\n+EXTENSIONS = {}\n+\n+EXTENSIONS_BASE = {\n     'scrapy.extensions.corestats.CoreStats': 0,\n     'scrapy.extensions.telnet.TelnetConsole': 0,\n     'scrapy.extensions.memusage.MemoryUsage': 0,\n@@ -130,14 +135,16 @@ FEED_URI_PARAMS = None  # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n FEED_EXPORT_FIELDS = None\n-FEED_STORAGES = {\n+FEED_STORAGES = {}\n+FEED_STORAGES_BASE = {\n     '': 'scrapy.extensions.feedexport.FileFeedStorage',\n     'file': 'scrapy.extensions.feedexport.FileFeedStorage',\n     'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage',\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_EXPORTERS = {\n+FEED_EXPORTERS = {}\n+FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n     'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n     'jl': 'scrapy.exporters.JsonLinesItemExporter',\n@@ -163,6 +170,7 @@ HTTPCACHE_GZIP = False\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n+ITEM_PIPELINES_BASE = {}\n \n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n@@ -221,7 +229,9 @@ SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n \n SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n \n-SPIDER_MIDDLEWARES = {\n+SPIDER_MIDDLEWARES = {}\n+\n+SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,\n     'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,\n@@ -248,7 +258,8 @@ TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n TELNETCONSOLE_HOST = '127.0.0.1'\n \n-SPIDER_CONTRACTS = {\n+SPIDER_CONTRACTS = {}\n+SPIDER_CONTRACTS_BASE = {\n     'scrapy.contracts.default.UrlContract': 1,\n     'scrapy.contracts.default.ReturnsContract': 2,\n     'scrapy.contracts.default.ScrapesContract': 3,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6a023ce987a064b222b1aa2de03a50991f387fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 51 | Churn Cumulative: 321 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n \n \n-def build_component_list(compdict, convert=update_classpath):\n+def build_component_list(compdict, custom=None, convert=update_classpath):\n     \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n \n     def _check_components(complist):\n@@ -34,9 +34,15 @@ def build_component_list(compdict, convert=update_classpath):\n             _check_components(compdict)\n             return {convert(k): v for k, v in six.iteritems(compdict)}\n \n-    if isinstance(compdict, (list, tuple)):\n-        _check_components(compdict)\n-        return type(compdict)(convert(c) for c in compdict)\n+    # BEGIN Backwards compatibility for old (base, custom) call signature\n+    if isinstance(custom, (list, tuple)):\n+        _check_components(custom)\n+        return type(custom)(convert(c) for c in custom)\n+\n+    if custom is not None:\n+        compdict.update(custom)\n+    # END Backwards compatibility\n+\n     compdict = without_none_values(_map_keys(compdict))\n     return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n \n\n@@ -8,46 +8,59 @@ class BuildComponentListTest(unittest.TestCase):\n \n     def test_build_dict(self):\n         d = {'one': 1, 'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(d, lambda x: x),\n+        self.assertEqual(build_component_list(d, convert=lambda x: x),\n                          ['one', 'four', 'three'])\n \n+    def test_backwards_compatible_build_dict(self):\n+        base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n+        custom = {'two': None, 'three': 8, 'four': 4}\n+        self.assertEqual(build_component_list(base, custom,\n+                                              convert=lambda x: x),\n+                         ['one', 'four', 'five', 'three'])\n+\n     def test_return_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(custom, lambda x: x), custom)\n+        self.assertEqual(build_component_list(None, custom,\n+                                              convert=lambda x: x),\n+                         custom)\n \n     def test_map_dict(self):\n         custom = {'one': 1, 'two': 2, 'three': 3}\n-        self.assertEqual(build_component_list(custom, lambda x: x.upper()),\n+        self.assertEqual(build_component_list({}, custom,\n+                                              convert=lambda x: x.upper()),\n                          ['ONE', 'TWO', 'THREE'])\n \n     def test_map_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(custom, lambda x: x.upper()),\n+        self.assertEqual(build_component_list(None, custom,\n+                                              lambda x: x.upper()),\n                          ['A', 'B', 'C'])\n \n     def test_duplicate_components_in_dict(self):\n         duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}\n-        self.assertRaises(ValueError,\n-                          build_component_list, duplicate_dict, lambda x: x.lower())\n+        self.assertRaises(ValueError, build_component_list, {}, duplicate_dict,\n+                          convert=lambda x: x.lower())\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = ['a', 'b', 'a']\n-        self.assertRaises(ValueError,\n-                          build_component_list, duplicate_list, lambda x: x)\n+        self.assertRaises(ValueError, build_component_list, None,\n+                          duplicate_list, convert=lambda x: x)\n \n     def test_duplicate_components_in_basesettings(self):\n         # Higher priority takes precedence\n         duplicate_bs = BaseSettings({'one': 1, 'two': 2}, priority=0)\n         duplicate_bs.set('ONE', 4, priority=10)\n-        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+        self.assertEqual(build_component_list(duplicate_bs,\n+                                              convert=lambda x: x.lower()),\n                          ['two', 'one'])\n         duplicate_bs.set('one', duplicate_bs['one'], priority=20)\n-        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+        self.assertEqual(build_component_list(duplicate_bs,\n+                                              convert=lambda x: x.lower()),\n                          ['one', 'two'])\n         # Same priority raises ValueError\n         duplicate_bs.set('ONE', duplicate_bs['ONE'], priority=20)\n-        self.assertRaises(ValueError,\n-                          build_component_list, duplicate_bs, convert=lambda x: x.lower())\n+        self.assertRaises(ValueError, build_component_list, duplicate_bs,\n+                          convert=lambda x: x.lower())\n \n \n class UtilsConfTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#52ecee6a6271ef2d981f7f1993311abd8ed31bd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 45 | Files Changed: 11 | Hunks: 12 | Methods Changed: 13 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 76 | Churn Cumulative: 2275 | Contributors (this commit): 20 | Commits (past 90d): 28 | Contributors (cumulative): 59 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,7 +58,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         # load contracts\n-        contracts = build_component_list(self.settings._getcomposite('SPIDER_CONTRACTS'))\n+        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))\n         conman = ContractsManager(load_object(c) for c in contracts)\n         runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n         result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n@@ -35,7 +35,8 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            feed_exporters = without_none_values(\n+                self.settings.getwithbase('FEED_EXPORTERS'))\n             valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n\n@@ -58,7 +58,7 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            feed_exporters = without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))\n             valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n\n@@ -20,7 +20,8 @@ class DownloadHandlers(object):\n         self._schemes = {}  # stores acceptable schemes on instancing\n         self._handlers = {}  # stores instanced handlers for schemes\n         self._notconfigured = {}  # remembers failed handlers\n-        handlers = without_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))\n+        handlers = without_none_values(\n+            crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n         for scheme, clspath in six.iteritems(handlers):\n             self._schemes[scheme] = clspath\n \n\n@@ -19,7 +19,8 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings._getcomposite('DOWNLOADER_MIDDLEWARES'))\n+        return build_component_list(\n+            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))\n \n     def _add_middleware(self, mw):\n         if hasattr(mw, 'process_request'):\n\n@@ -18,7 +18,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings._getcomposite('SPIDER_MIDDLEWARES'))\n+        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n \n     def _add_middleware(self, mw):\n         super(SpiderMiddlewareManager, self)._add_middleware(mw)\n\n@@ -12,4 +12,4 @@ class ExtensionManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings._getcomposite('EXTENSIONS'))\n+        return build_component_list(settings.getwithbase('EXTENSIONS'))\n\n@@ -196,7 +196,7 @@ class FeedExporter(object):\n         return item\n \n     def _load_components(self, setting_prefix):\n-        conf = without_none_values(self.settings._getcomposite(setting_prefix))\n+        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n         d = {}\n         for k, v in conf.items():\n             try:\n\n@@ -13,7 +13,7 @@ class ItemPipelineManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings._getcomposite('ITEM_PIPELINES'))\n+        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))\n \n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n\n@@ -195,25 +195,17 @@ class BaseSettings(MutableMapping):\n             value = json.loads(value)\n         return dict(value)\n \n-    def _getcomposite(self, name):\n-        # DO NOT USE THIS FUNCTION IN YOUR CUSTOM PROJECTS\n-        # It's for internal use in the transition away from the _BASE settings\n-        # and will be removed along with _BASE support in a future release\n-        basename = name + \"_BASE\"\n-        if basename in self:\n-            warnings.warn('_BASE settings are deprecated.',\n-                          category=ScrapyDeprecationWarning)\n-            # When users defined a _BASE setting, they explicitly don't want to\n-            # use any of Scrapy's defaults. Therefore, we only use these entries\n-            # from self[name] (where the defaults now live) that have a priority\n-            # higher than 'default'\n-            compsett = BaseSettings(self[basename], priority='default')\n-            for k in self[name]:\n-                prio = self[name].getpriority(k)\n-                if prio > get_settings_priority('default'):\n-                    compsett.set(k, self[name][k], prio)\n-            return compsett\n-        return self[name]\n+    def getwithbase(self, name):\n+        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n+        counterpart.\n+\n+        :param name: name of the dictionary-like setting\n+        :type name: string\n+        \"\"\"\n+        compbs = BaseSettings()\n+        compbs.update(self[name + '_BASE'])\n+        compbs.update(self[name])\n+        return compbs\n \n     def getpriority(self, name):\n         \"\"\"\n\n@@ -263,24 +263,15 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertEqual(settings.getpriority('key'), 99)\n         self.assertEqual(settings.getpriority('nonexistentkey'), None)\n \n-    def test_getcomposite(self):\n-        s = BaseSettings({'TEST_BASE': {1: 1, 2: 2},\n+    def test_getwithbase(self):\n+        s = BaseSettings({'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),\n                           'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n-                          'HASNOBASE': BaseSettings({1: 1}, 'default')})\n-        s['TEST'].set(4, 4, priority='project')\n-        # When users specify a _BASE setting they explicitly don't want to use\n-        # Scrapy's defaults, so we don't want to see anything that has a\n-        # 'default' priority from TEST\n-        cs = s._getcomposite('TEST')\n-        self.assertEqual(len(cs), 3)\n-        self.assertEqual(cs[1], 1)\n-        self.assertEqual(cs[2], 2)\n-        self.assertEqual(cs[4], 4)\n-        cs = s._getcomposite('HASNOBASE')\n-        self.assertEqual(len(cs), 1)\n-        self.assertEqual(cs[1], 1)\n-        cs = s._getcomposite('NONEXISTENT')\n-        self.assertIsNone(cs)\n+                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n+        s['TEST'].set(2, 200, 'cmdline')\n+        six.assertCountEqual(self, s.getwithbase('TEST'),\n+                             {1: 1, 2: 200, 3: 30})\n+        six.assertCountEqual(self, s.getwithbase('HASNOBASE'), s['HASNOBASE'])\n+        self.assertEqual(s.getwithbase('NONEXISTENT'), {})\n \n     def test_maxpriority(self):\n         # Empty settings should return 'default'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44f6ada034f9d75e0089f45d3ad9f3c6fa5777f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 51 | Churn Cumulative: 1140 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,12 +49,9 @@ class SettingsAttribute(object):\n \n     def set(self, value, priority):\n         \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n-        if isinstance(self.value, BaseSettings):\n-            # Ignore self.priority if self.value has per-key priorities\n-            self.value.update(value, priority)\n-            self.priority = max(self.value.maxpriority(), priority)\n-        else:\n         if priority >= self.priority:\n+            if isinstance(self.value, BaseSettings):\n+                value = BaseSettings(value, priority=priority)\n             self.value = value\n             self.priority = priority\n \n\n@@ -1,10 +1,11 @@\n-import os\n import json\n-import sys\n-import shutil\n+import os\n import pstats\n-import tempfile\n+import shutil\n+import six\n from subprocess import Popen, PIPE\n+import sys\n+import tempfile\n import unittest\n try:\n     from cStringIO import StringIO\n@@ -57,14 +58,14 @@ class CmdlineTest(unittest.TestCase):\n             shutil.rmtree(path)\n \n     def test_override_dict_settings(self):\n+        EXT_PATH = \"tests.test_cmdline.extensions.DummyExtension\"\n+        EXTENSIONS = {EXT_PATH: 200}\n         settingsstr = self._execute('settings', '--get', 'EXTENSIONS', '-s',\n-                                    ('EXTENSIONS={\"tests.test_cmdline.extensions.TestExtension\": '\n-                                     '100, \"tests.test_cmdline.extensions.DummyExtension\": 200}'))\n+                                    'EXTENSIONS=' + json.dumps(EXTENSIONS))\n         # XXX: There's gotta be a smarter way to do this...\n         self.assertNotIn(\"...\", settingsstr)\n         for char in (\"'\", \"<\", \">\", 'u\"'):\n             settingsstr = settingsstr.replace(char, '\"')\n         settingsdict = json.loads(settingsstr)\n-        self.assertIn('tests.test_cmdline.extensions.DummyExtension', settingsdict)\n-        self.assertIn('value=200', settingsdict['tests.test_cmdline.extensions.DummyExtension'])\n-        self.assertIn('value=100', settingsdict['tests.test_cmdline.extensions.TestExtension'])\n+        six.assertCountEqual(self, settingsdict.keys(), EXTENSIONS.keys())\n+        self.assertIn('value=200', settingsdict[EXT_PATH])\n\n@@ -37,21 +37,22 @@ class SettingsAttributeTest(unittest.TestCase):\n         self.assertEqual(self.attribute.value, 'value')\n         self.assertEqual(self.attribute.priority, 10)\n \n-    def test_set_per_key_priorities(self):\n-        attribute = SettingsAttribute(\n-            BaseSettings({'one': 10, 'two': 20}, 0), 0)\n+    def test_overwrite_basesettings(self):\n+        original_dict = {'one': 10, 'two': 20}\n+        original_settings = BaseSettings(original_dict, 0)\n+        attribute = SettingsAttribute(original_settings, 0)\n \n-        new_dict = {'one': 11, 'two': 21}\n+        new_dict = {'three': 11, 'four': 21}\n         attribute.set(new_dict, 10)\n-        self.assertEqual(attribute.value['one'], 11)\n-        self.assertEqual(attribute.value['two'], 21)\n+        self.assertIsInstance(attribute.value, BaseSettings)\n+        six.assertCountEqual(self, attribute.value, new_dict)\n+        six.assertCountEqual(self, original_settings, original_dict)\n \n-        new_settings = BaseSettings()\n-        new_settings.set('one', 12, 20)\n-        new_settings.set('two', 12, 0)\n-        attribute.set(new_settings, 0)\n-        self.assertEqual(attribute.value['one'], 12)\n-        self.assertEqual(attribute.value['two'], 21)\n+        new_settings = BaseSettings({'five': 12}, 0)\n+        attribute.set(new_settings, 0)  # Insufficient priority\n+        six.assertCountEqual(self, attribute.value, new_dict)\n+        attribute.set(new_settings, 10)\n+        six.assertCountEqual(self, attribute.value, new_settings)\n \n     def test_repr(self):\n         self.assertEqual(repr(self.attribute),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f364764aea624a3db9a6033b5dba8a70f93535d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 732 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -90,10 +90,9 @@ class BaseSettings(MutableMapping):\n         self.update(values, priority)\n \n     def __getitem__(self, opt_name):\n-        value = None\n-        if opt_name in self:\n-            value = self.attributes[opt_name].value\n-        return value\n+        if opt_name not in self:\n+            return None\n+        return self.attributes[opt_name].value\n \n     def __contains__(self, name):\n         return name in self.attributes\n@@ -212,10 +211,9 @@ class BaseSettings(MutableMapping):\n         :param name: the setting name\n         :type name: string\n         \"\"\"\n-        prio = None\n-        if name in self:\n-            prio = self.attributes[name].priority\n-        return prio\n+        if name not in self:\n+            return None\n+        return self.attributes[name].priority\n \n     def maxpriority(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfed9b6659c90e0799361911b1d72ed127edf471", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 29 | Churn Cumulative: 504 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 27 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -1,4 +1,5 @@\n from collections import defaultdict\n+from inspect import isclass\n import logging\n import pprint\n \n@@ -31,7 +32,9 @@ class MiddlewareManager(object):\n         for clspath in mwlist:\n             try:\n                 mwcls = load_object(clspath)\n-                if crawler and hasattr(mwcls, 'from_crawler'):\n+                if not isclass(mwcls):\n+                    mw = mwcls\n+                elif crawler and hasattr(mwcls, 'from_crawler'):\n                     mw = mwcls.from_crawler(crawler)\n                 elif hasattr(mwcls, 'from_settings'):\n                     mw = mwcls.from_settings(settings)\n\n@@ -31,10 +31,15 @@ def arg_to_iter(arg):\n def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n-    object can be a class, function, variable o instance.\n+    If ``path`` is not a string, it will be returned.\n+\n+    The object can be a class, function, variable, or instance.\n     path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n     \"\"\"\n \n+    if not isinstance(path, six.string_types):\n+        return path\n+\n     try:\n         dot = path.rindex('.')\n     except ValueError:\n\n@@ -91,3 +91,16 @@ class MiddlewareManagerTest(unittest.TestCase):\n         mwman = TestMiddlewareManager.from_settings(settings)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\n+\n+    def test_instances_from_settings(self):\n+        settings = Settings()\n+        myM3 = M3()\n+        class InstanceTestMiddlewareManager(MiddlewareManager):\n+            @classmethod\n+            def _get_mwlist_from_settings(cls, settings):\n+                return [ 'tests.test_middleware.M1', M2, myM3 ]\n+        mwman = InstanceTestMiddlewareManager.from_settings(settings)\n+        self.assertIsInstance(mwman.middlewares[0], M1)\n+        self.assertIsInstance(mwman.middlewares[1], M2)\n+        self.assertIs(mwman.middlewares[2], myM3)\n+\n\n@@ -11,7 +11,9 @@ class UtilsMiscTestCase(unittest.TestCase):\n \n     def test_load_object(self):\n         obj = load_object('scrapy.utils.misc.load_object')\n-        assert obj is load_object\n+        self.assertIs(obj, load_object)\n+        not_a_string = int(1000)\n+        self.assertIs(load_object(not_a_string), not_a_string)\n         self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07455b1883cacab141572df4310315bb53e65ec9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 70 | Churn Cumulative: 2855 | Contributors (this commit): 15 | Commits (past 90d): 36 | Contributors (cumulative): 35 | DMM Complexity: 0.9512195121951219\n\nDIFF:\n@@ -6,6 +6,7 @@ import inspect\n import pkg_resources\n \n import scrapy\n+from scrapy.addons import AddonManager\n from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.commands import ScrapyCommand\n@@ -118,6 +119,9 @@ def execute(argv=None, settings=None):\n         conf.settings = settings\n     # ------------------------------------------------------------------\n \n+    addons = AddonManager()\n+    addons.load_cfg()\n+\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n@@ -139,7 +143,7 @@ def execute(argv=None, settings=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n-    cmd.crawler_process = CrawlerProcess(settings)\n+    cmd.crawler_process = CrawlerProcess(settings, addons)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n \n\n@@ -6,6 +6,7 @@ import warnings\n from twisted.internet import reactor, defer\n from zope.interface.verify import verifyClass, DoesNotImplement\n \n+from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n from scrapy.interfaces import ISpiderLoader\n@@ -23,7 +24,7 @@ logger = logging.getLogger(__name__)\n \n class Crawler(object):\n \n-    def __init__(self, spidercls, settings=None):\n+    def __init__(self, spidercls, settings=None, addons=None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n@@ -31,6 +32,12 @@ class Crawler(object):\n         self.settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        self.addons = addons if addons is not None else AddonManager()\n+        self.addons.load_settings(self.settings)\n+        self.addons.update_addons()\n+        self.addons.check_dependency_clashes()\n+        self.addons.update_settings(self.settings)\n+\n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n \n@@ -69,6 +76,7 @@ class Crawler(object):\n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n             self.engine = self._create_engine()\n+            self.addons.check_configuration(self)\n             start_requests = iter(self.spider.start_requests())\n             yield self.engine.open_spider(self.spider, start_requests)\n             yield defer.maybeDeferred(self.engine.start)\n@@ -111,10 +119,11 @@ class CrawlerRunner(object):\n             \":meth:`crawl` and managed by this class.\"\n     )\n \n-    def __init__(self, settings=None):\n+    def __init__(self, settings=None, addons=None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n+        self.addons = addons\n         self.spider_loader = _get_spider_loader(settings)\n         self._crawlers = set()\n         self._active = set()\n@@ -181,7 +190,7 @@ class CrawlerRunner(object):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n             spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings)\n+        return Crawler(spidercls, self.settings, self.addons)\n \n     def stop(self):\n         \"\"\"\n@@ -223,8 +232,8 @@ class CrawlerProcess(CrawlerRunner):\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    def __init__(self, settings=None):\n-        super(CrawlerProcess, self).__init__(settings)\n+    def __init__(self, settings=None, addons=None):\n+        super(CrawlerProcess, self).__init__(settings, addons)\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings)\n         log_scrapy_info(self.settings)\n\n@@ -20,7 +20,7 @@ def assert_aws_environ():\n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n         raise SkipTest(\"AWS keys not found\")\n \n-def get_crawler(spidercls=None, settings_dict=None):\n+def get_crawler(spidercls=None, settings_dict=None, addons=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n     priority.\n@@ -28,7 +28,7 @@ def get_crawler(spidercls=None, settings_dict=None):\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.spiders import Spider\n \n-    runner = CrawlerRunner(settings_dict)\n+    runner = CrawlerRunner(settings_dict, addons)\n     return runner.create_crawler(spidercls or Spider)\n \n def get_pythonpath():\n\n@@ -6,6 +6,7 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n+from scrapy.addons import Addon, AddonManager\n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n from tests import mock\n@@ -266,3 +267,19 @@ with multiples lines\n \n         self._assert_retried(log)\n         self.assertIn(\"Got response 200\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_abort_on_addon_failed_check(self):\n+        class FailedCheckAddon(Addon):\n+            name = 'FailedCheckAddon'\n+            version = '1.0'\n+            def check_configuration(self, config, crawler):\n+                raise ValueError\n+        addonmgr = AddonManager()\n+        addonmgr.add(FailedCheckAddon())\n+        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler.addons = addonmgr\n+        # Doesn't work in 'precise' test environment:\n+        #with self.assertRaises(ValueError):\n+        #    yield crawler.crawl()\n+        yield self.assertFailure(crawler.crawl(), ValueError)\n\n@@ -2,6 +2,7 @@ import warnings\n import unittest\n \n import scrapy\n+from scrapy.addons import Addon, AddonManager\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n@@ -51,6 +52,29 @@ class CrawlerTestCase(BaseCrawlerTest):\n         self.assertFalse(settings.frozen)\n         self.assertTrue(crawler.settings.frozen)\n \n+    def test_populate_addons_settings(self):\n+        class TestAddon(Addon):\n+            name = 'TestAddon'\n+            version = '1.0'\n+        addonconfig = {'TEST1': 'addon', 'TEST2': 'addon', 'TEST3': 'addon'}\n+        class TestAddon2(Addon):\n+            name = 'testAddon2'\n+            version = '1.0'\n+        addonconfig2 = {'TEST': 'addon2'}\n+\n+        settings = Settings()\n+        settings.set('TESTADDON_TEST1', 'project', priority='project')\n+        settings.set('TESTADDON_TEST2', 'default', priority='default')\n+        addonmgr = AddonManager()\n+        addonmgr.add(TestAddon(), addonconfig)\n+        addonmgr.add(TestAddon2(), addonconfig2)\n+        crawler = Crawler(DefaultSpider, settings, addonmgr)\n+\n+        self.assertEqual(crawler.settings['TESTADDON_TEST1'], 'project')\n+        self.assertEqual(crawler.settings['TESTADDON_TEST2'], 'addon')\n+        self.assertEqual(crawler.settings['TESTADDON_TEST3'], 'addon')\n+        self.assertEqual(crawler.settings['TESTADDON2_TEST'], 'addon2')\n+\n     def test_crawler_accepts_dict(self):\n         crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n         self.assertEqual(crawler.settings['foo'], 'bar')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
