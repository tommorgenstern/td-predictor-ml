{"custom_id": "keras#7c3278ff7b62ec8b4edf6e8d61762efd54c7a153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 39 | Files Changed: 1 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 119 | Churn Cumulative: 1279 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1032,6 +1032,8 @@ class H5IOStore:\n         # Init H5 entry group.\n         self._h5_entry_path = None\n         self._h5_entry_group = {}\n+        self._h5_entry_metadata = None\n+        self._h5_entry_initialized = False\n \n     def __bool__(self):\n         # Delegate `__bool__` to the underlying `h5_file`. Otherwise, Python\n@@ -1049,13 +1051,59 @@ class H5IOStore:\n             return h5py.File(path_or_io, mode=self.mode)\n \n     def make(self, path, metadata=None):\n+        \"\"\"Make a new H5 entry group.\n+\n+        This method is only available in write mode. It defers the creation of\n+        the H5 entry group until `__setitem__` is called, preventing the\n+        creation of empty groups.\n+\n+        Args:\n+            path: `str`. The variable path.\n+            metadata: Optional `dict`. The metadata to save with the H5 entry\n+                group. Defaults to `None`.\n+        \"\"\"\n+        if self.mode != \"w\":\n+            raise ValueError(\"`make` is only allowed in write mode.\")\n+        if not isinstance(metadata, (dict, type(None))):\n+            raise ValueError(\n+                f\"`metadata` should be a dict or `None`. Received: {metadata}\"\n+            )\n+\n         self._h5_entry_path = path\n-        self.make_entry(path, metadata=metadata)\n+        if metadata:\n+            self._create_h5_group(path, metadata=metadata)\n+        else:\n+            # Defer to `__setitem__` for H5 group creation to prevent the\n+            # creation of empty groups when the store is unused.\n+            self._h5_entry_group = {}\n+            self._h5_entry_initialized = False\n         return self\n \n     def get(self, path):\n+        \"\"\"Get the H5 entry group.\n+\n+        This method is only available in read mode.\n+\n+        Args:\n+            path: `str`. The variable path.\n+        \"\"\"\n+        if self.mode != \"r\":\n+            raise ValueError(\"`get` is only allowed in read mode.\")\n+\n         self._h5_entry_path = path\n-        self.make_entry(path)\n+        self._h5_entry_group = {}  # Defaults to an empty dict if not found.\n+        if not path:\n+            if \"vars\" in self.h5_file:\n+                self._h5_entry_group = self.h5_file[\"vars\"]\n+        elif path in self.h5_file and \"vars\" in self.h5_file[path]:\n+            self._h5_entry_group = self.h5_file[path][\"vars\"]\n+        else:\n+            # No hit. Fix for 2.13 compatibility.\n+            if \"_layer_checkpoint_dependencies\" in self.h5_file:\n+                path = path.replace(\"layers\", \"_layer_checkpoint_dependencies\")\n+                if path in self.h5_file and \"vars\" in self.h5_file[path]:\n+                    self._h5_entry_group = self.h5_file[path][\"vars\"]\n+        self._h5_entry_initialized = True\n         return self\n \n     def close(self):\n@@ -1067,38 +1115,18 @@ class H5IOStore:\n \n     # H5 entry level methods.\n \n-    def make_entry(self, path, metadata=None):\n-        if not isinstance(metadata, (dict, type(None))):\n-            raise ValueError(\n-                f\"`metadata` should be a dict or `None`. Received: {metadata}\"\n-            )\n-\n-        if self.mode == \"w\":\n+    def _create_h5_group(self, path, metadata=None):\n         if not path:\n             self._h5_entry_group = self.h5_file.create_group(\"vars\")\n         else:\n-                self._h5_entry_group = self.h5_file.create_group(\n-                    path\n-                ).create_group(\"vars\")\n+            self._h5_entry_group = self.h5_file.create_group(path).create_group(\n+                \"vars\"\n+            )\n         if metadata:\n             for k, v in metadata.items():\n                 self._h5_entry_group.attrs[k] = v\n-        else:\n-            self._h5_entry_group = {}  # Defaults to an empty dict if not found.\n-            if not path:\n-                if \"vars\" in self.h5_file:\n-                    self._h5_entry_group = self.h5_file[\"vars\"]\n-            elif path in self.h5_file and \"vars\" in self.h5_file[path]:\n-                self._h5_entry_group = self.h5_file[path][\"vars\"]\n-            else:\n-                # No hit.\n-                # Fix for 2.13 compatibility\n-                if \"_layer_checkpoint_dependencies\" in self.h5_file:\n-                    path = path.replace(\n-                        \"layers\", \"_layer_checkpoint_dependencies\"\n-                    )\n-                    if path in self.h5_file and \"vars\" in self.h5_file[path]:\n-                        self._h5_entry_group = self.h5_file[path][\"vars\"]\n+\n+        self._h5_entry_initialized = True\n \n     def __len__(self):\n         return self._h5_entry_group.__len__()\n@@ -1125,6 +1153,9 @@ class H5IOStore:\n     def __setitem__(self, key, value):\n         if self.mode != \"w\":\n             raise ValueError(\"Setting a value is only allowed in write mode.\")\n+        if not self._h5_entry_initialized:\n+            self._create_h5_group(self._h5_entry_path)\n+\n         value = backend.convert_to_numpy(value)\n         if backend.standardize_dtype(value.dtype) == \"bfloat16\":\n             ds = self._h5_entry_group.create_dataset(key, data=value)\n@@ -1187,6 +1218,8 @@ class ShardedH5IOStore(H5IOStore):\n         # Init H5 entry group.\n         self._h5_entry_path = None\n         self._h5_entry_group = {}\n+        self._h5_entry_metadata = None\n+        self._h5_entry_initialized = False\n \n         # Init shard parameters.\n         self.current_shard_index = 0\n@@ -1210,25 +1243,27 @@ class ShardedH5IOStore(H5IOStore):\n                     self.sharding_config = json.load(map_file)\n         self.h5_file = self._create_new_shard_file()\n \n-    def make(self, path, metadata=None):\n-        h5_entry = super().make(path, metadata=metadata)\n-        self.sharding_config[\"weight_map\"][self._h5_entry_group.name] = (\n-            self.current_shard_path.name\n-        )\n-        return h5_entry\n-\n     def get(self, path):\n+        \"\"\"Get the H5 entry group.\n+\n+        This method is only available in read mode. If the path is not found in\n+        the current shard, it will switch to the correct shard.\n+\n+        Args:\n+            path: `str`. The variable path.\n+        \"\"\"\n         if not path:\n             parsed_path = \"/vars\"\n         else:\n             parsed_path = path\n \n         # If not found, check shard map and switch files.\n-        filename = self.sharding_config[\"weight_map\"].get(\n-            parsed_path\n-        ) or self.sharding_config[\"weight_map\"].get(\"/\" + parsed_path + \"/vars\")\n+        weight_map = self.sharding_config[\"weight_map\"]\n+        filename = weight_map.get(parsed_path) or weight_map.get(\n+            \"/\" + parsed_path + \"/vars\"\n+        )\n \n-        if filename != self.current_shard_path.name:\n+        if filename is not None and filename != self.current_shard_path.name:\n             self.close()\n             self.h5_file = self._get_h5_file(self.path.with_name(filename))\n         return super().get(path)\n@@ -1293,6 +1328,12 @@ class ShardedH5IOStore(H5IOStore):\n \n         super().__setitem__(key, value)\n \n+        variable_path = self._h5_entry_group.name\n+        if variable_path not in self.sharding_config[\"weight_map\"]:\n+            self.sharding_config[\"weight_map\"][variable_path] = (\n+                self.current_shard_path.name\n+            )\n+\n \n class NpzIOStore:\n     def __init__(self, root_path, archive=None, mode=\"r\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0a30808634dc8cffc8f25f67abad254b8b00ad5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1340 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1307,12 +1307,12 @@ class Dice(LossFunctionWrapper):\n     >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]],\n     ...           [[[0.4], [0.0]], [[0.0], [0.9]]]]\n     >>> axis = (1, 2, 3)\n-    >>> loss = keras.losses.dice(y_true, y_pred, axis=axis)\n+    >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([0.5, 0.75757575], shape=(2,), dtype=float32)\n \n-    >>> loss = keras.losses.dice(y_true, y_pred)\n+    >>> loss = keras.losses.Dice()(y_true, y_pred)\n     >>> assert loss.shape == ()\n     >>> loss\n     array(0.6164384, shape=(), dtype=float32)\n@@ -2543,6 +2543,24 @@ def dice(y_true, y_pred, axis=None):\n \n     Returns:\n         Dice loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]],\n+    ...           [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+    >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]],\n+    ...           [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+    >>> axis = (1, 2, 3)\n+    >>> loss = keras.losses.dice(y_true, y_pred, axis=axis)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n+\n+    >>> loss = keras.losses.dice(y_true, y_pred)\n+    >>> assert loss.shape == ()\n+    >>> loss\n+    array(0.6164384, shape=(), dtype=float32)\n+\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4aae578b7f67bb653da3c268ba0634e799f47df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 53 | Churn Cumulative: 907 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 27 | DMM Complexity: 0.6046511627906976\n\nDIFF:\n@@ -96,6 +96,7 @@ class Variable:\n         trainable=True,\n         autocast=True,\n         aggregation=\"none\",\n+        synchronization=\"auto\",\n         name=None,\n     ):\n         name = name or auto_name(self.__class__.__name__)\n@@ -113,13 +114,28 @@ class Variable:\n             \"only_first_replica\",\n         ):\n             raise ValueError(\n-                \"Invalid valid for argument `aggregation`. Expected \"\n+                \"Invalid value for argument `aggregation`. Expected \"\n                 \"one of `None`, `'none'`, `'mean'`, `'sum'`, \"\n                 \"`'only_first_replica'`. \"\n                 f\"Received: aggregation={aggregation}\"\n             )\n         if aggregation is None:\n             aggregation = \"none\"\n+        if synchronization not in (\n+            None,\n+            \"none\",\n+            \"on_read\",\n+            \"on_write\",\n+            \"auto\",\n+        ):\n+            raise ValueError(\n+                \"Invalid value for argument `synchronization`. Expected \"\n+                \"one of `None`, `'none'`, `'on_read'`, `'on_write'`, \"\n+                \"`'auto'`. \"\n+                f\"Received: synchronization={synchronization}\"\n+            )\n+        if synchronization is None:\n+            synchronization = \"none\"\n         self._name = name\n         parent_path = current_path()\n         if parent_path:\n@@ -133,6 +149,7 @@ class Variable:\n         self._trainable = bool(trainable)\n         self._autocast = bool(autocast)\n         self._aggregation = aggregation\n+        self._synchronization = synchronization\n         # `self._overwrite_with_gradient` is an internal property to determine\n         # whether this variable should be overwritten by the computed gradient.\n         # Ref: https://github.com/google/flax/blob/main/flax/linen/fp8_ops.py\n@@ -227,6 +244,11 @@ class Variable:\n         \"\"\"The strategy for aggregating this variable.\"\"\"\n         return self._aggregation\n \n+    @property\n+    def synchronization(self):\n+        \"\"\"The strategy for synchronizing this variable.\"\"\"\n+        return self._synchronization\n+\n     @property\n     def value(self):\n         \"\"\"The current value of the variable (numpy array or backend tensor).\"\"\"\n\n@@ -45,6 +45,7 @@ class Variable(\n                 trainable=self.trainable,\n                 name=self.name,\n                 aggregation=self._map_aggregation(self.aggregation),\n+                synchronization=self._map_synchronization(self.synchronization),\n             )\n \n     def _initialize_with_initializer(self, initializer):\n@@ -125,6 +126,15 @@ class Variable(\n         }\n         return mapping[aggregation]\n \n+    def _map_synchronization(self, synchronization):\n+        mapping = {\n+            \"none\": tf.VariableSynchronization.NONE,\n+            \"on_read\": tf.VariableSynchronization.ON_READ,\n+            \"on_write\": tf.VariableSynchronization.ON_WRITE,\n+            \"auto\": tf.VariableSynchronization.AUTO,\n+        }\n+        return mapping[synchronization]\n+\n \n def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n     if isinstance(x, tf.SparseTensor) and sparse is not None and not sparse:\n\n@@ -137,6 +137,24 @@ class DistributeTest(testing.TestCase):\n             self.assertEqual(v2.aggregation, \"sum\")\n             self.assertEqual(v2.value.aggregation, tf.VariableAggregation.SUM)\n \n+    def test_variable_synchronization(self):\n+        strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n+\n+        with strategy.scope():\n+            x = np.random.random((4, 4))\n+            v1 = backend.Variable(x, dtype=\"float32\")\n+            self.assertEqual(v1.synchronization, \"auto\")\n+            # AUTO with MirroredStrategy defaults to ON_WRITE\n+            self.assertEqual(\n+                v1.value.synchronization, tf.VariableSynchronization.ON_WRITE\n+            )\n+\n+            v2 = backend.Variable(x, dtype=\"float32\", synchronization=\"on_read\")\n+            self.assertEqual(v2.synchronization, \"on_read\")\n+            self.assertEqual(\n+                v2.value.synchronization, tf.VariableSynchronization.ON_READ\n+            )\n+\n     def test_seed_generator(self):\n         strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n         with strategy.scope():\n\n@@ -201,6 +201,7 @@ class Metric(KerasSaveable):\n                 dtype=dtype,\n                 trainable=False,\n                 aggregation=aggregation,\n+                synchronization=\"on_read\",\n                 name=name,\n             )\n         # Prevent double-tracking\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#717a64a3a467d4ee19dd9314e8e2402593d826bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 271 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 271 | Churn Cumulative: 351 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ from keras.src.optimizers.ftrl import Ftrl\n from keras.src.optimizers.lamb import Lamb\n from keras.src.optimizers.lion import Lion\n from keras.src.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.src.optimizers.muon import Muon\n from keras.src.optimizers.nadam import Nadam\n from keras.src.optimizers.optimizer import Optimizer\n from keras.src.optimizers.rmsprop import RMSprop\n\n@@ -19,6 +19,7 @@ from keras.src.optimizers.ftrl import Ftrl\n from keras.src.optimizers.lamb import Lamb\n from keras.src.optimizers.lion import Lion\n from keras.src.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.src.optimizers.muon import Muon\n from keras.src.optimizers.nadam import Nadam\n from keras.src.optimizers.optimizer import Optimizer\n from keras.src.optimizers.rmsprop import RMSprop\n\n@@ -8,6 +8,7 @@ from keras.src.optimizers.adamw import AdamW\n from keras.src.optimizers.ftrl import Ftrl\n from keras.src.optimizers.lion import Lion\n from keras.src.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.src.optimizers.muon import Muon\n from keras.src.optimizers.nadam import Nadam\n from keras.src.optimizers.optimizer import Optimizer\n from keras.src.optimizers.rmsprop import RMSprop\n\n@@ -0,0 +1,268 @@\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.optimizers import optimizer\n+\n+\n+@keras_export([\"keras.optimizers.Muon\"])\n+class Muon(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Muon algorithm.\n+\n+    The Muon optimizer is a new type of optimizer\n+    first implemented in https://github.com/KellerJordan/Muon\n+    first verified in the paper https://arxiv.org/abs/2502.16982\n+\n+    Note that this optimizer should not be used in the following layers:\n+    1. Embedding layer\n+    2. Final output fully connected layer\n+    3. Any {0,1}-D parameters\n+    These should all be optimized using AdamW.\n+    Args:\n+        learning_rate: A float,\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to `0.001`.\n+        adam_beta_1: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use.\n+            The exponential decay rate for the 1st moment estimates. Defaults to\n+            `0.9`.\n+        adam_beta_2: A float value or a constant float tensor, ora callable\n+            that takes no arguments and returns the actual value to use.\n+            The exponential decay rate for the 2nd moment estimates. Defaults to\n+            `0.999`.\n+        epsilon: A small constant for numerical stability. This is\n+            \"epsilon hat\" in the Kingma and Ba paper\n+            (in the formula just before Section 2.1),\n+            not the epsilon in Algorithm 1 of the paper.\n+            It be used at Adamw.Defaults to `1e-7`.\n+        exclude_layers: List of strings, keywords of layer names to exclude.\n+            All layers with keywords in their path will use adamw.\n+        exclude_embeddings: Boolean value\n+            If True, embedding layers will use adamw.\n+        muon_a: Float, parameter a of the muon algorithm.\n+            It is recommended to use the default value\n+        muon_b: Float, parameter b of the muon algorithm.\n+            It is recommended to use the default value\n+        muon_c: Float, parameter c of the muon algorithm.\n+            It is recommended to use the default value\n+        adam_lr_ratio: Float, the ratio of the learning rate when\n+                using Adam to the main learning rate.\n+                it is recommended to set it to 0.1\n+        momentum: Float, momentum used by internal SGD.\n+        ns_steps: Integer, number of Newton-Schulz iterations to run.\n+        nesterov: Boolean, whether to use Nesterov-style momentum\n+        {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        adam_beta_1=0.9,\n+        adam_beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=0.1,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n+        name=\"muon\",\n+        exclude_layers=None,\n+        exclude_embeddings=True,\n+        muon_a=3.4445,\n+        muon_b=-4.7750,\n+        muon_c=2.0315,\n+        adam_lr_ratio=0.1,\n+        momentum=0.95,\n+        ns_steps=6,\n+        nesterov=True,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n+            **kwargs,\n+        )\n+        self.adam_beta_1 = adam_beta_1\n+        self.adam_beta_2 = adam_beta_2\n+        self.epsilon = epsilon\n+        self.muon_a = muon_a\n+        self.muon_b = muon_b\n+        self.muon_c = muon_c\n+        self.adam_lr_ratio = adam_lr_ratio\n+        self.momentum = momentum\n+        self.ns_steps = ns_steps\n+        self.nesterov = nesterov\n+        self.exclude_embeddings = exclude_embeddings\n+        # exclude_layers is a keyword at variable path\n+        # so it must be a string\n+        assert isinstance(exclude_layers, str) or exclude_layers is None\n+        self.exclude_layers = exclude_layers.lower()\n+\n+    def _should_use_adamw(self, variable):\n+        # To use it with 4D convolutional filters,\n+        # it works well to just flatten their last 3 dimensions.\n+        # any {0,1}-D parameters should all be optimized by adam\n+        if not 1 < len(variable.shape) < 4:\n+            return True\n+        if self.exclude_embeddings and \"embedding\" in variable.path.lower():\n+            return True\n+        if self.exclude_layers in variable.path.lower():\n+            return True\n+        return False\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+            var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self.adam_momentums = {}\n+        self.adam_velocities = {}\n+\n+        self.muon_momentums = {}\n+        self.muon_velocities = {}\n+\n+        for var in var_list:\n+            self.adam_momentums[var.path] = self.add_variable_from_reference(\n+                reference_variable=var, name=\"momentum\"\n+            )\n+            if self._should_use_adamw(var):\n+                self.adam_velocities[var.path] = (\n+                    self.add_variable_from_reference(\n+                        reference_variable=var, name=\"velocity\"\n+                    )\n+                )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        if self._should_use_adamw(variable):\n+            # It should be noted that lr is one-tenth when using adamw.\n+            self.adamw_update_step(\n+                gradient, variable, learning_rate * self.adam_lr_ratio\n+            )\n+        else:\n+            self.muon_update_step(gradient, variable, learning_rate)\n+\n+    def muon_update_step(self, gradient, variable, lr):\n+        m = self.adam_momentums[variable.path]\n+        self.assign_add(m, ops.add(gradient, m * (self.momentum - 1)))\n+        shape = variable.shape\n+        if self.nesterov:\n+            g = ops.add(gradient, self.momentum * m)\n+        else:\n+            g = m\n+\n+        self.assign_sub(\n+            variable,\n+            lr\n+            * self.zeropower_via_newtonschulz5(g, self.ns_steps)\n+            * max(1, shape[0] / shape[1]) ** 0.5,\n+        )\n+\n+    def adamw_update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        adam_beta_1_power = ops.power(\n+            ops.cast(self.adam_beta_1, variable.dtype), local_step\n+        )\n+        adam_beta_2_power = ops.power(\n+            ops.cast(self.adam_beta_2, variable.dtype), local_step\n+        )\n+\n+        m = self.adam_momentums[variable.path]\n+        v = self.adam_velocities[variable.path]\n+\n+        alpha = lr * ops.sqrt(1 - adam_beta_2_power) / (1 - adam_beta_1_power)\n+\n+        self.assign_add(\n+            m, ops.multiply(ops.subtract(gradient, m), 1 - self.adam_beta_1)\n+        )\n+        self.assign_add(\n+            v,\n+            ops.multiply(\n+                ops.subtract(ops.square(gradient), v), 1 - self.adam_beta_2\n+            ),\n+        )\n+        self.assign_sub(\n+            variable,\n+            ops.divide(\n+                ops.multiply(m, alpha), ops.add(ops.sqrt(v), self.epsilon)\n+            ),\n+        )\n+\n+    def transpose_last_axis(self, X):\n+        shape = ops.shape(X)\n+        temp_order = list(range(len(shape)))\n+        temp_order[-2] = temp_order[-1]\n+        temp_order[-1] = len(shape) - 2\n+        X = ops.transpose(X, temp_order)\n+        return X\n+\n+    def zeropower_via_newtonschulz5(self, x, steps: int):\n+        \"\"\"\n+        We apply the Newton-Schulz iteration to compute matrix G.\n+        We select a quintic iteration that maximizes the slope at zero.\n+        This approach helps minimize steps, even if the iteration doesn't fully\n+        converge across the interval.\n+        The result isn't exactly UV^T (from the SVD of G),\n+        but rather an approximation like US'V^T. Despite this approximation,\n+        model performance remains unaffected\n+        compared to using the exact UV^T from the SVD.\n+        \"\"\"\n+        shape = ops.shape(x)\n+        assert len(shape) >= 2\n+\n+        a, b, c = self.muon_a, self.muon_b, self.muon_c\n+        if shape[-2] > shape[-1]:\n+            x = self.transpose_last_axis(x)\n+\n+        # Ensure spectral norm is at most 1\n+        x = x / (ops.norm(x, axis=(-2, -1), keepdims=True) + 1e-7)\n+        # Perform the NS iterations\n+        for _ in range(steps):\n+            temp_a = x @ self.transpose_last_axis(x)\n+            temp_b = b * temp_a + c * temp_a @ temp_a\n+            x = a * x + temp_b @ x\n+\n+        if shape[-2] > shape[-1]:\n+            x = self.transpose_last_axis(x)\n+        return x\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"adam_beta_1\": self.adam_beta_1,\n+                \"adam_beta_2\": self.adam_beta_2,\n+                \"epsilon\": self.epsilon,\n+                \"exclude_layers\": self.exclude_layers,\n+                \"muon_a\": self.muon_a,\n+                \"muon_b\": self.muon_b,\n+                \"muon_c\": self.muon_c,\n+                \"adam_lr_ratio\": self.adam_lr_ratio,\n+                \"momentum\": self.momentum,\n+                \"ns_steps\": self.ns_steps,\n+                \"nesterov\": self.nesterov,\n+                \"exclude_embeddings\": self.exclude_embeddings,\n+            }\n+        )\n+        return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af3d207afe1e706225fa36cb14138084c5cb5268", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 305 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,15 +7,34 @@ from keras.src.optimizers import optimizer\n class Muon(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Muon algorithm.\n \n-    The Muon optimizer is a new type of optimizer\n-    first implemented in https://github.com/KellerJordan/Muon\n-    first verified in the paper https://arxiv.org/abs/2502.16982\n-\n     Note that this optimizer should not be used in the following layers:\n+\n     1. Embedding layer\n     2. Final output fully connected layer\n-    3. Any {0,1}-D parameters\n+    3. Any {0,1}-D variables\n+\n     These should all be optimized using AdamW.\n+\n+    The Muon optimizer can use both the Muon update step or the\n+    AdamW update step based on the following:\n+\n+    - For any variable that isn't 2D, 3D or 4D, the AdamW step\n+        will be used. This is not configurable.\n+    - If the argument `exclude_embeddings` (defaults to `True`) is set\n+    to `True`, the AdamW step will be used.\n+    - For any variablewith a name that matches an expression\n+        listed in the argument `exclude_layers` (a list), the\n+        AdamW step will be used.\n+    - Any other variable uses the Muon step.\n+\n+    Typically, you only need to pass the name of your densely-connected\n+    output layer to `exclude_layers`, e.g.\n+    `exclude_layers=[\"output_dense\"]`.\n+\n+    References:\n+        - [Original implementation](https://github.com/KellerJordan/Muon)\n+        - [Liu et al, 2025](https://arxiv.org/abs/2502.16982)\n+\n     Args:\n         learning_rate: A float,\n             `keras.optimizers.schedules.LearningRateSchedule` instance, or\n@@ -154,13 +173,13 @@ class Muon(optimizer.Optimizer):\n     def update_step(self, gradient, variable, learning_rate):\n         if self._should_use_adamw(variable):\n             # It should be noted that lr is one-tenth when using adamw.\n-            self.adamw_update_step(\n+            self._adamw_update_step(\n                 gradient, variable, learning_rate * self.adam_lr_ratio\n             )\n         else:\n-            self.muon_update_step(gradient, variable, learning_rate)\n+            self._muon_update_step (gradient, variable, learning_rate)\n \n-    def muon_update_step(self, gradient, variable, lr):\n+    def _muon_update_step (self, gradient, variable, lr):\n         m = self.adam_momentums[variable.path]\n         self.assign_add(m, ops.add(gradient, m * (self.momentum - 1)))\n         shape = variable.shape\n@@ -176,7 +195,7 @@ class Muon(optimizer.Optimizer):\n             * max(1, shape[0] / shape[1]) ** 0.5,\n         )\n \n-    def adamw_update_step(self, gradient, variable, learning_rate):\n+    def _adamw_update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         lr = ops.cast(learning_rate, variable.dtype)\n         gradient = ops.cast(gradient, variable.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0cfd2aa18b9e321c06adfda7b65b036989d71697", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1209 | Contributors (this commit): 14 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -2078,7 +2078,7 @@ def signbit(x):\n             tf.bitwise.bitwise_and(\n                 tf.bitcast(x, tf.int32),\n                 # tf.float32 sign bit\n-                tf.constant(0x80000000, dtype=tf.int32),\n+                tf.constant(tf.int32.min, dtype=tf.int32),\n             ),\n             0,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9fea22aaca4ad23eafa46c4cf82700c61d5a88e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 176 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras.src.api_export import keras_export\n+from keras.src.backend import epsilon\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n@@ -95,7 +96,7 @@ class RandomSaturation(BaseImagePreprocessingLayer):\n             maxval=self.factor[1],\n             seed=seed,\n         )\n-        factor = factor / (1 - factor)\n+        factor = factor / (1 - factor + epsilon())\n         return {\"factor\": factor}\n \n     def transform_images(self, images, transformation=None, training=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3da0abd09ed14e1e84001d1a73e70f49bc18d777", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 36 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,19 @@ class Callback:\n \n     @property\n     def model(self):\n+        if backend.backend() == \"torch\":\n+            from torch.nn.parallel import DistributedDataParallel\n+\n+            if isinstance(self._model, DistributedDataParallel):\n+                # Keras Callbacks expect to work with Keras models. e.g\n+                # ModelCheckpoint and EarlyStopping both attempt to call\n+                # keras-specific APIs on the value returned from this\n+                # property. If this callback was created against a DDP\n+                # wrapper instead of the underlying keras.Model, it is\n+                # likely to fail. Return self._model.module for DDP\n+                # instances instead.\n+                return self._model.module\n+\n         if backend.backend() == \"jax\" and hasattr(\n             self._model, \"jax_state_sync\"\n         ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#895a123b0a0ab2c953b861f53092465fb1ea1536", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 93 | Churn Cumulative: 398 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.8636363636363636\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import re\n+\n from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.optimizers import optimizer\n@@ -124,10 +126,7 @@ class Muon(optimizer.Optimizer):\n         self.ns_steps = ns_steps\n         self.nesterov = nesterov\n         self.exclude_embeddings = exclude_embeddings\n-        # exclude_layers is a keyword at variable path\n-        # so it must be a string\n-        assert isinstance(exclude_layers, str) or exclude_layers is None\n-        self.exclude_layers = exclude_layers.lower()\n+        self.exclude_layers = exclude_layers or []\n \n     def _should_use_adamw(self, variable):\n         # To use it with 4D convolutional filters,\n@@ -137,7 +136,8 @@ class Muon(optimizer.Optimizer):\n             return True\n         if self.exclude_embeddings and \"embedding\" in variable.path.lower():\n             return True\n-        if self.exclude_layers in variable.path.lower():\n+        for keyword in self.exclude_layers:\n+            if re.search(keyword, variable.path):\n                 return True\n         return False\n \n\n@@ -0,0 +1,83 @@\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src import testing\n+from keras.src.layers import Dense\n+from keras.src.layers import Embedding\n+from keras.src.optimizers.muon import Muon\n+\n+\n+class MuonTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Muon(\n+            learning_rate=0.5,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_Newton_Schulz(self):\n+        optimizer = Muon()\n+        tensor_input = ops.array([[0.2499, 0.9105], [0.2655, 0.8824]])\n+        except_output = ops.array([[-0.4422, 0.6457], [0.7285, 0.2968]])\n+        output = optimizer.zeropower_via_newtonschulz5(tensor_input, 5)\n+        self.assertAllClose(output, except_output, rtol=1e-3, atol=1e-3)\n+\n+    def test_adamw_single_step(self):\n+        optimizer = Muon()\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0], name=\"test_vars\")\n+        optimizer.build([vars])\n+        optimizer._adamw_update_step(grads, vars, 0.5)\n+        self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n+\n+    def test_should_use_adamw(self):\n+        vars = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        optimizer = Muon(exclude_layers=[\"var\"])\n+        self.assertAllClose(\n+            True,\n+            optimizer._should_use_adamw(vars),\n+        )\n+        embeding = Embedding(2, 2)\n+        embeding.build()\n+        self.assertAllClose(\n+            True,\n+            optimizer._should_use_adamw(embeding.weights[0]),\n+        )\n+        vars = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        optimizer = Muon()\n+        self.assertAllClose(\n+            False,\n+            optimizer._should_use_adamw(vars),\n+        )\n+        dense = Dense(2)\n+        dense.build([None, 2])\n+        self.assertAllClose(\n+            False,\n+            optimizer._should_use_adamw(dense.weights[0]),\n+        )\n+\n+    def test_muon_single_step(self):\n+        optimizer = Muon(\n+            learning_rate=0.5,\n+            weight_decay=0,\n+        )\n+        grads = ops.array([[1.0, 6.0], [7.0, 2.0]])\n+        vars = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        optimizer.build([vars])\n+        optimizer._muon_update_step(grads, vars, 0.5)\n+        self.assertAllClose(\n+            vars, [[1.13, 1.51], [2.57, 4.06]], rtol=1e-2, atol=1e-2\n+        )\n+\n+    def test_clip_norm(self):\n+        optimizer = Muon(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Muon(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a69952e0c9a8e8b6649631ab5d611c3c33fad120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 1492 | Contributors (this commit): 13 | Commits (past 90d): 19 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -832,11 +832,24 @@ def linspace(\n \n def log(x):\n     x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if x_type.is_integral():\n+        x_type = OPENVINO_DTYPES[config.floatx()]\n+        x = ov_opset.convert(x, x_type)\n     return OpenVINOKerasTensor(ov_opset.log(x).output(0))\n \n \n def log10(x):\n-    raise NotImplementedError(\"`log10` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if x_type.is_integral():\n+        x_type = OPENVINO_DTYPES[config.floatx()]\n+        x = ov_opset.convert(x, x_type)\n+    log_x = ov_opset.log(x).output(0)\n+    const_10 = ov_opset.constant(10, x_type).output(0)\n+    log_10 = ov_opset.log(const_10).output(0)\n+    result = ov_opset.divide(log_x, log_10).output(0)\n+    return OpenVINOKerasTensor(result)\n \n \n def log1p(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4baba1e997622517a411e1a544505dc11a4db578", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 59 | Churn Cumulative: 206 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 0.02564102564102564\n\nDIFF:\n@@ -191,24 +191,37 @@ class SavedModelTest(testing.TestCase):\n     def test_multi_input_model(self):\n         input_1 = layers.Input(shape=(3,))\n         input_2 = layers.Input(shape=(5,))\n-        model = models.Model([input_1, input_2], [input_1, input_2])\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n \n-        tf.saved_model.save(model, path)\n-        restored_model = tf.saved_model.load(path)\n+        y1 = layers.Dense(1)(input_1)\n+        y2 = layers.Dense(1)(input_2)\n+        layer_2 = layers.Dense(1, activation=\"relu\")\n+        output_1 = layer_2(y1)\n+        output_2 = layer_2(y2)\n+        model = models.Model([input_1, input_2], [output_1, output_2])\n+\n         input_arr_1 = np.random.random((1, 3)).astype(\"float32\")\n         input_arr_2 = np.random.random((1, 5)).astype(\"float32\")\n \n-        outputs = restored_model.signatures[\"serving_default\"](\n+        model = models.Model([input_1, input_2], [output_1, output_2])\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n+        outputs_1 = model(\n+            inputs=[\n+                tf.convert_to_tensor(input_arr_1, dtype=tf.float32),\n+                tf.convert_to_tensor(input_arr_2, dtype=tf.float32),\n+            ],\n+        )\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+\n+        outputs_2 = restored_model.signatures[\"serving_default\"](\n             inputs=tf.convert_to_tensor(input_arr_1, dtype=tf.float32),\n             inputs_1=tf.convert_to_tensor(input_arr_2, dtype=tf.float32),\n         )\n-\n         self.assertAllClose(\n-            input_arr_1, outputs[\"output_0\"], rtol=1e-4, atol=1e-4\n+            outputs_1[0], outputs_2[\"output_0\"], rtol=1e-4, atol=1e-4\n         )\n         self.assertAllClose(\n-            input_arr_2, outputs[\"output_1\"], rtol=1e-4, atol=1e-4\n+            outputs_1[1], outputs_2[\"output_1\"], rtol=1e-4, atol=1e-4\n         )\n \n     def test_multi_input_custom_model_and_layer(self):\n\n@@ -81,6 +81,12 @@ class Function(Operation):\n         self._nodes_by_depth = nodes_by_depth\n         self._operations = operations\n         self._operations_by_depth = operations_by_depth\n+        for input in self._inputs:\n+            if (\n+                input._keras_history.operation\n+                and not input._keras_history.operation._outbound_nodes\n+            ):\n+                raise ValueError(\"`inputs` not connected to `outputs`\")\n \n     @property\n     def operations(self):\n\n@@ -7,6 +7,7 @@ from keras.src.backend.common import keras_tensor\n from keras.src.layers import Dense\n from keras.src.layers import Input\n from keras.src.models import Model\n+from keras.src.models import Sequential\n from keras.src.ops import function\n from keras.src.ops import numpy as knp\n \n@@ -142,3 +143,26 @@ class FunctionTest(testing.TestCase):\n             ValueError, \"`inputs` argument cannot be empty\"\n         ):\n             _ = function.Function(inputs=[], outputs=x)\n+\n+    def test_function_with_unconnected_inputs(self):\n+        model_1 = Sequential(\n+            [\n+                Input(shape=(6,)),\n+                Dense(3, activation=\"sigmoid\"),\n+            ]\n+        )\n+        model_2 = Sequential(\n+            [\n+                Input(shape=(3,)),\n+                Dense(2, activation=\"sigmoid\"),\n+            ],\n+        )\n+        with self.assertRaisesRegex(\n+            ValueError, \"`inputs` not connected to `outputs`\"\n+        ):\n+            _ = Model(Input(shape=(6,)), model_2(model_1(Input(shape=(6,)))))\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"`inputs` not connected to `outputs`\"\n+        ):\n+            _ = Model(model_1(Input(shape=(6,))), model_2(Input(shape=(3,))))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8333ef4705af6228024a62f2bdf7b82e1af8ef76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 32 | Churn Cumulative: 797 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -347,7 +347,7 @@ class Functional(Function, Model):\n                 x[0] = None\n             return tuple(x)\n \n-        def make_spec_for_tensor(x):\n+        def make_spec_for_tensor(x, name=None):\n             optional = False\n             if isinstance(x._keras_history[0], InputLayer):\n                 if x._keras_history[0].optional:\n@@ -355,7 +355,7 @@ class Functional(Function, Model):\n             return InputSpec(\n                 shape=shape_with_no_batch_size(x.shape),\n                 allow_last_axis_squeeze=True,\n-                name=x._keras_history[0].name,\n+                name=x._keras_history[0].name if name is None else name,\n                 optional=optional,\n             )\n \n@@ -367,13 +367,7 @@ class Functional(Function, Model):\n                 # Case where `_nested_inputs` is a plain dict of Inputs.\n                 names = sorted(self._inputs_struct.keys())\n                 return [\n-                    InputSpec(\n-                        shape=shape_with_no_batch_size(\n-                            self._inputs_struct[name].shape\n-                        ),\n-                        allow_last_axis_squeeze=True,\n-                        name=name,\n-                    )\n+                    make_spec_for_tensor(self._inputs_struct[name], name=name)\n                     for name in names\n                 ]\n             return None  # Deeply nested dict: skip checks.\n\n@@ -574,6 +574,26 @@ class FunctionalTest(testing.TestCase):\n         self.assertAllClose(out, np.ones((2, 2)))\n         # Note: it's not intended to work in symbolic mode (yet).\n \n+    def test_optional_dict_inputs(self):\n+        class OptionalInputLayer(layers.Layer):\n+            def call(self, x, y=None):\n+                if y is not None:\n+                    return x + y\n+                return x\n+\n+            def compute_output_shape(self, x_shape):\n+                return x_shape\n+\n+        i1 = Input((2,), name=\"input1\")\n+        i2 = Input((2,), name=\"input2\", optional=True)\n+        outputs = OptionalInputLayer()(i1, i2)\n+        model = Model({\"input1\": i1, \"input2\": i2}, outputs)\n+\n+        # Eager test\n+        out = model({\"input1\": np.ones((2, 2)), \"input2\": None})\n+        self.assertAllClose(out, np.ones((2, 2)))\n+        # Note: it's not intended to work in symbolic mode (yet).\n+\n     def test_warning_for_mismatched_inputs_structure(self):\n         def is_input_warning(w):\n             return str(w.message).startswith(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8b893f1fc62bd64b88db04c6e4a501aa94a0904", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1503 | Contributors (this commit): 13 | Commits (past 90d): 20 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -857,7 +857,16 @@ def log1p(x):\n \n \n def log2(x):\n-    raise NotImplementedError(\"`log2` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_type = x.get_element_type()\n+    if x_type.is_integral():\n+        x_type = OPENVINO_DTYPES[config.floatx()]\n+        x = ov_opset.convert(x, x_type)\n+    log_x = ov_opset.log(x).output(0)\n+    const_2 = ov_opset.constant(2, x_type).output(0)\n+    log_2 = ov_opset.log(const_2).output(0)\n+    result = ov_opset.divide(log_x, log_2).output(0)\n+    return OpenVINOKerasTensor(result)\n \n \n def logaddexp(x1, x2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
