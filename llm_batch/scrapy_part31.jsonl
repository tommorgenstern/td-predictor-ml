{"custom_id": "scrapy#8a404c4480b394e34ece358eac7907ea3a364b2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 213 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n import urllib\n-import warnings\n import hashlib\n from copy import copy\n from base64 import urlsafe_b64encode\n@@ -84,10 +83,6 @@ class Request(object):\n             http_pass = ''\n         self.headers['Authorization'] = 'Basic ' + urlsafe_b64encode(\"%s:%s\" % (http_user, http_pass))\n \n-    def __getitem__(self, key):\n-        warnings.warn(\"Request tuples have been deprecated, Request objects are now used. Index mappings: 0 -> r.url, 1 -> r.body, 2 -> r.deferred, 3 -> r.options \", DeprecationWarning)\n-        return (self.url, self.body, self.deferred, {})[key]\n-\n     def __str__(self):\n         if self.method != 'GET':\n             return \"<(%s) %s>\" % (self.method, self.url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e437ff67ae31d2401b3ff2c10951c1d09a12c51c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 44 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,30 +1,28 @@\n \"\"\"\n-jhis is an expertimental middleware to respect robots.txt policies. The biggest\n-problem it has is that it uses urllib directly (in RobotFileParser.read()\n-method) and that conflicts with twisted networking, so it should be ported to\n-use twisted networking API, but that is not as trivial as it may seem.\n-\n-This code is left here for future reference, when we resume the work on this\n-subject.\n+This is a middleware to respect robots.txt policies. To active it you must\n+enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n \"\"\"\n \n-import re\n import urlparse\n import robotparser\n \n from pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy import log\n+from scrapy.core.engine import scrapyengine\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.spider import spiders\n+from scrapy.http import Request\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n-BASEURL_RE = re.compile(\"http://.*?/\")\n-\n-class RobotsMiddleware(object):\n+class RobotsTxtMiddleware(object):\n \n     def __init__(self):\n+        if not settings.getbool('ROBOTSTXT_OBEY'):\n+            raise NotConfigured\n+\n         self._parsers = {}\n         self._spiderdomains = {}\n         self._pending = {}\n@@ -38,18 +36,22 @@ class RobotsMiddleware(object):\n             raise IgnoreRequest(\"URL forbidden by robots.txt: %s\" % request.url)\n \n     def robot_parser(self, url, spiderdomain):\n-        urldomain = urlparse.urlparse(url).hostname\n+        parsedurl = urlparse.urlparse(url)\n+        urldomain = parsedurl.hostname\n         if urldomain in self._parsers:\n-            rp = self._parsers[urldomain]\n+            return self._parsers[urldomain]\n         else:\n-            rp = robotparser.RobotFileParser()\n-            m = BASEURL_RE.search(url)\n-            if m:\n-                rp.set_url(\"%srobots.txt\" % m.group())\n-                rp.read()\n-            self._parsers[urldomain] = rp\n+            self._parsers[urldomain] = None\n+            robotsurl = \"%s://%s/robots.txt\" % parsedurl[0:2]\n+            robotsreq = Request(robotsurl)\n+            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=0)\n+            dfd.addCallbacks(callback=self._parse_robots, callbackArgs=[urldomain])\n             self._spiderdomains[spiderdomain].add(urldomain)\n-        return rp\n+\n+    def _parse_robots(self, response, urldomain):\n+        rp = robotparser.RobotFileParser()\n+        rp.parse(response.to_string().splitlines())\n+        self._parsers[urldomain] = rp\n \n     def domain_open(self, domain):\n         self._spiderdomains[domain] = set()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a3ba6957bc0b6ec37127510ce09a4546d6fee6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 182 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -54,6 +54,7 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n \n DOWNLOADER_MIDDLEWARES = [\n     # Engine side\n+    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware',\n     'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n     'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n@@ -134,6 +135,8 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n \n+ROBOTSTXT_OBEY = True\n+\n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d255f7bacf9746383a878fd538ca96a2ac31d8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 186 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,14 +2,14 @@\n This module contains the default values for all settings used by Scrapy. \n \n For more information about these settings you can read the settings\n-documentation in docs/reference/settings.rst\n+documentation in docs/ref/settings.rst\n \n Scrapy developers, if you add a setting here remember to:\n \n * add it in alphabetical order\n * group similar settings without leaving blank lines\n * add its documentation to the available settings documentation\n-  (docs/reference/settings.rst)\n+  (docs/ref/settings.rst)\n \n \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4bfb27f8351a9b59a36c441c6af87c767b04180", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 72 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 72 | Churn Cumulative: 288 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,72 +0,0 @@\n-\"\"\"\n-Utility for autodetecting and decompressing responses\n-\"\"\"\n-\n-import zipfile\n-import tarfile\n-import gzip\n-import bz2\n-from scrapy.http import ResponseBody\n-try:\n-    from cStringIO import StringIO\n-except:\n-    from StringIO import StringIO\n-\n-class Decompressor(object):\n-    class ArchiveIsEmpty(Exception):\n-        pass\n-    \n-    def __init__(self):\n-        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,\n-                              'gz': self.is_gzip, 'bz2': self.is_bzip2}        \n-    \n-    def is_tar(self, response):\n-        try:\n-            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)\n-        except tarfile.ReadError:\n-            return False\n-        if tar_file.members:\n-            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))\n-        else:\n-            raise self.ArchiveIsEmpty\n-    \n-    def is_zip(self, response):\n-        try:\n-            zip_file = zipfile.ZipFile(self.archive)\n-        except zipfile.BadZipfile:\n-            return False\n-        namelist = zip_file.namelist()\n-        if namelist:\n-            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))\n-        else:\n-            raise self.ArchiveIsEmpty\n-            \n-    def is_gzip(self, response):\n-        try:\n-            gzip_file = gzip.GzipFile(fileobj=self.archive)\n-            decompressed_body = gzip_file.read()\n-        except IOError:\n-            return False\n-        return response.replace(body=decompressed_body)\n-            \n-    def is_bzip2(self, response):\n-        try:\n-            decompressed_body = bz2.decompress(self.body)\n-        except IOError:\n-            return False\n-        return response.replace(body=ResponseBody(decompressed_body))\n-    \n-    def extract_winfo(self, response):\n-        self.body = response.body.to_string()\n-        self.archive = StringIO()\n-        self.archive.write(self.body)\n-        \n-        for decompressor in self.decompressors.keys():\n-            self.archive.seek(0)\n-            new_response = self.decompressors[decompressor](response)\n-            if new_response:\n-                return new_response, decompressor\n-        return response, ''\n-        \n-    def extract(self, response):\n-        return self.extract_winfo(response)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#604c86b779059bead44c7c4d6eff6124a17f56ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,9 +36,8 @@ class Unquote(object):\n     Output: list of strings\n     \"\"\"\n     def __init__(self, keep=None):\n-        self.keep = ['amp', 'lt'] if keep is None else keep\n+        self.keep = [] if keep is None else keep\n \n     def __call__(self, value, keep=None):\n-        if keep is not None:\n-            self.keep = keep\n-        return [ unquote_markup(v, keep=self.keep) for v in value ]\n+        keep = keep if keep is not None else self.keep\n+        return [ unquote_markup(v, keep=keep) for v in value ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc1a168d533229169171bdbd28f85b069208ad8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 318 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -119,11 +119,11 @@ class AdaptorsTestCase(unittest.TestCase):\n \n         \n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n \n         \n     def test_unquote(self):\n-        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        self.assertEqual(adaptors.Unquote(keep=['amp', 'lt'])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n \n         \n     def test_remove_tags(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ea631398e46e911d11b3d2965c8e24544d90f7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 24 | Files Changed: 7 | Hunks: 19 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 1063 | Contributors (this commit): 2 | Commits (past 90d): 30 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import zlib\n from gzip import GzipFile\n from cStringIO import StringIO\n \n-from scrapy.http import Response, ResponseBody\n+from scrapy.http import Response\n \n \n class CompressionMiddleware(object):\n@@ -18,9 +18,8 @@ class CompressionMiddleware(object):\n             if content_encoding:\n                 encoding = content_encoding[0].lower()\n                 raw_body = response.body.get_content()\n-                declared_encoding = response.body.declared_encoding\n                 decoded_body = self._decode(raw_body, encoding)\n-                response.body = ResponseBody(decoded_body, declared_encoding)\n+                response = response.replace(body=decoded_body)\n                 response.headers['Content-Encoding'] = content_encoding[1:]\n         return response\n \n\n@@ -8,7 +8,7 @@ except:\n     from StringIO import StringIO\n \n from scrapy import log\n-from scrapy.http import Response, ResponseBody\n+from scrapy.http import Response\n \n class DecompressionMiddleware(object):\n     \"\"\" This middleware tries to recognise and extract the possibly compressed\n@@ -28,7 +28,7 @@ class DecompressionMiddleware(object):\n         except tarfile.ReadError:\n             return False\n         if tar_file.members:\n-            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))\n+            return response.replace(body=tar_file.extractfile(tar_file.members[0]).read())\n         else:\n             raise self.ArchiveIsEmpty\n \n@@ -39,7 +39,7 @@ class DecompressionMiddleware(object):\n             return False\n         namelist = zip_file.namelist()\n         if namelist:\n-            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))\n+            return response.replace(body=zip_file.read(namelist[0]))\n         else:\n             raise self.ArchiveIsEmpty\n \n@@ -56,7 +56,7 @@ class DecompressionMiddleware(object):\n             decompressed_body = bz2.decompress(self.body)\n         except IOError:\n             return False\n-        return response.replace(body=ResponseBody(decompressed_body))\n+        return response.replace(body=decompressed_body)\n \n     def extract(self, response):\n         \"\"\" This method tries to decompress the given response, if possible,\n\n@@ -29,10 +29,8 @@ class Response(object) :\n         self.original_url = Url(original_url) if original_url else url # different if redirected or escaped\n         self.headers = Headers(headers or {})\n         self.status = status\n-        # FIXME this should be replaced with assert(isinstance(body, basestring)) since ResponseBody is not meant to be used directly\n-        if isinstance(body, ResponseBody):\n-            self.body = body\n-        else:\n+        # ResponseBody is not meant to be used directly (use .replace instead)\n+        assert(isinstance(body, basestring) or body is None)\n         self.body = ResponseBody(body, self.headers_encoding())\n         self.cached = False\n         self.request = None # request which originated this response\n\n@@ -4,7 +4,7 @@ import unittest\n import re\n \n from scrapy.contrib import adaptors\n-from scrapy.http import Response, ResponseBody, Headers\n+from scrapy.http import Response, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n \n class AdaptorsTestCase(unittest.TestCase):\n@@ -89,7 +89,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                            <a onclick=\"javascript: opensomething('dummy/my_html2.html');\">something2</a>\n                          </div>\n                        </body></html>\"\"\"\n-        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))\n+        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=test_data)\n         sample_xsel = XmlXPathSelector(sample_response)\n         sample_adaptor = adaptors.ExtractImages(response=sample_response)\n \n\n@@ -1,6 +1,6 @@\n from unittest import TestCase, main\n import libxml2\n-from scrapy.http import ResponseBody,  Response\n+from scrapy.http import Response\n \n class Libxml2Test(TestCase):\n     def setUp(self):\n@@ -35,8 +35,8 @@ class ResponseLibxml2DocTest(TestCase):\n         scrapymanager.configure()\n \n         self.body_content = 'test problematic \\x00 body'\n-        self.problematic_body = ResponseBody(self.body_content, 'utf-8')\n-        response = Response('example.com', 'http://example.com/catalog/product/blabla-123', body=self.problematic_body)\n+        response = Response('example.com', 'http://example.com/catalog/product/blabla-123',\n+                            headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n         response.getlibxml2doc()\n \n if __name__ == \"__main__\":\n\n@@ -1,7 +1,7 @@\n import os\n import unittest\n \n-from scrapy.http.response import Response, ResponseBody\n+from scrapy.http.response import Response\n from scrapy.link import LinkExtractor, Link\n from scrapy.link.extractors import RegexLinkExtractor\n from scrapy.contrib.link_extractors import HTMLImageLinkExtractor\n@@ -37,10 +37,10 @@ class LinkExtractorTestCase(unittest.TestCase):\n     def test_extraction_encoding(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()\n-        response_utf8 = Response(url='http://example.com/utf8', domain='example.com', body=ResponseBody(body), headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=ResponseBody(body))\n+        response_utf8 = Response(url='http://example.com/utf8', domain='example.com', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n+        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=body)\n         body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()\n-        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=ResponseBody(body))\n+        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=body)\n \n         lx = LinkExtractor()\n         self.assertEqual(lx.extract_links(response_utf8),\n@@ -67,7 +67,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()\n-        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))\n+        self.response = Response(url='http://example.com/index', domain='example.com', body=body)\n \n     def test_urls_type(self):\n         '''Test that the resulting urls are regular strings and not a unicode objects'''\n@@ -141,7 +141,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n #    def setUp(self):\n #        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n #        body = open(os.path.join(base_path 'image_linkextractor.html'), 'r').read()\n-#        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))\n+#        self.response = Response(url='http://example.com/index', domain='example.com', body=body)\n \n #    def test_urls_type(self):\n #        '''Test that the resulting urls are regular strings and not a unicode objects'''\n\n@@ -1,6 +1,6 @@\n import os\n from unittest import TestCase, main\n-from scrapy.http import Response, ResponseBody\n+from scrapy.http import Response\n from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n \n class ScrapyDecompressionTest(TestCase):\n@@ -17,7 +17,7 @@ class ScrapyDecompressionTest(TestCase):\n \n         for format in formats:\n             fd = open(os.path.join(self.datadir, 'feed-sample1.' + format), 'r')\n-            body = ResponseBody(fd.read())\n+            body = fd.read()\n             fd.close()\n             self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ca989a78125d1267966d985bf24b7e772e841f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 61 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,8 +11,13 @@ urlpatterns = patterns('',\n     url(r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n     url(r'^admin/(.*)', admin.site.root),\n \n+    # pablo: disabled documentation until we improve its usability. for now,\n+    # documentation will be served at http://doc.scrapy.org using static html\n+    # files generated by Sphinx, and a small notice landing page will be shown\n+    # in scrapy.org/doc\n     # docs\n-    url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n+#    url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n+    url(r\"docs/.*\", 'django.views.generic.simple.redirect_to', {'url': '/doc/'}),\n     # news\n     url(r\"^news/\", include(\"scrapyorg.blog.urls\")),\n )\n@@ -29,3 +34,4 @@ if settings.DEBUG: # devel\n urlpatterns += patterns('',         \n     url(r\"\", include(\"scrapyorg.article.urls\")),\n )\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c4012f924d6a77133e93cbc23d7724feb4d3a3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-__version__ = \"0.1.0\"\n+__version__ = \"0.7.0\"\n \n import sys, os\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1962c1c0af52e1ae6700d680685f65182b5097c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 37 | Files Changed: 3 | Hunks: 10 | Methods Changed: 14 | Complexity Δ (Sum/Max): 6/7 | Churn Δ: 163 | Churn Cumulative: 1034 | Contributors (this commit): 2 | Commits (past 90d): 29 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,16 +42,7 @@ class RobustScrapedItem(ScrapedItem):\n     }\n     \n     def __init__(self, data=None):\n-        \"\"\"\n-        A scraped item can be initialised with a dictionary that will be\n-        squirted directly into the object.\n-        \"\"\"\n-        if isinstance(data, dict):\n-            for attr, value in data.iteritems():\n-                setattr(self, attr, value)\n-        elif data is not None:\n-            raise UsageError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n-\n+        super(RobustScrapedItem, self).__init__(data)\n         self.__dict__['_version'] = None\n \n     def __getattr__(self, attr):\n@@ -113,15 +104,6 @@ class RobustScrapedItem(ScrapedItem):\n     def __sub__(self, other):\n         return RobustItemDelta(other, self)\n \n-    def __repr__(self):\n-        # Generate this format so that it can be deserialized easily:\n-        # ClassName({...})\n-        reprdict = {}\n-        for k, v in self.__dict__.iteritems():\n-            if not k.startswith('_'):\n-                reprdict[k] = v\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n-\n     def __str__(self) :\n         return \"%s: GUID=%s, url=%s\" % ( self.__class__.__name__ , self.guid, self.url )\n             \n\n@@ -7,7 +7,30 @@ class ScrapedItem(object):\n     'guid' attribute is required, and is an attribute\n     that identifies uniquely the given scraped item.\n     \"\"\"\n-    _adaptors_dict = {}\n+\n+    def __init__(self, data=None):\n+        \"\"\"\n+        A ScrapedItem can be initialised with a dictionary that will be\n+        squirted directly into the object.\n+        \"\"\"\n+        self._adaptors_dict = {}\n+\n+        if isinstance(data, dict):\n+            for attr, value in data.iteritems():\n+                setattr(self, attr, value)\n+        elif data is not None:\n+            raise UsageError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n+\n+    def __repr__(self):\n+        \"\"\"\n+        Generate the following format so that items can be deserialized\n+        easily: ClassName({'attrib': value, ...})\n+        \"\"\"\n+        reprdict = dict(items for items in self.__dict__.iteritems() if not items[0].startswith('_'))\n+        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n+\n+    def __sub__(self, other):\n+        raise NotImplementedError\n \n     def set_adaptors(self, adaptors_dict):\n         \"\"\"\n@@ -35,29 +58,43 @@ class ScrapedItem(object):\n                 pipe.insert(position, adaptor)\n             self.set_attrib_adaptors(attrib, pipe)\n \n-    def attribute(self, attrname, value, **kwargs):\n-        override = kwargs.pop('override', False)\n-        add = kwargs.pop('add', False)\n+    def attribute(self, attrname, value, override=False, add=False, **kwargs):\n+        \"\"\"\n+        Set the given value to the provided attribute (`attrname`) by filtering it\n+        through its adaptor pipeline first (if it has any).\n+\n+        If the attribute has been already set, it won't be overwritten unless\n+        `override` is True.\n+        If there was an old value and `add` is True (but `override` isn't), `value`\n+        will be appended/extended (depending on its type), to the old value, as long as\n+        the old value is a list.\n+        If both of the values are strings they will be joined by using the `add` delimiter\n+        (which may be a string, or True, in which case '' will be used as the delimiter).\n+\n+        The kwargs parameter is passed to the adaptors pipeline, which manages to transmit\n+        it to the adaptors themselves.\n+        \"\"\"\n         pipe = self._adaptors_dict.get(attrname)\n+        old_value = getattr(self, attrname, None)\n+\n         if pipe:\n-            val = pipe(value, **kwargs)\n-            if val or val is False:\n-                curr_val = getattr(self, attrname, None)\n-                if not curr_val:\n-                    setattr(self, attrname, val)\n-                else:\n+            value = pipe(value, **kwargs)\n+\n+        if old_value:\n             if override:\n-                        setattr(self, attrname, val)\n+                setattr(self, attrname, value)\n             elif add:\n-                        if all(isinstance(var, basestring) for var in (curr_val, val)):\n-                            setattr(self, attrname, '%s\\t%s' % (curr_val, val))\n-                        elif all(hasattr(var, '__iter__') for var in (curr_val, val)):\n-                            setattr(self, attrname, curr_val + val)\n-        elif value:\n+                if hasattr(old_value, '__iter__'):\n+                    if hasattr(value, '__iter__'):\n+                        self.__dict__[attrname].extend(list(value))\n+                    else:\n+                        self.__dict__[attrname].append(value)\n+                elif isinstance(old_value, basestring) and isinstance(value, basestring):\n+                    delimiter = add if isinstance(add, basestring) else ''\n+                    setattr(self, attrname, '%s%s%s' % (old_value, delimiter, value))\n+        else:\n             setattr(self, attrname, value)\n \n-    def __sub__(self, other):\n-        raise NotImplementedError\n \n class ItemDelta(object):\n     pass\n\n@@ -0,0 +1,70 @@\n+# -*- coding: utf8 -*-\n+import unittest\n+\n+from scrapy.item.models import ScrapedItem\n+from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.contrib import adaptors\n+\n+class ScrapedItemTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.item = ScrapedItem()\n+\n+    def test_attribute_basic(self):\n+        self.item.attribute('name', 'John')\n+        self.assertEqual(self.item.name, 'John')\n+\n+    def test_attribute_override(self):\n+        self.item.attribute('name', 'John')\n+        self.item.attribute('name', 'Charlie')\n+        self.assertEqual(self.item.name, 'John')\n+\n+        self.item.attribute('name', 'Charlie', override=True)\n+        self.assertEqual(self.item.name, 'Charlie')\n+\n+    def test_attribute_add(self):\n+        self.item.attribute('name', 'John')\n+      \n+        self.item.attribute('name', 'Doe', add=True)\n+        self.assertEqual(self.item.name, 'JohnDoe')\n+\n+        self.item.attribute('name', 'Smith', add=' ')\n+        self.assertEqual(self.item.name, 'JohnDoe Smith')\n+\n+        self.item.attribute('children', ['Ken', 'Tom'])\n+        self.item.attribute('children', 'Bobby')\n+        self.item.attribute('children', 'Jimmy', add=True)\n+        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy'])\n+\n+        self.item.attribute('children', ['Johnny', 'Rodrigo'], add=True)\n+        self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n+\n+    def test_set_adaptors(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        delist = adaptors.Delist()\n+        self.item.set_adaptors({'name': [adaptors.extract, delist]})\n+        self.assertEqual(self.item._adaptors_dict, {'name': [adaptors.extract, delist]})\n+\n+        self.item.set_adaptors({'description': [adaptors.extract]})\n+        self.assertEqual(self.item._adaptors_dict, {'description': [adaptors.extract]})\n+\n+    def test_set_attrib_adaptors(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        self.item.set_attrib_adaptors('name', [adaptors.extract, adaptors.strip])\n+        self.assertEqual(self.item._adaptors_dict['name'],\n+            AdaptorPipe([adaptors.extract, adaptors.strip]))\n+\n+        unquote = adaptors.Unquote()\n+        self.item.set_attrib_adaptors('name', [adaptors.extract, unquote])\n+        self.assertEqual(self.item._adaptors_dict['name'],\n+            AdaptorPipe([adaptors.extract, unquote]))\n+\n+    def test_add_adaptor(self):\n+        self.assertEqual(self.item._adaptors_dict, {})\n+\n+        self.item.add_adaptor('name', adaptors.strip)\n+        self.assertEqual(self.item._adaptors_dict['name'], [adaptors.strip])\n+        self.item.add_adaptor('name', adaptors.extract, position=0)\n+        self.assertEqual(self.item._adaptors_dict['name'], [adaptors.extract, adaptors.strip])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3c7203ad9c2213bb851990841d71ec5cff4d663", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -46,9 +46,9 @@ copyright = u'2008-2009, Insophia'\n # built documents.\n #\n # The short X.Y version.\n-version = '0.7'\n+version = '0.7.0'\n # The full version, including alpha/beta/rc tags.\n-release = '0.7'\n+release = version\n \n # The language for content autogenerated by Sphinx. Refer to documentation\n # for a list of supported languages.\n@@ -139,7 +139,7 @@ html_use_smartypants = True\n #html_split_index = False\n \n # If true, the reST sources are included in the HTML build as _sources/<name>.\n-html_copy_source = False\n+html_copy_source = True\n \n # If true, an OpenSearch description file will be output, and all pages will\n # contain a <link> tag referring to it.  The value of this option must be the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4df3118c03918e922303fb4e18aab1118bfc740e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 23 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n+# IMPORTANT: remember to also update the version in docs/conf.py\n __version__ = \"0.7.0\"\n \n import sys, os\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c80fef7eb072317b247b96ceeaccd48afdb2eaa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 151 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 151 | Churn Cumulative: 151 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -0,0 +1,6 @@\n+# Define here the models for your scraped items\n+\n+from scrapy.item import ScrapedItem\n+\n+class MyItem(ScrapedItem):\n+    pass\n\n@@ -0,0 +1,4 @@\n+#!/usr/bin/env python\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n@@ -0,0 +1,96 @@\n+import google_bot\n+\n+# ---------------------------------------------------------------------------\n+# - Scrapy settings for google_bot                                    -\n+# ---------------------------------------------------------------------------\n+\n+BOT_NAME = 'scrapybot'\n+BOT_VERSION = '1.0'\n+\n+SPIDER_MODULES = ['google_bot.spiders']\n+NEWSPIDER_MODULE = 'google_bot.spiders'\n+TEMPLATES_DIR = '%s/templates' % google_bot.__path__[0]\n+ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % google_bot.__path__[0]\n+DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+DOWNLOAD_TIMEOUT = 600\n+\n+# uncomment if you want to add your own custom scrapy commands\n+#COMMANDS_MODULE = 'google_bot.commands'\n+#COMMANDS_SETTINGS_MODULE = 'google_bot.conf.commands'\n+\n+#Global timeout between sucessive downloads (can be overrided by spider\n+#attribute download_timeout\n+#DOWNLOAD_TIMEOUT = 0\n+\n+MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n+MYSQL_CONNECTION_PING_PERIOD = 600\n+\n+SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n+SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n+\n+#CACHE2_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n+#CACHE2_IGNORE_MISSING = 0   # ignore requests not in cache\n+#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n+\n+#STATS_ENABLED = 1   # enable stats\n+#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n+#STATS_DEBUG = 0     # log stats on domain closed\n+\n+EXTENSIONS = (\n+    'scrapy.management.web.WebConsole',\n+    'scrapy.management.telnet.TelnetConsole',\n+)\n+\n+DOWNLOADER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    # Downloader side\n+)\n+\n+SPIDER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n+    # Spider side\n+)\n+\n+# Item pipelines are usually configured by commands (see conf/commands)\n+ITEM_PIPELINES = (\n+    'scrapy.contrib.pipeline.show.ShowItemPipeline',\n+)\n+\n+#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n+#DEPTH_STATS = 1    # enable depth stats\n+\n+# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n+URLLENGTH_LIMIT = 2083\n+\n+#WEBCONSOLE_ENABLED = 1\n+#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n+\n+#TELNETCONSOLE_ENABLED = 1\n+#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n+\n+# global mail sending settings\n+#MAIL_HOST = 'localhost'\n+#MAIL_FROM = 'scrapybot@localhost'\n+\n+# scrapy webservice\n+WS_ENABLED = 0\n+\n+SPIDERPROFILER_ENABLED = 0\n\n@@ -0,0 +1 @@\n+# Place here all your scrapy spiders\n\n@@ -0,0 +1,44 @@\n+# -*- coding: utf8 -*-\n+from scrapy.xpath import HtmlXPathSelector\n+from scrapy.item import ScrapedItem\n+from scrapy.contrib import adaptors\n+from scrapy.contrib.spiders import CrawlSpider, Rule\n+from scrapy.link.extractors import RegexLinkExtractor\n+from scrapy.utils.misc import items_to_csv\n+\n+class GoogleDirectorySpider(CrawlSpider):\n+    domain_name = 'google.com'\n+    start_urls = ['http://www.google.com/dirhp']\n+\n+    rules = (\n+        Rule(RegexLinkExtractor(allow=('google.com/[A-Z][a-zA-Z_/]+$', ), ),\n+            'parse_category',\n+            follow=True,\n+        ),\n+    )\n+\n+    adaptor_pipe = [adaptors.extract, adaptors.Delist(), adaptors.strip]\n+    csv_file = open('scraped_items.csv', 'w')\n+\n+    def parse_category(self, response):\n+        items = [] # The item (links to websites) list we're going to return\n+        hxs = HtmlXPathSelector(response) # The selector we're going to use in order to extract data from the page\n+        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n+\n+        for link in links:\n+            item = ScrapedItem()\n+            item.set_adaptors({\n+                'name': self.adaptor_pipe,\n+                'url': self.adaptor_pipe,\n+                'description': self.adaptor_pipe,\n+            })\n+\n+            item.attribute('name', link.x('a/text()'))\n+            item.attribute('url', link.x('a/@href'))\n+            item.attribute('description', link.x('font[2]/text()'))\n+            items.append(item)\n+\n+        items_to_csv(self.csv_file, items)\n+        return items\n+\n+SPIDER = GoogleDirectorySpider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e636909391eb04cc1edc87f12674bd884fb97df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 188 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -47,6 +47,8 @@ CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n \n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n \n+DEFAULT_SPIDER = None\n+\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f7196c150ac6309f014947300bf5d5fb38d6d6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 194 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,9 +2,6 @@\n Spider middleware manager\n \"\"\"\n \n-\n-from twisted.python.failure import Failure\n-\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9d050b91d9e939476cbecee6093d3c8aeef3623", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 23 | Files Changed: 10 | Hunks: 16 | Methods Changed: 28 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 663 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -83,13 +83,13 @@ class ItemSamplerMiddleware(object):\n         if not settings['ITEMSAMPLER_FILE']:\n             raise NotConfigured\n \n-    def process_scrape(self, response, spider):\n+    def process_spider_input(self, response, spider):\n         if stats.getpath(\"%s/items_sampled\" % spider.domain_name) >= items_per_domain:\n             return []\n         elif max_response_size and max_response_size > len(response):  \n             return []\n \n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         requests, items = [], []\n         for r in result:\n             if isinstance(r, Request):\n\n@@ -17,7 +17,7 @@ class DepthMiddleware(object):\n         if self.stats and self.maxdepth:\n             stats.setpath('_envinfo/request_depth_limit', self.maxdepth)\n \n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request):\n                 request.depth = response.request.depth + 1\n\n@@ -21,7 +21,7 @@ class RequestLimitMiddleware(object):\n         if not self.max_queue_size:\n             raise NotConfigured\n \n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         requests = []\n         items = []\n         for r in result:\n\n@@ -8,7 +8,7 @@ from scrapy.http import Request\n from scrapy.utils.url import url_is_from_spider\n \n class OffsiteMiddleware(object):\n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         def _filter(r):\n             if isinstance(r, Request) and not url_is_from_spider(r.url, spider):\n                 log.msg(\"Ignoring link (offsite): %s \" % r.url, level=log.DEBUG, domain=spider.domain_name)\n\n@@ -6,7 +6,7 @@ originated it.\n from scrapy.http import Request\n \n class RefererMiddleware(object):\n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         def _set_referer(r):\n             if isinstance(r, Request):\n                 r.headers.setdefault('Referer', response.url)\n\n@@ -12,7 +12,7 @@ class RestrictMiddleware(object):\n         if not self.allowed_urls:\n             raise NotConfigured\n \n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         def _filter(r):\n             if isinstance(r, Request) and r.url not in self.allowed_urls:\n                 return False\n\n@@ -6,7 +6,7 @@ from scrapy.http import Request\n from scrapy.utils.url import canonicalize_url\n \n class UrlFilterMiddleware(object):\n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         disabled = getattr(spider, 'urlfilter_disabled', False)\n         for r in result:\n             if isinstance(r, Request) and not disabled:\n\n@@ -13,7 +13,7 @@ class UrlLengthMiddleware(object):\n         if not self.maxlength:\n             raise NotConfigured\n \n-    def process_result(self, response, result, spider):\n+    def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n                 log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), level=log.DEBUG, domain=spider.domain_name)\n\n@@ -67,7 +67,7 @@ class CrawlSpider(BaseSpider):\n         return a list of ScrapedItems and/or Requests\"\"\"\n         return []\n \n-    def process_results(self, results, response):\n+    def process_spider_output(self, results, response):\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n\n@@ -1,5 +1,9 @@\n \"\"\"\n-Spider middleware manager\n+This module implements the Spider Middleware manager. For more information see\n+the Spider Middleware doc in:\n+\n+docs/topics/spider-middleware.rst\n+\n \"\"\"\n \n from scrapy import log\n@@ -23,12 +27,12 @@ class SpiderMiddlewareManager(object):\n         self.load()\n \n     def _add_middleware(self, mw):\n-        if hasattr(mw, 'process_scrape'):\n-            self.spider_middleware.append(mw.process_scrape)\n-        if hasattr(mw, 'process_result'):\n-            self.result_middleware.insert(0, mw.process_result)\n-        if hasattr(mw, 'process_exception'):\n-            self.exception_middleware.insert(0, mw.process_exception)\n+        if hasattr(mw, 'process_spider_input'):\n+            self.spider_middleware.append(mw.process_spider_input)\n+        if hasattr(mw, 'process_spider_output'):\n+            self.result_middleware.insert(0, mw.process_spider_output)\n+        if hasattr(mw, 'process_spider_exception'):\n+            self.exception_middleware.insert(0, mw.process_spider_exception)\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n@@ -48,7 +52,7 @@ class SpiderMiddlewareManager(object):\n     def scrape(self, request, response, spider):\n         fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n \n-        def process_scrape(response):\n+        def process_spider_input(response):\n             for method in self.spider_middleware:\n                 result = method(response=response, spider=spider)\n                 assert result is None or _isiterable(result), \\\n@@ -59,7 +63,7 @@ class SpiderMiddlewareManager(object):\n             return self.call(request=request, response=response, spider=spider)\n \n \n-        def process_result(result):\n+        def process_spider_output(result):\n             for method in self.result_middleware:\n                 result = method(response=response, result=result, spider=spider)\n                 assert _isiterable(result), \\\n@@ -67,7 +71,7 @@ class SpiderMiddlewareManager(object):\n                     (fname(method), type(result))\n             return result\n \n-        def process_exception(_failure):\n+        def process_spider_exception(_failure):\n             exception = _failure.value\n             for method in self.exception_middleware:\n                 result = method(response=response, exception=exception, spider=spider)\n@@ -78,9 +82,9 @@ class SpiderMiddlewareManager(object):\n                     return result\n             return _failure\n \n-        dfd = mustbe_deferred(process_scrape, response)\n-        dfd.addErrback(process_exception)\n-        dfd.addCallback(process_result)\n+        dfd = mustbe_deferred(process_spider_input, response)\n+        dfd.addErrback(process_spider_exception)\n+        dfd.addCallback(process_spider_output)\n         return dfd\n \n     def call(self, request, response, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
