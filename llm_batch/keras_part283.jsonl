{"custom_id": "keras#c0d1709cbae3d05efc6dd224230012bc120be8e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1026 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,7 @@\n # -*- coding: utf-8 -*-\n-'''This example uses a convolutional stack followed by a recurrent stack\n+'''\n+# Optical character recognition\n+This example uses a convolutional stack followed by a recurrent stack\n and a CTC logloss function to perform optical character recognition\n of generated text images. I have no evidence of whether it actually\n learns general shapes of text, or just is able to recognize all\n@@ -17,17 +19,18 @@ the word list to include two words separated by a space.\n The table below shows normalized edit distance values. Theano uses\n a slightly different CTC implementation, hence the different results.\n \n-            Norm. ED\n Epoch |   TF   |   TH\n-------------------------\n-    10   0.027   0.064\n-    15   0.038   0.035\n-    20   0.043   0.045\n-    25   0.014   0.019\n+-----:|-------:|-------:\n+    10|  0.027 | 0.064\n+    15|  0.038 | 0.035\n+    20|  0.043 | 0.045\n+    25|  0.014 | 0.019\n \n-This requires cairo and editdistance packages:\n+This requires ```cairo``` and ```editdistance``` packages:\n+```python\n pip install cairocffi\n pip install editdistance\n+```\n \n Created by Mike Henry\n https://github.com/mbhenry/\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c13d2723d01212d09dfdda39b0ad439803ec9230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 33 | Churn Cumulative: 13499 | Contributors (this commit): 92 | Commits (past 90d): 2 | Contributors (cumulative): 103 | DMM Complexity: 0.7692307692307693\n\nDIFF:\n@@ -137,9 +137,12 @@ def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n+                    if layer.supports_masking:\n                         output_masks = to_list(\n                             layer.compute_mask(computed_tensor,\n                                                computed_mask))\n+                    else:\n+                        output_masks = [None] * len(output_tensors)\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:\n@@ -150,9 +153,12 @@ def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n+                    if layer.supports_masking:\n                         output_masks = to_list(\n                             layer.compute_mask(computed_tensors,\n                                                computed_masks))\n+                    else:\n+                        output_masks = [None] * len(output_tensors)\n                 # Update tensor_map.\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,\n\n@@ -339,6 +339,33 @@ def test_clone_functional_model():\n     new_model.train_on_batch(None, val_out)\n \n \n+def test_clone_functional_model_with_multi_outputs():\n+    input_layer = keras.Input(shape=(4,))\n+\n+    # Layer with single input and multiple outputs\n+    layer1 = keras.layers.Lambda(lambda x: [x + 1, x],\n+                                 lambda shapes: [shapes, shapes])\n+    x_a, x_b = layer1(input_layer)\n+\n+    class SwapLayer(keras.layers.Layer):\n+        def call(self, inputs, **kwargs):\n+            return [inputs[1], inputs[0]]\n+\n+        def compute_output_shape(self, input_shape):\n+            return [input_shape[1], input_shape[0]]\n+\n+    # Layer with multiple inputs and outputs\n+    x_a, x_b = SwapLayer()([x_a, x_b])\n+    model = keras.Model(inputs=[input_layer], outputs=[x_a, x_b])\n+    new_model = keras.models.clone_model(model)\n+\n+    x_test = np.random.random((10, 4))\n+    pred_a, pred_b = model.predict(x_test)\n+    pred_new_a, pred_new_b = new_model.predict(x_test)\n+    assert(pred_a.all() == pred_new_a.all())\n+    assert(pred_b.all() == pred_new_b.all())\n+\n+\n def test_clone_sequential_model():\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#faf0e02621df87abde519466aacc130157e29c6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 537 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ latent vector from a Gaussian distribution with mean=0 and std=1.\n # Reference\n \n [1] Kingma, Diederik P., and Max Welling.\n-\"Auto-encoding variational bayes.\"\n+\"Auto-Encoding Variational Bayes.\"\n https://arxiv.org/abs/1312.6114\n '''\n \n@@ -31,10 +31,10 @@ import os\n \n \n # reparameterization trick\n-# instead of sampling from Q(z|X), sample eps = N(0,I)\n-# z = z_mean + sqrt(var)*eps\n+# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n+# z = z_mean + sqrt(var) * epsilon\n def sampling(args):\n-    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n+    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n \n     # Arguments\n         args (tensor): mean and log of variance of Q(z|X)\n@@ -55,7 +55,7 @@ def plot_results(models,\n                  data,\n                  batch_size=128,\n                  model_name=\"vae_mnist\"):\n-    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n+    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n \n     # Arguments\n         models (tuple): encoder and decoder models\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e6db0e99ef56b23b3977b5969413d0b636a6fad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf6ca32e7ccf05bdfb35aed80726265b61256ea1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 235 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n+'''\n+#Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n \n Output after 4 epochs on CPU: ~0.8146\n Time per epoch on CPU (Core i7): ~150s.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca5a4e3d6954eecc2910145111dd91f37d3ccf32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 209 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,8 @@\n-'''This example demonstrates the use of Convolution1D for text classification.\n+'''\n+#This example demonstrates the use of Convolution1D for text classification.\n \n-Gets to 0.89 test accuracy after 2 epochs.\n-90s/epoch on Intel i5 2.4Ghz CPU.\n+Gets to 0.89 test accuracy after 2 epochs. </br>\n+90s/epoch on Intel i5 2.4Ghz CPU. </br>\n 10s/epoch on Tesla K40 GPU.\n '''\n from __future__ import print_function\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d5cdd7efdf8c10ff7df322059dccc8013144521", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 219 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-\"\"\" This script demonstrates the use of a convolutional LSTM network.\n+\"\"\"\n+#This script demonstrates the use of a convolutional LSTM network.\n \n This network is used to predict the next frame of an artificially\n generated movie which contains moving squares.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c16931b49aceb38b763b4a24a55a99b6847e29b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 234 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-'''The example demonstrates how to write custom layers for Keras.\n+'''\n+#This example demonstrates how to write custom layers for Keras.\n \n We build a custom activation layer called 'Antirectifier',\n which modifies the shape of the tensor that passes through it.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e74d79997905a3ba7b247f60a93353b6a874ab5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1048 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,12 @@\n-'''Deep Dreaming in Keras.\n+'''\n+#Deep Dreaming in Keras.\n \n Run the script with:\n-```\n+```python\n python deep_dream.py path_to_your_base_image.jpg prefix_for_results\n ```\n e.g.:\n-```\n+```python\n python deep_dream.py img/mypic.jpg results/dream\n ```\n '''\n\n@@ -1,5 +1,5 @@\n-'''Train a recurrent convolutional network on the IMDB sentiment\n-classification task.\n+'''\n+#Train a recurrent convolutional network on the IMDB sentiment classification task.\n \n Gets to 0.8498 test accuracy after 2 epochs. 41 s/epoch on K520 GPU.\n '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a7fdfdf2d1b1d6ff6d4c5495f8223442f65e8447", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 242 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,13 +1,18 @@\n-'''This example demonstrates the use of fasttext for text classification\n+'''\n+#This example demonstrates the use of fasttext for text classification\n \n Based on Joulin et al's paper:\n \n-Bags of Tricks for Efficient Text Classification\n-https://arxiv.org/abs/1607.01759\n+[Bags of Tricks for Efficient Text Classification\n+](https://arxiv.org/abs/1607.01759)\n \n Results on IMDB datasets with uni and bi-gram embeddings:\n-    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n-    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n+\n+Embedding|Accuracy, 5 epochs|Speed (s/epoch)|Hardware\n+:--------|-----------------:|----:|:-------\n+Uni-gram |            0.8813|    8|i7 CPU\n+Bi-gram  |            0.9056|    2|GTx 980M GPU\n+\n '''\n \n from __future__ import print_function\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#304b3958cf0787b16f4d23e7e45b26c6603c7fc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 314 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,10 @@\n-'''Trains an LSTM model on the IMDB sentiment classification task.\n+'''\n+#Trains an LSTM model on the IMDB sentiment classification task.\n \n The dataset is actually too small for LSTM to be of any advantage\n compared to simpler, much faster methods such as TF-IDF + LogReg.\n \n-# Notes\n+**Notes**\n \n - RNNs are tricky. Choice of batch size is important,\n choice of loss and optimizer is critical, etc.\n@@ -11,6 +12,7 @@ Some configurations won't converge.\n \n - LSTM loss decrease patterns during training can be quite different\n from what you see with CNNs/MLPs/etc.\n+\n '''\n from __future__ import print_function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#979e880c53f44d502cfc3648740bcc1300d6c4f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 468 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-'''Sequence to sequence example in Keras (character-level).\n+'''\n+#Sequence to sequence example in Keras (character-level).\n \n This script demonstrates how to implement a basic character-level\n sequence-to-sequence model. We apply it to translating\n@@ -7,7 +8,7 @@ character-by-character. Note that it is fairly unusual to\n do character-level machine translation, as word-level\n models are more common in this domain.\n \n-# Summary of the algorithm\n+**Summary of the algorithm**\n \n - We start with input sequences from a domain (e.g. English sentences)\n     and corresponding target sequences from another domain\n@@ -32,21 +33,21 @@ models are more common in this domain.\n     - Repeat until we generate the end-of-sequence character or we\n         hit the character limit.\n \n-# Data download\n+**Data download**\n \n-English to French sentence pairs.\n-http://www.manythings.org/anki/fra-eng.zip\n+[English to French sentence pairs.\n+](http://www.manythings.org/anki/fra-eng.zip)\n \n-Lots of neat sentence pairs datasets can be found at:\n-http://www.manythings.org/anki/\n+[Lots of neat sentence pairs datasets.\n+](http://www.manythings.org/anki/)\n \n-# References\n+**References**\n \n-- Sequence to Sequence Learning with Neural Networks\n-    https://arxiv.org/abs/1409.3215\n-- Learning Phrase Representations using\n+- [Sequence to Sequence Learning with Neural Networks\n+   ](https://arxiv.org/abs/1409.3215)\n+- [Learning Phrase Representations using\n     RNN Encoder-Decoder for Statistical Machine Translation\n-    https://arxiv.org/abs/1406.1078\n+    ](https://arxiv.org/abs/1406.1078)\n '''\n from __future__ import print_function\n \n\n@@ -1,12 +1,13 @@\n-'''Restore a character-level sequence to sequence model from disk and use it\n-to generate predictions.\n+'''\n+#Restore a character-level sequence to sequence model from to generate predictions.\n \n-This script loads the s2s.h5 model saved by lstm_seq2seq.py and generates\n-sequences from it.  It assumes that no changes have been made (for example:\n-latent_dim is unchanged, and the input data and model architecture are unchanged).\n+This script loads the ```s2s.h5``` model saved by [lstm_seq2seq.py\n+](/examples/lstm_seq2seq/) and generates sequences from it. It assumes\n+that no changes have been made (for example: ```latent_dim``` is unchanged,\n+and the input data and model architecture are unchanged).\n \n-See lstm_seq2seq.py for more details on the model architecture and how\n-it is trained.\n+See [lstm_seq2seq.py](/examples/lstm_seq2seq/) for more details on the\n+model architecture and how it is trained.\n '''\n from __future__ import print_function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02642368427964aae6b28a90da754b2ed0703f8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 14 | Churn Cumulative: 3818 | Contributors (this commit): 31 | Commits (past 90d): 4 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -181,7 +181,7 @@ def variable(value, dtype=None, name=None, constraint=None):\n         value = value.astype(dtype)\n \n     # TODO: remove the conversion when cntk supports int32, int64\n-    # https://docs.microsoft.com/en-us/python/api/cntk.variables.parameter\n+    # https://www.cntk.ai/pythondocs/cntk.variables.html#cntk.variables.Parameter\n     dtype = 'float32' if 'int' in str(dtype) else dtype\n \n     v = C.parameter(shape=shape,\n@@ -386,7 +386,7 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e7)\n     if dtype is None:\n-        dtype = np.float32\n+        dtype = floatx()\n     else:\n         dtype = _convert_string_dtype(dtype)\n \n@@ -420,14 +420,12 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n \n def random_uniform_variable(shape, low, high,\n                             dtype=None, name=None, seed=None):\n-    if dtype is None:\n-        dtype = floatx()\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e3)\n \n     if dtype is None:\n-        dtype = np.float32\n+        dtype = floatx()\n     else:\n         dtype = _convert_string_dtype(dtype)\n \n@@ -452,13 +450,11 @@ def random_normal_variable(\n         dtype=None,\n         name=None,\n         seed=None):\n-    if dtype is None:\n-        dtype = floatx()\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e7)\n     if dtype is None:\n-        dtype = np.float32\n+        dtype = floatx()\n     else:\n         dtype = _convert_string_dtype(dtype)\n \n@@ -497,7 +493,7 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n     if dtype is None:\n-        dtype = np.float32\n+        dtype = floatx()\n     else:\n         dtype = _convert_string_dtype(dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71aadf5e8b4020d4c9423ee5a5f42271abd860e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 59 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -43,9 +43,9 @@ def contrastive_loss(y_true, y_pred):\n     http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n     '''\n     margin = 1\n-    sqaure_pred = K.square(y_pred)\n+    square_pred = K.square(y_pred)\n     margin_square = K.square(K.maximum(margin - y_pred, 0))\n-    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n+    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n \n \n def create_pairs(x, digit_indices):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#780b74df5fee729f7dc4781e991653f9b5f862b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 10354 | Contributors (this commit): 52 | Commits (past 90d): 15 | Contributors (cumulative): 69 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2731,7 +2731,8 @@ class LambdaFunc(C.ops.functions.UserFunction):\n \n \n def reset_uids():\n-    raise NotImplementedError\n+    global _UID_PREFIXES\n+    _UID_PREFIXES = defaultdict(int)\n \n \n def to_dense(tensor):\n\n@@ -553,6 +553,13 @@ class TestBackend(object):\n         check_two_tensor_operation('maximum', (4, 2), (4, 2), WITH_NP)\n         check_two_tensor_operation('minimum', (4, 2), (4, 2), WITH_NP)\n \n+    # assumes first uid will always be the same\n+    def test_reset_uids(self):\n+        first = K.get_uid()\n+        K.get_uid()\n+        K.reset_uids()\n+        assert K.get_uid() == first\n+\n     @pytest.mark.skipif(K.backend() == 'cntk', reason='cntk does not support '\n                                                       'cumsum and cumprod yet')\n     def test_cumsum_cumprod(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb8e377ca10432d95c142651a355ce440be80557", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 18 | Churn Cumulative: 2451 | Contributors (this commit): 33 | Commits (past 90d): 15 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -92,7 +92,6 @@ def clean_module_name(name):\n         name = name.replace('keras_applications', 'keras.applications')\n     if name.startswith('keras_preprocessing'):\n         name = name.replace('keras_preprocessing', 'keras.preprocessing')\n-    assert name[:6] == 'keras.', 'Invalid module name: %s' % name\n     return name\n \n \n\n@@ -395,5 +395,22 @@ def test_docs_in_custom_destination_dir(tmpdir):\n     assert os.listdir(os.path.join(tmpdir, 'examples'))\n \n \n+def test_module_name():\n+    for page in autogen.PAGES:\n+        list_of_classes = autogen.read_page_data(page, 'classes')\n+        for element in list_of_classes:\n+            if isinstance(element, (list, tuple)):\n+                cls = element[0]\n+            else:\n+                cls = element\n+            signature = autogen.get_class_signature(cls)\n+            assert signature.startswith('keras.')\n+\n+        list_of_functions = autogen.read_page_data(page, 'functions')\n+        for function_ in list_of_functions:\n+            signature = autogen.get_function_signature(function_)\n+            assert signature.startswith('keras.')\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7dd1c00e858ddbb1107aee1dcb6f65fff84c252d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 47 | Churn Cumulative: 47 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,32 @@\n+import pytest\n+import pyux\n+import keras\n+import json\n+import os\n+\n+import keras.backend.tensorflow_backend\n+import keras.backend.theano_backend\n+import keras.backend.cntk_backend\n+import keras.backend.numpy_backend\n+import keras.utils.test_utils\n+\n+\n+def test_api():\n+    api_file = os.path.join(os.getcwd(), 'api.json')\n+    with open(api_file, 'r') as f:\n+        previous_api = json.load(f)\n+    current_api = pyux.sign(keras)\n+    diff = pyux.diff(current_api, previous_api)\n+\n+    exceptions = [\n+        pyux.ADDED_ARG_WITH_DEFAULT_IN_METHOD,\n+        pyux.ADDED_DEFAULT_IN_METHOD\n+    ]\n+\n+    diff = list(filter(lambda c: c[0] not in exceptions, diff))\n+    if diff:\n+        raise pyux.APIChangedException(diff)\n+\n+\n+if __name__ == '__main__':\n+    test_api()\n\n@@ -0,0 +1,15 @@\n+import pyux\n+import keras\n+import json\n+\n+\n+import keras.backend.tensorflow_backend\n+import keras.backend.theano_backend\n+import keras.backend.cntk_backend\n+import keras.backend.numpy_backend\n+import keras.utils.test_utils\n+\n+sign = pyux.sign(keras)\n+\n+with open('api.json', 'w') as f:\n+    json.dump(sign, f)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91ccb284abcd83a091dc5f2549f6b24df849a2b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 121 | Churn Cumulative: 1770 | Contributors (this commit): 26 | Commits (past 90d): 11 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,9 +30,9 @@ def load_data(path='boston_housing.npz', test_split=0.2, seed=113):\n         x = f['x']\n         y = f['y']\n \n-    np.random.seed(seed)\n+    rng = np.random.RandomState(seed)\n     indices = np.arange(len(x))\n-    np.random.shuffle(indices)\n+    rng.shuffle(indices)\n     x = x[indices]\n     y = y[indices]\n \n\n@@ -59,14 +59,14 @@ def load_data(path='imdb.npz', num_words=None, skip_top=0,\n         x_train, labels_train = f['x_train'], f['y_train']\n         x_test, labels_test = f['x_test'], f['y_test']\n \n-    np.random.seed(seed)\n+    rng = np.random.RandomState(seed)\n     indices = np.arange(len(x_train))\n-    np.random.shuffle(indices)\n+    rng.shuffle(indices)\n     x_train = x_train[indices]\n     labels_train = labels_train[indices]\n \n     indices = np.arange(len(x_test))\n-    np.random.shuffle(indices)\n+    rng.shuffle(indices)\n     x_test = x_test[indices]\n     labels_test = labels_test[indices]\n \n\n@@ -56,9 +56,9 @@ def load_data(path='reuters.npz', num_words=None, skip_top=0,\n     with np.load(path) as f:\n         xs, labels = f['x'], f['y']\n \n-    np.random.seed(seed)\n+    rng = np.random.RandomState(seed)\n     indices = np.arange(len(xs))\n-    np.random.shuffle(indices)\n+    rng.shuffle(indices)\n     xs = xs[indices]\n     labels = labels[indices]\n \n\n@@ -248,9 +248,10 @@ class Orthogonal(Initializer):\n             num_rows *= dim\n         num_cols = shape[-1]\n         flat_shape = (num_rows, num_cols)\n+        rng = np.random\n         if self.seed is not None:\n-            np.random.seed(self.seed)\n-        a = np.random.normal(0.0, 1.0, flat_shape)\n+            rng = np.random.RandomState(self.seed)\n+        a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)\n         # Pick the one with the correct shape.\n         q = u if u.shape == flat_shape else v\n\n@@ -0,0 +1,90 @@\n+import tempfile\n+\n+import numpy as np\n+import pytest\n+\n+from keras.datasets import boston_housing\n+from keras.datasets import imdb\n+from keras.datasets import reuters\n+\n+\n+@pytest.fixture\n+def fake_downloaded_boston_path(monkeypatch):\n+    num_rows = 100\n+    num_cols = 10\n+    rng = np.random.RandomState(123)\n+\n+    x = rng.randint(1, 100, size=(num_rows, num_cols))\n+    y = rng.normal(loc=100, scale=15, size=num_rows)\n+\n+    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n+        np.savez(f, x=x, y=y)\n+        monkeypatch.setattr(boston_housing, 'get_file',\n+                            lambda *args, **kwargs: f.name)\n+        yield f.name\n+\n+\n+@pytest.fixture\n+def fake_downloaded_imdb_path(monkeypatch):\n+    train_rows = 100\n+    test_rows = 20\n+    seq_length = 10\n+    rng = np.random.RandomState(123)\n+\n+    x_train = rng.randint(1, 100, size=(train_rows, seq_length))\n+    y_train = rng.binomial(n=1, p=0.5, size=train_rows)\n+    x_test = rng.randint(1, 100, size=(test_rows, seq_length))\n+    y_test = rng.binomial(n=1, p=0.5, size=test_rows)\n+\n+    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n+        np.savez(f, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n+        monkeypatch.setattr(imdb, 'get_file', lambda *args, **kwargs: f.name)\n+        yield f.name\n+\n+\n+@pytest.fixture\n+def fake_downloaded_reuters_path(monkeypatch):\n+    num_rows = 100\n+    seq_length = 10\n+    rng = np.random.RandomState(123)\n+\n+    x = rng.randint(1, 100, size=(num_rows, seq_length))\n+    y = rng.binomial(n=1, p=0.5, size=num_rows)\n+\n+    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n+        np.savez(f, x=x, y=y)\n+        monkeypatch.setattr(reuters, 'get_file', lambda *args, **kwargs: f.name)\n+        yield f.name\n+\n+\n+def test_boston_load_does_not_affect_global_rng(fake_downloaded_boston_path):\n+    np.random.seed(1337)\n+    before = np.random.randint(0, 100, size=10)\n+\n+    np.random.seed(1337)\n+    boston_housing.load_data(path=fake_downloaded_boston_path, seed=9876)\n+    after = np.random.randint(0, 100, size=10)\n+\n+    assert np.array_equal(before, after)\n+\n+\n+def test_imdb_load_does_not_affect_global_rng(fake_downloaded_imdb_path):\n+    np.random.seed(1337)\n+    before = np.random.randint(0, 100, size=10)\n+\n+    np.random.seed(1337)\n+    imdb.load_data(path=fake_downloaded_imdb_path, seed=9876)\n+    after = np.random.randint(0, 100, size=10)\n+\n+    assert np.array_equal(before, after)\n+\n+\n+def test_reuters_load_does_not_affect_global_rng(fake_downloaded_reuters_path):\n+    np.random.seed(1337)\n+    before = np.random.randint(0, 100, size=10)\n+\n+    np.random.seed(1337)\n+    reuters.load_data(path=fake_downloaded_reuters_path, seed=9876)\n+    after = np.random.randint(0, 100, size=10)\n+\n+    assert np.array_equal(before, after)\n\n@@ -104,6 +104,18 @@ def test_orthogonal(tensor_shape):\n             target_mean=0.)\n \n \n+def test_orthogonal_init_does_not_affect_global_rng():\n+    np.random.seed(1337)\n+    before = np.random.randint(0, 100, size=10)\n+\n+    np.random.seed(1337)\n+    init = initializers.orthogonal(seed=9876)\n+    init(shape=(10, 5))\n+    after = np.random.randint(0, 100, size=10)\n+\n+    assert np.array_equal(before, after)\n+\n+\n @pytest.mark.parametrize('tensor_shape',\n                          [(100, 100), (10, 20), (30, 80), (1, 2, 3, 4)],\n                          ids=['FC', 'RNN', 'RNN_INVALID', 'CONV'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c92fa425e980f2a2a83bee8bfa5b5e4113066e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 12 | Churn Cumulative: 240 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,15 +18,16 @@ input_num_col = 5\n sequence_len = 2\n \n \n-def test_convolutional_recurrent():\n+@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\n+@pytest.mark.parametrize('return_sequences', [True, False])\n+@pytest.mark.parametrize('use_mask', [True, False])\n+def test_convolutional_recurrent(data_format, return_sequences, use_mask):\n \n     class Masking5D(Masking):\n         \"\"\"Regular masking layer returns wrong shape of mask for RNN\"\"\"\n         def compute_mask(self, inputs, mask=None):\n             return K.any(K.not_equal(inputs, 0.), axis=[2, 3, 4])\n \n-    for data_format in ['channels_first', 'channels_last']:\n-\n     if data_format == 'channels_first':\n         inputs = np.random.rand(num_samples, sequence_len,\n                                 input_channel,\n@@ -36,8 +37,6 @@ def test_convolutional_recurrent():\n                                 input_num_row, input_num_col,\n                                 input_channel)\n \n-        for use_mask in [False, True]:\n-            for return_sequences in [True, False]:\n     # test for return state:\n     x = Input(batch_shape=inputs.shape)\n     kwargs = {'data_format': data_format,\n@@ -57,8 +56,7 @@ def test_convolutional_recurrent():\n     assert len(states) == 2\n     model = Model(x, states[0])\n     state = model.predict(inputs)\n-                np.testing.assert_allclose(\n-                    K.eval(layer.states[0]), state, atol=1e-4)\n+    np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n \n     # test for output shape:\n     output = layer_test(convolutional_recurrent.ConvLSTM2D,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63320f807788e16a9a6310180714692ed5c113d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 14 | Files Changed: 5 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 67 | Churn Cumulative: 27914 | Contributors (this commit): 165 | Commits (past 90d): 39 | Contributors (cumulative): 294 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2759,7 +2759,8 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     raise NotImplementedError\n \n \n-def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n+def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,\n+               merge_repeated=False):\n     raise NotImplementedError\n \n \n\n@@ -707,7 +707,8 @@ def one_hot(indices, num_classes):\n     return to_categorical(indices, num_classes)\n \n \n-def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n+def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,\n+               merge_repeated=False):\n     num_samples = y_pred.shape[0]\n     num_classes = y_pred.shape[-1]\n     log_prob = np.zeros((num_samples, 1))\n\n@@ -4502,7 +4502,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n \n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n-               top_paths=1):\n+               top_paths=1, merge_repeated=False):\n     \"\"\"Decodes the output of a softmax.\n \n     Can use either greedy search (also known as best path)\n@@ -4513,18 +4513,20 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n             containing the prediction, or output of the softmax.\n         input_length: tensor `(samples, )` containing the sequence length for\n             each batch item in `y_pred`.\n-        greedy: perform much faster best-path search if `true`.\n+        greedy: perform much faster best-path search if `True`.\n             This does not use a dictionary.\n-        beam_width: if `greedy` is `false`: a beam search decoder will be used\n+        beam_width: if `greedy` is `False`: a beam search decoder will be used\n             with a beam of this width.\n-        top_paths: if `greedy` is `false`,\n+        top_paths: if `greedy` is `False`,\n             how many of the most probable paths will be returned.\n+        merge_repeated: if `greedy` is `False`,\n+            merge repeated classes in the output beams.\n \n     # Returns\n         Tuple:\n-            List: if `greedy` is `true`, returns a list of one element that\n+            List: if `greedy` is `True`, returns a list of one element that\n                 contains the decoded sequence.\n-                If `false`, returns the `top_paths` most probable\n+                If `False`, returns the `top_paths` most probable\n                 decoded sequences.\n                 Important: blank labels are returned as `-1`.\n             Tensor `(top_paths, )` that contains\n@@ -4541,14 +4543,11 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n         (decoded, log_prob) = ctc.ctc_beam_search_decoder(\n             inputs=y_pred,\n             sequence_length=input_length, beam_width=beam_width,\n-            top_paths=top_paths, merge_repeated=False)\n+            top_paths=top_paths, merge_repeated=merge_repeated)\n \n     decoded_dense = []\n     for st in decoded:\n-        dense_tensor = tf.sparse_to_dense(st.indices,\n-                                          st.dense_shape,\n-                                          st.values,\n-                                          default_value=-1)\n+        dense_tensor = tf.sparse.to_dense(st, default_value=-1)\n         decoded_dense.append(dense_tensor)\n     return (decoded_dense, log_prob)\n \n\n@@ -2944,5 +2944,6 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     raise NotImplementedError\n \n \n-def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n+def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,\n+               merge_repeated=False):\n     raise NotImplementedError\n\n@@ -1851,6 +1851,43 @@ class TestBackend(object):\n \n         assert np.allclose(log_prob_truth, log_prob_pred)\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Beam search is only implemented with '\n+                               'the TensorFlow backend.')\n+    def test_ctc_decode_beam_search_no_merge(self):\n+        # A simple CTC probability map with some repeating characters,\n+        # shape(batch, input_width, char_count)\n+        # Without merging should be decoded as: \"AABB\", with merging as: \"AB\".\n+        input_prob = np.array([\n+            [  # blank, A ,B\n+                [0, 0, 1],  # blank\n+                [1, 0, 0],  # A\n+                [0, 0, 1],  # blank\n+                [1, 0, 0],  # A\n+                [0, 1, 0],  # B\n+                [0, 0, 1],  # blank\n+                [0, 1, 0]  # B\n+            ]\n+        ])\n+        input_len = np.array(input_prob.shape[0] * [input_prob.shape[1]])\n+\n+        def decode(merge_repeated):\n+            input_prob_tensor = K.placeholder(shape=(None, None, None),\n+                                              dtype='float32')\n+            input_len_tensor = K.placeholder(shape=(None), dtype='int64')\n+            paths_tensors, _ = K.ctc_decode(input_prob_tensor, input_len_tensor,\n+                                            greedy=False, beam_width=1, top_paths=1,\n+                                            merge_repeated=merge_repeated)\n+            decode_func = K.function([input_prob_tensor, input_len_tensor],\n+                                     paths_tensors)\n+            paths = decode_func([input_prob, input_len])\n+            return paths\n+\n+        # merged: A B\n+        assert np.allclose(decode(merge_repeated=True), [np.array([[0, 1]])])\n+        # not merged: A A B B\n+        assert np.allclose(decode(merge_repeated=False), [np.array([[0, 0, 1, 1]])])\n+\n     def test_one_hot(self):\n         input_length = 10\n         num_classes = 20\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f69946854775d5c755983d9f585bd20cc97acae8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 3830 | Contributors (this commit): 33 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -527,11 +527,13 @@ def eye(size, dtype=None, name=None):\n \n \n def zeros_like(x, dtype=None, name=None):\n-    return x * 0\n+    name = name or ''\n+    return C.zeros_like(x, name)\n \n \n def ones_like(x, dtype=None, name=None):\n-    return zeros_like(x) + 1\n+    name = name or ''\n+    return C.ones_like(x, name)\n \n \n def count_params(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d48e97079914d897e82ddcb1a45261ce4415b8ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 293 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,38 +1,40 @@\n-'''Example script showing how to use a stateful LSTM model\n-and how its stateless counterpart performs.\n+'''\n+#How to use a stateful LSTM model, stateful vs stateless LSTM performance comparison\n \n-More documentation about the Keras LSTM model can be found at\n-https://keras.io/layers/recurrent/#lstm\n+[More documentation about the Keras LSTM model](/layers/recurrent/#lstm)\n \n The models are trained on an input/output pair, where\n the input is a generated uniformly distributed\n-random sequence of length = \"input_len\",\n-and the output is a moving average of the input with window length = \"tsteps\".\n-Both \"input_len\" and \"tsteps\" are defined in the \"editable parameters\" section.\n+random sequence of length = `input_len`,\n+and the output is a moving average of the input with window length = `tsteps`.\n+Both `input_len` and `tsteps` are defined in the \"editable parameters\"\n+section.\n \n-A larger \"tsteps\" value means that the LSTM will need more memory\n+A larger `tsteps` value means that the LSTM will need more memory\n to figure out the input-output relationship.\n-This memory length is controlled by the \"lahead\" variable (more details below).\n+This memory length is controlled by the `lahead` variable (more details below).\n \n The rest of the parameters are:\n-- input_len: the length of the generated input sequence\n-- lahead: the input sequence length that the LSTM\n+\n+- `input_len`: the length of the generated input sequence\n+- `lahead`: the input sequence length that the LSTM\n   is trained on for each output point\n-- batch_size, epochs: same parameters as in the model.fit(...) function\n+- `batch_size`, `epochs`: same parameters as in the `model.fit(...)`\n+  function\n \n-When lahead > 1, the model input is preprocessed to a \"rolling window view\"\n-of the data, with the window length = \"lahead\".\n-This is similar to sklearn's \"view_as_windows\"\n-with \"window_shape\" being a single number\n-Ref: http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows\n+When `lahead > 1`, the model input is preprocessed to a \"rolling window view\"\n+of the data, with the window length = `lahead`.\n+This is similar to sklearn's `view_as_windows`\n+with `window_shape` [being a single number.](\n+http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows)\n \n-When lahead < tsteps, only the stateful LSTM converges because its\n+When `lahead < tsteps`, only the stateful LSTM converges because its\n statefulness allows it to see beyond the capability that lahead\n gave it to fit the n-point average. The stateless LSTM does not have\n-this capability, and hence is limited by its \"lahead\" parameter,\n+this capability, and hence is limited by its `lahead` parameter,\n which is not sufficient to see the n-point average.\n \n-When lahead >= tsteps, both the stateful and stateless LSTM converge.\n+When `lahead >= tsteps`, both the stateful and stateless LSTM converge.\n '''\n from __future__ import print_function\n import numpy as np\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#050e8cb773e28924eed6e38903affa0738c17dc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 527 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -41,8 +41,9 @@ def set_epsilon(e):\n         1e-05\n     ```\n     \"\"\"\n+\n     global _EPSILON\n-    _EPSILON = e\n+    _EPSILON = float(e)\n \n \n def floatx():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2944988be3ce0398c1ebbcce3f2b57a6d8be05df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 17453 | Contributors (this commit): 136 | Commits (past 90d): 23 | Contributors (cumulative): 166 | DMM Complexity: 0.875\n\nDIFF:\n@@ -186,6 +186,7 @@ def get_session():\n             else:\n                 num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n+                                        inter_op_parallelism_threads=num_thread,\n                                         allow_soft_placement=True)\n             _SESSION = tf.Session(config=config)\n         session = _SESSION\n\n@@ -2118,6 +2118,15 @@ class TestBackend(object):\n         assert np.allclose(K.eval(K.clip(x, min_value, max_value)),\n                            np.asarray([-5., -4., 0., 4., 9.], dtype=np.float32))\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='This test is for tensorflow parallelism.')\n+    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n+        for threads in [0, 1, 4]:\n+            K.clear_session()\n+            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n+            cfg = K.get_session()._config\n+            assert cfg.intra_op_parallelism_threads == threads\n+            assert cfg.inter_op_parallelism_threads == threads\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de36e0426d4f01216653979af4beb0481a1b2735", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 10889 | Contributors (this commit): 129 | Commits (past 90d): 12 | Contributors (cumulative): 129 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4469,8 +4469,8 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n \n     vals_sparse = tf.gather_nd(labels, indices)\n \n-    indices = tf.to_int64(indices)\n-    label_shape = tf.to_int64(label_shape)\n+    indices = tf.cast(indices, tf.int64)\n+    label_shape = tf.cast(label_shape, tf.int64)\n     return tf.SparseTensor(indices, vals_sparse, label_shape)\n \n \n@@ -4491,12 +4491,11 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n     \"\"\"\n-    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n-    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n-    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n-\n+    label_length = tf.cast(tf.squeeze(label_length, axis=-1), tf.int32)\n+    input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)\n+    sparse_labels = tf.cast(\n+        ctc_label_dense_to_sparse(y_true, label_length), tf.int32)\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n-\n     return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                        labels=sparse_labels,\n                                        sequence_length=input_length), 1)\n@@ -4534,7 +4533,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                 the log probability of each decoded sequence.\n     \"\"\"\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n-    input_length = tf.to_int32(input_length)\n+    input_length = tf.cast(input_length, tf.int32)\n \n     if greedy:\n         (decoded, log_prob) = ctc.ctc_greedy_decoder(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
