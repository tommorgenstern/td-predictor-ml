{"custom_id": "scrapy#812fd2368f705d033f5f39c152130b12a0fe9b1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 16/11 | Churn Δ: 134 | Churn Cumulative: 1779 | Contributors (this commit): 21 | Commits (past 90d): 11 | Contributors (cumulative): 32 | DMM Complexity: 0.7263157894736842\n\nDIFF:\n@@ -33,6 +33,7 @@ logger = logging.getLogger(__name__)\n \n \n _split_domain = TLDExtract(include_psl_private_domains=True)\n+_UNSET = object()\n \n \n def _is_public_domain(domain: str) -> bool:\n@@ -133,6 +134,7 @@ class CookiesMiddleware:\n         Decode from bytes if necessary.\n         \"\"\"\n         decoded = {}\n+        flags = set()\n         for key in (\"name\", \"value\", \"path\", \"domain\"):\n             if cookie.get(key) is None:\n                 if key in (\"name\", \"value\"):\n@@ -152,10 +154,16 @@ class CookiesMiddleware:\n                         cookie,\n                     )\n                     decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n-\n+        for flag in (\"secure\",):\n+            value = cookie.get(flag, _UNSET)\n+            if value is _UNSET or not value:\n+                continue\n+            flags.add(flag)\n         cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n         for key, value in decoded.items():  # path, domain\n             cookie_str += f\"; {key.capitalize()}={value}\"\n+        for flag in flags:  # secure\n+            cookie_str += f\"; {flag.capitalize()}\"\n         return cookie_str\n \n     def _get_request_cookies(\n@@ -168,9 +176,11 @@ class CookiesMiddleware:\n             return []\n         cookies: Iterable[Dict[str, Any]]\n         if isinstance(request.cookies, dict):\n-            cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n+            cookies = tuple({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n         else:\n             cookies = request.cookies\n+        for cookie in cookies:\n+            cookie.setdefault(\"secure\", urlparse_cached(request).scheme == \"https\")\n         formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n         response = Response(request.url, headers={\"Set-Cookie\": formatted})\n         return jar.make_cookies(response, request)\n\n@@ -20,11 +20,10 @@ else:\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n                 \"Scrapy support for brotlipy is deprecated and will stop \"\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n-                \"deprecated, it has been superseded by brotlicffi. \"\n-                \"Please, uninstall brotlipy \"\n-                \"and install brotli or brotlicffi instead. brotlipy has the same import \"\n-                \"name as brotli, so keeping both installed is strongly \"\n-                \"discouraged.\"\n+                \"deprecated, it has been superseded by brotlicffi. Please, \"\n+                \"uninstall brotlipy and install brotli or brotlicffi instead. \"\n+                \"brotlipy has the same import name as brotli, so keeping both \"\n+                \"installed is strongly discouraged.\"\n             ),\n             ScrapyDeprecationWarning,\n         )\n\n@@ -14,6 +14,8 @@ from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n \n+UNSET = object()\n+\n \n def _cookie_to_set_cookie_value(cookie):\n     \"\"\"Given a cookie defined as a dictionary with name and value keys, and\n@@ -414,19 +416,19 @@ class CookiesMiddlewareTest(TestCase):\n                     \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/1>:\"\n-                    \" {'value': 'bar'} ('name' is missing)\",\n+                    \" {'value': 'bar', 'secure': False} ('name' is missing)\",\n                 ),\n                 (\n                     \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/2>:\"\n-                    \" {'name': 'foo'} ('value' is missing)\",\n+                    \" {'name': 'foo', 'secure': False} ('value' is missing)\",\n                 ),\n                 (\n                     \"scrapy.downloadermiddlewares.cookies\",\n                     \"WARNING\",\n                     \"Invalid cookie found in request <GET http://example.org/3>:\"\n-                    \" {'name': 'foo', 'value': None} ('value' is missing)\",\n+                    \" {'name': 'foo', 'value': None, 'secure': False} ('value' is missing)\",\n                 ),\n             )\n         self.assertCookieValEqual(req1.headers[\"Cookie\"], \"key=value1\")\n@@ -732,3 +734,106 @@ class CookiesMiddlewareTest(TestCase):\n             \"co.uk\",\n             cookies=True,\n         )\n+\n+    def _test_cookie_redirect_scheme_change(\n+        self, secure, from_scheme, to_scheme, cookies1, cookies2, cookies3\n+    ):\n+        \"\"\"When a redirect causes the URL scheme to change from *from_scheme*\n+        to *to_scheme*, while domain and port remain the same, and given a\n+        cookie on the initial request with its secure attribute set to\n+        *secure*, check if the cookie should be set on the Cookie header of the\n+        initial request (*cookies1*), if it should be kept by the redirect\n+        middleware (*cookies2*), and if it should be present on the Cookie\n+        header in the redirected request (*cookie3*).\"\"\"\n+        cookie_kwargs = {}\n+        if secure is not UNSET:\n+            cookie_kwargs[\"secure\"] = secure\n+        input_cookies = [{\"name\": \"a\", \"value\": \"b\", **cookie_kwargs}]\n+\n+        request1 = Request(f\"{from_scheme}://a.example\", cookies=input_cookies)\n+        self.mw.process_request(request1, self.spider)\n+        cookies = request1.headers.get(\"Cookie\")\n+        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n+\n+        response = Response(\n+            f\"{from_scheme}://a.example\",\n+            headers={\"Location\": f\"{to_scheme}://a.example\"},\n+            status=301,\n+        )\n+        self.assertEqual(\n+            self.mw.process_response(request1, response, self.spider),\n+            response,\n+        )\n+\n+        request2 = self.redirect_middleware.process_response(\n+            request1,\n+            response,\n+            self.spider,\n+        )\n+        self.assertIsInstance(request2, Request)\n+        cookies = request2.headers.get(\"Cookie\")\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+        self.mw.process_request(request2, self.spider)\n+        cookies = request2.headers.get(\"Cookie\")\n+        self.assertEqual(cookies, b\"a=b\" if cookies3 else None)\n+\n+    def test_cookie_redirect_secure_undefined_downgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=UNSET,\n+            from_scheme=\"https\",\n+            to_scheme=\"http\",\n+            cookies1=True,\n+            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies3=False,\n+        )\n+\n+    def test_cookie_redirect_secure_undefined_upgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=UNSET,\n+            from_scheme=\"http\",\n+            to_scheme=\"https\",\n+            cookies1=True,\n+            cookies2=True,\n+            cookies3=True,\n+        )\n+\n+    def test_cookie_redirect_secure_false_downgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=False,\n+            from_scheme=\"https\",\n+            to_scheme=\"http\",\n+            cookies1=True,\n+            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies3=True,\n+        )\n+\n+    def test_cookie_redirect_secure_false_upgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=False,\n+            from_scheme=\"http\",\n+            to_scheme=\"https\",\n+            cookies1=True,\n+            cookies2=True,\n+            cookies3=True,\n+        )\n+\n+    def test_cookie_redirect_secure_true_downgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=True,\n+            from_scheme=\"https\",\n+            to_scheme=\"http\",\n+            cookies1=True,\n+            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies3=False,\n+        )\n+\n+    def test_cookie_redirect_secure_true_upgrade(self):\n+        self._test_cookie_redirect_scheme_change(\n+            secure=True,\n+            from_scheme=\"http\",\n+            to_scheme=\"https\",\n+            cookies1=False,\n+            cookies2=False,\n+            cookies3=True,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#631fc65fadb874629787ae5f7fdd876b9ec96a29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1292 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -784,7 +784,7 @@ class CookiesMiddlewareTest(TestCase):\n             from_scheme=\"https\",\n             to_scheme=\"http\",\n             cookies1=True,\n-            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies2=False,\n             cookies3=False,\n         )\n \n@@ -804,7 +804,7 @@ class CookiesMiddlewareTest(TestCase):\n             from_scheme=\"https\",\n             to_scheme=\"http\",\n             cookies1=True,\n-            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies2=False,\n             cookies3=True,\n         )\n \n@@ -824,7 +824,7 @@ class CookiesMiddlewareTest(TestCase):\n             from_scheme=\"https\",\n             to_scheme=\"http\",\n             cookies1=True,\n-            cookies2=True,  # xfail, due to a bug in the redirect middleware fixed elsewhere\n+            cookies2=False,\n             cookies3=False,\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b99526b740890e63f1d05074b3e358a9ae59b77f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 34 | Files Changed: 2 | Hunks: 36 | Methods Changed: 37 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 94 | Churn Cumulative: 959 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -3,10 +3,23 @@ import sys\n from functools import wraps\n from inspect import getmembers\n from types import CoroutineType\n-from typing import AsyncGenerator, Dict, Optional, Type\n-from unittest import TestCase\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+)\n+from unittest import TestCase, TestResult\n \n-from scrapy.http import Request\n+from twisted.python.failure import Failure\n+\n+from scrapy import Spider\n+from scrapy.http import Request, Response\n from scrapy.utils.python import get_spec\n from scrapy.utils.spider import iterate_spider_output\n \n@@ -15,18 +28,20 @@ class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n \n     request_cls: Optional[Type[Request]] = None\n+    name: str\n \n-    def __init__(self, method, *args):\n+    def __init__(self, method: Callable, *args: Any):\n         self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n         self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n-        self.args = args\n+        self.args: Tuple[Any, ...] = args\n \n-    def add_pre_hook(self, request, results):\n+    def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n         if hasattr(self, \"pre_process\"):\n             cb = request.callback\n+            assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response, **cb_kwargs):\n+            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -49,12 +64,13 @@ class Contract:\n \n         return request\n \n-    def add_post_hook(self, request, results):\n+    def add_post_hook(self, request: Request, results: TestResult) -> Request:\n         if hasattr(self, \"post_process\"):\n             cb = request.callback\n+            assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response, **cb_kwargs):\n+            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -76,18 +92,18 @@ class Contract:\n \n         return request\n \n-    def adjust_request_args(self, args):\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n         return args\n \n \n class ContractsManager:\n-    contracts: Dict[str, Contract] = {}\n+    contracts: Dict[str, Type[Contract]] = {}\n \n-    def __init__(self, contracts):\n+    def __init__(self, contracts: Iterable[Type[Contract]]):\n         for contract in contracts:\n             self.contracts[contract.name] = contract\n \n-    def tested_methods_from_spidercls(self, spidercls):\n+    def tested_methods_from_spidercls(self, spidercls: Type[Spider]) -> List[str]:\n         is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n         methods = []\n         for key, value in getmembers(spidercls):\n@@ -96,21 +112,26 @@ class ContractsManager:\n \n         return methods\n \n-    def extract_contracts(self, method):\n-        contracts = []\n+    def extract_contracts(self, method: Callable) -> List[Contract]:\n+        contracts: List[Contract] = []\n+        assert method.__doc__ is not None\n         for line in method.__doc__.split(\"\\n\"):\n             line = line.strip()\n \n             if line.startswith(\"@\"):\n-                name, args = re.match(r\"@(\\w+)\\s*(.*)\", line).groups()\n+                m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n+                assert m is not None\n+                name, args = m.groups()\n                 args = re.split(r\"\\s+\", args)\n \n                 contracts.append(self.contracts[name](method, *args))\n \n         return contracts\n \n-    def from_spider(self, spider, results):\n-        requests = []\n+    def from_spider(\n+        self, spider: Spider, results: TestResult\n+    ) -> List[Optional[Request]]:\n+        requests: List[Optional[Request]] = []\n         for method in self.tested_methods_from_spidercls(type(spider)):\n             bound_method = spider.__getattribute__(method)\n             try:\n@@ -121,7 +142,7 @@ class ContractsManager:\n \n         return requests\n \n-    def from_method(self, method, results):\n+    def from_method(self, method: Callable, results: TestResult) -> Optional[Request]:\n         contracts = self.extract_contracts(method)\n         if contracts:\n             request_cls = Request\n@@ -154,14 +175,18 @@ class ContractsManager:\n \n                 self._clean_req(request, method, results)\n                 return request\n+        return None\n \n-    def _clean_req(self, request, method, results):\n+    def _clean_req(\n+        self, request: Request, method: Callable, results: TestResult\n+    ) -> None:\n         \"\"\"stop the request from returning objects and records any errors\"\"\"\n \n         cb = request.callback\n+        assert cb is not None\n \n         @wraps(cb)\n-        def cb_wrapper(response, **cb_kwargs):\n+        def cb_wrapper(response: Response, **cb_kwargs: Any) -> None:\n             try:\n                 output = cb(response, **cb_kwargs)\n                 output = list(iterate_spider_output(output))\n@@ -169,7 +194,7 @@ class ContractsManager:\n                 case = _create_testcase(method, \"callback\")\n                 results.addError(case, sys.exc_info())\n \n-        def eb_wrapper(failure):\n+        def eb_wrapper(failure: Failure) -> None:\n             case = _create_testcase(method, \"errback\")\n             exc_info = failure.type, failure.value, failure.getTracebackObject()\n             results.addError(case, exc_info)\n@@ -178,11 +203,11 @@ class ContractsManager:\n         request.errback = eb_wrapper\n \n \n-def _create_testcase(method, desc):\n-    spider = method.__self__.name\n+def _create_testcase(method: Callable, desc: str) -> TestCase:\n+    spider = method.__self__.name  # type: ignore[attr-defined]\n \n     class ContractTestCase(TestCase):\n-        def __str__(_self):\n+        def __str__(_self) -> str:\n             return f\"[{spider}] {method.__name__} ({desc})\"\n \n     name = f\"{spider}_{method.__name__}\"\n\n@@ -1,4 +1,5 @@\n import json\n+from typing import Any, Callable, Dict, List, Optional\n \n from itemadapter import ItemAdapter, is_item\n \n@@ -15,7 +16,7 @@ class UrlContract(Contract):\n \n     name = \"url\"\n \n-    def adjust_request_args(self, args):\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n         args[\"url\"] = self.args[0]\n         return args\n \n@@ -29,7 +30,7 @@ class CallbackKeywordArgumentsContract(Contract):\n \n     name = \"cb_kwargs\"\n \n-    def adjust_request_args(self, args):\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n         args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n         return args\n \n@@ -48,14 +49,14 @@ class ReturnsContract(Contract):\n     \"\"\"\n \n     name = \"returns\"\n-    object_type_verifiers = {\n+    object_type_verifiers: Dict[Optional[str], Callable[[Any], bool]] = {\n         \"request\": lambda x: isinstance(x, Request),\n         \"requests\": lambda x: isinstance(x, Request),\n         \"item\": is_item,\n         \"items\": is_item,\n     }\n \n-    def __init__(self, *args, **kwargs):\n+    def __init__(self, *args: Any, **kwargs: Any):\n         super().__init__(*args, **kwargs)\n \n         if len(self.args) not in [1, 2, 3]:\n@@ -66,16 +67,16 @@ class ReturnsContract(Contract):\n         self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n \n         try:\n-            self.min_bound = int(self.args[1])\n+            self.min_bound: float = int(self.args[1])\n         except IndexError:\n             self.min_bound = 1\n \n         try:\n-            self.max_bound = int(self.args[2])\n+            self.max_bound: float = int(self.args[2])\n         except IndexError:\n             self.max_bound = float(\"inf\")\n \n-    def post_process(self, output):\n+    def post_process(self, output: List[Any]) -> None:\n         occurrences = 0\n         for x in output:\n             if self.obj_type_verifier(x):\n@@ -85,7 +86,7 @@ class ReturnsContract(Contract):\n \n         if not assertion:\n             if self.min_bound == self.max_bound:\n-                expected = self.min_bound\n+                expected = str(self.min_bound)\n             else:\n                 expected = f\"{self.min_bound}..{self.max_bound}\"\n \n@@ -101,7 +102,7 @@ class ScrapesContract(Contract):\n \n     name = \"scrapes\"\n \n-    def post_process(self, output):\n+    def post_process(self, output: List[Any]) -> None:\n         for x in output:\n             if is_item(x):\n                 missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2facdd4fb08ec3edaf1752047dd86d5b565621a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 41 | Churn Cumulative: 4656 | Contributors (this commit): 38 | Commits (past 90d): 4 | Contributors (cumulative): 56 | DMM Complexity: 0.9\n\nDIFF:\n@@ -129,6 +129,8 @@ class Crawler:\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n+            log_reactor_info()\n+\n         self.extensions = ExtensionManager.from_crawler(self)\n         self.settings.freeze()\n \n\n@@ -0,0 +1,31 @@\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n+\n+\n+from scrapy.utils.reactor import install_reactor\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+runner = CrawlerRunner()\n+\n+d = runner.crawl(NoRequestsSpider)\n+\n+from twisted.internet import reactor\n+\n+d.addBoth(callback=lambda _: reactor.stop())\n+reactor.run()\n\n@@ -926,3 +926,11 @@ class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n         self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n         self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n+\n+    def test_change_default_reactor(self):\n+        log = self.run_script(\"change_reactor.py\")\n+        self.assertIn(\n+            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            log,\n+        )\n+        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ba4dd311dd9d2a5341ee9c0c6d1b50eb44ac406", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2634 | Contributors (this commit): 52 | Commits (past 90d): 7 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -59,7 +59,8 @@ class S3DownloadHandler:\n             assert aws_access_key_id is not None\n             assert aws_secret_access_key is not None\n             SignerCls = botocore.auth.AUTH_TYPE_MAPS[\"s3\"]\n-            self._signer = SignerCls(\n+            # botocore.auth.BaseSigner doesn't have an __init__() with args, only subclasses do\n+            self._signer = SignerCls(  # type: ignore[call-arg]\n                 botocore.credentials.Credentials(\n                     aws_access_key_id, aws_secret_access_key, aws_session_token\n                 )\n\n@@ -238,13 +238,16 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.acl: Optional[str] = acl\n         self.endpoint_url: Optional[str] = endpoint_url\n         self.region_name: Optional[str] = region_name\n+        # It can be either botocore.client.BaseClient or mypy_boto3_s3.S3Client,\n+        # there seems to be no good way to infer it statically.\n+        self.s3_client: Any\n \n         if IS_BOTO3_AVAILABLE:\n             import boto3.session\n \n-            session = boto3.session.Session()\n+            boto3_session = boto3.session.Session()\n \n-            self.s3_client = session.client(\n+            self.s3_client = boto3_session.client(\n                 \"s3\",\n                 aws_access_key_id=self.access_key,\n                 aws_secret_access_key=self.secret_key,\n@@ -261,9 +264,9 @@ class S3FeedStorage(BlockingFeedStorage):\n \n             import botocore.session\n \n-            session = botocore.session.get_session()\n+            botocore_session = botocore.session.get_session()\n \n-            self.s3_client = session.create_client(\n+            self.s3_client = botocore_session.create_client(\n                 \"s3\",\n                 aws_access_key_id=self.access_key,\n                 aws_secret_access_key=self.secret_key,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cadb0dd707fc54670cb0eab06f0af65dcaa99354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 126 | Files Changed: 2 | Hunks: 32 | Methods Changed: 35 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 199 | Churn Cumulative: 1763 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@ from __future__ import annotations\n \n import functools\n import logging\n+from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import TYPE_CHECKING\n \n@@ -27,7 +28,7 @@ def _DUMMY_CALLBACK(response):\n     return response\n \n \n-class MediaPipeline:\n+class MediaPipeline(ABC):\n     LOG_FAILED_RESULTS = True\n \n     class SpiderInfo:\n@@ -55,14 +56,6 @@ class MediaPipeline:\n             self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n     def _key_for_pipe(self, key, base_class_name=None, settings=None):\n-        \"\"\"\n-        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n-        'IMAGES'\n-        >>> class MyPipe(MediaPipeline):\n-        ...     pass\n-        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n-        'MYPIPE_IMAGES'\n-        \"\"\"\n         class_name = self.__class__.__name__\n         formatted_key = f\"{class_name.upper()}_{key}\"\n         if (\n@@ -192,21 +185,25 @@ class MediaPipeline:\n             defer_result(result).chainDeferred(wad)\n \n     # Overridable Interface\n+    @abstractmethod\n     def media_to_download(self, request, info, *, item=None):\n         \"\"\"Check request before starting download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def get_media_requests(self, item, info):\n         \"\"\"Returns the media requests to download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_downloaded(self, response, request, info, *, item=None):\n         \"\"\"Handler for success downloads\"\"\"\n-        return response\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_failed(self, failure, request, info):\n         \"\"\"Handler for failed downloads\"\"\"\n-        return failure\n+        raise NotImplementedError()\n \n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n@@ -221,6 +218,7 @@ class MediaPipeline:\n                     )\n         return item\n \n+    @abstractmethod\n     def file_path(self, request, response=None, info=None, *, item=None):\n         \"\"\"Returns the path where downloaded media should be stored\"\"\"\n-        pass\n+        raise NotImplementedError()\n\n@@ -1,4 +1,3 @@\n-import io\n from typing import Optional\n \n from testfixtures import LogCapture\n@@ -11,7 +10,6 @@ from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n-from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -35,8 +33,26 @@ def _mocked_download_func(request, info):\n     return response() if callable(response) else response\n \n \n+class UserDefinedPipeline(MediaPipeline):\n+\n+    def media_to_download(self, request, info, *, item=None):\n+        pass\n+\n+    def get_media_requests(self, item, info):\n+        pass\n+\n+    def media_downloaded(self, response, request, info, *, item=None):\n+        return {}\n+\n+    def media_failed(self, failure, request, info):\n+        return failure\n+\n+    def file_path(self, request, response=None, info=None, *, item=None):\n+        return \"\"\n+\n+\n class BaseMediaPipelineTestCase(unittest.TestCase):\n-    pipeline_class = MediaPipeline\n+    pipeline_class = UserDefinedPipeline\n     settings = None\n \n     def setUp(self):\n@@ -54,54 +70,6 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n             if not name.startswith(\"_\"):\n                 disconnect_all(signal)\n \n-    def test_default_media_to_download(self):\n-        request = Request(\"http://url\")\n-        assert self.pipe.media_to_download(request, self.info) is None\n-\n-    def test_default_get_media_requests(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.get_media_requests(item, self.info) is None\n-\n-    def test_default_media_downloaded(self):\n-        request = Request(\"http://url\")\n-        response = Response(\"http://url\", body=b\"\")\n-        assert self.pipe.media_downloaded(response, request, self.info) is response\n-\n-    def test_default_media_failed(self):\n-        request = Request(\"http://url\")\n-        fail = Failure(Exception())\n-        assert self.pipe.media_failed(fail, request, self.info) is fail\n-\n-    def test_default_item_completed(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.item_completed([], item, self.info) is item\n-\n-        # Check that failures are logged by default\n-        fail = Failure(Exception())\n-        results = [(True, 1), (False, fail)]\n-\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-\n-        assert new_item is item\n-        assert len(log.records) == 1\n-        record = log.records[0]\n-        assert record.levelname == \"ERROR\"\n-        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n-\n-        # disable failure logging and check again\n-        self.pipe.LOG_FAILED_RESULTS = False\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-        assert new_item is item\n-        assert len(log.records) == 0\n-\n-    @inlineCallbacks\n-    def test_default_process_item(self):\n-        item = {\"name\": \"name\"}\n-        new_item = yield self.pipe.process_item(item, self.spider)\n-        assert new_item is item\n-\n     def test_modify_media_request(self):\n         request = Request(\"http://url\")\n         self.pipe._modify_media_request(request)\n@@ -175,8 +143,38 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         context = getattr(info.downloaded[fp].value, \"__context__\", None)\n         self.assertIsNone(context)\n \n+    def test_default_item_completed(self):\n+        item = {\"name\": \"name\"}\n+        assert self.pipe.item_completed([], item, self.info) is item\n \n-class MockedMediaPipeline(MediaPipeline):\n+        # Check that failures are logged by default\n+        fail = Failure(Exception())\n+        results = [(True, 1), (False, fail)]\n+\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+\n+        assert new_item is item\n+        assert len(log.records) == 1\n+        record = log.records[0]\n+        assert record.levelname == \"ERROR\"\n+        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n+\n+        # disable failure logging and check again\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+        assert new_item is item\n+        assert len(log.records) == 0\n+\n+    @inlineCallbacks\n+    def test_default_process_item(self):\n+        item = {\"name\": \"name\"}\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        assert new_item is item\n+\n+\n+class MockedMediaPipeline(UserDefinedPipeline):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._mockcalled = []\n@@ -232,7 +230,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         )\n         item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n         self.assertEqual(\n             self.pipe._mockcalled,\n             [\n@@ -277,7 +275,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         req2 = Request(\"http://url2\", meta={\"response\": fail})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (False, fail)])\n         m = self.pipe._mockcalled\n         # only once\n         self.assertEqual(m[0], \"get_media_requests\")  # first hook called\n@@ -315,7 +313,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n@@ -325,7 +323,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n@@ -337,7 +335,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_wait_if_request_is_downloading(self):\n@@ -363,7 +361,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         req2 = Request(req1.url, meta={\"response\": rsp2_func})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n@@ -376,57 +374,15 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             [\"get_media_requests\", \"media_to_download\", \"item_completed\"],\n         )\n \n-\n-class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self._mockcalled = []\n-\n-    def get_media_requests(self, item, info):\n-        item_url = item[\"image_urls\"][0]\n-        output_img = io.BytesIO()\n-        img = Image.new(\"RGB\", (60, 30), color=\"red\")\n-        img.save(output_img, format=\"JPEG\")\n-        return Request(\n-            item_url,\n-            meta={\n-                \"response\": Response(item_url, status=200, body=output_img.getvalue())\n-            },\n+    def test_key_for_pipe(self):\n+        self.assertEqual(\n+            self.pipe._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\"),\n+            \"MOCKEDMEDIAPIPELINE_IMAGES\",\n         )\n \n-    def inc_stats(self, *args, **kwargs):\n-        return True\n-\n-    def media_to_download(self, request, info):\n-        self._mockcalled.append(\"media_to_download\")\n-        return super().media_to_download(request, info)\n-\n-    def media_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"media_downloaded\")\n-        return super().media_downloaded(response, request, info)\n-\n-    def file_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"file_downloaded\")\n-        return super().file_downloaded(response, request, info)\n-\n-    def file_path(self, request, response=None, info=None):\n-        self._mockcalled.append(\"file_path\")\n-        return super().file_path(request, response, info)\n-\n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n-        self._mockcalled.append(\"thumb_path\")\n-        return super().thumb_path(request, thumb_id, response, info)\n-\n-    def get_images(self, response, request, info):\n-        self._mockcalled.append(\"get_images\")\n-        return super().get_images(response, request, info)\n-\n-    def image_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"image_downloaded\")\n-        return super().image_downloaded(response, request, info)\n-\n \n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n+\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(settings=Settings(settings))\n         request = Request(\"http://url\")\n@@ -452,18 +408,11 @@ class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n             else:\n                 self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n \n-    def test_standard_setting(self):\n-        self._assert_request_no3xx(MediaPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n-\n     def test_subclass_standard_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_specific_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d58af86971ba54bfc1feffa88fc564ffba650f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 55 | Churn Cumulative: 3922 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: 0.5116279069767442\n\nDIFF:\n@@ -323,9 +323,11 @@ def _load_policy_class(\n     try:\n         return cast(Type[ReferrerPolicy], load_object(policy))\n     except ValueError:\n-        try:\n-            return _policy_classes[policy.lower()]\n-        except KeyError:\n+        tokens = [token.strip() for token in policy.lower().split(\",\")]\n+        for token in tokens[::-1]:\n+            if token in _policy_classes:\n+                return _policy_classes[token]\n+\n         msg = f\"Could not load referrer policy {policy!r}\"\n         if not warning_only:\n             raise RuntimeError(msg)\n\n@@ -884,6 +884,53 @@ class TestSettingsPolicyByName(TestCase):\n         with self.assertRaises(RuntimeError):\n             RefererMiddleware(settings)\n \n+    def test_multiple_policy_tokens(self):\n+        # test parsing without space(s) after the comma\n+        settings1 = Settings(\n+            {\n+                \"REFERRER_POLICY\": \",\".join(\n+                    [\n+                        \"some-custom-unknown-policy\",\n+                        POLICY_SAME_ORIGIN,\n+                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n+                        \"another-custom-unknown-policy\",\n+                    ]\n+                )\n+            }\n+        )\n+        mw1 = RefererMiddleware(settings1)\n+        self.assertEqual(mw1.default_policy, StrictOriginWhenCrossOriginPolicy)\n+\n+        # test parsing with space(s) after the comma\n+        settings2 = Settings(\n+            {\n+                \"REFERRER_POLICY\": \",    \".join(\n+                    [\n+                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n+                        \"another-custom-unknown-policy\",\n+                        POLICY_UNSAFE_URL,\n+                    ]\n+                )\n+            }\n+        )\n+        mw2 = RefererMiddleware(settings2)\n+        self.assertEqual(mw2.default_policy, UnsafeUrlPolicy)\n+\n+    def test_multiple_policy_tokens_all_invalid(self):\n+        settings = Settings(\n+            {\n+                \"REFERRER_POLICY\": \",\".join(\n+                    [\n+                        \"some-custom-unknown-policy\",\n+                        \"another-custom-unknown-policy\",\n+                        \"yet-another-custom-unknown-policy\",\n+                    ]\n+                )\n+            }\n+        )\n+        with self.assertRaises(RuntimeError):\n+            RefererMiddleware(settings)\n+\n \n class TestPolicyHeaderPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.SameOriginPolicy\"}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62a028b99dc73b8ddddd37f986780a5a070f2938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1184 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -324,6 +324,7 @@ def _load_policy_class(\n         return cast(Type[ReferrerPolicy], load_object(policy))\n     except ValueError:\n         tokens = [token.strip() for token in policy.lower().split(\",\")]\n+        # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\n         for token in tokens[::-1]:\n             if token in _policy_classes:\n                 return _policy_classes[token]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4293e8f9efac5046f92e4ebfd744be443b858b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 39 | Files Changed: 12 | Hunks: 44 | Methods Changed: 50 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 117 | Churn Cumulative: 7432 | Contributors (this commit): 53 | Commits (past 90d): 16 | Contributors (cumulative): 146 | DMM Complexity: 0.8636363636363636\n\nDIFF:\n@@ -119,7 +119,7 @@ class H2Agent:\n             self._reactor, self._context_factory, connect_timeout, bind_address\n         )\n \n-    def get_endpoint(self, uri: URI):\n+    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n         return self.endpoint_factory.endpointForURI(uri)\n \n     def get_key(self, uri: URI) -> Tuple:\n@@ -161,7 +161,7 @@ class ScrapyProxyH2Agent(H2Agent):\n         )\n         self._proxy_uri = proxy_uri\n \n-    def get_endpoint(self, uri: URI):\n+    def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n         return self.endpoint_factory.endpointForURI(self._proxy_uri)\n \n     def get_key(self, uri: URI) -> Tuple:\n\n@@ -22,7 +22,11 @@ from h2.events import (\n from h2.exceptions import FrameTooLargeError, H2Error\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n-from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory\n+from twisted.internet.interfaces import (\n+    IAddress,\n+    IHandshakeListener,\n+    IProtocolNegotiationFactory,\n+)\n from twisted.internet.protocol import Factory, Protocol, connectionDone\n from twisted.internet.ssl import Certificate\n from twisted.protocols.policies import TimeoutMixin\n@@ -431,7 +435,7 @@ class H2ClientFactory(Factory):\n         self.settings = settings\n         self.conn_lost_deferred = conn_lost_deferred\n \n-    def buildProtocol(self, addr) -> H2ClientProtocol:\n+    def buildProtocol(self, addr: IAddress) -> H2ClientProtocol:\n         return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n \n     def acceptableProtocols(self) -> List[bytes]:\n\n@@ -1,7 +1,7 @@\n import logging\n from enum import Enum\n from io import BytesIO\n-from typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -142,7 +142,7 @@ class Stream:\n             \"headers\": Headers({}),\n         }\n \n-        def _cancel(_) -> None:\n+        def _cancel(_: Any) -> None:\n             # Close this stream as gracefully as possible\n             # If the associated request is initiated we reset this stream\n             # else we directly call close() method\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import warnings\n from itertools import chain\n from logging import getLogger\n-from typing import TYPE_CHECKING, List, Optional, Union\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n from scrapy import Request, Spider, signals\n from scrapy.crawler import Crawler\n@@ -149,20 +149,24 @@ class HttpCompressionMiddleware:\n \n         return response\n \n-    def _handle_encoding(self, body, content_encoding, max_size):\n+    def _handle_encoding(\n+        self, body: bytes, content_encoding: List[bytes], max_size: int\n+    ) -> Tuple[bytes, List[bytes]]:\n         to_decode, to_keep = self._split_encodings(content_encoding)\n         for encoding in to_decode:\n             body = self._decode(body, encoding, max_size)\n         return body, to_keep\n \n-    def _split_encodings(self, content_encoding):\n-        to_keep = [\n+    def _split_encodings(\n+        self, content_encoding: List[bytes]\n+    ) -> Tuple[List[bytes], List[bytes]]:\n+        to_keep: List[bytes] = [\n             encoding.strip().lower()\n             for encoding in chain.from_iterable(\n                 encodings.split(b\",\") for encodings in content_encoding\n             )\n         ]\n-        to_decode = []\n+        to_decode: List[bytes] = []\n         while to_keep:\n             encoding = to_keep.pop()\n             if encoding not in ACCEPTED_ENCODINGS:\n\n@@ -1,33 +1,43 @@\n+from __future__ import annotations\n+\n import logging\n import re\n import warnings\n+from typing import TYPE_CHECKING, Set\n \n-from scrapy import signals\n+from scrapy import Request, Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.httpobj import urlparse_cached\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n class OffsiteMiddleware:\n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n         return o\n \n-    def __init__(self, stats):\n+    def __init__(self, stats: StatsCollector):\n         self.stats = stats\n-        self.domains_seen = set()\n+        self.domains_seen: Set[str] = set()\n \n-    def spider_opened(self, spider):\n-        self.host_regex = self.get_host_regex(spider)\n+    def spider_opened(self, spider: Spider) -> None:\n+        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n \n-    def request_scheduled(self, request, spider):\n+    def request_scheduled(self, request: Request, spider: Spider) -> None:\n         self.process_request(request, spider)\n \n-    def process_request(self, request, spider):\n+    def process_request(self, request: Request, spider: Spider) -> None:\n         if request.dont_filter or self.should_follow(request, spider):\n             return None\n         domain = urlparse_cached(request).hostname\n@@ -42,13 +52,13 @@ class OffsiteMiddleware:\n         self.stats.inc_value(\"offsite/filtered\", spider=spider)\n         raise IgnoreRequest\n \n-    def should_follow(self, request, spider):\n+    def should_follow(self, request: Request, spider: Spider) -> bool:\n         regex = self.host_regex\n         # hostname can be None for wrong urls (like javascript links)\n         host = urlparse_cached(request).hostname or \"\"\n         return bool(regex.search(host))\n \n-    def get_host_regex(self, spider):\n+    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n         allowed_domains = getattr(spider, \"allowed_domains\", None)\n         if not allowed_domains:\n\n@@ -4,8 +4,11 @@ Item Loader\n See documentation in docs/topics/loaders.rst\n \"\"\"\n \n+from typing import Any, Optional\n+\n import itemloaders\n \n+from scrapy.http import TextResponse\n from scrapy.item import Item\n from scrapy.selector import Selector\n \n@@ -82,7 +85,14 @@ class ItemLoader(itemloaders.ItemLoader):\n     default_item_class: type = Item\n     default_selector_class = Selector\n \n-    def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n+    def __init__(\n+        self,\n+        item: Any = None,\n+        selector: Optional[Selector] = None,\n+        response: Optional[TextResponse] = None,\n+        parent: Optional[itemloaders.ItemLoader] = None,\n+        **context: Any\n+    ):\n         if selector is None and response is not None:\n             try:\n                 selector = self.default_selector_class(response)\n\n@@ -1,21 +1,23 @@\n import random\n+from typing import Any\n from urllib.parse import urlencode\n \n from twisted.web.resource import Resource\n-from twisted.web.server import Site\n+from twisted.web.server import Request, Site\n \n \n class Root(Resource):\n     isLeaf = True\n \n-    def getChild(self, name, request):\n+    def getChild(self, name: str, request: Request) -> Resource:\n         return self\n \n-    def render(self, request):\n+    def render(self, request: Request) -> bytes:\n         total = _getarg(request, b\"total\", 100, int)\n         show = _getarg(request, b\"show\", 10, int)\n         nlist = [random.randint(1, total) for _ in range(show)]  # nosec\n         request.write(b\"<html><head></head><body>\")\n+        assert request.args is not None\n         args = request.args.copy()\n         for nl in nlist:\n             args[\"n\"] = nl\n@@ -27,7 +29,7 @@ class Root(Resource):\n         return b\"\"\n \n \n-def _getarg(request, name, default=None, type=str):\n+def _getarg(request, name: bytes, default: Any = None, type=str):\n     return type(request.args[name][0]) if name in request.args else default\n \n \n@@ -38,7 +40,7 @@ if __name__ == \"__main__\":\n     factory = Site(root)\n     httpPort = reactor.listenTCP(8998, Site(root))\n \n-    def _print_listening():\n+    def _print_listening() -> None:\n         httpHost = httpPort.getHost()\n         print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n \n\n@@ -2,13 +2,20 @@ import argparse\n import warnings\n from http.cookies import SimpleCookie\n from shlex import split\n+from typing import Any, Dict, List, NoReturn, Optional, Sequence, Tuple, Union\n from urllib.parse import urlparse\n \n from w3lib.http import basic_auth_header\n \n \n class DataAction(argparse.Action):\n-    def __call__(self, parser, namespace, values, option_string=None):\n+    def __call__(\n+        self,\n+        parser: argparse.ArgumentParser,\n+        namespace: argparse.Namespace,\n+        values: Union[str, Sequence[Any], None],\n+        option_string: Optional[str] = None,\n+    ) -> None:\n         value = str(values)\n         if value.startswith(\"$\"):\n             value = value[1:]\n@@ -16,7 +23,7 @@ class DataAction(argparse.Action):\n \n \n class CurlParser(argparse.ArgumentParser):\n-    def error(self, message):\n+    def error(self, message: str) -> NoReturn:\n         error_msg = f\"There was an error parsing the curl command: {message}\"\n         raise ValueError(error_msg)\n \n@@ -42,9 +49,11 @@ for argument in safe_to_ignore_arguments:\n     curl_parser.add_argument(*argument, action=\"store_true\")\n \n \n-def _parse_headers_and_cookies(parsed_args):\n-    headers = []\n-    cookies = {}\n+def _parse_headers_and_cookies(\n+    parsed_args: argparse.Namespace,\n+) -> Tuple[List[Tuple[str, bytes]], Dict[str, str]]:\n+    headers: List[Tuple[str, bytes]] = []\n+    cookies: Dict[str, str] = {}\n     for header in parsed_args.headers or ():\n         name, val = header.split(\":\", 1)\n         name = name.strip()\n@@ -64,7 +73,7 @@ def _parse_headers_and_cookies(parsed_args):\n \n def curl_to_request_kwargs(\n     curl_command: str, ignore_unknown_options: bool = True\n-) -> dict:\n+) -> Dict[str, Any]:\n     \"\"\"Convert a cURL command syntax to Request kwargs.\n \n     :param str curl_command: string containing the curl command\n@@ -98,7 +107,7 @@ def curl_to_request_kwargs(\n \n     method = parsed_args.method or \"GET\"\n \n-    result = {\"method\": method.upper(), \"url\": url}\n+    result: Dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n \n     headers, cookies = _parse_headers_and_cookies(parsed_args)\n \n\n@@ -110,7 +110,7 @@ class CaseInsensitiveDict(collections.UserDict):\n     as keys and allows case-insensitive lookups.\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs) -> None:\n+    def __init__(self, *args: Any, **kwargs: Any) -> None:\n         self._keys: dict = {}\n         super().__init__(*args, **kwargs)\n \n\n@@ -138,7 +138,7 @@ class RequestFingerprinter:\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler) -> Self:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n     def __init__(self, crawler: Optional[Crawler] = None):\n\n@@ -58,7 +58,7 @@ def response_status_message(status: Union[bytes, float, int, str]) -> str:\n     return f\"{status_int} {to_unicode(message)}\"\n \n \n-def _remove_html_comments(body):\n+def _remove_html_comments(body: bytes) -> bytes:\n     start = body.find(b\"<!--\")\n     while start != -1:\n         end = body.find(b\"-->\", start + 1)\n\n@@ -15,12 +15,12 @@ class SiteTest:\n         super().tearDown()\n         self.site.stopListening()\n \n-    def url(self, path):\n+    def url(self, path: str) -> str:\n         return urljoin(self.baseurl, path)\n \n \n class NoMetaRefreshRedirect(util.Redirect):\n-    def render(self, request):\n+    def render(self, request: server.Request) -> bytes:\n         content = util.Redirect.render(self, request)\n         return content.replace(\n             b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
