{"custom_id": "keras#d1244e6903fb27b620840b3cca1339b262ffdd10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 325 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -23,6 +23,7 @@ we use Keras image preprocessing layers for image standardization and data augme\n \"\"\"\n \n import os\n+import numpy as np\n import keras\n from keras import layers\n from tensorflow import data as tf_data\n@@ -107,7 +108,7 @@ plt.figure(figsize=(10, 10))\n for images, labels in train_ds.take(1):\n     for i in range(9):\n         ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(images[i].numpy().astype(\"uint8\"))\n+        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n         plt.title(int(labels[i]))\n         plt.axis(\"off\")\n \n@@ -139,7 +140,7 @@ for images, _ in train_ds.take(1):\n     for i in range(9):\n         augmented_images = data_augmentation(images)\n         ax = plt.subplot(3, 3, i + 1)\n-        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n+        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n         plt.axis(\"off\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4feb16c82b8e3d47721520e9b45ef4bebc1ead0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 335 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,13 +122,17 @@ helps expose the model to different aspects of the training data while slowing d\n overfitting.\n \"\"\"\n \n-data_augmentation = keras.Sequential(\n-    [\n+data_augmentation_layers = [\n     layers.RandomFlip(),\n     layers.RandomRotation(0.15),\n     layers.RandomZoom(0.15),\n ]\n-)\n+\n+def data_augmentation(images):\n+    for layer in data_augmentation_layers:\n+        images = layer(images)\n+    return images\n+\n \n \"\"\"\n Let's visualize what the augmented samples look like, by applying `data_augmentation`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8caf81e72a5cfe83b7abb93c8333fed2540e140f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 340 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -123,9 +123,8 @@ overfitting.\n \"\"\"\n \n data_augmentation_layers = [\n-    layers.RandomFlip(),\n-    layers.RandomRotation(0.15),\n-    layers.RandomZoom(0.15),\n+    layers.RandomFlip(\"horizontal\"),\n+    layers.RandomRotation(0.1),\n ]\n \n def data_augmentation(images):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4b95d2da96a4d7d628d284e5221f2a99900aade", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 342 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -291,7 +291,7 @@ callbacks = [\n     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n ]\n model.compile(\n-    optimizer=keras.optimizers.Adam(1e-3),\n+    optimizer=keras.optimizers.Adam(3e-4),\n     loss=\"binary_crossentropy\",\n     metrics=[\"accuracy\"],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#894b147c8cc9c14cafe6668d9fb8cac8cd560148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 353 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -267,14 +267,13 @@ def make_model(input_shape, num_classes):\n \n     x = layers.GlobalAveragePooling2D()(x)\n     if num_classes == 2:\n-        activation = \"sigmoid\"\n         units = 1\n     else:\n-        activation = \"softmax\"\n         units = num_classes\n \n     x = layers.Dropout(0.5)(x)\n-    outputs = layers.Dense(units, activation=activation)(x)\n+    # We specify activation=None so as to return logits\n+    outputs = layers.Dense(units, activation=None)(x)\n     return keras.Model(inputs, outputs)\n \n \n@@ -291,9 +290,9 @@ callbacks = [\n     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n ]\n model.compile(\n-    optimizer=keras.optimizers.Adam(3e-4),\n-    loss=\"binary_crossentropy\",\n-    metrics=[\"accuracy\"],\n+    optimizer=keras.optimizers.Adam(1e-2),\n+    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+    metrics=[keras.Metrics.BinaryAccuracy(name=\"acc\")],\n )\n model.fit(\n     train_ds,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#502eed431708caba6bfff0a772e22b0e85ad2200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 355 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ callbacks = [\n     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n ]\n model.compile(\n-    optimizer=keras.optimizers.Adam(1e-2),\n+    optimizer=keras.optimizers.Adam(1e-3),\n     loss=keras.losses.BinaryCrossentropy(from_logits=True),\n     metrics=[keras.Metrics.BinaryAccuracy(name=\"acc\")],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc89cc6edc66a589e7b84b3a1e1e8b0e47a5ff3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 357 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -292,7 +292,7 @@ callbacks = [\n model.compile(\n     optimizer=keras.optimizers.Adam(1e-3),\n     loss=keras.losses.BinaryCrossentropy(from_logits=True),\n-    metrics=[keras.Metrics.BinaryAccuracy(name=\"acc\")],\n+    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n )\n model.fit(\n     train_ds,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30fcae680d00031556b628033d1d0347425f8495", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 30 | Churn Cumulative: 1459 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 0.9285714285714286\n\nDIFF:\n@@ -439,7 +439,8 @@ def _get_logits(output, from_logits, op_type, fn_name):\n         from_logits_ = True\n \n     from_expected_op_type = (\n-        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n+        hasattr(output, \"op\")\n+        and not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n         and output.op.type == op_type\n     ) and not has_keras_logits\n \n@@ -564,6 +565,9 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         raise ValueError(\n             f\"Only axis=-1 is currently supported. Received: axis={axis}\"\n         )\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"sparse_categorical_crossentropy\"\n+    )\n \n     target = tf.convert_to_tensor(target)\n     target = tf.cast(target, dtype=\"int64\")\n@@ -591,9 +595,6 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n                 f\"target.shape={target.shape}, output.shape={output.shape}\"\n             )\n \n-    output, from_logits = _get_logits(\n-        output, from_logits, \"Softmax\", \"sparse_categorical_crossentropy\"\n-    )\n     if not from_logits:\n         output = tf.clip_by_value(output, epsilon(), 1 - epsilon())\n         output = tf.math.log(output)\n@@ -637,6 +638,7 @@ def binary_crossentropy(target, output, from_logits=False):\n     output, from_logits = _get_logits(\n         output, from_logits, \"Sigmoid\", \"binary_crossentropy\"\n     )\n+\n     if from_logits:\n         return tf.nn.sigmoid_cross_entropy_with_logits(\n             labels=target, logits=output\n\n@@ -3,6 +3,9 @@ import pytest\n from absl.testing import parameterized\n \n from keras import backend\n+from keras import layers\n+from keras import losses\n+from keras import models\n from keras import testing\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.layers.convolutional.conv_test import np_conv1d\n@@ -1607,3 +1610,20 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(mean.values[0], 4.5)\n             self.assertEqual(variance.values[0], 8.75)\n             self.assertEqual(variance.values[0], 8.75)\n+\n+\n+class TestLogitRecovery(testing.TestCase):\n+    def test_logit_recovery_binary_crossentropy(self):\n+        layer = layers.Dense(\n+            4, activation=\"sigmoid\", use_bias=False, kernel_initializer=\"ones\"\n+        )\n+        loss = losses.BinaryCrossentropy()\n+        x = np.array([[1.4, 1.6, 0.8]])\n+        y = np.array([[0.2, 0.6, 0.1, 0.3]])\n+        loss_value = loss(y, layer(x))\n+        self.assertAllClose(loss_value, 2.682124)\n+\n+        model = models.Sequential([layer])\n+        model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\")\n+        out = model.evaluate(x, y)\n+        self.assertAllClose(out, 2.682124)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0056455a0db64811c7fea9d0364566fec8b3432", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 453 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -157,7 +157,7 @@ class Trainer:\n             if run_eagerly:\n                 jit_compile = False\n             else:\n-                jit_compile = resolve_auto_jit_compile(self)\n+                jit_compile = self._resolve_auto_jit_compile()\n         if jit_compile and run_eagerly:\n             jit_compile = False\n             warnings.warn(\n@@ -192,11 +192,7 @@ class Trainer:\n     def jit_compile(self):\n         if self._jit_compile is None:\n             # Value was never set. Resolve it now.\n-            # torch always runs in eager unless jit_compile is explicitly set\n-            jit_compile = (\n-                model_supports_jit(self) and backend.backend() != \"torch\"\n-            )\n-            self._jit_compile = jit_compile\n+            self._jit_compile = self._resolve_auto_jit_compile()\n         return self._jit_compile\n \n     @jit_compile.setter\n@@ -210,6 +206,23 @@ class Trainer:\n         else:\n             self._jit_compile = value\n \n+    def _resolve_auto_jit_compile(self):\n+        if backend.backend() == \"torch\":\n+            # jit_compile = \"auto\" with the pytorch backend defaults to eager\n+            return False\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            devices = tf.config.list_physical_devices()\n+            if not list(filter(lambda x: x.device_type != \"CPU\", devices)):\n+                # Disable XLA on CPU-only machines.\n+                return False\n+\n+        if model_supports_jit(self):\n+            return True\n+        return False\n+\n     @property\n     def run_eagerly(self):\n         return self._run_eagerly\n@@ -866,23 +879,15 @@ class Trainer:\n             raise ValueError(msg)\n \n \n-def resolve_auto_jit_compile(model):\n-    if backend.backend() == \"torch\":\n-        # jit_compile = \"auto\" with the pytorch backend defaults to eager\n-        return False\n-\n-    if model_supports_jit(model):\n-        return True\n-    return False\n-\n-\n def model_supports_jit(model):\n+    # XLA not supported with TF on MacOS GPU\n     if platform.system() == \"Darwin\" and \"arm\" in platform.processor().lower():\n         if backend.backend() == \"tensorflow\":\n             from keras.utils.module_utils import tensorflow as tf\n \n             if tf.config.list_physical_devices(\"GPU\"):\n                 return False\n+    # XLA not supported by some layers\n     if all(x.supports_jit for x in model._flatten_layers()):\n         return True\n     return False\n\n@@ -530,6 +530,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             loss=\"mse\",\n             optimizer=\"adam\",\n             steps_per_execution=3,\n+            jit_compile=True,  # TODO: fails in eager?\n         )\n         step_count = StepCount()\n         model.fit(x=x, y=y, batch_size=16, callbacks=[step_count], verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8393193076558d5f1476a0d2fe7bc6709a3df9a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 359 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -319,5 +319,5 @@ img_array = keras.utils.img_to_array(img)\n img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis\n \n predictions = model.predict(img_array)\n-score = float(predictions[0])\n+score = float(keras.ops.sigmoid(predictions[0]))\n print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5999db29234e221740b39ff8487ad6c9a9ba6094", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 27 | Churn Cumulative: 204 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ cast\n convert_to_tensor\n convert_to_numpy\n cond\n+is_tensor\n \"\"\"\n \n import numpy as np\n@@ -402,7 +403,7 @@ def unstack(x, num=None, axis=0):\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n \n-    Note: On the tensorflow backend, when `x` is a `tf.Tensor` with dynamic\n+    Note: On the TensorFlow backend, when `x` is a `tf.Tensor` with dynamic\n     shape, dimensions which are dynamic in the context of a compiled function\n     will have a `tf.Tensor` value instead of a static integer value.\n \n@@ -629,3 +630,19 @@ def vectorized_map(function, elements):\n     a single list of tensor arguments.\n     \"\"\"\n     return backend.core.vectorized_map(function, elements)\n+\n+\n+@keras_export(\"keras.ops.is_tensor\")\n+def is_tensor(x):\n+    \"\"\"Check whether the given object is a tensor.\n+\n+    Note: This checks for backend specific tensors so passing a TensorFlow\n+    tensor would return `False` if your backend is PyTorch or JAX.\n+\n+    Args:\n+        x: A variable.\n+\n+    Returns:\n+        `True` if `x` is a tensor, otherwise `False`.\n+    \"\"\"\n+    return backend.core.is_tensor(x)\n\n@@ -411,3 +411,11 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(\n             backend.convert_to_numpy(output), 2 * np.ones((2, 3))\n         )\n+\n+    def test_is_tensor(self):\n+        np_x = np.array([[1, 2, 3], [3, 2, 1]])\n+        x = backend.convert_to_tensor(np_x)\n+        if backend.backend() != \"numpy\":\n+            self.assertFalse(ops.is_tensor(np_x))\n+        self.assertTrue(ops.is_tensor(x))\n+        self.assertFalse(ops.is_tensor([1, 2, 3]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d771345b62c4707b95a0f9f0c4da37d881d6a070", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 48 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,15 +78,15 @@ def get_metric(identifier, y_true, y_pred):\n                 name=str(identifier)\n             )\n \n-    if not isinstance(metric_obj, metrics_module.Metric):\n-        metric_obj = metrics_module.MeanMetricWrapper(metric_obj)\n-\n     if isinstance(identifier, str):\n         metric_name = identifier\n     else:\n         metric_name = get_object_name(metric_obj)\n-    metric_obj.name = metric_name\n \n+    if not isinstance(metric_obj, metrics_module.Metric):\n+        metric_obj = metrics_module.MeanMetricWrapper(metric_obj)\n+\n+    metric_obj.name = metric_name\n     return metric_obj\n \n \n\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n from keras import backend\n from keras import metrics as losses_module\n from keras import metrics as metrics_module\n+from keras import ops\n from keras import testing\n from keras.trainers.compile_utils import CompileLoss\n from keras.trainers.compile_utils import CompileMetrics\n@@ -212,6 +213,23 @@ class TestCompileMetrics(testing.TestCase):\n         self.assertAllClose(result[\"accuracy\"], 0.333333)\n         self.assertTrue(\"mse\" in result)\n \n+    def test_custom_metric_function(self):\n+        def my_custom_metric(y_true, y_pred):\n+            return ops.mean(ops.square(y_true - y_pred), axis=-1)\n+\n+        compile_metrics = CompileMetrics(\n+            metrics=[my_custom_metric],\n+            weighted_metrics=[],\n+        )\n+        y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n+        y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n+        compile_metrics.build(y_true, y_pred)\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=None)\n+        result = compile_metrics.result()\n+        self.assertIsInstance(result, dict)\n+        self.assertEqual(len(result), 1)\n+        self.assertTrue(\"my_custom_metric\" in result)\n+\n \n class TestCompileLoss(testing.TestCase, parameterized.TestCase):\n     def test_single_output_case(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7e2bcb9fd54aa0cd80bcb3e75df72018d54078e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 361 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ callbacks = [\n     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n ]\n model.compile(\n-    optimizer=keras.optimizers.Adam(1e-3),\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n     loss=keras.losses.BinaryCrossentropy(from_logits=True),\n     metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66bb075f765e0c7fe80bf7af6d3cbb9148824675", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 365 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -271,7 +271,7 @@ def make_model(input_shape, num_classes):\n     else:\n         units = num_classes\n \n-    x = layers.Dropout(0.5)(x)\n+    x = layers.Dropout(0.25)(x)\n     # We specify activation=None so as to return logits\n     outputs = layers.Dense(units, activation=None)(x)\n     return keras.Model(inputs, outputs)\n@@ -290,7 +290,7 @@ callbacks = [\n     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n ]\n model.compile(\n-    optimizer=keras.optimizers.RMSprop(1e-3),\n+    optimizer=keras.optimizers.Adam(3e-4),\n     loss=keras.losses.BinaryCrossentropy(from_logits=True),\n     metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a480f4525dc136f383c9c9e4c8f77b611ef5c94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 443 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -49,18 +49,18 @@ import tensorflow.strings as tf_strings\n \n # Data\n BATCH_SIZE = 64\n-SEQ_LEN = 128\n-MIN_TRAINING_SEQ_LEN = 450\n+MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n+SEQ_LEN = 128  # Length of training sequences, in tokens\n \n # Model\n EMBED_DIM = 256\n-FEED_FORWARD_DIM = 256\n+FEED_FORWARD_DIM = 128\n NUM_HEADS = 3\n NUM_LAYERS = 2\n VOCAB_SIZE = 5000  # Limits parameters in model.\n \n # Training\n-EPOCHS = 6\n+EPOCHS = 5\n \n # Inference\n NUM_TOKENS_TO_GENERATE = 80\n@@ -82,7 +82,7 @@ dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n # Load simplebooks-92 train set and filter out short lines.\n raw_train_ds = (\n     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n-    .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)\n+    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n     .batch(BATCH_SIZE)\n     .shuffle(buffer_size=256)\n )\n@@ -90,7 +90,7 @@ raw_train_ds = (\n # Load simplebooks-92 validation set and filter out short lines.\n raw_val_ds = (\n     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n-    .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)\n+    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n     .batch(BATCH_SIZE)\n )\n \n@@ -214,7 +214,7 @@ model.summary()\n Now that we have our model, let's train it with the `fit()` method.\n \"\"\"\n \n-model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)\n+model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n \n \"\"\"\n ## Inference\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5422e70cb94af1ade57176e49b85cd7dcdb7bd6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -178,8 +178,6 @@ class RandomRotation(TFDataLayer):\n             else:\n                 image_height = shape[1]\n                 image_width = shape[2]\n-        image_height = float(image_height)\n-        image_width = float(image_width)\n \n         lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n         upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n@@ -194,6 +192,8 @@ class RandomRotation(TFDataLayer):\n \n         cos_theta = self.backend.numpy.cos(angle)\n         sin_theta = self.backend.numpy.sin(angle)\n+        image_height = self.backend.core.cast(image_height, cos_theta.dtype)\n+        image_width = self.backend.core.cast(image_width, cos_theta.dtype)\n \n         x_offset = (\n             (image_width - 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4803b5497ad060cce345a323be2546152315ec3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 305 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -182,6 +182,9 @@ class CosineSimilarity(LossFunctionWrapper):\n             cosine_similarity, reduction=reduction, name=name, axis=axis\n         )\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_export(\"keras.losses.Huber\")\n class Huber(LossFunctionWrapper):\n@@ -217,6 +220,9 @@ class Huber(LossFunctionWrapper):\n     ):\n         super().__init__(huber, name=name, reduction=reduction, delta=delta)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_export(\"keras.losses.LogCosh\")\n class LogCosh(LossFunctionWrapper):\n@@ -240,6 +246,9 @@ class LogCosh(LossFunctionWrapper):\n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"log_cosh\"):\n         super().__init__(log_cosh, name=name, reduction=reduction)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_export(\"keras.losses.Hinge\")\n class Hinge(LossFunctionWrapper):\n\n@@ -433,6 +433,8 @@ class CosineSimilarityTest(testing.TestCase):\n         )\n         self.assertEqual(cosine_obj.name, \"cosine_loss\")\n         self.assertEqual(cosine_obj.reduction, \"sum\")\n+        config = cosine_obj.get_config()\n+        self.assertEqual(config, {\"name\": \"cosine_loss\", \"reduction\": \"sum\"})\n \n     def test_unweighted(self):\n         self.setup()\n@@ -517,6 +519,8 @@ class HuberLossTest(testing.TestCase):\n         h_obj = losses.Huber(reduction=\"sum\", name=\"huber\")\n         self.assertEqual(h_obj.name, \"huber\")\n         self.assertEqual(h_obj.reduction, \"sum\")\n+        config = h_obj.get_config()\n+        self.assertEqual(config, {\"name\": \"huber\", \"reduction\": \"sum\"})\n \n     def test_all_correct(self):\n         self.setup()\n@@ -615,6 +619,8 @@ class LogCoshTest(testing.TestCase):\n         logcosh_obj = losses.LogCosh(reduction=\"sum\", name=\"logcosh_loss\")\n         self.assertEqual(logcosh_obj.name, \"logcosh_loss\")\n         self.assertEqual(logcosh_obj.reduction, \"sum\")\n+        config = logcosh_obj.get_config()\n+        self.assertEqual(config, {\"name\": \"logcosh_loss\", \"reduction\": \"sum\"})\n \n     def test_unweighted(self):\n         self.setup()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#541d186500bbebe59a3b9375706b04ff20a983f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 32 | Churn Cumulative: 1010 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -374,7 +374,9 @@ def standardize_dtype(dtype):\n     dtype = PYTHON_DTYPES_MAP.get(dtype, dtype)\n     if hasattr(dtype, \"name\"):\n         dtype = dtype.name\n-    elif hasattr(dtype, \"__str__\") and \"torch\" in str(dtype):\n+    elif hasattr(dtype, \"__str__\") and (\n+        \"torch\" in str(dtype) or \"jax.numpy\" in str(dtype)\n+    ):\n         dtype = str(dtype).split(\".\")[-1]\n \n     if dtype not in ALLOWED_DTYPES:\n\n@@ -2,9 +2,11 @@ from unittest.mock import patch\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras import backend\n from keras import initializers\n+from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.backend.common.variables import AutocastScope\n from keras.backend.common.variables import KerasVariable\n from keras.backend.common.variables import shape_equal\n@@ -78,7 +80,7 @@ class VariableInitializationTest(test_case.TestCase):\n             KerasVariable(initializer=lambda: np.ones((2, 2)))\n \n \n-class VariablePropertiesTest(test_case.TestCase):\n+class VariablePropertiesTest(test_case.TestCase, parameterized.TestCase):\n     \"\"\"Tests for KerasVariable._deferred_initialize\n     KerasVariable._maybe_autocast\"\"\"\n \n@@ -140,6 +142,30 @@ class VariablePropertiesTest(test_case.TestCase):\n         with AutocastScope(\"float16\"):\n             self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n \n+    @parameterized.parameters(\n+        *((dtype for dtype in ALLOWED_DTYPES if dtype != \"string\"))\n+    )\n+    def test_standardize_dtype(self, dtype):\n+        \"\"\"Tests standardize_dtype for all ALLOWED_DTYPES except string.\"\"\"\n+        if backend.backend() == \"torch\" and dtype in (\n+            \"uint16\",\n+            \"uint32\",\n+            \"uint64\",\n+        ):\n+            self.skipTest(f\"torch backend does not support dtype {dtype}\")\n+\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            if not jax.config.x64_enabled and \"64\" in dtype:\n+                self.skipTest(\n+                    f\"jax backend does not support {dtype} without x64 enabled\"\n+                )\n+\n+        x = backend.convert_to_tensor(np.zeros(()), dtype)\n+        actual = standardize_dtype(x.dtype)\n+        self.assertEqual(actual, dtype)\n+\n     def test_standardize_dtype_with_torch_dtype(self):\n         \"\"\"Tests dtype standardization with PyTorch dtypes.\"\"\"\n         import torch\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#972c3ce507ac17b45d478c71f5eba0d17738073f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 89 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -106,9 +106,9 @@ class ModelCheckpoint(Callback):\n             `\"min\"`, etc. In `\"auto\"` mode, the mode is set to `\"max\"` if the\n             quantities monitored are `\"acc\"` or start with `\"fmeasure\"` and are\n             set to `\"min\"` for the rest of the quantities.\n-        save_weights_only: if True, then only the model's weights will be saved\n-            (`model.save_weights(filepath)`), else the full model is saved\n-            (`model.save(filepath)`).\n+        save_weights_only: if `True`, then only the model's weights will be\n+            saved (`model.save_weights(filepath)`), else the full model is\n+            saved (`model.save(filepath)`).\n         save_freq: `\"epoch\"` or integer. When using `\"epoch\"`, the callback\n             saves the model after each epoch. When using integer, the callback\n             saves the model at end of this many batches. If the `Model` is\n@@ -177,6 +177,22 @@ class ModelCheckpoint(Callback):\n                 \"Expected save_freq are 'epoch' or integer values\"\n             )\n \n+        if save_weights_only:\n+            if not self.filepath.endswith(\".weights.h5\"):\n+                raise ValueError(\n+                    \"When using `save_weights_only=True` in `ModelCheckpoint`\"\n+                    \", the filepath provided must end in `.weights.h5` \"\n+                    \"(Keras weights format). Received: \"\n+                    f\"filepath={self.filepath}\"\n+                )\n+        else:\n+            if not self.filepath.endswith(\".keras\"):\n+                raise ValueError(\n+                    \"The filepath provided must end in `.keras` \"\n+                    \"(Keras model format). Received: \"\n+                    f\"filepath={self.filepath}\"\n+                )\n+\n     def on_train_batch_end(self, batch, logs=None):\n         if self._should_save_on_batch(batch):\n             self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n\n@@ -261,6 +261,7 @@ class ModelCheckpointTest(testing.TestCase):\n                 filepath,\n                 monitor=monitor,\n                 save_best_only=save_best_only,\n+                save_weights_only=True,\n                 mode=mode,\n                 save_freq=\"invalid_save_freq\",\n             )\n@@ -269,6 +270,7 @@ class ModelCheckpointTest(testing.TestCase):\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n+            save_weights_only=True,\n             mode=mode,\n             save_freq=\"epoch\",\n         )\n@@ -276,6 +278,7 @@ class ModelCheckpointTest(testing.TestCase):\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n+            save_weights_only=True,\n             mode=mode,\n             save_freq=3,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d779b997c1a1bc2067ecbf8ccbfc8b1683274cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 605 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 15 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 605 | Churn Cumulative: 605 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,605 @@\n+\"\"\"\n+Title: Point cloud segmentation with PointNet\n+Author: [Soumik Rakshit](https://github.com/soumik12345), [Sayak Paul](https://github.com/sayakpaul)\n+Date created: 2020/10/23\n+Last modified: 2020/10/24\n+Description: Implementation of a PointNet-based model for segmenting point clouds.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+A \"point cloud\" is an important type of data structure for storing geometric shape data.\n+Due to its irregular format, it's often transformed into\n+regular 3D voxel grids or collections of images before being used in deep learning applications,\n+a step which makes the data unnecessarily large.\n+The PointNet family of models solves this problem by directly consuming point clouds, respecting\n+the permutation-invariance property of the point data. The PointNet family of\n+models provides a simple, unified architecture\n+for applications ranging from **object classification**, **part segmentation**, to\n+**scene semantic parsing**.\n+\n+In this example, we demonstrate the implementation of the PointNet architecture\n+for shape segmentation.\n+\n+### References\n+\n+- [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)\n+- [Point cloud classification with PointNet](https://keras.io/examples/vision/pointnet/)\n+- [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)\n+\"\"\"\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import os\n+import json\n+import random\n+import numpy as np\n+import pandas as pd\n+from tqdm import tqdm\n+from glob import glob\n+\n+import tensorflow as tf  # For tf.data\n+import keras\n+from keras import layers\n+\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Downloading Dataset\n+\n+The [ShapeNet dataset](https://shapenet.org/) is an ongoing effort to establish a richly-annotated,\n+large-scale dataset of 3D shapes. **ShapeNetCore** is a subset of the full ShapeNet\n+dataset with clean single 3D models and manually verified category and alignment\n+annotations. It covers 55 common object categories, with about 51,300 unique 3D models.\n+\n+For this example, we use one of the 12 object categories of\n+[PASCAL 3D+](http://cvgl.stanford.edu/projects/pascal3d.html),\n+included as part of the ShapenetCore dataset.\n+\"\"\"\n+\n+dataset_url = \"https://git.io/JiY4i\"\n+\n+dataset_path = keras.utils.get_file(\n+    fname=\"shapenet.zip\",\n+    origin=dataset_url,\n+    cache_subdir=\"datasets\",\n+    hash_algorithm=\"auto\",\n+    extract=True,\n+    archive_format=\"auto\",\n+    cache_dir=\"datasets\",\n+)\n+\n+\"\"\"\n+## Loading the dataset\n+\n+We parse the dataset metadata in order to easily map model categories to their\n+respective directories and segmentation classes to colors for the purpose of\n+visualization.\n+\"\"\"\n+\n+with open(\"/tmp/.keras/datasets/PartAnnotation/metadata.json\") as json_file:\n+    metadata = json.load(json_file)\n+\n+print(metadata)\n+\n+\"\"\"\n+In this example, we train PointNet to segment the parts of an `Airplane` model.\n+\"\"\"\n+\n+points_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points\".format(\n+    metadata[\"Airplane\"][\"directory\"]\n+)\n+labels_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points_label\".format(\n+    metadata[\"Airplane\"][\"directory\"]\n+)\n+LABELS = metadata[\"Airplane\"][\"lables\"]\n+COLORS = metadata[\"Airplane\"][\"colors\"]\n+\n+VAL_SPLIT = 0.2\n+NUM_SAMPLE_POINTS = 1024\n+BATCH_SIZE = 32\n+EPOCHS = 60\n+INITIAL_LR = 1e-3\n+\n+\"\"\"\n+## Structuring the dataset\n+\n+We generate the following in-memory data structures from the Airplane point clouds and\n+their labels:\n+\n+- `point_clouds` is a list of `np.array` objects that represent the point cloud data in\n+the form of x, y and z coordinates. Axis 0 represents the number of points in the\n+point cloud, while axis 1 represents the coordinates. `all_labels` is the list\n+that represents the label of each coordinate as a string (needed mainly for\n+visualization purposes).\n+- `test_point_clouds` is in the same format as `point_clouds`, but doesn't have\n+corresponding the labels of the point clouds.\n+- `all_labels` is a list of `np.array` objects that represent the point cloud labels\n+for each coordinate, corresponding to the `point_clouds` list.\n+- `point_cloud_labels` is a list of `np.array` objects that represent the point cloud\n+labels for each coordinate in one-hot encoded form, corresponding to the `point_clouds`\n+list.\n+\"\"\"\n+\n+point_clouds, test_point_clouds = [], []\n+point_cloud_labels, all_labels = [], []\n+\n+points_files = glob(os.path.join(points_dir, \"*.pts\"))\n+for point_file in tqdm(points_files):\n+    point_cloud = np.loadtxt(point_file)\n+    if point_cloud.shape[0] < NUM_SAMPLE_POINTS:\n+        continue\n+\n+    # Get the file-id of the current point cloud for parsing its\n+    # labels.\n+    file_id = point_file.split(\"/\")[-1].split(\".\")[0]\n+    label_data, num_labels = {}, 0\n+    for label in LABELS:\n+        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n+        if os.path.exists(label_file):\n+            label_data[label] = np.loadtxt(label_file).astype(\"float32\")\n+            num_labels = len(label_data[label])\n+\n+    # Point clouds having labels will be our training samples.\n+    try:\n+        label_map = [\"none\"] * num_labels\n+        for label in LABELS:\n+            for i, data in enumerate(label_data[label]):\n+                label_map[i] = label if data == 1 else label_map[i]\n+        label_data = [\n+            LABELS.index(label) if label != \"none\" else len(LABELS)\n+            for label in label_map\n+        ]\n+        # Apply one-hot encoding to the dense label representation.\n+        label_data = keras.utils.to_categorical(label_data, num_classes=len(LABELS) + 1)\n+\n+        point_clouds.append(point_cloud)\n+        point_cloud_labels.append(label_data)\n+        all_labels.append(label_map)\n+    except KeyError:\n+        test_point_clouds.append(point_cloud)\n+\n+\"\"\"\n+Next, we take a look at some samples from the in-memory arrays we just generated:\n+\"\"\"\n+\n+for _ in range(5):\n+    i = random.randint(0, len(point_clouds) - 1)\n+    print(f\"point_clouds[{i}].shape:\", point_clouds[0].shape)\n+    print(f\"point_cloud_labels[{i}].shape:\", point_cloud_labels[0].shape)\n+    for j in range(5):\n+        print(\n+            f\"all_labels[{i}][{j}]:\",\n+            all_labels[i][j],\n+            f\"\\tpoint_cloud_labels[{i}][{j}]:\",\n+            point_cloud_labels[i][j],\n+            \"\\n\",\n+        )\n+\n+\"\"\"\n+Now, let's visualize some of the point clouds along with their labels.\n+\"\"\"\n+\n+\n+def visualize_data(point_cloud, labels):\n+    df = pd.DataFrame(\n+        data={\n+            \"x\": point_cloud[:, 0],\n+            \"y\": point_cloud[:, 1],\n+            \"z\": point_cloud[:, 2],\n+            \"label\": labels,\n+        }\n+    )\n+    fig = plt.figure(figsize=(15, 10))\n+    ax = plt.axes(projection=\"3d\")\n+    for index, label in enumerate(LABELS):\n+        c_df = df[df[\"label\"] == label]\n+        try:\n+            ax.scatter(\n+                c_df[\"x\"], c_df[\"y\"], c_df[\"z\"], label=label, alpha=0.5, c=COLORS[index]\n+            )\n+        except IndexError:\n+            pass\n+    ax.legend()\n+    plt.show()\n+\n+\n+visualize_data(point_clouds[0], all_labels[0])\n+visualize_data(point_clouds[300], all_labels[300])\n+\n+\"\"\"\n+### Preprocessing\n+\n+Note that all the point clouds that we have loaded consist of a variable number of points,\n+which makes it difficult for us to batch them together. In order to overcome this problem, we\n+randomly sample a fixed number of points from each point cloud. We also normalize the\n+point clouds in order to make the data scale-invariant.\n+\"\"\"\n+\n+for index in tqdm(range(len(point_clouds))):\n+    current_point_cloud = point_clouds[index]\n+    current_label_cloud = point_cloud_labels[index]\n+    current_labels = all_labels[index]\n+    num_points = len(current_point_cloud)\n+    # Randomly sampling respective indices.\n+    sampled_indices = random.sample(list(range(num_points)), NUM_SAMPLE_POINTS)\n+    # Sampling points corresponding to sampled indices.\n+    sampled_point_cloud = np.array([current_point_cloud[i] for i in sampled_indices])\n+    # Sampling corresponding one-hot encoded labels.\n+    sampled_label_cloud = np.array([current_label_cloud[i] for i in sampled_indices])\n+    # Sampling corresponding labels for visualization.\n+    sampled_labels = np.array([current_labels[i] for i in sampled_indices])\n+    # Normalizing sampled point cloud.\n+    norm_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n+    norm_point_cloud /= np.max(np.linalg.norm(norm_point_cloud, axis=1))\n+    point_clouds[index] = norm_point_cloud\n+    point_cloud_labels[index] = sampled_label_cloud\n+    all_labels[index] = sampled_labels\n+\n+\"\"\"\n+Let's visualize the sampled and normalized point clouds along with their corresponding\n+labels.\n+\"\"\"\n+\n+visualize_data(point_clouds[0], all_labels[0])\n+visualize_data(point_clouds[300], all_labels[300])\n+\n+\"\"\"\n+### Creating TensorFlow datasets\n+\n+We create `tf.data.Dataset` objects for the training and validation data.\n+We also augment the training point clouds by applying random jitter to them.\n+\"\"\"\n+\n+\n+def load_data(point_cloud_batch, label_cloud_batch):\n+    point_cloud_batch.set_shape([NUM_SAMPLE_POINTS, 3])\n+    label_cloud_batch.set_shape([NUM_SAMPLE_POINTS, len(LABELS) + 1])\n+    return point_cloud_batch, label_cloud_batch\n+\n+\n+def augment(point_cloud_batch, label_cloud_batch):\n+    noise = tf.random.uniform(\n+        tf.shape(label_cloud_batch), -0.005, 0.005, dtype=tf.float64\n+    )\n+    point_cloud_batch += noise[:, :, :3]\n+    return point_cloud_batch, label_cloud_batch\n+\n+\n+def generate_dataset(point_clouds, label_clouds, is_training=True):\n+    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n+    dataset = dataset.shuffle(BATCH_SIZE * 100) if is_training else dataset\n+    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n+    dataset = dataset.batch(batch_size=BATCH_SIZE)\n+    dataset = (\n+        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n+        if is_training\n+        else dataset\n+    )\n+    return dataset\n+\n+\n+split_index = int(len(point_clouds) * (1 - VAL_SPLIT))\n+train_point_clouds = point_clouds[:split_index]\n+train_label_cloud = point_cloud_labels[:split_index]\n+total_training_examples = len(train_point_clouds)\n+\n+val_point_clouds = point_clouds[split_index:]\n+val_label_cloud = point_cloud_labels[split_index:]\n+\n+print(\"Num train point clouds:\", len(train_point_clouds))\n+print(\"Num train point cloud labels:\", len(train_label_cloud))\n+print(\"Num val point clouds:\", len(val_point_clouds))\n+print(\"Num val point cloud labels:\", len(val_label_cloud))\n+\n+train_dataset = generate_dataset(train_point_clouds, train_label_cloud)\n+val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False)\n+\n+print(\"Train Dataset:\", train_dataset)\n+print(\"Validation Dataset:\", val_dataset)\n+\n+\"\"\"\n+## PointNet model\n+\n+The figure below depicts the internals of the PointNet model family:\n+\n+![](https://i.imgur.com/qFLNw5L.png)\n+\n+Given that PointNet is meant to consume an ***unordered set*** of coordinates as its input data,\n+its architecture needs to match the following characteristic properties\n+of point cloud data:\n+\n+### Permutation invariance\n+\n+Given the unstructured nature of point cloud data, a scan made up of `n` points has `n!`\n+permutations. The subsequent data processing must be invariant to the different\n+representations. In order to make PointNet invariant to input permutations, we use a\n+symmetric function (such as max-pooling) once the `n` input points are mapped to\n+higher-dimensional space. The result is a **global feature vector** that aims to capture\n+an aggregate signature of the `n` input points. The global feature vector is used alongside\n+local point features for segmentation.\n+\n+![](https://i.imgur.com/0mrvvjb.png)\n+\n+### Transformation invariance\n+\n+Segmentation outputs should be unchanged if the object undergoes certain transformations,\n+such as translation or scaling. For a given input point cloud, we apply an appropriate\n+rigid or affine transformation to achieve pose normalization. Because each of the `n` input\n+points are represented as a vector and are mapped to the embedding spaces independently,\n+applying a geometric transformation simply amounts to matrix multiplying each point with\n+a transformation matrix. This is motivated by the concept of\n+[Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n+\n+The operations comprising the T-Net are motivated by the higher-level architecture of\n+PointNet. MLPs (or fully-connected layers) are used to map the input points independently\n+and identically to a higher-dimensional space; max-pooling is used to encode a global\n+feature vector whose dimensionality is then reduced with fully-connected layers. The\n+input-dependent features at the final fully-connected layer are then combined with\n+globally trainable weights and biases, resulting in a 3-by-3 transformation matrix.\n+\n+![](https://i.imgur.com/aEj3GYi.png)\n+\n+### Point interactions\n+\n+The interaction between neighboring points often carries useful information (i.e., a\n+single point should not be treated in isolation). Whereas classification need only make\n+use of global features, segmentation must be able to leverage local point features along\n+with global point features.\n+\n+\n+**Note**: The figures presented in this section have been taken from the\n+[original paper](https://arxiv.org/abs/1612.00593).\n+\"\"\"\n+\n+\"\"\"\n+Now that we know the pieces that compose the PointNet model, we can implement the model.\n+We start by implementing the basic blocks i.e., the convolutional block and the multi-layer\n+perceptron block.\n+\"\"\"\n+\n+\n+def conv_block(x, filters, name):\n+    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=f\"{name}_conv\")(x)\n+    x = layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n+    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n+\n+\n+def mlp_block(x, filters, name):\n+    x = layers.Dense(filters, name=f\"{name}_dense\")(x)\n+    x = layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n+    return layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n+\n+\n+\"\"\"\n+We implement a regularizer (taken from\n+[this example](https://keras.io/examples/vision/pointnet/#build-a-model))\n+to enforce orthogonality in the feature space. This is needed to ensure\n+that the magnitudes of the transformed features do not vary too much.\n+\"\"\"\n+\n+\n+class OrthogonalRegularizer(keras.regularizers.Regularizer):\n+    \"\"\"Reference: https://keras.io/examples/vision/pointnet/#build-a-model\"\"\"\n+\n+    def __init__(self, num_features, l2reg=0.001):\n+        self.num_features = num_features\n+        self.l2reg = l2reg\n+        self.identity = keras.ops.eye(num_features)\n+\n+    def __call__(self, x):\n+        x = keras.ops.reshape(x, (-1, self.num_features, self.num_features))\n+        xxt = keras.ops.tensordot(x, x, axes=(2, 2))\n+        xxt = keras.ops.reshape(xxt, (-1, self.num_features, self.num_features))\n+        return keras.ops.sum(self.l2reg * keras.ops.square(xxt - self.identity))\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg})\n+        return config\n+\n+\n+\"\"\"\n+The next piece is the transformation network which we explained earlier.\n+\"\"\"\n+\n+\n+def transformation_net(inputs, num_features, name):\n+    \"\"\"\n+    Reference: https://keras.io/examples/vision/pointnet/#build-a-model.\n+\n+    The `filters` values come from the original paper:\n+    https://arxiv.org/abs/1612.00593.\n+    \"\"\"\n+    x = conv_block(inputs, filters=64, name=f\"{name}_1\")\n+    x = conv_block(x, filters=128, name=f\"{name}_2\")\n+    x = conv_block(x, filters=1024, name=f\"{name}_3\")\n+    x = layers.GlobalMaxPooling1D()(x)\n+    x = mlp_block(x, filters=512, name=f\"{name}_1_1\")\n+    x = mlp_block(x, filters=256, name=f\"{name}_2_1\")\n+    return layers.Dense(\n+        num_features * num_features,\n+        kernel_initializer=\"zeros\",\n+        bias_initializer=keras.initializers.Constant(np.eye(num_features).flatten()),\n+        activity_regularizer=OrthogonalRegularizer(num_features),\n+        name=f\"{name}_final\",\n+    )(x)\n+\n+\n+def transformation_block(inputs, num_features, name):\n+    transformed_features = transformation_net(inputs, num_features, name=name)\n+    transformed_features = layers.Reshape((num_features, num_features))(\n+        transformed_features\n+    )\n+    return layers.Dot(axes=(2, 1), name=f\"{name}_mm\")([inputs, transformed_features])\n+\n+\n+\"\"\"\n+Finally, we piece the above blocks together and implement the segmentation model.\n+\"\"\"\n+\n+\n+def get_shape_segmentation_model(num_points, num_classes):\n+    input_points = keras.Input(shape=(None, 3))\n+\n+    # PointNet Classification Network.\n+    transformed_inputs = transformation_block(\n+        input_points, num_features=3, name=\"input_transformation_block\"\n+    )\n+    features_64 = conv_block(transformed_inputs, filters=64, name=\"features_64\")\n+    features_128_1 = conv_block(features_64, filters=128, name=\"features_128_1\")\n+    features_128_2 = conv_block(features_128_1, filters=128, name=\"features_128_2\")\n+    transformed_features = transformation_block(\n+        features_128_2, num_features=128, name=\"transformed_features\"\n+    )\n+    features_512 = conv_block(transformed_features, filters=512, name=\"features_512\")\n+    features_2048 = conv_block(features_512, filters=2048, name=\"pre_maxpool_block\")\n+    global_features = layers.MaxPool1D(pool_size=num_points, name=\"global_features\")(\n+        features_2048\n+    )\n+    global_features = keras.ops.tile(global_features, [1, num_points, 1])\n+\n+    # Segmentation head.\n+    segmentation_input = layers.Concatenate(name=\"segmentation_input\")(\n+        [\n+            features_64,\n+            features_128_1,\n+            features_128_2,\n+            transformed_features,\n+            features_512,\n+            global_features,\n+        ]\n+    )\n+    segmentation_features = conv_block(\n+        segmentation_input, filters=128, name=\"segmentation_features\"\n+    )\n+    outputs = layers.Conv1D(\n+        num_classes, kernel_size=1, activation=\"softmax\", name=\"segmentation_head\"\n+    )(segmentation_features)\n+    return keras.Model(input_points, outputs)\n+\n+\n+\"\"\"\n+## Instantiate the model\n+\"\"\"\n+\n+x, y = next(iter(train_dataset))\n+\n+num_points = x.shape[1]\n+num_classes = y.shape[-1]\n+\n+segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n+segmentation_model.summary()\n+\n+\"\"\"\n+## Training\n+\n+For the training the authors recommend using a learning rate schedule that decays the\n+initial learning rate by half every 20 epochs. In this example, we resort to 15 epochs.\n+\"\"\"\n+\n+training_step_size = total_training_examples // BATCH_SIZE\n+total_training_steps = training_step_size * EPOCHS\n+print(f\"Total training steps: {total_training_steps}.\")\n+\n+lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n+    boundaries=[training_step_size * 15, training_step_size * 15],\n+    values=[INITIAL_LR, INITIAL_LR * 0.5, INITIAL_LR * 0.25],\n+)\n+\n+steps = range(total_training_steps)\n+lrs = [lr_schedule(step) for step in steps]\n+\n+plt.plot(lrs)\n+plt.xlabel(\"Steps\")\n+plt.ylabel(\"Learning Rate\")\n+plt.show()\n+\n+\"\"\"\n+Finally, we implement a utility for running our experiments and launch model training.\n+\"\"\"\n+\n+\n+def run_experiment(epochs):\n+    segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n+    segmentation_model.compile(\n+        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n+        loss=keras.losses.CategoricalCrossentropy(),\n+        metrics=[\"accuracy\"],\n+    )\n+\n+    checkpoint_filepath = \"checkpoint.weights.h5\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_loss\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    history = segmentation_model.fit(\n+        train_dataset,\n+        validation_data=val_dataset,\n+        epochs=epochs,\n+        callbacks=[checkpoint_callback],\n+    )\n+\n+    segmentation_model.load_weights(checkpoint_filepath)\n+    return segmentation_model, history\n+\n+\n+segmentation_model, history = run_experiment(epochs=EPOCHS)\n+\n+\"\"\"\n+## Visualize the training landscape\n+\"\"\"\n+\n+\n+def plot_result(item):\n+    plt.plot(history.history[item], label=item)\n+    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(item)\n+    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_result(\"loss\")\n+plot_result(\"accuracy\")\n+\n+\"\"\"\n+## Inference\n+\"\"\"\n+\n+validation_batch = next(iter(val_dataset))\n+val_predictions = segmentation_model.predict(validation_batch[0])\n+print(f\"Validation prediction shape: {val_predictions.shape}\")\n+\n+\n+def visualize_single_point_cloud(point_clouds, label_clouds, idx):\n+    label_map = LABELS + [\"none\"]\n+    point_cloud = point_clouds[idx]\n+    label_cloud = label_clouds[idx]\n+    visualize_data(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])\n+\n+\n+idx = np.random.choice(len(validation_batch[0]))\n+print(f\"Index selected: {idx}\")\n+\n+# Plotting with ground-truth.\n+visualize_single_point_cloud(validation_batch[0], validation_batch[1], idx)\n+\n+# Plotting with predicted labels.\n+visualize_single_point_cloud(validation_batch[0], val_predictions, idx)\n+\n+\"\"\"\n+## Final notes\n+\n+If you are interested in learning more about this topic, you may find\n+[this repository](https://github.com/soumik12345/point-cloud-segmentation)\n+useful.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
