{"custom_id": "keras#3e69e6833816d4d390ee3da6a1d2a68ceb10fc5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 177 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 15 | Methods Changed: 15 | Complexity Δ (Sum/Max): 24/7 | Churn Δ: 190 | Churn Cumulative: 3050 | Contributors (this commit): 8 | Commits (past 90d): 108 | Contributors (cumulative): 28 | DMM Complexity: 0.8053097345132744\n\nDIFF:\n@@ -58,8 +58,6 @@ class Layer(Operation):\n         self._seed_generators = []\n         self._losses = []\n         self._variables = []\n-        self._trainable_variables = []\n-        self._non_trainable_variables = []\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         self._build_shapes_dict = None\n         self._call_signature_parameters = [\n@@ -448,6 +446,39 @@ class Layer(Operation):\n         losses.extend(weight_regularization_losses)\n         return losses\n \n+    def save_own_variables(self, store):\n+        \"\"\"Saves the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is saved upon calling `model.save()`.\n+\n+        Args:\n+            store: Dict where the state of the model will be saved.\n+        \"\"\"\n+        all_vars = self._variables\n+        for i, v in enumerate(all_vars):\n+            store[f\"{i}\"] = np.array(v)\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Loads the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is loaded upon calling `keras.models.load_model()`.\n+\n+        Args:\n+            store: Dict from which the state of the model will be loaded.\n+        \"\"\"\n+        all_vars = self._variables\n+        if len(store.keys()) != len(all_vars):\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n+        for i, v in enumerate(all_vars):\n+            v.assign(store[f\"{i}\"])\n+\n     def _clear_losses(self):\n         if backend.in_stateless_scope():\n             scope = backend.get_stateless_scope()\n\n@@ -1,10 +1,12 @@\n+import copy\n+\n from tensorflow import nest\n \n-from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.core.input_layer import InputLayer\n from keras_core.models.functional import Functional\n from keras_core.models.model import Model\n+from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n@@ -162,8 +164,40 @@ class Sequential(Model):\n         return True\n \n     def get_config(self):\n-        raise NotImplementedError\n+        layer_configs = []\n+        for layer in super().layers:\n+            # `super().layers` include the InputLayer if available (it is\n+            # filtered out of `self.layers`).\n+            layer_configs.append(\n+                serialization_lib.serialize_keras_object(layer)\n+            )\n+        config = Model.get_config(self)\n+        config[\"name\"] = self.name\n+        config[\"layers\"] = copy.deepcopy(layer_configs)\n+        if self._functional is not None:\n+            config[\"build_input_shape\"] = self._layers[0].batch_shape\n+        return config\n \n     @classmethod\n-    def from_config(cls, config):\n-        raise NotImplementedError\n+    def from_config(cls, config, custom_objects=None):\n+        if \"name\" in config:\n+            name = config[\"name\"]\n+            build_input_shape = config.get(\"build_input_shape\")\n+            layer_configs = config[\"layers\"]\n+        else:\n+            name = None\n+            layer_configs = config\n+        model = cls(name=name)\n+        for layer_config in layer_configs:\n+            layer = serialization_lib.deserialize_keras_object(\n+                layer_config,\n+                custom_objects=custom_objects,\n+            )\n+            model.add(layer)\n+        if (\n+            not model._functional\n+            and build_input_shape\n+            and isinstance(build_input_shape, (tuple, list))\n+        ):\n+            model.build(build_input_shape)\n+        return model\n\n@@ -170,5 +170,35 @@ class SequentialTest(testing.TestCase):\n         model.summary()\n \n     def test_serialization(self):\n-        # TODO\n-        pass\n+        # Unbuilt deferred\n+        model = Sequential(name=\"seq\")\n+        model.add(layers.Dense(4))\n+        model.add(layers.Dense(5))\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 2)\n+\n+        # Built deferred\n+        model.build((2, 3))\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 2)\n+\n+        # Regular\n+        model = Sequential(name=\"seq\")\n+        model.add(Input(shape=(2,), batch_size=3))\n+        model.add(layers.Dense(4))\n+        model.add(layers.Dense(5))\n+        model.add(layers.Dense(6))\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 3)\n+\n+        # Weird\n+        class DictLayer(layers.Layer):\n+            def call(self, inputs):\n+                assert isinstance(inputs, dict)\n+                return inputs\n+\n+        model = Sequential([DictLayer()])\n+        revived = self.run_class_serialization_test(\n+            model, custom_objects={\"DictLayer\": DictLayer}\n+        )\n+        self.assertLen(revived.layers, 1)\n\n@@ -1,6 +1,8 @@\n import re\n import warnings\n \n+import numpy as np\n+\n from keras_core import backend\n from keras_core import initializers\n from keras_core import operations as ops\n@@ -274,6 +276,29 @@ class Optimizer:\n     def learning_rate(self):\n         return self._get_current_learning_rate()\n \n+    def save_own_variables(self, store):\n+        \"\"\"Get the state of this optimizer object.\"\"\"\n+        for i, variable in enumerate(self.variables):\n+            store[str(i)] = np.array(variable)\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Set the state of this optimizer object.\"\"\"\n+        if len(store.keys()) != len(self.variables):\n+            msg = (\n+                f\"Skipping variable loading for optimizer '{self.name}', \"\n+                f\"because it has {len(self.variables)} variables whereas \"\n+                f\"the saved optimizer has {len(store.keys())} variables. \"\n+            )\n+            if len(self.variables) == 0:\n+                msg += (\n+                    \"This is likely because the optimizer has not been \"\n+                    \"called/built yet.\"\n+                )\n+            warnings.warn(msg, stacklevel=2)\n+            return\n+        for i, variable in enumerate(self.variables):\n+            variable.assign(store[str(i)])\n+\n     def _get_current_learning_rate(self):\n         if isinstance(\n             self._learning_rate, learning_rate_schedule.LearningRateSchedule\n\n@@ -49,6 +49,7 @@ class TestCase(unittest.TestCase):\n         )\n         self.assertEqual(config_json, revived_config_json)\n         self.assertEqual(ref_dir, dir(revived_instance))\n+        return revived_instance\n \n     def run_layer_test(\n         self,\n\n@@ -3,6 +3,7 @@ import warnings\n from keras_core import backend\n from keras_core import metrics as metrics_module\n from keras_core import operations as ops\n+from keras_core.saving import serialization_lib\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n from keras_core.utils import tracking\n@@ -48,6 +49,16 @@ class Trainer:\n         self.compiled = True\n         self._loss_tracker = metrics_module.Mean(name=\"loss\")\n \n+        self._compile_config = serialization_lib.SerializableDict(\n+            optimizer=optimizer,\n+            loss=loss,\n+            loss_weights=loss_weights,\n+            metrics=metrics,\n+            weighted_metrics=weighted_metrics,\n+            run_eagerly=run_eagerly,\n+            jit_compile=jit_compile,\n+        )\n+\n     @property\n     def jit_compile(self):\n         return self._jit_compile\n@@ -249,12 +260,44 @@ class Trainer:\n         raise NotImplementedError\n \n     def get_compile_config(self):\n-        # TODO\n-        pass\n+        \"\"\"Returns a serialized config with information for compiling the model.\n \n-    def compile_from_config(self):\n-        # TODO\n-        return {}\n+        This method returns a config dictionary containing all the information\n+        (optimizer, loss, metrics, etc.) with which the model was compiled.\n+\n+        Returns:\n+            A dict containing information for compiling the model.\n+        \"\"\"\n+        if self.compiled and hasattr(self, \"_compile_config\"):\n+            return self._compile_config.serialize()\n+\n+    def compile_from_config(self, config):\n+        \"\"\"Compiles the model with the information given in config.\n+\n+        This method uses the information in the config (optimizer, loss,\n+        metrics, etc.) to compile the model.\n+\n+        Args:\n+            config: Dict containing information for compiling the model.\n+        \"\"\"\n+        has_overridden_compile = self.__class__.compile != Trainer.compile\n+        if has_overridden_compile:\n+            warnings.warn(\n+                \"`compile()` was not called as part of model loading \"\n+                \"because the model's `compile()` method is custom. \"\n+                \"All subclassed Models that have `compile()` \"\n+                \"overridden should also override \"\n+                \"`get_compile_config()` and `compile_from_config(config)`. \"\n+                \"Alternatively, you can \"\n+                \"call `compile()` manually after loading.\",\n+                stacklevel=2,\n+            )\n+            return\n+        config = serialization_lib.deserialize_keras_object(config)\n+        self.compile(**config)\n+        if hasattr(self, \"optimizer\") and self.built:\n+            # Create optimizer variables.\n+            self.optimizer.build(self.trainable_variables)\n \n     def _should_eval(self, epoch, validation_freq):\n         epoch = epoch + 1  # one-index the user-facing epoch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c5863e8fcaaf079c82bad1268486aee3ec726da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 53 | Churn Cumulative: 346 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: 0.3023255813953488\n\nDIFF:\n@@ -1,6 +1,10 @@\n+import os\n+\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.saving import saving_lib\n+from keras_core.utils import io_utils\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n@@ -153,8 +157,53 @@ class Model(Trainer, Layer):\n             layer_range=layer_range,\n         )\n \n-    def save(self, filepath):\n-        raise NotImplementedError\n+    def save(self, filepath, overwrite=True):\n+        if not filepath.endswith(\".keras\"):\n+            raise ValueError(\n+                \"The filename must end in `.keras`. \"\n+                f\"Received: filepath={filepath}\"\n+            )\n+        try:\n+            exists = os.path.exists(filepath)\n+        except TypeError:\n+            exists = False\n+        if exists and not overwrite:\n+            proceed = io_utils.ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        saving_lib.save_model(self, filepath)\n+\n+    def save_weights(self, filepath, overwrite=True):\n+        if not filepath.endswith(\".weights.h5\"):\n+            raise ValueError(\n+                \"The filename must end in `.weights.h5`. \"\n+                f\"Received: filepath={filepath}\"\n+            )\n+        try:\n+            exists = os.path.exists(filepath)\n+        except TypeError:\n+            exists = False\n+        if exists and not overwrite:\n+            proceed = io_utils.ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        saving_lib.save_weights_only(self, filepath)\n+\n+    def load_weights(self, filepath, skip_mismatch=False):\n+        if str(filepath).endswith(\".keras\"):\n+            saving_lib.load_weights_only(\n+                self, filepath, skip_mismatch=skip_mismatch\n+            )\n+        elif str(filepath).endswith(\".weights.h5\"):\n+            saving_lib.load_weights_only(\n+                self, filepath, skip_mismatch=skip_mismatch\n+            )\n+        else:\n+            raise ValueError(\n+                f\"File format not supported: filepath={filepath}. \"\n+                \"Keras Core only supports V3 `.keras` and `.weights.h5` \"\n+                \"files.\"\n+            )\n \n     def export(self, filepath):\n         raise NotImplementedError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08f9b1a2f8248b2158b50d02312129551997f596", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 75/55 | Churn Δ: 7 | Churn Cumulative: 3049 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,6 @@ from keras.dtensor import utils\n from keras.engine import base_layer\n \n # isort: off\n-from tensorflow.python.util.deprecation import deprecated\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -235,10 +234,6 @@ class LayoutMap(collections.abc.MutableMapping):\n LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n \n \n-@keras_export(\"keras.dtensor.experimental.layout_map_scope\", v1=[])\n-@deprecated(\n-    None, \"use tf.keras.dtensor.experimental.LayoutMap.scope() instead.\"\n-)\n @contextlib.contextmanager\n def layout_map_scope(layout_map):\n     \"\"\"Apply the layout to all the tf.Variables created under the scope.\n\n@@ -340,7 +340,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n     def test_weight_regularization(self):\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = models.Sequential(\n                 [\n                     layers.Dense(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ff7cccb08f3e35fe3c8b730a288e0522c005ef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1231 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -73,8 +73,8 @@ def load_data(\n       maxlen: int or None. Maximum sequence length.\n           Any longer sequence will be truncated. None means no truncation.\n           Defaults to `None`.\n-      test_split: Float between `0.` and `1.`. Fraction of the dataset to be used\n-        as test data. 0.2 means that 20% of the dataset is used as\n+      test_split: Float between `0.` and `1.`. Fraction of the dataset to be\n+        used as test data. `0.2` means that 20% of the dataset is used as\n         test data. Defaults to `0.2`.\n       seed: int. Seed for reproducible data shuffling.\n       start_char: int. The start of a sequence will be marked with this\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3788a9958261b3400e345972e53bb80ff2516b3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 110 | Churn Cumulative: 1975 | Contributors (this commit): 8 | Commits (past 90d): 65 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,75 @@\n from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n+from keras_core.losses.losses import CategoricalHinge\n+from keras_core.losses.losses import Hinge\n from keras_core.losses.losses import LossFunctionWrapper\n+from keras_core.losses.losses import MeanAbsoluteError\n+from keras_core.losses.losses import MeanAbsolutePercentageError\n from keras_core.losses.losses import MeanSquaredError\n+from keras_core.losses.losses import MeanSquaredLogarithmicError\n+from keras_core.losses.losses import SquaredHinge\n+from keras_core.losses.losses import categorical_hinge\n+from keras_core.losses.losses import hinge\n+from keras_core.losses.losses import mean_absolute_error\n+from keras_core.losses.losses import mean_absolute_percentage_error\n+from keras_core.losses.losses import mean_squared_error\n+from keras_core.losses.losses import mean_squared_logarithmic_error\n+from keras_core.losses.losses import squared_hinge\n+from keras_core.saving import serialization_lib\n+\n+ALL_OBJECTS = {\n+    Loss,\n+    LossFunctionWrapper,\n+    MeanSquaredError,\n+    MeanAbsoluteError,\n+    MeanAbsolutePercentageError,\n+    MeanSquaredLogarithmicError,\n+    Hinge,\n+    SquaredHinge,\n+    CategoricalHinge,\n+    mean_squared_error,\n+    mean_absolute_error,\n+    mean_absolute_percentage_error,\n+    mean_squared_logarithmic_error,\n+    hinge,\n+    squared_hinge,\n+    categorical_hinge,\n+}\n+\n+ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n+@keras_core_export(\"keras_core.losses.serialize\")\n+def serialize(loss):\n+    \"\"\"Serializes loss function or `Loss` instance.\n+\n+    Args:\n+        loss: A Keras `Loss` instance or a loss function.\n+\n+    Returns:\n+        Loss configuration dictionary.\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(loss)\n+\n+\n+@keras_core_export(\"keras_core.losses.deserialize\")\n+def deserialize(name, custom_objects=None):\n+    \"\"\"Deserializes a serialized loss class/function instance.\n+\n+    Args:\n+        name: Loss configuration.\n+        custom_objects: Optional dictionary mapping names (strings) to custom\n+          objects (classes and functions) to be considered during\n+          deserialization.\n+\n+    Returns:\n+        A Keras `Loss` instance or a loss function.\n+    \"\"\"\n+    return serialization_lib.deserialize_keras_object(\n+        name,\n+        module_objects=ALL_OBJECTS_DICT,\n+        custom_objects=custom_objects,\n+    )\n \n \n @keras_core_export(\"keras_core.losses.get\")\n\n@@ -45,7 +45,7 @@ class Loss:\n         raise NotImplementedError\n \n     def get_config(self):\n-        return {\"name\": self.name}\n+        return {\"name\": self.name, \"reduction\": self.reduction}\n \n     @classmethod\n     def from_config(cls, config):\n\n@@ -3,6 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n from keras_core.losses.loss import squeeze_to_same_rank\n+from keras_core.saving import serialization_lib\n \n \n class LossFunctionWrapper(Loss):\n@@ -18,11 +19,16 @@ class LossFunctionWrapper(Loss):\n         return self.fn(y_true, y_pred, **self._fn_kwargs)\n \n     def get_config(self):\n-        raise NotImplementedError\n+        base_config = super().get_config()\n+        config = {\"fn\": serialization_lib.serialize_keras_object(self.fn)}\n+        config.update(serialization_lib.serialize_keras_object(self._fn_kwargs))\n+        return {**base_config, **config}\n \n     @classmethod\n-    def from_config(clf, config):\n-        raise NotImplementedError\n+    def from_config(cls, config):\n+        if \"fn\" in config:\n+            config = serialization_lib.deserialize_keras_object(config)\n+        return cls(**config)\n \n \n @keras_core_export(\"keras_core.losses.MeanSquaredError\")\n@@ -47,6 +53,9 @@ class MeanSquaredError(LossFunctionWrapper):\n     ):\n         super().__init__(mean_squared_error, reduction=reduction, name=name)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.MeanAbsoluteError\")\n class MeanAbsoluteError(LossFunctionWrapper):\n@@ -70,6 +79,9 @@ class MeanAbsoluteError(LossFunctionWrapper):\n     ):\n         super().__init__(mean_absolute_error, reduction=reduction, name=name)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(LossFunctionWrapper):\n@@ -97,6 +109,9 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n             mean_absolute_percentage_error, reduction=reduction, name=name\n         )\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n@@ -124,6 +139,9 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n             mean_squared_logarithmic_error, reduction=reduction, name=name\n         )\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.Hinge\")\n class Hinge(LossFunctionWrapper):\n@@ -148,6 +166,9 @@ class Hinge(LossFunctionWrapper):\n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n         super().__init__(hinge, reduction=reduction, name=name)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.SquaredHinge\")\n class SquaredHinge(LossFunctionWrapper):\n@@ -172,6 +193,9 @@ class SquaredHinge(LossFunctionWrapper):\n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n         super().__init__(squared_hinge, reduction=reduction, name=name)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n @keras_core_export(\"keras_core.losses.CategoricalHinge\")\n class CategoricalHinge(LossFunctionWrapper):\n@@ -198,6 +222,9 @@ class CategoricalHinge(LossFunctionWrapper):\n     ):\n         super().__init__(categorical_hinge, reduction=reduction, name=name)\n \n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n \n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n\n@@ -1,4 +1,6 @@\n import json\n+import os\n+import tempfile\n import unittest\n \n import numpy as np\n@@ -8,6 +10,11 @@ from tensorflow import nest\n class TestCase(unittest.TestCase):\n     maxDiff = None\n \n+    def get_temp_dir(self):\n+        temp_dir = tempfile.mkdtemp()\n+        self.addCleanup(lambda: os.rmdir(temp_dir))\n+        return temp_dir\n+\n     def assertAllClose(self, x1, x2, atol=1e-7, rtol=1e-7):\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7dadbe62be32ce545b13e7552063aa4be038588", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2634 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -670,7 +670,10 @@ def get_training_or_validation_split(samples, labels, validation_split, subset):\n \n     num_val_samples = int(validation_split * len(samples))\n     if subset == \"training\":\n-        io_utils.print_msg(f\"Using {len(samples) - num_val_samples} files for training.\")\n+    \tio_utils.print_msg(\n+                f\"Using {len(samples) - num_val_samples} \"\n+                f\"files for training.\"\n+        )\n         samples = samples[:-num_val_samples]\n         labels = labels[:-num_val_samples]\n     elif subset == \"validation\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83c356a79162d5c38c0074d28e8fc3c637c6dbe4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 363 | Lines Deleted: 159 | Files Changed: 6 | Hunks: 30 | Methods Changed: 25 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 522 | Churn Cumulative: 6667 | Contributors (this commit): 5 | Commits (past 90d): 31 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core.backend.common import backend_utils\n from keras_core.backend.common import random\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import standardize_dtype\n\n@@ -0,0 +1,79 @@\n+def _compute_conv_transpose_output_length(\n+    input_length,\n+    kernel_size,\n+    padding,\n+    output_padding=None,\n+    stride=1,\n+    dilation=1,\n+):\n+    \"\"\"Computes output size of a transposed convolution given input size.\"\"\"\n+    assert padding in {\"same\", \"valid\"}\n+    if input_length is None:\n+        return None\n+\n+    # Get the dilated kernel size\n+    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n+\n+    # Infer length if output padding is None, else compute the exact length\n+    if output_padding is None:\n+        if padding == \"valid\":\n+            length = input_length * stride + max(kernel_size - stride, 0)\n+        else:\n+            length = input_length * stride\n+    else:\n+        if padding == \"same\":\n+            pad = kernel_size // 2\n+        else:\n+            pad = 0\n+\n+        length = (\n+            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n+        )\n+    return length\n+\n+\n+def compute_conv_transpose_output_shape(\n+    inputs,\n+    kernel,\n+    strides,\n+    padding,\n+    output_padding=None,\n+    data_format=\"channels_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = len(inputs.shape) - 2\n+    kernel_spatial_shape = kernel.shape[:-2]\n+\n+    if isinstance(output_padding, int):\n+        output_padding = (output_padding,) * len(kernel_spatial_shape)\n+    if isinstance(strides, int):\n+        strides = (strides,) * num_spatial_dims\n+    if isinstance(dilation_rate, int):\n+        dilation_rate = (dilation_rate,) * num_spatial_dims\n+\n+    if data_format == \"channels_last\":\n+        inputs_spatial_shape = inputs.shape[1:-1]\n+    else:\n+        inputs_spatial_shape = inputs.shape[2:]\n+\n+    output_shape = []\n+    for i in range(num_spatial_dims):\n+        current_output_padding = (\n+            None if output_padding is None else output_padding[i]\n+        )\n+        output_shape.append(\n+            _compute_conv_transpose_output_length(\n+                inputs_spatial_shape[i],\n+                kernel_spatial_shape[i],\n+                padding=padding,\n+                output_padding=current_output_padding,\n+                stride=strides[i],\n+                dilation=dilation_rate[0],\n+            )\n+        )\n+\n+    if data_format == \"channels_last\":\n+        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n+    else:\n+        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n+    return output_shape\n\n@@ -1,6 +1,8 @@\n import jax\n+import jax.numpy as jnp\n+import numpy as np\n+from jax import lax\n from jax import nn as jnn\n-from jax import numpy as jnp\n \n \n def relu(x):\n@@ -63,46 +65,293 @@ def log_softmax(x, axis=-1):\n     return jnn.log_softmax(x, axis=axis)\n \n \n-def max_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `max_pool` with JAX ops.\n-    raise NotImplementedError\n+def _convert_to_spatial_operand(\n+    x,\n+    num_spatial_dims,\n+    data_format=\"channels_last\",\n+    include_batch_and_channels=True,\n+):\n+    # Helper function that converts an operand to a spatial operand.\n+    x = (x,) * num_spatial_dims if isinstance(x, int) else x\n+    if not include_batch_and_channels:\n+        return x\n+    if data_format == \"channels_last\":\n+        x = (1,) + x + (1,)\n+    else:\n+        x = (1,) + (1,) + x\n+    return x\n \n \n-def average_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `average_pool` with JAX ops.\n-    raise NotImplementedError\n+def _pool(\n+    inputs,\n+    initial_value,\n+    reduce_fn,\n+    pool_size,\n+    strides=None,\n+    padding=\"valid\",\n+):\n+    \"\"\"Helper function to define pooling functions.\n \n+    Args:\n+        inputs: input data of shape `N+2`.\n+        initial_value: the initial value for the reduction.\n+        reduce_fn: a reduce function of the form `(T, T) -> T`.\n+        pool_size: a sequence of `N` integers, representing the window size to\n+            reduce over.\n+        strides: a sequence of `N` integers, representing the inter-window\n+            strides (default: `(1, ..., 1)`).\n+        padding: either the string `same` or `valid`.\n \n-def conv(inputs, kernel, strides, padding, dilation_rate=None):\n-    # TODO: Add missing args.\n-    return jax.lax.conv_general_dilated(\n-        inputs, kernel, strides, padding, rhs_dilation=dilation_rate\n+    Returns:\n+        The output of the reduction for each window slice.\n+    \"\"\"\n+    if padding not in (\"same\", \"valid\"):\n+        raise ValueError(\n+            f\"Invalid padding '{padding}', must be 'same' or 'valid'.\"\n+        )\n+    padding = padding.upper()\n+    return lax.reduce_window(\n+        inputs,\n+        initial_value,\n+        reduce_fn,\n+        pool_size,\n+        strides,\n+        padding,\n     )\n \n \n-def depthwise_conv(inputs, filter, strides, padding):\n-    # TODO: Implement `depthwise_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n+def max_pool(\n+    inputs,\n+    pool_size,\n+    strides=None,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = _convert_to_spatial_operand(\n+        pool_size, num_spatial_dims, data_format\n+    )\n+    strides = pool_size if strides is None else strides\n+    strides = _convert_to_spatial_operand(\n+        strides, num_spatial_dims, data_format\n+    )\n+    return _pool(inputs, -jnp.inf, lax.max, pool_size, strides, padding)\n+\n+\n+def average_pool(\n+    inputs,\n+    pool_size,\n+    strides,\n+    padding,\n+    data_format=\"channels_last\",\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = _convert_to_spatial_operand(\n+        pool_size, num_spatial_dims, data_format\n+    )\n+    strides = pool_size if strides is None else strides\n+    strides = _convert_to_spatial_operand(\n+        strides, num_spatial_dims, data_format\n+    )\n+\n+    pooled = _pool(inputs, 0.0, lax.add, pool_size, strides, padding)\n+    if padding == \"valid\":\n+        # Avoid the extra reduce_window.\n+        return pooled / np.prod(pool_size)\n+    else:\n+        # Count the number of valid entries at each input point, then use that\n+        # for computing average. Assumes that any two arrays of same shape will\n+        # be padded the same. Avoid broadcasting on axis where pooling is\n+        # skipped.\n+        shape = [\n+            (a if b != 1 else 1) for (a, b) in zip(inputs.shape, pool_size)\n+        ]\n+        window_counts = _pool(\n+            jnp.ones(shape, inputs.dtype),\n+            0.0,\n+            lax.add,\n+            pool_size,\n+            strides,\n+            padding,\n+        )\n+        return pooled / window_counts\n+\n+\n+def _convert_to_lax_conv_dimension_numbers(\n+    num_spatial_dims,\n+    data_format=\"channels_last\",\n+    transpose=False,\n+):\n+    \"\"\"Create a `lax.ConvDimensionNumbers` for the given inputs.\"\"\"\n+    num_dims = num_spatial_dims + 2\n+\n+    if data_format == \"channels_last\":\n+        spatial_dims = tuple(range(1, num_dims - 1))\n+        inputs_dn = (0, num_dims - 1) + spatial_dims\n+    else:\n+        spatial_dims = tuple(range(2, num_dims))\n+        inputs_dn = (0, 1) + spatial_dims\n+\n+    if transpose:\n+        kernel_dn = (num_dims - 2, num_dims - 1) + tuple(range(num_dims - 2))\n+    else:\n+        kernel_dn = (num_dims - 1, num_dims - 2) + tuple(range(num_dims - 2))\n+\n+    return lax.ConvDimensionNumbers(\n+        lhs_spec=inputs_dn, rhs_spec=kernel_dn, out_spec=inputs_dn\n+    )\n+\n+\n+def conv(\n+    inputs,\n+    kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channel_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    return jax.lax.conv_general_dilated(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+    )\n+\n+\n+def depthwise_conv(\n+    inputs,\n+    kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channel_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    feature_group_count = (\n+        inputs.shape[-1] if data_format == \"channels_last\" else inputs.shape[1]\n+    )\n+    kernel = jnp.reshape(\n+        kernel,\n+        kernel.shape[:-2] + (1, feature_group_count * kernel.shape[-1]),\n+    )\n+    return jax.lax.conv_general_dilated(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+        feature_group_count=feature_group_count,\n+    )\n \n \n def separable_conv(\n-    inputs, depthwise_kernel, pointwise_kernel, strides, padding\n+    inputs,\n+    depthwise_kernel,\n+    pointwise_kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+    dilation_rate=1,\n ):\n-    # TODO: Implement `separable_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n+    depthwise_conv_output = depthwise_conv(\n+        inputs,\n+        depthwise_kernel,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+    )\n+    return conv(\n+        depthwise_conv_output,\n+        pointwise_kernel,\n+        strides=1,\n+        padding=\"valid\",\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+    )\n \n \n def conv_transpose(\n     inputs,\n     kernel,\n-    strides,\n-    output_padding,\n+    strides=1,\n     padding=\"valid\",\n+    output_padding=None,\n     data_format=\"channels_last\",\n     dilation_rate=1,\n ):\n-    # TODO: Implement `conv_transpose`.\n-    raise NotImplementedError\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+\n+    if output_padding is not None:\n+        raise ValueError(\n+            \"Custom `output_padding` is not supported yet, please set \"\n+            \"`output_padding=None`.\"\n+        )\n+    padding = padding.upper()\n+    return jax.lax.conv_transpose(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+        transpose_kernel=True,\n+    )\n \n \n def one_hot(x, num_classes, axis=-1):\n\n@@ -1,5 +1,9 @@\n import tensorflow as tf\n \n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_shape,\n+)\n+\n \n def relu(x):\n     return tf.nn.relu(x)\n@@ -252,10 +256,8 @@ def separable_conv(\n \n     if data_format == \"channels_last\":\n         strides = (1,) + strides + (1,)\n-        spatial_start_dim = 1\n     else:\n         strides = (1, 1) + strides\n-        spatial_start_dim = 2\n     return tf.nn.separable_conv2d(\n         inputs,\n         depthwise_kernel,\n@@ -267,100 +269,6 @@ def separable_conv(\n     )\n \n \n-def _deconv_output_length(\n-    input_length,\n-    kernel_size,\n-    padding,\n-    output_padding=None,\n-    stride=1,\n-    dilation=1,\n-):\n-    \"\"\"Determines output length of a transposed convolution given input length.\n-\n-    Args:\n-        input_length: Integer.\n-        kernel_size: Integer.\n-        padding: one of `\"same\"` or `\"valid\"`.\n-        output_padding: Integer, amount of padding along the output dimension.\n-          Can be set to `None` in which case the output length is inferred.\n-        stride: Integer.\n-        dilation: Integer.\n-\n-    Returns:\n-        The output length (integer).\n-    \"\"\"\n-    assert padding in {\"same\", \"valid\"}\n-    if input_length is None:\n-        return None\n-\n-    # Get the dilated kernel size\n-    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n-\n-    # Infer length if output padding is None, else compute the exact length\n-    if output_padding is None:\n-        if padding == \"valid\":\n-            length = input_length * stride + max(kernel_size - stride, 0)\n-        else:\n-            length = input_length * stride\n-    else:\n-        if padding == \"same\":\n-            pad = kernel_size // 2\n-        else:\n-            pad = 0\n-\n-        length = (\n-            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n-        )\n-    return length\n-\n-\n-def compute_output_shape_conv_transpose(\n-    inputs,\n-    kernel,\n-    strides,\n-    padding,\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    num_spatial_dims = len(inputs.shape) - 2\n-    kernel_spatial_shape = kernel.shape[:-2]\n-\n-    if isinstance(output_padding, int):\n-        output_padding = (output_padding,) * len(kernel_spatial_shape)\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-\n-    if data_format == \"channels_last\":\n-        inputs_spatial_shape = inputs.shape[1:-1]\n-    else:\n-        inputs_spatial_shape = inputs.shape[2:]\n-\n-    output_shape = []\n-    for i in range(num_spatial_dims):\n-        current_output_padding = (\n-            None if output_padding is None else output_padding[i]\n-        )\n-        output_shape.append(\n-            _deconv_output_length(\n-                inputs_spatial_shape[i],\n-                kernel_spatial_shape[i],\n-                padding=padding,\n-                output_padding=current_output_padding,\n-                stride=strides[i],\n-                dilation=dilation_rate[0],\n-            )\n-        )\n-\n-    if data_format == \"channels_last\":\n-        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n-    else:\n-        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n-    return output_shape\n-\n-\n def conv_transpose(\n     inputs,\n     kernel,\n@@ -371,7 +279,7 @@ def conv_transpose(\n     dilation_rate=1,\n ):\n     tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    output_shape = compute_output_shape_conv_transpose(\n+    output_shape = compute_conv_transpose_output_shape(\n         inputs,\n         kernel,\n         strides,\n\n@@ -25,13 +25,14 @@ conv_transpose\n ctc ??\n \"\"\"\n \n-import math\n-\n import numpy as np\n \n from keras_core import backend\n from keras_core.backend import KerasTensor\n from keras_core.backend import any_symbolic_tensors\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_shape,\n+)\n from keras_core.operations.operation import Operation\n \n \n@@ -901,7 +902,7 @@ class ConvTranspose(Operation):\n         )\n \n     def compute_output_spec(self, inputs, kernel):\n-        output_shape = backend.nn.compute_output_shape_conv_transpose(\n+        output_shape = compute_conv_transpose_output_shape(\n             inputs,\n             kernel,\n             self.strides,\n@@ -934,7 +935,7 @@ def conv_transpose(\n             `data_format=\"channels_first\"`. Pooling happens over the spatial\n             dimensions only.\n         kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_output_channels],\n+            [kernel_spatial_shape, num_output_channels, num_input_channels],\n             `num_input_channels` should match the number of channels in\n             `inputs`.\n         strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n\n@@ -281,10 +281,6 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -547,10 +543,6 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n@@ -777,7 +769,7 @@ class NNOpsCorrectnessTest(testing.TestCase):\n         expected = tf.nn.conv3d(\n             inputs_3d, kernel, (1, 1, 1, 1, 1), padding=\"VALID\"\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(\n             inputs_3d,\n@@ -793,13 +785,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             padding=\"VALID\",\n             dilations=(1, 1, 1, 1, 1),\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(inputs_3d, kernel, 2, padding=\"same\")\n         expected = tf.nn.conv3d(\n             inputs_3d, kernel, (1, 2, 2, 2, 1), padding=\"SAME\"\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n     def test_depthwise_conv(self):\n         # Test 2D conv.\n@@ -912,14 +904,6 @@ class NNOpsCorrectnessTest(testing.TestCase):\n         )\n         self.assertAllClose(outputs, expected)\n \n-        outputs = knn.conv_transpose(\n-            inputs_1d, kernel, 5, output_padding=4, padding=\"valid\"\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 21, 5], 5, padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n         # Test 2D conv.\n         inputs_2d = np.arange(96, dtype=float).reshape([2, 4, 4, 3])\n         kernel = np.arange(60, dtype=float).reshape([2, 2, 5, 3])\n@@ -935,21 +919,3 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             inputs_2d, kernel, [2, 8, 8, 5], 2, padding=\"SAME\"\n         )\n         self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            5,\n-            output_padding=4,\n-            padding=\"valid\",\n-            dilation_rate=(1, 1),\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            [2, 21, 21, 5],\n-            5,\n-            padding=\"VALID\",\n-            dilations=(1, 1),\n-        )\n-        self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f80d8c1ed8c7f1934ba8438c155d4ddd93f4e90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 363 | Lines Deleted: 159 | Files Changed: 6 | Hunks: 30 | Methods Changed: 25 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 522 | Churn Cumulative: 7709 | Contributors (this commit): 5 | Commits (past 90d): 43 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core.backend.common import backend_utils\n from keras_core.backend.common import random\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import standardize_dtype\n\n@@ -0,0 +1,79 @@\n+def _compute_conv_transpose_output_length(\n+    input_length,\n+    kernel_size,\n+    padding,\n+    output_padding=None,\n+    stride=1,\n+    dilation=1,\n+):\n+    \"\"\"Computes output size of a transposed convolution given input size.\"\"\"\n+    assert padding in {\"same\", \"valid\"}\n+    if input_length is None:\n+        return None\n+\n+    # Get the dilated kernel size\n+    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n+\n+    # Infer length if output padding is None, else compute the exact length\n+    if output_padding is None:\n+        if padding == \"valid\":\n+            length = input_length * stride + max(kernel_size - stride, 0)\n+        else:\n+            length = input_length * stride\n+    else:\n+        if padding == \"same\":\n+            pad = kernel_size // 2\n+        else:\n+            pad = 0\n+\n+        length = (\n+            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n+        )\n+    return length\n+\n+\n+def compute_conv_transpose_output_shape(\n+    inputs,\n+    kernel,\n+    strides,\n+    padding,\n+    output_padding=None,\n+    data_format=\"channels_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = len(inputs.shape) - 2\n+    kernel_spatial_shape = kernel.shape[:-2]\n+\n+    if isinstance(output_padding, int):\n+        output_padding = (output_padding,) * len(kernel_spatial_shape)\n+    if isinstance(strides, int):\n+        strides = (strides,) * num_spatial_dims\n+    if isinstance(dilation_rate, int):\n+        dilation_rate = (dilation_rate,) * num_spatial_dims\n+\n+    if data_format == \"channels_last\":\n+        inputs_spatial_shape = inputs.shape[1:-1]\n+    else:\n+        inputs_spatial_shape = inputs.shape[2:]\n+\n+    output_shape = []\n+    for i in range(num_spatial_dims):\n+        current_output_padding = (\n+            None if output_padding is None else output_padding[i]\n+        )\n+        output_shape.append(\n+            _compute_conv_transpose_output_length(\n+                inputs_spatial_shape[i],\n+                kernel_spatial_shape[i],\n+                padding=padding,\n+                output_padding=current_output_padding,\n+                stride=strides[i],\n+                dilation=dilation_rate[0],\n+            )\n+        )\n+\n+    if data_format == \"channels_last\":\n+        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n+    else:\n+        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n+    return output_shape\n\n@@ -1,6 +1,8 @@\n import jax\n+import jax.numpy as jnp\n+import numpy as np\n+from jax import lax\n from jax import nn as jnn\n-from jax import numpy as jnp\n \n \n def relu(x):\n@@ -63,46 +65,293 @@ def log_softmax(x, axis=-1):\n     return jnn.log_softmax(x, axis=axis)\n \n \n-def max_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `max_pool` with JAX ops.\n-    raise NotImplementedError\n+def _convert_to_spatial_operand(\n+    x,\n+    num_spatial_dims,\n+    data_format=\"channels_last\",\n+    include_batch_and_channels=True,\n+):\n+    # Helper function that converts an operand to a spatial operand.\n+    x = (x,) * num_spatial_dims if isinstance(x, int) else x\n+    if not include_batch_and_channels:\n+        return x\n+    if data_format == \"channels_last\":\n+        x = (1,) + x + (1,)\n+    else:\n+        x = (1,) + (1,) + x\n+    return x\n \n \n-def average_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `average_pool` with JAX ops.\n-    raise NotImplementedError\n+def _pool(\n+    inputs,\n+    initial_value,\n+    reduce_fn,\n+    pool_size,\n+    strides=None,\n+    padding=\"valid\",\n+):\n+    \"\"\"Helper function to define pooling functions.\n \n+    Args:\n+        inputs: input data of shape `N+2`.\n+        initial_value: the initial value for the reduction.\n+        reduce_fn: a reduce function of the form `(T, T) -> T`.\n+        pool_size: a sequence of `N` integers, representing the window size to\n+            reduce over.\n+        strides: a sequence of `N` integers, representing the inter-window\n+            strides (default: `(1, ..., 1)`).\n+        padding: either the string `same` or `valid`.\n \n-def conv(inputs, kernel, strides, padding, dilation_rate=None):\n-    # TODO: Add missing args.\n-    return jax.lax.conv_general_dilated(\n-        inputs, kernel, strides, padding, rhs_dilation=dilation_rate\n+    Returns:\n+        The output of the reduction for each window slice.\n+    \"\"\"\n+    if padding not in (\"same\", \"valid\"):\n+        raise ValueError(\n+            f\"Invalid padding '{padding}', must be 'same' or 'valid'.\"\n+        )\n+    padding = padding.upper()\n+    return lax.reduce_window(\n+        inputs,\n+        initial_value,\n+        reduce_fn,\n+        pool_size,\n+        strides,\n+        padding,\n     )\n \n \n-def depthwise_conv(inputs, filter, strides, padding):\n-    # TODO: Implement `depthwise_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n+def max_pool(\n+    inputs,\n+    pool_size,\n+    strides=None,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = _convert_to_spatial_operand(\n+        pool_size, num_spatial_dims, data_format\n+    )\n+    strides = pool_size if strides is None else strides\n+    strides = _convert_to_spatial_operand(\n+        strides, num_spatial_dims, data_format\n+    )\n+    return _pool(inputs, -jnp.inf, lax.max, pool_size, strides, padding)\n+\n+\n+def average_pool(\n+    inputs,\n+    pool_size,\n+    strides,\n+    padding,\n+    data_format=\"channels_last\",\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = _convert_to_spatial_operand(\n+        pool_size, num_spatial_dims, data_format\n+    )\n+    strides = pool_size if strides is None else strides\n+    strides = _convert_to_spatial_operand(\n+        strides, num_spatial_dims, data_format\n+    )\n+\n+    pooled = _pool(inputs, 0.0, lax.add, pool_size, strides, padding)\n+    if padding == \"valid\":\n+        # Avoid the extra reduce_window.\n+        return pooled / np.prod(pool_size)\n+    else:\n+        # Count the number of valid entries at each input point, then use that\n+        # for computing average. Assumes that any two arrays of same shape will\n+        # be padded the same. Avoid broadcasting on axis where pooling is\n+        # skipped.\n+        shape = [\n+            (a if b != 1 else 1) for (a, b) in zip(inputs.shape, pool_size)\n+        ]\n+        window_counts = _pool(\n+            jnp.ones(shape, inputs.dtype),\n+            0.0,\n+            lax.add,\n+            pool_size,\n+            strides,\n+            padding,\n+        )\n+        return pooled / window_counts\n+\n+\n+def _convert_to_lax_conv_dimension_numbers(\n+    num_spatial_dims,\n+    data_format=\"channels_last\",\n+    transpose=False,\n+):\n+    \"\"\"Create a `lax.ConvDimensionNumbers` for the given inputs.\"\"\"\n+    num_dims = num_spatial_dims + 2\n+\n+    if data_format == \"channels_last\":\n+        spatial_dims = tuple(range(1, num_dims - 1))\n+        inputs_dn = (0, num_dims - 1) + spatial_dims\n+    else:\n+        spatial_dims = tuple(range(2, num_dims))\n+        inputs_dn = (0, 1) + spatial_dims\n+\n+    if transpose:\n+        kernel_dn = (num_dims - 2, num_dims - 1) + tuple(range(num_dims - 2))\n+    else:\n+        kernel_dn = (num_dims - 1, num_dims - 2) + tuple(range(num_dims - 2))\n+\n+    return lax.ConvDimensionNumbers(\n+        lhs_spec=inputs_dn, rhs_spec=kernel_dn, out_spec=inputs_dn\n+    )\n+\n+\n+def conv(\n+    inputs,\n+    kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channel_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    return jax.lax.conv_general_dilated(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+    )\n+\n+\n+def depthwise_conv(\n+    inputs,\n+    kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channel_last\",\n+    dilation_rate=1,\n+):\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    feature_group_count = (\n+        inputs.shape[-1] if data_format == \"channels_last\" else inputs.shape[1]\n+    )\n+    kernel = jnp.reshape(\n+        kernel,\n+        kernel.shape[:-2] + (1, feature_group_count * kernel.shape[-1]),\n+    )\n+    return jax.lax.conv_general_dilated(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+        feature_group_count=feature_group_count,\n+    )\n \n \n def separable_conv(\n-    inputs, depthwise_kernel, pointwise_kernel, strides, padding\n+    inputs,\n+    depthwise_kernel,\n+    pointwise_kernel,\n+    strides=1,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+    dilation_rate=1,\n ):\n-    # TODO: Implement `separable_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n+    depthwise_conv_output = depthwise_conv(\n+        inputs,\n+        depthwise_kernel,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+    )\n+    return conv(\n+        depthwise_conv_output,\n+        pointwise_kernel,\n+        strides=1,\n+        padding=\"valid\",\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+    )\n \n \n def conv_transpose(\n     inputs,\n     kernel,\n-    strides,\n-    output_padding,\n+    strides=1,\n     padding=\"valid\",\n+    output_padding=None,\n     data_format=\"channels_last\",\n     dilation_rate=1,\n ):\n-    # TODO: Implement `conv_transpose`.\n-    raise NotImplementedError\n+    num_spatial_dims = inputs.ndim - 2\n+    dimension_numbers = _convert_to_lax_conv_dimension_numbers(\n+        num_spatial_dims,\n+        data_format,\n+        transpose=False,\n+    )\n+    strides = _convert_to_spatial_operand(\n+        strides,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+    dilation_rate = _convert_to_spatial_operand(\n+        dilation_rate,\n+        num_spatial_dims,\n+        data_format,\n+        include_batch_and_channels=False,\n+    )\n+\n+    if output_padding is not None:\n+        raise ValueError(\n+            \"Custom `output_padding` is not supported yet, please set \"\n+            \"`output_padding=None`.\"\n+        )\n+    padding = padding.upper()\n+    return jax.lax.conv_transpose(\n+        inputs,\n+        kernel,\n+        strides,\n+        padding,\n+        rhs_dilation=dilation_rate,\n+        dimension_numbers=dimension_numbers,\n+        transpose_kernel=True,\n+    )\n \n \n def one_hot(x, num_classes, axis=-1):\n\n@@ -1,5 +1,9 @@\n import tensorflow as tf\n \n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_shape,\n+)\n+\n \n def relu(x):\n     return tf.nn.relu(x)\n@@ -252,10 +256,8 @@ def separable_conv(\n \n     if data_format == \"channels_last\":\n         strides = (1,) + strides + (1,)\n-        spatial_start_dim = 1\n     else:\n         strides = (1, 1) + strides\n-        spatial_start_dim = 2\n     return tf.nn.separable_conv2d(\n         inputs,\n         depthwise_kernel,\n@@ -267,100 +269,6 @@ def separable_conv(\n     )\n \n \n-def _deconv_output_length(\n-    input_length,\n-    kernel_size,\n-    padding,\n-    output_padding=None,\n-    stride=1,\n-    dilation=1,\n-):\n-    \"\"\"Determines output length of a transposed convolution given input length.\n-\n-    Args:\n-        input_length: Integer.\n-        kernel_size: Integer.\n-        padding: one of `\"same\"` or `\"valid\"`.\n-        output_padding: Integer, amount of padding along the output dimension.\n-          Can be set to `None` in which case the output length is inferred.\n-        stride: Integer.\n-        dilation: Integer.\n-\n-    Returns:\n-        The output length (integer).\n-    \"\"\"\n-    assert padding in {\"same\", \"valid\"}\n-    if input_length is None:\n-        return None\n-\n-    # Get the dilated kernel size\n-    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n-\n-    # Infer length if output padding is None, else compute the exact length\n-    if output_padding is None:\n-        if padding == \"valid\":\n-            length = input_length * stride + max(kernel_size - stride, 0)\n-        else:\n-            length = input_length * stride\n-    else:\n-        if padding == \"same\":\n-            pad = kernel_size // 2\n-        else:\n-            pad = 0\n-\n-        length = (\n-            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n-        )\n-    return length\n-\n-\n-def compute_output_shape_conv_transpose(\n-    inputs,\n-    kernel,\n-    strides,\n-    padding,\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    num_spatial_dims = len(inputs.shape) - 2\n-    kernel_spatial_shape = kernel.shape[:-2]\n-\n-    if isinstance(output_padding, int):\n-        output_padding = (output_padding,) * len(kernel_spatial_shape)\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-\n-    if data_format == \"channels_last\":\n-        inputs_spatial_shape = inputs.shape[1:-1]\n-    else:\n-        inputs_spatial_shape = inputs.shape[2:]\n-\n-    output_shape = []\n-    for i in range(num_spatial_dims):\n-        current_output_padding = (\n-            None if output_padding is None else output_padding[i]\n-        )\n-        output_shape.append(\n-            _deconv_output_length(\n-                inputs_spatial_shape[i],\n-                kernel_spatial_shape[i],\n-                padding=padding,\n-                output_padding=current_output_padding,\n-                stride=strides[i],\n-                dilation=dilation_rate[0],\n-            )\n-        )\n-\n-    if data_format == \"channels_last\":\n-        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n-    else:\n-        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n-    return output_shape\n-\n-\n def conv_transpose(\n     inputs,\n     kernel,\n@@ -371,7 +279,7 @@ def conv_transpose(\n     dilation_rate=1,\n ):\n     tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    output_shape = compute_output_shape_conv_transpose(\n+    output_shape = compute_conv_transpose_output_shape(\n         inputs,\n         kernel,\n         strides,\n\n@@ -25,13 +25,14 @@ conv_transpose\n ctc ??\n \"\"\"\n \n-import math\n-\n import numpy as np\n \n from keras_core import backend\n from keras_core.backend import KerasTensor\n from keras_core.backend import any_symbolic_tensors\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_shape,\n+)\n from keras_core.operations.operation import Operation\n \n \n@@ -901,7 +902,7 @@ class ConvTranspose(Operation):\n         )\n \n     def compute_output_spec(self, inputs, kernel):\n-        output_shape = backend.nn.compute_output_shape_conv_transpose(\n+        output_shape = compute_conv_transpose_output_shape(\n             inputs,\n             kernel,\n             self.strides,\n@@ -934,7 +935,7 @@ def conv_transpose(\n             `data_format=\"channels_first\"`. Pooling happens over the spatial\n             dimensions only.\n         kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_output_channels],\n+            [kernel_spatial_shape, num_output_channels, num_input_channels],\n             `num_input_channels` should match the number of channels in\n             `inputs`.\n         strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n\n@@ -281,10 +281,6 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -547,10 +543,6 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n@@ -777,7 +769,7 @@ class NNOpsCorrectnessTest(testing.TestCase):\n         expected = tf.nn.conv3d(\n             inputs_3d, kernel, (1, 1, 1, 1, 1), padding=\"VALID\"\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(\n             inputs_3d,\n@@ -793,13 +785,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             padding=\"VALID\",\n             dilations=(1, 1, 1, 1, 1),\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(inputs_3d, kernel, 2, padding=\"same\")\n         expected = tf.nn.conv3d(\n             inputs_3d, kernel, (1, 2, 2, 2, 1), padding=\"SAME\"\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n     def test_depthwise_conv(self):\n         # Test 2D conv.\n@@ -912,14 +904,6 @@ class NNOpsCorrectnessTest(testing.TestCase):\n         )\n         self.assertAllClose(outputs, expected)\n \n-        outputs = knn.conv_transpose(\n-            inputs_1d, kernel, 5, output_padding=4, padding=\"valid\"\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 21, 5], 5, padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n         # Test 2D conv.\n         inputs_2d = np.arange(96, dtype=float).reshape([2, 4, 4, 3])\n         kernel = np.arange(60, dtype=float).reshape([2, 2, 5, 3])\n@@ -935,21 +919,3 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             inputs_2d, kernel, [2, 8, 8, 5], 2, padding=\"SAME\"\n         )\n         self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            5,\n-            output_padding=4,\n-            padding=\"valid\",\n-            dilation_rate=(1, 1),\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            [2, 21, 21, 5],\n-            5,\n-            padding=\"VALID\",\n-            dilations=(1, 1),\n-        )\n-        self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
