{"custom_id": "keras#a1446dc877a2116f90c40438af6528e2b25ee5a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 33 | Churn Cumulative: 4266 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -285,14 +285,9 @@ class RandomCrop(base_layer.Layer):\n       outputs = tf.slice(resized_inputs, bbox_begin, bbox_size)\n       return outputs\n \n-    input_shape = inputs.shape.as_list()\n-\n-    if self.height > input_shape[H_AXIS] or self.width > input_shape[W_AXIS]:\n-      output = resize_and_center_cropped_inputs()\n-    else:\n     output = control_flow_util.smart_cond(training, random_cropped_inputs,\n                                           resize_and_center_cropped_inputs)\n-\n+    input_shape = inputs.shape.as_list()\n     if unbatched:\n       output_shape = [self.height, self.width, input_shape[-1]]\n     else:\n\n@@ -148,7 +148,7 @@ class ResizingTest(keras_parameterized.TestCase):\n           [1, 2],\n           [5, 6],\n           [9, 10],\n-          [13, 14]\n+          [13, 14],\n       ]).astype('float32')\n       expected_output = np.reshape(expected_output, (1, 4, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n@@ -299,6 +299,17 @@ class RandomCropTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n+  @parameterized.named_parameters(('random_crop_5_by_12', 5, 12),\n+                                  ('random_crop_10_by_8', 10, 8),\n+                                  ('random_crop_10_by_12', 10, 12))\n+  def test_invalid_random_crop(self, expected_height, expected_width):\n+    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n+    # is enabled.\n+    with self.assertRaises(\n+        (tf.errors.InvalidArgumentError, tf.errors.InternalError)):\n+      with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n+        self._run_test(expected_height, expected_width)\n+\n   def test_training_with_mock(self):\n     np.random.seed(1337)\n     height, width = 3, 4\n@@ -379,16 +390,6 @@ class RandomCropTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=1)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n-  def test_input_smaller_than_crop_box(self):\n-    np.random.seed(1337)\n-    height, width = 10, 8\n-    inp = np.random.random((12, 3, 3, 3))\n-    with testing_utils.use_gpu():\n-      layer = image_preprocessing.RandomCrop(height, width)\n-      actual_output = layer(inp)\n-      expected_output_shape = (12, 10, 8, 3)\n-      self.assertEqual(expected_output_shape, actual_output.shape)\n-\n \n class RescalingTest(keras_parameterized.TestCase):\n \n@@ -1222,8 +1223,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n         self.assertAllClose(expected_output, actual_output)\n \n   def test_distribution_strategy(self):\n-    \"\"\"Tests that RandomRotation can be created within distribution strategies.\n-    \"\"\"\n+    \"\"\"Tests that RandomRotation can be created within distribution strategies.\"\"\"\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     with testing_utils.use_gpu():\n       strat = MirroredStrategy(devices=['cpu', 'gpu'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b60438ac41bc4456d58187eb854ba2474d512e2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 330 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,9 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n         keras.layers.LayerNormalization,\n         kwargs={'axis': (-3, -2, -1)},\n         input_shape=(2, 8, 8, 3))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        input_shape=(1, 0, 10))\n \n   @keras_parameterized.run_all_keras_modes\n   def test_non_fused_layernorm(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71d6d6eb0412f4c66ff40cf2393bb157cec281e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 42 | Churn Cumulative: 235 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,11 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for Keras metrics.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n from keras import metrics\n from keras.engine import base_layer\n+import tensorflow.compat.v2 as tf\n+\n+combinations = tf.__internal__.distribute.combinations\n \n \n def _labeled_dataset_fn():\n@@ -67,18 +68,18 @@ def _regression_dataset_fn():\n def all_combinations():\n   return tf.__internal__.test.combinations.combine(\n       distribution=[\n-          tf.__internal__.distribute.combinations.default_strategy,\n-          tf.__internal__.distribute.combinations.one_device_strategy,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n+          combinations.default_strategy, combinations.one_device_strategy,\n+          combinations.mirrored_strategy_with_gpu_and_cpu,\n+          combinations.mirrored_strategy_with_two_gpus\n       ],\n       mode=[\"graph\"])\n \n \n def tpu_combinations():\n   return tf.__internal__.test.combinations.combine(\n-      distribution=[tf.__internal__.distribute.combinations.tpu_strategy,],\n-      mode=[\"graph\"])\n+      distribution=[\n+          combinations.tpu_strategy,\n+      ], mode=[\"graph\"])\n \n \n class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n@@ -106,7 +107,7 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n         if batches_consumed >= 4:  # Consume 4 input batches in total.\n           break\n \n-  @tf.__internal__.distribute.combinations.generate(all_combinations() + tpu_combinations())\n+  @combinations.generate(all_combinations() + tpu_combinations())\n   def testMean(self, distribution):\n     def _dataset_fn():\n       return tf.data.Dataset.range(1000).map(tf.compat.v1.to_float).batch(\n@@ -118,18 +119,19 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n     self._test_metric(distribution, _dataset_fn, metrics.Mean, _expected_fn)\n \n-  @tf.__internal__.distribute.combinations.generate(\n+  @combinations.generate(\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_one_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-              tf.__internal__.distribute.combinations.tpu_strategy_packed_var\n+              combinations.mirrored_strategy_with_one_cpu,\n+              combinations.mirrored_strategy_with_gpu_and_cpu,\n+              combinations.mirrored_strategy_with_two_gpus,\n+              combinations.tpu_strategy_packed_var,\n+              combinations.parameter_server_strategy_1worker_2ps_cpu,\n+              combinations.parameter_server_strategy_1worker_2ps_1gpu,\n           ],\n           mode=[\"eager\"],\n-          jit_compile=[False]) +\n-      tf.__internal__.test.combinations.combine(\n-          distribution=[tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus],\n+          jit_compile=[False]) + tf.__internal__.test.combinations.combine(\n+              distribution=[combinations.mirrored_strategy_with_two_gpus],\n               mode=[\"eager\"],\n               jit_compile=[True]))\n   def testAddMetric(self, distribution, jit_compile):\n@@ -164,6 +166,12 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n     def run():\n       return distribution.run(func)\n \n+    if distribution._should_use_with_coordinator:\n+      coord = tf.distribute.experimental.coordinator.ClusterCoordinator(\n+          distribution)\n+      coord.schedule(run)\n+      coord.join()\n+    else:\n       run()\n \n     self.assertEqual(layer.metrics[0].result().numpy(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#751469e85442895dc020bfa4841a871d96b3cfc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 18431 | Contributors (this commit): 74 | Commits (past 90d): 4 | Contributors (cumulative): 74 | DMM Complexity: None\n\nDIFF:\n@@ -2258,22 +2258,24 @@ class SeparableConv2D(SeparableConv):\n class DepthwiseConv2D(Conv2D):\n   \"\"\"Depthwise 2D convolution.\n \n-  Depthwise convolution is a type of convolution in which a single convolutional\n-  filter is apply to each input channel (i.e. in a depthwise way).\n-  You can understand depthwise convolution as being\n-  the first step in a depthwise separable convolution.\n+  Depthwise convolution is a type of convolution in which each input channel is\n+  convolved with a different filter of depth 1 (called a depthwise kernel). You\n+  can understand depthwise convolution as the first step in a depthwise\n+  separable convolution.\n \n-  It is implemented via the following steps:\n+  It could be implemented via the following steps:\n \t\n   - Split the input into individual channels.\n-  - Convolve each input with the layer's kernel (called a depthwise kernel).\n+  - Repeat each input channel `depth_multiplier` many times.\n+  - Convolve each channel with an individual depthwise kernel.\n   - Stack the convolved outputs together (along the channels axis).\n \n   Unlike a regular 2D convolution, depthwise convolution does not mix\n   information across different input channels.\n \n-  The `depth_multiplier` argument controls how many\n-  output channels are generated per input channel in the depthwise step.\n+  The `depth_multiplier` argument determines how many filter are applied to one\n+  input channel. Thereby, it controls the amount of output channels that are\n+  generated per input channel in the depthwise step.\n \n   Args:\n     kernel_size: An integer or tuple/list of 2 integers, specifying the\n@@ -2290,10 +2292,10 @@ class DepthwiseConv2D(Conv2D):\n       `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n       to the left/right or up/down of the input such that output has the same\n       height/width dimension as the input.\n-    depth_multiplier: The number of depthwise convolution output channels\n-      for each input channel.\n-      The total number of depthwise convolution output\n-      channels will be equal to `filters_in * depth_multiplier`.\n+    depth_multiplier: The number of depthwise kernels that are applied to one\n+      input channel.\n+      The total number of depthwise convolution output channels per input\n+      channel will be equal to `filters_in * depth_multiplier`.\n     data_format: A string,\n       one of `channels_last` (default) or `channels_first`.\n       The ordering of the dimensions in the inputs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e07487ae829da29c434cf20d1436612d3f7df30d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 376 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,6 @@ from keras.engine.training import Model\n \n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = '2.6.0'\n+__version__ = '2.7.0'\n \n keras_export('keras.__version__').export_constant(__name__, '__version__')\n\n@@ -30,7 +30,7 @@ DOCLINES = __doc__.split('\\n')\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = '2.6.0'\n+_VERSION = '2.7.0'\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8101f6a7256319c598a0a28c5222ef36ba329453", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 32 | Methods Changed: 20 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 86 | Churn Cumulative: 16180 | Contributors (this commit): 90 | Commits (past 90d): 8 | Contributors (cumulative): 93 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -1200,6 +1200,11 @@ class Dense(Layer):\n     if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:\n       inputs = tf.cast(inputs, dtype=self._compute_dtype_object)\n \n+    if isinstance(inputs, tf.RaggedTensor) and inputs.shape[-1] is not None:\n+      # In case we encounter a RaggedTensor with a fixed last dimension (last\n+      # dimension not ragged), we can map the call method to the flat values.\n+      return tf.ragged.map_flat_values(self.call, inputs)\n+\n     rank = inputs.shape.rank\n     if rank == 2 or rank is None:\n       # We use embedding_lookup_sparse as a more efficient matmul operation for\n\n@@ -36,8 +36,10 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n \n     testing_utils.layer_test(\n         keras.layers.Dropout,\n-        kwargs={'rate': 0.5,\n-                'noise_shape': [3, 1]},\n+        kwargs={\n+            'rate': 0.5,\n+            'noise_shape': [3, 1]\n+        },\n         input_shape=(3, 2))\n \n   def test_dropout_supports_masking(self):\n@@ -58,7 +60,10 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n \n     testing_utils.layer_test(\n         keras.layers.SpatialDropout2D,\n-        kwargs={'rate': 0.5, 'data_format': 'channels_first'},\n+        kwargs={\n+            'rate': 0.5,\n+            'data_format': 'channels_first'\n+        },\n         input_shape=(2, 3, 4, 5))\n \n   def test_spatial_dropout_3d(self):\n@@ -69,7 +74,10 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n \n     testing_utils.layer_test(\n         keras.layers.SpatialDropout3D,\n-        kwargs={'rate': 0.5, 'data_format': 'channels_first'},\n+        kwargs={\n+            'rate': 0.5,\n+            'data_format': 'channels_first'\n+        },\n         input_shape=(2, 3, 4, 4, 5))\n \n   def test_dropout_partial_noise_shape(self):\n@@ -109,10 +117,7 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n \n     ld = keras.layers.Lambda(f)\n     config = ld.get_config()\n-    ld = keras.layers.deserialize({\n-        'class_name': 'Lambda',\n-        'config': config\n-    })\n+    ld = keras.layers.deserialize({'class_name': 'Lambda', 'config': config})\n     self.assertEqual(ld.function(3), 4)\n \n     # test with lambda\n@@ -135,6 +140,7 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n     self.assertEqual((1, 1), l.get_config()['output_shape'])\n \n   def test_lambda_output_shape_function(self):\n+\n     def get_output_shape(input_shape):\n       return 1 * input_shape\n \n@@ -152,7 +158,8 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual((10, 20), output_shape)\n     output_signature = l.compute_output_signature([\n         tf.TensorSpec(dtype=tf.float64, shape=(10, 10)),\n-        tf.TensorSpec(dtype=tf.float64, shape=(10, 20))])\n+        tf.TensorSpec(dtype=tf.float64, shape=(10, 20))\n+    ])\n     self.assertAllEqual((10, 20), output_signature.shape)\n     self.assertAllEqual(tf.float64, output_signature.dtype)\n \n@@ -198,16 +205,11 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n   def test_lambda_config_serialization(self):\n     # Test serialization with output_shape and output_shape_type\n     layer = keras.layers.Lambda(\n-        lambda x: x + 1,\n-        output_shape=(1, 1),\n-        mask=lambda i, m: m)\n+        lambda x: x + 1, output_shape=(1, 1), mask=lambda i, m: m)\n     layer(keras.backend.variable(np.ones((1, 1))))\n     config = layer.get_config()\n \n-    layer = keras.layers.deserialize({\n-        'class_name': 'Lambda',\n-        'config': config\n-    })\n+    layer = keras.layers.deserialize({'class_name': 'Lambda', 'config': config})\n     self.assertAllEqual(layer.function(1), 2)\n     self.assertAllEqual(layer._output_shape, (1, 1))\n     self.assertAllEqual(layer.mask(1, True), True)\n@@ -257,6 +259,7 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n \n     def add_one(inputs):\n       return inputs + 1.0\n+\n     layer = keras.layers.Lambda(add_one)\n \n     ragged_input = tf.ragged.constant([[1.0], [2.0, 3.0]])\n@@ -277,6 +280,7 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n   @keras_parameterized.run_with_all_model_types\n   def test_lambda_with_variable_in_model(self):\n     v = tf.Variable(1., trainable=True)\n+\n     def lambda_fn(x, v):\n       return x * v\n \n@@ -301,17 +305,18 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes\n   @keras_parameterized.run_with_all_model_types\n   def test_creation_inside_lambda(self):\n+\n     def lambda_fn(x):\n       scale = tf.Variable(1., trainable=True, name='scale')\n       shift = tf.Variable(1., trainable=True, name='shift')\n       return x * scale + shift\n \n-    expected_error = textwrap.dedent(r'''\n+    expected_error = textwrap.dedent(r\"\"\"\n     (    )?The following Variables were created within a Lambda layer \\(shift_and_scale\\)\n     (    )?but are not tracked by said layer:\n     (    )?  <tf.Variable \\'.*shift_and_scale/scale:0\\'.+\n     (    )?  <tf.Variable \\'.*shift_and_scale/shift:0\\'.+\n-    (    )?The layer cannot safely ensure proper Variable reuse.+''')\n+    (    )?The layer cannot safely ensure proper Variable reuse.+\"\"\")\n \n     with self.assertRaisesRegex(ValueError, expected_error):\n       layer = keras.layers.Lambda(lambda_fn, name='shift_and_scale')\n@@ -322,14 +327,15 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n   @keras_parameterized.run_with_all_model_types\n   def test_transitive_variable_creation(self):\n     dense = keras.layers.Dense(1, use_bias=False, kernel_initializer='ones')\n+\n     def bad_lambda_fn(x):\n       return dense(x + 1)  # Dense layer is built on first call\n \n-    expected_error = textwrap.dedent(r'''\n+    expected_error = textwrap.dedent(r\"\"\"\n     (    )?The following Variables were created within a Lambda layer \\(bias_dense\\)\n     (    )?but are not tracked by said layer:\n     (    )?  <tf.Variable \\'.*bias_dense/dense/kernel:0\\'.+\n-    (    )?The layer cannot safely ensure proper Variable reuse.+''')\n+    (    )?The layer cannot safely ensure proper Variable reuse.+\"\"\")\n \n     with self.assertRaisesRegex(ValueError, expected_error):\n       layer = keras.layers.Lambda(bad_lambda_fn, name='bias_dense')\n@@ -340,18 +346,21 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n   @keras_parameterized.run_with_all_model_types\n   def test_warns_on_variable_capture(self):\n     v = tf.Variable(1., trainable=True)\n+\n     def lambda_fn(x):\n       return x * v\n \n-    expected_warning = textwrap.dedent(r'''\n+    expected_warning = textwrap.dedent(r\"\"\"\n     (    )?The following Variables were used a Lambda layer\\'s call \\(lambda\\), but\n     (    )?are not present in its tracked objects:\n     (    )?  <tf.Variable \\'.*Variable:0\\'.+\n-    (    )?It is possible that this is intended behavior.+''')\n+    (    )?It is possible that this is intended behavior.+\"\"\")\n \n     layer = keras.layers.Lambda(lambda_fn)\n+\n     def patched_warn(msg):\n       raise ValueError(msg)\n+\n     layer._warn = patched_warn\n \n     with self.assertRaisesRegex(ValueError, expected_warning):\n@@ -440,13 +449,15 @@ class CoreLayersTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n       testing_utils.layer_test(\n           keras.layers.Permute,\n-          kwargs={'dims': (0, 1, 2)}, input_shape=(3, 2, 4))\n+          kwargs={'dims': (0, 1, 2)},\n+          input_shape=(3, 2, 4))\n \n   def test_permute_errors_on_invalid_set_of_dims_indices(self):\n     with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n       testing_utils.layer_test(\n           keras.layers.Permute,\n-          kwargs={'dims': (1, 4, 2)}, input_shape=(3, 2, 4))\n+          kwargs={'dims': (1, 4, 2)},\n+          input_shape=(3, 2, 4))\n \n   def test_flatten(self):\n     testing_utils.layer_test(\n@@ -463,8 +474,7 @@ class CoreLayersTest(keras_parameterized.TestCase):\n     self.assertAllClose(outputs, target_outputs)\n \n   def test_flatten_scalar_channels(self):\n-    testing_utils.layer_test(\n-        keras.layers.Flatten, kwargs={}, input_shape=(3,))\n+    testing_utils.layer_test(keras.layers.Flatten, kwargs={}, input_shape=(3,))\n \n     # Test channels_first\n     inputs = np.random.random((10,)).astype('float32')\n@@ -501,6 +511,10 @@ class CoreLayersTest(keras_parameterized.TestCase):\n         values=np.random.uniform(size=(5,)).astype('f'),\n         dense_shape=[10, 10])\n     sparse_inputs = tf.sparse.reorder(sparse_inputs)\n+    # Create some ragged data.\n+    ragged_inputs = tf.RaggedTensor.from_row_splits(\n+        np.random.uniform(size=(10, 10)).astype('f'),\n+        row_splits=[0, 4, 6, 6, 9, 10])\n \n     layer = keras.layers.Dense(\n         5,\n@@ -509,6 +523,7 @@ class CoreLayersTest(keras_parameterized.TestCase):\n         dtype='float32')\n     dense_outputs = layer(dense_inputs)\n     sparse_outpus = layer(sparse_inputs)\n+    ragged_outputs = layer(ragged_inputs)\n \n     expected_dense = tf.add(\n         tf.matmul(dense_inputs, keras.backend.get_value(layer.kernel)),\n@@ -518,20 +533,25 @@ class CoreLayersTest(keras_parameterized.TestCase):\n             tf.sparse.to_dense(sparse_inputs),\n             keras.backend.get_value(layer.kernel)),\n         keras.backend.get_value(layer.bias))\n+    expected_ragged_values = tf.add(\n+        tf.matmul(ragged_inputs.flat_values,\n+                  keras.backend.get_value(layer.kernel)),\n+        keras.backend.get_value(layer.bias))\n+    expected_ragged = tf.RaggedTensor.from_row_splits(\n+        expected_ragged_values, row_splits=[0, 4, 6, 6, 9, 10])\n \n     self.assertAllClose(dense_outputs, expected_dense)\n     self.assertAllClose(sparse_outpus, expected_sparse)\n+    self.assertAllClose(ragged_outputs, expected_ragged)\n \n   def test_dense_dtype(self):\n-    inputs = tf.convert_to_tensor(\n-        np.random.randint(low=0, high=7, size=(2, 2)))\n+    inputs = tf.convert_to_tensor(np.random.randint(low=0, high=7, size=(2, 2)))\n     layer = keras.layers.Dense(5, dtype='float32')\n     outputs = layer(inputs)\n     self.assertEqual(outputs.dtype, 'float32')\n \n   def test_dense_with_policy(self):\n-    inputs = tf.convert_to_tensor(\n-        np.random.randint(low=0, high=7, size=(2, 2)))\n+    inputs = tf.convert_to_tensor(np.random.randint(low=0, high=7, size=(2, 2)))\n     layer = keras.layers.Dense(5, dtype=policy.Policy('mixed_float16'))\n     outputs = layer(inputs)\n     output_signature = layer.compute_output_signature(\n@@ -582,6 +602,7 @@ class CoreLayersTest(keras_parameterized.TestCase):\n class TFOpLambdaTest(keras_parameterized.TestCase):\n \n   def test_non_tf_symbol(self):\n+\n     def dummy_func(a, b):\n       return a + b\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23a8355dde02e43263ab53502e7a2e88c46615d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 18686 | Contributors (this commit): 105 | Commits (past 90d): 22 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -142,7 +142,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   class MyModel(tf.keras.Model):\n \n     def __init__(self):\n-      super(MyModel, self).__init__()\n+      super().__init__()\n       self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n       self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n \n@@ -163,7 +163,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   class MyModel(tf.keras.Model):\n \n     def __init__(self):\n-      super(MyModel, self).__init__()\n+      super().__init__()\n       self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n       self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n       self.dropout = tf.keras.layers.Dropout(0.5)\n\n@@ -82,7 +82,7 @@ class Model(training_lib.Model):\n   class MyModel(tf.keras.Model):\n \n     def __init__(self):\n-      super(MyModel, self).__init__()\n+      super().__init__()\n       self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n       self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n \n@@ -103,7 +103,7 @@ class Model(training_lib.Model):\n   class MyModel(tf.keras.Model):\n \n     def __init__(self):\n-      super(MyModel, self).__init__()\n+      super().__init__()\n       self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n       self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n       self.dropout = tf.keras.layers.Dropout(0.5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1b1939490f440c2d5e6a712476db7a164efe813", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 376 | Files Changed: 2 | Hunks: 20 | Methods Changed: 36 | Complexity Δ (Sum/Max): -39/0 | Churn Δ: 396 | Churn Cumulative: 1973 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,6 @@\n \"\"\"Contains the base ProcessingLayer and a subclass that uses Combiners.\"\"\"\n \n import abc\n-import collections\n \n from keras import backend\n from keras.engine import data_adapter\n@@ -285,134 +284,6 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n       self.built = True\n \n \n-# TODO(omalleyt): This class will be gradually replaced.\n-class CombinerPreprocessingLayer(PreprocessingLayer):\n-  \"\"\"Base class for PreprocessingLayers that do computation using a Combiner.\n-\n-  This class provides several helper methods to make creating a\n-  PreprocessingLayer easier. It assumes that the core of your computation will\n-  be done via a Combiner object. Subclassing this class to create a\n-  PreprocessingLayer allows your layer to be compatible with distributed\n-  computation.\n-\n-  This class is compatible with Tensorflow 2.0+.\n-  \"\"\"\n-\n-  def __init__(self, combiner, **kwargs):\n-    super(CombinerPreprocessingLayer, self).__init__(**kwargs)\n-    self.state_variables = collections.OrderedDict()\n-    self._combiner = combiner\n-    self._adapt_accumulator = None\n-\n-  def reset_state(self):  # pylint: disable=method-hidden\n-    self._adapt_accumulator = None\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def update_state(self, data):\n-    if self._adapt_accumulator is None:\n-      self._adapt_accumulator = self._get_accumulator()\n-    self._adapt_accumulator = self._combiner.compute(data,\n-                                                     self._adapt_accumulator)\n-\n-  def merge_state(self, layers):\n-    accumulators = ([self._get_accumulator()] +\n-                    [l._get_accumulator() for l in layers])  # pylint: disable=protected-access\n-    merged_accumulator = self._combiner.merge(accumulators)\n-    self._set_accumulator(merged_accumulator)\n-\n-  def finalize_state(self):\n-    if self._adapt_accumulator is not None:\n-      self._set_accumulator(self._adapt_accumulator)\n-\n-  def compile(self, run_eagerly=None, steps_per_execution=None):\n-    # TODO(omalleyt): Remove this once sublayers are switched to new APIs.\n-    if run_eagerly is None:\n-      run_eagerly = True\n-    super(CombinerPreprocessingLayer, self).compile(\n-        run_eagerly=run_eagerly, steps_per_execution=steps_per_execution)\n-\n-  def adapt(self, data, batch_size=None, steps=None):\n-    super(CombinerPreprocessingLayer, self).adapt(\n-        data, batch_size=batch_size, steps=steps)\n-\n-  def _add_state_variable(self,\n-                          name,\n-                          shape,\n-                          dtype,\n-                          initializer=None,\n-                          partitioner=None,\n-                          use_resource=None,\n-                          **kwargs):\n-    \"\"\"Add a variable that can hold state which is updated during adapt().\n-\n-    Args:\n-      name: Variable name.\n-      shape: Variable shape. Defaults to scalar if unspecified.\n-      dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n-      initializer: initializer instance (callable).\n-      partitioner: Partitioner to be passed to the `Trackable` API.\n-      use_resource: Whether to use `ResourceVariable`\n-      **kwargs: Additional keyword arguments. Accepted values are `getter` and\n-        `collections`.\n-\n-    Returns:\n-      The created variable.\n-    \"\"\"\n-    weight = self.add_weight(\n-        name=name,\n-        shape=shape,\n-        dtype=dtype,\n-        initializer=initializer,\n-        regularizer=None,\n-        trainable=False,\n-        constraint=None,\n-        partitioner=partitioner,\n-        use_resource=use_resource,\n-        **kwargs)\n-    # TODO(momernick): Do not allow collisions here.\n-    self.state_variables[name] = weight\n-    return weight\n-\n-  def _restore_updates(self):\n-    \"\"\"Recreates a dict of updates from the layer's weights.\"\"\"\n-    data_dict = {}\n-    for name, var in self.state_variables.items():\n-      data_dict[name] = var.numpy()\n-    return data_dict\n-\n-  def _get_accumulator(self):\n-    if self._is_adapted:\n-      return self._combiner.restore(self._restore_updates())\n-    else:\n-      return None\n-\n-  def _set_accumulator(self, accumulator):\n-    updates = self._combiner.extract(accumulator)\n-    self._set_state_variables(updates)\n-    self._adapt_accumulator = None  # Reset accumulator from adapt.\n-\n-  def _set_state_variables(self, updates):\n-    \"\"\"Directly update the internal state of this Layer.\n-\n-    This method expects a string-keyed dict of {state_variable_name: state}. The\n-    precise nature of the state, and the names associated, are describe by\n-    the subclasses of CombinerPreprocessingLayer.\n-\n-    Args:\n-      updates: A string keyed dict of weights to update.\n-\n-    Raises:\n-      RuntimeError: if 'build()' was not called before 'set_processing_state'.\n-    \"\"\"\n-    # TODO(momernick): Do we need to do any more input sanitization?\n-    if not self.built:\n-      raise RuntimeError('_set_state_variables() must be called after build().')\n-\n-    with tf.init_scope():\n-      for var_name, value in updates.items():\n-        self.state_variables[var_name].assign(value)\n-\n-\n def convert_to_list(values, sparse_default_value=None):\n   \"\"\"Convert a TensorLike, CompositeTensor, or ndarray into a Python list.\"\"\"\n   if tf_utils.is_ragged(values):\n@@ -450,132 +321,6 @@ def convert_to_list(values, sparse_default_value=None):\n   return values\n \n \n-# TODO(omalleyt): This class will be gradually replaced.\n-class Combiner(object):\n-  \"\"\"Functional object that defines a shardable computation.\n-\n-  This object defines functions required to create and manipulate data objects.\n-  These data objects, referred to below as 'accumulators', are computation-\n-  specific and may be implemented alongside concrete subclasses of Combiner\n-  (if necessary - some computations may be simple enough that standard Python\n-  types can be used as accumulators).\n-\n-  The intent for this class is that by describing computations in this way, we\n-  can arbitrarily shard a dataset, perform computations on a subset, and then\n-  merge the computation into a final result. This enables distributed\n-  computation.\n-\n-  The combiner itself does not own any state - all computational state is owned\n-  by the accumulator objects. This is so that we can have an arbitrary number of\n-  Combiners (thus sharding the computation N ways) without risking any change\n-  to the underlying computation. These accumulator objects are uniquely\n-  associated with each Combiner; a Combiner defines what the accumulator object\n-  should be and will only work with accumulators of that type.\n-  \"\"\"\n-  __metaclass__ = abc.ABCMeta\n-\n-  def __repr__(self):\n-    return '<{}>'.format(self.__class__.__name__)\n-\n-  @abc.abstractmethod\n-  def compute(self, batch_values, accumulator=None):\n-    \"\"\"Compute a step in this computation, returning a new accumulator.\n-\n-    This method computes a step of the computation described by this Combiner.\n-    If an accumulator is passed, the data in that accumulator is also used; so\n-    compute(batch_values) results in f(batch_values), while\n-    compute(batch_values, accumulator) results in\n-    merge(f(batch_values), accumulator).\n-\n-    Args:\n-      batch_values: A list of ndarrays representing the values of the inputs for\n-        this step of the computation.\n-      accumulator: the current accumulator. Can be None.\n-\n-    Returns:\n-      An accumulator that includes the passed batch of inputs.\n-    \"\"\"\n-    pass\n-\n-  @abc.abstractmethod\n-  def merge(self, accumulators):\n-    \"\"\"Merge several accumulators to a single accumulator.\n-\n-    This method takes the partial values in several accumulators and combines\n-    them into a single accumulator. This computation must not be order-specific\n-    (that is, merge([a, b]) must return the same result as merge([b, a]).\n-\n-    Args:\n-      accumulators: the accumulators to merge, as a list.\n-\n-    Returns:\n-      A merged accumulator.\n-    \"\"\"\n-    pass\n-\n-  @abc.abstractmethod\n-  def extract(self, accumulator):\n-    \"\"\"Convert an accumulator into a dict of output values.\n-\n-    Args:\n-      accumulator: The accumulator to convert.\n-\n-    Returns:\n-      A dict of ndarrays representing the data in this accumulator.\n-    \"\"\"\n-    pass\n-\n-  @abc.abstractmethod\n-  def restore(self, output):\n-    \"\"\"Create an accumulator based on 'output'.\n-\n-    This method creates a new accumulator with identical internal state to the\n-    one used to create the data in 'output'. This means that if you do\n-\n-    output_data = combiner.extract(accumulator_1)\n-    accumulator_2 = combiner.restore(output_data)\n-\n-    then accumulator_1 and accumulator_2 will have identical internal state, and\n-    computations using either of them will be equivalent.\n-\n-    Args:\n-      output: The data output from a previous computation. Should be in the same\n-        form as provided by 'extract_output'.\n-\n-    Returns:\n-      A new accumulator.\n-    \"\"\"\n-    pass\n-\n-  @abc.abstractmethod\n-  def serialize(self, accumulator):\n-    \"\"\"Serialize an accumulator for a remote call.\n-\n-    This function serializes an accumulator to be sent to a remote process.\n-\n-    Args:\n-      accumulator: The accumulator to serialize.\n-\n-    Returns:\n-      A byte string representing the passed accumulator.\n-    \"\"\"\n-    pass\n-\n-  @abc.abstractmethod\n-  def deserialize(self, encoded_accumulator):\n-    \"\"\"Deserialize an accumulator received from 'serialize()'.\n-\n-    This function deserializes an accumulator serialized by 'serialize()'.\n-\n-    Args:\n-      encoded_accumulator: A byte string representing an accumulator.\n-\n-    Returns:\n-      The accumulator represented by the passed byte_string.\n-    \"\"\"\n-    pass\n-\n-\n def _disallow_inside_tf_function(method_name):\n   \"\"\"Disallow calling a method inside a `tf.function`.\"\"\"\n   if tf.inside_function():\n\n@@ -14,95 +14,42 @@\n # ==============================================================================\n \"\"\"Tests for Keras' base preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import json\n import os\n-\n from absl.testing import parameterized\n-import numpy as np\n+\n \n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import base_preprocessing_layer\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n-# Define a test-only implementation of CombinerPreprocessingLayer to validate\n+# Define a test-only implementation of BasePreprocessingLayer to validate\n # its correctness directly.\n-class AddingPreprocessingLayer(\n-    base_preprocessing_layer.CombinerPreprocessingLayer):\n-  _SUM_NAME = \"sum\"\n-\n-  def __init__(self, **kwargs):\n-    super(AddingPreprocessingLayer, self).__init__(\n-        combiner=self.AddingCombiner(), **kwargs)\n+class AddingPreprocessingLayer(base_preprocessing_layer.PreprocessingLayer):\n \n   def build(self, input_shape):\n     super(AddingPreprocessingLayer, self).build(input_shape)\n-    self._sum = self._add_state_variable(\n-        name=self._SUM_NAME,\n-        shape=(1,),\n-        dtype=tf.float32,\n-        initializer=tf.compat.v1.zeros_initializer)\n+    self.sum = tf.Variable(0., dtype=tf.float32)\n+\n+  def update_state(self, data):\n+    self.sum.assign_add(tf.reduce_sum(tf.cast(data, tf.float32)))\n \n   def reset_state(self):  # pylint: disable=method-hidden\n-    self._sum.assign([0.])\n+    self.sum.assign(0.)\n \n   def set_total(self, sum_value):\n     \"\"\"This is an example of how a subclass would implement a direct setter.\n \n-    These methods should generally just create a dict mapping the correct names\n-    to the relevant passed values, and call self._set_state_variables() with the\n-    dict of data.\n-\n     Args:\n       sum_value: The total to set.\n     \"\"\"\n-    self._set_state_variables({self._SUM_NAME: [sum_value]})\n+    self.sum.assign(sum_value)\n \n   def call(self, inputs):\n-    return inputs + self._sum\n-\n-  # Define a Combiner for this layer class.\n-  class AddingCombiner(base_preprocessing_layer.Combiner):\n-\n-    def compute(self, batch_values, accumulator=None):\n-      \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n-      new_accumulator = 0 if batch_values is None else np.sum(batch_values)\n-      if accumulator is None:\n-        return new_accumulator\n-      else:\n-        return self.merge([accumulator, new_accumulator])\n-\n-    def merge(self, accumulators):\n-      \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n-      # Combine accumulators and return the result.\n-      result = accumulators[0]\n-      for accumulator in accumulators[1:]:\n-        result = np.sum([np.sum(result), np.sum(accumulator)])\n-      return result\n-\n-    def extract(self, accumulator):\n-      \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n-      # We have to add an additional dimension here because the weight shape\n-      # is (1,) not None.\n-      return {AddingPreprocessingLayer._SUM_NAME: [accumulator]}\n-\n-    def restore(self, output):\n-      \"\"\"Create an accumulator based on 'output'.\"\"\"\n-      # There is no special internal state here, so we just return the relevant\n-      # internal value. We take the [0] value here because the weight itself\n-      # is of the shape (1,) and we want the scalar contained inside it.\n-      return output[AddingPreprocessingLayer._SUM_NAME][0]\n-\n-    def serialize(self, accumulator):\n-      \"\"\"Serialize an accumulator for a remote call.\"\"\"\n-      return tf.compat.as_bytes(json.dumps(accumulator))\n-\n-    def deserialize(self, encoded_accumulator):\n-      \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n-      return json.loads(tf.compat.as_text(encoded_accumulator))\n+    return inputs + self.sum\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -134,17 +81,6 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n                                   \".*infinite number of elements.*\"):\n         layer.adapt(input_dataset)\n \n-  def test_pre_build_injected_update_with_no_build_fails(self):\n-    \"\"\"Test external update injection before build() is called fails.\"\"\"\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-\n-    layer = AddingPreprocessingLayer()\n-    combiner = layer._combiner\n-    updates = combiner.extract(combiner.compute(input_dataset))\n-\n-    with self.assertRaisesRegex(RuntimeError, \".*called after build.*\"):\n-      layer._set_state_variables(updates)\n-\n   def test_setter_update(self):\n     \"\"\"Test the prototyped setter method.\"\"\"\n     input_data = keras.Input(shape=(1,))\n@@ -185,39 +121,6 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n \n-  def test_pre_build_injected_update(self):\n-    \"\"\"Test external update injection before build() is called.\"\"\"\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-\n-    layer = AddingPreprocessingLayer()\n-    combiner = layer._combiner\n-    updates = combiner.extract(combiner.compute(input_dataset))\n-\n-    layer.build((1,))\n-    layer._set_state_variables(updates)\n-\n-    input_data = keras.Input(shape=(1,))\n-    output = layer(input_data)\n-    model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n-\n-    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n-\n-  def test_post_build_injected_update(self):\n-    \"\"\"Test external update injection after build() is called.\"\"\"\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-    input_data = keras.Input(shape=(1,))\n-    layer = AddingPreprocessingLayer()\n-    output = layer(input_data)\n-    model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n-\n-    combiner = layer._combiner\n-    updates = combiner.extract(combiner.compute(input_dataset))\n-    layer._set_state_variables(updates)\n-\n-    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n-\n   def test_pre_build_adapt_update_dataset(self):\n     \"\"\"Test that preproc layers can adapt() before build() is called.\"\"\"\n     input_dataset = tf.data.Dataset.from_tensor_slices(\n@@ -291,15 +194,14 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n   def test_adapt_sets_input_shape_rank(self):\n     \"\"\"Check that `.adapt()` sets the `input_shape`'s rank.\"\"\"\n     # Shape: (3,1,2)\n-    adapt_dataset = np.array([[[1., 2.]],\n-                              [[3., 4.]],\n-                              [[5., 6.]]], dtype=np.float32)\n+    adapt_dataset = np.array([[[1., 2.]], [[3., 4.]], [[5., 6.]]],\n+                             dtype=np.float32)\n \n     layer = AddingPreprocessingLayer()\n     layer.adapt(adapt_dataset)\n \n-    input_dataset = np.array([[[1., 2.], [3., 4.]],\n-                              [[3., 4.], [5., 6.]]], dtype=np.float32)\n+    input_dataset = np.array([[[1., 2.], [3., 4.]], [[3., 4.], [5., 6.]]],\n+                             dtype=np.float32)\n     layer(input_dataset)\n \n     model = keras.Sequential([layer])\n@@ -309,9 +211,8 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n   def test_adapt_doesnt_overwrite_input_shape(self):\n     \"\"\"Check that `.adapt()` doesn't change the `input_shape`.\"\"\"\n     # Shape: (3, 1, 2)\n-    adapt_dataset = np.array([[[1., 2.]],\n-                              [[3., 4.]],\n-                              [[5., 6.]]], dtype=np.float32)\n+    adapt_dataset = np.array([[[1., 2.]], [[3., 4.]], [[5., 6.]]],\n+                             dtype=np.float32)\n \n     layer = AddingPreprocessingLayer(input_shape=[1, 2])\n     layer.adapt(adapt_dataset)\n@@ -355,10 +256,8 @@ class ConvertToListTest(keras_parameterized.TestCase):\n           \"inputs\": lambda: tf.constant([[1, 2, 3], [4, 5, 6]]),\n           \"expected\": [[1, 2, 3], [4, 5, 6]]\n       }, {\n-          \"testcase_name\":\n-              \"ragged_tensor\",\n-          \"inputs\":\n-              lambda: tf.ragged.constant([[1, 2, 3, 4], [4, 5, 6]]),\n+          \"testcase_name\": \"ragged_tensor\",\n+          \"inputs\": lambda: tf.ragged.constant([[1, 2, 3, 4], [4, 5, 6]]),\n           \"expected\": [[1, 2, 3, 4], [4, 5, 6]]\n       }, {\n           \"testcase_name\": \"sparse_tensor\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1b50c4b77c685816710e9c79cfa92be04d62fc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 40 | Files Changed: 4 | Hunks: 32 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 4498 | Contributors (this commit): 6 | Commits (past 90d): 28 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,17 +14,17 @@\n # ==============================================================================\n \"\"\"Tests for image preprocessing layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import functools\n from absl.testing import parameterized\n-import numpy as np\n-from tensorflow.python.distribute.mirrored_strategy import MirroredStrategy\n+\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import sequential\n from keras.layers.preprocessing import image_preprocessing\n-from keras.utils.generic_utils import CustomObjectScope\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.distribute.mirrored_strategy import MirroredStrategy\n from tensorflow.python.ops import gen_stateful_random_ops\n from tensorflow.python.ops import gen_stateless_random_ops_v2\n from tensorflow.python.ops import stateless_random_ops\n@@ -65,7 +65,6 @@ class ResizingTest(keras_parameterized.TestCase):\n       'crop_to_aspect_ratio': True,\n   }, 3, 2))\n   def test_down_sampling(self, kwargs, expected_height, expected_width):\n-    with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n     self._run_test(kwargs, expected_height, expected_width)\n \n   @parameterized.named_parameters(('up_sample_bilinear_10_by_12', {\n@@ -85,7 +84,6 @@ class ResizingTest(keras_parameterized.TestCase):\n       'crop_to_aspect_ratio': True,\n   }, 12, 14))\n   def test_up_sampling(self, kwargs, expected_height, expected_width):\n-    with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n     self._run_test(kwargs, expected_height, expected_width)\n \n   def test_down_sampling_numeric(self):\n@@ -126,7 +124,6 @@ class ResizingTest(keras_parameterized.TestCase):\n       'interpolation': 'bilinear'\n   }, 10, 4))\n   def test_reshaping(self, kwargs, expected_height, expected_width):\n-    with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n     self._run_test(kwargs, expected_height, expected_width)\n \n   def test_invalid_interpolation(self):\n@@ -236,19 +233,16 @@ class CenterCropTest(keras_parameterized.TestCase):\n   @parameterized.named_parameters(('center_crop_3_by_4', 3, 4),\n                                   ('center_crop_3_by_2', 3, 2))\n   def test_center_crop_aligned(self, expected_height, expected_width):\n-    with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n     self._run_test(expected_height, expected_width)\n \n   @parameterized.named_parameters(('center_crop_4_by_5', 4, 5),\n                                   ('center_crop_4_by_3', 4, 3))\n   def test_center_crop_mis_aligned(self, expected_height, expected_width):\n-    with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n     self._run_test(expected_height, expected_width)\n \n   @parameterized.named_parameters(('center_crop_4_by_6', 4, 6),\n                                   ('center_crop_3_by_2', 3, 2))\n   def test_center_crop_half_mis_aligned(self, expected_height, expected_width):\n-    with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n     self._run_test(expected_height, expected_width)\n \n   @parameterized.named_parameters(('center_crop_5_by_12', 5, 12),\n@@ -307,7 +301,6 @@ class RandomCropTest(keras_parameterized.TestCase):\n     # is enabled.\n     with self.assertRaises(\n         (tf.errors.InvalidArgumentError, tf.errors.InternalError)):\n-      with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n       self._run_test(expected_height, expected_width)\n \n   def test_training_with_mock(self):\n@@ -331,7 +324,6 @@ class RandomCropTest(keras_parameterized.TestCase):\n   @parameterized.named_parameters(('random_crop_4_by_6', 4, 6),\n                                   ('random_crop_3_by_2', 3, 2))\n   def test_random_crop_output_shape(self, expected_height, expected_width):\n-    with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n     self._run_test(expected_height, expected_width)\n \n   def test_random_crop_full_height(self):\n@@ -465,11 +457,9 @@ class RandomFlipTest(keras_parameterized.TestCase):\n       ('random_flip_vertical', 'vertical'),\n       ('random_flip_both', 'horizontal_and_vertical'))\n   def test_random_flip(self, mode):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     self._run_test(mode)\n \n   def test_random_flip_horizontal_half(self):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     np.random.seed(1337)\n     mock_random = [1, 0]\n     mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n@@ -479,7 +469,6 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     self._run_test('horizontal', expected_output, mock_random)\n \n   def test_random_flip_vertical_half(self):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     np.random.seed(1337)\n     mock_random = [1, 0]\n     mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n@@ -489,7 +478,6 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     self._run_test('vertical', expected_output, mock_random)\n \n   def test_random_flip_inference(self):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -498,7 +486,6 @@ class RandomFlipTest(keras_parameterized.TestCase):\n       self.assertAllClose(expected_output, actual_output)\n \n   def test_random_flip_default(self):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = np.flip(np.flip(input_images, axis=1), axis=2)\n     mock_random = [1, 1]\n@@ -521,7 +508,6 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_random_flip_unbatched_image(self):\n-    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n     input_image = np.random.random((4, 4, 1)).astype(np.float32)\n     expected_output = np.flip(input_image, axis=0)\n     # mock_random = np.reshape([0.], [1, 1, 1])\n@@ -569,23 +555,17 @@ class RandomContrastTest(keras_parameterized.TestCase):\n                                   ('random_contrast_5_by_2', 0.5, 0.2),\n                                   ('random_contrast_10_by_10', 1.0, 1.0))\n   def test_random_contrast(self, lower, upper):\n-    with CustomObjectScope(\n-        {'RandomContrast': image_preprocessing.RandomContrast}):\n     self._run_test(lower, upper)\n \n   @parameterized.named_parameters(('random_contrast_amplitude_2', 0.2),\n                                   ('random_contrast_amplitude_5', 0.5))\n   def test_random_contrast_amplitude(self, amplitude):\n-    with CustomObjectScope(\n-        {'RandomContrast': image_preprocessing.RandomContrast}):\n     input_images = np.random.random((2, 5, 8, 3))\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomContrast(amplitude)\n       layer(input_images)\n \n   def test_random_contrast_inference(self):\n-    with CustomObjectScope(\n-        {'RandomContrast': image_preprocessing.RandomContrast}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -594,8 +574,6 @@ class RandomContrastTest(keras_parameterized.TestCase):\n       self.assertAllClose(expected_output, actual_output)\n \n   def test_random_contrast_int_dtype(self):\n-    with CustomObjectScope(\n-        {'RandomContrast': image_preprocessing.RandomContrast}):\n     input_images = np.random.randint(low=0, high=255, size=(2, 5, 8, 3))\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomContrast((0.1, 0.2))\n@@ -791,8 +769,6 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         self.assertAllEqual(expected_output, output_image)\n \n   def test_random_translation_inference(self):\n-    with CustomObjectScope(\n-        {'RandomTranslation': image_preprocessing.RandomTranslation}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -1213,8 +1189,6 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     self._run_test(factor)\n \n   def test_random_rotation_inference(self):\n-    with CustomObjectScope(\n-        {'RandomTranslation': image_preprocessing.RandomRotation}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -1340,7 +1314,6 @@ class RandomZoomTest(keras_parameterized.TestCase):\n         self.assertAllEqual(expected_output, output_image)\n \n   def test_random_zoom_inference(self):\n-    with CustomObjectScope({'RandomZoom': image_preprocessing.RandomZoom}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -1451,7 +1424,6 @@ class RandomHeightTest(keras_parameterized.TestCase):\n       image_preprocessing.RandomHeight((-1.5, .4))\n \n   def test_random_height_inference(self):\n-    with CustomObjectScope({'RandomHeight': image_preprocessing.RandomHeight}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n@@ -1560,7 +1532,6 @@ class RandomWidthTest(keras_parameterized.TestCase):\n       image_preprocessing.RandomWidth((-1.5, .4))\n \n   def test_random_width_inference(self):\n-    with CustomObjectScope({'RandomWidth': image_preprocessing.RandomWidth}):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n     with testing_utils.use_gpu():\n\n@@ -29,7 +29,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import integer_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.utils.generic_utils import CustomObjectScope\n \n \n def _get_end_to_end_test_cases():\n@@ -99,7 +98,6 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n       vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(\n           input_shape[0])\n \n-    with CustomObjectScope({\"IntegerLookup\": cls}):\n     output_data = testing_utils.layer_test(\n         cls,\n         kwargs=kwargs,\n\n@@ -27,7 +27,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import normalization\n from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.utils.generic_utils import CustomObjectScope\n \n \n def _get_layer_computation_test_cases():\n@@ -192,7 +191,6 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n \n   def test_layer_api_compatibility(self):\n     cls = normalization.Normalization\n-    with CustomObjectScope({\"Normalization\": cls}):\n     output_data = testing_utils.layer_test(\n         cls,\n         kwargs={\"axis\": -1},\n\n@@ -25,7 +25,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import string_lookup\n-from keras.utils.generic_utils import CustomObjectScope\n \n \n def _get_end_to_end_test_cases():\n@@ -93,7 +92,6 @@ class StringLookupLayerTest(keras_parameterized.TestCase,\n       vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(\n           input_shape[0])\n \n-    with CustomObjectScope({\"StringLookup\": cls}):\n     output_data = testing_utils.layer_test(\n         cls,\n         kwargs=kwargs,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
