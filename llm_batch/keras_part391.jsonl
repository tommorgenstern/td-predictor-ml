{"custom_id": "keras#5c4074a820a08505cf87562f60d21adfdc2dfb05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 134 | Files Changed: 2 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): -6/4 | Churn Δ: 156 | Churn Cumulative: 273 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,105 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Benchmark for Keras categorical_encoding preprocessing layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import itertools\n-import time\n-\n-import numpy as np\n-\n-import keras\n-from keras.layers.preprocessing import category_crossing\n-\n-tf.compat.v1.enable_v2_behavior()\n-\n-\n-# word_gen creates random sequences of ASCII letters (both lowercase and upper).\n-# The number of unique strings is ~2,700.\n-def int_gen():\n-  for _ in itertools.count(1):\n-    yield (np.random.randint(0, 5, (1,)), np.random.randint(0, 7, (1,)))\n-\n-\n-class BenchmarkLayer(tf.test.Benchmark):\n-  \"\"\"Benchmark the layer forward pass.\"\"\"\n-\n-  def run_dataset_implementation(self, batch_size):\n-    num_repeats = 5\n-    starts = []\n-    ends = []\n-    for _ in range(num_repeats):\n-      ds = tf.data.Dataset.from_generator(\n-          int_gen, (tf.int64, tf.int64),\n-          (tf.TensorShape([1]), tf.TensorShape([1])))\n-      ds = ds.shuffle(batch_size * 100)\n-      ds = ds.batch(batch_size)\n-      num_batches = 5\n-      ds = ds.take(num_batches)\n-      ds = ds.prefetch(num_batches)\n-      starts.append(time.time())\n-      # Benchmarked code begins here.\n-      for i in ds:\n-        _ = tf.sparse.cross([i[0], i[1]])\n-      # Benchmarked code ends here.\n-      ends.append(time.time())\n-\n-    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches\n-    return avg_time\n-\n-  def bm_layer_implementation(self, batch_size):\n-    input_1 = keras.Input(shape=(1,), dtype=tf.int64, name=\"word\")\n-    input_2 = keras.Input(shape=(1,), dtype=tf.int64, name=\"int\")\n-    layer = category_crossing.CategoryCrossing()\n-    _ = layer([input_1, input_2])\n-\n-    num_repeats = 5\n-    starts = []\n-    ends = []\n-    for _ in range(num_repeats):\n-      ds = tf.data.Dataset.from_generator(\n-          int_gen, (tf.int64, tf.int64),\n-          (tf.TensorShape([1]), tf.TensorShape([1])))\n-      ds = ds.shuffle(batch_size * 100)\n-      ds = ds.batch(batch_size)\n-      num_batches = 5\n-      ds = ds.take(num_batches)\n-      ds = ds.prefetch(num_batches)\n-      starts.append(time.time())\n-      # Benchmarked code begins here.\n-      for i in ds:\n-        _ = layer([i[0], i[1]])\n-      # Benchmarked code ends here.\n-      ends.append(time.time())\n-\n-    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches\n-    name = \"category_crossing|batch_%s\" % batch_size\n-    baseline = self.run_dataset_implementation(batch_size)\n-    extras = {\n-        \"dataset implementation baseline\": baseline,\n-        \"delta seconds\": (baseline - avg_time),\n-        \"delta percent\": ((baseline - avg_time) / baseline) * 100\n-    }\n-    self.report_benchmark(\n-        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)\n-\n-  def benchmark_vocab_size_by_batch(self):\n-    for batch in [32, 64, 256]:\n-      self.bm_layer_implementation(batch_size=batch)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -13,39 +13,34 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Benchmark for KPL implementation of categorical cross hash columns with dense inputs.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n \n import keras\n-from tensorflow.python.eager.def_function import function as tf_function\n-from keras.layers.preprocessing import category_crossing\n-from keras.layers.preprocessing import hashing\n+from keras.layers.preprocessing import hashed_crossing\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.eager.def_function import function as tf_function\n \n-# This is required as of 3/2021 because otherwise we drop into graph mode.\n tf.compat.v1.enable_v2_behavior()\n \n NUM_REPEATS = 10\n BATCH_SIZES = [32, 256]\n \n \n-def embedding_varlen(batch_size, max_length):\n+def embedding_varlen(batch_size):\n   \"\"\"Benchmark a variable-length embedding.\"\"\"\n   # Data and constants.\n-\n   num_buckets = 10000\n-  vocab = fc_bm.create_vocabulary(32768)\n-  data_a = fc_bm.create_string_data(\n-      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n-  data_b = fc_bm.create_string_data(\n-      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n+  data_a = tf.random.uniform(shape=(batch_size * NUM_REPEATS, 1),\n+                             maxval=32768,\n+                             dtype=tf.int64)\n+  data_b = tf.strings.as_string(data_a)\n \n   # Keras implementation\n-  input_1 = keras.Input(shape=(None,), name=\"data_a\", dtype=tf.string)\n-  input_2 = keras.Input(shape=(None,), name=\"data_b\", dtype=tf.string)\n-  crossed_data = category_crossing.CategoryCrossing()([input_1, input_2])\n-  hashed_data = hashing.Hashing(num_buckets)(crossed_data)\n-  model = keras.Model([input_1, input_2], hashed_data)\n+  input_1 = keras.Input(shape=(1,), name=\"data_a\", dtype=tf.int64)\n+  input_2 = keras.Input(shape=(1,), name=\"data_b\", dtype=tf.string)\n+  outputs = hashed_crossing.HashedCrossing(num_buckets)([input_1, input_2])\n+  model = keras.Model([input_1, input_2], outputs)\n \n   # FC implementation\n   fc = tf.feature_column.crossed_column([\"data_a\", \"data_b\"], num_buckets)\n@@ -53,22 +48,20 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(\n+        tf.__internal__.feature_column.FeatureTransformationCache(tensors),\n+        None)\n \n   # Benchmark runs\n   keras_data = {\n-      \"data_a\":\n-          data_a.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n-      \"data_b\":\n-          data_b.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+      \"data_a\": data_a,\n+      \"data_b\": data_b,\n   }\n   k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n \n   fc_data = {\n-      \"data_a\":\n-          data_a.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n-      \"data_b\":\n-          data_b.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+      \"data_a\": data_a,\n+      \"data_b\": data_b,\n   }\n   fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n \n@@ -80,8 +73,8 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n   def benchmark_layer(self):\n     for batch in BATCH_SIZES:\n-      name = \"cross_hash|dense|batch_%s\" % batch\n-      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      name = \"hashed_cross|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch)\n       self.report(name, k_time, f_time, NUM_REPEATS)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdf220c4212f0743afabd7ff0bafcd3f0edc5ef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 335 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ def timeseries_dataset_from_array(\n     sampling_rate: Period between successive individual timesteps\n       within sequences. For rate `r`, timesteps\n       `data[i], data[i + r], ... data[i + sequence_length]`\n-      are used for create a sample sequence.\n+      are used for creating a sample sequence.\n     batch_size: Number of timeseries samples in each batch\n       (except maybe the last one). If `None`, the data will not be batched\n       (the dataset will yield individual samples).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e05f2edacdfe951260d518f872d4818a286297c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 413 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 12 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 413 | Churn Cumulative: 6839 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -598,6 +598,126 @@ class BinaryCrossentropy(LossFunctionWrapper):\n     self.from_logits = from_logits\n \n \n+@keras_export('keras.losses.BinaryFocalCrossentropy')\n+class BinaryFocalCrossentropy(LossFunctionWrapper):\n+  \"\"\"Computes the focal cross-entropy loss between true labels and predictions.\n+\n+  Binary cross-entropy loss is often used for binary (0 or 1) classification\n+  tasks. The loss function requires the following inputs:\n+\n+  - `y_true` (true label): This is either 0 or 1.\n+  - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n+    floating-point value which either represents a\n+    [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n+    when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when\n+    `from_logits=False`).\n+\n+  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+  helps to apply a \"focal factor\" to down-weight easy examples and focus more on\n+  hard examples. By default, the focal tensor is computed as follows:\n+\n+  `focal_factor = (1 - output) ** gamma` for class 1\n+  `focal_factor = output ** gamma` for class 0\n+  where `gamma` is a focusing parameter. When `gamma=0`, this function is\n+  equivalent to the binary crossentropy loss.\n+\n+  With the `compile()` API:\n+\n+  ```python\n+  model.compile(\n+    loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),\n+    ....\n+  )\n+  ```\n+\n+  As a standalone function:\n+\n+  >>> # Example 1: (batch_size = 1, number of samples = 4)\n+  >>> y_true = [0, 1, 0, 0]\n+  >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2, from_logits=True)\n+  >>> loss(y_true, y_pred).numpy()\n+  0.691\n+\n+  >>> # Example 2: (batch_size = 2, number of samples = 4)\n+  >>> y_true = [[0, 1], [0, 0]]\n+  >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+  >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)\n+  >>> loss(y_true, y_pred).numpy()\n+  0.647\n+\n+  >>> # Using 'sample_weight' attribute\n+  >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n+  0.133\n+\n+  >>> # Using 'sum' reduction` type.\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4, from_logits=True,\n+  ...     reduction=tf.keras.losses.Reduction.SUM)\n+  >>> loss(y_true, y_pred).numpy()\n+  1.222\n+\n+  >>> # Using 'none' reduction type.\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=5, from_logits=True,\n+  ...     reduction=tf.keras.losses.Reduction.NONE)\n+  >>> loss(y_true, y_pred).numpy()\n+  array([0.0017 1.1561], dtype=float32)\n+\n+  Args:\n+    gamma: A focusing parameter used to compute the focal factor, default is\n+      `2.0` as mentioned in the reference\n+      [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+    from_logits: Whether to interpret `y_pred` as a tensor of\n+      [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n+      assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).\n+    label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs. When >\n+      `0`, we compute the loss between the predicted labels and a smoothed\n+      version of the true labels, where the smoothing squeezes the labels\n+      towards `0.5`. Larger values of `label_smoothing` correspond to heavier\n+      smoothing.\n+    axis: The axis along which to compute crossentropy (the features axis).\n+      Defaults to `-1`.\n+    reduction: Type of `tf.keras.losses.Reduction` to apply to\n+      loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n+      option will be determined by the usage context. For almost all cases\n+      this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n+      `tf.distribute.Strategy`, outside of built-in training loops such as\n+      `tf.keras`, `compile()` and `fit()`, using `SUM_OVER_BATCH_SIZE` or\n+      `AUTO` will raise an error. Please see this custom training [tutorial](\n+      https://www.tensorflow.org/tutorials/distribute/custom_training) for\n+      more details.\n+    name: Name for the op. Defaults to 'binary_focal_crossentropy'.\n+  \"\"\"\n+\n+  def __init__(\n+      self,\n+      gamma=2.0,\n+      from_logits=False,\n+      label_smoothing=0.,\n+      axis=-1,\n+      reduction=losses_utils.ReductionV2.AUTO,\n+      name='binary_focal_crossentropy',\n+  ):\n+    \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n+    super().__init__(\n+        binary_focal_crossentropy,\n+        gamma=gamma,\n+        name=name,\n+        reduction=reduction,\n+        from_logits=from_logits,\n+        label_smoothing=label_smoothing,\n+        axis=axis)\n+    self.from_logits = from_logits\n+    self.gamma = gamma\n+\n+  def get_config(self):\n+    config = {\n+        'gamma': self.gamma,\n+    }\n+    base_config = super(BinaryFocalCrossentropy, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n @keras_export('keras.losses.CategoricalCrossentropy')\n class CategoricalCrossentropy(LossFunctionWrapper):\n   \"\"\"Computes the crossentropy loss between the labels and predictions.\n@@ -1850,6 +1970,116 @@ def _ragged_tensor_binary_crossentropy(y_true,\n   return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n \n \n+@keras_export('keras.metrics.binary_focal_crossentropy',\n+              'keras.losses.binary_focal_crossentropy')\n+@tf.__internal__.dispatch.add_dispatch_support\n+def binary_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.,\n+    axis=-1,\n+):\n+  \"\"\"Computes the binary focal crossentropy loss.\n+\n+  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+  helps to apply a focal factor to down-weight easy examples and focus more on\n+  hard examples. By default, the focal tensor is computed as follows:\n+\n+  `focal_factor = (1 - output)**gamma` for class 1\n+  `focal_factor = output**gamma` for class 0\n+  where `gamma` is a focusing parameter. When `gamma` = 0, this function is\n+  equivalent to the binary crossentropy loss.\n+\n+  Standalone usage:\n+\n+  >>> y_true = [[0, 1], [0, 0]]\n+  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n+  >>> loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred, gamma=2)\n+  >>> assert loss.shape == (2,)\n+  >>> loss.numpy()\n+  array([0.330, 0.206], dtype=float32)\n+\n+  Args:\n+    y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n+    y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n+    gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n+    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n+      we assume that `y_pred` encodes a probability distribution.\n+    label_smoothing: Float in `[0, 1]`. If higher than 0 then smooth the labels\n+      by squeezing them towards `0.5`, i.e., using `1. - 0.5 * label_smoothing`\n+      for the target class and `0.5 * label_smoothing` for the non-target class.\n+    axis: The axis along which the mean is computed. Defaults to `-1`.\n+\n+  Returns:\n+    Binary focal crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n+  \"\"\"\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true = tf.cast(y_true, y_pred.dtype)\n+  label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n+\n+  def _smooth_labels():\n+    return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+  y_true = tf.__internal__.smart_cond.smart_cond(label_smoothing,\n+                                                 _smooth_labels, lambda: y_true)\n+\n+  return backend.mean(\n+      backend.binary_focal_crossentropy(\n+          target=y_true,\n+          output=y_pred,\n+          gamma=gamma,\n+          from_logits=from_logits,\n+      ),\n+      axis=axis,\n+  )\n+\n+\n+@dispatch.dispatch_for_types(binary_focal_crossentropy, tf.RaggedTensor)\n+def _ragged_tensor_binary_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.,\n+    axis=-1,\n+):\n+  \"\"\"Implements support for handling RaggedTensors.\n+\n+  Expected shape: `(batch, sequence_len)` with sequence_len being variable per\n+  batch.\n+  Return shape: `(batch,)`; returns the per batch mean of the loss values.\n+\n+  When used by BinaryFocalCrossentropy() with the default reduction\n+  (SUM_OVER_BATCH_SIZE), the reduction averages the per batch losses over\n+  the number of batches.\n+\n+  Args:\n+    y_true: Tensor of one-hot true targets.\n+    y_pred: Tensor of predicted targets.\n+    gamma: A focusing parameter, default is `2.0` as mentioned in the reference\n+      [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n+      we assume that `y_pred` encodes a probability distribution.\n+    label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels. For\n+      example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+      and `0.9 + 0.1 / num_classes` for target labels.\n+    axis: Axis along which to compute crossentropy.\n+\n+  Returns:\n+    Binary focal crossentropy loss value.\n+  \"\"\"\n+  fn = functools.partial(\n+      binary_focal_crossentropy,\n+      gamma=gamma,\n+      from_logits=from_logits,\n+      label_smoothing=label_smoothing,\n+      axis=axis,\n+  )\n+  return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n+\n+\n @keras_export('keras.metrics.kl_divergence',\n               'keras.metrics.kullback_leibler_divergence', 'keras.metrics.kld',\n               'keras.metrics.KLD', 'keras.losses.kl_divergence',\n\n@@ -998,6 +998,189 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 75., 3)\n \n \n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class BinaryFocalCrossentropyTest(tf.test.TestCase):\n+\n+  def test_config(self):\n+    obj = losses.BinaryFocalCrossentropy(gamma=1.5, name='bfce_0')\n+    self.assertEqual(obj.name, 'bfce_0')\n+    self.assertAlmostEqual(obj.gamma, 1.5)\n+\n+    obj_2 = losses.BinaryFocalCrossentropy.from_config(obj.get_config())\n+    self.assertEqual(obj_2.name, 'bfce_0')\n+    self.assertAlmostEqual(obj_2.gamma, 1.5)\n+\n+  def test_all_correct_unweighted(self):\n+    y_true = tf.constant([\n+        [1, 0, 0],\n+        [0, 1, 0],\n+        [0, 0, 1],\n+    ], dtype=tf.float32)\n+    obj = losses.BinaryFocalCrossentropy(gamma=1.5)\n+    loss = obj(y_true, y_true)\n+    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+    # Test with logits.\n+    logits = tf.constant([\n+        [100.0, -100.0, -100.0],\n+        [-100.0, 100.0, -100.0],\n+        [-100.0, -100.0, 100.0],\n+    ])\n+    obj = losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True)\n+    loss = obj(y_true, logits)\n+    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+  def test_unweighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+    loss = obj(y_true, y_pred)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n+    # focalLoss = focal bceLoss = [[0.001, 1.03], [0.032, 0.009]]\n+    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) / 4 = 0.268\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.268, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+    loss = obj(y_true, logits)\n+\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # bceLoss = -log(p_t)\n+    #         = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]]\n+\n+    # focalLoss = focal bceLoss\n+    #           = [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]]\n+    # Reduced loss = 0.799\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.799, 3)\n+\n+  def test_scalar_weighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+    loss = obj(y_true, y_pred, sample_weight=1.23)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n+    # focalLoss = focal bceLoss\n+    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight\n+    # Reduced loss = (0.001 + 1.03 + 0.032 + 0.009) * 1.23 / 4 = 0.3296\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.3296, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+    loss = obj(y_true, logits, sample_weight=3.21)\n+\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+\n+    # focalLoss = focal * bceLoss =\n+    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight\n+    # Reduced loss = 0.799 * 3.21 = 2.565\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 2.565, 3)\n+\n+  def test_sample_weighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+    obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+    loss = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    #         = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n+    # focalLoss = focal * bceLoss\n+    #           = [[0.001, 1.03], [0.032, 0.009]] * sample_weight\n+    #           = [[0.0012, 1.236], [0.1088, 0.0306]]\n+    # Reduced loss = (0.0012 + 1.236 + 0.1088 + 0.0306) / 4 = 0.34415\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.34415, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+    loss = obj(y_true, logits, sample_weight=sample_weight)\n+\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+\n+    # focalLoss = focal * bceLoss =\n+    # [[0.0012, 2.2743, 2.514], [0.0000002, 0.0033, 0.00000001]] * sample_weight\n+    # focalLoss = [[0.00144, 2.72916, 3.0168], [6.8e-7, 0.01122, 3.4e-8]]\n+    # Reduced loss = 0.799\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.95977, 3)\n+\n+  def test_no_reduction(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(\n+        gamma=2.0,\n+        reduction=losses_utils.ReductionV2.NONE,\n+    )\n+    loss = obj(y_true, y_pred)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n+    # focalLoss = focal bceLoss = [[0.001, 1.03], [0.032, 0.009]]\n+    # Reduced loss = [(0.001 + 1.03) / 2, (0.032 + 0.009) / 2]\n+\n+    self.assertAllClose(self.evaluate(loss), (0.5155, 0.0205), 3)\n+\n+  def test_ragged_tensors(self):\n+    y_true = tf.ragged.constant([[1, 0, 1], [0]])\n+    y_pred = tf.ragged.constant([[0.9, 0.8, 0.7], [0.2]])\n+    obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+    loss = obj(y_true, y_pred)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7], [0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64, 0.09], [0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609, 0.357], [0.223]]\n+    # focalLoss = focal bceLoss = [[0.001, 1.03, 0.032], [0.009]]\n+    # Reduced loss = ((0.001 + 1.03 + 0.032) / 3 + 0.009) / 2 = 0.18166\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.18166, 3)\n+\n+\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class CategoricalCrossentropyTest(tf.test.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc3c73077fc48a78208c0175e06180128f130962", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 28 | Churn Cumulative: 2302 | Contributors (this commit): 6 | Commits (past 90d): 24 | Contributors (cumulative): 10 | DMM Complexity: 0.25\n\nDIFF:\n@@ -251,10 +251,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       raise ValueError(\"Both `num_bins` and `bin_boundaries` should not be \"\n                        \"set. You passed `num_bins={}` and \"\n                        \"`bin_boundaries={}`\".format(num_bins, bin_boundaries))\n-    if bin_boundaries is not None:\n-      # Force everything through ensure_tensor to ensure a uniform type.\n-      bin_boundaries = utils.listify_tensors(\n-          utils.ensure_tensor(bin_boundaries))\n+    bin_boundaries = utils.listify_tensors(bin_boundaries)\n     self.input_bin_boundaries = bin_boundaries\n     self.bin_boundaries = bin_boundaries if bin_boundaries is not None else []\n     self.num_bins = num_bins\n@@ -287,7 +284,9 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     if not self.built:\n       raise RuntimeError(\"`build` must be called before `update_state`.\")\n \n-    data = utils.ensure_tensor(data, tf.float32)\n+    data = tf.convert_to_tensor(data)\n+    if data.dtype != tf.float32:\n+      data = tf.cast(data, tf.float32)\n     summary = summarize(data, self.epsilon)\n     self.summary.assign(merge_summaries(summary, self.summary, self.epsilon))\n \n@@ -327,7 +326,6 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=self.compute_dtype)\n \n   def call(self, inputs):\n-    inputs = utils.ensure_tensor(inputs)\n     def bucketize(inputs):\n       return tf.raw_ops.Bucketize(input=inputs, boundaries=self.bin_boundaries)\n \n\n@@ -30,21 +30,6 @@ import tensorflow.compat.v2 as tf\n class DiscretizationTest(keras_parameterized.TestCase,\n                          preprocessing_test_utils.PreprocessingLayerTest):\n \n-  @parameterized.named_parameters(\n-      (\"list\", list),\n-      (\"tuple\", tuple),\n-      (\"numpy\", np.array),\n-      (\"array_like\", preprocessing_test_utils.ArrayLike),\n-  )\n-  def test_tensor_like_inputs(self, data_fn):\n-    input_data = data_fn([0.5, 1.5, 2.5])\n-    bin_boundaries = data_fn([0.5, 1.5, 2.5])\n-    expected_output = [1, 2, 3]\n-\n-    layer = discretization.Discretization(bin_boundaries=bin_boundaries)\n-    output_data = layer(input_data)\n-    self.assertAllEqual(output_data, expected_output)\n-\n   def test_bucketize_with_explicit_buckets_integer(self):\n     input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n \n\n@@ -14,7 +14,6 @@\n # ==============================================================================\n \"\"\"Utils for preprocessing layers.\"\"\"\n \n-from keras import backend\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -38,7 +37,7 @@ def ensure_tensor(inputs, dtype=None):\n def listify_tensors(x):\n   \"\"\"Convert any tensors or numpy arrays to lists for config serialization.\"\"\"\n   if tf.is_tensor(x):\n-    x = backend.get_value(x)\n+    x = x.numpy()\n   if isinstance(x, np.ndarray):\n     x = x.tolist()\n   return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1876404713e6d157d1dd05d0d74e510b53442f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 75 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -60,4 +60,4 @@ def get_load_options():\n \n def in_load_context():\n   \"\"\"Returns whether under a load context.\"\"\"\n-  return _load_context.in_load_context\n+  return _load_context.in_load_context()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8dec7a9efe766f21b4326ffa6c25eafdd918c696", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 534 | Files Changed: 5 | Hunks: 6 | Methods Changed: 23 | Complexity Δ (Sum/Max): -53/0 | Churn Δ: 536 | Churn Cumulative: 2348 | Contributors (this commit): 18 | Commits (past 90d): 11 | Contributors (cumulative): 34 | DMM Complexity: 0.11003236245954692\n\nDIFF:\n@@ -40,7 +40,6 @@ from keras.layers.preprocessing.image_preprocessing import Resizing\n from keras.layers.preprocessing.image_preprocessing import Rescaling\n \n # Preprocessing layers.\n-from keras.layers.preprocessing.category_crossing import CategoryCrossing\n from keras.layers.preprocessing.category_encoding import CategoryEncoding\n from keras.layers.preprocessing.discretization import Discretization\n from keras.layers.preprocessing.hashing import Hashing\n\n@@ -1,205 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras category crossing preprocessing layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n-\n-import itertools\n-import numpy as np\n-from keras.engine import base_layer\n-from keras.engine import base_preprocessing_layer\n-from keras.utils import tf_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.layers.experimental.preprocessing.CategoryCrossing')\n-class CategoryCrossing(base_layer.Layer):\n-  \"\"\"A preprocessing layer which crosses categorical features.\n-\n-  This layer concatenates multiple categorical inputs into a single categorical\n-  output (similar to Cartesian product). The output dtype is string.\n-\n-  For an overview and full list of preprocessing layers, see the preprocessing\n-  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n-\n-  Usage:\n-  >>> inp_1 = ['a', 'b', 'c']\n-  >>> inp_2 = ['d', 'e', 'f']\n-  >>> layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing()\n-  >>> layer([inp_1, inp_2])\n-  <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n-    array([[b'a_X_d'],\n-           [b'b_X_e'],\n-           [b'c_X_f']], dtype=object)>\n-\n-\n-  >>> inp_1 = ['a', 'b', 'c']\n-  >>> inp_2 = ['d', 'e', 'f']\n-  >>> layer = tf.keras.layers.experimental.preprocessing.CategoryCrossing(\n-  ...    separator='-')\n-  >>> layer([inp_1, inp_2])\n-  <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n-    array([[b'a-d'],\n-           [b'b-e'],\n-           [b'c-f']], dtype=object)>\n-\n-  Args:\n-    depth: depth of input crossing. By default None, all inputs are crossed into\n-      one output. It can also be an int or tuple/list of ints. Passing an\n-      integer will create combinations of crossed outputs with depth up to that\n-      integer, i.e., [1, 2, ..., `depth`), and passing a tuple of integers will\n-      create crossed outputs with depth for the specified values in the tuple,\n-      i.e., `depth`=(N1, N2) will create all possible crossed outputs with depth\n-      equal to N1 or N2. Passing `None` means a single crossed output with all\n-      inputs. For example, with inputs `a`, `b` and `c`, `depth=2` means the\n-      output will be [a;b;c;cross(a, b);cross(bc);cross(ca)].\n-    separator: A string added between each input being joined. Defaults to\n-      '_X_'.\n-    name: Name to give to the layer.\n-    **kwargs: Keyword arguments to construct a layer.\n-\n-  Input shape: a list of string or int tensors or sparse tensors of shape\n-    `[batch_size, d1, ..., dm]`\n-\n-  Output shape: a single string or int tensor or sparse tensor of shape\n-    `[batch_size, d1, ..., dm]`\n-\n-  Returns:\n-    If any input is `RaggedTensor`, the output is `RaggedTensor`.\n-    Else, if any input is `SparseTensor`, the output is `SparseTensor`.\n-    Otherwise, the output is `Tensor`.\n-\n-  Example: (`depth`=None)\n-    If the layer receives three inputs:\n-    `a=[[1], [4]]`, `b=[[2], [5]]`, `c=[[3], [6]]`\n-    the output will be a string tensor:\n-    `[[b'1_X_2_X_3'], [b'4_X_5_X_6']]`\n-\n-  Example: (`depth` is an integer)\n-    With the same input above, and if `depth`=2,\n-    the output will be a list of 6 string tensors:\n-    `[[b'1'], [b'4']]`\n-    `[[b'2'], [b'5']]`\n-    `[[b'3'], [b'6']]`\n-    `[[b'1_X_2'], [b'4_X_5']]`,\n-    `[[b'2_X_3'], [b'5_X_6']]`,\n-    `[[b'3_X_1'], [b'6_X_4']]`\n-\n-  Example: (`depth` is a tuple/list of integers)\n-    With the same input above, and if `depth`=(2, 3)\n-    the output will be a list of 4 string tensors:\n-    `[[b'1_X_2'], [b'4_X_5']]`,\n-    `[[b'2_X_3'], [b'5_X_6']]`,\n-    `[[b'3_X_1'], [b'6_X_4']]`,\n-    `[[b'1_X_2_X_3'], [b'4_X_5_X_6']]`\n-  \"\"\"\n-\n-  def __init__(self, depth=None, name=None, separator='_X_', **kwargs):\n-    super(CategoryCrossing, self).__init__(name=name, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        'CategoryCrossing').set(True)\n-    self.depth = depth\n-    self.separator = separator\n-    if isinstance(depth, (tuple, list)):\n-      self._depth_tuple = depth\n-    elif depth is not None:\n-      self._depth_tuple = tuple([i for i in range(1, depth + 1)])\n-\n-  def partial_crossing(self, partial_inputs, ragged_out, sparse_out):\n-    \"\"\"Gets the crossed output from a partial list/tuple of inputs.\"\"\"\n-    # If ragged_out=True, convert output from sparse to ragged.\n-    if ragged_out:\n-      # TODO(momernick): Support separator with ragged_cross.\n-      if self.separator != '_X_':\n-        raise ValueError(\n-            f'Non-default separator with ragged input is not implemented. '\n-            f'Received separator: {self.separator}.')\n-      return tf.ragged.cross(partial_inputs)\n-    elif sparse_out:\n-      return tf.sparse.cross(partial_inputs, separator=self.separator)\n-    else:\n-      return tf.sparse.to_dense(\n-          tf.sparse.cross(partial_inputs, separator=self.separator))\n-\n-  def _preprocess_input(self, inp):\n-    if isinstance(inp, (list, tuple, np.ndarray)):\n-      inp = tf.convert_to_tensor(inp)\n-    if inp.shape.rank == 1:\n-      inp = tf.expand_dims(inp, axis=-1)\n-    return inp\n-\n-  def call(self, inputs):\n-    inputs = [self._preprocess_input(inp) for inp in inputs]\n-    depth_tuple = self._depth_tuple if self.depth else (len(inputs),)\n-    ragged_out = sparse_out = False\n-    if any(tf_utils.is_ragged(inp) for inp in inputs):\n-      ragged_out = True\n-    elif any(isinstance(inp, tf.SparseTensor) for inp in inputs):\n-      sparse_out = True\n-\n-    outputs = []\n-    for depth in depth_tuple:\n-      if len(inputs) < depth:\n-        raise ValueError(\n-            f'Number of inputs cannot be less than depth. Received '\n-            f'{len(inputs)} input tensors, and depth {depth}.')\n-      for partial_inps in itertools.combinations(inputs, depth):\n-        partial_out = self.partial_crossing(\n-            partial_inps, ragged_out, sparse_out)\n-        outputs.append(partial_out)\n-    if sparse_out:\n-      return tf.sparse.concat(axis=1, sp_inputs=outputs)\n-    return tf.concat(outputs, axis=1)\n-\n-  def compute_output_shape(self, input_shape):\n-    if not isinstance(input_shape, (tuple, list)):\n-      raise ValueError(\n-          f'A `CategoryCrossing` layer should be called on a list of inputs. '\n-          f'Received: input_shape={input_shape}.')\n-    input_shapes = input_shape\n-    batch_size = None\n-    for inp_shape in input_shapes:\n-      inp_tensor_shape = tf.TensorShape(inp_shape).as_list()\n-      if len(inp_tensor_shape) != 2:\n-        raise ValueError(\n-            f'Inputs must be rank 2. Received: input_shape={input_shape}.')\n-      if batch_size is None:\n-        batch_size = inp_tensor_shape[0]\n-    # The second dimension is dynamic based on inputs.\n-    output_shape = [batch_size, None]\n-    return tf.TensorShape(output_shape)\n-\n-  def compute_output_signature(self, input_spec):\n-    input_shapes = [x.shape for x in input_spec]\n-    output_shape = self.compute_output_shape(input_shapes)\n-    if any(\n-        isinstance(inp_spec, tf.RaggedTensorSpec)\n-        for inp_spec in input_spec):\n-      return tf.TensorSpec(shape=output_shape, dtype=tf.string)\n-    elif any(\n-        isinstance(inp_spec, tf.SparseTensorSpec)\n-        for inp_spec in input_spec):\n-      return tf.SparseTensorSpec(\n-          shape=output_shape, dtype=tf.string)\n-    return tf.TensorSpec(shape=output_shape, dtype=tf.string)\n-\n-  def get_config(self):\n-    config = {\n-        'depth': self.depth,\n-        'separator': self.separator,\n-    }\n-    base_config = super(CategoryCrossing, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,75 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Distribution tests for keras.layers.preprocessing.category_crossing.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-\n-import keras\n-from keras import backend\n-from keras import keras_parameterized\n-from keras.distribute.strategy_combinations import all_strategies\n-from keras.layers.preprocessing import category_crossing\n-from keras.layers.preprocessing import preprocessing_test_utils\n-\n-\n-def batch_wrapper(dataset, batch_size, distribution, repeat=None):\n-  if repeat:\n-    dataset = dataset.repeat(repeat)\n-  # TPUs currently require fully defined input shapes, drop_remainder ensures\n-  # the input will have fully defined shapes.\n-  if backend.is_tpu_strategy(distribution):\n-    return dataset.batch(batch_size, drop_remainder=True)\n-  else:\n-    return dataset.batch(batch_size)\n-\n-\n-@tf.__internal__.distribute.combinations.generate(\n-    tf.__internal__.test.combinations.combine(\n-        # Investigate why crossing is not supported with TPU.\n-        distribution=all_strategies,\n-        mode=['eager', 'graph']))\n-class CategoryCrossingDistributionTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_distribution(self, distribution):\n-    input_array_1 = np.array([['a', 'b'], ['c', 'd']])\n-    input_array_2 = np.array([['e', 'f'], ['g', 'h']])\n-    inp_dataset = tf.data.Dataset.from_tensor_slices(\n-        {'input_1': input_array_1, 'input_2': input_array_2})\n-    inp_dataset = batch_wrapper(inp_dataset, 2, distribution)\n-\n-    # pyformat: disable\n-    expected_output = [[b'a_X_e', b'a_X_f', b'b_X_e', b'b_X_f'],\n-                       [b'c_X_g', b'c_X_h', b'd_X_g', b'd_X_h']]\n-    tf.config.set_soft_device_placement(True)\n-\n-    with distribution.scope():\n-      input_data_1 = keras.Input(shape=(2,), dtype=tf.string,\n-                                 name='input_1')\n-      input_data_2 = keras.Input(shape=(2,), dtype=tf.string,\n-                                 name='input_2')\n-      input_data = [input_data_1, input_data_2]\n-      layer = category_crossing.CategoryCrossing()\n-      int_data = layer(input_data)\n-      model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(inp_dataset)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -1,250 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for categorical preprocessing layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-from keras import keras_parameterized\n-from keras import testing_utils\n-from keras.engine import input_layer\n-from keras.engine import training\n-from keras.layers.preprocessing import category_crossing\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CategoryCrossingTest(keras_parameterized.TestCase):\n-\n-  def test_crossing_sparse_inputs(self):\n-    layer = category_crossing.CategoryCrossing()\n-    inputs_0 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [1, 1]],\n-        values=['a', 'b', 'c'],\n-        dense_shape=[2, 2])\n-    inputs_1 = tf.SparseTensor(\n-        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)\n-    self.assertAllEqual([b'a_X_d', b'b_X_e', b'c_X_e'], output.values)\n-\n-  def test_crossing_sparse_inputs_custom_sep(self):\n-    layer = category_crossing.CategoryCrossing(separator='_Y_')\n-    inputs_0 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [1, 1]],\n-        values=['a', 'b', 'c'],\n-        dense_shape=[2, 2])\n-    inputs_1 = tf.SparseTensor(\n-        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)\n-    self.assertAllEqual([b'a_Y_d', b'b_Y_e', b'c_Y_e'], output.values)\n-\n-  def test_crossing_sparse_inputs_empty_sep(self):\n-    layer = category_crossing.CategoryCrossing(separator='')\n-    inputs_0 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [1, 1]],\n-        values=['a', 'b', 'c'],\n-        dense_shape=[2, 2])\n-    inputs_1 = tf.SparseTensor(\n-        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)\n-    self.assertAllEqual([b'ad', b'be', b'ce'], output.values)\n-\n-  def test_crossing_sparse_inputs_depth_int(self):\n-    layer = category_crossing.CategoryCrossing(depth=1)\n-    inputs_0 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [2, 0]],\n-        values=['a', 'b', 'c'],\n-        dense_shape=[3, 1])\n-    inputs_1 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [2, 0]],\n-        values=['d', 'e', 'f'],\n-        dense_shape=[3, 1])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertIsInstance(output, tf.SparseTensor)\n-    output = tf.sparse.to_dense(output)\n-    expected_out = [[b'a', b'd'], [b'b', b'e'], [b'c', b'f']]\n-    self.assertAllEqual(expected_out, output)\n-\n-  def test_crossing_sparse_inputs_depth_tuple(self):\n-    layer = category_crossing.CategoryCrossing(depth=(2, 3))\n-    inputs_0 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [2, 0]],\n-        values=['a', 'b', 'c'],\n-        dense_shape=[3, 1])\n-    inputs_1 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [2, 0]],\n-        values=['d', 'e', 'f'],\n-        dense_shape=[3, 1])\n-    inputs_2 = tf.SparseTensor(\n-        indices=[[0, 0], [1, 0], [2, 0]],\n-        values=['g', 'h', 'i'],\n-        dense_shape=[3, 1])\n-    inp_0_t = input_layer.Input(shape=(1,), sparse=True, dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(1,), sparse=True, dtype=tf.string)\n-    inp_2_t = input_layer.Input(shape=(1,), sparse=True, dtype=tf.string)\n-    out_t = layer([inp_0_t, inp_1_t, inp_2_t])\n-    model = training.Model([inp_0_t, inp_1_t, inp_2_t], out_t)\n-    output = model.predict([inputs_0, inputs_1, inputs_2])\n-    self.assertIsInstance(output, tf.SparseTensor)\n-    output = tf.sparse.to_dense(output)\n-    expected_outputs_0 = [[b'a_X_d', b'a_X_g', b'd_X_g', b'a_X_d_X_g']]\n-    expected_outputs_1 = [[b'b_X_e', b'b_X_h', b'e_X_h', b'b_X_e_X_h']]\n-    expected_outputs_2 = [[b'c_X_f', b'c_X_i', b'f_X_i', b'c_X_f_X_i']]\n-    expected_out = tf.concat(\n-        [expected_outputs_0, expected_outputs_1, expected_outputs_2], axis=0)\n-    self.assertAllEqual(expected_out, output)\n-\n-  def test_crossing_ragged_inputs(self):\n-    inputs_0 = tf.ragged.constant(\n-        [['omar', 'skywalker'], ['marlo']],\n-        dtype=tf.string)\n-    inputs_1 = tf.ragged.constant(\n-        [['a'], ['b']],\n-        dtype=tf.string)\n-    inp_0_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-\n-    non_hashed_layer = category_crossing.CategoryCrossing()\n-    out_t = non_hashed_layer([inp_0_t, inp_1_t])\n-    model = training.Model(inputs=[inp_0_t, inp_1_t], outputs=out_t)\n-    expected_output = [[b'omar_X_a', b'skywalker_X_a'], [b'marlo_X_b']]\n-    self.assertAllEqual(expected_output, model.predict([inputs_0, inputs_1]))\n-\n-  def test_crossing_ragged_inputs_depth_int(self):\n-    layer = category_crossing.CategoryCrossing(depth=1)\n-    inputs_0 = tf.ragged.constant([['a'], ['b'], ['c']])\n-    inputs_1 = tf.ragged.constant([['d'], ['e'], ['f']])\n-    output = layer([inputs_0, inputs_1])\n-    expected_output = [[b'a', b'd'], [b'b', b'e'], [b'c', b'f']]\n-    self.assertIsInstance(output, tf.RaggedTensor)\n-    self.assertAllEqual(expected_output, output)\n-\n-    layer = category_crossing.CategoryCrossing(depth=2)\n-    inp_0_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    out_t = layer([inp_0_t, inp_1_t])\n-    model = training.Model([inp_0_t, inp_1_t], out_t)\n-    expected_output = [[b'a', b'd', b'a_X_d'], [b'b', b'e', b'b_X_e'],\n-                       [b'c', b'f', b'c_X_f']]\n-    self.assertAllEqual(expected_output, model.predict([inputs_0, inputs_1]))\n-\n-  def test_crossing_ragged_inputs_depth_tuple(self):\n-    layer = category_crossing.CategoryCrossing(depth=[2, 3])\n-    inputs_0 = tf.ragged.constant([['a'], ['b'], ['c']])\n-    inputs_1 = tf.ragged.constant([['d'], ['e'], ['f']])\n-    inputs_2 = tf.ragged.constant([['g'], ['h'], ['i']])\n-    inp_0_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    inp_2_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    out_t = layer([inp_0_t, inp_1_t, inp_2_t])\n-    model = training.Model([inp_0_t, inp_1_t, inp_2_t], out_t)\n-    expected_output = [[b'a_X_d', b'a_X_g', b'd_X_g', b'a_X_d_X_g'],\n-                       [b'b_X_e', b'b_X_h', b'e_X_h', b'b_X_e_X_h'],\n-                       [b'c_X_f', b'c_X_i', b'f_X_i', b'c_X_f_X_i']]\n-    output = model.predict([inputs_0, inputs_1, inputs_2])\n-    self.assertIsInstance(output, tf.RaggedTensor)\n-    self.assertAllEqual(expected_output, output)\n-\n-  def test_crossing_with_dense_inputs(self):\n-    layer = category_crossing.CategoryCrossing()\n-    inputs_0 = np.asarray([[1, 2]])\n-    inputs_1 = np.asarray([[1, 3]])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllEqual([[b'1_X_1', b'1_X_3', b'2_X_1', b'2_X_3']], output)\n-\n-  def test_crossing_with_list_inputs(self):\n-    layer = category_crossing.CategoryCrossing()\n-    inputs_0 = [[1, 2]]\n-    inputs_1 = [[1, 3]]\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllEqual([[b'1_X_1', b'1_X_3', b'2_X_1', b'2_X_3']], output)\n-\n-    inputs_0 = [1, 2]\n-    inputs_1 = [1, 3]\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllEqual([[b'1_X_1'], [b'2_X_3']], output)\n-\n-    inputs_0 = np.asarray([1, 2])\n-    inputs_1 = np.asarray([1, 3])\n-    output = layer([inputs_0, inputs_1])\n-    self.assertAllEqual([[b'1_X_1'], [b'2_X_3']], output)\n-\n-  def test_crossing_dense_inputs_depth_int(self):\n-    layer = category_crossing.CategoryCrossing(depth=1)\n-    inputs_0 = tf.constant([['a'], ['b'], ['c']])\n-    inputs_1 = tf.constant([['d'], ['e'], ['f']])\n-    output = layer([inputs_0, inputs_1])\n-    expected_output = [[b'a', b'd'], [b'b', b'e'], [b'c', b'f']]\n-    self.assertAllEqual(expected_output, output)\n-\n-    layer = category_crossing.CategoryCrossing(depth=2)\n-    inp_0_t = input_layer.Input(shape=(1,), dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(1,), dtype=tf.string)\n-    out_t = layer([inp_0_t, inp_1_t])\n-    model = training.Model([inp_0_t, inp_1_t], out_t)\n-    crossed_output = [[b'a_X_d'], [b'b_X_e'], [b'c_X_f']]\n-    expected_output = tf.concat([expected_output, crossed_output],\n-                                       axis=1)\n-    self.assertAllEqual(expected_output, model.predict([inputs_0, inputs_1]))\n-\n-  def test_crossing_dense_inputs_depth_tuple(self):\n-    layer = category_crossing.CategoryCrossing(depth=[2, 3])\n-    inputs_0 = tf.constant([['a'], ['b'], ['c']])\n-    inputs_1 = tf.constant([['d'], ['e'], ['f']])\n-    inputs_2 = tf.constant([['g'], ['h'], ['i']])\n-    inp_0_t = input_layer.Input(shape=(1,), dtype=tf.string)\n-    inp_1_t = input_layer.Input(shape=(1,), dtype=tf.string)\n-    inp_2_t = input_layer.Input(shape=(1,), dtype=tf.string)\n-    out_t = layer([inp_0_t, inp_1_t, inp_2_t])\n-    model = training.Model([inp_0_t, inp_1_t, inp_2_t], out_t)\n-    expected_outputs_0 = [[b'a_X_d', b'a_X_g', b'd_X_g', b'a_X_d_X_g']]\n-    expected_outputs_1 = [[b'b_X_e', b'b_X_h', b'e_X_h', b'b_X_e_X_h']]\n-    expected_outputs_2 = [[b'c_X_f', b'c_X_i', b'f_X_i', b'c_X_f_X_i']]\n-    expected_output = tf.concat(\n-        [expected_outputs_0, expected_outputs_1, expected_outputs_2], axis=0)\n-    self.assertAllEqual(expected_output,\n-                        model.predict([inputs_0, inputs_1, inputs_2]))\n-\n-  def test_crossing_compute_output_signature(self):\n-    input_shapes = [\n-        tf.TensorShape([2, 2]),\n-        tf.TensorShape([2, 3])\n-    ]\n-    input_specs = [\n-        tf.TensorSpec(input_shape, tf.string)\n-        for input_shape in input_shapes\n-    ]\n-    layer = category_crossing.CategoryCrossing()\n-    output_spec = layer.compute_output_signature(input_specs)\n-    self.assertEqual(output_spec.shape.dims[0], input_shapes[0].dims[0])\n-    self.assertEqual(output_spec.dtype, tf.string)\n-\n-  @testing_utils.run_v2_only\n-  def test_config_with_custom_name(self):\n-    layer = category_crossing.CategoryCrossing(depth=2, name='hashing')\n-    config = layer.get_config()\n-    layer_1 = category_crossing.CategoryCrossing.from_config(config)\n-    self.assertEqual(layer_1.name, layer.name)\n-\n-    layer = category_crossing.CategoryCrossing(name='hashing')\n-    config = layer.get_config()\n-    layer_1 = category_crossing.CategoryCrossing.from_config(config)\n-    self.assertEqual(layer_1.name, layer.name)\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -42,7 +42,6 @@ from keras.layers import wrappers\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import batch_normalization_v1\n from keras.layers.normalization import layer_normalization\n-from keras.layers.preprocessing import category_crossing\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import hashing\n@@ -61,8 +60,8 @@ ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                embeddings, einsum_dense, local, merge, noise,\n                batch_normalization_v1, layer_normalization, pooling,\n                image_preprocessing, recurrent, wrappers, hashing,\n-               hashed_crossing, category_crossing, category_encoding,\n-               discretization, multi_head_attention, integer_lookup,\n+               hashed_crossing, category_encoding, discretization,\n+               multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n                   recurrent_v2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
