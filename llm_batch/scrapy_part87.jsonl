{"custom_id": "scrapy#87e322e568036cbe50f7f57680569cae822b21ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 119 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from lxml import etree\n import lxml.html\n \n from scrapy.link import Link\n-from scrapy.utils.python import unique as unique_list\n+from scrapy.utils.python import unique as unique_list, str_to_unicode\n from scrapy.utils.url import safe_url_string, urljoin_rfc\n \n class LxmlLinkExtractor(object):\n@@ -33,7 +33,7 @@ class LxmlLinkExtractor(object):\n         for link in links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n             link.url = safe_url_string(link.url, response_encoding)\n-            link.text = link.text.decode(response_encoding)\n+            link.text = str_to_unicode(link.text, response_encoding)\n             ret.append(link)\n \n         return ret\n@@ -43,6 +43,10 @@ class LxmlLinkExtractor(object):\n         return self._extract_links(response.body, response.url, \n                                    response.encoding)\n \n+    def matches(self, url):\n+        \"\"\"This extractor matches with any url, since it doesn't contain any patterns\"\"\"\n+        return True\n+\n \n class LinkTarget(object):\n     def __init__(self, scan_tag, scan_attr, process_attr):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55d584e478385e8bbb8ec12bc85054c7e2d6b31b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 387 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,7 @@ class Downloader(object):\n             # downloader middleware, to speed-up the closing process\n             if site.closing:\n                 log.msg(\"Crawled while closing spider: %s\" % request, \\\n-                    level=log.DEBUG)\n+                    level=log.DEBUG, spider=spider)\n                 raise IgnoreRequest\n             return _\n         return dfd.addBoth(finish_transferring)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69058e3b501b9ec12e5e4a1ba0dac2aec73d4fdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 99 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):\n         return\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n         domain=domain, spider=spider)\n-    system = domain or spider.domain_name if spider else component\n+    system = domain or (spider.domain_name if spider else component)\n     msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n     log.msg(msg_txt, system=system)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7728a23e9936e387ee5dca01d937f6e512e5d7ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 39 | Files Changed: 5 | Hunks: 23 | Methods Changed: 25 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 77 | Churn Cumulative: 363 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,8 +36,8 @@ from scrapy.http import Request\n from scrapy import log\n from scrapy.conf import settings\n \n-items_per_domain = settings.getint('ITEMSAMPLER_COUNT', 1)\n-close_domain = settings.getbool('ITEMSAMPLER_CLOSE_DOMAIN', False)\n+items_per_spider = settings.getint('ITEMSAMPLER_COUNT', 1)\n+close_spider = settings.getbool('ITEMSAMPLER_CLOSE_SPIDER', False)\n max_response_size = settings.getbool('ITEMSAMPLER_MAX_RESPONSE_SIZE', )\n \n class ItemSamplerPipeline(object):\n@@ -47,20 +47,19 @@ class ItemSamplerPipeline(object):\n         if not self.filename:\n             raise NotConfigured\n         self.items = {}\n-        self.domains_count = 0\n+        self.spiders_count = 0\n         self.empty_domains = set()\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n-    def process_item(self, item, spider):\n-        domain = spider.domain_name\n-        sampled = stats.get_value(\"items_sampled\", 0, domain=domain)\n-        if sampled < items_per_domain:\n+    def process_item(self, spider, item):\n+        sampled = stats.get_value(\"items_sampled\", 0, domain=spider.domain_name)\n+        if sampled < items_per_spider:\n             self.items[item.guid] = item\n             sampled += 1\n-            stats.set_value(\"items_sampled\", sampled, domain=domain)\n-            log.msg(\"Sampled %s\" % item, domain=domain, level=log.INFO)\n-            if close_domain and sampled == items_per_domain:\n+            stats.set_value(\"items_sampled\", sampled, domain=spider.domain_name)\n+            log.msg(\"Sampled %s\" % item, spider=spider, level=log.INFO)\n+            if close_spider and sampled == items_per_spider:\n                 scrapyengine.close_spider(spider)\n         return item\n \n@@ -68,14 +67,15 @@ class ItemSamplerPipeline(object):\n         with open(self.filename, 'w') as f:\n             pickle.dump(self.items, f)\n         if self.empty_domains:\n-            log.msg(\"No products sampled for: %s\" % \" \".join(self.empty_domains), level=log.WARNING)\n+            log.msg(\"No products sampled for: %s\" % \" \".join(self.empty_domains), \\\n+                level=log.WARNING)\n \n     def spider_closed(self, spider, reason):\n-        domain = spider.domain_name\n-        if reason == 'finished' and not stats.get_value(\"items_sampled\", domain=domain):\n-            self.empty_domains.add(domain)\n-        self.domains_count += 1\n-        log.msg(\"Sampled %d domains so far (%d empty)\" % (self.domains_count, len(self.empty_domains)), level=log.INFO)\n+        if reason == 'finished' and not stats.get_value(\"items_sampled\", domain=spider.domain_name):\n+            self.empty_domains.add(spider.domain_name)\n+        self.spiders_count += 1\n+        log.msg(\"Sampled %d domains so far (%d empty)\" % (self.spiders_count, \\\n+            len(self.empty_domains)), level=log.INFO)\n \n \n class ItemSamplerMiddleware(object):\n@@ -87,7 +87,7 @@ class ItemSamplerMiddleware(object):\n             raise NotConfigured\n \n     def process_spider_input(self, response, spider):\n-        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_domain:\n+        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_spider:\n             return []\n         elif max_response_size and max_response_size > len(response_httprepr(response)):  \n             return []\n@@ -100,7 +100,7 @@ class ItemSamplerMiddleware(object):\n             else:\n                 items.append(r)\n \n-        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_domain:\n+        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_spider:\n             return []\n         else:\n             # TODO: this needs some revision, as keeping only the first item\n\n@@ -57,7 +57,7 @@ class ItemPipelineManager(object):\n             if not stages_left:\n                 return item\n             current_stage = stages_left.pop(0)\n-            d = mustbe_deferred(current_stage.process_item, spider.domain_name, item)\n+            d = mustbe_deferred(current_stage.process_item, spider, item)\n             d.addCallback(next_stage, stages_left)\n             return d\n \n\n@@ -17,7 +17,7 @@ class FileExportPipeline(object):\n         self.exporter.start_exporting()\n         dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n \n-    def process_item(self, domain, item):\n+    def process_item(self, spider, item):\n         self.exporter.export_item(item)\n         return item\n \n\n@@ -12,27 +12,26 @@ from scrapy.utils.misc import arg_to_iter\n class MediaPipeline(object):\n     DOWNLOAD_PRIORITY = 1000\n \n-    class DomainInfo(object):\n+    class SpiderInfo(object):\n         def __init__(self, spider):\n-            self.domain = spider.domain_name\n             self.spider = spider\n             self.downloading = {}\n             self.downloaded = {}\n             self.waiting = {}\n \n     def __init__(self):\n-        self.domaininfo = {}\n+        self.spiderinfo = {}\n         dispatcher.connect(self.spider_opened, signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n     def spider_opened(self, spider):\n-        self.domaininfo[spider.domain_name] = self.DomainInfo(spider)\n+        self.spiderinfo[spider] = self.SpiderInfo(spider)\n \n     def spider_closed(self, spider):\n-        del self.domaininfo[spider.domain_name]\n+        del self.spiderinfo[spider]\n \n-    def process_item(self, domain, item):\n-        info = self.domaininfo[domain]\n+    def process_item(self, spider, item):\n+        info = self.spiderinfo[spider]\n         requests = arg_to_iter(self.get_media_requests(item, info))\n         dlist = []\n         for request in requests:\n@@ -83,7 +82,7 @@ class MediaPipeline(object):\n \n             info.downloading[fp] = (request, dwld) # fill downloading state data\n             dwld.addBoth(_downloaded) # append post-download hook\n-            dwld.addErrback(log.err, domain=info.domain)\n+            dwld.addErrback(log.err, spider=info.spider)\n \n         # declare request in downloading state (None is used as place holder)\n         info.downloading[fp] = None\n\n@@ -27,11 +27,11 @@ class ShoveItemPipeline(object):\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n-    def process_item(self, domain, item):\n+    def process_item(self, spider, item):\n         guid = str(item.guid)\n \n-        if guid in self.stores[domain]:\n-            if self.stores[domain][guid] == item:\n+        if guid in self.stores[spider]:\n+            if self.stores[spider][guid] == item:\n                 status = 'old'\n             else:\n                 status = 'upd'\n@@ -39,17 +39,17 @@ class ShoveItemPipeline(object):\n             status = 'new'\n \n         if not status == 'old':\n-            self.stores[domain][guid] = item\n-        self.log(domain, item, status)\n+            self.stores[spider][guid] = item\n+        self.log(spider, item, status)\n         return item\n \n     def spider_opened(self, spider):\n-        domain = spider.domain_name\n-        uri = Template(self.uritpl).substitute(domain=domain)\n-        self.stores[domain] = Shove(uri, **self.opts)\n+        uri = Template(self.uritpl).substitute(domain=spider.domain_name)\n+        self.stores[spider] = Shove(uri, **self.opts)\n \n     def spider_closed(self, spider):\n-        self.stores[spider.domain_name].sync()\n+        self.stores[spider].sync()\n \n-    def log(self, domain, item, status):\n-        log.msg(\"Shove (%s): Item guid=%s\" % (status, item.guid), level=log.DEBUG, domain=domain)\n+    def log(self, spider, item, status):\n+        log.msg(\"Shove (%s): Item guid=%s\" % (status, item.guid), level=log.DEBUG, \\\n+            spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5cae1e6c923041e22403a87c6002c67db103da2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 69 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,8 +15,8 @@ Settings that affect this module:\n \n ITEMSAMPLER_FILE\n   file where to store the pickled dict of scraped items\n-ITEMSAMPLER_CLOSE_DOMAIN\n-  wether to close the domain after enough products have been sampled\n+ITEMSAMPLER_CLOSE_SPIDER\n+  wether to close the spider after enough products have been sampled\n ITEMSAMPLER_MAX_RESPONSE_SIZE\n   maximum response size to process\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#580d82468e3baae6aa71c88bf5c3e91047758d70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 453 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -207,28 +207,28 @@ class ImagesPipeline(MediaPipeline):\n \n         if response.status != 200:\n             log.msg('Image (http-error): Error downloading image from %s referred in <%s>' \\\n-                    % (request, referer), level=log.WARNING, domain=info.domain)\n+                    % (request, referer), level=log.WARNING, spider=info.spider)\n             raise ImageException\n \n         if not response.body:\n             log.msg('Image (empty-content): Empty image from %s referred in <%s>: no-content' \\\n-                    % (request, referer), level=log.WARNING, domain=info.domain)\n+                    % (request, referer), level=log.WARNING, spider=info.spider)\n             raise ImageException\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n         msg = 'Image (%s): Downloaded image from %s referred in <%s>' % \\\n                 (status, request, referer)\n-        log.msg(msg, level=log.DEBUG, domain=info.domain)\n-        self.inc_stats(info.domain, status)\n+        log.msg(msg, level=log.DEBUG, spider=info.spider)\n+        self.inc_stats(info.spider.domain_name, status)\n \n         try:\n             key = self.image_key(request.url)\n             checksum = self.image_downloaded(response, request, info)\n         except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            log.msg(str(ex), level=log.WARNING, spider=info.spider)\n             raise\n         except Exception:\n-            log.err(domain=info.domain)\n+            log.err(spider=info.spider)\n             raise ImageException\n \n         return {'url': request.url, 'path': key, 'checksum': checksum}\n@@ -238,7 +238,7 @@ class ImagesPipeline(MediaPipeline):\n             referer = request.headers.get('Referer')\n             msg = 'Image (unknown-error): Error downloading %s from %s referred in <%s>: %s' \\\n                     % (self.MEDIA_NAME, request, referer, str(failure))\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            log.msg(msg, level=log.WARNING, spider=info.spider)\n         raise ImageException\n \n     def media_to_download(self, request, info):\n@@ -257,8 +257,8 @@ class ImagesPipeline(MediaPipeline):\n \n             referer = request.headers.get('Referer')\n             log.msg('Image (uptodate): Downloaded %s from <%s> referred in <%s>' % \\\n-                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, domain=info.domain)\n-            self.inc_stats(info.domain, 'uptodate')\n+                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, spider=info.spider)\n+            self.inc_stats(info.spider.domain_name, 'uptodate')\n \n             checksum = result.get('checksum', None)\n             return {'url': request.url, 'path': key, 'checksum': checksum}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d604dca96d308f1875f335ee9e7938d1fdc63260", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 633 | Contributors (this commit): 3 | Commits (past 90d): 32 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -136,7 +136,7 @@ REQUEST_HANDLERS_BASE = {\n }\n \n REQUESTS_QUEUE_SIZE = 0\n-REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n+REQUESTS_PER_SPIDER = 8     # max simultaneous requests per domain\n \n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n\n@@ -27,7 +27,7 @@ class SiteInfo(object):\n         if download_delay:\n             self.max_concurrent_requests = 1\n         elif max_concurrent_requests is None:\n-            self.max_concurrent_requests = settings.getint('REQUESTS_PER_DOMAIN')\n+            self.max_concurrent_requests = settings.getint('REQUESTS_PER_SPIDER')\n         else:\n             self.max_concurrent_requests =  max_concurrent_requests\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#919cd5b7898650424ef7f1e369bd6474d8ab7c02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 819 | Contributors (this commit): 4 | Commits (past 90d): 40 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -24,10 +24,10 @@ CLOSEDOMAIN_ITEMPASSED = 0\n COMMANDS_MODULE = ''\n COMMANDS_SETTINGS_MODULE = ''\n \n-CONCURRENT_DOMAINS = 8\n-\n CONCURRENT_ITEMS = 100\n \n+CONCURRENT_SPIDERS = 8\n+\n COOKIES_DEBUG = False\n \n DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n\n@@ -46,7 +46,7 @@ class Spiderctl(object):\n                 (len(self.idle),\n                  len(self.scheduled),\n                  len(self.running),\n-                 settings['CONCURRENT_DOMAINS'],\n+                 settings['CONCURRENT_SPIDERS'],\n                  len(self.finished))\n         s += \"<tr>\\n\"\n \n\n@@ -54,7 +54,7 @@ class Downloader(object):\n     def __init__(self):\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n-        self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n+        self.concurrent_domains = settings.getint('CONCURRENT_SPIDERS')\n \n     def fetch(self, request, spider):\n         \"\"\"Main method to use to request a download\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cff4592b64c0762d6f3e582d11f67d5735e407b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 63 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class LiveStats(object):\n         totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0\n         s = banner(self)\n         s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n+        s += \"<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n         for spider in sorted(self.domains.keys()):\n             scheduled = len(sch.pending_requests[spider]) if spider in sch.pending_requests else 0\n             active = len(dwl.sites[spider].active) if spider in dwl.sites else 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74d0e82dbe50f1730eaaa4de1cfe0c3c4dee94fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 18 | Churn Cumulative: 266 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -18,8 +18,8 @@ from os.path import join, abspath, dirname\n BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n-CLOSEDOMAIN_TIMEOUT = 0\n-CLOSEDOMAIN_ITEMPASSED = 0\n+CLOSESPIDER_TIMEOUT = 0\n+CLOSESPIDER_ITEMPASSED = 0\n \n COMMANDS_MODULE = ''\n COMMANDS_SETTINGS_MODULE = ''\n@@ -86,7 +86,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.webconsole.stats.StatsDump': 0,\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n-    'scrapy.contrib.closedomain.CloseDomain': 0,\n+    'scrapy.contrib.closespider.CloseSpider': 0,\n }\n \n GROUPSETTINGS_ENABLED = False\n\n@@ -1,4 +1,4 @@\n-\"\"\"CloseDomain is an extension that forces spiders to be closed after certain\n+\"\"\"CloseSpider is an extension that forces spiders to be closed after certain\n conditions are met.\n \n See documentation in docs/topics/extensions.rst\n@@ -13,11 +13,11 @@ from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n \n-class CloseDomain(object):\n+class CloseSpider(object):\n \n     def __init__(self):\n-        self.timeout = settings.getint('CLOSEDOMAIN_TIMEOUT')\n-        self.itempassed = settings.getint('CLOSEDOMAIN_ITEMPASSED')\n+        self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n+        self.itempassed = settings.getint('CLOSESPIDER_ITEMPASSED')\n \n         self.counts = defaultdict(int)\n         self.tasks = {}\n@@ -30,12 +30,12 @@ class CloseDomain(object):\n \n     def spider_opened(self, spider):\n         self.tasks[spider] = reactor.callLater(self.timeout, scrapyengine.close_spider, \\\n-            spider=spider, reason='closedomain_timeout')\n+            spider=spider, reason='closespider_timeout')\n         \n     def item_passed(self, item, spider):\n         self.counts[spider] += 1\n         if self.counts[spider] == self.itempassed:\n-            scrapyengine.close_spider(spider, 'closedomain_itempassed')\n+            scrapyengine.close_spider(spider, 'closespider_itempassed')\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fee1b751e5f2e826f9cfdb3084a1527634853b32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 107 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ level_names = {\n BOT_NAME = settings['BOT_NAME']\n \n # signal sent when log message is received\n-# args: message, level, domain\n+# args: message, level, spider\n logmessage_received = object()\n \n # default logging level\n@@ -67,8 +67,12 @@ def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):\n     \"\"\"Log message according to the level\"\"\"\n     if level > log_level:\n         return\n+    if domain is not None:\n+        import warnings\n+        warnings.warn(\"'domain' argument of scrapy.log.msg() is deprecated, \" \\\n+            \"use 'spider' argument instead\", DeprecationWarning, stacklevel=2)\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n-        domain=domain, spider=spider)\n+        spider=spider)\n     system = domain or (spider.domain_name if spider else component)\n     msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n     log.msg(msg_txt, system=system)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40646d3cd167e26b4e1eb29666712e73fee86a9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 16 | Files Changed: 9 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 1215 | Contributors (this commit): 5 | Commits (past 90d): 42 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,7 +118,8 @@ class Cache(object):\n         if expiration_secs >= 0:\n             expiration_date = metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs)\n             if datetime.datetime.utcnow() > expiration_date:\n-                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG, domain=domain)\n+                log.msg('dropping old cached response from %s' % metadata['timestamp'], \\\n+                    level=log.DEBUG, domain=domain)\n                 return None\n         return metadata\n \n\n@@ -41,11 +41,11 @@ class RedirectMiddleware(object):\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n             log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request),\n-                    domain=spider.domain_name, level=log.DEBUG)\n+                    spider=spider, level=log.DEBUG)\n             return redirected\n         else:\n             log.msg(\"Discarding %s: max redirections reached\" % request,\n-                    domain=spider.domain_name, level=log.DEBUG)\n+                    spider=spider, level=log.DEBUG)\n             raise IgnoreRequest\n \n     def _redirect_request_using_get(self, request, redirect_url):\n\n@@ -58,7 +58,7 @@ class RetryMiddleware(object):\n \n         if retries <= self.max_retry_times:\n             log.msg(\"Retrying %s (failed %d times): %s\" % (request, retries, reason),\n-                    domain=spider.domain_name, level=log.DEBUG)\n+                    spider=spider, level=log.DEBUG)\n             retryreq = request.copy()\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n@@ -66,5 +66,5 @@ class RetryMiddleware(object):\n             return retryreq\n         else:\n             log.msg(\"Discarding %s (failed %d times): %s\" % (request, retries, reason),\n-                    domain=spider.domain_name, level=log.DEBUG)\n+                    spider=spider, level=log.DEBUG)\n \n\n@@ -84,7 +84,7 @@ class TwistedPluginSpiderManager(object):\n         module_name = spider.__module__\n         module = sys.modules[module_name]\n         if hasattr(module, 'SPIDER'):\n-            log.msg(\"Reloading module %s\" % module_name, domain=domain, \\\n+            log.msg(\"Reloading module %s\" % module_name, spider=spider, \\\n                 level=log.DEBUG)\n             new_module = rebuild(module, doLog=0)\n             self._spiders[domain] = new_module.SPIDER\n\n@@ -25,7 +25,7 @@ class DepthMiddleware(object):\n                 request.meta['depth'] = depth\n                 if self.maxdepth and depth > self.maxdepth:\n                     log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), \\\n-                        level=log.DEBUG, domain=domain)\n+                        level=log.DEBUG, spider=spider)\n                     return False\n                 elif self.stats:\n                     stats.inc_value('request_depth_count/%s' % depth, domain=domain)\n\n@@ -19,7 +19,7 @@ class UrlLengthMiddleware(object):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n                 log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), \\\n-                    level=log.DEBUG, domain=spider.domain_name)\n+                    level=log.DEBUG, spider=spider)\n                 return False\n             else:\n                 return True\n\n@@ -76,6 +76,6 @@ class DecompressionMiddleware(object):\n             new_response = func(response)\n             if new_response:\n                 log.msg('Decompressed response with format: %s' % \\\n-                        fmt, log.DEBUG, domain=spider.domain_name)\n+                        fmt, log.DEBUG, spider=spider)\n                 return new_response\n         return response\n\n@@ -136,7 +136,7 @@ class Scraper(object):\n         referer = request.headers.get('Referer', None)\n         msg = \"Spider exception caught while processing <%s> (referer: <%s>): %s\" % \\\n             (request.url, referer, _failure)\n-        log.msg(msg, log.ERROR, domain=spider.domain_name)\n+        log.msg(msg, log.ERROR, spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n             domain=spider.domain_name)\n \n@@ -161,7 +161,7 @@ class Scraper(object):\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n             log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n-                domain=domain)\n+                spider=spider)\n             send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n                 item=output, spider=spider, response=response)\n             self.sites[spider].itemproc_size += 1\n@@ -176,7 +176,7 @@ class Scraper(object):\n             pass\n         else:\n             log.msg(\"Spider must return Request, BaseItem or None, got %r in %s\" % \\\n-                (type(output).__name__, request), log.ERROR, domain=domain)\n+                (type(output).__name__, request), log.ERROR, spider=spider)\n \n     def _check_propagated_failure(self, spider_failure, propagated_failure, request, spider):\n         \"\"\"Log and silence the bugs raised outside of spiders, but still allow\n@@ -200,14 +200,14 @@ class Scraper(object):\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, domain=domain)\n+                log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, spider=spider)\n                 send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n-                    log.ERROR, domain=domain)\n+                    log.ERROR, spider=spider)\n         else:\n-            log.msg(\"Passed %s\" % item, log.INFO, domain=domain)\n+            log.msg(\"Passed %s\" % item, log.INFO, spider=spider)\n             send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n                 item=item, spider=spider, output=output)\n \n\n@@ -66,7 +66,7 @@ class BaseSpider(object):\n         \"\"\"Log the given messages at the given log level. Always use this\n         method to send log messages from your spider\n         \"\"\"\n-        log.msg(message, domain=self.domain_name, level=level)\n+        log.msg(message, spider=self, level=level)\n \n     def start_requests(self):\n         reqs = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9bf4e87753ba9fce088a4b3e32f56aeec2e564ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 319 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +0,0 @@\n-from scrapy.selector import *\n-\n-import warnings\n-warnings.warn(\"scrapy.xpath module is deprecated, use scrapy.selector instead\",\n-    DeprecationWarning, stacklevel=2)\n\n@@ -1,5 +0,0 @@\n-from scrapy.selector import *\n-\n-import warnings\n-warnings.warn(\"scrapy.xpath.selector module is deprecated, use scrapy.selector instead\",\n-    DeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a2a20489d3d379a23c7eac3573993fb1d61e9ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 46 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 48 | Churn Cumulative: 295 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ BOT_VERSION = '1.0'\n \n SPIDER_MODULES = ['googledir.spiders']\n NEWSPIDER_MODULE = 'googledir.spiders'\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']\n\n@@ -83,29 +83,3 @@ class Item(DictItem):\n \n     __metaclass__ = ItemMeta\n \n-\n-class ScrapedItem(BaseItem):\n-\n-    def __init__(self, data=None):\n-        \"\"\"\n-        A ScrapedItem can be initialised with a dictionary that will be\n-        squirted directly into the object.\n-        \"\"\"\n-        import warnings\n-        warnings.warn(\"scrapy.item.ScrapedItem is deprecated, use scrapy.item.Item instead\",\n-            DeprecationWarning, stacklevel=2)\n-        if isinstance(data, dict):\n-            for attr, value in data.iteritems():\n-                setattr(self, attr, value)\n-        elif data is not None:\n-            raise TypeError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n-\n-    def __repr__(self):\n-        \"\"\"\n-        Generate the following format so that items can be deserialized\n-        easily: ClassName({'attrib': value, ...})\n-        \"\"\"\n-        reprdict = dict(items for items in self.__dict__.iteritems() \\\n-            if not items[0].startswith('_'))\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n-\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.item import Item, Field, ScrapedItem\n+from scrapy.item import Item, Field\n \n \n class ItemTest(unittest.TestCase):\n@@ -127,23 +127,5 @@ class ItemTest(unittest.TestCase):\n         self.assertEqual(dict(i), {'name': u'John'})\n \n \n-# NOTE: ScrapedItem is deprecated and will be removed in the next stable\n-# release, and so will these tests.\n-\n-class ScrapedItemTestCase(unittest.TestCase):\n-\n-    def test_item(self):\n-        \n-        class MyItem(ScrapedItem):\n-            pass\n-\n-        item = MyItem()\n-        self.assertEqual(repr(item), 'MyItem({})')\n-\n-        item = ScrapedItem({'key': 'value'})\n-        self.assertEqual(item.key, 'value')\n-\n-        self.assertRaises(TypeError, ScrapedItem, 10)\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee08d38ab66aa3a0e9cacf46d3fc1add0c7a0683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 101 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,14 +6,9 @@ See documentation in docs/topics/settings.rst\n \n import os\n import cPickle as pickle\n-import warnings\n \n from scrapy.conf import default_settings\n \n-if 'SCRAPYSETTINGS_MODULE' in os.environ:\n-    warnings.warn(\"SCRAPYSETTINGS_MODULE environment variable is deprecated, \" \\\n-        \"use SCRAPY_SETTINGS_MODULE instead\", DeprecationWarning, stacklevel=2)\n-\n import_ = lambda x: __import__(x, {}, {}, [''])\n \n class Settings(object):\n@@ -23,7 +18,7 @@ class Settings(object):\n         self.global_defaults = default_settings\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n         settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n-            os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings'))\n+            'scrapy_settings')\n         self.set_settings_module(settings_module_path)\n \n         # XXX: find a better solution for this hack\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#415dec4e1616a57423172e21b17de5d8d773802a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 98 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,17 +10,29 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy import log\n \n class OffsiteMiddleware(object):\n \n     def __init__(self):\n         self.host_regexes = {}\n+        self.domains_seen = {}\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def process_spider_output(self, response, result, spider):\n-        return (x for x in result if not isinstance(x, Request) or \\\n-            self.should_follow(x, spider))\n+        for x in result:\n+            if isinstance(x, Request):\n+                if self.should_follow(x, spider):\n+                    yield x\n+                else:\n+                    domain = urlparse_cached(x).hostname\n+                    if domain and domain not in self.domains_seen[spider]:\n+                        log.msg(\"Filtered offsite request to %r: %s\" % (domain, x),\n+                            level=log.DEBUG, spider=spider)\n+                        self.domains_seen[spider].add(domain)\n+            else:\n+                yield x\n \n     def should_follow(self, request, spider):\n         regex = self.host_regexes[spider]\n@@ -37,6 +49,8 @@ class OffsiteMiddleware(object):\n     def spider_opened(self, spider):\n         domains = [spider.domain_name] + spider.extra_domain_names\n         self.host_regexes[spider] = self.get_host_regex(domains)\n+        self.domains_seen[spider] = set()\n \n     def spider_closed(self, spider):\n         del self.host_regexes[spider]\n+        del self.domains_seen[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#564abd10ad3a0770a3e07d7624614d7cf54e6e65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 132 | Files Changed: 4 | Hunks: 35 | Methods Changed: 36 | Complexity Δ (Sum/Max): 0/11 | Churn Δ: 293 | Churn Cumulative: 881 | Contributors (this commit): 5 | Commits (past 90d): 37 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,7 @@ import_ = lambda x: __import__(x, {}, {}, [''])\n \n class Settings(object):\n \n-    def __init__(self):\n+    def __init__(self, overrides=None):\n         self.defaults = {}\n         self.global_defaults = default_settings\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n@@ -24,6 +24,8 @@ class Settings(object):\n         # XXX: find a better solution for this hack\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n+        if overrides:\n+            self.overrides.update(overrides)\n \n     def __getitem__(self, opt_name):\n         if not self.disabled:\n\n@@ -94,7 +94,7 @@ GROUPSETTINGS_MODULE = ''\n \n HTTPCACHE_DIR = ''\n HTTPCACHE_IGNORE_MISSING = False\n-HTTPCACHE_SECTORIZE = True\n+HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n\n@@ -1,181 +1,122 @@\n from __future__ import with_statement\n \n-import errno\n import os\n-import hashlib\n-import datetime\n+from os.path import join, exists\n+from time import time\n import cPickle as pickle\n-from scrapy.xlib.pydispatch import dispatcher\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n-from scrapy import log\n from scrapy.http import Headers\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.conf import settings\n+from scrapy.utils.misc import load_object\n+from scrapy import conf\n \n \n class HttpCacheMiddleware(object):\n-    def __init__(self):\n-        if not settings['HTTPCACHE_DIR']:\n-            raise NotConfigured\n-        self.cache = Cache(settings['HTTPCACHE_DIR'], sectorize=settings.getbool('HTTPCACHE_SECTORIZE'))\n-        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n-        dispatcher.connect(self.open_domain, signal=signals.spider_opened)\n \n-    def open_domain(self, spider):\n-        self.cache.open_domain(spider.domain_name)\n+    def __init__(self, settings=conf.settings):\n+        self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n+        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+\n+    def spider_opened(self, spider):\n+        self.storage.open_spider(spider)\n+\n+    def spider_closed(self, spider):\n+        self.storage.close_spider(spider)\n \n     def process_request(self, request, spider):\n-        if not is_cacheable(request):\n+        if not self.is_cacheable(request):\n             return\n-\n-        key = request_fingerprint(request)\n-        domain = spider.domain_name\n-\n-        try:\n-            response = self.cache.retrieve_response(domain, key)\n-        except:\n-            log.msg(\"Corrupt cache for %s\" % request.url, log.WARNING)\n-            response = False\n-\n+        response = self.storage.retrieve_response(spider, request)\n         if response:\n+            response.flags.append('cached')\n             return response\n         elif self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if is_cacheable(request):\n-            key = request_fingerprint(request)\n-            self.cache.store(spider.domain_name, key, request, response)\n-\n+        if self.is_cacheable(request):\n+            self.storage.store_response(spider, request, response)\n         return response\n \n-\n-def is_cacheable(request):\n+    def is_cacheable(self, request):\n         return urlparse_cached(request).scheme in ['http', 'https']\n \n \n-class Cache(object):\n-    DOMAIN_SECTORDIR = 'data'\n-    DOMAIN_LINKDIR = 'domains'\n+class FilesystemCacheStorage(object):\n \n-    def __init__(self, cachedir, sectorize=False):\n+    def __init__(self, settings=conf.settings):\n+        cachedir = settings['HTTPCACHE_DIR']\n+        if not cachedir:\n+            raise NotConfigured\n         self.cachedir = cachedir\n-        self.sectorize = sectorize\n+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n \n-        self.baselinkpath = os.path.join(self.cachedir, self.DOMAIN_LINKDIR)\n-        if not os.path.exists(self.baselinkpath):\n-            os.makedirs(self.baselinkpath)\n+    def open_spider(self, spider):\n+        pass\n \n-        self.basesectorpath = os.path.join(self.cachedir, self.DOMAIN_SECTORDIR)\n-        if not os.path.exists(self.basesectorpath):\n-            os.makedirs(self.basesectorpath)\n+    def close_spider(self, spider):\n+        pass\n \n-    def domainsectorpath(self, domain):\n-        sector = hashlib.sha1(domain).hexdigest()[0]\n-        return os.path.join(self.basesectorpath, sector, domain)\n-\n-    def domainlinkpath(self, domain):\n-        return os.path.join(self.baselinkpath, domain)\n-\n-    def requestpath(self, domain, key):\n-        linkpath = self.domainlinkpath(domain)\n-        return os.path.join(linkpath, key[0:2], key)\n-\n-    def open_domain(self, domain):\n-        if domain:\n-            linkpath = self.domainlinkpath(domain)\n-            if self.sectorize:\n-                sectorpath = self.domainsectorpath(domain)\n-                if not os.path.exists(sectorpath):\n-                    os.makedirs(sectorpath)\n-                if not os.path.exists(linkpath):\n-                    try:\n-                        os.symlink(sectorpath, linkpath)\n-                    except:\n-                        os.makedirs(linkpath) # windows filesystem\n-            else:\n-                if not os.path.exists(linkpath):\n-                    os.makedirs(linkpath)\n-\n-    def read_meta(self, domain, key):\n-        \"\"\"Return the metadata dictionary (possibly empty) if the entry is\n-        cached, None otherwise.\n-        \"\"\"\n-        requestpath = self.requestpath(domain, key)\n-        try:\n-            with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:\n-                metadata = pickle.load(f)\n-        except IOError, e:\n-            if e.errno != errno.ENOENT:\n-                raise\n-            return None\n-        expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        if expiration_secs >= 0:\n-            expiration_date = metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs)\n-            if datetime.datetime.utcnow() > expiration_date:\n-                log.msg('dropping old cached response from %s' % metadata['timestamp'], \\\n-                    level=log.DEBUG, domain=domain)\n-                return None\n-        return metadata\n-\n-    def retrieve_response(self, domain, key):\n-        \"\"\"\n-        Return response dictionary if request has correspondent cache record;\n-        return None if not.\n-        \"\"\"\n-        metadata = self.read_meta(domain, key)\n+    def retrieve_response(self, spider, request):\n+        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n+        metadata = self._read_meta(spider, request)\n         if metadata is None:\n-            return None # not cached\n-\n-        requestpath = self.requestpath(domain, key)\n-        responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'response_body')) as f:\n-            responsebody = f.read()\n-        with open(os.path.join(requestpath, 'response_headers')) as f:\n-            responseheaders = f.read()\n-\n+            return # not cached\n+        rpath = self._get_request_path(spider, request)\n+        with open(join(rpath, 'response_body'), 'rb') as f:\n+            body = f.read()\n+        with open(join(rpath, 'response_headers'), 'rb') as f:\n+            rawheaders = f.read()\n         url = metadata['url']\n-        headers = Headers(headers_raw_to_dict(responseheaders))\n         status = metadata['status']\n-\n+        headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url)\n-        response = respcls(url=url, headers=headers, status=status, body=responsebody)\n-        response.meta['cached'] = True\n-        response.flags.append('cached')\n+        response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n-    def store(self, domain, key, request, response):\n-        requestpath = self.requestpath(domain, key)\n-        if not os.path.exists(requestpath):\n-            os.makedirs(requestpath)\n-\n+    def store_response(self, spider, request, response):\n+        \"\"\"Store the given response in the cache.\"\"\"\n+        rpath = self._get_request_path(spider, request)\n+        if not exists(rpath):\n+            os.makedirs(rpath)\n         metadata = {\n             'url': request.url,\n             'method': request.method,\n             'status': response.status,\n-                'domain': domain,\n-                'timestamp': datetime.datetime.utcnow(),\n+            'timestamp': time(),\n         }\n-\n-        # metadata\n-        with open(os.path.join(requestpath, 'meta_data'), 'w') as f:\n+        with open(join(rpath, 'meta'), 'wb') as f:\n             f.write(repr(metadata))\n-        # pickled metadata (to recover without using eval)\n-        with open(os.path.join(requestpath, 'pickled_meta'), 'w') as f:\n-            pickle.dump(metadata, f)\n-        # response\n-        with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n+        with open(join(rpath, 'pickled_meta'), 'wb') as f:\n+            pickle.dump(metadata, f, protocol=2)\n+        with open(join(rpath, 'response_headers'), 'wb') as f:\n             f.write(headers_dict_to_raw(response.headers))\n-        with open(os.path.join(requestpath, 'response_body'), 'w') as f:\n+        with open(join(rpath, 'response_body'), 'wb') as f:\n             f.write(response.body)\n-        # request\n-        with open(os.path.join(requestpath, 'request_headers'), 'w') as f:\n+        with open(join(rpath, 'request_headers'), 'wb') as f:\n             f.write(headers_dict_to_raw(request.headers))\n-        if request.body:\n-            with open(os.path.join(requestpath, 'request_body'), 'w') as f:\n+        with open(join(rpath, 'request_body'), 'wb') as f:\n             f.write(request.body)\n+\n+    def _get_request_path(self, spider, request):\n+        key = request_fingerprint(request)\n+        return join(self.cachedir, spider.domain_name, key[0:2], key)\n+\n+    def _read_meta(self, spider, request):\n+        rpath = self._get_request_path(spider, request)\n+        metapath = join(rpath, 'pickled_meta')\n+        if not exists(metapath):\n+            return # not found\n+        mtime = os.stat(rpath).st_mtime\n+        if 0 <= self.expiration_secs < time() - mtime:\n+            return # expired\n+        with open(metapath, 'rb') as f:\n+            return pickle.load(f)\n\n@@ -0,0 +1,86 @@\n+import unittest, tempfile, shutil, time\n+\n+from scrapy.http import Response, HtmlResponse, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware\n+from scrapy.conf import Settings\n+from scrapy.core.exceptions import IgnoreRequest\n+\n+\n+class HttpCacheMiddlewareTest(unittest.TestCase):\n+\n+    storage_class = FilesystemCacheStorage\n+\n+    def setUp(self):\n+        self.spider = BaseSpider('example.com')\n+        self.tmpdir = tempfile.mkdtemp()\n+        self.request = Request('http://www.example.com', headers={'User-Agent': 'test'})\n+        self.response = Response('http://www.example.com', headers={'Content-Type': 'text/html'}, body='test body', status=202)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdir)\n+\n+    def _get_settings(self, **new_settings):\n+        settings = {\n+            'HTTPCACHE_DIR': self.tmpdir,\n+            'HTTPCACHE_EXPIRATION_SECS': 1,\n+        }\n+        settings.update(new_settings)\n+        return Settings(settings)\n+\n+    def _get_storage(self, **new_settings):\n+        return self.storage_class(self._get_settings(**new_settings))\n+\n+    def _get_middleware(self, **new_settings):\n+        return HttpCacheMiddleware(self._get_settings(**new_settings))\n+\n+    def test_storage(self):\n+        storage = self._get_storage()\n+        request2 = self.request.copy()\n+        assert storage.retrieve_response(self.spider, request2) is None\n+        storage.store_response(self.spider, self.request, self.response)\n+        response2 = storage.retrieve_response(self.spider, request2)\n+        assert isinstance(response2, HtmlResponse) # inferred from content-type header\n+        self.assertEqualResponse(self.response, response2)\n+        time.sleep(2) # wait for cache to expire\n+        assert storage.retrieve_response(self.spider, request2) is None\n+\n+    def test_storage_expire_immediately(self):\n+        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=0)\n+        assert storage.retrieve_response(self.spider, self.request) is None\n+        storage.store_response(self.spider, self.request, self.response)\n+        assert storage.retrieve_response(self.spider, self.request) is None\n+\n+    def test_storage_never_expire(self):\n+        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=-1)\n+        assert storage.retrieve_response(self.spider, self.request) is None\n+        storage.store_response(self.spider, self.request, self.response)\n+        assert storage.retrieve_response(self.spider, self.request)\n+\n+    def test_middleware(self):\n+        mw = HttpCacheMiddleware(self._get_settings())\n+        assert mw.process_request(self.request, self.spider) is None\n+        mw.process_response(self.request, self.response, self.spider)\n+        response = mw.process_request(self.request, self.spider)\n+        assert isinstance(response, HtmlResponse)\n+        self.assertEqualResponse(self.response, response)\n+        assert 'cached' in response.flags\n+\n+    def test_middleware_ignore_missing(self):\n+        mw = self._get_middleware(HTTPCACHE_IGNORE_MISSING=True)\n+        self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n+        mw.process_response(self.request, self.response, self.spider)\n+        response = mw.process_request(self.request, self.spider)\n+        assert isinstance(response, HtmlResponse)\n+        self.assertEqualResponse(self.response, response)\n+        assert 'cached' in response.flags\n+\n+    def assertEqualResponse(self, response1, response2):\n+        self.assertEqual(response1.url, response2.url)\n+        self.assertEqual(response1.status, response2.status)\n+        self.assertEqual(response1.headers, response2.headers)\n+        self.assertEqual(response1.body, response2.body)\n+\n+if __name__ == '__main__':\n+    unittest.main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
