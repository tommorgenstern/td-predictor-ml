{"custom_id": "keras#91807cef2e0fd1b0955f5c3109bc1c042962ddeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 263 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 264 | Churn Cumulative: 384 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -35,6 +35,30 @@ class SegmentSum(Operation):\n \n @keras_core_export(\"keras_core.ops.segment_sum\")\n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n+    \"\"\"Computes the sum of segments in a tensor.\n+\n+    Args:\n+        data: Input tensor.\n+        segment_ids: A 1-D tensor containing segment indices for each\n+            element in `data`.\n+        num_segments: An integer representing the total number of\n+            segments. If not specified, it is inferred from the maximum\n+            value in `segment_ids`.\n+        sorted: A boolean indicating whether `segment_ids` is sorted.\n+            Default is `False`.\n+\n+    Returns:\n+        A tensor containing the sum of segments, where each element\n+        represents the sum of the corresponding segment in `data`.\n+\n+    Example:\n+    ```python\n+    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n+    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n+    >>> segment_sum(data, segment_ids)\n+    array([9 12], shape=(2,), dtype=int32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n     return backend.math.segment_sum(\n@@ -63,6 +87,29 @@ class TopK(Operation):\n \n @keras_core_export(\"keras_core.ops.top_k\")\n def top_k(x, k, sorted=True):\n+    \"\"\"Finds the top-k values and their indices in a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        k: An integer representing the number of top elements to retrieve.\n+        sorted: A boolean indicating whether to sort the output in\n+        descending order. Default is `True`.\n+\n+    Returns:\n+        A tuple containing two tensors. The first tensor contains the\n+        top-k values, and the second tensor contains the indices of the\n+        top-k values in the input tensor.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n+    >>> values, indices = top_k(x, k=3)\n+    >>> print(values)\n+    array([9 7 5], shape=(3,), dtype=int32)\n+    >>> print(indices)\n+    array([4 2 0], shape=(3,), dtype=int32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return TopK(k, sorted).symbolic_call(x)\n     return backend.math.top_k(x, k, sorted)\n@@ -82,6 +129,28 @@ class InTopK(Operation):\n \n @keras_core_export(\"keras_core.ops.in_top_k\")\n def in_top_k(targets, predictions, k):\n+    \"\"\"Checks if the targets are in the top-k predictions.\n+\n+    Args:\n+        targets: A tensor of true labels.\n+        predictions: A tensor of predicted labels.\n+        k: An integer representing the number of predictions to consider.\n+\n+    Returns:\n+        A boolean tensor of the same shape as `targets`, where each element\n+        indicates whether the corresponding target is in the top-k predictions.\n+        \n+    Example:\n+    ```python\n+    >>> targets = keras_core.ops.convert_to_tensor([2, 5, 3])\n+    >>> predictions = keras_core.ops.convert_to_tensor(\n+                                                    [[0.1, 0.4, 0.6, 0.9, 0.5],\n+                                                    [0.1, 0.7, 0.9, 0.8, 0.3],\n+                                                    [0.1, 0.6, 0.9, 0.9, 0.5]])\n+    >>> in_top_k(targets, predictions, k=3)\n+    array([ True False  True], shape=(3,), dtype=bool)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((targets, predictions)):\n         return InTopK(k).symbolic_call(targets, predictions)\n     return backend.math.in_top_k(targets, predictions, k)\n@@ -103,6 +172,27 @@ class Logsumexp(Operation):\n \n @keras_core_export(\"keras_core.ops.logsumexp\")\n def logsumexp(x, axis=None, keepdims=False):\n+    \"\"\"Computes the logarithm of sum of exponentials of elements in a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: An integer or a tuple of integers specifying the axis/axes\n+            along which to compute the sum. If `None`, the sum is computed\n+            over all elements. Default is `None`.\n+        keepdims: A boolean indicating whether to keep the dimensions of\n+            the input tensor when computing the sum. Default is `False`.\n+\n+    Returns:\n+        A tensor containing the logarithm of the sum of exponentials of\n+        elements in `x`.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([1., 2., 3.])\n+    >>> logsumexp(x)\n+    array(3.407606, shape=(), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Logsumexp(axis, keepdims).symbolic_call(x)\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n@@ -152,6 +242,30 @@ class Qr(Operation):\n \n @keras_core_export(\"keras_core.ops.qr\")\n def qr(x, mode=\"reduced\"):\n+    \"\"\"Computes the QR decomposition of a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        mode: A string specifying the mode of the QR decomposition.\n+            - 'reduced': Returns the reduced QR decomposition. (default)\n+            - 'complete': Returns the complete QR decomposition.\n+\n+    Returns:\n+        A tuple containing two tensors. The first tensor represents the\n+        orthogonal matrix Q, and the second tensor represents the upper\n+        triangular matrix R.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n+    >>> q, r = qr(x)\n+    >>> print(q)\n+    array([[-0.16903079  0.897085]\n+     [-0.5070925   0.2760267 ]\n+     [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n+    ```\n+    \"\"\"\n+\n     if any_symbolic_tensors((x,)):\n         return Qr(mode=mode).symbolic_call(x)\n     return backend.math.qr(x, mode=mode)\n\n@@ -899,7 +899,7 @@ def conv_transpose(\n             the output tensor. Can be a single integer to specify the same\n             value for all spatial dimensions. The amount of output padding\n             along a given dimension must be lower than the stride along that\n-            same dimension. If set to None (default), the output shape is\n+            same dimension. If set to `None` (default), the output shape is\n             inferred.\n         data_format: A string, either \"channels_last\" or `channels_first`.\n             `data_format` determines the ordering of the dimensions in the\n@@ -959,6 +959,38 @@ class OneHot(Operation):\n \n @keras_core_export([\"keras_core.ops.one_hot\", \"keras_core.ops.nn.one_hot\"])\n def one_hot(x, num_classes, axis=-1, dtype=None):\n+    \"\"\"Converts integer tensor `x` into a one-hot tensor.\n+\n+    The one-hot encoding is a representation where each integer value is\n+    converted into a binary vector with a length equal to `num_classes`,\n+    and the index corresponding to the integer value is marked as 1, while\n+    all other indices are marked as 0.\n+\n+    Args:\n+        x : Integer tensor to be encoded. The shape can be\n+            arbitrary, but the dtype should be integer.\n+        num_classes: Number of classes for the one-hot encoding.\n+        axis: Axis along which the encoding is performed. Default is\n+            -1, which represents the last axis.\n+        dtype: (Optional) Data type of the output tensor. If not\n+            provided, it defaults to the default data type of the backend.\n+\n+    Returns:\n+        Integer tensor: One-hot encoded tensor with the same shape as `x`\n+        except for the specified `axis` dimension, which will have\n+        a length of `num_classes`. The dtype of the output tensor\n+        is determined by `dtype` or the default data type of the backend.\n+\n+    Example:\n+    ```python\n+    >>> x = keras_core.ops.convert_to_tensor([1, 3, 2, 0])\n+    >>> one_hot(x, num_classes=4)\n+    array([[0. 1. 0. 0.]\n+     [0. 0. 0. 1.]\n+     [0. 0. 1. 0.]\n+     [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n     return backend.nn.one_hot(\n@@ -993,6 +1025,40 @@ class BinaryCrossentropy(Operation):\n     ]\n )\n def binary_crossentropy(target, output, from_logits=False):\n+    \"\"\"Computes binary cross-entropy loss between target and output tensor.\n+\n+    The binary cross-entropy loss is commonly used in binary\n+    classification tasks where each input sample belongs to one\n+    of the two classes. It measures the dissimilarity between the\n+    target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true binary labels.\n+            Its shape should match the shape of the `output` tensor.\n+        output: The output tensor representing the predicted probabilities\n+            or logits. Its shape should match the shape of the\n+            `target` tensor.\n+        from_logits: (optional) Whether `output` is a tensor of logits or\n+            probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+\n+    Returns:\n+        Integer tensor: The computed binary cross-entropy loss between\n+        `target` and `output`.\n+\n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0],\n+                                                  dtype=float32)\n+    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2],\n+                                                  dtype=float32)\n+    >>> binary_crossentropy(target, output)\n+    array([0.10536054 0.10536054 0.22314355 0.22314355], \n+        shape=(4,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return BinaryCrossentropy(from_logits=from_logits).symbolic_call(\n             target, output\n@@ -1036,6 +1102,48 @@ class CategoricalCrossentropy(Operation):\n     ]\n )\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Computes categorical cross-entropy loss between target and output tensor.\n+\n+    The categorical cross-entropy loss is commonly used in multi-class\n+    classification tasks where each input sample can belong to one of\n+    multiple classes. It measures the dissimilarity\n+    between the target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true categorical labels.\n+            Its shape should match the shape of the `output` tensor\n+            except for the last dimension.\n+        output: The output tensor representing the predicted probabilities\n+            or logits. Its shape should match the shape of the `target`\n+            tensor except for the last dimension.\n+        from_logits: (optional) Whether `output` is a tensor of logits or\n+            probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+        axis: (optional) The axis along which the categorical cross-entropy\n+            is computed.\n+            Default is -1, which corresponds to the last dimension of\n+            the tensors.\n+\n+    Returns:\n+        Integer tensor: The computed categorical cross-entropy loss between\n+        `target` and `output`.\n+\n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([[1, 0, 0],\n+                                                   [0, 1, 0],\n+                                                   [0, 0, 1]],\n+                                                    dtype=float32)\n+    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05],\n+                                                   [0.1, 0.8, 0.1],\n+                                                   [0.2, 0.3, 0.5]],\n+                                                   dtype=float32)\n+    >>> categorical_crossentropy(target, output)\n+    array([0.10536054 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return CategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n@@ -1082,6 +1190,46 @@ class SparseCategoricalCrossentropy(Operation):\n     ]\n )\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Computes sparse categorical cross-entropy loss.\n+\n+    The sparse categorical cross-entropy loss is similar to categorical\n+    cross-entropy, but it is used when the target tensor contains integer\n+    class labels instead of one-hot encoded vectors. It measures the\n+    dissimilarity between the target and output probabilities or logits.\n+\n+    Args:\n+        target: The target tensor representing the true class labels as integers.\n+            Its shape should match the shape of the `output` tensor except\n+            for the last dimension.\n+        output: The output tensor representing the predicted probabilities\n+            or logits.\n+            Its shape should match the shape of the `target` tensor except \n+            for the last dimension.\n+        from_logits: (optional) Whether `output` is a tensor of logits\n+            or probabilities.\n+            Set it to `True` if `output` represents logits; otherwise,\n+            set it to `False` if `output` represents probabilities.\n+            Default is `False`.\n+        axis: (optional) The axis along which the sparse categorical\n+            cross-entropy is computed.\n+            Default is -1, which corresponds to the last dimension\n+            of the tensors.\n+\n+    Returns:\n+        Integer tensor: The computed sparse categorical cross-entropy\n+        loss between `target` and `output`.\n+        \n+    Example:\n+    ```python\n+    >>> target = keras_core.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n+    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05], \n+                                                   [0.1, 0.8, 0.1], \n+                                                   [0.2, 0.3, 0.5]],\n+                                                   dtype=float32)\n+    >>> sparse_categorical_crossentropy(target, output)\n+    array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n+    ```\n+    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return SparseCategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccb9db5b927b474acc88cbacd8d1802eb1d892e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 814 | Files Changed: 3 | Hunks: 43 | Methods Changed: 35 | Complexity Δ (Sum/Max): -24/0 | Churn Δ: 870 | Churn Cumulative: 1746 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,492 +0,0 @@\n-\"\"\"\n-Title: Zero-DCE for low-light image enhancement\n-Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/09/18\n-Last modified: 2023/07/15\n-Description: Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-**Zero-Reference Deep Curve Estimation** or **Zero-DCE** formulates low-light image\n-enhancement as the task of estimating an image-specific\n-[*tonal curve*](https://en.wikipedia.org/wiki/Curve_(tonality)) with a deep neural network.\n-In this example, we train a lightweight deep network, **DCE-Net**, to estimate\n-pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.\n-\n-Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output.\n-These curves are then used for pixel-wise adjustment on the dynamic range of the input to\n-obtain an enhanced image. The curve estimation process is done in such a way that it maintains\n-the range of the enhanced image and preserves the contrast of neighboring pixels. This\n-curve estimation is inspired by curves adjustment used in photo editing software such as\n-Adobe Photoshop where users can adjust points throughout an image’s tonal range.\n-\n-Zero-DCE is appealing because of its relaxed assumptions with regard to reference images:\n-it does not require any input/output image pairs during training.\n-This is achieved through a set of carefully formulated non-reference loss functions,\n-which implicitly measure the enhancement quality and guide the training of the network.\n-\n-### References\n-\n-- [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](https://arxiv.org/pdf/2001.06826.pdf)\n-- [Curves adjustment in Adobe Photoshop](https://helpx.adobe.com/photoshop/using/curves-adjustment.html)\n-\"\"\"\n-\n-\"\"\"\n-## Downloading LOLDataset\n-\n-The **LoL Dataset** has been created for low-light image enhancement. It provides 485\n-images for training and 15 for testing. Each image pair in the dataset consists of a\n-low-light input image and its corresponding well-exposed reference image.\n-\"\"\"\n-\n-import os\n-import random\n-import numpy as np\n-from glob import glob\n-from PIL import Image, ImageOps\n-import matplotlib.pyplot as plt\n-\n-import keras_core as keras\n-from keras_core import layers\n-\n-import tensorflow as tf\n-\n-\"\"\"shell\n-wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n-unzip -q lol_dataset.zip && rm lol_dataset.zip\n-\"\"\"\n-\n-\"\"\"\n-## Creating a TensorFlow Dataset\n-\n-We use 300 low-light images from the LoL Dataset training set for training, and we use\n-the remaining 185 low-light images for validation. We resize the images to size `256 x\n-256` to be used for both training and validation. Note that in order to train the DCE-Net,\n-we will not require the corresponding enhanced images.\n-\"\"\"\n-\n-IMAGE_SIZE = 256\n-BATCH_SIZE = 16\n-MAX_TRAIN_IMAGES = 400\n-\n-\n-def load_data(image_path):\n-    image = tf.io.read_file(image_path)\n-    image = tf.image.decode_png(image, channels=3)\n-    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n-    image = image / 255.0\n-    return image\n-\n-\n-def data_generator(low_light_images):\n-    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n-    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n-    return dataset\n-\n-\n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n-test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n-\n-\n-train_dataset = data_generator(train_low_light_images)\n-val_dataset = data_generator(val_low_light_images)\n-\n-print(\"Train Dataset:\", train_dataset)\n-print(\"Validation Dataset:\", val_dataset)\n-\n-\"\"\"\n-## The Zero-DCE Framework\n-\n-The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n-(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n-channels by applying the curves iteratively to obtain the final enhanced image.\n-\n-### Understanding light-enhancement curves\n-\n-A ligh-enhancement curve is a kind of curve that can map a low-light image\n-to its enhanced version automatically,\n-where the self-adaptive curve parameters are solely dependent on the input image.\n-When designing such a curve, three objectives should be taken into account:\n-\n-- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n-avoid information loss induced by overflow truncation.\n-- It should be monotonous, to preserve the contrast between neighboring pixels.\n-- The shape of this curve should be as simple as possible,\n-and the curve should be differentiable to allow backpropagation.\n-\n-The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n-illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n-the risk of over-saturation.\n-\n-![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n-\n-### DCE-Net\n-\n-The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n-image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n-image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n-higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n-concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n-followed by the ReLU activation function. The last convolutional layer is followed by the\n-Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n-iteration requires three curve parameter maps for the three channels.\n-\n-![](https://i.imgur.com/HtIg34W.png)\n-\"\"\"\n-\n-\n-def build_dce_net():\n-    input_img = keras.Input(shape=[None, None, 3])\n-    conv1 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(input_img)\n-    conv2 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv1)\n-    conv3 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv2)\n-    conv4 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(conv3)\n-    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n-    conv5 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(int_con1)\n-    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n-    conv6 = layers.Conv2D(\n-        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n-    )(int_con2)\n-    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n-    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n-        int_con3\n-    )\n-    return keras.Model(inputs=input_img, outputs=x_r)\n-\n-\n-\"\"\"\n-## Loss functions\n-\n-To enable zero-reference learning in DCE-Net, we use a set of differentiable\n-zero-reference losses that allow us to evaluate the quality of enhanced images.\n-\"\"\"\n-\n-\"\"\"\n-### Color constancy loss\n-\n-The *color constancy loss* is used to correct the potential color deviations in the\n-enhanced image.\n-\"\"\"\n-\n-\n-def color_constancy_loss(x):\n-    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n-    mr, mg, mb = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n-    d_rg = tf.square(mr - mg)\n-    d_rb = tf.square(mr - mb)\n-    d_gb = tf.square(mb - mg)\n-    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n-\n-\n-\"\"\"\n-### Exposure loss\n-\n-To restrain under-/over-exposed regions, we use the *exposure control loss*.\n-It measures the distance between the average intensity value of a local region\n-and a preset well-exposedness level (set to `0.6`).\n-\"\"\"\n-\n-\n-def exposure_loss(x, mean_val=0.6):\n-    x = tf.reduce_mean(x, axis=3, keepdims=True)\n-    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n-    return tf.reduce_mean(tf.square(mean - mean_val))\n-\n-\n-\"\"\"\n-### Illumination smoothness loss\n-\n-To preserve the monotonicity relations between neighboring pixels, the\n-*illumination smoothness loss* is added to each curve parameter map.\n-\"\"\"\n-\n-\n-def illumination_smoothness_loss(x):\n-    batch_size = tf.shape(x)[0]\n-    h_x = tf.shape(x)[1]\n-    w_x = tf.shape(x)[2]\n-    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n-    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n-    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n-    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n-    batch_size = tf.cast(batch_size, dtype=tf.float32)\n-    count_h = tf.cast(count_h, dtype=tf.float32)\n-    count_w = tf.cast(count_w, dtype=tf.float32)\n-    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n-\n-\n-\"\"\"\n-### Spatial consistency loss\n-\n-The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n-preserving the contrast between neighboring regions across the input image and its enhanced version.\n-\"\"\"\n-\n-\n-class SpatialConsistencyLoss(keras.losses.Loss):\n-    def __init__(self, **kwargs):\n-        super().__init__(reduction=\"none\")\n-\n-        self.left_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.right_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.up_kernel = tf.constant(\n-            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n-        )\n-        self.down_kernel = tf.constant(\n-            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n-        )\n-\n-    def call(self, y_true, y_pred):\n-        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n-        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n-        original_pool = tf.nn.avg_pool2d(\n-            original_mean, ksize=4, strides=4, padding=\"VALID\"\n-        )\n-        enhanced_pool = tf.nn.avg_pool2d(\n-            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n-        )\n-\n-        d_original_left = tf.nn.conv2d(\n-            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_original_right = tf.nn.conv2d(\n-            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_original_up = tf.nn.conv2d(\n-            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_original_down = tf.nn.conv2d(\n-            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-\n-        d_enhanced_left = tf.nn.conv2d(\n-            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_enhanced_right = tf.nn.conv2d(\n-            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_enhanced_up = tf.nn.conv2d(\n-            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-        d_enhanced_down = tf.nn.conv2d(\n-            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n-        )\n-\n-        d_left = tf.square(d_original_left - d_enhanced_left)\n-        d_right = tf.square(d_original_right - d_enhanced_right)\n-        d_up = tf.square(d_original_up - d_enhanced_up)\n-        d_down = tf.square(d_original_down - d_enhanced_down)\n-        return d_left + d_right + d_up + d_down\n-\n-\n-\"\"\"\n-### Deep curve estimation model\n-\n-We implement the Zero-DCE framework as a Keras subclassed model.\n-\"\"\"\n-\n-\n-class ZeroDCE(keras.Model):\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-        self.dce_model = build_dce_net()\n-\n-    def compile(self, learning_rate, **kwargs):\n-        super().compile(**kwargs)\n-        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n-        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n-        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n-        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(name=\"illumination_smoothness_loss\")\n-        self.spatial_constancy_loss_tracker = keras.metrics.Mean(name=\"spatial_constancy_loss\")\n-        self.color_constancy_loss_tracker = keras.metrics.Mean(name=\"color_constancy_loss\")\n-        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.total_loss_tracker,\n-            self.illumination_smoothness_loss_tracker,\n-            self.spatial_constancy_loss_tracker,\n-            self.color_constancy_loss_tracker,\n-            self.exposure_loss_tracker,\n-        ]\n-\n-    def get_enhanced_image(self, data, output):\n-        r1 = output[:, :, :, :3]\n-        r2 = output[:, :, :, 3:6]\n-        r3 = output[:, :, :, 6:9]\n-        r4 = output[:, :, :, 9:12]\n-        r5 = output[:, :, :, 12:15]\n-        r6 = output[:, :, :, 15:18]\n-        r7 = output[:, :, :, 18:21]\n-        r8 = output[:, :, :, 21:24]\n-        x = data + r1 * (tf.square(data) - data)\n-        x = x + r2 * (tf.square(x) - x)\n-        x = x + r3 * (tf.square(x) - x)\n-        enhanced_image = x + r4 * (tf.square(x) - x)\n-        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n-        x = x + r6 * (tf.square(x) - x)\n-        x = x + r7 * (tf.square(x) - x)\n-        enhanced_image = x + r8 * (tf.square(x) - x)\n-        return enhanced_image\n-\n-    def call(self, data):\n-        dce_net_output = self.dce_model(data)\n-        return self.get_enhanced_image(data, dce_net_output)\n-\n-    def compute_losses(self, data, output):\n-        enhanced_image = self.get_enhanced_image(data, output)\n-        loss_illumination = 200 * illumination_smoothness_loss(output)\n-        loss_spatial_constancy = tf.reduce_mean(\n-            self.spatial_constancy_loss(enhanced_image, data)\n-        )\n-        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n-        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n-        total_loss = (\n-            loss_illumination\n-            + loss_spatial_constancy\n-            + loss_color_constancy\n-            + loss_exposure\n-        )\n-\n-        return {\n-            \"total_loss\": total_loss,\n-            \"illumination_smoothness_loss\": loss_illumination,\n-            \"spatial_constancy_loss\": loss_spatial_constancy,\n-            \"color_constancy_loss\": loss_color_constancy,\n-            \"exposure_loss\": loss_exposure,\n-        }\n-\n-    def train_step(self, data):\n-        with tf.GradientTape() as tape:\n-            output = self.dce_model(data)\n-            losses = self.compute_losses(data, output)\n-\n-        gradients = tape.gradient(\n-            losses[\"total_loss\"], self.dce_model.trainable_weights\n-        )\n-        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n-\n-        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n-        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n-        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n-        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n-\n-        return {metric.name: metric.result() for metric in self.metrics}\n-\n-    def test_step(self, data):\n-        output = self.dce_model(data)\n-        losses = self.compute_losses(data, output)\n-\n-        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n-        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n-        self.spatial_constancy_loss_tracker.update_state(losses[\"spatial_constancy_loss\"])\n-        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n-        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n-\n-        return {metric.name: metric.result() for metric in self.metrics}\n-\n-    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n-        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n-        self.dce_model.save_weights(\n-            filepath, overwrite=overwrite, save_format=save_format, options=options\n-        )\n-\n-    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n-        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n-        self.dce_model.load_weights(\n-            filepath=filepath,\n-            by_name=by_name,\n-            skip_mismatch=skip_mismatch,\n-            options=options,\n-        )\n-\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-zero_dce_model = ZeroDCE()\n-zero_dce_model.compile(learning_rate=1e-4)\n-history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n-\n-\n-def plot_result(item):\n-    plt.plot(history.history[item], label=item)\n-    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(item)\n-    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_result(\"total_loss\")\n-plot_result(\"illumination_smoothness_loss\")\n-plot_result(\"spatial_constancy_loss\")\n-plot_result(\"color_constancy_loss\")\n-plot_result(\"exposure_loss\")\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-\n-def plot_results(images, titles, figure_size=(12, 12)):\n-    fig = plt.figure(figsize=figure_size)\n-    for i in range(len(images)):\n-        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n-        _ = plt.imshow(images[i])\n-        plt.axis(\"off\")\n-    plt.show()\n-\n-\n-def infer(original_image):\n-    image = keras.utils.img_to_array(original_image)\n-    image = image.astype(\"float32\") / 255.0\n-    image = np.expand_dims(image, axis=0)\n-    output_image = zero_dce_model(image)\n-    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n-    output_image = Image.fromarray(output_image.numpy())\n-    return output_image\n-\n-\n-\"\"\"\n-### Inference on test images\n-\n-We compare the test images from LOLDataset enhanced by MIRNet with images enhanced via\n-the `PIL.ImageOps.autocontrast()` function.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/low-light-image-enhancement)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/low-light-image-enhancement).\n-\"\"\"\n-\n-for val_image_file in test_low_light_images:\n-    original_image = Image.open(val_image_file)\n-    enhanced_image = infer(original_image)\n-    plot_results(\n-        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n-        [\"Original\", \"PIL Autocontrast\", \"Enhanced\"],\n-        (20, 12),\n-    )\n\n@@ -35,30 +35,6 @@ class SegmentSum(Operation):\n \n @keras_core_export(\"keras_core.ops.segment_sum\")\n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n-    \"\"\"Computes the sum of segments in a tensor.\n-\n-    Args:\n-        data: Input tensor.\n-        segment_ids: A 1-D tensor containing segment indices for each\n-            element in `data`.\n-        num_segments: An integer representing the total number of\n-            segments. If not specified, it is inferred from the maximum\n-            value in `segment_ids`.\n-        sorted: A boolean indicating whether `segment_ids` is sorted.\n-            Default is `False`.\n-\n-    Returns:\n-        A tensor containing the sum of segments, where each element\n-        represents the sum of the corresponding segment in `data`.\n-\n-    Example:\n-    ```python\n-    >>> data = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n-    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 1, 0, 1, 0, 1])\n-    >>> segment_sum(data, segment_ids)\n-    array([9 12], shape=(2,), dtype=int32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((data,)):\n         return SegmentSum(num_segments, sorted).symbolic_call(data, segment_ids)\n     return backend.math.segment_sum(\n@@ -87,29 +63,6 @@ class TopK(Operation):\n \n @keras_core_export(\"keras_core.ops.top_k\")\n def top_k(x, k, sorted=True):\n-    \"\"\"Finds the top-k values and their indices in a tensor.\n-\n-    Args:\n-        x: Input tensor.\n-        k: An integer representing the number of top elements to retrieve.\n-        sorted: A boolean indicating whether to sort the output in\n-        descending order. Default is `True`.\n-\n-    Returns:\n-        A tuple containing two tensors. The first tensor contains the\n-        top-k values, and the second tensor contains the indices of the\n-        top-k values in the input tensor.\n-\n-    Example:\n-    ```python\n-    >>> x = keras_core.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n-    >>> values, indices = top_k(x, k=3)\n-    >>> print(values)\n-    array([9 7 5], shape=(3,), dtype=int32)\n-    >>> print(indices)\n-    array([4 2 0], shape=(3,), dtype=int32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return TopK(k, sorted).symbolic_call(x)\n     return backend.math.top_k(x, k, sorted)\n@@ -129,28 +82,6 @@ class InTopK(Operation):\n \n @keras_core_export(\"keras_core.ops.in_top_k\")\n def in_top_k(targets, predictions, k):\n-    \"\"\"Checks if the targets are in the top-k predictions.\n-\n-    Args:\n-        targets: A tensor of true labels.\n-        predictions: A tensor of predicted labels.\n-        k: An integer representing the number of predictions to consider.\n-\n-    Returns:\n-        A boolean tensor of the same shape as `targets`, where each element\n-        indicates whether the corresponding target is in the top-k predictions.\n-        \n-    Example:\n-    ```python\n-    >>> targets = keras_core.ops.convert_to_tensor([2, 5, 3])\n-    >>> predictions = keras_core.ops.convert_to_tensor(\n-                                                    [[0.1, 0.4, 0.6, 0.9, 0.5],\n-                                                    [0.1, 0.7, 0.9, 0.8, 0.3],\n-                                                    [0.1, 0.6, 0.9, 0.9, 0.5]])\n-    >>> in_top_k(targets, predictions, k=3)\n-    array([ True False  True], shape=(3,), dtype=bool)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((targets, predictions)):\n         return InTopK(k).symbolic_call(targets, predictions)\n     return backend.math.in_top_k(targets, predictions, k)\n@@ -172,27 +103,6 @@ class Logsumexp(Operation):\n \n @keras_core_export(\"keras_core.ops.logsumexp\")\n def logsumexp(x, axis=None, keepdims=False):\n-    \"\"\"Computes the logarithm of sum of exponentials of elements in a tensor.\n-\n-    Args:\n-        x: Input tensor.\n-        axis: An integer or a tuple of integers specifying the axis/axes\n-            along which to compute the sum. If `None`, the sum is computed\n-            over all elements. Default is `None`.\n-        keepdims: A boolean indicating whether to keep the dimensions of\n-            the input tensor when computing the sum. Default is `False`.\n-\n-    Returns:\n-        A tensor containing the logarithm of the sum of exponentials of\n-        elements in `x`.\n-\n-    Example:\n-    ```python\n-    >>> x = keras_core.ops.convert_to_tensor([1., 2., 3.])\n-    >>> logsumexp(x)\n-    array(3.407606, shape=(), dtype=float32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Logsumexp(axis, keepdims).symbolic_call(x)\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n@@ -242,30 +152,6 @@ class Qr(Operation):\n \n @keras_core_export(\"keras_core.ops.qr\")\n def qr(x, mode=\"reduced\"):\n-    \"\"\"Computes the QR decomposition of a tensor.\n-\n-    Args:\n-        x: Input tensor.\n-        mode: A string specifying the mode of the QR decomposition.\n-            - 'reduced': Returns the reduced QR decomposition. (default)\n-            - 'complete': Returns the complete QR decomposition.\n-\n-    Returns:\n-        A tuple containing two tensors. The first tensor represents the\n-        orthogonal matrix Q, and the second tensor represents the upper\n-        triangular matrix R.\n-\n-    Example:\n-    ```python\n-    >>> x = keras_core.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n-    >>> q, r = qr(x)\n-    >>> print(q)\n-    array([[-0.16903079  0.897085]\n-     [-0.5070925   0.2760267 ]\n-     [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n-    ```\n-    \"\"\"\n-\n     if any_symbolic_tensors((x,)):\n         return Qr(mode=mode).symbolic_call(x)\n     return backend.math.qr(x, mode=mode)\n\n@@ -364,9 +364,9 @@ def max_pool(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`. Pooling happens over the spatial\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n@@ -379,15 +379,15 @@ def max_pool(\n             each spatial dimension of the input tensor. If `strides` is int,\n             then every spatial dimension shares the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n \n     Returns:\n         A tensor of rank N+2, the result of the max pooling operation.\n@@ -454,9 +454,9 @@ def average_pool(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n             `data_format=\"channels_first\"`. Pooling happens over the spatial\n             dimensions only.\n         pool_size: int or tuple/list of integers of size\n@@ -469,15 +469,15 @@ def average_pool(\n             each spatial dimension of the input tensor. If `strides` is int,\n             then every spatial dimension shares the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n \n     Returns:\n         A tensor of rank N+2, the result of the average pooling operation.\n@@ -547,13 +547,12 @@ def conv(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,) + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n+            `data_format=\"channels_first\"`.\n         kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_output_channels],\n+            `(kernel_spatial_shape, num_input_channels, num_output_channels)`.\n             `num_input_channels` should match the number of channels in\n             `inputs`.\n         strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n@@ -561,15 +560,15 @@ def conv(\n             dimension. If `strides` is int, then every spatial dimension shares\n             the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n         dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n             specifying the dilation rate to use for dilated convolution. If\n             `dilation_rate` is int, then every spatial dimension shares\n@@ -645,11 +644,10 @@ def depthwise_conv(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n+            `data_format=\"channels_first\"`.\n         kernel: Tensor of rank N+2. `kernel` has shape\n             [kernel_spatial_shape, num_input_channels, num_channels_multiplier],\n             `num_input_channels` should match the number of channels in\n@@ -659,15 +657,15 @@ def depthwise_conv(\n             dimension. If `strides` is int, then every spatial dimension shares\n             the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n         dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n             specifying the dilation rate to use for dilated convolution. If\n             `dilation_rate` is int, then every spatial dimension shares\n@@ -756,32 +754,31 @@ def separable_conv(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n+            `data_format=\"channels_first\"`.\n         depthwise_kernel: Tensor of rank N+2. `depthwise_kernel` has shape\n             [kernel_spatial_shape, num_input_channels, num_channels_multiplier],\n             `num_input_channels` should match the number of channels in\n             `inputs`.\n         pointwise_kernel: Tensor of rank N+2. `pointwise_kernel` has shape\n-            [ones_like(kernel_spatial_shape),\n-            num_input_channels * num_channels_multiplier, num_output_channels].\n+            `(*ones_like(kernel_spatial_shape),\n+            num_input_channels * num_channels_multiplier, num_output_channels)`.\n         strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n             specifying the strides of the convolution along each spatial\n             dimension. If `strides` is int, then every spatial dimension shares\n             the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n         dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n             specifying the dilation rate to use for dilated convolution. If\n             `dilation_rate` is int, then every spatial dimension shares\n@@ -877,11 +874,10 @@ def conv_transpose(\n \n     Args:\n         inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n+            `(batch_size,)` + inputs_spatial_shape + (num_channels,)` if\n             `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n+            `(batch_size, num_channels) + inputs_spatial_shape` if\n+            `data_format=\"channels_first\"`.\n         kernel: Tensor of rank N+2. `kernel` has shape\n             [kernel_spatial_shape, num_output_channels, num_input_channels],\n             `num_input_channels` should match the number of channels in\n@@ -891,7 +887,7 @@ def conv_transpose(\n             dimension. If `strides` is int, then every spatial dimension shares\n             the same `strides`.\n         padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n+            padding is applied, and `\"same\"` results in padding evenly to the\n             left/right or up/down of the input such that output has the\n             same height/width dimension as the input when `strides=1`.\n         output_padding: int or int tuple/list of `len(inputs_spatial_shape)`,\n@@ -899,14 +895,14 @@ def conv_transpose(\n             the output tensor. Can be a single integer to specify the same\n             value for all spatial dimensions. The amount of output padding\n             along a given dimension must be lower than the stride along that\n-            same dimension. If set to `None` (default), the output shape is\n+            same dimension. If set to None (default), the output shape is\n             inferred.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`.\n             `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n+            inputs. If `data_format=\"channels_last\"`, `inputs` is of shape\n+            `(batch_size, ..., channels)` while if\n+            `data_format=\"channels_first\"`, `inputs` is of shape\n+            `(batch_size, channels, ...)`.\n         dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n             specifying the dilation rate to use for dilated convolution. If\n             `dilation_rate` is int, then every spatial dimension shares\n@@ -959,38 +955,6 @@ class OneHot(Operation):\n \n @keras_core_export([\"keras_core.ops.one_hot\", \"keras_core.ops.nn.one_hot\"])\n def one_hot(x, num_classes, axis=-1, dtype=None):\n-    \"\"\"Converts integer tensor `x` into a one-hot tensor.\n-\n-    The one-hot encoding is a representation where each integer value is\n-    converted into a binary vector with a length equal to `num_classes`,\n-    and the index corresponding to the integer value is marked as 1, while\n-    all other indices are marked as 0.\n-\n-    Args:\n-        x : Integer tensor to be encoded. The shape can be\n-            arbitrary, but the dtype should be integer.\n-        num_classes: Number of classes for the one-hot encoding.\n-        axis: Axis along which the encoding is performed. Default is\n-            -1, which represents the last axis.\n-        dtype: (Optional) Data type of the output tensor. If not\n-            provided, it defaults to the default data type of the backend.\n-\n-    Returns:\n-        Integer tensor: One-hot encoded tensor with the same shape as `x`\n-        except for the specified `axis` dimension, which will have\n-        a length of `num_classes`. The dtype of the output tensor\n-        is determined by `dtype` or the default data type of the backend.\n-\n-    Example:\n-    ```python\n-    >>> x = keras_core.ops.convert_to_tensor([1, 3, 2, 0])\n-    >>> one_hot(x, num_classes=4)\n-    array([[0. 1. 0. 0.]\n-     [0. 0. 0. 1.]\n-     [0. 0. 1. 0.]\n-     [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n     return backend.nn.one_hot(\n@@ -1025,40 +989,6 @@ class BinaryCrossentropy(Operation):\n     ]\n )\n def binary_crossentropy(target, output, from_logits=False):\n-    \"\"\"Computes binary cross-entropy loss between target and output tensor.\n-\n-    The binary cross-entropy loss is commonly used in binary\n-    classification tasks where each input sample belongs to one\n-    of the two classes. It measures the dissimilarity between the\n-    target and output probabilities or logits.\n-\n-    Args:\n-        target: The target tensor representing the true binary labels.\n-            Its shape should match the shape of the `output` tensor.\n-        output: The output tensor representing the predicted probabilities\n-            or logits. Its shape should match the shape of the\n-            `target` tensor.\n-        from_logits: (optional) Whether `output` is a tensor of logits or\n-            probabilities.\n-            Set it to `True` if `output` represents logits; otherwise,\n-            set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n-\n-    Returns:\n-        Integer tensor: The computed binary cross-entropy loss between\n-        `target` and `output`.\n-\n-    Example:\n-    ```python\n-    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0],\n-                                                  dtype=float32)\n-    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2],\n-                                                  dtype=float32)\n-    >>> binary_crossentropy(target, output)\n-    array([0.10536054 0.10536054 0.22314355 0.22314355], \n-        shape=(4,), dtype=float32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return BinaryCrossentropy(from_logits=from_logits).symbolic_call(\n             target, output\n@@ -1102,48 +1032,6 @@ class CategoricalCrossentropy(Operation):\n     ]\n )\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n-    \"\"\"Computes categorical cross-entropy loss between target and output tensor.\n-\n-    The categorical cross-entropy loss is commonly used in multi-class\n-    classification tasks where each input sample can belong to one of\n-    multiple classes. It measures the dissimilarity\n-    between the target and output probabilities or logits.\n-\n-    Args:\n-        target: The target tensor representing the true categorical labels.\n-            Its shape should match the shape of the `output` tensor\n-            except for the last dimension.\n-        output: The output tensor representing the predicted probabilities\n-            or logits. Its shape should match the shape of the `target`\n-            tensor except for the last dimension.\n-        from_logits: (optional) Whether `output` is a tensor of logits or\n-            probabilities.\n-            Set it to `True` if `output` represents logits; otherwise,\n-            set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n-        axis: (optional) The axis along which the categorical cross-entropy\n-            is computed.\n-            Default is -1, which corresponds to the last dimension of\n-            the tensors.\n-\n-    Returns:\n-        Integer tensor: The computed categorical cross-entropy loss between\n-        `target` and `output`.\n-\n-    Example:\n-    ```python\n-    >>> target = keras_core.ops.convert_to_tensor([[1, 0, 0],\n-                                                   [0, 1, 0],\n-                                                   [0, 0, 1]],\n-                                                    dtype=float32)\n-    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05],\n-                                                   [0.1, 0.8, 0.1],\n-                                                   [0.2, 0.3, 0.5]],\n-                                                   dtype=float32)\n-    >>> categorical_crossentropy(target, output)\n-    array([0.10536054 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return CategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n@@ -1190,46 +1078,6 @@ class SparseCategoricalCrossentropy(Operation):\n     ]\n )\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n-    \"\"\"Computes sparse categorical cross-entropy loss.\n-\n-    The sparse categorical cross-entropy loss is similar to categorical\n-    cross-entropy, but it is used when the target tensor contains integer\n-    class labels instead of one-hot encoded vectors. It measures the\n-    dissimilarity between the target and output probabilities or logits.\n-\n-    Args:\n-        target: The target tensor representing the true class labels as integers.\n-            Its shape should match the shape of the `output` tensor except\n-            for the last dimension.\n-        output: The output tensor representing the predicted probabilities\n-            or logits.\n-            Its shape should match the shape of the `target` tensor except \n-            for the last dimension.\n-        from_logits: (optional) Whether `output` is a tensor of logits\n-            or probabilities.\n-            Set it to `True` if `output` represents logits; otherwise,\n-            set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n-        axis: (optional) The axis along which the sparse categorical\n-            cross-entropy is computed.\n-            Default is -1, which corresponds to the last dimension\n-            of the tensors.\n-\n-    Returns:\n-        Integer tensor: The computed sparse categorical cross-entropy\n-        loss between `target` and `output`.\n-        \n-    Example:\n-    ```python\n-    >>> target = keras_core.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n-    >>> output = keras_core.ops.convert_to_tensor([[0.9, 0.05, 0.05], \n-                                                   [0.1, 0.8, 0.1], \n-                                                   [0.2, 0.3, 0.5]],\n-                                                   dtype=float32)\n-    >>> sparse_categorical_crossentropy(target, output)\n-    array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)\n-    ```\n-    \"\"\"\n     if any_symbolic_tensors((target, output)):\n         return SparseCategoricalCrossentropy(\n             from_logits=from_logits, axis=axis\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
