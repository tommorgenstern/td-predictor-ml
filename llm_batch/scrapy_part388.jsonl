{"custom_id": "scrapy#f65e64a7243d725d35bbf86ca6f5ae4c350dbcc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 35 | Files Changed: 12 | Hunks: 33 | Methods Changed: 38 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 77 | Churn Cumulative: 17053 | Contributors (this commit): 126 | Commits (past 90d): 29 | Contributors (cumulative): 319 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,8 +13,7 @@ from scrapy.utils.misc import load_object, set_environ\n class TextTestResult(_TextTestResult):\n     def printSummary(self, start: float, stop: float) -> None:\n         write = self.stream.write\n-        # _WritelnDecorator isn't implemented in typeshed yet\n-        writeln = self.stream.writeln  # type: ignore[attr-defined]\n+        writeln = self.stream.writeln\n \n         run = self.testsRun\n         plural = \"s\" if run != 1 else \"\"\n@@ -84,7 +83,7 @@ class Command(ScrapyCommand):\n         with set_environ(SCRAPY_CHECK=\"true\"):\n             for spidername in args or spider_loader.list():\n                 spidercls = spider_loader.load(spidername)\n-                spidercls.start_requests = lambda s: conman.from_spider(s, result)\n+                spidercls.start_requests = lambda s: conman.from_spider(s, result)  # type: ignore[assignment,method-assign,return-value]\n \n                 tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                 if opts.list:\n\n@@ -4,7 +4,7 @@ import shutil\n import string\n from importlib import import_module\n from pathlib import Path\n-from typing import Optional, Union, cast\n+from typing import Any, Optional, Union, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -122,7 +122,7 @@ class Command(ScrapyCommand):\n         name: str,\n         url: str,\n         template_name: str,\n-    ):\n+    ) -> dict[str, Any]:\n         capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n         return {\n             \"project_name\": self.settings.get(\"BOT_NAME\"),\n\n@@ -38,9 +38,10 @@ _T = TypeVar(\"_T\")\n class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n-    spider = None\n+    spider: Optional[Spider] = None\n     items: dict[int, list[Any]] = {}\n     requests: dict[int, list[Request]] = {}\n+    spidercls: Optional[type[Spider]]\n \n     first_response = None\n \n@@ -261,10 +262,11 @@ class Command(BaseRunSpiderCommand):\n             yield self.prepare_request(spider, Request(url), opts)\n \n         if self.spidercls:\n-            self.spidercls.start_requests = _start_requests\n+            self.spidercls.start_requests = _start_requests  # type: ignore[assignment,method-assign]\n \n     def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n         assert self.crawler_process\n+        assert self.spidercls\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n         self.pcrawler = list(self.crawler_process.crawlers)[0]\n         self.crawler_process.start()\n\n@@ -100,7 +100,7 @@ class ExecutionEngine:\n         )\n         downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader: Downloader = downloader_cls(crawler)\n-        self.scraper = Scraper(crawler)\n+        self.scraper: Scraper = Scraper(crawler)\n         self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n             spider_closed_callback\n         )\n@@ -325,7 +325,7 @@ class ExecutionEngine:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n         d: Deferred[Union[Response, Request]] = self._download(request)\n         # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n-        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[arg-type]\n+        d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[call-overload]\n         return d2\n \n     def _downloaded(\n\n@@ -55,7 +55,7 @@ class Slot:\n     MIN_RESPONSE_SIZE = 1024\n \n     def __init__(self, max_active_size: int = 5000000):\n-        self.max_active_size = max_active_size\n+        self.max_active_size: int = max_active_size\n         self.queue: deque[QueueTuple] = deque()\n         self.active: set[Request] = set()\n         self.active_size: int = 0\n@@ -316,7 +316,9 @@ class Scraper:\n             )\n         return None\n \n-    def start_itemproc(self, item, *, response: Optional[Response]) -> Deferred[Any]:\n+    def start_itemproc(\n+        self, item: Any, *, response: Optional[Response]\n+    ) -> Deferred[Any]:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n\n@@ -72,7 +72,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     def _process_spider_input(\n         self,\n-        scrape_func: ScrapeFunc,\n+        scrape_func: ScrapeFunc[_T],\n         response: Response,\n         request: Request,\n         spider: Spider,\n@@ -306,7 +306,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     def scrape_response(\n         self,\n-        scrape_func: ScrapeFunc,\n+        scrape_func: ScrapeFunc[_T],\n         response: Response,\n         request: Request,\n         spider: Spider,\n\n@@ -42,8 +42,9 @@ from scrapy.utils.reactor import (\n )\n \n if TYPE_CHECKING:\n-    from collections.abc import Generator\n+    from collections.abc import Generator, Iterable\n \n+    from scrapy.spiderloader import SpiderLoader\n     from scrapy.utils.request import RequestFingerprinter\n \n \n@@ -178,16 +179,18 @@ class Crawler:\n             yield maybeDeferred(self.engine.stop)\n \n     @staticmethod\n-    def _get_component(component_class, components):\n+    def _get_component(\n+        component_class: type[_T], components: Iterable[Any]\n+    ) -> Optional[_T]:\n         for component in components:\n             if isinstance(component, component_class):\n                 return component\n         return None\n \n-    def get_addon(self, cls):\n+    def get_addon(self, cls: type[_T]) -> Optional[_T]:\n         return self._get_component(cls, self.addons.addons)\n \n-    def get_downloader_middleware(self, cls):\n+    def get_downloader_middleware(self, cls: type[_T]) -> Optional[_T]:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_downloader_middleware() can only be called after \"\n@@ -195,7 +198,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n \n-    def get_extension(self, cls):\n+    def get_extension(self, cls: type[_T]) -> Optional[_T]:\n         if not self.extensions:\n             raise RuntimeError(\n                 \"Crawler.get_extension() can only be called after the \"\n@@ -203,7 +206,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.extensions.middlewares)\n \n-    def get_item_pipeline(self, cls):\n+    def get_item_pipeline(self, cls: type[_T]) -> Optional[_T]:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_item_pipeline() can only be called after the \"\n@@ -211,7 +214,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n \n-    def get_spider_middleware(self, cls):\n+    def get_spider_middleware(self, cls: type[_T]) -> Optional[_T]:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_spider_middleware() can only be called after the \"\n@@ -240,18 +243,18 @@ class CrawlerRunner:\n     )\n \n     @staticmethod\n-    def _get_spider_loader(settings: BaseSettings):\n+    def _get_spider_loader(settings: BaseSettings) -> SpiderLoader:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n         cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n         loader_cls = load_object(cls_path)\n         verifyClass(ISpiderLoader, loader_cls)\n-        return loader_cls.from_settings(settings.frozencopy())\n+        return cast(\"SpiderLoader\", loader_cls.from_settings(settings.frozencopy()))\n \n     def __init__(self, settings: Union[dict[str, Any], Settings, None] = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n-        self.settings = settings\n-        self.spider_loader = self._get_spider_loader(settings)\n+        self.settings: Settings = settings\n+        self.spider_loader: SpiderLoader = self._get_spider_loader(settings)\n         self._crawlers: set[Crawler] = set()\n         self._active: set[Deferred[None]] = set()\n         self.bootstrap_failed = False\n@@ -329,8 +332,7 @@ class CrawlerRunner:\n     def _create_crawler(self, spidercls: Union[str, type[Spider]]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        # temporary cast until self.spider_loader is typed\n-        return Crawler(cast(type[Spider], spidercls), self.settings)\n+        return Crawler(spidercls, self.settings)\n \n     def stop(self) -> Deferred[Any]:\n         \"\"\"\n@@ -384,7 +386,7 @@ class CrawlerProcess(CrawlerRunner):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n-        self._initialized_reactor = False\n+        self._initialized_reactor: bool = False\n \n     def _signal_shutdown(self, signum: int, _: Any) -> None:\n         from twisted.internet import reactor\n@@ -413,9 +415,7 @@ class CrawlerProcess(CrawlerRunner):\n         init_reactor = not self._initialized_reactor\n         self._initialized_reactor = True\n         # temporary cast until self.spider_loader is typed\n-        return Crawler(\n-            cast(type[Spider], spidercls), self.settings, init_reactor=init_reactor\n-        )\n+        return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n     def start(\n         self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n\n@@ -88,7 +88,7 @@ class HttpCompressionMiddleware:\n             crawler.signals.connect(mw.open_spider, signals.spider_opened)\n             return mw\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider) -> None:\n         if hasattr(spider, \"download_maxsize\"):\n             self._max_size = spider.download_maxsize\n         if hasattr(spider, \"download_warnsize\"):\n\n@@ -67,7 +67,7 @@ class RobotsTxtMiddleware:\n         if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n             return None\n         d: Deferred[Optional[RobotParser]] = maybeDeferred(\n-            self.robot_parser, request, spider  # type: ignore[arg-type]\n+            self.robot_parser, request, spider  # type: ignore[call-overload]\n         )\n         d2: Deferred[None] = d.addCallback(self.process_request_2, request, spider)\n         return d2\n\n@@ -578,7 +578,7 @@ class FeedExporter:\n             return None\n \n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d: Deferred[None] = maybeDeferred(slot.storage.store, get_file(slot))  # type: ignore[arg-type]\n+        d: Deferred[None] = maybeDeferred(slot.storage.store, get_file(slot))  # type: ignore[call-overload]\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n\n@@ -550,7 +550,7 @@ class FilesPipeline(MediaPipeline):\n \n         path = self.file_path(request, info=info, item=item)\n         # maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n-        dfd: Deferred[StatInfo] = maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n+        dfd: Deferred[StatInfo] = maybeDeferred(self.store.stat_file, path, info)  # type: ignore[call-overload]\n         dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n         dfd2.addErrback(lambda _: None)\n         dfd2.addErrback(\n\n@@ -305,7 +305,11 @@ def process_parallel(\n         dfds, fireOnOneErrback=True, consumeErrors=True\n     )\n     d2: Deferred[list[_T2]] = d.addCallback(lambda r: [x[1] for x in r])\n-    d2.addErrback(lambda f: f.value.subFailure)\n+\n+    def eb(failure: Failure) -> Failure:\n+        return failure.value.subFailure\n+\n+    d2.addErrback(eb)\n     return d2\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9095ef927bc42e8f23c5d02c05a7b918f7aa5bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 942 | Lines Deleted: 976 | Files Changed: 121 | Hunks: 732 | Methods Changed: 647 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1918 | Churn Cumulative: 97898 | Contributors (this commit): 311 | Commits (past 90d): 243 | Contributors (cumulative): 2070 | DMM Complexity: 0.17391304347826086\n\nDIFF:\n@@ -6,7 +6,7 @@ import inspect\n import os\n import sys\n from importlib.metadata import entry_points\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n@@ -30,7 +30,7 @@ if TYPE_CHECKING:\n class ScrapyArgumentParser(argparse.ArgumentParser):\n     def _parse_optional(\n         self, arg_string: str\n-    ) -> Optional[tuple[Optional[argparse.Action], str, Optional[str]]]:\n+    ) -> tuple[argparse.Action | None, str, str | None] | None:\n         # if starts with -: it means that is a parameter not a argument\n         if arg_string[:2] == \"-:\":\n             return None\n@@ -89,7 +89,7 @@ def _get_commands_dict(\n     return cmds\n \n \n-def _pop_command_name(argv: list[str]) -> Optional[str]:\n+def _pop_command_name(argv: list[str]) -> str | None:\n     i = 0\n     for arg in argv[1:]:\n         if not arg.startswith(\"-\"):\n@@ -147,9 +147,7 @@ def _run_print_help(\n         sys.exit(2)\n \n \n-def execute(\n-    argv: Optional[list[str]] = None, settings: Optional[Settings] = None\n-) -> None:\n+def execute(argv: list[str] | None = None, settings: Settings | None = None) -> None:\n     if argv is None:\n         argv = sys.argv\n \n\n@@ -8,7 +8,7 @@ import argparse\n import builtins\n import os\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from twisted.python import failure\n \n@@ -23,7 +23,7 @@ if TYPE_CHECKING:\n \n class ScrapyCommand:\n     requires_project: bool = False\n-    crawler_process: Optional[CrawlerProcess] = None\n+    crawler_process: CrawlerProcess | None = None\n \n     # default settings to be used for this command instead of global defaults\n     default_settings: dict[str, Any] = {}\n@@ -195,7 +195,7 @@ class ScrapyHelpFormatter(argparse.HelpFormatter):\n         prog: str,\n         indent_increment: int = 2,\n         max_help_position: int = 24,\n-        width: Optional[int] = None,\n+        width: int | None = None,\n     ):\n         super().__init__(\n             prog,\n\n@@ -1,10 +1,12 @@\n+from __future__ import annotations\n+\n import argparse\n import os\n import shutil\n import string\n from importlib import import_module\n from pathlib import Path\n-from typing import Any, Optional, Union, cast\n+from typing import Any, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -140,7 +142,7 @@ class Command(ScrapyCommand):\n         name: str,\n         url: str,\n         template_name: str,\n-        template_file: Union[str, os.PathLike],\n+        template_file: str | os.PathLike,\n     ) -> None:\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         tvars = self._generate_template_variables(module, name, url, template_name)\n@@ -161,7 +163,7 @@ class Command(ScrapyCommand):\n         if spiders_module:\n             print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n \n-    def _find_template(self, template: str) -> Optional[Path]:\n+    def _find_template(self, template: str) -> Path | None:\n         template_file = Path(self.templates_dir, f\"{template}.tmpl\")\n         if template_file.exists():\n             return template_file\n\n@@ -5,7 +5,7 @@ import functools\n import inspect\n import json\n import logging\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, overload\n+from typing import TYPE_CHECKING, Any, TypeVar, overload\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet.defer import Deferred, maybeDeferred\n@@ -38,10 +38,10 @@ _T = TypeVar(\"_T\")\n class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n-    spider: Optional[Spider] = None\n+    spider: Spider | None = None\n     items: dict[int, list[Any]] = {}\n     requests: dict[int, list[Request]] = {}\n-    spidercls: Optional[type[Spider]]\n+    spidercls: type[Spider] | None\n \n     first_response = None\n \n@@ -137,13 +137,13 @@ class Command(BaseRunSpiderCommand):\n \n     @overload\n     def iterate_spider_output(\n-        self, result: Union[AsyncGenerator[_T, None], Coroutine[Any, Any, _T]]\n+        self, result: AsyncGenerator[_T] | Coroutine[Any, Any, _T]\n     ) -> Deferred[_T]: ...\n \n     @overload\n     def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n \n-    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred[Any]]:\n+    def iterate_spider_output(self, result: Any) -> Iterable[Any] | Deferred[Any]:\n         if inspect.isasyncgen(result):\n             d = deferred_from_coro(\n                 collect_asyncgen(aiter_errback(result, self.handle_exception))\n@@ -164,7 +164,7 @@ class Command(BaseRunSpiderCommand):\n         old_reqs = self.requests.get(lvl, [])\n         self.requests[lvl] = old_reqs + new_reqs\n \n-    def print_items(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n+    def print_items(self, lvl: int | None = None, colour: bool = True) -> None:\n         if lvl is None:\n             items = [item for lst in self.items.values() for item in lst]\n         else:\n@@ -173,7 +173,7 @@ class Command(BaseRunSpiderCommand):\n         print(\"# Scraped Items \", \"-\" * 60)\n         display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n \n-    def print_requests(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n+    def print_requests(self, lvl: int | None = None, colour: bool = True) -> None:\n         if lvl is None:\n             if self.requests:\n                 requests = self.requests[max(self.requests)]\n@@ -222,7 +222,7 @@ class Command(BaseRunSpiderCommand):\n         self,\n         response: Response,\n         callback: CallbackT,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n     ) -> Deferred[Any]:\n         cb_kwargs = cb_kwargs or {}\n         d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n@@ -230,7 +230,7 @@ class Command(BaseRunSpiderCommand):\n \n     def get_callback_from_rules(\n         self, spider: Spider, response: Response\n-    ) -> Union[CallbackT, str, None]:\n+    ) -> CallbackT | str | None:\n         if getattr(spider, \"rules\", None):\n             for rule in spider.rules:  # type: ignore[attr-defined]\n                 if rule.link_extractor.matches(response.url):\n@@ -303,9 +303,9 @@ class Command(BaseRunSpiderCommand):\n         *,\n         spider: Spider,\n         opts: argparse.Namespace,\n-        response: Optional[Response] = None,\n+        response: Response | None = None,\n     ) -> CallbackT:\n-        cb: Union[str, CallbackT, None] = None\n+        cb: str | CallbackT | None = None\n         if response:\n             cb = response.meta[\"_callback\"]\n         if not cb:\n\n@@ -4,7 +4,7 @@ import argparse\n import sys\n from importlib import import_module\n from pathlib import Path\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n@@ -15,7 +15,7 @@ if TYPE_CHECKING:\n     from types import ModuleType\n \n \n-def _import_file(filepath: Union[str, PathLike[str]]) -> ModuleType:\n+def _import_file(filepath: str | PathLike[str]) -> ModuleType:\n     abspath = Path(filepath).resolve()\n     if abspath.suffix not in (\".py\", \".pyw\"):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import argparse\n import os\n import re\n@@ -6,7 +8,6 @@ from importlib.util import find_spec\n from pathlib import Path\n from shutil import copy2, copystat, ignore_patterns, move\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n-from typing import Union\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -24,7 +25,7 @@ TEMPLATES_TO_RENDER: tuple[tuple[str, ...], ...] = (\n IGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n \n \n-def _make_writable(path: Union[str, os.PathLike]) -> None:\n+def _make_writable(path: str | os.PathLike) -> None:\n     current_permissions = os.stat(path).st_mode\n     os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n \n\n@@ -6,7 +6,7 @@ from collections.abc import AsyncGenerator, Iterable\n from functools import wraps\n from inspect import getmembers\n from types import CoroutineType\n-from typing import TYPE_CHECKING, Any, Optional, cast\n+from typing import TYPE_CHECKING, Any, cast\n from unittest import TestCase, TestResult\n \n from scrapy.http import Request, Response\n@@ -24,7 +24,7 @@ if TYPE_CHECKING:\n class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n \n-    request_cls: Optional[type[Request]] = None\n+    request_cls: type[Request] | None = None\n     name: str\n \n     def __init__(self, method: Callable, *args: Any):\n@@ -126,10 +126,8 @@ class ContractsManager:\n \n         return contracts\n \n-    def from_spider(\n-        self, spider: Spider, results: TestResult\n-    ) -> list[Optional[Request]]:\n-        requests: list[Optional[Request]] = []\n+    def from_spider(self, spider: Spider, results: TestResult) -> list[Request | None]:\n+        requests: list[Request | None] = []\n         for method in self.tested_methods_from_spidercls(type(spider)):\n             bound_method = spider.__getattribute__(method)\n             try:\n@@ -140,7 +138,7 @@ class ContractsManager:\n \n         return requests\n \n-    def from_method(self, method: Callable, results: TestResult) -> Optional[Request]:\n+    def from_method(self, method: Callable, results: TestResult) -> Request | None:\n         contracts = self.extract_contracts(method)\n         if contracts:\n             request_cls = Request\n\n@@ -1,5 +1,7 @@\n+from __future__ import annotations\n+\n import json\n-from typing import Any, Callable, Optional\n+from typing import Any, Callable\n \n from itemadapter import ItemAdapter, is_item\n \n@@ -63,7 +65,7 @@ class ReturnsContract(Contract):\n     \"\"\"\n \n     name = \"returns\"\n-    object_type_verifiers: dict[Optional[str], Callable[[Any], bool]] = {\n+    object_type_verifiers: dict[str | None, Callable[[Any], bool]] = {\n         \"request\": lambda x: isinstance(x, Request),\n         \"requests\": lambda x: isinstance(x, Request),\n         \"item\": is_item,\n\n@@ -5,7 +5,7 @@ import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from twisted.internet import task\n from twisted.internet.defer import Deferred\n@@ -37,7 +37,7 @@ class Slot:\n         delay: float,\n         randomize_delay: bool,\n         *,\n-        throttle: Optional[bool] = None,\n+        throttle: bool | None = None,\n     ):\n         self.concurrency: int = concurrency\n         self.delay: float = delay\n@@ -119,15 +119,13 @@ class Downloader:\n             \"DOWNLOAD_SLOTS\", {}\n         )\n \n-    def fetch(\n-        self, request: Request, spider: Spider\n-    ) -> Deferred[Union[Response, Request]]:\n+    def fetch(self, request: Request, spider: Spider) -> Deferred[Response | Request]:\n         def _deactivate(response: _T) -> _T:\n             self.active.remove(request)\n             return response\n \n         self.active.add(request)\n-        dfd: Deferred[Union[Response, Request]] = self.middleware.download(\n+        dfd: Deferred[Response | Request] = self.middleware.download(\n             self._enqueue_request, request, spider\n         )\n         return dfd.addBoth(_deactivate)\n@@ -164,7 +162,7 @@ class Downloader:\n \n         return key\n \n-    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+    def _get_slot_key(self, request: Request, spider: Spider | None) -> str:\n         warnings.warn(\n             \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n             ScrapyDeprecationWarning,\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import warnings\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from OpenSSL import SSL\n from twisted.internet._sslverify import _setAcceptableProtocols\n@@ -49,7 +49,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         self,\n         method: int = SSL.SSLv23_METHOD,\n         tls_verbose_logging: bool = False,\n-        tls_ciphers: Optional[str] = None,\n+        tls_ciphers: str | None = None,\n         *args: Any,\n         **kwargs: Any,\n     ):\n@@ -73,7 +73,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n         )\n-        tls_ciphers: Optional[str] = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n+        tls_ciphers: str | None = settings[\"DOWNLOADER_CLIENT_TLS_CIPHERS\"]\n         return cls(  # type: ignore[misc]\n             method=method,\n             tls_verbose_logging=tls_verbose_logging,\n\n@@ -4,7 +4,7 @@ from __future__ import annotations\n \n import logging\n from collections.abc import Callable\n-from typing import TYPE_CHECKING, Any, Optional, Protocol, Union, cast\n+from typing import TYPE_CHECKING, Any, Protocol, cast\n \n from twisted.internet import defer\n \n@@ -35,16 +35,16 @@ class DownloadHandlerProtocol(Protocol):\n class DownloadHandlers:\n     def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n-        self._schemes: dict[str, Union[str, Callable[..., Any]]] = (\n+        self._schemes: dict[str, str | Callable[..., Any]] = (\n             {}\n         )  # stores acceptable schemes on instancing\n         self._handlers: dict[str, DownloadHandlerProtocol] = (\n             {}\n         )  # stores instanced handlers for schemes\n         self._notconfigured: dict[str, str] = {}  # remembers failed handlers\n-        handlers: dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n+        handlers: dict[str, str | Callable[..., Any]] = without_none_values(\n             cast(\n-                dict[str, Union[str, Callable[..., Any]]],\n+                \"dict[str, str | Callable[..., Any]]\",\n                 crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n             )\n         )\n@@ -54,7 +54,7 @@ class DownloadHandlers:\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n-    def _get_handler(self, scheme: str) -> Optional[DownloadHandlerProtocol]:\n+    def _get_handler(self, scheme: str) -> DownloadHandlerProtocol | None:\n         \"\"\"Lazy-load the downloadhandler for a scheme\n         only on the first request for that scheme.\n         \"\"\"\n@@ -70,7 +70,7 @@ class DownloadHandlers:\n \n     def _load_handler(\n         self, scheme: str, skip_lazy: bool = False\n-    ) -> Optional[DownloadHandlerProtocol]:\n+    ) -> DownloadHandlerProtocol | None:\n         path = self._schemes[scheme]\n         try:\n             dhcls: type[DownloadHandlerProtocol] = load_object(path)\n\n@@ -32,7 +32,7 @@ from __future__ import annotations\n \n import re\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, BinaryIO, Optional\n+from typing import TYPE_CHECKING, Any, BinaryIO\n from urllib.parse import unquote\n \n from twisted.internet.protocol import ClientCreator, Protocol\n@@ -56,8 +56,8 @@ if TYPE_CHECKING:\n \n \n class ReceivedDataProtocol(Protocol):\n-    def __init__(self, filename: Optional[str] = None):\n-        self.__filename: Optional[str] = filename\n+    def __init__(self, filename: str | None = None):\n+        self.__filename: str | None = filename\n         self.body: BinaryIO = open(filename, \"wb\") if filename else BytesIO()\n         self.size: int = 0\n \n@@ -66,7 +66,7 @@ class ReceivedDataProtocol(Protocol):\n         self.size += len(data)\n \n     @property\n-    def filename(self) -> Optional[str]:\n+    def filename(self) -> str | None:\n         return self.__filename\n \n     def close(self) -> None:\n\n@@ -8,7 +8,7 @@ import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from typing import TYPE_CHECKING, Any, Optional, TypedDict, TypeVar, Union\n+from typing import TYPE_CHECKING, Any, TypedDict, TypeVar\n from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import ssl\n@@ -52,10 +52,10 @@ _T = TypeVar(\"_T\")\n class _ResultT(TypedDict):\n     txresponse: TxResponse\n     body: bytes\n-    flags: Optional[list[str]]\n-    certificate: Optional[ssl.Certificate]\n-    ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None]\n-    failure: NotRequired[Optional[Failure]]\n+    flags: list[str] | None\n+    certificate: ssl.Certificate | None\n+    ip_address: ipaddress.IPv4Address | ipaddress.IPv6Address | None\n+    failure: NotRequired[Failure | None]\n \n \n class HTTP11DownloadHandler:\n@@ -143,10 +143,10 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         reactor: ReactorBase,\n         host: str,\n         port: int,\n-        proxyConf: tuple[str, int, Optional[bytes]],\n+        proxyConf: tuple[str, int, bytes | None],\n         contextFactory: IPolicyForHTTPS,\n         timeout: float = 30,\n-        bindAddress: Optional[tuple[str, int]] = None,\n+        bindAddress: tuple[str, int] | None = None,\n     ):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n         super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n@@ -220,7 +220,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n \n def tunnel_request_data(\n-    host: str, port: int, proxy_auth_header: Optional[bytes] = None\n+    host: str, port: int, proxy_auth_header: bytes | None = None\n ) -> bytes:\n     r\"\"\"\n     Return binary content of a CONNECT request.\n@@ -254,14 +254,14 @@ class TunnelingAgent(Agent):\n         self,\n         *,\n         reactor: ReactorBase,\n-        proxyConf: tuple[str, int, Optional[bytes]],\n+        proxyConf: tuple[str, int, bytes | None],\n         contextFactory: IPolicyForHTTPS,\n-        connectTimeout: Optional[float] = None,\n-        bindAddress: Optional[bytes] = None,\n-        pool: Optional[HTTPConnectionPool] = None,\n+        connectTimeout: float | None = None,\n+        bindAddress: bytes | None = None,\n+        pool: HTTPConnectionPool | None = None,\n     ):\n         super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n-        self._proxyConf: tuple[str, int, Optional[bytes]] = proxyConf\n+        self._proxyConf: tuple[str, int, bytes | None] = proxyConf\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n \n     def _getEndpoint(self, uri: URI) -> TunnelingTCP4ClientEndpoint:\n@@ -281,8 +281,8 @@ class TunnelingAgent(Agent):\n         endpoint: TCP4ClientEndpoint,\n         method: bytes,\n         parsedURI: bytes,\n-        headers: Optional[TxHeaders],\n-        bodyProducer: Optional[IBodyProducer],\n+        headers: TxHeaders | None,\n+        bodyProducer: IBodyProducer | None,\n         requestPath: bytes,\n     ) -> Deferred[TxResponse]:\n         # proxy host and port are required for HTTP pool `key`\n@@ -305,9 +305,9 @@ class ScrapyProxyAgent(Agent):\n         self,\n         reactor: ReactorBase,\n         proxyURI: bytes,\n-        connectTimeout: Optional[float] = None,\n-        bindAddress: Optional[bytes] = None,\n-        pool: Optional[HTTPConnectionPool] = None,\n+        connectTimeout: float | None = None,\n+        bindAddress: bytes | None = None,\n+        pool: HTTPConnectionPool | None = None,\n     ):\n         super().__init__(\n             reactor=reactor,\n@@ -321,8 +321,8 @@ class ScrapyProxyAgent(Agent):\n         self,\n         method: bytes,\n         uri: bytes,\n-        headers: Optional[TxHeaders] = None,\n-        bodyProducer: Optional[IBodyProducer] = None,\n+        headers: TxHeaders | None = None,\n+        bodyProducer: IBodyProducer | None = None,\n     ) -> Deferred[TxResponse]:\n         \"\"\"\n         Issue a new request via the configured proxy.\n@@ -350,8 +350,8 @@ class ScrapyAgent:\n         *,\n         contextFactory: IPolicyForHTTPS,\n         connectTimeout: float = 10,\n-        bindAddress: Optional[bytes] = None,\n-        pool: Optional[HTTPConnectionPool] = None,\n+        bindAddress: bytes | None = None,\n+        pool: HTTPConnectionPool | None = None,\n         maxsize: int = 0,\n         warnsize: int = 0,\n         fail_on_dataloss: bool = True,\n@@ -359,12 +359,12 @@ class ScrapyAgent:\n     ):\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n         self._connectTimeout: float = connectTimeout\n-        self._bindAddress: Optional[bytes] = bindAddress\n-        self._pool: Optional[HTTPConnectionPool] = pool\n+        self._bindAddress: bytes | None = bindAddress\n+        self._pool: HTTPConnectionPool | None = pool\n         self._maxsize: int = maxsize\n         self._warnsize: int = warnsize\n         self._fail_on_dataloss: bool = fail_on_dataloss\n-        self._txresponse: Optional[TxResponse] = None\n+        self._txresponse: TxResponse | None = None\n         self._crawler: Crawler = crawler\n \n     def _get_agent(self, request: Request, timeout: float) -> Agent:\n@@ -462,7 +462,7 @@ class ScrapyAgent:\n \n     def _cb_bodyready(\n         self, txresponse: TxResponse, request: Request\n-    ) -> Union[_ResultT, Deferred[_ResultT]]:\n+    ) -> _ResultT | Deferred[_ResultT]:\n         headers_received_result = self._crawler.signals.send_catch_log(\n             signal=signals.headers_received,\n             headers=self._headers_from_twisted_response(txresponse),\n@@ -551,7 +551,7 @@ class ScrapyAgent:\n \n     def _cb_bodydone(\n         self, result: _ResultT, request: Request, url: str\n-    ) -> Union[Response, Failure]:\n+    ) -> Response | Failure:\n         headers = self._headers_from_twisted_response(result[\"txresponse\"])\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n         try:\n@@ -614,14 +614,12 @@ class _ResponseReader(Protocol):\n         self._fail_on_dataloss_warned: bool = False\n         self._reached_warnsize: bool = False\n         self._bytes_received: int = 0\n-        self._certificate: Optional[ssl.Certificate] = None\n-        self._ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None] = (\n-            None\n-        )\n+        self._certificate: ssl.Certificate | None = None\n+        self._ip_address: ipaddress.IPv4Address | ipaddress.IPv6Address | None = None\n         self._crawler: Crawler = crawler\n \n     def _finish_response(\n-        self, flags: Optional[list[str]] = None, failure: Optional[Failure] = None\n+        self, flags: list[str] | None = None, failure: Failure | None = None\n     ) -> None:\n         self._finished.callback(\n             {\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from time import time\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n from urllib.parse import urldefrag\n \n from twisted.internet.error import TimeoutError\n@@ -60,8 +60,8 @@ class ScrapyH2Agent:\n         context_factory: IPolicyForHTTPS,\n         pool: H2ConnectionPool,\n         connect_timeout: int = 10,\n-        bind_address: Optional[bytes] = None,\n-        crawler: Optional[Crawler] = None,\n+        bind_address: bytes | None = None,\n+        crawler: Crawler | None = None,\n     ) -> None:\n         self._context_factory = context_factory\n         self._connect_timeout = connect_timeout\n@@ -69,7 +69,7 @@ class ScrapyH2Agent:\n         self._pool = pool\n         self._crawler = crawler\n \n-    def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:\n+    def _get_agent(self, request: Request, timeout: float | None) -> H2Agent:\n         from twisted.internet import reactor\n \n         bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n@@ -26,9 +26,9 @@ class S3DownloadHandler:\n         settings: BaseSettings,\n         *,\n         crawler: Crawler,\n-        aws_access_key_id: Optional[str] = None,\n-        aws_secret_access_key: Optional[str] = None,\n-        aws_session_token: Optional[str] = None,\n+        aws_access_key_id: str | None = None,\n+        aws_secret_access_key: str | None = None,\n+        aws_session_token: str | None = None,\n         httpdownloadhandler: type[HTTPDownloadHandler] = HTTPDownloadHandler,\n         **kw: Any,\n     ):\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/downloader-middleware.rst\n from __future__ import annotations\n \n from collections.abc import Callable\n-from typing import TYPE_CHECKING, Any, Union, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n \n@@ -46,11 +46,11 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         download_func: Callable[[Request, Spider], Deferred[Response]],\n         request: Request,\n         spider: Spider,\n-    ) -> Deferred[Union[Response, Request]]:\n+    ) -> Deferred[Response | Request]:\n         @inlineCallbacks\n         def process_request(\n             request: Request,\n-        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n+        ) -> Generator[Deferred[Any], Any, Response | Request]:\n             for method in self.methods[\"process_request\"]:\n                 method = cast(Callable, method)\n                 response = yield deferred_from_coro(\n@@ -69,8 +69,8 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n \n         @inlineCallbacks\n         def process_response(\n-            response: Union[Response, Request]\n-        ) -> Generator[Deferred[Any], Any, Union[Response, Request]]:\n+            response: Response | Request,\n+        ) -> Generator[Deferred[Any], Any, Response | Request]:\n             if response is None:\n                 raise TypeError(\"Received None in process_response\")\n             elif isinstance(response, Request):\n@@ -93,7 +93,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         @inlineCallbacks\n         def process_exception(\n             failure: Failure,\n-        ) -> Generator[Deferred[Any], Any, Union[Failure, Response, Request]]:\n+        ) -> Generator[Deferred[Any], Any, Failure | Response | Request]:\n             exception = failure.value\n             for method in self.methods[\"process_exception\"]:\n                 method = cast(Callable, method)\n@@ -111,7 +111,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                     return response\n             return failure\n \n-        deferred: Deferred[Union[Response, Request]] = mustbe_deferred(\n+        deferred: Deferred[Response | Request] = mustbe_deferred(\n             process_request, request\n         )\n         deferred.addErrback(process_exception)\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import re\n from time import time\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n@@ -144,9 +144,9 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         # converting to bytes to comply to Twisted interface\n         self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n         self.method: bytes = to_bytes(request.method, encoding=\"ascii\")\n-        self.body: Optional[bytes] = request.body or None\n+        self.body: bytes | None = request.body or None\n         self.headers: Headers = Headers(request.headers)\n-        self.response_headers: Optional[Headers] = None\n+        self.response_headers: Headers | None = None\n         self.timeout: float = request.meta.get(\"download_timeout\") or timeout\n         self.start_time: float = time()\n         self.deferred: defer.Deferred[Response] = defer.Deferred().addCallback(\n\n@@ -9,7 +9,7 @@ from __future__ import annotations\n \n import logging\n from time import time\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n@@ -18,7 +18,7 @@ from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.downloader import Downloader\n-from scrapy.core.scraper import Scraper\n+from scrapy.core.scraper import Scraper, _HandleOutputDeferred\n from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.logformatter import LogFormatter\n@@ -32,7 +32,6 @@ if TYPE_CHECKING:\n     from collections.abc import Callable, Generator, Iterable, Iterator\n \n     from scrapy.core.scheduler import BaseScheduler\n-    from scrapy.core.scraper import _HandleOutputDeferred\n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n     from scrapy.spiders import Spider\n@@ -51,9 +50,9 @@ class Slot:\n         nextcall: CallLaterOnce[None],\n         scheduler: BaseScheduler,\n     ) -> None:\n-        self.closing: Optional[Deferred[None]] = None\n+        self.closing: Deferred[None] | None = None\n         self.inprogress: set[Request] = set()\n-        self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n+        self.start_requests: Iterator[Request] | None = iter(start_requests)\n         self.close_if_idle: bool = close_if_idle\n         self.nextcall: CallLaterOnce[None] = nextcall\n         self.scheduler: BaseScheduler = scheduler\n@@ -84,15 +83,15 @@ class ExecutionEngine:\n     def __init__(\n         self,\n         crawler: Crawler,\n-        spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]],\n+        spider_closed_callback: Callable[[Spider], Deferred[None] | None],\n     ) -> None:\n         self.crawler: Crawler = crawler\n         self.settings: Settings = crawler.settings\n         self.signals: SignalManager = crawler.signals\n         assert crawler.logformatter\n         self.logformatter: LogFormatter = crawler.logformatter\n-        self.slot: Optional[Slot] = None\n-        self.spider: Optional[Spider] = None\n+        self.slot: Slot | None = None\n+        self.spider: Spider | None = None\n         self.running: bool = False\n         self.paused: bool = False\n         self.scheduler_cls: type[BaseScheduler] = self._get_scheduler_class(\n@@ -101,10 +100,10 @@ class ExecutionEngine:\n         downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader: Downloader = downloader_cls(crawler)\n         self.scraper: Scraper = Scraper(crawler)\n-        self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n+        self._spider_closed_callback: Callable[[Spider], Deferred[None] | None] = (\n             spider_closed_callback\n         )\n-        self.start_time: Optional[float] = None\n+        self.start_time: float | None = None\n \n     def _get_scheduler_class(self, settings: BaseSettings) -> type[BaseScheduler]:\n         from scrapy.core.scheduler import BaseScheduler\n@@ -218,7 +217,7 @@ class ExecutionEngine:\n             or self.scraper.slot.needs_backout()\n         )\n \n-    def _next_request_from_scheduler(self) -> Optional[Deferred[None]]:\n+    def _next_request_from_scheduler(self) -> Deferred[None] | None:\n         assert self.slot is not None  # typing\n         assert self.spider is not None  # typing\n \n@@ -226,7 +225,7 @@ class ExecutionEngine:\n         if request is None:\n             return None\n \n-        d: Deferred[Union[Response, Request]] = self._download(request)\n+        d: Deferred[Response | Request] = self._download(request)\n         d.addBoth(self._handle_downloader_output, request)\n         d.addErrback(\n             lambda f: logger.info(\n@@ -260,8 +259,8 @@ class ExecutionEngine:\n         return d2\n \n     def _handle_downloader_output(\n-        self, result: Union[Request, Response, Failure], request: Request\n-    ) -> Optional[_HandleOutputDeferred]:\n+        self, result: Request | Response | Failure, request: Request\n+    ) -> _HandleOutputDeferred | None:\n         assert self.spider is not None  # typing\n \n         if not isinstance(result, (Request, Response, Failure)):\n@@ -323,24 +322,24 @@ class ExecutionEngine:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        d: Deferred[Union[Response, Request]] = self._download(request)\n+        d: Deferred[Response | Request] = self._download(request)\n         # Deferred.addBoth() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n         d2: Deferred[Response] = d.addBoth(self._downloaded, request)  # type: ignore[call-overload]\n         return d2\n \n     def _downloaded(\n-        self, result: Union[Response, Request, Failure], request: Request\n-    ) -> Union[Deferred[Response], Response, Failure]:\n+        self, result: Response | Request | Failure, request: Request\n+    ) -> Deferred[Response] | Response | Failure:\n         assert self.slot is not None  # typing\n         self.slot.remove_request(request)\n         return self.download(result) if isinstance(result, Request) else result\n \n-    def _download(self, request: Request) -> Deferred[Union[Response, Request]]:\n+    def _download(self, request: Request) -> Deferred[Response | Request]:\n         assert self.slot is not None  # typing\n \n         self.slot.add_request(request)\n \n-        def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n+        def _on_success(result: Response | Request) -> Response | Request:\n             if not isinstance(result, (Response, Request)):\n                 raise TypeError(\n                     f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\"\n@@ -368,9 +367,7 @@ class ExecutionEngine:\n             return _\n \n         assert self.spider is not None\n-        dwld: Deferred[Union[Response, Request]] = self.downloader.fetch(\n-            request, self.spider\n-        )\n+        dwld: Deferred[Response | Request] = self.downloader.fetch(request, self.spider)\n         dwld.addCallback(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from collections import deque\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n@@ -121,8 +121,8 @@ class H2Agent:\n         reactor: ReactorBase,\n         pool: H2ConnectionPool,\n         context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n-        connect_timeout: Optional[float] = None,\n-        bind_address: Optional[bytes] = None,\n+        connect_timeout: float | None = None,\n+        bind_address: bytes | None = None,\n     ) -> None:\n         self._reactor = reactor\n         self._pool = pool\n@@ -165,8 +165,8 @@ class ScrapyProxyH2Agent(H2Agent):\n         proxy_uri: URI,\n         pool: H2ConnectionPool,\n         context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n-        connect_timeout: Optional[float] = None,\n-        bind_address: Optional[bytes] = None,\n+        connect_timeout: float | None = None,\n+        bind_address: bytes | None = None,\n     ) -> None:\n         super().__init__(\n             reactor=reactor,\n\n@@ -4,7 +4,7 @@ import ipaddress\n import itertools\n import logging\n from collections import deque\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -63,7 +63,7 @@ class InvalidNegotiatedProtocol(H2Error):\n class RemoteTerminatedConnection(H2Error):\n     def __init__(\n         self,\n-        remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],\n+        remote_ip_address: IPv4Address | IPv6Address | None,\n         event: ConnectionTerminated,\n     ) -> None:\n         self.remote_ip_address = remote_ip_address\n@@ -74,9 +74,7 @@ class RemoteTerminatedConnection(H2Error):\n \n \n class MethodNotAllowed405(H2Error):\n-    def __init__(\n-        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]\n-    ) -> None:\n+    def __init__(self, remote_ip_address: IPv4Address | IPv6Address | None) -> None:\n         self.remote_ip_address = remote_ip_address\n \n     def __str__(self) -> str:\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n from enum import Enum\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -382,7 +382,7 @@ class Stream:\n     def close(\n         self,\n         reason: StreamCloseReason,\n-        errors: Optional[list[BaseException]] = None,\n+        errors: list[BaseException] | None = None,\n         from_protocol: bool = False,\n     ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\"\"\"\n\n@@ -4,7 +4,7 @@ import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n # working around https://github.com/sphinx-doc/sphinx/issues/10400\n from twisted.internet.defer import Deferred  # noqa: TC002\n@@ -73,7 +73,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         \"\"\"\n         return cls()\n \n-    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n+    def open(self, spider: Spider) -> Deferred[None] | None:\n         \"\"\"\n         Called when the spider is opened by the engine. It receives the spider\n         instance as argument and it's useful to execute initialization code.\n@@ -83,7 +83,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         \"\"\"\n         pass\n \n-    def close(self, reason: str) -> Optional[Deferred[None]]:\n+    def close(self, reason: str) -> Deferred[None] | None:\n         \"\"\"\n         Called when the spider is closed by the engine. It receives the reason why the crawl\n         finished as argument and it's useful to execute cleaning code.\n@@ -115,7 +115,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def next_request(self) -> Optional[Request]:\n+    def next_request(self) -> Request | None:\n         \"\"\"\n         Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n         to indicate that there are no requests to be considered ready at the moment.\n@@ -181,22 +181,22 @@ class Scheduler(BaseScheduler):\n     def __init__(\n         self,\n         dupefilter: BaseDupeFilter,\n-        jobdir: Optional[str] = None,\n-        dqclass: Optional[type[BaseQueue]] = None,\n-        mqclass: Optional[type[BaseQueue]] = None,\n+        jobdir: str | None = None,\n+        dqclass: type[BaseQueue] | None = None,\n+        mqclass: type[BaseQueue] | None = None,\n         logunser: bool = False,\n-        stats: Optional[StatsCollector] = None,\n-        pqclass: Optional[type[ScrapyPriorityQueue]] = None,\n-        crawler: Optional[Crawler] = None,\n+        stats: StatsCollector | None = None,\n+        pqclass: type[ScrapyPriorityQueue] | None = None,\n+        crawler: Crawler | None = None,\n     ):\n         self.df: BaseDupeFilter = dupefilter\n-        self.dqdir: Optional[str] = self._dqdir(jobdir)\n-        self.pqclass: Optional[type[ScrapyPriorityQueue]] = pqclass\n-        self.dqclass: Optional[type[BaseQueue]] = dqclass\n-        self.mqclass: Optional[type[BaseQueue]] = mqclass\n+        self.dqdir: str | None = self._dqdir(jobdir)\n+        self.pqclass: type[ScrapyPriorityQueue] | None = pqclass\n+        self.dqclass: type[BaseQueue] | None = dqclass\n+        self.mqclass: type[BaseQueue] | None = mqclass\n         self.logunser: bool = logunser\n-        self.stats: Optional[StatsCollector] = stats\n-        self.crawler: Optional[Crawler] = crawler\n+        self.stats: StatsCollector | None = stats\n+        self.crawler: Crawler | None = crawler\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n@@ -218,7 +218,7 @@ class Scheduler(BaseScheduler):\n     def has_pending_requests(self) -> bool:\n         return len(self) > 0\n \n-    def open(self, spider: Spider) -> Optional[Deferred[None]]:\n+    def open(self, spider: Spider) -> Deferred[None] | None:\n         \"\"\"\n         (1) initialize the memory queue\n         (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n@@ -226,10 +226,10 @@ class Scheduler(BaseScheduler):\n         \"\"\"\n         self.spider: Spider = spider\n         self.mqs: ScrapyPriorityQueue = self._mq()\n-        self.dqs: Optional[ScrapyPriorityQueue] = self._dq() if self.dqdir else None\n+        self.dqs: ScrapyPriorityQueue | None = self._dq() if self.dqdir else None\n         return self.df.open()\n \n-    def close(self, reason: str) -> Optional[Deferred[None]]:\n+    def close(self, reason: str) -> Deferred[None] | None:\n         \"\"\"\n         (1) dump pending requests to disk if there is a disk queue\n         (2) return the result of the dupefilter's ``close`` method\n@@ -263,7 +263,7 @@ class Scheduler(BaseScheduler):\n         self.stats.inc_value(\"scheduler/enqueued\", spider=self.spider)\n         return True\n \n-    def next_request(self) -> Optional[Request]:\n+    def next_request(self) -> Request | None:\n         \"\"\"\n         Return a :class:`~scrapy.http.Request` object from the memory queue,\n         falling back to the disk queue if the memory queue is empty.\n@@ -272,7 +272,7 @@ class Scheduler(BaseScheduler):\n         Increment the appropriate stats, such as: ``scheduler/dequeued``,\n         ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n         \"\"\"\n-        request: Optional[Request] = self.mqs.pop()\n+        request: Request | None = self.mqs.pop()\n         assert self.stats is not None\n         if request is not None:\n             self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n@@ -318,7 +318,7 @@ class Scheduler(BaseScheduler):\n     def _mqpush(self, request: Request) -> None:\n         self.mqs.push(request)\n \n-    def _dqpop(self) -> Optional[Request]:\n+    def _dqpop(self) -> Request | None:\n         if self.dqs is not None:\n             return self.dqs.pop()\n         return None\n@@ -355,7 +355,7 @@ class Scheduler(BaseScheduler):\n             )\n         return q\n \n-    def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n+    def _dqdir(self, jobdir: str | None) -> str | None:\n         \"\"\"Return a folder name to keep disk queue state at\"\"\"\n         if jobdir:\n             dqdir = Path(jobdir, \"requests.queue\")\n\n@@ -6,7 +6,7 @@ from __future__ import annotations\n import logging\n from collections import deque\n from collections.abc import AsyncIterable, Iterator\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -42,9 +42,6 @@ logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n _ParallelResult = list[tuple[bool, Iterator[Any]]]\n-\n-if TYPE_CHECKING:\n-    # parameterized Deferreds require Twisted 21.7.0\n _HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n QueueTuple = tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n \n@@ -60,10 +57,10 @@ class Slot:\n         self.active: set[Request] = set()\n         self.active_size: int = 0\n         self.itemproc_size: int = 0\n-        self.closing: Optional[Deferred[Spider]] = None\n+        self.closing: Deferred[Spider] | None = None\n \n     def add_response_request(\n-        self, result: Union[Response, Failure], request: Request\n+        self, result: Response | Failure, request: Request\n     ) -> _HandleOutputDeferred:\n         deferred: _HandleOutputDeferred = Deferred()\n         self.queue.append((result, request, deferred))\n@@ -78,9 +75,7 @@ class Slot:\n         self.active.add(request)\n         return response, request, deferred\n \n-    def finish_response(\n-        self, result: Union[Response, Failure], request: Request\n-    ) -> None:\n+    def finish_response(self, result: Response | Failure, request: Request) -> None:\n         self.active.remove(request)\n         if isinstance(result, Response):\n             self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n@@ -96,7 +91,7 @@ class Slot:\n \n class Scraper:\n     def __init__(self, crawler: Crawler) -> None:\n-        self.slot: Optional[Slot] = None\n+        self.slot: Slot | None = None\n         self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n             crawler\n         )\n@@ -135,7 +130,7 @@ class Scraper:\n             self.slot.closing.callback(spider)\n \n     def enqueue_scrape(\n-        self, result: Union[Response, Failure], request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request, spider: Spider\n     ) -> _HandleOutputDeferred:\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n@@ -167,7 +162,7 @@ class Scraper:\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n     def _scrape(\n-        self, result: Union[Response, Failure], request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request, spider: Spider\n     ) -> _HandleOutputDeferred:\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n@@ -176,7 +171,7 @@ class Scraper:\n             raise TypeError(\n                 f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n             )\n-        dfd: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = self._scrape2(\n+        dfd: Deferred[Iterable[Any] | AsyncIterable[Any]] = self._scrape2(\n             result, request, spider\n         )  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, result, spider)\n@@ -186,8 +181,8 @@ class Scraper:\n         return dfd2\n \n     def _scrape2(\n-        self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n+        self, result: Response | Failure, request: Request, spider: Spider\n+    ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n@@ -202,8 +197,8 @@ class Scraper:\n         return dfd\n \n     def call_spider(\n-        self, result: Union[Response, Failure], request: Request, spider: Spider\n-    ) -> Deferred[Union[Iterable[Any], AsyncIterable[Any]]]:\n+        self, result: Response | Failure, request: Request, spider: Spider\n+    ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n         dfd: Deferred[Any]\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n@@ -222,7 +217,7 @@ class Scraper:\n             if request.errback:\n                 warn_on_generator_with_return_value(spider, request.errback)\n                 dfd.addErrback(request.errback)\n-        dfd2: Deferred[Union[Iterable[Any], AsyncIterable[Any]]] = dfd.addCallback(\n+        dfd2: Deferred[Iterable[Any] | AsyncIterable[Any]] = dfd.addCallback(\n             iterate_spider_output\n         )\n         return dfd2\n@@ -231,7 +226,7 @@ class Scraper:\n         self,\n         _failure: Failure,\n         request: Request,\n-        response: Union[Response, Failure],\n+        response: Response | Failure,\n         spider: Spider,\n     ) -> None:\n         exc = _failure.value\n@@ -258,14 +253,14 @@ class Scraper:\n \n     def handle_spider_output(\n         self,\n-        result: Union[Iterable[_T], AsyncIterable[_T]],\n+        result: Iterable[_T] | AsyncIterable[_T],\n         request: Request,\n         response: Response,\n         spider: Spider,\n     ) -> _HandleOutputDeferred:\n         if not result:\n             return defer_succeed(None)\n-        it: Union[Iterable[_T], AsyncIterable[_T]]\n+        it: Iterable[_T] | AsyncIterable[_T]\n         dfd: Deferred[_ParallelResult]\n         if isinstance(result, AsyncIterable):\n             it = aiter_errback(\n@@ -296,7 +291,7 @@ class Scraper:\n \n     def _process_spidermw_output(\n         self, output: Any, request: Request, response: Response, spider: Spider\n-    ) -> Optional[Deferred[Any]]:\n+    ) -> Deferred[Any] | None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n@@ -316,9 +311,7 @@ class Scraper:\n             )\n         return None\n \n-    def start_itemproc(\n-        self, item: Any, *, response: Optional[Response]\n-    ) -> Deferred[Any]:\n+    def start_itemproc(self, item: Any, *, response: Response | None) -> Deferred[Any]:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n@@ -337,7 +330,7 @@ class Scraper:\n         download_failure: Failure,\n         request: Request,\n         spider: Spider,\n-    ) -> Union[Failure, None]:\n+    ) -> Failure | None:\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here).\n \n@@ -371,7 +364,7 @@ class Scraper:\n         return None\n \n     def _itemproc_finished(\n-        self, output: Any, item: Any, response: Optional[Response], spider: Spider\n+        self, output: Any, item: Any, response: Response | None, spider: Spider\n     ) -> Deferred[Any]:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n\n@@ -10,7 +10,7 @@ import logging\n from collections.abc import AsyncIterable, Callable, Iterable\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n@@ -76,7 +76,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         response: Response,\n         request: Request,\n         spider: Spider,\n-    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n+    ) -> Iterable[_T] | AsyncIterable[_T]:\n         for method in self.methods[\"process_spider_input\"]:\n             method = cast(Callable, method)\n             try:\n@@ -97,10 +97,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        iterable: Union[Iterable[_T], AsyncIterable[_T]],\n+        iterable: Iterable[_T] | AsyncIterable[_T],\n         exception_processor_index: int,\n-        recover_to: Union[MutableChain[_T], MutableAsyncChain[_T]],\n-    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n+        recover_to: MutableChain[_T] | MutableAsyncChain[_T],\n+    ) -> Iterable[_T] | AsyncIterable[_T]:\n         def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n             try:\n                 yield from iterable\n@@ -142,7 +142,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         spider: Spider,\n         _failure: Failure,\n         start_index: int = 0,\n-    ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n+    ) -> Failure | MutableChain[_T] | MutableAsyncChain[_T]:\n         exception = _failure.value\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n@@ -158,7 +158,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             if _isiterable(result):\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n-                dfd: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n+                dfd: Deferred[MutableChain[_T] | MutableAsyncChain[_T]] = (\n                     self._process_spider_output(\n                         response, spider, result, method_index + 1\n                     )\n@@ -192,12 +192,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        result: Union[Iterable[_T], AsyncIterable[_T]],\n+        result: Iterable[_T] | AsyncIterable[_T],\n         start_index: int = 0,\n-    ) -> Generator[Deferred[Any], Any, Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n+    ) -> Generator[Deferred[Any], Any, MutableChain[_T] | MutableAsyncChain[_T]]:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n-        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n+        recovered: MutableChain[_T] | MutableAsyncChain[_T]\n         last_result_is_async = isinstance(result, AsyncIterable)\n         if last_result_is_async:\n             recovered = MutableAsyncChain()\n@@ -248,11 +248,11 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 # might fail directly if the output value is not a generator\n                 result = method(response=response, result=result, spider=spider)\n             except Exception as ex:\n-                exception_result: Union[\n-                    Failure, MutableChain[_T], MutableAsyncChain[_T]\n-                ] = self._process_spider_exception(\n+                exception_result: Failure | MutableChain[_T] | MutableAsyncChain[_T] = (\n+                    self._process_spider_exception(\n                         response, spider, Failure(ex), method_index + 1\n                     )\n+                )\n                 if isinstance(exception_result, Failure):\n                     raise\n                 return exception_result\n@@ -283,9 +283,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        result: Union[Iterable[_T], AsyncIterable[_T]],\n-    ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n-        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n+        result: Iterable[_T] | AsyncIterable[_T],\n+    ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n+        recovered: MutableChain[_T] | MutableAsyncChain[_T]\n         if isinstance(result, AsyncIterable):\n             recovered = MutableAsyncChain()\n         else:\n@@ -293,7 +293,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n         result = await maybe_deferred_to_future(\n             cast(\n-                \"Deferred[Union[Iterable[_T], AsyncIterable[_T]]]\",\n+                \"Deferred[Iterable[_T] | AsyncIterable[_T]]\",\n                 self._process_spider_output(response, spider, result),\n             )\n         )\n@@ -310,22 +310,22 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         response: Response,\n         request: Request,\n         spider: Spider,\n-    ) -> Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n+    ) -> Deferred[MutableChain[_T] | MutableAsyncChain[_T]]:\n         async def process_callback_output(\n-            result: Union[Iterable[_T], AsyncIterable[_T]]\n-        ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n+            result: Iterable[_T] | AsyncIterable[_T],\n+        ) -> MutableChain[_T] | MutableAsyncChain[_T]:\n             return await self._process_callback_output(response, spider, result)\n \n         def process_spider_exception(\n             _failure: Failure,\n-        ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n+        ) -> Failure | MutableChain[_T] | MutableAsyncChain[_T]:\n             return self._process_spider_exception(response, spider, _failure)\n \n-        dfd: Deferred[Union[Iterable[_T], AsyncIterable[_T]]] = mustbe_deferred(\n+        dfd: Deferred[Iterable[_T] | AsyncIterable[_T]] = mustbe_deferred(\n             self._process_spider_input, scrape_func, response, request, spider\n         )\n-        dfd2: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n-            dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n+        dfd2: Deferred[MutableChain[_T] | MutableAsyncChain[_T]] = dfd.addCallback(\n+            deferred_f_from_coro_f(process_callback_output)\n         )\n         dfd2.addErrback(process_spider_exception)\n         return dfd2\n@@ -339,10 +339,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     @staticmethod\n     def _get_async_method_pair(\n         mw: Any, methodname: str\n-    ) -> Union[None, Callable, tuple[Callable, Callable]]:\n-        normal_method: Optional[Callable] = getattr(mw, methodname, None)\n+    ) -> None | Callable | tuple[Callable, Callable]:\n+        normal_method: Callable | None = getattr(mw, methodname, None)\n         methodname_async = methodname + \"_async\"\n-        async_method: Optional[Callable] = getattr(mw, methodname_async, None)\n+        async_method: Callable | None = getattr(mw, methodname_async, None)\n         if not async_method:\n             return normal_method\n         if not normal_method:\n\n@@ -4,7 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from twisted.internet.defer import (\n     Deferred,\n@@ -57,7 +57,7 @@ class Crawler:\n     def __init__(\n         self,\n         spidercls: type[Spider],\n-        settings: Union[None, dict[str, Any], Settings] = None,\n+        settings: None | dict[str, Any] | Settings = None,\n         init_reactor: bool = False,\n     ):\n         if isinstance(spidercls, Spider):\n@@ -78,12 +78,12 @@ class Crawler:\n         self.crawling: bool = False\n         self._started: bool = False\n \n-        self.extensions: Optional[ExtensionManager] = None\n-        self.stats: Optional[StatsCollector] = None\n-        self.logformatter: Optional[LogFormatter] = None\n-        self.request_fingerprinter: Optional[RequestFingerprinter] = None\n-        self.spider: Optional[Spider] = None\n-        self.engine: Optional[ExecutionEngine] = None\n+        self.extensions: ExtensionManager | None = None\n+        self.stats: StatsCollector | None = None\n+        self.logformatter: LogFormatter | None = None\n+        self.request_fingerprinter: RequestFingerprinter | None = None\n+        self.spider: Spider | None = None\n+        self.engine: ExecutionEngine | None = None\n \n     def _update_root_log_handler(self) -> None:\n         if get_scrapy_root_handler() is not None:\n@@ -181,16 +181,16 @@ class Crawler:\n     @staticmethod\n     def _get_component(\n         component_class: type[_T], components: Iterable[Any]\n-    ) -> Optional[_T]:\n+    ) -> _T | None:\n         for component in components:\n             if isinstance(component, component_class):\n                 return component\n         return None\n \n-    def get_addon(self, cls: type[_T]) -> Optional[_T]:\n+    def get_addon(self, cls: type[_T]) -> _T | None:\n         return self._get_component(cls, self.addons.addons)\n \n-    def get_downloader_middleware(self, cls: type[_T]) -> Optional[_T]:\n+    def get_downloader_middleware(self, cls: type[_T]) -> _T | None:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_downloader_middleware() can only be called after \"\n@@ -198,7 +198,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n \n-    def get_extension(self, cls: type[_T]) -> Optional[_T]:\n+    def get_extension(self, cls: type[_T]) -> _T | None:\n         if not self.extensions:\n             raise RuntimeError(\n                 \"Crawler.get_extension() can only be called after the \"\n@@ -206,7 +206,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.extensions.middlewares)\n \n-    def get_item_pipeline(self, cls: type[_T]) -> Optional[_T]:\n+    def get_item_pipeline(self, cls: type[_T]) -> _T | None:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_item_pipeline() can only be called after the \"\n@@ -214,7 +214,7 @@ class Crawler:\n             )\n         return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n \n-    def get_spider_middleware(self, cls: type[_T]) -> Optional[_T]:\n+    def get_spider_middleware(self, cls: type[_T]) -> _T | None:\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_spider_middleware() can only be called after the \"\n@@ -250,7 +250,7 @@ class CrawlerRunner:\n         verifyClass(ISpiderLoader, loader_cls)\n         return cast(\"SpiderLoader\", loader_cls.from_settings(settings.frozencopy()))\n \n-    def __init__(self, settings: Union[dict[str, Any], Settings, None] = None):\n+    def __init__(self, settings: dict[str, Any] | Settings | None = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings: Settings = settings\n@@ -261,7 +261,7 @@ class CrawlerRunner:\n \n     def crawl(\n         self,\n-        crawler_or_spidercls: Union[type[Spider], str, Crawler],\n+        crawler_or_spidercls: type[Spider] | str | Crawler,\n         *args: Any,\n         **kwargs: Any,\n     ) -> Deferred[None]:\n@@ -308,7 +308,7 @@ class CrawlerRunner:\n         return d.addBoth(_done)\n \n     def create_crawler(\n-        self, crawler_or_spidercls: Union[type[Spider], str, Crawler]\n+        self, crawler_or_spidercls: type[Spider] | str | Crawler\n     ) -> Crawler:\n         \"\"\"\n         Return a :class:`~scrapy.crawler.Crawler` object.\n@@ -329,7 +329,7 @@ class CrawlerRunner:\n             return crawler_or_spidercls\n         return self._create_crawler(crawler_or_spidercls)\n \n-    def _create_crawler(self, spidercls: Union[str, type[Spider]]) -> Crawler:\n+    def _create_crawler(self, spidercls: str | type[Spider]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n@@ -380,7 +380,7 @@ class CrawlerProcess(CrawlerRunner):\n \n     def __init__(\n         self,\n-        settings: Union[dict[str, Any], Settings, None] = None,\n+        settings: dict[str, Any] | Settings | None = None,\n         install_root_handler: bool = True,\n     ):\n         super().__init__(settings)\n@@ -409,7 +409,7 @@ class CrawlerProcess(CrawlerRunner):\n         )\n         reactor.callFromThread(self._stop_reactor)\n \n-    def _create_crawler(self, spidercls: Union[type[Spider], str]) -> Crawler:\n+    def _create_crawler(self, spidercls: type[Spider] | str) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         init_reactor = not self._initialized_reactor\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n import re\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from w3lib import html\n \n@@ -43,7 +43,7 @@ class AjaxCrawlMiddleware:\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if not isinstance(response, HtmlResponse) or response.status != 200:\n             return response\n \n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from tldextract import TLDExtract\n \n@@ -70,7 +70,7 @@ class CookiesMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         if request.meta.get(\"dont_merge_cookies\", False):\n             return None\n \n@@ -87,7 +87,7 @@ class CookiesMiddleware:\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if request.meta.get(\"dont_merge_cookies\", False):\n             return response\n \n@@ -123,7 +123,7 @@ class CookiesMiddleware:\n                 msg = f\"Received cookies from: {response}\\n{cookies}\"\n                 logger.debug(msg, extra={\"spider\": spider})\n \n-    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> Optional[str]:\n+    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> str | None:\n         \"\"\"\n         Given a dict consisting of cookie components, return its string representation.\n         Decode from bytes if necessary.\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from scrapy.utils.python import without_none_values\n \n@@ -32,7 +32,7 @@ class DefaultHeadersMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         for k, v in self._headers:\n             request.headers.setdefault(k, v)\n         return None\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from scrapy import Request, Spider, signals\n \n@@ -33,7 +33,7 @@ class DownloadTimeoutMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         if self._timeout:\n             request.meta.setdefault(\"download_timeout\", self._timeout)\n         return None\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from w3lib.http import basic_auth_header\n \n@@ -40,7 +40,7 @@ class HttpAuthMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         auth = getattr(self, \"auth\", None)\n         if auth and b\"Authorization\" not in request.headers:\n             if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from email.utils import formatdate\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from twisted.internet import defer\n from twisted.internet.error import (\n@@ -69,7 +69,7 @@ class HttpCacheMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         if request.meta.get(\"dont_cache\", False):\n             return None\n \n@@ -79,7 +79,7 @@ class HttpCacheMiddleware:\n             return None\n \n         # Look for cached response and check if expired\n-        cachedresponse: Optional[Response] = self.storage.retrieve_response(\n+        cachedresponse: Response | None = self.storage.retrieve_response(\n             spider, request\n         )\n         if cachedresponse is None:\n@@ -103,7 +103,7 @@ class HttpCacheMiddleware:\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if request.meta.get(\"dont_cache\", False):\n             return response\n \n@@ -118,7 +118,7 @@ class HttpCacheMiddleware:\n             response.headers[\"Date\"] = formatdate(usegmt=True)\n \n         # Do not validate first-hand responses\n-        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n+        cachedresponse: Response | None = request.meta.pop(\"cached_response\", None)\n         if cachedresponse is None:\n             self.stats.inc_value(\"httpcache/firsthand\", spider=spider)\n             self._cache_response(spider, response, request, cachedresponse)\n@@ -134,8 +134,8 @@ class HttpCacheMiddleware:\n \n     def process_exception(\n         self, request: Request, exception: Exception, spider: Spider\n-    ) -> Union[Request, Response, None]:\n-        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n+    ) -> Request | Response | None:\n+        cachedresponse: Response | None = request.meta.pop(\"cached_response\", None)\n         if cachedresponse is not None and isinstance(\n             exception, self.DOWNLOAD_EXCEPTIONS\n         ):\n@@ -148,7 +148,7 @@ class HttpCacheMiddleware:\n         spider: Spider,\n         response: Response,\n         request: Request,\n-        cachedresponse: Optional[Response],\n+        cachedresponse: Response | None,\n     ) -> None:\n         if self.policy.should_cache_response(response, request):\n             self.stats.inc_value(\"httpcache/store\", spider=spider)\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import warnings\n from itertools import chain\n from logging import getLogger\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n@@ -54,9 +54,9 @@ class HttpCompressionMiddleware:\n \n     def __init__(\n         self,\n-        stats: Optional[StatsCollector] = None,\n+        stats: StatsCollector | None = None,\n         *,\n-        crawler: Optional[Crawler] = None,\n+        crawler: Crawler | None = None,\n     ):\n         if not crawler:\n             self.stats = stats\n@@ -96,13 +96,13 @@ class HttpCompressionMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n         return None\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if request.method == \"HEAD\":\n             return response\n         if isinstance(response, Response):\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import base64\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n from urllib.parse import unquote, urlunparse\n from urllib.request import (  # type: ignore[attr-defined]\n     _parse_proxy,\n@@ -23,9 +23,9 @@ if TYPE_CHECKING:\n \n \n class HttpProxyMiddleware:\n-    def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n-        self.auth_encoding: Optional[str] = auth_encoding\n-        self.proxies: dict[str, tuple[Optional[bytes], str]] = {}\n+    def __init__(self, auth_encoding: str | None = \"latin-1\"):\n+        self.auth_encoding: str | None = auth_encoding\n+        self.proxies: dict[str, tuple[bytes | None, str]] = {}\n         for type_, url in getproxies().items():\n             try:\n                 self.proxies[type_] = self._get_proxy(url, type_)\n@@ -38,7 +38,7 @@ class HttpProxyMiddleware:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n             raise NotConfigured\n-        auth_encoding: Optional[str] = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n+        auth_encoding: str | None = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n         return cls(auth_encoding)\n \n     def _basic_auth_header(self, username: str, password: str) -> bytes:\n@@ -47,7 +47,7 @@ class HttpProxyMiddleware:\n         )\n         return base64.b64encode(user_pass)\n \n-    def _get_proxy(self, url: str, orig_type: str) -> tuple[Optional[bytes], str]:\n+    def _get_proxy(self, url: str, orig_type: str) -> tuple[bytes | None, str]:\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n \n@@ -60,7 +60,7 @@ class HttpProxyMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         creds, proxy_url, scheme = None, None, None\n         if \"proxy\" in request.meta:\n             if request.meta[\"proxy\"] is not None:\n@@ -82,9 +82,9 @@ class HttpProxyMiddleware:\n     def _set_proxy_and_creds(\n         self,\n         request: Request,\n-        proxy_url: Optional[str],\n-        creds: Optional[bytes],\n-        scheme: Optional[str],\n+        proxy_url: str | None,\n+        creds: bytes | None,\n+        scheme: str | None,\n     ) -> None:\n         if scheme:\n             request.meta[\"_scheme_proxy\"] = True\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Union, cast\n+from typing import TYPE_CHECKING, Any, cast\n from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n@@ -144,7 +144,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if (\n             request.meta.get(\"dont_redirect\", False)\n             or response.status in getattr(spider, \"handle_httpstatus_list\", [])\n@@ -185,7 +185,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if (\n             request.meta.get(\"dont_redirect\", False)\n             or request.method == \"HEAD\"\n\n@@ -14,7 +14,7 @@ from __future__ import annotations\n \n import warnings\n from logging import Logger, getLogger\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.settings import BaseSettings, Settings\n@@ -60,12 +60,12 @@ def get_retry_request(\n     request: Request,\n     *,\n     spider: Spider,\n-    reason: Union[str, Exception, type[Exception]] = \"unspecified\",\n-    max_retry_times: Optional[int] = None,\n-    priority_adjust: Optional[int] = None,\n+    reason: str | Exception | type[Exception] = \"unspecified\",\n+    max_retry_times: int | None = None,\n+    priority_adjust: int | None = None,\n     logger: Logger = retry_logger,\n     stats_base_key: str = \"retry\",\n-) -> Optional[Request]:\n+) -> Request | None:\n     \"\"\"\n     Returns a new :class:`~scrapy.Request` object to retry the specified\n     request, or ``None`` if retries of the specified request have been\n@@ -167,7 +167,7 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         if request.meta.get(\"dont_retry\", False):\n             return response\n         if response.status in self.retry_http_codes:\n@@ -177,7 +177,7 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n \n     def process_exception(\n         self, request: Request, exception: Exception, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n             \"dont_retry\", False\n         ):\n@@ -187,9 +187,9 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n     def _retry(\n         self,\n         request: Request,\n-        reason: Union[str, Exception, type[Exception]],\n+        reason: str | Exception | type[Exception],\n         spider: Spider,\n-    ) -> Optional[Request]:\n+    ) -> Request | None:\n         max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n         priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n         return get_retry_request(\n\n@@ -7,7 +7,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional, TypeVar, Union\n+from typing import TYPE_CHECKING, TypeVar\n \n from twisted.internet.defer import Deferred, maybeDeferred\n \n@@ -41,13 +41,11 @@ class RobotsTxtMiddleware:\n         if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n             raise NotConfigured\n         self._default_useragent: str = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n-        self._robotstxt_useragent: Optional[str] = crawler.settings.get(\n+        self._robotstxt_useragent: str | None = crawler.settings.get(\n             \"ROBOTSTXT_USER_AGENT\", None\n         )\n         self.crawler: Crawler = crawler\n-        self._parsers: dict[\n-            str, Union[RobotParser, Deferred[Optional[RobotParser]], None]\n-        ] = {}\n+        self._parsers: dict[str, RobotParser | Deferred[RobotParser | None] | None] = {}\n         self._parserimpl: RobotParser = load_object(\n             crawler.settings.get(\"ROBOTSTXT_PARSER\")\n         )\n@@ -61,24 +59,24 @@ class RobotsTxtMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Optional[Deferred[None]]:\n+    ) -> Deferred[None] | None:\n         if request.meta.get(\"dont_obey_robotstxt\"):\n             return None\n         if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n             return None\n-        d: Deferred[Optional[RobotParser]] = maybeDeferred(\n+        d: Deferred[RobotParser | None] = maybeDeferred(\n             self.robot_parser, request, spider  # type: ignore[call-overload]\n         )\n         d2: Deferred[None] = d.addCallback(self.process_request_2, request, spider)\n         return d2\n \n     def process_request_2(\n-        self, rp: Optional[RobotParser], request: Request, spider: Spider\n+        self, rp: RobotParser | None, request: Request, spider: Spider\n     ) -> None:\n         if rp is None:\n             return\n \n-        useragent: Union[str, bytes, None] = self._robotstxt_useragent\n+        useragent: str | bytes | None = self._robotstxt_useragent\n         if not useragent:\n             useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n             assert useragent is not None\n@@ -94,7 +92,7 @@ class RobotsTxtMiddleware:\n \n     def robot_parser(\n         self, request: Request, spider: Spider\n-    ) -> Union[RobotParser, Deferred[Optional[RobotParser]], None]:\n+    ) -> RobotParser | Deferred[RobotParser | None] | None:\n         url = urlparse_cached(request)\n         netloc = url.netloc\n \n@@ -117,9 +115,9 @@ class RobotsTxtMiddleware:\n \n         parser = self._parsers[netloc]\n         if isinstance(parser, Deferred):\n-            d: Deferred[Optional[RobotParser]] = Deferred()\n+            d: Deferred[RobotParser | None] = Deferred()\n \n-            def cb(result: Optional[RobotParser]) -> Optional[RobotParser]:\n+            def cb(result: RobotParser | None) -> RobotParser | None:\n                 d.callback(result)\n                 return result\n \n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from twisted.web import http\n \n@@ -19,7 +19,7 @@ if TYPE_CHECKING:\n \n \n def get_header_size(\n-    headers: dict[str, Union[list[Union[str, bytes]], tuple[Union[str, bytes], ...]]]\n+    headers: dict[str, list[str | bytes] | tuple[str | bytes, ...]]\n ) -> int:\n     size = 0\n     for key, value in headers.items():\n@@ -47,7 +47,7 @@ class DownloaderStats:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         self.stats.inc_value(\"downloader/request_count\", spider=spider)\n         self.stats.inc_value(\n             f\"downloader/request_method_count/{request.method}\", spider=spider\n@@ -58,7 +58,7 @@ class DownloaderStats:\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Union[Request, Response]:\n+    ) -> Request | Response:\n         self.stats.inc_value(\"downloader/response_count\", spider=spider)\n         self.stats.inc_value(\n             f\"downloader/response_status_count/{response.status}\", spider=spider\n@@ -75,7 +75,7 @@ class DownloaderStats:\n \n     def process_exception(\n         self, request: Request, exception: Exception, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         ex_class = global_object_name(exception.__class__)\n         self.stats.inc_value(\"downloader/exception_count\", spider=spider)\n         self.stats.inc_value(\n\n@@ -2,7 +2,7 @@\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from scrapy import Request, Spider, signals\n \n@@ -31,7 +31,7 @@ class UserAgentMiddleware:\n \n     def process_request(\n         self, request: Request, spider: Spider\n-    ) -> Union[Request, Response, None]:\n+    ) -> Request | Response | None:\n         if self.user_agent:\n             request.headers.setdefault(b\"User-Agent\", self.user_agent)\n         return None\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n from pathlib import Path\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n@@ -31,10 +31,10 @@ class BaseDupeFilter:\n     def request_seen(self, request: Request) -> bool:\n         return False\n \n-    def open(self) -> Optional[Deferred[None]]:\n+    def open(self) -> Deferred[None] | None:\n         pass\n \n-    def close(self, reason: str) -> Optional[Deferred[None]]:\n+    def close(self, reason: str) -> Deferred[None] | None:\n         pass\n \n     def log(self, request: Request, spider: Spider) -> None:\n@@ -47,10 +47,10 @@ class RFPDupeFilter(BaseDupeFilter):\n \n     def __init__(\n         self,\n-        path: Optional[str] = None,\n+        path: str | None = None,\n         debug: bool = False,\n         *,\n-        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> None:\n         self.file = None\n         self.fingerprinter: RequestFingerprinterProtocol = (\n@@ -70,7 +70,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         cls,\n         settings: BaseSettings,\n         *,\n-        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n         debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n         return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n\n@@ -2,6 +2,8 @@\n Item Exporters are used to export/serialize items into different formats.\n \"\"\"\n \n+from __future__ import annotations\n+\n import csv\n import marshal\n import pickle  # nosec\n@@ -9,7 +11,7 @@ import pprint\n from collections.abc import Callable, Iterable, Mapping\n from io import BytesIO, TextIOWrapper\n from json import JSONEncoder\n-from typing import Any, Optional, Union\n+from typing import Any\n from xml.sax.saxutils import XMLGenerator  # nosec\n from xml.sax.xmlreader import AttributesImpl  # nosec\n \n@@ -41,12 +43,12 @@ class BaseItemExporter:\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses ``__init__`` methods)\n         \"\"\"\n-        self.encoding: Optional[str] = options.pop(\"encoding\", None)\n-        self.fields_to_export: Union[Mapping[str, str], Iterable[str], None] = (\n-            options.pop(\"fields_to_export\", None)\n+        self.encoding: str | None = options.pop(\"encoding\", None)\n+        self.fields_to_export: Mapping[str, str] | Iterable[str] | None = options.pop(\n+            \"fields_to_export\", None\n         )\n         self.export_empty_fields: bool = options.pop(\"export_empty_fields\", False)\n-        self.indent: Optional[int] = options.pop(\"indent\", None)\n+        self.indent: int | None = options.pop(\"indent\", None)\n         if not dont_fail and options:\n             raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n \n@@ -54,7 +56,7 @@ class BaseItemExporter:\n         raise NotImplementedError\n \n     def serialize_field(\n-        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+        self, field: Mapping[str, Any] | Field, name: str, value: Any\n     ) -> Any:\n         serializer: Callable[[Any], Any] = field.get(\"serializer\", lambda x: x)\n         return serializer(value)\n@@ -66,7 +68,7 @@ class BaseItemExporter:\n         pass\n \n     def _get_serialized_fields(\n-        self, item: Any, default_value: Any = None, include_empty: Optional[bool] = None\n+        self, item: Any, default_value: Any = None, include_empty: bool | None = None\n     ) -> Iterable[tuple[str, Any]]:\n         \"\"\"Return the fields to export as an iterable of tuples\n         (name, serialized_value)\n@@ -225,7 +227,7 @@ class CsvItemExporter(BaseItemExporter):\n         file: BytesIO,\n         include_headers_line: bool = True,\n         join_multivalued: str = \",\",\n-        errors: Optional[str] = None,\n+        errors: str | None = None,\n         **kwargs: Any,\n     ):\n         super().__init__(dont_fail=True, **kwargs)\n@@ -245,7 +247,7 @@ class CsvItemExporter(BaseItemExporter):\n         self._join_multivalued = join_multivalued\n \n     def serialize_field(\n-        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+        self, field: Mapping[str, Any] | Field, name: str, value: Any\n     ) -> Any:\n         serializer: Callable[[Any], Any] = field.get(\"serializer\", self._join_if_needed)\n         return serializer(value)\n@@ -346,7 +348,7 @@ class PythonItemExporter(BaseItemExporter):\n             self.encoding = \"utf-8\"\n \n     def serialize_field(\n-        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+        self, field: Mapping[str, Any] | Field, name: str, value: Any\n     ) -> Any:\n         serializer: Callable[[Any], Any] = field.get(\n             \"serializer\", self._serialize_value\n@@ -364,10 +366,10 @@ class PythonItemExporter(BaseItemExporter):\n             return to_unicode(value, encoding=self.encoding)\n         return value\n \n-    def _serialize_item(self, item: Any) -> Iterable[tuple[Union[str, bytes], Any]]:\n+    def _serialize_item(self, item: Any) -> Iterable[tuple[str | bytes, Any]]:\n         for key, value in ItemAdapter(item).items():\n             yield key, self._serialize_value(value)\n \n-    def export_item(self, item: Any) -> dict[Union[str, bytes], Any]:  # type: ignore[override]\n-        result: dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n+    def export_item(self, item: Any) -> dict[str | bytes, Any]:  # type: ignore[override]\n+        result: dict[str | bytes, Any] = dict(self._get_serialized_fields(item))\n         return result\n\n@@ -5,7 +5,7 @@ Extension for collecting core stats like items scraped and start/finish times\n from __future__ import annotations\n \n from datetime import datetime, timezone\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from scrapy import Spider, signals\n \n@@ -20,7 +20,7 @@ if TYPE_CHECKING:\n class CoreStats:\n     def __init__(self, stats: StatsCollector):\n         self.stats: StatsCollector = stats\n-        self.start_time: Optional[datetime] = None\n+        self.start_time: datetime | None = None\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n\n@@ -12,7 +12,7 @@ import sys\n import threading\n import traceback\n from pdb import Pdb\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from scrapy.utils.engine import format_engine_status\n from scrapy.utils.trackref import format_live_refs\n@@ -43,7 +43,7 @@ class StackTraceDump:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def dump_stacktrace(self, signum: int, frame: Optional[FrameType]) -> None:\n+    def dump_stacktrace(self, signum: int, frame: FrameType | None) -> None:\n         assert self.crawler.engine\n         log_args = {\n             \"stackdumps\": self._thread_stacks(),\n@@ -75,6 +75,6 @@ class Debugger:\n             # win32 platforms don't support SIGUSR signals\n             pass\n \n-    def _enter_debugger(self, signum: int, frame: Optional[FrameType]) -> None:\n+    def _enter_debugger(self, signum: int, frame: FrameType | None) -> None:\n         assert frame\n         Pdb().set_trace(frame.f_back)  # noqa: T100\n\n@@ -14,7 +14,7 @@ from collections.abc import Callable\n from datetime import datetime, timezone\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n-from typing import IO, TYPE_CHECKING, Any, Optional, Protocol, TypeVar, Union, cast\n+from typing import IO, TYPE_CHECKING, Any, Optional, Protocol, TypeVar, cast\n from urllib.parse import unquote, urlparse\n \n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n@@ -67,7 +67,7 @@ def build_storage(\n     builder: Callable[..., _StorageT],\n     uri: str,\n     *args: Any,\n-    feed_options: Optional[dict[str, Any]] = None,\n+    feed_options: dict[str, Any] | None = None,\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n@@ -84,10 +84,10 @@ class ItemFilter:\n     :type feed_options: dict\n     \"\"\"\n \n-    feed_options: Optional[dict[str, Any]]\n+    feed_options: dict[str, Any] | None\n     item_classes: tuple[type, ...]\n \n-    def __init__(self, feed_options: Optional[dict[str, Any]]) -> None:\n+    def __init__(self, feed_options: dict[str, Any] | None) -> None:\n         self.feed_options = feed_options\n         if feed_options is not None:\n             self.item_classes = tuple(\n@@ -129,7 +129,7 @@ class IFeedStorage(Interface):\n class FeedStorageProtocol(Protocol):\n     \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n \n-    def __init__(self, uri: str, *, feed_options: Optional[dict[str, Any]] = None):\n+    def __init__(self, uri: str, *, feed_options: dict[str, Any] | None = None):\n         \"\"\"Initialize the storage with the parameters given in the URI and the\n         feed-specific options (see :setting:`FEEDS`)\"\"\"\n \n@@ -137,7 +137,7 @@ class FeedStorageProtocol(Protocol):\n         \"\"\"Open the storage for the given spider. It must return a file-like\n         object that will be used for the exporters\"\"\"\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n+    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n         \"\"\"Store the given file stream\"\"\"\n \n \n@@ -150,7 +150,7 @@ class BlockingFeedStorage:\n \n         return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n+    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n         return deferToThread(self._store_in_thread, file)\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n@@ -162,9 +162,9 @@ class StdoutFeedStorage:\n     def __init__(\n         self,\n         uri: str,\n-        _stdout: Optional[IO[bytes]] = None,\n+        _stdout: IO[bytes] | None = None,\n         *,\n-        feed_options: Optional[dict[str, Any]] = None,\n+        feed_options: dict[str, Any] | None = None,\n     ):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n@@ -180,13 +180,13 @@ class StdoutFeedStorage:\n     def open(self, spider: Spider) -> IO[bytes]:\n         return self._stdout\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n+    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n         pass\n \n \n @implementer(IFeedStorage)\n class FileFeedStorage:\n-    def __init__(self, uri: str, *, feed_options: Optional[dict[str, Any]] = None):\n+    def __init__(self, uri: str, *, feed_options: dict[str, Any] | None = None):\n         self.path: str = file_uri_to_path(uri)\n         feed_options = feed_options or {}\n         self.write_mode: OpenBinaryMode = (\n@@ -199,7 +199,7 @@ class FileFeedStorage:\n             dirname.mkdir(parents=True)\n         return Path(self.path).open(self.write_mode)\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n+    def store(self, file: IO[bytes]) -> Deferred[None] | None:\n         file.close()\n         return None\n \n@@ -208,27 +208,27 @@ class S3FeedStorage(BlockingFeedStorage):\n     def __init__(\n         self,\n         uri: str,\n-        access_key: Optional[str] = None,\n-        secret_key: Optional[str] = None,\n-        acl: Optional[str] = None,\n-        endpoint_url: Optional[str] = None,\n+        access_key: str | None = None,\n+        secret_key: str | None = None,\n+        acl: str | None = None,\n+        endpoint_url: str | None = None,\n         *,\n-        feed_options: Optional[dict[str, Any]] = None,\n-        session_token: Optional[str] = None,\n-        region_name: Optional[str] = None,\n+        feed_options: dict[str, Any] | None = None,\n+        session_token: str | None = None,\n+        region_name: str | None = None,\n     ):\n         if not is_botocore_available():\n             raise NotConfigured(\"missing botocore library\")\n         u = urlparse(uri)\n         assert u.hostname\n         self.bucketname: str = u.hostname\n-        self.access_key: Optional[str] = u.username or access_key\n-        self.secret_key: Optional[str] = u.password or secret_key\n-        self.session_token: Optional[str] = session_token\n+        self.access_key: str | None = u.username or access_key\n+        self.secret_key: str | None = u.password or secret_key\n+        self.session_token: str | None = session_token\n         self.keyname: str = u.path[1:]  # remove first \"/\"\n-        self.acl: Optional[str] = acl\n-        self.endpoint_url: Optional[str] = endpoint_url\n-        self.region_name: Optional[str] = region_name\n+        self.acl: str | None = acl\n+        self.endpoint_url: str | None = endpoint_url\n+        self.region_name: str | None = region_name\n         # It can be either botocore.client.BaseClient or mypy_boto3_s3.S3Client,\n         # there seems to be no good way to infer it statically.\n         self.s3_client: Any\n@@ -279,7 +279,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         crawler: Crawler,\n         uri: str,\n         *,\n-        feed_options: Optional[dict[str, Any]] = None,\n+        feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n         return build_storage(\n             cls,\n@@ -310,9 +310,9 @@ class S3FeedStorage(BlockingFeedStorage):\n \n \n class GCSFeedStorage(BlockingFeedStorage):\n-    def __init__(self, uri: str, project_id: Optional[str], acl: Optional[str]):\n-        self.project_id: Optional[str] = project_id\n-        self.acl: Optional[str] = acl\n+    def __init__(self, uri: str, project_id: str | None, acl: str | None):\n+        self.project_id: str | None = project_id\n+        self.acl: str | None = acl\n         u = urlparse(uri)\n         assert u.hostname\n         self.bucket_name: str = u.hostname\n@@ -342,7 +342,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         uri: str,\n         use_active_mode: bool = False,\n         *,\n-        feed_options: Optional[dict[str, Any]] = None,\n+        feed_options: dict[str, Any] | None = None,\n     ):\n         u = urlparse(uri)\n         if not u.hostname:\n@@ -361,7 +361,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         crawler: Crawler,\n         uri: str,\n         *,\n-        feed_options: Optional[dict[str, Any]] = None,\n+        feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n         return build_storage(\n             cls,\n@@ -399,8 +399,8 @@ class FeedSlot:\n         settings: BaseSettings,\n         crawler: Crawler,\n     ):\n-        self.file: Optional[IO[bytes]] = None\n-        self.exporter: Optional[BaseItemExporter] = None\n+        self.file: IO[bytes] | None = None\n+        self.exporter: BaseItemExporter | None = None\n         self.storage: FeedStorageProtocol = storage\n         # feed params\n         self.batch_id: int = batch_id\n@@ -558,7 +558,7 @@ class FeedExporter:\n             self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n         )\n \n-    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred[None]]:\n+    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Deferred[None] | None:\n         def get_file(slot_: FeedSlot) -> IO[bytes]:\n             assert slot_.file\n             if isinstance(slot_.file, PostProcessingManager):\n@@ -770,8 +770,8 @@ class FeedExporter:\n     def _get_uri_params(\n         self,\n         spider: Spider,\n-        uri_params_function: Union[str, UriParamsCallableT, None],\n-        slot: Optional[FeedSlot] = None,\n+        uri_params_function: str | UriParamsCallableT | None,\n+        slot: FeedSlot | None = None,\n     ) -> dict[str, Any]:\n         params = {}\n         for k in dir(spider):\n\n@@ -9,7 +9,7 @@ from importlib import import_module\n from pathlib import Path\n from time import time\n from types import ModuleType\n-from typing import IO, TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import IO, TYPE_CHECKING, Any, cast\n from weakref import WeakKeyDictionary\n \n from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n@@ -66,16 +66,14 @@ class RFC2616Policy:\n         self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n         self.ignore_schemes: list[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n         self._cc_parsed: WeakKeyDictionary[\n-            Union[Request, Response], dict[bytes, Optional[bytes]]\n+            Request | Response, dict[bytes, bytes | None]\n         ] = WeakKeyDictionary()\n         self.ignore_response_cache_controls: list[bytes] = [\n             to_bytes(cc)\n             for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n         ]\n \n-    def _parse_cachecontrol(\n-        self, r: Union[Request, Response]\n-    ) -> dict[bytes, Optional[bytes]]:\n+    def _parse_cachecontrol(self, r: Request | Response) -> dict[bytes, bytes | None]:\n         if r not in self._cc_parsed:\n             cch = r.headers.get(b\"Cache-Control\", b\"\")\n             assert cch is not None\n@@ -191,7 +189,7 @@ class RFC2616Policy:\n         if b\"ETag\" in cachedresponse.headers:\n             request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n \n-    def _get_max_age(self, cc: dict[bytes, Optional[bytes]]) -> Optional[int]:\n+    def _get_max_age(self, cc: dict[bytes, bytes | None]) -> int | None:\n         try:\n             return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n         except (KeyError, ValueError):\n@@ -275,7 +273,7 @@ class DbmCacheStorage:\n     def close_spider(self, spider: Spider) -> None:\n         self.db.close()\n \n-    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n+    def retrieve_response(self, spider: Spider, request: Request) -> Response | None:\n         data = self._read_data(spider, request)\n         if data is None:\n             return None  # not cached\n@@ -300,7 +298,7 @@ class DbmCacheStorage:\n         self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n         self.db[f\"{key}_time\"] = str(time())\n \n-    def _read_data(self, spider: Spider, request: Request) -> Optional[dict[str, Any]]:\n+    def _read_data(self, spider: Spider, request: Request) -> dict[str, Any] | None:\n         key = self._fingerprinter.fingerprint(request).hex()\n         db = self.db\n         tkey = f\"{key}_time\"\n@@ -320,9 +318,7 @@ class FilesystemCacheStorage:\n         self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n         self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n         # https://github.com/python/mypy/issues/10740\n-        self._open: Callable[\n-            Concatenate[Union[str, os.PathLike], str, ...], IO[bytes]\n-        ] = (\n+        self._open: Callable[Concatenate[str | os.PathLike, str, ...], IO[bytes]] = (\n             gzip.open if self.use_gzip else open  # type: ignore[assignment]\n         )\n \n@@ -339,7 +335,7 @@ class FilesystemCacheStorage:\n     def close_spider(self, spider: Spider) -> None:\n         pass\n \n-    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n+    def retrieve_response(self, spider: Spider, request: Request) -> Response | None:\n         \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n         metadata = self._read_meta(spider, request)\n         if metadata is None:\n@@ -387,7 +383,7 @@ class FilesystemCacheStorage:\n         key = self._fingerprinter.fingerprint(request).hex()\n         return str(Path(self.cachedir, spider.name, key[0:2], key))\n \n-    def _read_meta(self, spider: Spider, request: Request) -> Optional[dict[str, Any]]:\n+    def _read_meta(self, spider: Spider, request: Request) -> dict[str, Any] | None:\n         rpath = Path(self._get_request_path(spider, request))\n         metapath = rpath / \"pickled_meta\"\n         if not metapath.exists():\n@@ -399,7 +395,7 @@ class FilesystemCacheStorage:\n             return cast(dict[str, Any], pickle.load(f))  # nosec\n \n \n-def parse_cachecontrol(header: bytes) -> dict[bytes, Optional[bytes]]:\n+def parse_cachecontrol(header: bytes) -> dict[bytes, bytes | None]:\n     \"\"\"Parse Cache-Control header\n \n     https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n@@ -419,7 +415,7 @@ def parse_cachecontrol(header: bytes) -> dict[bytes, Optional[bytes]]:\n     return directives\n \n \n-def rfc1123_to_epoch(date_str: Union[str, bytes, None]) -> Optional[int]:\n+def rfc1123_to_epoch(date_str: str | bytes | None) -> int | None:\n     try:\n         date_str = to_unicode(date_str, encoding=\"ascii\")  # type: ignore[arg-type]\n         return mktime_tz(parsedate_tz(date_str))  # type: ignore[arg-type]\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from twisted.internet import task\n \n@@ -29,7 +29,7 @@ class LogStats:\n         self.stats: StatsCollector = stats\n         self.interval: float = interval\n         self.multiplier: float = 60.0 / self.interval\n-        self.task: Optional[task.LoopingCall] = None\n+        self.task: task.LoopingCall | None = None\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n@@ -81,7 +81,7 @@ class LogStats:\n \n     def calculate_final_stats(\n         self, spider: Spider\n-    ) -> Union[tuple[None, None], tuple[float, float]]:\n+    ) -> tuple[None, None] | tuple[float, float]:\n         start_time = self.stats.get_value(\"start_time\")\n         finished_time = self.stats.get_value(\"finished_time\")\n \n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n from datetime import datetime, timezone\n from json import JSONEncoder\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from twisted.internet import task\n \n@@ -36,7 +36,7 @@ class PeriodicLog:\n         self.stats: StatsCollector = stats\n         self.interval: float = interval\n         self.multiplier: float = 60.0 / self.interval\n-        self.task: Optional[task.LoopingCall] = None\n+        self.task: task.LoopingCall | None = None\n         self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n         self.ext_stats_enabled: bool = bool(ext_stats)\n         self.ext_stats_include: list[str] = ext_stats.get(\"include\", [])\n@@ -52,7 +52,7 @@ class PeriodicLog:\n         if not interval:\n             raise NotConfigured\n         try:\n-            ext_stats: Optional[dict[str, Any]] = crawler.settings.getdict(\n+            ext_stats: dict[str, Any] | None = crawler.settings.getdict(\n                 \"PERIODIC_LOG_STATS\"\n             )\n         except (TypeError, ValueError):\n@@ -62,7 +62,7 @@ class PeriodicLog:\n                 else None\n             )\n         try:\n-            ext_delta: Optional[dict[str, Any]] = crawler.settings.getdict(\n+            ext_delta: dict[str, Any] | None = crawler.settings.getdict(\n                 \"PERIODIC_LOG_DELTA\"\n             )\n         except (TypeError, ValueError):\n@@ -93,8 +93,8 @@ class PeriodicLog:\n \n     def spider_opened(self, spider: Spider) -> None:\n         self.time_prev: datetime = datetime.now(tz=timezone.utc)\n-        self.delta_prev: dict[str, Union[int, float]] = {}\n-        self.stats_prev: dict[str, Union[int, float]] = {}\n+        self.delta_prev: dict[str, int | float] = {}\n+        self.stats_prev: dict[str, int | float] = {}\n \n         self.task = task.LoopingCall(self.log)\n         self.task.start(self.interval)\n@@ -110,7 +110,7 @@ class PeriodicLog:\n         logger.info(self.encoder.encode(data))\n \n     def log_delta(self) -> dict[str, Any]:\n-        num_stats: dict[str, Union[int, float]] = {\n+        num_stats: dict[str, int | float] = {\n             k: v\n             for k, v in self.stats._stats.items()\n             if isinstance(v, (int, float))\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import pickle  # nosec\n from pathlib import Path\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -18,8 +18,8 @@ if TYPE_CHECKING:\n class SpiderState:\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n-    def __init__(self, jobdir: Optional[str] = None):\n-        self.jobdir: Optional[str] = jobdir\n+    def __init__(self, jobdir: str | None = None):\n+        self.jobdir: str | None = jobdir\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n\n@@ -6,7 +6,7 @@ Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -39,7 +39,7 @@ class StatsMailer:\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_closed(self, spider: Spider) -> Optional[Deferred[None]]:\n+    def spider_closed(self, spider: Spider) -> Deferred[None] | None:\n         spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n         body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -90,8 +90,8 @@ class AutoThrottle:\n \n     def _get_slot(\n         self, request: Request, spider: Spider\n-    ) -> tuple[Optional[str], Optional[Slot]]:\n-        key: Optional[str] = request.meta.get(\"download_slot\")\n+    ) -> tuple[str | None, Slot | None]:\n+        key: str | None = request.meta.get(\"download_slot\")\n         if key is None:\n             return None, None\n         assert self.crawler.engine\n\n@@ -5,7 +5,7 @@ import time\n from http.cookiejar import Cookie\n from http.cookiejar import CookieJar as _CookieJar\n from http.cookiejar import CookiePolicy, DefaultCookiePolicy\n-from typing import TYPE_CHECKING, Any, Optional, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n@@ -28,7 +28,7 @@ IPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n class CookieJar:\n     def __init__(\n         self,\n-        policy: Optional[CookiePolicy] = None,\n+        policy: CookiePolicy | None = None,\n         check_expired_frequency: int = 10000,\n     ):\n         self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n@@ -83,9 +83,9 @@ class CookieJar:\n \n     def clear(\n         self,\n-        domain: Optional[str] = None,\n-        path: Optional[str] = None,\n-        name: Optional[str] = None,\n+        domain: str | None = None,\n+        path: str | None = None,\n+        name: str | None = None,\n     ) -> None:\n         self.jar.clear(domain, path, name)\n \n@@ -188,7 +188,7 @@ class WrappedRequest:\n     def has_header(self, name: str) -> bool:\n         return name in self.request.headers\n \n-    def get_header(self, name: str, default: Optional[str] = None) -> Optional[str]:\n+    def get_header(self, name: str, default: str | None = None) -> str | None:\n         value = self.request.headers.get(name, default)\n         return to_unicode(value, errors=\"replace\") if value is not None else None\n \n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, AnyStr, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, AnyStr, Union, cast\n \n from w3lib.http import headers_dict_to_raw\n \n@@ -25,14 +25,14 @@ class Headers(CaselessDict):\n \n     def __init__(\n         self,\n-        seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n+        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n         encoding: str = \"utf-8\",\n     ):\n         self.encoding: str = encoding\n         super().__init__(seq)\n \n     def update(  # type: ignore[override]\n-        self, seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]]]\n+        self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]\n     ) -> None:\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq: dict[bytes, list[bytes]] = {}\n@@ -44,7 +44,7 @@ class Headers(CaselessDict):\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n \n-    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> list[bytes]:\n+    def normvalue(self, value: _RawValueT | Iterable[_RawValueT]) -> list[bytes]:\n         \"\"\"Normalize values to bytes\"\"\"\n         _value: Iterable[_RawValueT]\n         if value is None:\n@@ -67,13 +67,13 @@ class Headers(CaselessDict):\n             return str(x).encode(self.encoding)\n         raise TypeError(f\"Unsupported value type: {type(x)}\")\n \n-    def __getitem__(self, key: AnyStr) -> Optional[bytes]:\n+    def __getitem__(self, key: AnyStr) -> bytes | None:\n         try:\n             return cast(list[bytes], super().__getitem__(key))[-1]\n         except IndexError:\n             return None\n \n-    def get(self, key: AnyStr, def_val: Any = None) -> Optional[bytes]:\n+    def get(self, key: AnyStr, def_val: Any = None) -> bytes | None:\n         try:\n             return cast(list[bytes], super().get(key, def_val))[-1]\n         except IndexError:\n@@ -103,7 +103,7 @@ class Headers(CaselessDict):\n     def items(self) -> Iterable[tuple[bytes, list[bytes]]]:  # type: ignore[override]\n         return ((k, self.getlist(k)) for k in self.keys())\n \n-    def values(self) -> list[Optional[bytes]]:  # type: ignore[override]\n+    def values(self) -> list[bytes | None]:  # type: ignore[override]\n         return [\n             self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n         ]\n\n@@ -13,7 +13,6 @@ from typing import (\n     Any,\n     AnyStr,\n     NoReturn,\n-    Optional,\n     TypedDict,\n     TypeVar,\n     Union,\n@@ -112,18 +111,18 @@ class Request(object_ref):\n     def __init__(\n         self,\n         url: str,\n-        callback: Optional[CallbackT] = None,\n+        callback: CallbackT | None = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n-        body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[CookiesT] = None,\n-        meta: Optional[dict[str, Any]] = None,\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n+        body: bytes | str | None = None,\n+        cookies: CookiesT | None = None,\n+        meta: dict[str, Any] | None = None,\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable[[Failure], Any]] = None,\n-        flags: Optional[list[str]] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n+        errback: Callable[[Failure], Any] | None = None,\n+        flags: list[str] | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n     ) -> None:\n         self._encoding: str = encoding  # this one has to be set first\n         self.method: str = str(method).upper()\n@@ -139,17 +138,15 @@ class Request(object_ref):\n             )\n         if not (callable(errback) or errback is None):\n             raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n-        self.callback: Optional[CallbackT] = callback\n-        self.errback: Optional[Callable[[Failure], Any]] = errback\n+        self.callback: CallbackT | None = callback\n+        self.errback: Callable[[Failure], Any] | None = errback\n \n         self.cookies: CookiesT = cookies or {}\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n         self.dont_filter: bool = dont_filter\n \n-        self._meta: Optional[dict[str, Any]] = dict(meta) if meta else None\n-        self._cb_kwargs: Optional[dict[str, Any]] = (\n-            dict(cb_kwargs) if cb_kwargs else None\n-        )\n+        self._meta: dict[str, Any] | None = dict(meta) if meta else None\n+        self._cb_kwargs: dict[str, Any] | None = dict(cb_kwargs) if cb_kwargs else None\n         self.flags: list[str] = [] if flags is None else list(flags)\n \n     @property\n@@ -186,7 +183,7 @@ class Request(object_ref):\n     def body(self) -> bytes:\n         return self._body\n \n-    def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n+    def _set_body(self, body: str | bytes | None) -> None:\n         self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n \n     @property\n@@ -208,7 +205,7 @@ class Request(object_ref):\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[type[Request]] = None, **kwargs: Any\n+        self, *args: Any, cls: type[Request] | None = None, **kwargs: Any\n     ) -> Request:\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n@@ -255,7 +252,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> dict[str, Any]:\n+    def to_dict(self, *, spider: scrapy.Spider | None = None) -> dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n@@ -62,14 +62,14 @@ class FormRequest(Request):\n     def from_response(\n         cls,\n         response: TextResponse,\n-        formname: Optional[str] = None,\n-        formid: Optional[str] = None,\n+        formname: str | None = None,\n+        formid: str | None = None,\n         formnumber: int = 0,\n         formdata: FormdataType = None,\n-        clickdata: Optional[dict[str, Union[str, int]]] = None,\n+        clickdata: dict[str, str | int] | None = None,\n         dont_click: bool = False,\n-        formxpath: Optional[str] = None,\n-        formcss: Optional[str] = None,\n+        formxpath: str | None = None,\n+        formcss: str | None = None,\n         **kwargs: Any,\n     ) -> Self:\n         kwargs.setdefault(\"encoding\", response.encoding)\n@@ -92,7 +92,7 @@ class FormRequest(Request):\n         return cls(url=url, method=method, formdata=formdata, **kwargs)\n \n \n-def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n+def _get_form_url(form: FormElement, url: str | None) -> str:\n     assert form.base_url is not None  # typing\n     if url is None:\n         action = form.get(\"action\")\n@@ -113,10 +113,10 @@ def _urlencode(seq: Iterable[FormdataKVType], enc: str) -> str:\n \n def _get_form(\n     response: TextResponse,\n-    formname: Optional[str],\n-    formid: Optional[str],\n+    formname: str | None,\n+    formid: str | None,\n     formnumber: int,\n-    formxpath: Optional[str],\n+    formxpath: str | None,\n ) -> FormElement:\n     \"\"\"Find the wanted form element within the given response.\"\"\"\n     root = response.selector.root\n@@ -160,7 +160,7 @@ def _get_inputs(\n     form: FormElement,\n     formdata: FormdataType,\n     dont_click: bool,\n-    clickdata: Optional[dict[str, Union[str, int]]],\n+    clickdata: dict[str, str | int] | None,\n ) -> list[FormdataKVType]:\n     \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n     try:\n@@ -196,8 +196,8 @@ def _get_inputs(\n \n \n def _value(\n-    ele: Union[InputElement, SelectElement, TextareaElement]\n-) -> tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n+    ele: InputElement | SelectElement | TextareaElement,\n+) -> tuple[str | None, None | str | MultipleSelectOptions]:\n     n = ele.name\n     v = ele.value\n     if ele.tag == \"select\":\n@@ -206,8 +206,8 @@ def _value(\n \n \n def _select_value(\n-    ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]\n-) -> tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n+    ele: SelectElement, n: str | None, v: None | str | MultipleSelectOptions\n+) -> tuple[str | None, None | str | MultipleSelectOptions]:\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n@@ -218,8 +218,8 @@ def _select_value(\n \n \n def _get_clickable(\n-    clickdata: Optional[dict[str, Union[str, int]]], form: FormElement\n-) -> Optional[tuple[str, str]]:\n+    clickdata: dict[str, str | int] | None, form: FormElement\n+) -> tuple[str, str] | None:\n     \"\"\"\n     Returns the clickable element specified in clickdata,\n     if the latter is given. If not, it returns the first\n\n@@ -10,7 +10,7 @@ from __future__ import annotations\n import copy\n import json\n import warnings\n-from typing import TYPE_CHECKING, Any, Optional, overload\n+from typing import TYPE_CHECKING, Any, overload\n \n from scrapy.http.request import Request, RequestTypeVar\n \n@@ -23,7 +23,7 @@ class JsonRequest(Request):\n     attributes: tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n     def __init__(\n-        self, *args: Any, dumps_kwargs: Optional[dict[str, Any]] = None, **kwargs: Any\n+        self, *args: Any, dumps_kwargs: dict[str, Any] | None = None, **kwargs: Any\n     ) -> None:\n         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n         dumps_kwargs.setdefault(\"sort_keys\", True)\n@@ -59,7 +59,7 @@ class JsonRequest(Request):\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[type[Request]] = None, **kwargs: Any\n+        self, *args: Any, cls: type[Request] | None = None, **kwargs: Any\n     ) -> Request:\n         body_passed = kwargs.get(\"body\", None) is not None\n         data: Any = kwargs.pop(\"data\", None)\n\n@@ -5,8 +5,10 @@ This module implements the XmlRpcRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import xmlrpc.client as xmlrpclib\n-from typing import Any, Optional\n+from typing import Any\n \n import defusedxml.xmlrpc\n \n@@ -19,7 +21,7 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n \n class XmlRpcRequest(Request):\n-    def __init__(self, *args: Any, encoding: Optional[str] = None, **kwargs: Any):\n+    def __init__(self, *args: Any, encoding: str | None = None, **kwargs: Any):\n         if \"body\" not in kwargs and \"params\" in kwargs:\n             kw = {k: kwargs.pop(k) for k in DUMPS_ARGS if k in kwargs}\n             kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, AnyStr, Optional, TypeVar, Union, overload\n+from typing import TYPE_CHECKING, Any, AnyStr, TypeVar, overload\n from urllib.parse import urljoin\n \n from scrapy.exceptions import NotSupported\n@@ -60,23 +60,23 @@ class Response(object_ref):\n         self,\n         url: str,\n         status: int = 200,\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n         body: bytes = b\"\",\n-        flags: Optional[list[str]] = None,\n-        request: Optional[Request] = None,\n-        certificate: Optional[Certificate] = None,\n-        ip_address: Union[IPv4Address, IPv6Address, None] = None,\n-        protocol: Optional[str] = None,\n+        flags: list[str] | None = None,\n+        request: Request | None = None,\n+        certificate: Certificate | None = None,\n+        ip_address: IPv4Address | IPv6Address | None = None,\n+        protocol: str | None = None,\n     ):\n         self.headers: Headers = Headers(headers or {})\n         self.status: int = int(status)\n         self._set_body(body)\n         self._set_url(url)\n-        self.request: Optional[Request] = request\n+        self.request: Request | None = request\n         self.flags: list[str] = [] if flags is None else list(flags)\n-        self.certificate: Optional[Certificate] = certificate\n-        self.ip_address: Union[IPv4Address, IPv6Address, None] = ip_address\n-        self.protocol: Optional[str] = protocol\n+        self.certificate: Certificate | None = certificate\n+        self.ip_address: IPv4Address | IPv6Address | None = ip_address\n+        self.protocol: str | None = protocol\n \n     @property\n     def cb_kwargs(self) -> dict[str, Any]:\n@@ -114,7 +114,7 @@ class Response(object_ref):\n     def body(self) -> bytes:\n         return self._body\n \n-    def _set_body(self, body: Optional[bytes]) -> None:\n+    def _set_body(self, body: bytes | None) -> None:\n         if body is None:\n             self._body = b\"\"\n         elif not isinstance(body, bytes):\n@@ -142,7 +142,7 @@ class Response(object_ref):\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[type[Response]] = None, **kwargs: Any\n+        self, *args: Any, cls: type[Response] | None = None, **kwargs: Any\n     ) -> Response:\n         \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n@@ -183,19 +183,19 @@ class Response(object_ref):\n \n     def follow(\n         self,\n-        url: Union[str, Link],\n-        callback: Optional[CallbackT] = None,\n+        url: str | Link,\n+        callback: CallbackT | None = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n-        body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[CookiesT] = None,\n-        meta: Optional[dict[str, Any]] = None,\n-        encoding: Optional[str] = \"utf-8\",\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n+        body: bytes | str | None = None,\n+        cookies: CookiesT | None = None,\n+        meta: dict[str, Any] | None = None,\n+        encoding: str | None = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n-        flags: Optional[list[str]] = None,\n+        errback: Callable[[Failure], Any] | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n+        flags: list[str] | None = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -236,19 +236,19 @@ class Response(object_ref):\n \n     def follow_all(\n         self,\n-        urls: Iterable[Union[str, Link]],\n-        callback: Optional[CallbackT] = None,\n+        urls: Iterable[str | Link],\n+        callback: CallbackT | None = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n-        body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[CookiesT] = None,\n-        meta: Optional[dict[str, Any]] = None,\n-        encoding: Optional[str] = \"utf-8\",\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n+        body: bytes | str | None = None,\n+        cookies: CookiesT | None = None,\n+        meta: dict[str, Any] | None = None,\n+        encoding: str | None = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n-        flags: Optional[list[str]] = None,\n+        errback: Callable[[Failure], Any] | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n+        flags: list[str] | None = None,\n     ) -> Iterable[Request]:\n         \"\"\"\n         .. versionadded:: 2.0\n\n@@ -8,9 +8,8 @@ See documentation in docs/topics/request-response.rst\n from __future__ import annotations\n \n import json\n-from collections.abc import Iterable\n from contextlib import suppress\n-from typing import TYPE_CHECKING, Any, AnyStr, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, AnyStr, cast\n from urllib.parse import urljoin\n \n import parsel\n@@ -24,16 +23,16 @@ from w3lib.encoding import (\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.response import Response\n-from scrapy.link import Link\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n-    from collections.abc import Callable, Mapping\n+    from collections.abc import Callable, Iterable, Mapping\n \n     from twisted.python.failure import Failure\n \n     from scrapy.http.request import CallbackT, CookiesT, Request\n+    from scrapy.link import Link\n     from scrapy.selector import Selector, SelectorList\n \n \n@@ -47,13 +46,13 @@ class TextResponse(Response):\n     attributes: tuple[str, ...] = Response.attributes + (\"encoding\",)\n \n     def __init__(self, *args: Any, **kwargs: Any):\n-        self._encoding: Optional[str] = kwargs.pop(\"encoding\", None)\n-        self._cached_benc: Optional[str] = None\n-        self._cached_ubody: Optional[str] = None\n-        self._cached_selector: Optional[Selector] = None\n+        self._encoding: str | None = kwargs.pop(\"encoding\", None)\n+        self._cached_benc: str | None = None\n+        self._cached_ubody: str | None = None\n+        self._cached_selector: Selector | None = None\n         super().__init__(*args, **kwargs)\n \n-    def _set_body(self, body: Union[str, bytes, None]) -> None:\n+    def _set_body(self, body: str | bytes | None) -> None:\n         self._body: bytes = b\"\"  # used by encoding detection\n         if isinstance(body, str):\n             if self._encoding is None:\n@@ -69,7 +68,7 @@ class TextResponse(Response):\n     def encoding(self) -> str:\n         return self._declared_encoding() or self._body_inferred_encoding()\n \n-    def _declared_encoding(self) -> Optional[str]:\n+    def _declared_encoding(self) -> str | None:\n         return (\n             self._encoding\n             or self._bom_encoding()\n@@ -104,7 +103,7 @@ class TextResponse(Response):\n         return urljoin(get_base_url(self), url)\n \n     @memoizemethod_noargs\n-    def _headers_encoding(self) -> Optional[str]:\n+    def _headers_encoding(self) -> str | None:\n         content_type = cast(bytes, self.headers.get(b\"Content-Type\", b\"\"))\n         return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n@@ -123,7 +122,7 @@ class TextResponse(Response):\n             self._cached_ubody = ubody\n         return self._cached_benc\n \n-    def _auto_detect_fun(self, text: bytes) -> Optional[str]:\n+    def _auto_detect_fun(self, text: bytes) -> str | None:\n         for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n             try:\n                 text.decode(enc)\n@@ -133,11 +132,11 @@ class TextResponse(Response):\n         return None\n \n     @memoizemethod_noargs\n-    def _body_declared_encoding(self) -> Optional[str]:\n+    def _body_declared_encoding(self) -> str | None:\n         return html_body_declared_encoding(self.body)\n \n     @memoizemethod_noargs\n-    def _bom_encoding(self) -> Optional[str]:\n+    def _bom_encoding(self) -> str | None:\n         return read_bom(self.body)[0]\n \n     @property\n@@ -170,19 +169,19 @@ class TextResponse(Response):\n \n     def follow(\n         self,\n-        url: Union[str, Link, parsel.Selector],\n-        callback: Optional[CallbackT] = None,\n+        url: str | Link | parsel.Selector,\n+        callback: CallbackT | None = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n-        body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[CookiesT] = None,\n-        meta: Optional[dict[str, Any]] = None,\n-        encoding: Optional[str] = None,\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n+        body: bytes | str | None = None,\n+        cookies: CookiesT | None = None,\n+        meta: dict[str, Any] | None = None,\n+        encoding: str | None = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n-        flags: Optional[list[str]] = None,\n+        errback: Callable[[Failure], Any] | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n+        flags: list[str] | None = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -223,21 +222,21 @@ class TextResponse(Response):\n \n     def follow_all(\n         self,\n-        urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n-        callback: Optional[CallbackT] = None,\n+        urls: Iterable[str | Link] | parsel.SelectorList | None = None,\n+        callback: CallbackT | None = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n-        body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[CookiesT] = None,\n-        meta: Optional[dict[str, Any]] = None,\n-        encoding: Optional[str] = None,\n+        headers: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n+        body: bytes | str | None = None,\n+        cookies: CookiesT | None = None,\n+        meta: dict[str, Any] | None = None,\n+        encoding: str | None = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n-        flags: Optional[list[str]] = None,\n-        css: Optional[str] = None,\n-        xpath: Optional[str] = None,\n+        errback: Callable[[Failure], Any] | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n+        flags: list[str] | None = None,\n+        css: str | None = None,\n+        xpath: str | None = None,\n     ) -> Iterable[Request]:\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n@@ -279,7 +278,7 @@ class TextResponse(Response):\n                 with suppress(_InvalidSelector):\n                     urls.append(_url_from_selector(sel))\n         return super().follow_all(\n-            urls=cast(Iterable[Union[str, Link]], urls),\n+            urls=cast(\"Iterable[str | Link]\", urls),\n             callback=callback,\n             method=method,\n             headers=headers,\n\n@@ -9,7 +9,7 @@ import operator\n import re\n from collections.abc import Callable, Iterable\n from functools import partial\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Union, cast\n from urllib.parse import urljoin, urlparse\n \n from lxml import etree  # nosec\n@@ -58,9 +58,9 @@ def _canonicalize_link_url(link: Link) -> str:\n class LxmlParserLinkExtractor:\n     def __init__(\n         self,\n-        tag: Union[str, Callable[[str], bool]] = \"a\",\n-        attr: Union[str, Callable[[str], bool]] = \"href\",\n-        process: Optional[Callable[[Any], Any]] = None,\n+        tag: str | Callable[[str], bool] = \"a\",\n+        attr: str | Callable[[str], bool] = \"href\",\n+        process: Callable[[Any], Any] | None = None,\n         unique: bool = False,\n         strip: bool = True,\n         canonicalized: bool = False,\n@@ -166,18 +166,18 @@ class LxmlLinkExtractor:\n         self,\n         allow: _RegexOrSeveralT = (),\n         deny: _RegexOrSeveralT = (),\n-        allow_domains: Union[str, Iterable[str]] = (),\n-        deny_domains: Union[str, Iterable[str]] = (),\n-        restrict_xpaths: Union[str, Iterable[str]] = (),\n-        tags: Union[str, Iterable[str]] = (\"a\", \"area\"),\n-        attrs: Union[str, Iterable[str]] = (\"href\",),\n+        allow_domains: str | Iterable[str] = (),\n+        deny_domains: str | Iterable[str] = (),\n+        restrict_xpaths: str | Iterable[str] = (),\n+        tags: str | Iterable[str] = (\"a\", \"area\"),\n+        attrs: str | Iterable[str] = (\"href\",),\n         canonicalize: bool = False,\n         unique: bool = True,\n-        process_value: Optional[Callable[[Any], Any]] = None,\n-        deny_extensions: Union[str, Iterable[str], None] = None,\n-        restrict_css: Union[str, Iterable[str]] = (),\n+        process_value: Callable[[Any], Any] | None = None,\n+        deny_extensions: str | Iterable[str] | None = None,\n+        restrict_css: str | Iterable[str] = (),\n         strip: bool = True,\n-        restrict_text: Optional[_RegexOrSeveralT] = None,\n+        restrict_text: _RegexOrSeveralT | None = None,\n     ):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         self.link_extractor = LxmlParserLinkExtractor(\n@@ -206,7 +206,7 @@ class LxmlLinkExtractor:\n         self.restrict_text: list[re.Pattern[str]] = self._compile_regexes(restrict_text)\n \n     @staticmethod\n-    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> list[re.Pattern[str]]:\n+    def _compile_regexes(value: _RegexOrSeveralT | None) -> list[re.Pattern[str]]:\n         return [\n             x if isinstance(x, re.Pattern) else re.compile(x)\n             for x in arg_to_iter(value)\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/loaders.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n import itemloaders\n \n@@ -92,9 +92,9 @@ class ItemLoader(itemloaders.ItemLoader):\n     def __init__(\n         self,\n         item: Any = None,\n-        selector: Optional[Selector] = None,\n-        response: Optional[TextResponse] = None,\n-        parent: Optional[itemloaders.ItemLoader] = None,\n+        selector: Selector | None = None,\n+        response: TextResponse | None = None,\n+        parent: itemloaders.ItemLoader | None = None,\n         **context: Any,\n     ):\n         if selector is None and response is not None:\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n import os\n-from typing import TYPE_CHECKING, Any, Optional, TypedDict, Union\n+from typing import TYPE_CHECKING, Any, TypedDict\n \n from twisted.python.failure import Failure\n \n@@ -31,7 +31,7 @@ DOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n class LogFormatterResult(TypedDict):\n     level: int\n     msg: str\n-    args: Union[dict[str, Any], tuple[Any, ...]]\n+    args: dict[str, Any] | tuple[Any, ...]\n \n \n class LogFormatter:\n@@ -93,7 +93,7 @@ class LogFormatter:\n         }\n \n     def scraped(\n-        self, item: Any, response: Union[Response, Failure, None], spider: Spider\n+        self, item: Any, response: Response | Failure | None, spider: Spider\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n         src: Any\n@@ -116,7 +116,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Optional[Response],\n+        response: Response | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n@@ -133,7 +133,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Optional[Response],\n+        response: Response | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item causes an error while it is passing\n@@ -153,7 +153,7 @@ class LogFormatter:\n         self,\n         failure: Failure,\n         request: Request,\n-        response: Union[Response, Failure],\n+        response: Response | Failure,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs an error message from a spider.\n@@ -174,7 +174,7 @@ class LogFormatter:\n         failure: Failure,\n         request: Request,\n         spider: Spider,\n-        errmsg: Optional[str] = None,\n+        errmsg: str | None = None,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a download error message from a spider (typically coming from\n         the engine).\n\n@@ -14,7 +14,7 @@ from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n-from typing import IO, TYPE_CHECKING, Any, Optional, Union\n+from typing import IO, TYPE_CHECKING, Any\n \n from twisted import version as twisted_version\n from twisted.internet import ssl\n@@ -45,7 +45,7 @@ logger = logging.getLogger(__name__)\n COMMASPACE = \", \"\n \n \n-def _to_bytes_or_none(text: Union[str, bytes, None]) -> Optional[bytes]:\n+def _to_bytes_or_none(text: str | bytes | None) -> bytes | None:\n     if text is None:\n         return None\n     return to_bytes(text)\n@@ -56,8 +56,8 @@ class MailSender:\n         self,\n         smtphost: str = \"localhost\",\n         mailfrom: str = \"scrapy@localhost\",\n-        smtpuser: Optional[str] = None,\n-        smtppass: Optional[str] = None,\n+        smtpuser: str | None = None,\n+        smtppass: str | None = None,\n         smtpport: int = 25,\n         smtptls: bool = False,\n         smtpssl: bool = False,\n@@ -65,8 +65,8 @@ class MailSender:\n     ):\n         self.smtphost: str = smtphost\n         self.smtpport: int = smtpport\n-        self.smtpuser: Optional[bytes] = _to_bytes_or_none(smtpuser)\n-        self.smtppass: Optional[bytes] = _to_bytes_or_none(smtppass)\n+        self.smtpuser: bytes | None = _to_bytes_or_none(smtpuser)\n+        self.smtppass: bytes | None = _to_bytes_or_none(smtppass)\n         self.smtptls: bool = smtptls\n         self.smtpssl: bool = smtpssl\n         self.mailfrom: str = mailfrom\n@@ -86,15 +86,15 @@ class MailSender:\n \n     def send(\n         self,\n-        to: Union[str, list[str]],\n+        to: str | list[str],\n         subject: str,\n         body: str,\n-        cc: Union[str, list[str], None] = None,\n+        cc: str | list[str] | None = None,\n         attachs: Sequence[tuple[str, str, IO[Any]]] = (),\n         mimetype: str = \"text/plain\",\n-        charset: Optional[str] = None,\n-        _callback: Optional[Callable[..., None]] = None,\n-    ) -> Optional[Deferred[None]]:\n+        charset: str | None = None,\n+        _callback: Callable[..., None] | None = None,\n+    ) -> Deferred[None] | None:\n         from twisted.internet import reactor\n \n         msg: MIMEBase\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.defer import process_chain, process_parallel\n@@ -40,9 +40,9 @@ class MiddlewareManager:\n         self.middlewares = middlewares\n         # Only process_spider_output and process_spider_exception can be None.\n         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n-        self.methods: dict[\n-            str, deque[Union[None, Callable, tuple[Callable, Callable]]]\n-        ] = defaultdict(deque)\n+        self.methods: dict[str, deque[None | Callable | tuple[Callable, Callable]]] = (\n+            defaultdict(deque)\n+        )\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n@@ -51,9 +51,7 @@ class MiddlewareManager:\n         raise NotImplementedError\n \n     @classmethod\n-    def from_settings(\n-        cls, settings: Settings, crawler: Optional[Crawler] = None\n-    ) -> Self:\n+    def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n\n@@ -17,17 +17,7 @@ from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n from pathlib import Path\n-from typing import (\n-    IO,\n-    TYPE_CHECKING,\n-    Any,\n-    NoReturn,\n-    Optional,\n-    Protocol,\n-    TypedDict,\n-    Union,\n-    cast,\n-)\n+from typing import IO, TYPE_CHECKING, Any, NoReturn, Protocol, TypedDict, cast\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -61,7 +51,7 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, PathLike[str]]) -> str:\n+def _to_string(path: str | PathLike[str]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n@@ -99,17 +89,17 @@ class FilesStoreProtocol(Protocol):\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[dict[str, Any]] = None,\n-        headers: Optional[dict[str, str]] = None,\n-    ) -> Optional[Deferred[Any]]: ...\n+        meta: dict[str, Any] | None = None,\n+        headers: dict[str, str] | None = None,\n+    ) -> Deferred[Any] | None: ...\n \n     def stat_file(\n         self, path: str, info: MediaPipeline.SpiderInfo\n-    ) -> Union[StatInfo, Deferred[StatInfo]]: ...\n+    ) -> StatInfo | Deferred[StatInfo]: ...\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, PathLike[str]]):\n+    def __init__(self, basedir: str | PathLike[str]):\n         basedir = _to_string(basedir)\n         if \"://\" in basedir:\n             basedir = basedir.split(\"://\", 1)[1]\n@@ -121,18 +111,18 @@ class FSFilesStore:\n \n     def persist_file(\n         self,\n-        path: Union[str, PathLike[str]],\n+        path: str | PathLike[str],\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[dict[str, Any]] = None,\n-        headers: Optional[dict[str, str]] = None,\n+        meta: dict[str, Any] | None = None,\n+        headers: dict[str, str] | None = None,\n     ) -> None:\n         absolute_path = self._get_filesystem_path(path)\n         self._mkdir(absolute_path.parent, info)\n         absolute_path.write_bytes(buf.getvalue())\n \n     def stat_file(\n-        self, path: Union[str, PathLike[str]], info: MediaPipeline.SpiderInfo\n+        self, path: str | PathLike[str], info: MediaPipeline.SpiderInfo\n     ) -> StatInfo:\n         absolute_path = self._get_filesystem_path(path)\n         try:\n@@ -145,12 +135,12 @@ class FSFilesStore:\n \n         return {\"last_modified\": last_modified, \"checksum\": checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, PathLike[str]]) -> Path:\n+    def _get_filesystem_path(self, path: str | PathLike[str]) -> Path:\n         path_comps = _to_string(path).split(\"/\")\n         return Path(self.basedir, *path_comps)\n \n     def _mkdir(\n-        self, dirname: Path, domain: Optional[MediaPipeline.SpiderInfo] = None\n+        self, dirname: Path, domain: MediaPipeline.SpiderInfo | None = None\n     ) -> None:\n         seen: set[str] = self.created_directories[domain] if domain else set()\n         if str(dirname) not in seen:\n@@ -218,8 +208,8 @@ class S3FilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[dict[str, Any]] = None,\n-        headers: Optional[dict[str, str]] = None,\n+        meta: dict[str, Any] | None = None,\n+        headers: dict[str, str] | None = None,\n     ) -> Deferred[Any]:\n         \"\"\"Upload file to S3 storage\"\"\"\n         key_name = f\"{self.prefix}{path}\"\n@@ -327,7 +317,7 @@ class GCSFilesStore:\n             deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess),\n         )\n \n-    def _get_content_type(self, headers: Optional[dict[str, str]]) -> str:\n+    def _get_content_type(self, headers: dict[str, str] | None) -> str:\n         if headers and \"Content-Type\" in headers:\n             return headers[\"Content-Type\"]\n         return \"application/octet-stream\"\n@@ -340,8 +330,8 @@ class GCSFilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[dict[str, Any]] = None,\n-        headers: Optional[dict[str, str]] = None,\n+        meta: dict[str, Any] | None = None,\n+        headers: dict[str, str] | None = None,\n     ) -> Deferred[Any]:\n         blob_path = self._get_blob_path(path)\n         blob = self.bucket.blob(blob_path)\n@@ -356,9 +346,9 @@ class GCSFilesStore:\n \n \n class FTPFilesStore:\n-    FTP_USERNAME: Optional[str] = None\n-    FTP_PASSWORD: Optional[str] = None\n-    USE_ACTIVE_MODE: Optional[bool] = None\n+    FTP_USERNAME: str | None = None\n+    FTP_PASSWORD: str | None = None\n+    USE_ACTIVE_MODE: bool | None = None\n \n     def __init__(self, uri: str):\n         if not uri.startswith(\"ftp://\"):\n@@ -380,8 +370,8 @@ class FTPFilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[dict[str, Any]] = None,\n-        headers: Optional[dict[str, str]] = None,\n+        meta: dict[str, Any] | None = None,\n+        headers: dict[str, str] | None = None,\n     ) -> Deferred[Any]:\n         path = f\"{self.basedir}/{path}\"\n         return deferToThread(\n@@ -450,9 +440,9 @@ class FilesPipeline(MediaPipeline):\n \n     def __init__(\n         self,\n-        store_uri: Union[str, PathLike[str]],\n-        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, dict[str, Any], None] = None,\n+        store_uri: str | PathLike[str],\n+        download_func: Callable[[Request, Spider], Response] | None = None,\n+        settings: Settings | dict[str, Any] | None = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n@@ -517,8 +507,8 @@ class FilesPipeline(MediaPipeline):\n \n     def media_to_download(\n         self, request: Request, info: MediaPipeline.SpiderInfo, *, item: Any = None\n-    ) -> Deferred[Optional[FileInfo]]:\n-        def _onsuccess(result: StatInfo) -> Optional[FileInfo]:\n+    ) -> Deferred[FileInfo | None]:\n+        def _onsuccess(result: StatInfo) -> FileInfo | None:\n             if not result:\n                 return None  # returning None force download\n \n@@ -551,7 +541,7 @@ class FilesPipeline(MediaPipeline):\n         path = self.file_path(request, info=info, item=item)\n         # maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n         dfd: Deferred[StatInfo] = maybeDeferred(self.store.stat_file, path, info)  # type: ignore[call-overload]\n-        dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n+        dfd2: Deferred[FileInfo | None] = dfd.addCallback(_onsuccess)\n         dfd2.addErrback(lambda _: None)\n         dfd2.addErrback(\n             lambda f: logger.error(\n@@ -684,8 +674,8 @@ class FilesPipeline(MediaPipeline):\n     def file_path(\n         self,\n         request: Request,\n-        response: Optional[Response] = None,\n-        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        response: Response | None = None,\n+        info: MediaPipeline.SpiderInfo | None = None,\n         *,\n         item: Any = None,\n     ) -> str:\n\n@@ -11,7 +11,7 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from itemadapter import ItemAdapter\n \n@@ -74,9 +74,9 @@ class ImagesPipeline(FilesPipeline):\n \n     def __init__(\n         self,\n-        store_uri: Union[str, PathLike[str]],\n-        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, dict[str, Any], None] = None,\n+        store_uri: str | PathLike[str],\n+        download_func: Callable[[Request, Spider], Response] | None = None,\n+        settings: Settings | dict[str, Any] | None = None,\n     ):\n         try:\n             from PIL import Image\n@@ -120,7 +120,7 @@ class ImagesPipeline(FilesPipeline):\n             resolve(\"IMAGES_THUMBS\"), self.THUMBS\n         )\n \n-        self._deprecated_convert_image: Optional[bool] = None\n+        self._deprecated_convert_image: bool | None = None\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n@@ -168,7 +168,7 @@ class ImagesPipeline(FilesPipeline):\n         *,\n         item: Any = None,\n     ) -> str:\n-        checksum: Optional[str] = None\n+        checksum: str | None = None\n         for path, image, buf in self.get_images(response, request, info, item=item):\n             if checksum is None:\n                 buf.seek(0)\n@@ -235,8 +235,8 @@ class ImagesPipeline(FilesPipeline):\n     def convert_image(\n         self,\n         image: Image.Image,\n-        size: Optional[tuple[int, int]] = None,\n-        response_body: Optional[BytesIO] = None,\n+        size: tuple[int, int] | None = None,\n+        response_body: BytesIO | None = None,\n     ) -> tuple[Image.Image, BytesIO]:\n         if response_body is None:\n             warnings.warn(\n@@ -291,8 +291,8 @@ class ImagesPipeline(FilesPipeline):\n     def file_path(\n         self,\n         request: Request,\n-        response: Optional[Response] = None,\n-        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        response: Response | None = None,\n+        info: MediaPipeline.SpiderInfo | None = None,\n         *,\n         item: Any = None,\n     ) -> str:\n@@ -303,8 +303,8 @@ class ImagesPipeline(FilesPipeline):\n         self,\n         request: Request,\n         thumb_id: str,\n-        response: Optional[Response] = None,\n-        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        response: Response | None = None,\n+        info: MediaPipeline.SpiderInfo | None = None,\n         *,\n         item: Any = None,\n     ) -> str:\n\n@@ -9,7 +9,6 @@ from typing import (\n     Any,\n     Literal,\n     NoReturn,\n-    Optional,\n     TypedDict,\n     TypeVar,\n     Union,\n@@ -44,7 +43,7 @@ _T = TypeVar(\"_T\")\n class FileInfo(TypedDict):\n     url: str\n     path: str\n-    checksum: Optional[str]\n+    checksum: str | None\n     status: str\n \n \n@@ -64,15 +63,15 @@ class MediaPipeline(ABC):\n         def __init__(self, spider: Spider):\n             self.spider: Spider = spider\n             self.downloading: set[bytes] = set()\n-            self.downloaded: dict[bytes, Union[FileInfo, Failure]] = {}\n+            self.downloaded: dict[bytes, FileInfo | Failure] = {}\n             self.waiting: defaultdict[bytes, list[Deferred[FileInfo]]] = defaultdict(\n                 list\n             )\n \n     def __init__(\n         self,\n-        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, dict[str, Any], None] = None,\n+        download_func: Callable[[Request, Spider], Response] | None = None,\n+        settings: Settings | dict[str, Any] | None = None,\n     ):\n         self.download_func = download_func\n \n@@ -94,8 +93,8 @@ class MediaPipeline(ABC):\n     def _key_for_pipe(\n         self,\n         key: str,\n-        base_class_name: Optional[str] = None,\n-        settings: Optional[Settings] = None,\n+        base_class_name: str | None = None,\n+        settings: Settings | None = None,\n     ) -> str:\n         class_name = self.__class__.__name__\n         formatted_key = f\"{class_name.upper()}_{key}\"\n@@ -161,7 +160,7 @@ class MediaPipeline(ABC):\n \n         # Download request checking media_to_download hook output first\n         info.downloading.add(fp)\n-        dfd: Deferred[Optional[FileInfo]] = mustbe_deferred(\n+        dfd: Deferred[FileInfo | None] = mustbe_deferred(\n             self.media_to_download, request, info, item=item\n         )\n         dfd2: Deferred[FileInfo] = dfd.addCallback(\n@@ -182,8 +181,8 @@ class MediaPipeline(ABC):\n             request.meta[\"handle_httpstatus_all\"] = True\n \n     def _check_media_to_download(\n-        self, result: Optional[FileInfo], request: Request, info: SpiderInfo, item: Any\n-    ) -> Union[FileInfo, Deferred[FileInfo]]:\n+        self, result: FileInfo | None, request: Request, info: SpiderInfo, item: Any\n+    ) -> FileInfo | Deferred[FileInfo]:\n         if result is not None:\n             return result\n         dfd: Deferred[Response]\n@@ -201,7 +200,7 @@ class MediaPipeline(ABC):\n         return dfd2\n \n     def _cache_result_and_execute_waiters(\n-        self, result: Union[FileInfo, Failure], fp: bytes, info: SpiderInfo\n+        self, result: FileInfo | Failure, fp: bytes, info: SpiderInfo\n     ) -> None:\n         if isinstance(result, Failure):\n             # minimize cached information for failure\n@@ -243,7 +242,7 @@ class MediaPipeline(ABC):\n     @abstractmethod\n     def media_to_download(\n         self, request: Request, info: SpiderInfo, *, item: Any = None\n-    ) -> Deferred[Optional[FileInfo]]:\n+    ) -> Deferred[FileInfo | None]:\n         \"\"\"Check request before starting download\"\"\"\n         raise NotImplementedError()\n \n@@ -291,8 +290,8 @@ class MediaPipeline(ABC):\n     def file_path(\n         self,\n         request: Request,\n-        response: Optional[Response] = None,\n-        info: Optional[SpiderInfo] = None,\n+        response: Response | None = None,\n+        info: SpiderInfo | None = None,\n         *,\n         item: Any = None,\n     ) -> str:\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import hashlib\n import logging\n-from typing import TYPE_CHECKING, Optional, Protocol, cast\n+from typing import TYPE_CHECKING, Protocol, cast\n \n from scrapy import Request\n from scrapy.core.downloader import Downloader\n@@ -42,7 +42,7 @@ class QueueProtocol(Protocol):\n \n     def push(self, request: Request) -> None: ...\n \n-    def pop(self) -> Optional[Request]: ...\n+    def pop(self) -> Request | None: ...\n \n     def close(self) -> None: ...\n \n@@ -96,7 +96,7 @@ class ScrapyPriorityQueue:\n         self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n         self.key: str = key\n         self.queues: dict[int, QueueProtocol] = {}\n-        self.curprio: Optional[int] = None\n+        self.curprio: int | None = None\n         self.init_prios(startprios)\n \n     def init_prios(self, startprios: Iterable[int]) -> None:\n@@ -127,7 +127,7 @@ class ScrapyPriorityQueue:\n         if self.curprio is None or priority < self.curprio:\n             self.curprio = priority\n \n-    def pop(self) -> Optional[Request]:\n+    def pop(self) -> Request | None:\n         if self.curprio is None:\n             return None\n         q = self.queues[self.curprio]\n@@ -139,7 +139,7 @@ class ScrapyPriorityQueue:\n             self.curprio = min(prios) if prios else None\n         return m\n \n-    def peek(self) -> Optional[Request]:\n+    def peek(self) -> Request | None:\n         \"\"\"Returns the next object to be returned by :meth:`pop`,\n         but without removing it from the queue.\n \n@@ -193,7 +193,7 @@ class DownloaderAwarePriorityQueue:\n         crawler: Crawler,\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n-        startprios: Optional[dict[str, Iterable[int]]] = None,\n+        startprios: dict[str, Iterable[int]] | None = None,\n     ) -> Self:\n         return cls(crawler, downstream_queue_cls, key, startprios)\n \n@@ -202,7 +202,7 @@ class DownloaderAwarePriorityQueue:\n         crawler: Crawler,\n         downstream_queue_cls: type[QueueProtocol],\n         key: str,\n-        slot_startprios: Optional[dict[str, Iterable[int]]] = None,\n+        slot_startprios: dict[str, Iterable[int]] | None = None,\n     ):\n         if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n             raise ValueError(\n@@ -239,7 +239,7 @@ class DownloaderAwarePriorityQueue:\n             startprios,\n         )\n \n-    def pop(self) -> Optional[Request]:\n+    def pop(self) -> Request | None:\n         stats = self._downloader_interface.stats(self.pqueues)\n \n         if not stats:\n@@ -259,7 +259,7 @@ class DownloaderAwarePriorityQueue:\n         queue = self.pqueues[slot]\n         queue.push(request)\n \n-    def peek(self) -> Optional[Request]:\n+    def peek(self) -> Request | None:\n         \"\"\"Returns the next object to be returned by :meth:`pop`,\n         but without removing it from the queue.\n \n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from twisted.internet import defer\n from twisted.internet.base import ReactorBase, ThreadedResolver\n@@ -128,7 +128,7 @@ class CachingHostnameResolver:\n         resolutionReceiver: IResolutionReceiver,\n         hostName: str,\n         portNumber: int = 0,\n-        addressTypes: Optional[Sequence[type[IAddress]]] = None,\n+        addressTypes: Sequence[type[IAddress]] | None = None,\n         transportSemantics: str = \"TCP\",\n     ) -> IHostResolution:\n         try:\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n from io import StringIO\n from mimetypes import MimeTypes\n from pkgutil import get_data\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n@@ -58,7 +58,7 @@ class ResponseTypes:\n         return self.classes.get(basetype, Response)\n \n     def from_content_type(\n-        self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None\n+        self, content_type: str | bytes, content_encoding: bytes | None = None\n     ) -> type[Response]:\n         \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n         header\"\"\"\n@@ -70,7 +70,7 @@ class ResponseTypes:\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(\n-        self, content_disposition: Union[str, bytes]\n+        self, content_disposition: str | bytes\n     ) -> type[Response]:\n         try:\n             filename = (\n@@ -123,10 +123,10 @@ class ResponseTypes:\n \n     def from_args(\n         self,\n-        headers: Optional[Mapping[bytes, bytes]] = None,\n-        url: Optional[str] = None,\n-        filename: Optional[str] = None,\n-        body: Optional[bytes] = None,\n+        headers: Mapping[bytes, bytes] | None = None,\n+        url: str | None = None,\n+        filename: str | None = None,\n+        body: bytes | None = None,\n     ) -> type[Response]:\n         \"\"\"Guess the most appropriate Response class based on\n         the given arguments.\"\"\"\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n import sys\n from abc import ABCMeta, abstractmethod\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n from warnings import warn\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -21,7 +21,7 @@ logger = logging.getLogger(__name__)\n \n \n def decode_robotstxt(\n-    robotstxt_body: bytes, spider: Optional[Spider], to_native_str_type: bool = False\n+    robotstxt_body: bytes, spider: Spider | None, to_native_str_type: bool = False\n ) -> str:\n     try:\n         if to_native_str_type:\n@@ -57,7 +57,7 @@ class RobotParser(metaclass=ABCMeta):\n         pass\n \n     @abstractmethod\n-    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n+    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n \n         :param url: Absolute URL\n@@ -70,10 +70,10 @@ class RobotParser(metaclass=ABCMeta):\n \n \n class PythonRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n+    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n         from urllib.robotparser import RobotFileParser\n \n-        self.spider: Optional[Spider] = spider\n+        self.spider: Spider | None = spider\n         body_decoded = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n         self.rp: RobotFileParser = RobotFileParser()\n         self.rp.parse(body_decoded.splitlines())\n@@ -84,18 +84,18 @@ class PythonRobotParser(RobotParser):\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n+    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.can_fetch(user_agent, url)\n \n \n class ReppyRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n+    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n         warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n         from reppy.robots import Robots\n \n-        self.spider: Optional[Spider] = spider\n+        self.spider: Spider | None = spider\n         self.rp = Robots.parse(\"\", robotstxt_body)\n \n     @classmethod\n@@ -104,15 +104,15 @@ class ReppyRobotParser(RobotParser):\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n+    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         return self.rp.allowed(url, user_agent)\n \n \n class RerpRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n+    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n         from robotexclusionrulesparser import RobotExclusionRulesParser\n \n-        self.spider: Optional[Spider] = spider\n+        self.spider: Spider | None = spider\n         self.rp: RobotExclusionRulesParser = RobotExclusionRulesParser()\n         body_decoded = decode_robotstxt(robotstxt_body, spider)\n         self.rp.parse(body_decoded)\n@@ -123,17 +123,17 @@ class RerpRobotParser(RobotParser):\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n+    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.is_allowed(user_agent, url)\n \n \n class ProtegoRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n+    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n         from protego import Protego\n \n-        self.spider: Optional[Spider] = spider\n+        self.spider: Spider | None = spider\n         body_decoded = decode_robotstxt(robotstxt_body, spider)\n         self.rp = Protego.parse(body_decoded)\n \n@@ -143,7 +143,7 @@ class ProtegoRobotParser(RobotParser):\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n+    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.can_fetch(url, user_agent)\n\n@@ -2,7 +2,9 @@\n XPath selectors based on lxml\n \"\"\"\n \n-from typing import Any, Optional, Union\n+from __future__ import annotations\n+\n+from typing import Any\n \n from parsel import Selector as _ParselSelector\n \n@@ -16,13 +18,13 @@ __all__ = [\"Selector\", \"SelectorList\"]\n _NOT_SET = object()\n \n \n-def _st(response: Optional[TextResponse], st: Optional[str]) -> str:\n+def _st(response: TextResponse | None, st: str | None) -> str:\n     if st is None:\n         return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n     return st\n \n \n-def _response_from_text(text: Union[str, bytes], st: Optional[str]) -> TextResponse:\n+def _response_from_text(text: str | bytes, st: str | None) -> TextResponse:\n     rt: type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n     return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n \n@@ -71,10 +73,10 @@ class Selector(_ParselSelector, object_ref):\n \n     def __init__(\n         self,\n-        response: Optional[TextResponse] = None,\n-        text: Optional[str] = None,\n-        type: Optional[str] = None,\n-        root: Optional[Any] = _NOT_SET,\n+        response: TextResponse | None = None,\n+        text: str | None = None,\n+        type: str | None = None,\n+        root: Any | None = _NOT_SET,\n         **kwargs: Any,\n     ):\n         if response is not None and text is not None:\n\n@@ -5,7 +5,7 @@ import json\n from collections.abc import Iterable, Iterator, Mapping, MutableMapping\n from importlib import import_module\n from pprint import pformat\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Union, cast\n \n from scrapy.settings import default_settings\n \n@@ -35,7 +35,7 @@ SETTINGS_PRIORITIES: dict[str, int] = {\n }\n \n \n-def get_settings_priority(priority: Union[int, str]) -> int:\n+def get_settings_priority(priority: int | str) -> int:\n     \"\"\"\n     Small helper function that looks up a given string priority in the\n     :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n@@ -97,9 +97,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n \n     __default = object()\n \n-    def __init__(\n-        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n-    ):\n+    def __init__(self, values: _SettingsInputT = None, priority: int | str = \"project\"):\n         self.frozen: bool = False\n         self.attributes: dict[_SettingsKeyT, SettingsAttribute] = {}\n         if values:\n@@ -180,7 +178,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         return float(self.get(name, default))\n \n     def getlist(\n-        self, name: _SettingsKeyT, default: Optional[list[Any]] = None\n+        self, name: _SettingsKeyT, default: list[Any] | None = None\n     ) -> list[Any]:\n         \"\"\"\n         Get a setting value as a list. If the setting original type is a list, a\n@@ -201,7 +199,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         return list(value)\n \n     def getdict(\n-        self, name: _SettingsKeyT, default: Optional[dict[Any, Any]] = None\n+        self, name: _SettingsKeyT, default: dict[Any, Any] | None = None\n     ) -> dict[Any, Any]:\n         \"\"\"\n         Get a setting value as a dictionary. If the setting original type is a\n@@ -226,8 +224,8 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n     def getdictorlist(\n         self,\n         name: _SettingsKeyT,\n-        default: Union[dict[Any, Any], list[Any], tuple[Any], None] = None,\n-    ) -> Union[dict[Any, Any], list[Any]]:\n+        default: dict[Any, Any] | list[Any] | tuple[Any] | None = None,\n+    ) -> dict[Any, Any] | list[Any]:\n         \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n         If the setting is already a dict or a list, a copy of it will be\n@@ -278,7 +276,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         compbs.update(self[name])\n         return compbs\n \n-    def getpriority(self, name: _SettingsKeyT) -> Optional[int]:\n+    def getpriority(self, name: _SettingsKeyT) -> int | None:\n         \"\"\"\n         Return the current numerical priority value of a setting, or ``None`` if\n         the given ``name`` does not exist.\n@@ -305,7 +303,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         self.set(name, value)\n \n     def set(\n-        self, name: _SettingsKeyT, value: Any, priority: Union[int, str] = \"project\"\n+        self, name: _SettingsKeyT, value: Any, priority: int | str = \"project\"\n     ) -> None:\n         \"\"\"\n         Store a key/value attribute with a given priority.\n@@ -338,7 +336,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         self,\n         name: _SettingsKeyT,\n         default: Any = None,\n-        priority: Union[int, str] = \"project\",\n+        priority: int | str = \"project\",\n     ) -> Any:\n         if name not in self:\n             self.set(name, default, priority)\n@@ -346,13 +344,11 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n \n         return self.attributes[name].value\n \n-    def setdict(\n-        self, values: _SettingsInputT, priority: Union[int, str] = \"project\"\n-    ) -> None:\n+    def setdict(self, values: _SettingsInputT, priority: int | str = \"project\") -> None:\n         self.update(values, priority)\n \n     def setmodule(\n-        self, module: Union[ModuleType, str], priority: Union[int, str] = \"project\"\n+        self, module: ModuleType | str, priority: int | str = \"project\"\n     ) -> None:\n         \"\"\"\n         Store settings from a module with a given priority.\n@@ -376,7 +372,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n                 self.set(key, getattr(module, key), priority)\n \n     # BaseSettings.update() doesn't support all inputs that MutableMapping.update() supports\n-    def update(self, values: _SettingsInputT, priority: Union[int, str] = \"project\") -> None:  # type: ignore[override]\n+    def update(self, values: _SettingsInputT, priority: int | str = \"project\") -> None:  # type: ignore[override]\n         \"\"\"\n         Store key/value pairs with a given priority.\n \n@@ -409,9 +405,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n                 for name, value in values.items():\n                     self.set(name, value, priority)\n \n-    def delete(\n-        self, name: _SettingsKeyT, priority: Union[int, str] = \"project\"\n-    ) -> None:\n+    def delete(self, name: _SettingsKeyT, priority: int | str = \"project\") -> None:\n         if name not in self:\n             raise KeyError(name)\n         self._assert_mutability()\n@@ -525,9 +519,7 @@ class Settings(BaseSettings):\n     described on :ref:`topics-settings-ref` already populated.\n     \"\"\"\n \n-    def __init__(\n-        self, values: _SettingsInputT = None, priority: Union[int, str] = \"project\"\n-    ):\n+    def __init__(self, values: _SettingsInputT = None, priority: int | str = \"project\"):\n         # Do not pass kwarg values here. We don't want to promote user-defined\n         # dicts, and we want to update, not replace, default dicts with the\n         # values given by the user\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n \n import os\n import signal\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import is_item\n from twisted.internet import defer, threads\n@@ -37,25 +37,25 @@ class Shell:\n     def __init__(\n         self,\n         crawler: Crawler,\n-        update_vars: Optional[Callable[[dict[str, Any]], None]] = None,\n-        code: Optional[str] = None,\n+        update_vars: Callable[[dict[str, Any]], None] | None = None,\n+        code: str | None = None,\n     ):\n         self.crawler: Crawler = crawler\n         self.update_vars: Callable[[dict[str, Any]], None] = update_vars or (\n             lambda x: None\n         )\n         self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n-        self.spider: Optional[Spider] = None\n+        self.spider: Spider | None = None\n         self.inthread: bool = not threadable.isInIOThread()\n-        self.code: Optional[str] = code\n+        self.code: str | None = code\n         self.vars: dict[str, Any] = {}\n \n     def start(\n         self,\n-        url: Optional[str] = None,\n-        request: Optional[Request] = None,\n-        response: Optional[Response] = None,\n-        spider: Optional[Spider] = None,\n+        url: str | None = None,\n+        request: Request | None = None,\n+        response: Response | None = None,\n+        spider: Spider | None = None,\n         redirect: bool = True,\n     ) -> None:\n         # disable accidental Ctrl-C key press from shutting down the engine\n@@ -97,9 +97,7 @@ class Shell:\n                 self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n             )\n \n-    def _schedule(\n-        self, request: Request, spider: Optional[Spider]\n-    ) -> defer.Deferred[Any]:\n+    def _schedule(self, request: Request, spider: Spider | None) -> defer.Deferred[Any]:\n         if is_asyncio_reactor_installed():\n             # set the asyncio event loop for the current thread\n             event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n@@ -111,7 +109,7 @@ class Shell:\n         self.crawler.engine.crawl(request)\n         return d\n \n-    def _open_spider(self, request: Request, spider: Optional[Spider]) -> Spider:\n+    def _open_spider(self, request: Request, spider: Spider | None) -> Spider:\n         if self.spider:\n             return self.spider\n \n@@ -126,8 +124,8 @@ class Shell:\n \n     def fetch(\n         self,\n-        request_or_url: Union[Request, str],\n-        spider: Optional[Spider] = None,\n+        request_or_url: Request | str,\n+        spider: Spider | None = None,\n         redirect: bool = True,\n         **kwargs: Any,\n     ) -> None:\n@@ -155,9 +153,9 @@ class Shell:\n \n     def populate_vars(\n         self,\n-        response: Optional[Response] = None,\n-        request: Optional[Request] = None,\n-        spider: Optional[Spider] = None,\n+        response: Response | None = None,\n+        request: Request | None = None,\n+        spider: Spider | None = None,\n     ) -> None:\n         import scrapy\n \n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.exceptions import IgnoreRequest\n \n@@ -65,7 +65,7 @@ class HttpErrorMiddleware:\n \n     def process_spider_exception(\n         self, response: Response, exception: Exception, spider: Spider\n-    ) -> Optional[Iterable[Any]]:\n+    ) -> Iterable[Any] | None:\n         if isinstance(exception, HttpError):\n             assert spider.crawler.stats\n             spider.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n\n@@ -6,7 +6,7 @@ originated it.\n from __future__ import annotations\n \n import warnings\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, cast\n from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n@@ -50,20 +50,20 @@ class ReferrerPolicy:\n     NOREFERRER_SCHEMES: tuple[str, ...] = LOCAL_SCHEMES\n     name: str\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         raise NotImplementedError()\n \n-    def stripped_referrer(self, url: str) -> Optional[str]:\n+    def stripped_referrer(self, url: str) -> str | None:\n         if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n             return self.strip_url(url)\n         return None\n \n-    def origin_referrer(self, url: str) -> Optional[str]:\n+    def origin_referrer(self, url: str) -> str | None:\n         if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n             return self.origin(url)\n         return None\n \n-    def strip_url(self, url: str, origin_only: bool = False) -> Optional[str]:\n+    def strip_url(self, url: str, origin_only: bool = False) -> str | None:\n         \"\"\"\n         https://www.w3.org/TR/referrer-policy/#strip-url\n \n@@ -87,7 +87,7 @@ class ReferrerPolicy:\n             origin_only=origin_only,\n         )\n \n-    def origin(self, url: str) -> Optional[str]:\n+    def origin(self, url: str) -> str | None:\n         \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n         return self.strip_url(url, origin_only=True)\n \n@@ -113,7 +113,7 @@ class NoReferrerPolicy(ReferrerPolicy):\n \n     name: str = POLICY_NO_REFERRER\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         return None\n \n \n@@ -134,7 +134,7 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n \n     name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         if not self.tls_protected(response_url) or self.tls_protected(request_url):\n             return self.stripped_referrer(response_url)\n         return None\n@@ -153,7 +153,7 @@ class SameOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_SAME_ORIGIN\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         if self.origin(response_url) == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n         return None\n@@ -171,7 +171,7 @@ class OriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_ORIGIN\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         return self.origin_referrer(response_url)\n \n \n@@ -191,7 +191,7 @@ class StrictOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_STRICT_ORIGIN\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         if (\n             self.tls_protected(response_url)\n             and self.potentially_trustworthy(request_url)\n@@ -215,7 +215,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n@@ -242,7 +242,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n@@ -271,7 +271,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n \n     name: str = POLICY_UNSAFE_URL\n \n-    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n+    def referrer(self, response_url: str, request_url: str) -> str | None:\n         return self.stripped_referrer(response_url)\n \n \n@@ -307,7 +307,7 @@ _policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n \n def _load_policy_class(\n     policy: str, warning_only: bool = False\n-) -> Optional[type[ReferrerPolicy]]:\n+) -> type[ReferrerPolicy] | None:\n     \"\"\"\n     Expect a string for the path to the policy class,\n     otherwise try to interpret the string as a standard value\n@@ -331,7 +331,7 @@ def _load_policy_class(\n \n \n class RefererMiddleware:\n-    def __init__(self, settings: Optional[BaseSettings] = None):\n+    def __init__(self, settings: BaseSettings | None = None):\n         self.default_policy: type[ReferrerPolicy] = DefaultReferrerPolicy\n         if settings is not None:\n             settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n@@ -349,9 +349,7 @@ class RefererMiddleware:\n \n         return mw\n \n-    def policy(\n-        self, resp_or_url: Union[Response, str], request: Request\n-    ) -> ReferrerPolicy:\n+    def policy(self, resp_or_url: Response | str, request: Request) -> ReferrerPolicy:\n         \"\"\"\n         Determine Referrer-Policy to use from a parent Response (or URL),\n         and a Request to be sent.\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Optional, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -34,9 +34,9 @@ class Spider(object_ref):\n     \"\"\"\n \n     name: str\n-    custom_settings: Optional[dict[_SettingsKeyT, Any]] = None\n+    custom_settings: dict[_SettingsKeyT, Any] | None = None\n \n-    def __init__(self, name: Optional[str] = None, **kwargs: Any):\n+    def __init__(self, name: str | None = None, **kwargs: Any):\n         if name is not None:\n             self.name: str = name\n         elif not getattr(self, \"name\", None):\n@@ -103,10 +103,10 @@ class Spider(object_ref):\n         return url_is_from_spider(request.url, cls)\n \n     @staticmethod\n-    def close(spider: Spider, reason: str) -> Optional[Deferred[None]]:\n+    def close(spider: Spider, reason: str) -> Deferred[None] | None:\n         closed = getattr(spider, \"closed\", None)\n         if callable(closed):\n-            return cast(\"Optional[Deferred[None]]\", closed(reason))\n+            return cast(\"Deferred[None] | None\", closed(reason))\n         return None\n \n     def __repr__(self) -> str:\n\n@@ -9,7 +9,7 @@ from __future__ import annotations\n \n import copy\n from collections.abc import AsyncIterable, Awaitable, Callable\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, cast\n \n from twisted.python.failure import Failure\n \n@@ -39,15 +39,11 @@ def _identity(x: _T) -> _T:\n     return x\n \n \n-def _identity_process_request(\n-    request: Request, response: Response\n-) -> Optional[Request]:\n+def _identity_process_request(request: Request, response: Response) -> Request | None:\n     return request\n \n \n-def _get_method(\n-    method: Union[Callable, str, None], spider: Spider\n-) -> Optional[Callable]:\n+def _get_method(method: Callable | str | None, spider: Spider) -> Callable | None:\n     if callable(method):\n         return method\n     if isinstance(method, str):\n@@ -61,20 +57,20 @@ _default_link_extractor = LinkExtractor()\n class Rule:\n     def __init__(\n         self,\n-        link_extractor: Optional[LinkExtractor] = None,\n-        callback: Union[CallbackT, str, None] = None,\n-        cb_kwargs: Optional[dict[str, Any]] = None,\n-        follow: Optional[bool] = None,\n-        process_links: Union[ProcessLinksT, str, None] = None,\n-        process_request: Union[ProcessRequestT, str, None] = None,\n-        errback: Union[Callable[[Failure], Any], str, None] = None,\n+        link_extractor: LinkExtractor | None = None,\n+        callback: CallbackT | str | None = None,\n+        cb_kwargs: dict[str, Any] | None = None,\n+        follow: bool | None = None,\n+        process_links: ProcessLinksT | str | None = None,\n+        process_request: ProcessRequestT | str | None = None,\n+        errback: Callable[[Failure], Any] | str | None = None,\n     ):\n         self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n-        self.callback: Union[CallbackT, str, None] = callback\n-        self.errback: Union[Callable[[Failure], Any], str, None] = errback\n+        self.callback: CallbackT | str | None = callback\n+        self.errback: Callable[[Failure], Any] | str | None = errback\n         self.cb_kwargs: dict[str, Any] = cb_kwargs or {}\n-        self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n-        self.process_request: Union[ProcessRequestT, str] = (\n+        self.process_links: ProcessLinksT | str = process_links or _identity\n+        self.process_request: ProcessRequestT | str = (\n             process_request or _identity_process_request\n         )\n         self.follow: bool = follow if follow is not None else not callback\n@@ -124,7 +120,7 @@ class CrawlSpider(Spider):\n             meta={\"rule\": rule_index, \"link_text\": link.text},\n         )\n \n-    def _requests_to_follow(self, response: Response) -> Iterable[Optional[Request]]:\n+    def _requests_to_follow(self, response: Response) -> Iterable[Request | None]:\n         if not isinstance(response, HtmlResponse):\n             return\n         seen: set[Link] = set()\n@@ -157,7 +153,7 @@ class CrawlSpider(Spider):\n     async def _parse_response(\n         self,\n         response: Response,\n-        callback: Optional[CallbackT],\n+        callback: CallbackT | None,\n         cb_kwargs: dict[str, Any],\n         follow: bool = True,\n     ) -> AsyncIterable[Any]:\n@@ -176,7 +172,7 @@ class CrawlSpider(Spider):\n                 yield request_or_item\n \n     def _handle_failure(\n-        self, failure: Failure, errback: Optional[Callable[[Failure], Any]]\n+        self, failure: Failure, errback: Callable[[Failure], Any] | None\n     ) -> Iterable[Any]:\n         if errback:\n             results = errback(failure) or ()\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.http import Response, TextResponse\n@@ -117,13 +117,13 @@ class CSVFeedSpider(Spider):\n     and the file's headers.\n     \"\"\"\n \n-    delimiter: Optional[str] = (\n+    delimiter: str | None = (\n         None  # When this is None, python's csv module's default delimiter is used\n     )\n-    quotechar: Optional[str] = (\n+    quotechar: str | None = (\n         None  # When this is None, python's csv module's default quotechar is used\n     )\n-    headers: Optional[list[str]] = None\n+    headers: list[str] | None = None\n \n     def process_results(\n         self, response: Response, results: Iterable[Any]\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Any, Optional, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from scrapy import Request\n from scrapy.spiders import Spider\n@@ -18,7 +18,7 @@ class InitSpider(Spider):\n         self._postinit_reqs: Iterable[Request] = super().start_requests()\n         return cast(Iterable[Request], iterate_spider_output(self.init_request()))\n \n-    def initialized(self, response: Optional[Response] = None) -> Any:\n+    def initialized(self, response: Response | None = None) -> Any:\n         \"\"\"This method must be set as the callback of your last initialization\n         request. See self.init_request() docstring for more info.\n         \"\"\"\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n import re\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, cast\n \n from scrapy.http import Request, Response, XmlResponse\n from scrapy.spiders import Spider\n@@ -24,10 +24,10 @@ logger = logging.getLogger(__name__)\n \n class SitemapSpider(Spider):\n     sitemap_urls: Sequence[str] = ()\n-    sitemap_rules: Sequence[\n-        tuple[Union[re.Pattern[str], str], Union[str, CallbackT]]\n-    ] = [(\"\", \"parse\")]\n-    sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n+    sitemap_rules: Sequence[tuple[re.Pattern[str] | str, str | CallbackT]] = [\n+        (\"\", \"parse\")\n+    ]\n+    sitemap_follow: Sequence[re.Pattern[str] | str] = [\"\"]\n     sitemap_alternate_links: bool = False\n     _max_size: int\n     _warn_size: int\n@@ -93,7 +93,7 @@ class SitemapSpider(Spider):\n                             yield Request(loc, callback=c)\n                             break\n \n-    def _get_sitemap_body(self, response: Response) -> Optional[bytes]:\n+    def _get_sitemap_body(self, response: Response) -> bytes | None:\n         \"\"\"Return the sitemap body contained in the given response,\n         or None if the response is not a sitemap.\n         \"\"\"\n@@ -127,7 +127,7 @@ class SitemapSpider(Spider):\n         return None\n \n \n-def regex(x: Union[re.Pattern[str], str]) -> re.Pattern[str]:\n+def regex(x: re.Pattern[str] | str) -> re.Pattern[str]:\n     if isinstance(x, str):\n         return re.compile(x)\n     return x\n\n@@ -7,7 +7,7 @@ from __future__ import annotations\n import marshal\n import pickle  # nosec\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from queuelib import queue\n \n@@ -26,7 +26,7 @@ if TYPE_CHECKING:\n \n def _with_mkdir(queue_class: type[queue.BaseQueue]) -> type[queue.BaseQueue]:\n     class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n-        def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n+        def __init__(self, path: str | PathLike, *args: Any, **kwargs: Any):\n             dirname = Path(path).parent\n             if not dirname.exists():\n                 dirname.mkdir(parents=True, exist_ok=True)\n@@ -45,13 +45,13 @@ def _serializable_queue(\n             s = serialize(obj)\n             super().push(s)\n \n-        def pop(self) -> Optional[Any]:\n+        def pop(self) -> Any | None:\n             s = super().pop()\n             if s:\n                 return deserialize(s)\n             return None\n \n-        def peek(self) -> Optional[Any]:\n+        def peek(self) -> Any | None:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n@@ -89,13 +89,13 @@ def _scrapy_serialization_queue(\n             request_dict = request.to_dict(spider=self.spider)\n             super().push(request_dict)\n \n-        def pop(self) -> Optional[Request]:\n+        def pop(self) -> Request | None:\n             request = super().pop()\n             if not request:\n                 return None\n             return request_from_dict(request, spider=self.spider)\n \n-        def peek(self) -> Optional[Request]:\n+        def peek(self) -> Request | None:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n@@ -118,7 +118,7 @@ def _scrapy_non_serialization_queue(\n         def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n             return cls()\n \n-        def peek(self) -> Optional[Any]:\n+        def peek(self) -> Any | None:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n\n@@ -6,7 +6,7 @@ from __future__ import annotations\n \n import logging\n import pprint\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n if TYPE_CHECKING:\n     from scrapy import Spider\n@@ -25,32 +25,32 @@ class StatsCollector:\n         self._stats: StatsT = {}\n \n     def get_value(\n-        self, key: str, default: Any = None, spider: Optional[Spider] = None\n+        self, key: str, default: Any = None, spider: Spider | None = None\n     ) -> Any:\n         return self._stats.get(key, default)\n \n-    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n+    def get_stats(self, spider: Spider | None = None) -> StatsT:\n         return self._stats\n \n-    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def set_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         self._stats[key] = value\n \n-    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n+    def set_stats(self, stats: StatsT, spider: Spider | None = None) -> None:\n         self._stats = stats\n \n     def inc_value(\n-        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n+        self, key: str, count: int = 1, start: int = 0, spider: Spider | None = None\n     ) -> None:\n         d = self._stats\n         d[key] = d.setdefault(key, start) + count\n \n-    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def max_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         self._stats[key] = max(self._stats.setdefault(key, value), value)\n \n-    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def min_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         self._stats[key] = min(self._stats.setdefault(key, value), value)\n \n-    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n+    def clear_stats(self, spider: Spider | None = None) -> None:\n         self._stats.clear()\n \n     def open_spider(self, spider: Spider) -> None:\n@@ -79,23 +79,23 @@ class MemoryStatsCollector(StatsCollector):\n \n class DummyStatsCollector(StatsCollector):\n     def get_value(\n-        self, key: str, default: Any = None, spider: Optional[Spider] = None\n+        self, key: str, default: Any = None, spider: Spider | None = None\n     ) -> Any:\n         return default\n \n-    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def set_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         pass\n \n-    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n+    def set_stats(self, stats: StatsT, spider: Spider | None = None) -> None:\n         pass\n \n     def inc_value(\n-        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n+        self, key: str, count: int = 1, start: int = 0, spider: Spider | None = None\n     ) -> None:\n         pass\n \n-    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def max_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         pass\n \n-    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n+    def min_value(self, key: str, value: Any, spider: Spider | None = None) -> None:\n         pass\n\n@@ -1,5 +1,7 @@\n+from __future__ import annotations\n+\n from collections.abc import AsyncGenerator, AsyncIterable, Iterable\n-from typing import TypeVar, Union\n+from typing import TypeVar\n \n _T = TypeVar(\"_T\")\n \n@@ -12,8 +14,8 @@ async def collect_asyncgen(result: AsyncIterable[_T]) -> list[_T]:\n \n \n async def as_async_generator(\n-    it: Union[Iterable[_T], AsyncIterable[_T]]\n-) -> AsyncGenerator[_T, None]:\n+    it: Iterable[_T] | AsyncIterable[_T],\n+) -> AsyncGenerator[_T]:\n     \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n     if isinstance(it, AsyncIterable):\n         async for r in it:\n\n@@ -8,7 +8,7 @@ from collections.abc import Iterable\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Callable, cast\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings\n@@ -33,7 +33,7 @@ def build_component_list(\n                 \"please update your settings\"\n             )\n \n-    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, dict[Any, Any]]:\n+    def _map_keys(compdict: Mapping[Any, Any]) -> BaseSettings | dict[Any, Any]:\n         if isinstance(compdict, BaseSettings):\n             compbs = BaseSettings()\n             for k, v in compdict.items():\n@@ -86,8 +86,8 @@ def arglist_to_dict(arglist: list[str]) -> dict[str, str]:\n \n \n def closest_scrapy_cfg(\n-    path: Union[str, os.PathLike] = \".\",\n-    prevpath: Optional[Union[str, os.PathLike]] = None,\n+    path: str | os.PathLike = \".\",\n+    prevpath: str | os.PathLike | None = None,\n ) -> str:\n     \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n     directory and its parents\n@@ -159,8 +159,8 @@ def feed_complete_default_values_from_settings(\n def feed_process_params_from_cli(\n     settings: BaseSettings,\n     output: list[str],\n-    output_format: Optional[str] = None,\n-    overwrite_output: Optional[list[str]] = None,\n+    output_format: str | None = None,\n+    overwrite_output: list[str] | None = None,\n ) -> dict[str, dict[str, Any]]:\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n from collections.abc import Callable\n from functools import wraps\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n if TYPE_CHECKING:\n     from collections.abc import Iterable\n@@ -100,7 +100,7 @@ DEFAULT_PYTHON_SHELLS: KnownShellsT = {\n \n \n def get_shell_embed_func(\n-    shells: Optional[Iterable[str]] = None, known_shells: Optional[KnownShellsT] = None\n+    shells: Iterable[str] | None = None, known_shells: KnownShellsT | None = None\n ) -> Any:\n     \"\"\"Return the first acceptable shell-embed function\n     from a given list of shell names.\n@@ -120,9 +120,9 @@ def get_shell_embed_func(\n \n \n def start_python_console(\n-    namespace: Optional[dict[str, Any]] = None,\n+    namespace: dict[str, Any] | None = None,\n     banner: str = \"\",\n-    shells: Optional[Iterable[str]] = None,\n+    shells: Iterable[str] | None = None,\n ) -> None:\n     \"\"\"Start Python console bound to the given namespace.\n     Readline support and tab completion will be used on Unix, if available.\n\n@@ -4,7 +4,7 @@ import argparse\n import warnings\n from http.cookies import SimpleCookie\n from shlex import split\n-from typing import TYPE_CHECKING, Any, NoReturn, Optional, Union\n+from typing import TYPE_CHECKING, Any, NoReturn\n from urllib.parse import urlparse\n \n from w3lib.http import basic_auth_header\n@@ -18,8 +18,8 @@ class DataAction(argparse.Action):\n         self,\n         parser: argparse.ArgumentParser,\n         namespace: argparse.Namespace,\n-        values: Union[str, Sequence[Any], None],\n-        option_string: Optional[str] = None,\n+        values: str | Sequence[Any] | None,\n+        option_string: str | None = None,\n     ) -> None:\n         value = str(values)\n         if value.startswith(\"$\"):\n\n@@ -12,7 +12,7 @@ import warnings\n import weakref\n from collections import OrderedDict\n from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, AnyStr, Optional, TypeVar, Union\n+from typing import TYPE_CHECKING, Any, AnyStr, TypeVar\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -44,7 +44,7 @@ class CaselessDict(dict):\n \n     def __init__(\n         self,\n-        seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n+        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n     ):\n         super().__init__()\n         if seq:\n@@ -84,7 +84,7 @@ class CaselessDict(dict):\n         return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n \n     # doesn't fully implement MutableMapping.update()\n-    def update(self, seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]]]) -> None:  # type: ignore[override]\n+    def update(self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]) -> None:  # type: ignore[override]\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super().update(iseq)\n@@ -145,9 +145,9 @@ class LocalCache(OrderedDict[_KT, _VT]):\n     Older items expires first.\n     \"\"\"\n \n-    def __init__(self, limit: Optional[int] = None):\n+    def __init__(self, limit: int | None = None):\n         super().__init__()\n-        self.limit: Optional[int] = limit\n+        self.limit: int | None = limit\n \n     def __setitem__(self, key: _KT, value: _VT) -> None:\n         if self.limit:\n@@ -168,7 +168,7 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n     it cannot be instantiated with an initial dictionary.\n     \"\"\"\n \n-    def __init__(self, limit: Optional[int] = None):\n+    def __init__(self, limit: int | None = None):\n         super().__init__()\n         self.data: LocalCache = LocalCache(limit=limit)\n \n@@ -178,7 +178,7 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n         except TypeError:\n             pass  # key is not weak-referenceable, skip caching\n \n-    def __getitem__(self, key: _KT) -> Optional[_VT]:  # type: ignore[override]\n+    def __getitem__(self, key: _KT) -> _VT | None:  # type: ignore[override]\n         try:\n             return super().__getitem__(key)\n         except (TypeError, KeyError):\n\n@@ -11,7 +11,7 @@ from asyncio import Future\n from collections.abc import Awaitable, Coroutine, Iterable, Iterator\n from functools import wraps\n from types import CoroutineType\n-from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar, Union, cast, overload\n+from typing import TYPE_CHECKING, Any, Generic, TypeVar, Union, cast, overload\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n@@ -93,7 +93,7 @@ def mustbe_deferred(\n \n \n def mustbe_deferred(\n-    f: Callable[_P, Union[Deferred[_T], Coroutine[Deferred[Any], Any, _T], _T]],\n+    f: Callable[_P, Deferred[_T] | Coroutine[Deferred[Any], Any, _T] | _T],\n     *args: _P.args,\n     **kw: _P.kwargs,\n ) -> Deferred[_T]:\n@@ -179,17 +179,17 @@ class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n     def __init__(\n         self,\n         aiterable: AsyncIterable[_T],\n-        callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]],\n+        callable: Callable[Concatenate[_T, _P], Deferred[Any] | None],\n         *callable_args: _P.args,\n         **callable_kwargs: _P.kwargs,\n     ):\n         self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n-        self.callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]] = callable\n+        self.callable: Callable[Concatenate[_T, _P], Deferred[Any] | None] = callable\n         self.callable_args: tuple[Any, ...] = callable_args\n         self.callable_kwargs: dict[str, Any] = callable_kwargs\n         self.finished: bool = False\n         self.waiting_deferreds: list[Deferred[Any]] = []\n-        self.anext_deferred: Optional[Deferred[_T]] = None\n+        self.anext_deferred: Deferred[_T] | None = None\n \n     def _callback(self, result: _T) -> None:\n         # This gets called when the result from aiterator.__anext__() is available.\n@@ -237,7 +237,7 @@ class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n def parallel_async(\n     async_iterable: AsyncIterable[_T],\n     count: int,\n-    callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]],\n+    callable: Callable[Concatenate[_T, _P], Deferred[Any] | None],\n     *args: _P.args,\n     **named: _P.kwargs,\n ) -> Deferred[list[tuple[bool, Iterator[Deferred[Any]]]]]:\n@@ -362,7 +362,7 @@ def deferred_from_coro(o: _CT) -> Deferred: ...\n def deferred_from_coro(o: _T) -> _T: ...\n \n \n-def deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n+def deferred_from_coro(o: _T) -> Deferred | _T:\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, Deferred):\n         return o\n@@ -433,7 +433,7 @@ def deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n     return d.asFuture(_get_asyncio_event_loop())\n \n \n-def maybe_deferred_to_future(d: Deferred[_T]) -> Union[Deferred[_T], Future[_T]]:\n+def maybe_deferred_to_future(d: Deferred[_T]) -> Deferred[_T] | Future[_T]:\n     \"\"\"\n     .. versionadded:: 2.6.0\n \n\n@@ -1,8 +1,10 @@\n \"\"\"Some helpers for deprecation messages\"\"\"\n \n+from __future__ import annotations\n+\n import inspect\n import warnings\n-from typing import Any, Optional, overload\n+from typing import Any, overload\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -20,11 +22,11 @@ def attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> No\n def create_deprecated_class(\n     name: str,\n     new_class: type,\n-    clsdict: Optional[dict[str, Any]] = None,\n+    clsdict: dict[str, Any] | None = None,\n     warn_category: type[Warning] = ScrapyDeprecationWarning,\n     warn_once: bool = True,\n-    old_class_path: Optional[str] = None,\n-    new_class_path: Optional[str] = None,\n+    old_class_path: str | None = None,\n+    new_class_path: str | None = None,\n     subclass_warn_message: str = \"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n     instance_warn_message: str = \"{cls} is deprecated, instantiate {new} instead.\",\n ) -> type:\n@@ -55,7 +57,7 @@ def create_deprecated_class(\n \n     # https://github.com/python/mypy/issues/4177\n     class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n-        deprecated_class: Optional[type] = None\n+        deprecated_class: type | None = None\n         warned_on_subclass: bool = False\n \n         def __new__(\n@@ -128,7 +130,7 @@ def create_deprecated_class(\n     return deprecated_cls\n \n \n-def _clspath(cls: type, forced: Optional[str] = None) -> str:\n+def _clspath(cls: type, forced: str | None = None) -> str:\n     if forced is not None:\n         return forced\n     return f\"{cls.__module__}.{cls.__name__}\"\n\n@@ -2,7 +2,7 @@\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n from urllib.parse import ParseResult, urlparse\n from weakref import WeakKeyDictionary\n \n@@ -10,12 +10,12 @@ if TYPE_CHECKING:\n     from scrapy.http import Request, Response\n \n \n-_urlparse_cache: WeakKeyDictionary[Union[Request, Response], ParseResult] = (\n+_urlparse_cache: WeakKeyDictionary[Request | Response, ParseResult] = (\n     WeakKeyDictionary()\n )\n \n \n-def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n+def urlparse_cached(request_or_response: Request | Response) -> ParseResult:\n     \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n     Request or Response object\n     \"\"\"\n\n@@ -4,7 +4,7 @@ import csv\n import logging\n import re\n from io import StringIO\n-from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast, overload\n+from typing import TYPE_CHECKING, Any, Literal, cast, overload\n from warnings import warn\n \n from lxml import etree  # nosec\n@@ -20,7 +20,7 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n-def xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selector]:\n+def xmliter(obj: Response | str | bytes, nodename: str) -> Iterator[Selector]:\n     \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n        given the name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -77,9 +77,9 @@ def xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selecto\n \n \n def xmliter_lxml(\n-    obj: Union[Response, str, bytes],\n+    obj: Response | str | bytes,\n     nodename: str,\n-    namespace: Optional[str] = None,\n+    namespace: str | None = None,\n     prefix: str = \"x\",\n ) -> Iterator[Selector]:\n     reader = _StreamReader(obj)\n@@ -120,9 +120,9 @@ def xmliter_lxml(\n \n \n class _StreamReader:\n-    def __init__(self, obj: Union[Response, str, bytes]):\n+    def __init__(self, obj: Response | str | bytes):\n         self._ptr: int = 0\n-        self._text: Union[str, bytes]\n+        self._text: str | bytes\n         if isinstance(obj, TextResponse):\n             self._text, self.encoding = obj.body, obj.encoding\n         elif isinstance(obj, Response):\n@@ -154,11 +154,11 @@ class _StreamReader:\n \n \n def csviter(\n-    obj: Union[Response, str, bytes],\n-    delimiter: Optional[str] = None,\n-    headers: Optional[list[str]] = None,\n-    encoding: Optional[str] = None,\n-    quotechar: Optional[str] = None,\n+    obj: Response | str | bytes,\n+    delimiter: str | None = None,\n+    headers: list[str] | None = None,\n+    encoding: str | None = None,\n+    quotechar: str | None = None,\n ) -> Iterator[dict[str, str]]:\n     \"\"\"Returns an iterator of dictionaries from the given csv object\n \n@@ -214,22 +214,18 @@ def csviter(\n \n \n @overload\n-def _body_or_str(obj: Union[Response, str, bytes]) -> str: ...\n+def _body_or_str(obj: Response | str | bytes) -> str: ...\n \n \n @overload\n-def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str: ...\n+def _body_or_str(obj: Response | str | bytes, unicode: Literal[True]) -> str: ...\n \n \n @overload\n-def _body_or_str(\n-    obj: Union[Response, str, bytes], unicode: Literal[False]\n-) -> bytes: ...\n+def _body_or_str(obj: Response | str | bytes, unicode: Literal[False]) -> bytes: ...\n \n \n-def _body_or_str(\n-    obj: Union[Response, str, bytes], unicode: bool = True\n-) -> Union[str, bytes]:\n+def _body_or_str(obj: Response | str | bytes, unicode: bool = True) -> str | bytes:\n     expected_types = (Response, str, bytes)\n     if not isinstance(obj, expected_types):\n         expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n\n@@ -1,14 +1,14 @@\n from __future__ import annotations\n \n from pathlib import Path\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n if TYPE_CHECKING:\n     from scrapy.settings import BaseSettings\n \n \n-def job_dir(settings: BaseSettings) -> Optional[str]:\n-    path: Optional[str] = settings[\"JOBDIR\"]\n+def job_dir(settings: BaseSettings) -> str | None:\n+    path: str | None = settings[\"JOBDIR\"]\n     if not path:\n         return None\n     if not Path(path).exists():\n\n@@ -5,7 +5,7 @@ import sys\n from collections.abc import MutableMapping\n from logging.config import dictConfig\n from types import TracebackType\n-from typing import TYPE_CHECKING, Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Optional, cast\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -25,7 +25,7 @@ logger = logging.getLogger(__name__)\n \n def failure_to_exc_info(\n     failure: Failure,\n-) -> Optional[tuple[type[BaseException], BaseException, Optional[TracebackType]]]:\n+) -> tuple[type[BaseException], BaseException, TracebackType | None] | None:\n     \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n         assert failure.type\n@@ -50,7 +50,7 @@ class TopLevelFormatter(logging.Filter):\n     ``loggers`` list where it should act.\n     \"\"\"\n \n-    def __init__(self, loggers: Optional[list[str]] = None):\n+    def __init__(self, loggers: list[str] | None = None):\n         self.loggers: list[str] = loggers or []\n \n     def filter(self, record: logging.LogRecord) -> bool:\n@@ -80,7 +80,7 @@ DEFAULT_LOGGING = {\n \n \n def configure_logging(\n-    settings: Union[Settings, dict[_SettingsKeyT, Any], None] = None,\n+    settings: Settings | dict[_SettingsKeyT, Any] | None = None,\n     install_root_handler: bool = True,\n ) -> None:\n     \"\"\"\n@@ -125,7 +125,7 @@ def configure_logging(\n         install_scrapy_root_handler(settings)\n \n \n-_scrapy_root_handler: Optional[logging.Handler] = None\n+_scrapy_root_handler: logging.Handler | None = None\n \n \n def install_scrapy_root_handler(settings: Settings) -> None:\n@@ -141,7 +141,7 @@ def install_scrapy_root_handler(settings: Settings) -> None:\n     logging.root.addHandler(_scrapy_root_handler)\n \n \n-def get_scrapy_root_handler() -> Optional[logging.Handler]:\n+def get_scrapy_root_handler() -> logging.Handler | None:\n     return _scrapy_root_handler\n \n \n@@ -231,7 +231,7 @@ class LogCounterHandler(logging.Handler):\n \n def logformatter_adapter(\n     logkws: LogFormatterResult,\n-) -> tuple[int, str, Union[dict[str, Any], tuple[Any, ...]]]:\n+) -> tuple[int, str, dict[str, Any] | tuple[Any, ...]]:\n     \"\"\"\n     Helper that takes the dictionary output from the methods in LogFormatter\n     and adapts it into a tuple of positional arguments for logger.log calls,\n\n@@ -14,7 +14,7 @@ from contextlib import contextmanager\n from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n-from typing import IO, TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n+from typing import IO, TYPE_CHECKING, Any, TypeVar, cast\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import Item\n@@ -46,7 +46,7 @@ def arg_to_iter(arg: Any) -> Iterable[Any]:\n     return [arg]\n \n \n-def load_object(path: Union[str, Callable[..., Any]]) -> Any:\n+def load_object(path: str | Callable[..., Any]) -> Any:\n     \"\"\"Load an object given its absolute object path, and return it.\n \n     The object can be the import path of a class, function, variable or an\n@@ -126,7 +126,7 @@ def md5sum(file: IO[bytes]) -> str:\n     return m.hexdigest()\n \n \n-def rel_has_nofollow(rel: Optional[str]) -> bool:\n+def rel_has_nofollow(rel: str | None) -> bool:\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n     return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n \n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import signal\n from collections.abc import Callable\n from types import FrameType\n\n@@ -1,8 +1,9 @@\n+from __future__ import annotations\n+\n import os\n import warnings\n from importlib import import_module\n from pathlib import Path\n-from typing import Union\n \n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n@@ -45,7 +46,7 @@ def project_data_dir(project: str = \"default\") -> str:\n     return str(d)\n \n \n-def data_path(path: Union[str, os.PathLike[str]], createdir: bool = False) -> str:\n+def data_path(path: str | os.PathLike[str], createdir: bool = False) -> str:\n     \"\"\"\n     Return the given path joined with the .scrapy data directory.\n     If given an absolute path, return it unmodified.\n\n@@ -12,7 +12,7 @@ import weakref\n from collections.abc import AsyncIterable, Iterable, Mapping\n from functools import partial, wraps\n from itertools import chain\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, overload\n+from typing import TYPE_CHECKING, Any, TypeVar, overload\n \n from scrapy.utils.asyncgen import as_async_generator\n \n@@ -99,7 +99,7 @@ def unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> list[\n \n \n def to_unicode(\n-    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n+    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n ) -> str:\n     \"\"\"Return the unicode representation of a bytes object ``text``. If\n     ``text`` is already an unicode object, return it as-is.\"\"\"\n@@ -116,7 +116,7 @@ def to_unicode(\n \n \n def to_bytes(\n-    text: Union[str, bytes], encoding: Optional[str] = None, errors: str = \"strict\"\n+    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n ) -> bytes:\n     \"\"\"Return the binary representation of ``text``. If ``text``\n     is already a bytes object, return it as-is.\"\"\"\n@@ -132,8 +132,8 @@ def to_bytes(\n \n \n def re_rsearch(\n-    pattern: Union[str, Pattern[str]], text: str, chunk_size: int = 1024\n-) -> Optional[tuple[int, int]]:\n+    pattern: str | Pattern[str], text: str, chunk_size: int = 1024\n+) -> tuple[int, int] | None:\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n     given in the attribute 'pattern'.\n@@ -269,7 +269,7 @@ def get_spec(func: Callable[..., Any]) -> tuple[list[str], dict[str, Any]]:\n \n \n def equal_attributes(\n-    obj1: Any, obj2: Any, attributes: Optional[list[Union[str, Callable[[Any], Any]]]]\n+    obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n ) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n@@ -297,8 +297,8 @@ def without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n \n \n def without_none_values(\n-    iterable: Union[Mapping[_KT, _VT], Iterable[_KT]]\n-) -> Union[dict[_KT, _VT], Iterable[_KT]]:\n+    iterable: Mapping[_KT, _VT] | Iterable[_KT]\n+) -> dict[_KT, _VT] | Iterable[_KT]:\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n     If ``iterable`` is a mapping, return a dictionary where all pairs that have\n@@ -354,7 +354,7 @@ class MutableChain(Iterable[_T]):\n \n \n async def _async_chain(\n-    *iterables: Union[Iterable[_T], AsyncIterable[_T]]\n+    *iterables: Iterable[_T] | AsyncIterable[_T],\n ) -> AsyncIterator[_T]:\n     for it in iterables:\n         async for o in as_async_generator(it):\n@@ -366,10 +366,10 @@ class MutableAsyncChain(AsyncIterable[_T]):\n     Similar to MutableChain but for async iterables\n     \"\"\"\n \n-    def __init__(self, *args: Union[Iterable[_T], AsyncIterable[_T]]):\n+    def __init__(self, *args: Iterable[_T] | AsyncIterable[_T]):\n         self.data: AsyncIterator[_T] = _async_chain(*args)\n \n-    def extend(self, *iterables: Union[Iterable[_T], AsyncIterable[_T]]) -> None:\n+    def extend(self, *iterables: Iterable[_T] | AsyncIterable[_T]) -> None:\n         self.data = _async_chain(self.data, _async_chain(*iterables))\n \n     def __aiter__(self) -> AsyncIterator[_T]:\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import asyncio\n import sys\n from contextlib import suppress\n-from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n+from typing import TYPE_CHECKING, Any, Generic, TypeVar\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n@@ -54,7 +54,7 @@ class CallLaterOnce(Generic[_T]):\n         self._func: Callable[_P, _T] = func\n         self._a: tuple[Any, ...] = a\n         self._kw: dict[str, Any] = kw\n-        self._call: Optional[DelayedCall] = None\n+        self._call: DelayedCall | None = None\n \n     def schedule(self, delay: float = 0) -> None:\n         from twisted.internet import reactor\n@@ -107,7 +107,7 @@ def _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n     return policy\n \n \n-def install_reactor(reactor_path: str, event_loop_path: Optional[str] = None) -> None:\n+def install_reactor(reactor_path: str, event_loop_path: str | None = None) -> None:\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n     import path. Also installs the asyncio event loop with the specified import\n     path if the asyncio reactor is enabled\"\"\"\n@@ -129,7 +129,7 @@ def _get_asyncio_event_loop() -> AbstractEventLoop:\n     return set_asyncio_event_loop(None)\n \n \n-def set_asyncio_event_loop(event_loop_path: Optional[str]) -> AbstractEventLoop:\n+def set_asyncio_event_loop(event_loop_path: str | None) -> AbstractEventLoop:\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     if event_loop_path is not None:\n         event_loop_class: type[AbstractEventLoop] = load_object(event_loop_path)\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n import hashlib\n import json\n import warnings\n-from typing import TYPE_CHECKING, Any, Optional, Protocol, Union\n+from typing import TYPE_CHECKING, Any, Protocol\n from urllib.parse import urlunparse\n from weakref import WeakKeyDictionary\n \n@@ -38,7 +38,7 @@ def _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[b\n \n \n _fingerprint_cache: WeakKeyDictionary[\n-    Request, dict[tuple[Optional[tuple[bytes, ...]], bool], bytes]\n+    Request, dict[tuple[tuple[bytes, ...] | None, bool], bytes]\n ]\n _fingerprint_cache = WeakKeyDictionary()\n \n@@ -46,7 +46,7 @@ _fingerprint_cache = WeakKeyDictionary()\n def fingerprint(\n     request: Request,\n     *,\n-    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n+    include_headers: Iterable[bytes | str] | None = None,\n     keep_fragments: bool = False,\n ) -> bytes:\n     \"\"\"\n@@ -79,7 +79,7 @@ def fingerprint(\n     If you want to include them, set the keep_fragments argument to True\n     (for instance when handling requests with a headless browser).\n     \"\"\"\n-    processed_include_headers: Optional[tuple[bytes, ...]] = None\n+    processed_include_headers: tuple[bytes, ...] | None = None\n     if include_headers:\n         processed_include_headers = tuple(\n             to_bytes(h.lower()) for h in sorted(include_headers)\n@@ -129,7 +129,7 @@ class RequestFingerprinter:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def __init__(self, crawler: Optional[Crawler] = None):\n+    def __init__(self, crawler: Crawler | None = None):\n         if crawler:\n             implementation = crawler.settings.get(\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n@@ -177,7 +177,7 @@ def request_httprepr(request: Request) -> bytes:\n     return s\n \n \n-def referer_str(request: Request) -> Optional[str]:\n+def referer_str(request: Request) -> str | None:\n     \"\"\"Return Referer HTTP header suitable for logging.\"\"\"\n     referrer = request.headers.get(\"Referer\")\n     if referrer is None:\n@@ -185,7 +185,7 @@ def referer_str(request: Request) -> Optional[str]:\n     return to_unicode(referrer, errors=\"replace\")\n \n \n-def request_from_dict(d: dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n+def request_from_dict(d: dict[str, Any], *, spider: Spider | None = None) -> Request:\n     \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n \n     If a spider is given, it will try to resolve the callbacks looking at the\n\n@@ -9,7 +9,7 @@ import os\n import re\n import tempfile\n import webbrowser\n-from typing import TYPE_CHECKING, Any, Union\n+from typing import TYPE_CHECKING, Any\n from weakref import WeakKeyDictionary\n \n from twisted.web import http\n@@ -35,15 +35,15 @@ def get_base_url(response: TextResponse) -> str:\n     return _baseurl_cache[response]\n \n \n-_metaref_cache: WeakKeyDictionary[\n-    Response, Union[tuple[None, None], tuple[float, str]]\n-] = WeakKeyDictionary()\n+_metaref_cache: WeakKeyDictionary[Response, tuple[None, None] | tuple[float, str]] = (\n+    WeakKeyDictionary()\n+)\n \n \n def get_meta_refresh(\n     response: TextResponse,\n     ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n-) -> Union[tuple[None, None], tuple[float, str]]:\n+) -> tuple[None, None] | tuple[float, str]:\n     \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n@@ -53,7 +53,7 @@ def get_meta_refresh(\n     return _metaref_cache[response]\n \n \n-def response_status_message(status: Union[bytes, float, int, str]) -> str:\n+def response_status_message(status: bytes | float | int | str) -> str:\n     \"\"\"Return status code plus status text descriptive message\"\"\"\n     status_int = int(status)\n     message = http.RESPONSES.get(status_int, \"Unknown Status\")\n\n@@ -7,7 +7,7 @@ SitemapSpider, its API is subject to change without notice.\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n from urllib.parse import urljoin\n \n import lxml.etree  # nosec\n@@ -20,7 +20,7 @@ class Sitemap:\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n \n-    def __init__(self, xmltext: Union[str, bytes]):\n+    def __init__(self, xmltext: str | bytes):\n         xmlp = lxml.etree.XMLParser(\n             recover=True, remove_comments=True, resolve_entities=False\n         )\n@@ -46,7 +46,7 @@ class Sitemap:\n \n \n def sitemap_urls_from_robots(\n-    robots_text: str, base_url: Optional[str] = None\n+    robots_text: str, base_url: str | None = None\n ) -> Iterable[str]:\n     \"\"\"Return an iterator over all sitemap urls contained in the given\n     robots.txt file\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import inspect\n import logging\n-from typing import TYPE_CHECKING, Any, Literal, Optional, TypeVar, Union, overload\n+from typing import TYPE_CHECKING, Any, Literal, TypeVar, overload\n \n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n@@ -25,7 +25,7 @@ _T = TypeVar(\"_T\")\n \n # https://stackoverflow.com/questions/60222982\n @overload\n-def iterate_spider_output(result: AsyncGenerator[_T, None]) -> AsyncGenerator[_T, None]: ...  # type: ignore[overload-overlap]\n+def iterate_spider_output(result: AsyncGenerator[_T]) -> AsyncGenerator[_T]: ...  # type: ignore[overload-overlap]\n \n \n @overload\n@@ -38,7 +38,7 @@ def iterate_spider_output(result: _T) -> Iterable[Any]: ...\n \n def iterate_spider_output(\n     result: Any,\n-) -> Union[Iterable[Any], AsyncGenerator[_T, None], Deferred[_T]]:\n+) -> Iterable[Any] | AsyncGenerator[_T] | Deferred[_T]:\n     if inspect.isasyncgen(result):\n         return result\n     if inspect.iscoroutine(result):\n@@ -83,7 +83,7 @@ def spidercls_for_request(\n     default_spidercls: Literal[None],\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[type[Spider]]: ...\n+) -> type[Spider] | None: ...\n \n \n @overload\n@@ -93,16 +93,16 @@ def spidercls_for_request(\n     *,\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[type[Spider]]: ...\n+) -> type[Spider] | None: ...\n \n \n def spidercls_for_request(\n     spider_loader: SpiderLoader,\n     request: Request,\n-    default_spidercls: Optional[type[Spider]] = None,\n+    default_spidercls: type[Spider] | None = None,\n     log_none: bool = False,\n     log_multiple: bool = False,\n-) -> Optional[type[Spider]]:\n+) -> type[Spider] | None:\n     \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any\n \n import OpenSSL._util as pyOpenSSLutil\n import OpenSSL.SSL\n@@ -26,7 +26,7 @@ def x509name_to_string(x509name: X509Name) -> str:\n     return ffi_buf_to_string(result_buffer)\n \n \n-def get_temp_key_info(ssl_object: Any) -> Optional[str]:\n+def get_temp_key_info(ssl_object: Any) -> str | None:\n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n     if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n         # removed in cryptography 40.0.0\n\n@@ -5,13 +5,13 @@ from __future__ import annotations\n import re\n import string\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Union\n+from typing import TYPE_CHECKING, Any\n \n if TYPE_CHECKING:\n     from os import PathLike\n \n \n-def render_templatefile(path: Union[str, PathLike], **kwargs: Any) -> None:\n+def render_templatefile(path: str | PathLike, **kwargs: Any) -> None:\n     path_obj = Path(path)\n     raw = path_obj.read_text(\"utf8\")\n \n\n@@ -9,7 +9,7 @@ import os\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar\n+from typing import TYPE_CHECKING, Any, TypeVar\n from unittest import TestCase, mock\n \n from twisted.trial.unittest import SkipTest\n@@ -84,8 +84,8 @@ class TestSpider(Spider):\n \n \n def get_crawler(\n-    spidercls: Optional[type[Spider]] = None,\n-    settings_dict: Optional[dict[str, Any]] = None,\n+    spidercls: type[Spider] | None = None,\n+    settings_dict: dict[str, Any] | None = None,\n     prevent_warnings: bool = True,\n ) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n@@ -120,7 +120,7 @@ def get_testenv() -> dict[str, str]:\n \n \n def assert_samelines(\n-    testcase: TestCase, text1: str, text2: str, msg: Optional[str] = None\n+    testcase: TestCase, text1: str, text2: str, msg: str | None = None\n ) -> None:\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import os\n import sys\n-from typing import TYPE_CHECKING, Optional, cast\n+from typing import TYPE_CHECKING, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n@@ -15,7 +15,7 @@ if TYPE_CHECKING:\n \n \n class ProcessTest:\n-    command: Optional[str] = None\n+    command: str | None = None\n     prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n     cwd = os.getcwd()  # trial chdirs to temp dir\n \n@@ -23,7 +23,7 @@ class ProcessTest:\n         self,\n         args: Iterable[str],\n         check_code: bool = True,\n-        settings: Optional[str] = None,\n+        settings: str | None = None,\n     ) -> Deferred[TestProcessProtocol]:\n         from twisted.internet import reactor\n \n@@ -54,7 +54,7 @@ class TestProcessProtocol(ProcessProtocol):\n         self.deferred: Deferred[TestProcessProtocol] = Deferred()\n         self.out: bytes = b\"\"\n         self.err: bytes = b\"\"\n-        self.exitcode: Optional[int] = None\n+        self.exitcode: int | None = None\n \n     def outReceived(self, data: bytes) -> None:\n         self.out += data\n\n@@ -9,7 +9,7 @@ to the w3lib.url module. Always import those from there instead.\n from __future__ import annotations\n \n import re\n-from typing import TYPE_CHECKING, Optional, Union, cast\n+from typing import TYPE_CHECKING, Union, cast\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n@@ -50,7 +50,7 @@ def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n     return any(lowercase_path.endswith(ext) for ext in extensions)\n \n \n-def parse_url(url: UrlT, encoding: Optional[str] = None) -> ParseResult:\n+def parse_url(url: UrlT, encoding: str | None = None) -> ParseResult:\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n     \"\"\"\n\n@@ -1,6 +1,7 @@\n+from __future__ import annotations\n+\n import asyncio\n import sys\n-from typing import Optional\n \n from scrapy import Spider\n from scrapy.crawler import CrawlerProcess\n@@ -31,7 +32,7 @@ class UrlSpider(Spider):\n \n \n if __name__ == \"__main__\":\n-    ASYNCIO_EVENT_LOOP: Optional[str]\n+    ASYNCIO_EVENT_LOOP: str | None\n     try:\n         ASYNCIO_EVENT_LOOP = sys.argv[1]\n     except IndexError:\n\n@@ -2,9 +2,10 @@\n Some spiders used for testing and benchmarking\n \"\"\"\n \n+from __future__ import annotations\n+\n import asyncio\n import time\n-from typing import Optional\n from urllib.parse import urlencode\n \n from twisted.internet import defer\n@@ -82,19 +83,19 @@ class DelaySpider(MetaSpider):\n class LogSpider(MetaSpider):\n     name = \"log_spider\"\n \n-    def log_debug(self, message: str, extra: Optional[dict] = None):\n+    def log_debug(self, message: str, extra: dict | None = None):\n         self.logger.debug(message, extra=extra)\n \n-    def log_info(self, message: str, extra: Optional[dict] = None):\n+    def log_info(self, message: str, extra: dict | None = None):\n         self.logger.info(message, extra=extra)\n \n-    def log_warning(self, message: str, extra: Optional[dict] = None):\n+    def log_warning(self, message: str, extra: dict | None = None):\n         self.logger.warning(message, extra=extra)\n \n-    def log_error(self, message: str, extra: Optional[dict] = None):\n+    def log_error(self, message: str, extra: dict | None = None):\n         self.logger.error(message, extra=extra)\n \n-    def log_critical(self, message: str, extra: Optional[dict] = None):\n+    def log_critical(self, message: str, extra: dict | None = None):\n         self.logger.critical(message, extra=extra)\n \n     def parse(self, response):\n\n@@ -15,7 +15,7 @@ from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n from unittest import skipIf\n \n from pytest import mark\n@@ -117,9 +117,7 @@ class ProjectTest(unittest.TestCase):\n \n         return p, to_unicode(stdout), to_unicode(stderr)\n \n-    def find_in_file(\n-        self, filename: Union[str, os.PathLike], regex\n-    ) -> Optional[re.Match]:\n+    def find_in_file(self, filename: str | os.PathLike, regex) -> re.Match | None:\n         \"\"\"Find first pattern occurrence in file\"\"\"\n         pattern = re.compile(regex)\n         with Path(filename).open(\"r\", encoding=\"utf-8\") as f:\n@@ -198,7 +196,7 @@ class StartprojectTest(ProjectTest):\n \n \n def get_permissions_dict(\n-    path: Union[str, os.PathLike], renamings=None, ignore=None\n+    path: str | os.PathLike, renamings=None, ignore=None\n ) -> dict[str, str]:\n     def get_permissions(path: Path) -> str:\n         return oct(path.stat().st_mode)\n\n@@ -1,10 +1,11 @@\n+from __future__ import annotations\n+\n import contextlib\n import os\n import shutil\n import sys\n from pathlib import Path\n from tempfile import mkdtemp, mkstemp\n-from typing import Optional\n from unittest import SkipTest, mock\n \n from testfixtures import LogCapture\n@@ -692,7 +693,7 @@ class Https11CustomCiphers(unittest.TestCase):\n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n \n-    settings_dict: Optional[dict] = None\n+    settings_dict: dict | None = None\n \n     def setUp(self):\n         self.mockserver = MockServer()\n\n@@ -18,7 +18,7 @@ from io import BytesIO\n from logging import getLogger\n from pathlib import Path\n from string import ascii_letters, digits\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n from unittest import mock\n from urllib.parse import quote, urljoin\n from urllib.request import pathname2url\n@@ -66,7 +66,7 @@ def printf_escape(string):\n     return string.replace(\"%\", \"%%\")\n \n \n-def build_url(path: Union[str, PathLike]) -> str:\n+def build_url(path: str | PathLike) -> str:\n     path_str = str(path)\n     if path_str[0] != \"/\":\n         path_str = \"/\" + path_str\n\n@@ -1,7 +1,8 @@\n+from __future__ import annotations\n+\n import pickle\n import re\n import unittest\n-from typing import Optional\n \n from packaging.version import Version\n from pytest import mark\n@@ -16,7 +17,7 @@ from tests import get_testdata\n # a hack to skip base class tests in pytest\n class Base:\n     class LinkExtractorTestCase(unittest.TestCase):\n-        extractor_cls: Optional[type] = None\n+        extractor_cls: type | None = None\n \n         def setUp(self):\n             body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n\n@@ -1,6 +1,7 @@\n+from __future__ import annotations\n+\n import dataclasses\n import unittest\n-from typing import Optional\n \n import attr\n from itemadapter import ItemAdapter\n@@ -88,7 +89,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n \n class InitializationTestMixin:\n-    item_class: Optional[type] = None\n+    item_class: type | None = None\n \n     def test_keep_single_value(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n\n@@ -1,7 +1,8 @@\n+from __future__ import annotations\n+\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import Optional\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -57,7 +58,7 @@ class FileDownloadCrawlTestCase(TestCase):\n     store_setting_key = \"FILES_STORE\"\n     media_key = \"files\"\n     media_urls_key = \"file_urls\"\n-    expected_checksums: Optional[set[str]] = {\n+    expected_checksums: set[str] | None = {\n         \"5547178b89448faf0015a13f904c936e\",\n         \"c2281c83670e31d8aaab7cb642b824db\",\n         \"ed3f6538dc15d4d9179dae57319edc5f\",\n@@ -216,7 +217,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertIn(\"ZeroDivisionError\", str(log))\n \n \n-skip_pillow: Optional[str]\n+skip_pillow: str | None\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import dataclasses\n import hashlib\n import io\n@@ -5,7 +7,6 @@ import random\n import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n-from typing import Optional\n from unittest.mock import patch\n \n import attr\n@@ -19,7 +20,7 @@ from scrapy.pipelines.images import ImageException, ImagesPipeline, NoimagesDrop\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n-skip_pillow: Optional[str]\n+skip_pillow: str | None\n try:\n     from PIL import Image\n except ImportError:\n\n@@ -1,4 +1,4 @@\n-from typing import Optional\n+from __future__ import annotations\n \n from testfixtures import LogCapture\n from twisted.internet import reactor\n@@ -20,7 +20,7 @@ from scrapy.utils.test import get_crawler\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n-    skip_pillow: Optional[str] = (\n+    skip_pillow: str | None = (\n         \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n     )\n else:\n\n@@ -1,8 +1,9 @@\n+from __future__ import annotations\n+\n import collections\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -60,7 +61,7 @@ class MockCrawler(Crawler):\n \n \n class SchedulerHandler:\n-    priority_queue_cls: Optional[str] = None\n+    priority_queue_cls: str | None = None\n     jobdir = None\n \n     def create_scheduler(self):\n@@ -254,7 +255,7 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n \n \n class DownloaderAwareSchedulerTestMixin:\n-    priority_queue_cls: Optional[str] = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n+    priority_queue_cls: str | None = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n     reopen = False\n \n     def test_logic(self):\n\n@@ -1,4 +1,5 @@\n-from typing import Optional\n+from __future__ import annotations\n+\n from unittest import TestCase\n from urllib.parse import urljoin\n \n@@ -32,7 +33,7 @@ class MinimalScheduler:\n             return True\n         return False\n \n-    def next_request(self) -> Optional[Request]:\n+    def next_request(self) -> Request | None:\n         if self.has_pending_requests():\n             fp, request = self.requests.popitem()\n             return request\n\n@@ -1,5 +1,6 @@\n+from __future__ import annotations\n+\n from collections.abc import AsyncIterator, Iterable\n-from typing import Optional, Union\n from unittest import mock\n \n from testfixtures import LogCapture\n@@ -112,11 +113,11 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n     Should work for process_spider_output and, when it's supported, process_start_requests.\n     \"\"\"\n \n-    ITEM_TYPE: Union[type, tuple]\n+    ITEM_TYPE: type | tuple\n     RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n \n     @staticmethod\n-    def _construct_mw_setting(*mw_classes, start_index: Optional[int] = None):\n+    def _construct_mw_setting(*mw_classes, start_index: int | None = None):\n         if start_index is None:\n             start_index = 10\n         return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n@@ -127,7 +128,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n         yield {\"foo\": 3}\n \n     @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+    def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n         self.crawler = get_crawler(\n             Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n@@ -141,7 +142,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n \n     @defer.inlineCallbacks\n     def _test_simple_base(\n-        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n+        self, *mw_classes, downgrade: bool = False, start_index: int | None = None\n     ):\n         with LogCapture() as log:\n             result = yield self._get_middleware_result(\n@@ -155,7 +156,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n \n     @defer.inlineCallbacks\n     def _test_asyncgen_base(\n-        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None\n+        self, *mw_classes, downgrade: bool = False, start_index: int | None = None\n     ):\n         with LogCapture() as log:\n             result = yield self._get_middleware_result(\n@@ -337,7 +338,7 @@ class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n         yield {\"name\": \"test item\"}\n \n     @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+    def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n         self.crawler = get_crawler(\n             Spider, {\"SPIDER_MIDDLEWARES_BASE\": {}, \"SPIDER_MIDDLEWARES\": setting}\n@@ -441,7 +442,7 @@ class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n \n     @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+    def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n         self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES\": setting})\n         self.spider = self.crawler._create_spider(\"foo\")\n\n@@ -1,5 +1,7 @@\n+from __future__ import annotations\n+\n import warnings\n-from typing import Any, Optional\n+from typing import Any\n from unittest import TestCase\n from urllib.parse import urlparse\n \n@@ -35,7 +37,7 @@ class TestRefererMiddleware(TestCase):\n     req_meta: dict[str, Any] = {}\n     resp_headers: dict[str, str] = {}\n     settings: dict[str, Any] = {}\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n     ]\n \n@@ -65,7 +67,7 @@ class MixinDefault:\n     with some additional filtering of s3://\n     \"\"\"\n \n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n         (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n         (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n@@ -86,7 +88,7 @@ class MixinDefault:\n \n \n class MixinNoReferrer:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         (\"https://example.com/page.html\", \"https://example.com/\", None),\n         (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n         (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n@@ -96,7 +98,7 @@ class MixinNoReferrer:\n \n \n class MixinNoReferrerWhenDowngrade:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # TLS to TLS: send non-empty referrer\n         (\n             \"https://example.com/page.html\",\n@@ -178,7 +180,7 @@ class MixinNoReferrerWhenDowngrade:\n \n \n class MixinSameOrigin:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -247,7 +249,7 @@ class MixinSameOrigin:\n \n \n class MixinOrigin:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n         (\n             \"https://example.com/page.html\",\n@@ -271,7 +273,7 @@ class MixinOrigin:\n \n \n class MixinStrictOrigin:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n         (\n             \"https://example.com/page.html\",\n@@ -299,7 +301,7 @@ class MixinStrictOrigin:\n \n \n class MixinOriginWhenCrossOrigin:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -406,7 +408,7 @@ class MixinOriginWhenCrossOrigin:\n \n \n class MixinStrictOriginWhenCrossOrigin:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -518,7 +520,7 @@ class MixinStrictOriginWhenCrossOrigin:\n \n \n class MixinUnsafeUrl:\n-    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, bytes | None]] = [\n         # TLS to TLS: send referrer\n         (\n             \"https://example.com/sekrit.html\",\n@@ -969,7 +971,7 @@ class TestPolicyHeaderPrecedence004(\n class TestReferrerOnRedirect(TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n     scenarii: list[\n-        tuple[str, str, tuple[tuple[int, str], ...], Optional[bytes], Optional[bytes]]\n+        tuple[str, str, tuple[tuple[int, str], ...], bytes | None, bytes | None]\n     ] = [  # type: ignore[assignment]\n         (\n             \"http://scrapytest.org/1\",  # parent\n\n@@ -1,8 +1,9 @@\n+from __future__ import annotations\n+\n import json\n import unittest\n import warnings\n from hashlib import sha1\n-from typing import Optional, Union\n from weakref import WeakKeyDictionary\n \n from scrapy.http import Request\n@@ -56,12 +57,12 @@ class FingerprintTest(unittest.TestCase):\n     maxDiff = None\n \n     function: staticmethod = staticmethod(fingerprint)\n-    cache: Union[\n-        \"WeakKeyDictionary[Request, dict[tuple[Optional[tuple[bytes, ...]], bool], bytes]]\",\n-        \"WeakKeyDictionary[Request, dict[tuple[Optional[tuple[bytes, ...]], bool], str]]\",\n-    ] = _fingerprint_cache\n+    cache: (\n+        WeakKeyDictionary[Request, dict[tuple[tuple[bytes, ...] | None, bool], bytes]]\n+        | WeakKeyDictionary[Request, dict[tuple[tuple[bytes, ...] | None, bool], str]]\n+    ) = _fingerprint_cache\n     default_cache_key = (None, False)\n-    known_hashes: tuple[tuple[Request, Union[bytes, str], dict], ...] = (\n+    known_hashes: tuple[tuple[Request, bytes | str, dict], ...] = (\n         (\n             Request(\"http://example.org\"),\n             b\"xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
