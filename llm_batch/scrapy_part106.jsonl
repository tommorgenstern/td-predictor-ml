{"custom_id": "scrapy#30d94b5bf577857649aa6c2469957705e476d2b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 45 | Files Changed: 5 | Hunks: 33 | Methods Changed: 26 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 107 | Churn Cumulative: 847 | Contributors (this commit): 6 | Commits (past 90d): 30 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -185,9 +185,9 @@ REDIRECT_PRIORITY_ADJUST = +2\n \n REQUEST_HANDLERS = {}\n REQUEST_HANDLERS_BASE = {\n-    'file': 'scrapy.core.downloader.handlers.file.download_file',\n-    'http': 'scrapy.core.downloader.handlers.http.download_http',\n-    'https': 'scrapy.core.downloader.handlers.http.download_http',\n+    'file': 'scrapy.core.downloader.handlers.file.FileRequestHandler',\n+    'http': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n+    'https': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n }\n \n REQUESTS_QUEUE_SIZE = 0\n\n@@ -1,6 +1,6 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n-from scrapy.exceptions import NotSupported\n+from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n from scrapy.utils.misc import load_object\n@@ -10,17 +10,26 @@ class RequestHandlers(object):\n \n     def __init__(self):\n         self._handlers = {}\n+        self._notconfigured = {}\n         handlers = settings.get('REQUEST_HANDLERS_BASE')\n         handlers.update(settings.get('REQUEST_HANDLERS', {}))\n-        for scheme, cls in handlers.iteritems():\n-            self._handlers[scheme] = load_object(cls)\n+        for scheme, clspath in handlers.iteritems():\n+            cls = load_object(clspath)\n+            try:\n+                dh = cls()\n+            except NotConfigured, ex:\n+                self._notconfigured[scheme] = str(ex)\n+            else:\n+                self._handlers[scheme] = dh.download_request\n \n     def download_request(self, request, spider):\n         scheme = urlparse_cached(request).scheme\n         try:\n             handler = self._handlers[scheme]\n         except KeyError:\n-            raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (scheme, request.url))\n+            msg = self._notconfigured.get(scheme, \\\n+                    'no handler available for that scheme')\n+            raise NotSupported(\"Unsupported URL scheme '%s': %s\" % (scheme, msg))\n         return handler(request, spider)\n \n \n\n@@ -7,14 +7,15 @@ from twisted.internet import defer\n from scrapy.core.downloader.responsetypes import responsetypes\n \n \n-def download_file(request, spider):\n-    \"\"\"Return a deferred for a file download.\"\"\"\n-    return defer.maybeDeferred(_all_in_one_read_download_file, request, spider)\n+class FileRequestHandler(object):\n+    \"\"\"file download\"\"\"\n \n-def _all_in_one_read_download_file(request, spider):\n+    def download_request(self, request, spider):\n+        return defer.maybeDeferred(self._one_pass_read, request)\n+\n+    def _one_pass_read(self, request):\n         filepath = url2pathname(request.url.split(\"file://\")[1])\n         with open(filepath) as f:\n             body = f.read()\n         respcls = responsetypes.from_args(filename=filepath, body=body)\n         return respcls(url=request.url, body=body)\n-\n\n@@ -13,11 +13,24 @@ ssl_supported = 'ssl' in optional_features\n if ssl_supported:\n     from twisted.internet.ssl import ClientContextFactory\n \n-\n HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n+DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')\n \n-def _create_factory(request, spider):\n+\n+class HttpRequestHandler(object):\n+\n+    def __init__(self, httpclientfactory=HTTPClientFactory, \\\n+            download_timeout=DOWNLOAD_TIMEOUT):\n+        self.httpclientfactory = httpclientfactory\n+        self.download_timeout = download_timeout\n+\n+    def download_request(self, request, spider):\n+        \"\"\"Return a deferred for the HTTP download\"\"\"\n+        factory = self._create_factory(request, spider)\n+        self._connect(factory)\n+        return factory.deferred\n+\n+    def _create_factory(self, request, spider):\n         def _download_signals(response):\n             send_catch_log(signal=signals.request_uploaded, request=request, \\\n                 spider=spider)\n@@ -25,26 +38,17 @@ def _create_factory(request, spider):\n                 spider=spider)\n             return response\n \n-    timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n-    factory = HTTPClientFactory(request, timeout)\n+        timeout = getattr(spider, \"download_timeout\", None) or self.download_timeout\n+        factory = self.httpclientfactory(request, timeout)\n         factory.deferred.addCallbacks(_download_signals)\n         return factory\n \n-\n-def _connect(factory):\n+    def _connect(self, factory):\n         host, port = factory.host, factory.port\n         if factory.scheme == 'https':\n             if ssl_supported:\n-            return reactor.connectSSL(host, port, factory, ClientContextFactory())\n+                return reactor.connectSSL(host, port, factory, \\\n+                        ClientContextFactory())\n             raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n         else:\n             return reactor.connectTCP(host, port, factory)\n-\n-\n-def download_http(request, spider):\n-    \"\"\"Return a deferred for the HTTP download\"\"\"\n-    factory = _create_factory(request, spider)\n-    _connect(factory)\n-    return factory.deferred\n-\n-\n\n@@ -10,8 +10,8 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         PayloadResource, BrokenDownloadResource\n \n from scrapy.core.downloader.webclient import PartialDownloadError\n-from scrapy.core.downloader.handlers.file import download_file\n-from scrapy.core.downloader.handlers.http import download_http\n+from scrapy.core.downloader.handlers.file import FileRequestHandler\n+from scrapy.core.downloader.handlers.http import HttpRequestHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n \n@@ -23,6 +23,7 @@ class FileTestCase(unittest.TestCase):\n         fd = open(self.tmpname + '^', 'w')\n         fd.write('0123456789')\n         fd.close()\n+        self.download_request = FileRequestHandler().download_request\n \n     def test_download(self):\n         def _test(response):\n@@ -32,11 +33,11 @@ class FileTestCase(unittest.TestCase):\n \n         request = Request('file://%s' % self.tmpname + '^')\n         assert request.url.upper().endswith('%5E')\n-        return download_file(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_non_existent(self):\n         request = Request('file://%s' % self.mktemp())\n-        d = download_file(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         return self.assertFailure(d, IOError)\n \n \n@@ -57,6 +58,7 @@ class HttpTestCase(unittest.TestCase):\n         self.wrapper = WrappingFactory(self.site)\n         self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n+        self.download_request = HttpRequestHandler().download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -66,28 +68,28 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n     def test_download_head(self):\n         request = Request(self.getURL('file'), method='HEAD')\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, '')\n         return d\n \n     def test_redirect_status(self):\n         request = Request(self.getURL('redirect'))\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n     def test_redirect_status_head(self):\n         request = Request(self.getURL('redirect'), method='HEAD')\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEquals, 302)\n         return d\n@@ -96,7 +98,7 @@ class HttpTestCase(unittest.TestCase):\n         spider = BaseSpider('foo')\n         spider.download_timeout = 0.000001\n         request = Request(self.getURL('wait'))\n-        d = download_http(request, spider)\n+        d = self.download_request(request, spider)\n         return self.assertFailure(d, defer.TimeoutError)\n \n     def test_host_header_not_in_request_headers(self):\n@@ -105,7 +107,7 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers, {})\n \n         request = Request(self.getURL('host'))\n-        return download_http(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_host_header_seted_in_request_headers(self):\n         def _test(response):\n@@ -113,9 +115,9 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers.get('Host'), 'example.com')\n \n         request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n-        return download_http(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, 'example.com')\n         return d\n@@ -123,14 +125,14 @@ class HttpTestCase(unittest.TestCase):\n     def test_payload(self):\n         body = '1'*100 # PayloadResource requires body length to be 100\n         request = Request(self.getURL('payload'), method='POST', body=body)\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, body)\n         return d\n \n     def test_broken_download(self):\n         request = Request(self.getURL('broken'))\n-        d = download_http(request, BaseSpider('foo'))\n+        d = self.download_request(request, BaseSpider('foo'))\n         return self.assertFailure(d, PartialDownloadError)\n \n \n@@ -151,6 +153,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n+        self.download_request = HttpRequestHandler().download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -166,7 +169,7 @@ class HttpProxyTestCase(unittest.TestCase):\n \n         http_proxy = self.getURL('')\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n-        return download_http(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_download_without_proxy(self):\n         def _test(response):\n@@ -175,4 +178,4 @@ class HttpProxyTestCase(unittest.TestCase):\n             self.assertEquals(response.body, '/path/to/resource')\n \n         request = Request(self.getURL('path/to/resource'))\n-        return download_http(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b68c3c1b15eedeb1cfce8c5e7110e20084999fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 137 | Churn Cumulative: 875 | Contributors (this commit): 6 | Commits (past 90d): 31 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,3 +26,10 @@ except ImportError:\n     pass\n else:\n     optional_features.add('ssl')\n+\n+try:\n+    import boto\n+except ImportError:\n+    pass\n+else:\n+    optional_features.add('boto')\n\n@@ -188,6 +188,7 @@ REQUEST_HANDLERS_BASE = {\n     'file': 'scrapy.core.downloader.handlers.file.FileRequestHandler',\n     'http': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n     'https': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n+    's3': 'scrapy.core.downloader.handlers.s3.S3RequestHandler',\n }\n \n REQUESTS_QUEUE_SIZE = 0\n\n@@ -0,0 +1,34 @@\n+from scrapy import optional_features\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.conf import settings\n+from .http import HttpRequestHandler\n+\n+\n+class S3RequestHandler(object):\n+\n+    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, \\\n+            httprequesthandler=HttpRequestHandler):\n+        if 'boto' not in optional_features:\n+            raise NotConfigured(\"missing boto library\")\n+\n+        if not aws_access_key_id:\n+            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n+        if not aws_secret_access_key:\n+            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n+\n+        from boto import connect_s3\n+        try:\n+            self.conn = connect_s3(aws_access_key_id, aws_secret_access_key)\n+        except Exception, ex:\n+            raise NotConfigured(str(ex))\n+        self._download_http = httprequesthandler().download_request\n+\n+    def download_request(self, request, spider):\n+        p = urlparse_cached(request)\n+        scheme = 'https' if request.meta.get('is_secure') else 'http'\n+        url = '%s://%s.s3.amazonaws.com%s' % (scheme, p.hostname, p.path)\n+        httpreq = request.replace(url=url)\n+        self.conn.add_aws_auth_header(httpreq.headers, httpreq.method, \\\n+                '%s/%s' % (p.hostname, p.path))\n+        return self._download_http(httpreq, spider)\n\n@@ -12,8 +12,10 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n from scrapy.core.downloader.webclient import PartialDownloadError\n from scrapy.core.downloader.handlers.file import FileRequestHandler\n from scrapy.core.downloader.handlers.http import HttpRequestHandler\n+from scrapy.core.downloader.handlers.s3 import S3RequestHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n+from scrapy import optional_features\n \n \n class FileTestCase(unittest.TestCase):\n@@ -179,3 +181,96 @@ class HttpProxyTestCase(unittest.TestCase):\n \n         request = Request(self.getURL('path/to/resource'))\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+\n+\n+class HttpRequestHandlerMock(object):\n+    def download_request(self, request, spider):\n+        return request\n+\n+class S3TestCase(unittest.TestCase):\n+    skip = 'boto' not in optional_features and 'missing boto library'\n+\n+    # test use same example keys than amazon developer guide\n+    # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n+    # and the tests described here are the examples from that manual\n+\n+    AWS_ACCESS_KEY_ID = '0PN5J17HBGZHT7JJ3X82'\n+    AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n+\n+    def setUp(self):\n+        s3reqh = S3RequestHandler(self.AWS_ACCESS_KEY_ID, \\\n+                self.AWS_SECRET_ACCESS_KEY, \\\n+                httprequesthandler=HttpRequestHandlerMock)\n+        self.download_request = s3reqh.download_request\n+        self.spider = BaseSpider('foo')\n+\n+    def test_request_signing1(self):\n+        # gets an object from the johnsmith bucket.\n+        req = Request('s3://johnsmith/photos/puppy.jpg',\n+                headers={'Date': 'Tue, 27 Mar 2007 19:36:42 +0000'})\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n+\n+    def test_request_signing2(self):\n+        # puts an object into the johnsmith bucket.\n+        req = Request('s3://johnsmith/photos/puppy.jpg', method='PUT', headers={\n+            'Content-Type': 'image/jpeg',\n+            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n+            'Content-Length': '94328',\n+            })\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n+\n+    def test_request_signing3(self):\n+        # lists the content of the johnsmith bucket.\n+        req = Request('s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy', \\\n+                method='GET', headers={\n+                    'User-Agent': 'Mozilla/5.0',\n+                    'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n+                    })\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n+\n+    def test_request_signing4(self):\n+        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n+        req = Request('s3://johnsmith/?acl', \\\n+                method='GET', headers={'Date': 'Tue, 27 Mar 2007 19:44:46 +0000'})\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n+\n+    def test_request_signing5(self):\n+        # deletes an object from the 'johnsmith' bucket using the \n+        # path-style and Date alternative.\n+        req = Request('s3://johnsmith/photos/puppy.jpg', \\\n+                method='DELETE', headers={\n+                    'Date': 'Tue, 27 Mar 2007 21:20:27 +0000',\n+                    'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n+                    })\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n+\n+    def test_request_signing6(self):\n+        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n+        req = Request('s3://static.johnsmith.net:8080/db-backup.dat.gz', \\\n+                method='PUT', headers={\n+                    'User-Agent': 'curl/7.15.5',\n+                    'Host': 'static.johnsmith.net:8080',\n+                    'Date': 'Tue, 27 Mar 2007 21:06:08 +0000',\n+                    'x-amz-acl': 'public-read',\n+                    'content-type': 'application/x-download',\n+                    'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',\n+                    'X-Amz-Meta-ReviewedBy': 'joe@johnsmith.net,jane@johnsmith.net',\n+                    'X-Amz-Meta-FileChecksum': '0x02661779',\n+                    'X-Amz-Meta-ChecksumAlgorithm': 'crc32',\n+                    'Content-Disposition': 'attachment; filename=database.dat',\n+                    'Content-Encoding': 'gzip',\n+                    'Content-Length': '5913339',\n+                    })\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(httpreq.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b9fa7fbaaf007106cd87bc88cffa7c35ca9967c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 34 | Churn Cumulative: 179 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,14 +40,17 @@ class OffsiteMiddleware(object):\n         host = urlparse_cached(request).hostname or ''\n         return bool(regex.search(host))\n \n-    def get_host_regex(self, domains):\n+    def get_host_regex(self, spider):\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n-        domains = [d.replace('.', r'\\.') for d in domains]\n+        allowed_domains = getattr(spider, 'allowed_domains', None)\n+        if not allowed_domains:\n+            return re.compile('') # allow all by default\n+        domains = [d.replace('.', r'\\.') for d in allowed_domains]\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n-        self.host_regexes[spider] = self.get_host_regex(spider.allowed_domains)\n+        self.host_regexes[spider] = self.get_host_regex(spider)\n         self.domains_seen[spider] = set()\n \n     def spider_closed(self, spider):\n\n@@ -8,13 +8,13 @@ from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n-        self.spider.name = 'scrapytest.org'\n-        self.spider.allowed_domains = ['scrapytest.org', 'scrapy.org']\n-\n+        self.spider = self._get_spider()\n         self.mw = OffsiteMiddleware()\n         self.mw.spider_opened(self.spider)\n \n+    def _get_spider(self):\n+        return BaseSpider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n+\n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n \n@@ -30,3 +30,20 @@ class TestOffsiteMiddleware(TestCase):\n     def tearDown(self):\n         self.mw.spider_closed(self.spider)\n \n+\n+class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n+\n+    def _get_spider(self):\n+        return BaseSpider('foo', allowed_domains=None)\n+\n+    def test_process_spider_output(self):\n+        res = Response('http://scrapytest.org')\n+        reqs = [Request('http://a.com/b.html'), Request('http://b.com/1')]\n+        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        self.assertEquals(out, reqs)\n+\n+class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n+\n+    def _get_spider(self):\n+        return BaseSpider('foo')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58feb155283a5c30e7d251990f8bc6e34ac840c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 15 | Churn Cumulative: 569 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,8 @@ class FilesystemCacheStorage(object):\n             body = f.read()\n         with open(join(rpath, 'response_headers'), 'rb') as f:\n             rawheaders = f.read()\n-        url = metadata['url']\n+        # We failback to metadata['url'] to support old generated caches ' should be removed for Scrapy 0.11\n+        url = metadata.get('response_url') or metadata['url']\n         status = metadata['status']\n         headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url)\n@@ -95,6 +96,7 @@ class FilesystemCacheStorage(object):\n             'url': request.url,\n             'method': request.method,\n             'status': response.status,\n+            'response_url': response.url,\n             'timestamp': time(),\n         }\n         with open(join(rpath, 'meta'), 'wb') as f:\n\n@@ -68,6 +68,17 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.assertEqualResponse(self.response, response)\n         assert 'cached' in response.flags\n \n+    def test_different_request_response_urls(self):\n+        mw = HttpCacheMiddleware(self._get_settings())\n+        req = Request('http://host.com/path')\n+        res = Response('http://host2.net/test.html')\n+        assert mw.process_request(req, self.spider) is None\n+        mw.process_response(req, res, self.spider)\n+        cached = mw.process_request(req, self.spider)\n+        assert isinstance(cached, Response)\n+        self.assertEqualResponse(res, cached)\n+        assert 'cached' in cached.flags\n+\n     def test_middleware_ignore_missing(self):\n         mw = self._get_middleware(HTTPCACHE_IGNORE_MISSING=True)\n         self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1abaa7946916fc40ca3af82120f03b50cadbba65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 41 | Churn Cumulative: 1030 | Contributors (this commit): 6 | Commits (past 90d): 34 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -144,6 +144,7 @@ HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n+HTTPCACHE_IGNORE_SCHEMES = ['file']\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -22,6 +22,7 @@ class HttpCacheMiddleware(object):\n     def __init__(self, settings=conf.settings):\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n+        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n@@ -51,7 +52,7 @@ class HttpCacheMiddleware(object):\n         return response.status not in self.ignore_http_codes\n \n     def is_cacheable(self, request):\n-        return urlparse_cached(request).scheme in ['http', 'https']\n+        return urlparse_cached(request).scheme not in self.ignore_schemes\n \n \n class FilesystemCacheStorage(object):\n\n@@ -88,6 +88,43 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.assertEqualResponse(self.response, response)\n         assert 'cached' in response.flags\n \n+    def test_middleware_ignore_schemes(self):\n+        # http responses are cached by default\n+        req, res = Request('http://test.com/'), Response('http://test.com/')\n+        mw = self._get_middleware()\n+        assert mw.process_request(req, self.spider) is None\n+        mw.process_response(req, res, self.spider)\n+        cached = mw.process_request(req, self.spider)\n+        assert isinstance(cached, Response), type(cached)\n+        self.assertEqualResponse(res, cached)\n+        assert 'cached' in cached.flags\n+\n+        # file response is not cached by default\n+        req, res = Request('file:///tmp/t.txt'), Response('file:///tmp/t.txt')\n+        mw = self._get_middleware()\n+        assert mw.process_request(req, self.spider) is None\n+        mw.process_response(req, res, self.spider)\n+        assert mw.storage.retrieve_response(self.spider, req) is None\n+        assert mw.process_request(req, self.spider) is None\n+\n+        # s3 scheme response is cached by default\n+        req, res = Request('s3://bucket/key'), Response('http://bucket/key')\n+        mw = self._get_middleware()\n+        assert mw.process_request(req, self.spider) is None\n+        mw.process_response(req, res, self.spider)\n+        cached = mw.process_request(req, self.spider)\n+        assert isinstance(cached, Response), type(cached)\n+        self.assertEqualResponse(res, cached)\n+        assert 'cached' in cached.flags\n+\n+        # ignore s3 scheme\n+        req, res = Request('s3://bucket/key2'), Response('http://bucket/key2')\n+        mw = self._get_middleware(HTTPCACHE_IGNORE_SCHEMES=['s3'])\n+        assert mw.process_request(req, self.spider) is None\n+        mw.process_response(req, res, self.spider)\n+        assert mw.storage.retrieve_response(self.spider, req) is None\n+        assert mw.process_request(req, self.spider) is None\n+\n     def test_middleware_ignore_http_codes(self):\n         # test response is not cached\n         mw = self._get_middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3921ab016b871ee430fbd4be27e2f84520122f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 192 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,8 +26,6 @@ class BaseSpider(object_ref):\n         self.__dict__.update(kwargs)\n         if not hasattr(self, 'start_urls'):\n             self.start_urls = []\n-        if not hasattr(self, 'allowed_domains'):\n-            self.allowed_domains = []\n \n     def log(self, message, level=log.DEBUG):\n         \"\"\"Log the given messages at the given log level. Always use this\n\n@@ -24,7 +24,6 @@ class BaseSpiderTest(unittest.TestCase):\n         spider = self.spider_class(\"example.com\")\n         self.assertEqual(spider.name, 'example.com')\n         self.assertEqual(spider.start_urls, [])\n-        self.assertEqual(spider.allowed_domains, [])\n \n     def test_spider_args(self):\n         \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b800fdcb4de9e8eb8176be1f47dec92aee0f120d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,8 @@\n+import sqlite3\n+\n from zope.interface import implements\n \n+from scrapy import log\n from scrapy.interfaces import ISpiderQueue\n from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n \n@@ -9,7 +12,12 @@ class SqliteSpiderQueue(object):\n     implements(ISpiderQueue)\n \n     def __init__(self, database=None, table='spider_queue'):\n+        try:\n             self.q = JsonSqlitePriorityQueue(database, table)\n+        except sqlite3.Error, e:\n+            self.q = JsonSqlitePriorityQueue(':memory:', table)\n+            log.msg(\"Cannot open SQLite %r - using in-memory spider queue \" \\\n+                \"instead. Error was: %r\" % (database, str(e)), log.WARNING)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a66bef7925d2d9f33fecb7ac5e7ace499ea66cf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 5 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1005 | Contributors (this commit): 5 | Commits (past 90d): 46 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -178,6 +178,8 @@ MEMUSAGE_WARNING_MB = 0\n \n NEWSPIDER_MODULE = ''\n \n+QUEUE_POLL_INTERVAL = 5\n+\n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REDIRECT_MAX_METAREFRESH_DELAY = 100\n@@ -234,7 +236,6 @@ SQLITE_DB = 'scrapy.db'\n \n SQS_QUEUE = 'scrapy'\n SQS_VISIBILITY_TIMEOUT = 7200\n-SQS_POLLING_DELAY = 30\n SQS_REGION = 'us-east-1'\n \n STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n\n@@ -13,7 +13,6 @@ class SQSSpiderQueue(object):\n     implements(ISpiderQueue)\n \n     def __init__(self, *a, **kw):\n-        self.polling_delay = kw.pop('polling_delay', 30)\n         self.queue_name = kw.pop('queue_name', 'scrapy')\n         self.region_name = kw.pop('region_name', 'us-east-1')\n         self.visibility_timeout = kw.pop('visibility_timeout', 7200)\n@@ -26,7 +25,6 @@ class SQSSpiderQueue(object):\n     @classmethod\n     def from_settings(cls, settings):\n         return cls(\n-            polling_delay=settings.getint('SQS_POLLING_DELAY'),\n             queue_name=settings['SQS_QUEUE'],\n             region_name=settings['SQS_REGION'],\n             visibility_timeout=settings.getint('SQS_VISIBILITY_TIMEOUT'),\n\n@@ -37,7 +37,9 @@ class Crawler(object):\n         spq_cls = load_object(self.settings['SPIDER_QUEUE_CLASS'])\n         spq = spq_cls.from_settings(self.settings)\n         keepalive = self.settings.getbool('KEEP_ALIVE')\n-        self.queue = ExecutionQueue(self.spiders, spq, keepalive)\n+        pollint = self.settings.getfloat('QUEUE_POLL_INTERVAL')\n+        self.queue = ExecutionQueue(self.spiders, spq, poll_interval=pollint,\n+            keep_alive=keepalive)\n         self.engine = ExecutionEngine(self.settings, self._spider_closed)\n \n     @defer.inlineCallbacks\n@@ -46,7 +48,7 @@ class Crawler(object):\n         if spider:\n             self._start_spider(spider, requests)\n         if self.engine.has_capacity() and not self._nextcall.active():\n-            self._nextcall = reactor.callLater(self.queue.polling_delay, \\\n+            self._nextcall = reactor.callLater(self.queue.poll_interval, \\\n                 self._spider_closed)\n \n     @defer.inlineCallbacks\n\n@@ -8,13 +8,12 @@ from scrapy import log\n \n class ExecutionQueue(object):\n \n-    polling_delay = 5\n-\n-    def __init__(self, spiders, queue, keepalive=False):\n+    def __init__(self, spiders, queue, poll_interval=5, keep_alive=False):\n         self.spider_requests = []\n+        self.poll_interval = poll_interval\n         self._spiders = spiders\n         self._queue = queue\n-        self._keepalive = keepalive\n+        self._keepalive = keep_alive\n \n     def _append_next(self):\n         \"\"\"Called when there are no more items left in self.spider_requests.\n\n@@ -30,7 +30,7 @@ class ExecutionQueueTest(unittest.TestCase):\n     keep_alive = False\n \n     def setUp(self):\n-        self.queue = ExecutionQueue(TestSpiderManager(), None, self.keep_alive)\n+        self.queue = ExecutionQueue(TestSpiderManager(), None, keep_alive=self.keep_alive)\n         self.spider = TestSpider()\n         self.request = Request('about:none')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1190f97944fac0daf012eab7e622a8673a43a105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 425 | Contributors (this commit): 5 | Commits (past 90d): 26 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -136,9 +136,6 @@ FEED_EXPORTERS_BASE = {\n     'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n }\n \n-GROUPSETTINGS_ENABLED = False\n-GROUPSETTINGS_MODULE = ''\n-\n HTTPCACHE_DIR = ''\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#630db4fecf70e45e646a3f461ad9adeb082678db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 28 | Churn Cumulative: 112 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,21 +1,12 @@\n-\"\"\"Download handler for file:// scheme\"\"\"\n-from __future__ import with_statement\n-\n-from urllib import url2pathname\n-\n-from twisted.internet import defer\n from scrapy.core.downloader.responsetypes import responsetypes\n-\n+from scrapy.utils.url import file_uri_to_path\n+from scrapy.utils.decorator import defers\n \n class FileRequestHandler(object):\n-    \"\"\"file download\"\"\"\n \n+    @defers\n     def download_request(self, request, spider):\n-        return defer.maybeDeferred(self._one_pass_read, request)\n-\n-    def _one_pass_read(self, request):\n-        filepath = url2pathname(request.url.split(\"file://\")[1])\n-        with open(filepath) as f:\n-            body = f.read()\n+        filepath = file_uri_to_path(request.url)\n+        body = open(filepath, 'rb').read()\n         respcls = responsetypes.from_args(filename=filepath, body=body)\n         return respcls(url=request.url, body=body)\n\n@@ -1,6 +1,8 @@\n import warnings\n from functools import wraps\n \n+from twisted.internet.defer import maybeDeferred\n+\n \n def deprecated(use_instead=None):\n     \"\"\"This is a decorator which can be used to mark functions\n@@ -17,3 +19,10 @@ def deprecated(use_instead=None):\n             return func(*args, **kwargs)\n         return new_func\n     return wrapped\n+\n+def defers(func):\n+    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n+    @wraps(func)\n+    def wrapped(*a, **kw):\n+        return maybeDeferred(func, *a, **kw)\n+    return wrapped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce884192c9789326b23f5207d4e0cacd842ba0f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 349 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,7 @@ from scrapy.core.downloader.handlers.http import HttpRequestHandler\n from scrapy.core.downloader.handlers.s3 import S3RequestHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n+from scrapy.utils.url import path_to_file_uri\n from scrapy import optional_features\n \n \n@@ -33,7 +34,7 @@ class FileTestCase(unittest.TestCase):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.body, '0123456789')\n \n-        request = Request('file://%s' % self.tmpname + '^')\n+        request = Request(path_to_file_uri(self.tmpname + '^'))\n         assert request.url.upper().endswith('%5E')\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6bf52fb50ee279f5b530f48cfda96cb5b2bb73ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 14 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 42 | Churn Cumulative: 650 | Contributors (this commit): 5 | Commits (past 90d): 39 | Contributors (cumulative): 8 | DMM Complexity: 0.25\n\nDIFF:\n@@ -251,11 +251,11 @@ URLLENGTH_LIMIT = 2083\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n TELNETCONSOLE_ENABLED = 1\n-TELNETCONSOLE_PORT = 6023  # if None, uses a dynamic port\n+TELNETCONSOLE_PORT = [6023, 6073]\n \n WEBSERVICE_ENABLED = True\n WEBSERVICE_LOGFILE = None\n-WEBSERVICE_PORT = 6080\n+WEBSERVICE_PORT = [6080, 7030]\n WEBSERVICE_RESOURCES = {}\n WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n\n@@ -8,16 +8,17 @@ import pprint\n \n from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n-from twisted.internet import reactor, protocol\n+from twisted.internet import protocol\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy.project import crawler\n from scrapy.stats import stats\n-from scrapy import signals\n+from scrapy import log, signals\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n+from scrapy.utils.reactor import listen_tcp\n from scrapy.conf import settings\n \n try:\n@@ -37,12 +38,14 @@ class TelnetConsole(protocol.ServerFactory):\n         if not settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n         self.noisy = False\n-        self.portnum = settings.getint('TELNETCONSOLE_PORT')\n+        self.portrange = map(int, settings.getlist('TELNETCONSOLE_PORT'))\n         dispatcher.connect(self.start_listening, signals.engine_started)\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n     def start_listening(self):\n-        self.port = reactor.listenTCP(self.portnum, self)\n+        self.port = listen_tcp(self.portrange, self)\n+        log.msg(\"Telnet console listening on port %d\" % self.port.getHost().port,\n+            log.DEBUG)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n@@ -0,0 +1,15 @@\n+from twisted.internet import reactor, error\n+\n+def listen_tcp(portrange, factory):\n+    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n+    assert len(portrange) in [1, 2], \"invalid portrange: %s\" % portrange\n+    if not hasattr(portrange, '__iter__'):\n+        return reactor.listenTCP(portrange, factory)\n+    if len(portrange) == 1:\n+        return reactor.listenTCP(portrange[0], factory)\n+    for x in range(portrange[0], portrange[1]+1):\n+        try:\n+            return reactor.listenTCP(x, factory)\n+        except error.CannotListenError:\n+            if x == portrange[1]:\n+                raise\n\n@@ -1,19 +1,19 @@\n \"\"\"\n Scrapy web services extension\n \n-See docs/topics/ws.rst\n+See docs/topics/webservice.rst\n \"\"\"\n \n-from twisted.internet import reactor\n from twisted.web import server, error\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n-from scrapy import signals\n+from scrapy import log, signals\n from scrapy.utils.jsonrpc import jsonrpc_server_call\n from scrapy.utils.serialize import ScrapyJSONEncoder, ScrapyJSONDecoder\n from scrapy.utils.misc import load_object\n from scrapy.utils.txweb import JsonResource as JsonResource_\n+from scrapy.utils.reactor import listen_tcp\n from scrapy.utils.conf import build_component_list\n from scrapy.conf import settings\n \n@@ -67,7 +67,7 @@ class WebService(server.Site):\n         if not settings.getbool('WEBSERVICE_ENABLED'):\n             raise NotConfigured\n         logfile = settings['WEBSERVICE_LOGFILE']\n-        self.portnum = settings.getint('WEBSERVICE_PORT')\n+        self.portrange = map(int, settings.getlist('WEBSERVICE_PORT'))\n         root = RootResource()\n         reslist = build_component_list(settings['WEBSERVICE_RESOURCES_BASE'], \\\n             settings['WEBSERVICE_RESOURCES'])\n@@ -80,7 +80,9 @@ class WebService(server.Site):\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n     def start_listening(self):\n-        self.port = reactor.listenTCP(self.portnum, self)\n+        self.port = listen_tcp(self.portrange, self)\n+        log.msg(\"Web service listening on port %d\" % self.port.getHost().port,\n+            log.DEBUG)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#067ec65d97c3f1af84240a04652bfb30337f953a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 529 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,3 @@ class RequestHandlers(object):\n                     'no handler available for that scheme')\n             raise NotSupported(\"Unsupported URL scheme '%s': %s\" % (scheme, msg))\n         return handler(request, spider)\n-\n-\n-download_any = RequestHandlers().download_request\n\n@@ -13,7 +13,7 @@ from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n-from .handlers import download_any\n+from .handlers import RequestHandlers\n \n \n class SpiderInfo(object):\n@@ -71,6 +71,7 @@ class Downloader(object):\n \n     def __init__(self):\n         self.sites = {}\n+        self.handlers = RequestHandlers()\n         self.middleware = DownloaderMiddlewareManager()\n         self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n \n@@ -145,7 +146,7 @@ class Downloader(object):\n         # The order is very important for the following deferreds. Do not change!\n \n         # 1. Create the download deferred\n-        dfd = mustbe_deferred(download_any, request, spider)\n+        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n \n         # 2. After response arrives,  remove the request from transferring\n         # state to free up the transferring slot so it can be used by the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#766f2d910dbb4fd8d4257e9e93e9cb9da7a884a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 28 | Files Changed: 7 | Hunks: 19 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 1550 | Contributors (this commit): 6 | Commits (past 90d): 49 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -42,6 +42,15 @@ DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n DOWNLOAD_DELAY = 0\n+\n+DOWNLOAD_HANDLERS = {}\n+DOWNLOAD_HANDLERS_BASE = {\n+    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n+    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n+    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n+    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n+}\n+\n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n DOWNLOADER_DEBUG = False\n@@ -183,14 +192,6 @@ REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n-REQUEST_HANDLERS = {}\n-REQUEST_HANDLERS_BASE = {\n-    'file': 'scrapy.core.downloader.handlers.file.FileRequestHandler',\n-    'http': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n-    'https': 'scrapy.core.downloader.handlers.http.HttpRequestHandler',\n-    's3': 'scrapy.core.downloader.handlers.s3.S3RequestHandler',\n-}\n-\n REQUESTS_QUEUE_SIZE = 0\n \n # contrib.middleware.retry.RetryMiddleware default settings\n\n@@ -6,13 +6,13 @@ from scrapy.conf import settings\n from scrapy.utils.misc import load_object\n \n \n-class RequestHandlers(object):\n+class DownloadHandlers(object):\n \n     def __init__(self):\n         self._handlers = {}\n         self._notconfigured = {}\n-        handlers = settings.get('REQUEST_HANDLERS_BASE')\n-        handlers.update(settings.get('REQUEST_HANDLERS', {}))\n+        handlers = settings.get('DOWNLOAD_HANDLERS_BASE')\n+        handlers.update(settings.get('DOWNLOAD_HANDLERS', {}))\n         for scheme, clspath in handlers.iteritems():\n             cls = load_object(clspath)\n             try:\n\n@@ -2,7 +2,7 @@ from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.url import file_uri_to_path\n from scrapy.utils.decorator import defers\n \n-class FileRequestHandler(object):\n+class FileDownloadHandler(object):\n \n     @defers\n     def download_request(self, request, spider):\n\n@@ -17,7 +17,7 @@ HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')\n \n \n-class HttpRequestHandler(object):\n+class HttpDownloadHandler(object):\n \n     def __init__(self, httpclientfactory=HTTPClientFactory, \\\n             download_timeout=DOWNLOAD_TIMEOUT):\n\n@@ -2,13 +2,13 @@ from scrapy import optional_features\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n-from .http import HttpRequestHandler\n+from .http import HttpDownloadHandler\n \n \n-class S3RequestHandler(object):\n+class S3DownloadHandler(object):\n \n     def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, \\\n-            httprequesthandler=HttpRequestHandler):\n+            httpdownloadhandler=HttpDownloadHandler):\n         if 'boto' not in optional_features:\n             raise NotConfigured(\"missing boto library\")\n \n@@ -22,7 +22,7 @@ class S3RequestHandler(object):\n             self.conn = connect_s3(aws_access_key_id, aws_secret_access_key)\n         except Exception, ex:\n             raise NotConfigured(str(ex))\n-        self._download_http = httprequesthandler().download_request\n+        self._download_http = httpdownloadhandler().download_request\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n@@ -13,7 +13,7 @@ from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n-from .handlers import RequestHandlers\n+from .handlers import DownloadHandlers\n \n \n class SpiderInfo(object):\n@@ -71,7 +71,7 @@ class Downloader(object):\n \n     def __init__(self):\n         self.sites = {}\n-        self.handlers = RequestHandlers()\n+        self.handlers = DownloadHandlers()\n         self.middleware = DownloaderMiddlewareManager()\n         self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n \n\n@@ -10,9 +10,9 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         PayloadResource, BrokenDownloadResource\n \n from scrapy.core.downloader.webclient import PartialDownloadError\n-from scrapy.core.downloader.handlers.file import FileRequestHandler\n-from scrapy.core.downloader.handlers.http import HttpRequestHandler\n-from scrapy.core.downloader.handlers.s3 import S3RequestHandler\n+from scrapy.core.downloader.handlers.file import FileDownloadHandler\n+from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n+from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.utils.url import path_to_file_uri\n@@ -26,7 +26,7 @@ class FileTestCase(unittest.TestCase):\n         fd = open(self.tmpname + '^', 'w')\n         fd.write('0123456789')\n         fd.close()\n-        self.download_request = FileRequestHandler().download_request\n+        self.download_request = FileDownloadHandler().download_request\n \n     def test_download(self):\n         def _test(response):\n@@ -61,7 +61,7 @@ class HttpTestCase(unittest.TestCase):\n         self.wrapper = WrappingFactory(self.site)\n         self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpRequestHandler().download_request\n+        self.download_request = HttpDownloadHandler().download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -156,7 +156,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpRequestHandler().download_request\n+        self.download_request = HttpDownloadHandler().download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -184,7 +184,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n \n-class HttpRequestHandlerMock(object):\n+class HttpDownloadHandlerMock(object):\n     def download_request(self, request, spider):\n         return request\n \n@@ -199,9 +199,9 @@ class S3TestCase(unittest.TestCase):\n     AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n \n     def setUp(self):\n-        s3reqh = S3RequestHandler(self.AWS_ACCESS_KEY_ID, \\\n+        s3reqh = S3DownloadHandler(self.AWS_ACCESS_KEY_ID, \\\n                 self.AWS_SECRET_ACCESS_KEY, \\\n-                httprequesthandler=HttpRequestHandlerMock)\n+                httpdownloadhandler=HttpDownloadHandlerMock)\n         self.download_request = s3reqh.download_request\n         self.spider = BaseSpider('foo')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
