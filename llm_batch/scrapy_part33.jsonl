{"custom_id": "scrapy#9e930703825bac59be75c2a9da0899301e82024d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n Mail sending helpers\n \n-See documentation in docs/topics/email.rst\n+See documentation in docs/ref/email.rst\n \"\"\"\n import smtplib\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0e37dc36a5c840e35229253571b99d4aed313bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 225 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ EXTENSIONS = [\n     'scrapy.contrib.pbcluster.ClusterMasterWeb',\n     'scrapy.contrib.pbcluster.ClusterCrawler',\n     'scrapy.contrib.closedomain.CloseDomain',\n-    'scrapy.contrib.debug.StackTraceDebug',\n+    'scrapy.contrib.debug.StackTraceDump',\n     'scrapy.contrib.response.soup.ResponseSoup',\n ]\n \n\n@@ -1,10 +1,12 @@\n \"\"\"\n Extensions for debugging Scrapy \n+\n+See documentation in docs/ref/extensions.rst\n \"\"\"\n import signal\n import traceback\n \n-class StackTraceDebug(object):\n+class StackTraceDump(object):\n     def __init__(self):\n         try:\n             signal.signal(signal.SIGUSR1, self.dump_stacktrace)\n\n@@ -1,5 +1,7 @@\n \"\"\" \n-Crawler logging functionality\n+Scrapy logging facility\n+\n+See documentation in docs/ref/logging.rst\n \"\"\"\n import sys\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0046196d84411ebf059453c0b519e9e6e5e9074", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 43 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,14 +3,17 @@ Mail sending helpers\n \n See documentation in docs/ref/email.rst\n \"\"\"\n-import smtplib\n-\n+from cStringIO import StringIO\n from email.MIMEMultipart import MIMEMultipart\n+from email.MIMENonMultipart import MIMENonMultipart\n from email.MIMEBase import MIMEBase\n from email.MIMEText import MIMEText\n from email.Utils import COMMASPACE, formatdate\n from email import Encoders\n \n+from twisted.internet import defer, reactor\n+from twisted.mail.smtp import SMTPSenderFactory\n+\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n@@ -24,8 +27,11 @@ class MailSender(object):\n         if not self.smtphost or not self.mailfrom:\n             raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n \n-    def send(self, to, subject, body, cc=None, attachs=None):\n+    def send(self, to, subject, body, cc=None, attachs=()):\n+        if attachs:\n             msg = MIMEMultipart()\n+        else:\n+            msg = MIMENonMultipart('text', 'plain')\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n         msg['Date'] = formatdate(localtime=True)\n@@ -35,17 +41,36 @@ class MailSender(object):\n             rcpts.extend(cc)\n             msg['Cc'] = COMMASPACE.join(cc)\n \n+        if attachs:\n             msg.attach(MIMEText(body))\n-\n-        for attach_name, mimetype, f in (attachs or []):\n+            for attach_name, mimetype, f in attachs:\n                 part = MIMEBase(*mimetype.split('/'))\n                 part.set_payload(f.read())\n                 Encoders.encode_base64(part)\n                 part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % attach_name)\n                 msg.attach(part)\n+        else:\n+            msg.set_payload(body)\n \n-        smtp = smtplib.SMTP(self.smtphost)\n-        smtp.sendmail(self.mailfrom, rcpts, msg.as_string())\n-        log.msg('Mail sent: To=%s Cc=%s Subject=\"%s\"' % (to, cc, subject))\n-        smtp.close()\n+        dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())\n+        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n+            callbackArgs=[to, cc, subject, len(attachs)],\n+            errbackArgs=[to, cc, subject, len(attachs)])\n+\n+    def _sent_ok(self, result, to, cc, subject, nattachs):\n+        log.msg('Mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % (to, cc, subject, nattachs))\n+\n+    def _sent_failed(self, failure, to, cc, subject, nattachs):\n+        errstr = str(failure.value)\n+        log.msg('Unable to send mail: To=%s Cc=%s Subject=\"%s\" Attachs=%d - %s' % (to, cc, subject, nattachs, errstr), level=log.ERROR)\n+\n+    def _sendmail(self, smtphost, from_addr, to_addrs, msg, port=25):\n+        \"\"\" This is based on twisted.mail.smtp.sendmail except that it\n+        instantiates a quiet (noisy=False) SMTPSenderFactory \"\"\"\n+        msg = StringIO(msg)\n+        d = defer.Deferred()\n+        factory = SMTPSenderFactory(from_addr, to_addrs, msg, d)\n+        factory.noisy = False\n+        reactor.connectTCP(smtphost, port, factory)\n+        return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30e44c9a5836d9c99655de30dcea90c9b2890b0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 222 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,6 +129,9 @@ NEWSPIDER_MODULE = ''\n \n PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n \n+REQUEST_HEADER_ACCEPT = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n+REQUEST_HEADER_ACCEPT_LANGUAGE = 'en'\n+\n REQUESTS_QUEUE_SIZE = 0\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n \n\n@@ -1,10 +1,20 @@\n+\"\"\"\n+Common downloader middleare\n+\n+See documentation in docs/ref/downloader-middleware.rst\n+\"\"\"\n+\n+from scrapy.conf import settings\n+\n class CommonMiddleware(object):\n-    \"\"\"This middleware provides common/basic functionality, and should always\n-    be enabled\"\"\"\n+\n+    def __init__(self):\n+        self.header_accept = settings.get('REQUEST_HEADER_ACCEPT')\n+        self.header_accept_language = settings.get('REQUEST_HEADER_ACCEPT_LANGUAGE')\n \n     def process_request(self, request, spider):\n-        request.headers.setdefault('Accept-Language', 'en')\n-        request.headers.setdefault('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')\n+        request.headers.setdefault('Accept', self.header_accept)\n+        request.headers.setdefault('Accept-Language', self.header_accept_language)\n         if request.method == 'POST':\n             request.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eca60c7c4d796b154622f11f53ee544464b31681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 324 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,13 +143,12 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n \n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n-\n     url = unicode_to_str(url)\n-    parts = list(urlparse.urlparse(url))\n-    keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n+    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)\n+    keyvals = cgi.parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n-    parts[2] = urllib.quote(urllib.unquote(parts[2]))\n-    parts[4] = urllib.urlencode(keyvals)\n-    if not keep_fragments:\n-        parts[5] = ''\n-    return urlparse.urlunparse(parts)\n+    query = urllib.urlencode(keyvals)\n+    path = urllib.quote(urllib.unquote(path))\n+    fragment = '' if not keep_fragments else fragment\n+    return urlparse.urlunparse((scheme, netloc, path, params, query, fragment))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b28d365b1cd01853512d465468f747c63fdcdd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 191 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,10 +86,7 @@ class BaseSpider(object):\n         \"\"\"\n         if urls is None:\n             urls = self.start_urls\n-        requests = [Request(url, callback=self.parse, dont_filter=True) for url in urls]\n-        return requests\n-        \n-        return []\n+        return [Request(url, callback=self.parse, dont_filter=True) for url in urls]\n \n     def parse(self, response):\n         \"\"\"This is the default callback function used to parse the start\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2434000cda588e21d274e1c82965f1154328642a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 60 | Files Changed: 3 | Hunks: 23 | Methods Changed: 17 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 115 | Churn Cumulative: 776 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,17 +14,16 @@ from scrapy.conf import settings\n \n class Scheduler(object) :\n     \"\"\"\n-    The scheduler decides what to scrape, how fast, and in what order.\n-    The scheduler schedules websites and pages to be scraped. Individual\n-    web pages that are to be scraped are batched up into a \"run\" for a website.\n-    As the domain is being scraped, pages that are discovered are added to the\n-    scheduler. The scheduler must not allow the same page to be requested\n-    multiple times within the same batch.\n+    The scheduler decides what to scrape next. In other words, it defines the\n+    crawling order.  The scheduler schedules websites and requests to be\n+    scraped.  Individual web pages that are to be scraped are batched up into a\n+    \"run\" for a website.  As the domain is being scraped, pages that are\n+    discovered are added to the scheduler. \n \n     Typical usage:\n \n-    * next_availble_domain() called to find out when there is something to do\n-    * begin_domain() called to commence scraping a website\n+    * next_available_domain() called to find out when there is something to do\n+    * open_domain() called to commence scraping a website\n     * enqueue_request() called multiple times when new links found\n     * next_request() called multiple times when there is capacity to process urls\n     * close_domain() called when there are no more pages or upon error\n@@ -66,14 +65,14 @@ class Scheduler(object) :\n \n     def domain_has_pending(self, domain):\n         if domain in self.pending_requests:\n-            return not self.pending_requests[domain].empty()\n+            return bool(self.pending_requests[domain])\n \n     def next_domain(self) :\n         \"\"\"\n         Return next domain available to scrape and remove it from available domains queue\n         \"\"\"\n         if self.pending_domains_count:\n-            priority, domain = self.domains_queue.get_nowait()\n+            domain, priority = self.domains_queue.pop()\n             if self.pending_domains_count[domain] == 1:\n                 del self.pending_domains_count[domain]\n             else:\n@@ -88,7 +87,7 @@ class Scheduler(object) :\n         domain can be scheduled twice, either with the same or with different\n         priority.\n         \"\"\"\n-        self.domains_queue.put(domain, priority=priority)\n+        self.domains_queue.push(domain, priority)\n         if domain not in self.pending_domains_count:\n             self.pending_domains_count[domain] = 1\n         else:\n@@ -114,7 +113,7 @@ class Scheduler(object) :\n \n         if request.dont_filter or added:\n             deferred = defer.Deferred()\n-            self.pending_requests[domain].put((request, deferred), priority)\n+            self.pending_requests[domain].push((request, deferred), priority)\n             return deferred\n         else:\n             return defer_fail(IgnoreRequest('Skipped (already visited): %s' % request))\n@@ -133,8 +132,8 @@ class Scheduler(object) :\n         None should be returned if there are no more request pending for the domain passed.\n         \"\"\"\n         pending_list = self.pending_requests.get(domain)\n-        if pending_list and not pending_list.empty():\n-            return pending_list.get_nowait()[1]\n+        if pending_list:\n+            return pending_list.pop()[0]\n         else:\n             return (None, None)\n \n\n@@ -1,26 +1,34 @@\n import unittest\n-from scrapy.utils.datatypes import PriorityQueue\n+\n+from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n \n class DatatypesTestCase(unittest.TestCase):\n \n     def test_priority_queue(self):\n+\n+        input = [('five', 5), ('three-1', 3), ('three-2', 3), ('six', 6), ('one', 1)]\n+        output = [('one', 1), ('three-1', 3), ('three-2', 3), ('five', 5), ('six', 6)]\n+\n         pq = PriorityQueue()\n+        for item, prio in input:\n+            pq.push(item, prio)\n+        out = []\n+        while pq:\n+            out.append(pq.pop())\n+        self.assertEqual(out, output)\n \n-        pq.put('b', priority=1)\n-        pq.put('a', priority=1)\n-        pq.put('c', priority=1)\n-        pq.put('z', priority=0)\n-        pq.put('d', priority=2)\n+    def test_priority_stack(self):\n \n-        v = []\n-        p = []\n-        while not pq.empty():\n-            priority, value = pq.get()\n-            v.append(value)\n-            p.append(priority)\n+        input = [('five', 5), ('three-1', 3), ('three-2', 3), ('six', 6), ('one', 1)]\n+        output = [('one', 1), ('three-2', 3), ('three-1', 3), ('five', 5), ('six', 6)]\n \n-        self.assertEqual(v, ['z', 'b', 'a', 'c', 'd'])\n-        self.assertEqual(p, [0, 1, 1, 1, 2])\n+        pq = PriorityStack()\n+        for item, prio in input:\n+            pq.push(item, prio)\n+        out = []\n+        while pq:\n+            out.append(pq.pop())\n+        self.assertEqual(out, output)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -5,11 +5,11 @@ Python Standard Library.\n This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n+import time\n import copy\n import gzip\n-import Queue\n-import bisect\n from cStringIO import StringIO\n+from heapq import heappush, heappop\n \n class MergeDict(object):\n     \"\"\"\n@@ -407,43 +407,31 @@ class CaselessDict(dict):\n     def pop(self, key, def_val=None):\n         return dict.pop(self, self.normkey(key), def_val)\n \n+class PriorityQueue(object):\n+    \"\"\"A simple priority queue\"\"\"\n \n-class PriorityQueue(Queue.Queue):\n+    def __init__(self):\n+        self.items = []\n \n-    def __len__(self):\n-        return self.qsize()\n+    def push(self, item, priority=0):\n+        heappush(self.items, (priority, time.time(), item))\n \n-    def _init(self, maxsize):\n-        self.maxsize = maxsize\n-        # Python 2.5 uses collections.deque, but we can't because\n-        # we need insert(pos, item) for our priority stuff\n-        self.queue = []\n+    def pop(self):\n+        priority, _, item = heappop(self.items)\n+        return item, priority\n \n-    def put(self, item, priority=0, block=True, timeout=None):\n-        \"\"\"Puts an item onto the queue with a numeric priority (default is zero).\n-        \n-        Note that we are \"shadowing\" the original Queue.Queue put() method here.\n-        \"\"\"\n-        Queue.Queue.put(self, (priority, item), block, timeout)\n-\n-    def _put(self, item):\n-        \"\"\"Override of the Queue._put to support prioritisation.\"\"\"\n-        # Priorities must be integers!\n-        priority = int(item[0])\n-\n-        # Using a tuple (priority+1,) finds us the correct insertion\n-        # position to maintain the existing ordering.\n-        self.queue.insert(bisect.bisect_left(self.queue, (priority+1,)), item)\n-\n-    def _get(self):\n-        \"\"\"Override of Queue._get().  Strips the priority.\"\"\"\n-        return self.queue.pop(0)\n+    def __iter__(self):\n+        return ((priority, item) for priority, _, item in self.items)\n \n+    def __nonzero__(self):\n+        return bool(self.items)\n \n class PriorityStack(PriorityQueue):\n-    def _put(self, item):\n-        priority = int(item[0])\n-        self.queue.insert(bisect.bisect_left(self.queue, (priority,)), item)\n+    \"\"\"A simple priority stack which is similar to PriorityQueue but pops its\n+    items in reverse order (for the same priority)\"\"\"\n+\n+    def push(self, item, priority=0):\n+        heappush(self.items, (priority, -time.time(), item))\n \n class gzStringIO:\n     \"\"\"a file like object, similar to StringIO, but gzip-compressed.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff637d9a0abf423c4d52e7115783f235411d173f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 585 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,11 +10,15 @@ class DatatypesTestCase(unittest.TestCase):\n         output = [('one', 1), ('three-1', 3), ('three-2', 3), ('five', 5), ('six', 6)]\n \n         pq = PriorityQueue()\n+        assert not bool(pq)\n         for item, prio in input:\n             pq.push(item, prio)\n         out = []\n+        assert bool(pq)\n+        self.assertEqual(len(pq), len(input))\n         while pq:\n             out.append(pq.pop())\n+        self.assertEqual(len(pq), 0)\n         self.assertEqual(out, output)\n \n     def test_priority_stack(self):\n@@ -23,11 +27,15 @@ class DatatypesTestCase(unittest.TestCase):\n         output = [('one', 1), ('three-2', 3), ('three-1', 3), ('five', 5), ('six', 6)]\n \n         pq = PriorityStack()\n+        assert not bool(pq)\n         for item, prio in input:\n             pq.push(item, prio)\n+        assert bool(pq)\n         out = []\n+        self.assertEqual(len(pq), len(input))\n         while pq:\n             out.append(pq.pop())\n+        self.assertEqual(len(pq), 0)\n         self.assertEqual(out, output)\n \n if __name__ == \"__main__\":\n\n@@ -420,8 +420,11 @@ class PriorityQueue(object):\n         priority, _, item = heappop(self.items)\n         return item, priority\n \n+    def __len__(self):\n+        return len(self.items)\n+\n     def __iter__(self):\n-        return ((priority, item) for priority, _, item in self.items)\n+        return ((item, priority) for priority, _, item in self.items)\n \n     def __nonzero__(self):\n         return bool(self.items)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#deb960526bd29266427551dbb37a27811001abe3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 185 | Files Changed: 1 | Hunks: 3 | Methods Changed: 25 | Complexity Δ (Sum/Max): -51/0 | Churn Δ: 185 | Churn Cumulative: 700 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,135 +11,6 @@ import gzip\n from cStringIO import StringIO\n from heapq import heappush, heappop\n \n-class MergeDict(object):\n-    \"\"\"\n-    A simple class for creating new \"virtual\" dictionaries that actualy look\n-    up values in more than one dictionary, passed in the constructor.\n-    \"\"\"\n-    def __init__(self, *dicts):\n-        self.dicts = dicts\n-\n-    def __getitem__(self, key):\n-        for dict_ in self.dicts:\n-            try:\n-                return dict_[key]\n-            except KeyError:\n-                pass\n-        raise KeyError\n-\n-    def __copy__(self):\n-        return self.__class__(*self.dicts)\n-\n-    def get(self, key, default=None):\n-        try:\n-            return self[key]\n-        except KeyError:\n-            return default\n-\n-    def getlist(self, key):\n-        for dict in self.dicts:\n-            try:\n-                return dict.getlist(key)\n-            except KeyError:\n-                pass\n-        raise KeyError\n-\n-    def items(self):\n-        item_list = []\n-        for dict in self.dicts:\n-            item_list.extend(dict.items())\n-        return item_list\n-\n-    def has_key(self, key):\n-        for dict in self.dicts:\n-            if key in dict:\n-                return True\n-        return False\n-\n-    __contains__ = has_key\n-\n-    def copy(self):\n-        \"\"\" returns a copy of this object\"\"\"\n-        return self.__copy__()\n-\n-class SortedDict(dict):\n-    \"A dictionary that keeps its keys in the order in which they're inserted.\"\n-    def __init__(self, data=None):\n-        if data is None: \n-            data = {}\n-        dict.__init__(self, data)\n-        self.keyOrder = data.keys()\n-\n-    def __setitem__(self, key, value):\n-        dict.__setitem__(self, key, value)\n-        if key not in self.keyOrder:\n-            self.keyOrder.append(key)\n-\n-    def __delitem__(self, key):\n-        dict.__delitem__(self, key)\n-        self.keyOrder.remove(key)\n-\n-    def __iter__(self):\n-        for k in self.keyOrder:\n-            yield k\n-\n-    def items(self):\n-        return zip(self.keyOrder, self.values())\n-\n-    def iteritems(self):\n-        for key in self.keyOrder:\n-            yield key, dict.__getitem__(self, key)\n-\n-    def keys(self):\n-        return self.keyOrder[:]\n-\n-    def iterkeys(self):\n-        return iter(self.keyOrder)\n-\n-    def values(self):\n-        return [dict.__getitem__(self, k) for k in self.keyOrder]\n-\n-    def itervalues(self):\n-        for key in self.keyOrder:\n-            yield dict.__getitem__(self, key)\n-\n-    def update(self, dict):\n-        for k, v in dict.items():\n-            self.__setitem__(k, v)\n-\n-    def setdefault(self, key, default):\n-        if key not in self.keyOrder:\n-            self.keyOrder.append(key)\n-        return dict.setdefault(self, key, default)\n-\n-    def value_for_index(self, index):\n-        \"Returns the value of the item at the given zero-based index.\"\n-        return self[self.keyOrder[index]]\n-\n-    def insert(self, index, key, value):\n-        \"Inserts the key, value pair before the item with the given index.\"\n-        if key in self.keyOrder:\n-            n = self.keyOrder.index(key)\n-            del self.keyOrder[n]\n-            if n < index: \n-                index -= 1\n-        self.keyOrder.insert(index, key)\n-        dict.__setitem__(self, key, value)\n-\n-    def copy(self):\n-        \"Returns a copy of this object.\"\n-        # This way of initializing the copy means it works for subclasses, too.\n-        obj = self.__class__(self)\n-        obj.keyOrder = self.keyOrder\n-        return obj\n-\n-    def __repr__(self):\n-        \"\"\"\n-        Replaces the normal dict.__repr__ with a version that returns the keys\n-        in their sorted order.\n-        \"\"\"\n-        return '{%s}' % ', '.join(['%r: %r' % (k, v) for k, v in self.items()])\n-\n class MultiValueDictKeyError(KeyError):\n     pass\n \n@@ -267,51 +138,6 @@ class MultiValueDict(dict):\n         for key, value in kwargs.iteritems():\n             self.setlistdefault(key, []).append(value)\n \n-class DotExpandedDict(dict):\n-    \"\"\"\n-    A special dictionary constructor that takes a dictionary in which the keys\n-    may contain dots to specify inner dictionaries. It's confusing, but this\n-    example should make sense.\n-\n-    >>> d = DotExpandedDict({'person.1.firstname': ['Simon'], \\\n-            'person.1.lastname': ['Willison'], \\\n-            'person.2.firstname': ['Adrian'], \\\n-            'person.2.lastname': ['Holovaty']})\n-    >>> d\n-    {'person': {'1': {'lastname': ['Willison'], 'firstname': ['Simon']}, '2': {'lastname': ['Holovaty'], 'firstname': ['Adrian']}}}\n-    >>> d['person']\n-    {'1': {'lastname': ['Willison'], 'firstname': ['Simon']}, '2': {'lastname': ['Holovaty'], 'firstname': ['Adrian']}}\n-    >>> d['person']['1']\n-    {'lastname': ['Willison'], 'firstname': ['Simon']}\n-\n-    # Gotcha: Results are unpredictable if the dots are \"uneven\":\n-    >>> DotExpandedDict({'c.1': 2, 'c.2': 3, 'c': 1})\n-    {'c': 1}\n-    \"\"\"\n-    def __init__(self, key_to_list_mapping):\n-        for k, v in key_to_list_mapping.items():\n-            current = self\n-            bits = k.split('.')\n-            for bit in bits[:-1]:\n-                current = current.setdefault(bit, {})\n-            # Now assign value to current position\n-            try:\n-                current[bits[-1]] = v\n-            except TypeError: # Special-case if current isn't a dict.\n-                current = {bits[-1] : v}\n-\n-class FileDict(dict):\n-    \"\"\"\n-    A dictionary used to hold uploaded file contents. The only special feature\n-    here is that repr() of this object won't dump the entire contents of the\n-    file to the output. A handy safeguard for a large file upload.\n-    \"\"\"\n-    def __repr__(self):\n-        if 'content' in self:\n-            d = dict(self, content='<omitted>')\n-            return dict.__repr__(d)\n-        return dict.__repr__(self)\n-\n class Sitemap(object):\n     \"\"\"Sitemap class is used to build a map of the traversed pages\"\"\"\n \n@@ -436,14 +262,3 @@ class PriorityStack(PriorityQueue):\n     def push(self, item, priority=0):\n         heappush(self.items, (priority, -time.time(), item))\n \n-class gzStringIO:\n-    \"\"\"a file like object, similar to StringIO, but gzip-compressed.\"\"\"\n-    def __init__(self, data, compress_level = 9, filename = \"\"):\n-        self._s = StringIO()\n-        g = gzip.GzipFile(filename, \"wb\", compress_level, self._s)\n-        g.write(data)\n-        g.flush()\n-        g.close()\n-    def read(self):\n-        return self._s.getvalue()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#95d99d51b92d226d56fb3bb3b41870f0e8ed542f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 21 | Churn Cumulative: 215 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,6 +82,7 @@ EXTENSIONS = [\n     'scrapy.xpath.extension.ResponseLibxml2',\n     'scrapy.management.web.WebConsole',\n     'scrapy.management.telnet.TelnetConsole',\n+    'scrapy.contrib.webconsole.scheduler.SchedulerQueue',\n     'scrapy.contrib.webconsole.livestats.LiveStats',\n     'scrapy.contrib.webconsole.spiderctl.Spiderctl',\n     'scrapy.contrib.webconsole.enginestatus.EngineStatus',\n\n@@ -1,16 +1,14 @@\n \"\"\"\n-Scheduler information module for Scrapy webconsole\n+Scheduler queue web console module \n \n-FIXME: this webconsole extension needs to be fixed after we removed\n-PriorityStack/Queue from the scheduler and replace them by bisect or something\n-like that.\n+See documentation in docs/ref/extensions.rst\n \"\"\"\n \n from pydispatch import dispatcher\n from scrapy.core.engine import scrapyengine\n from scrapy.management.web import banner, webconsole_discover_module\n \n-class SchedulerStats(object):\n+class SchedulerQueue(object):\n     webconsole_id = 'scheduler'\n     webconsole_name = 'Scheduler queue'\n \n@@ -23,16 +21,12 @@ class SchedulerStats(object):\n     def webconsole_render(self, wc_request):\n         s = banner(self)\n         s += \"<ul>\\n\"\n-        for domain, requests in scrapyengine.scheduler.pending_requests.iteritems():\n+        for domain, request_queue in scrapyengine.scheduler.pending_requests.iteritems():\n             s += \"<li>\\n\"\n-            s += \"%s (<b>%s</b> pages)\\n\" % (domain, len(requests))\n+            s += \"%s (<b>%s</b> requests)\\n\" % (domain, len(request_queue))\n             s += \"<ul>\\n\"\n-            # requests is a tuple of request and deffered now, as I understand\n-            for r, d in requests:\n-                if hasattr(r, 'url'):\n-                    s += \"<li><a href='%s'>%s</a></li>\\n\" % (r.url, r.url)\n-                #else:\n-                #    s += \"<li>%s</li>\\n\" % (repr(r))\n+            for ((req, _), prio) in request_queue:\n+                s += \"<li><a href='%s'>%s</a> (priority: %d)</li>\\n\" % (req.url, req.url, prio)\n             s += \"</ul>\\n\"\n             s += \"</li>\\n\" \n         s += \"</ul>\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#468bfeb278ed9bb7b2a14c09e03aa57384fa7540", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 702 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,8 +7,6 @@ This module must not depend on any module outside the Standard Library.\n \n import time\n import copy\n-import gzip\n-from cStringIO import StringIO\n from heapq import heappush, heappop\n \n class MultiValueDictKeyError(KeyError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e78003c92fa5bb21ae09aa255664e0de7960ec9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 197 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ CACHE2_SECTORIZE = True\n CACHE2_EXPIRATION_SECS = 0\n \n CLOSEDOMAIN_TIMEOUT = 0\n-CLOSEDOMAIN_NOTIFY = ['pablo@insophia.com']\n+CLOSEDOMAIN_NOTIFY = []\n \n CLUSTER_LOGDIR = ''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ed811b4d37a72100f9e72e7f7d9b53b137b3b5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 198 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -52,6 +52,7 @@ DEFAULT_SPIDER = None\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n+DOWNLOAD_DELAY = 0\n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n DOWNLOADER_MIDDLEWARES = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64d1f67c5775122280fef8d6c29ebae2959a45e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 124 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,5 +38,5 @@ class RequestLimitMiddleware(object):\n             if dropped_count > 0:\n                 requests = requests[:free_slots]\n                 log.msg(\"Dropping %d request(s) because the maximum schedule size (%d) has been exceeded\" % \\\n-                        (dropped_count, max_pending), level=log.WARNING, domain=spider.domain_name)\n+                        (dropped_count, max_pending), level=log.DEBUG, domain=spider.domain_name)\n         return requests + items\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ea37f51dbdbc57e70d34e555f317e1588e5e45f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 175 | Lines Deleted: 236 | Files Changed: 13 | Hunks: 50 | Methods Changed: 34 | Complexity Δ (Sum/Max): -11/8 | Churn Δ: 411 | Churn Cumulative: 3445 | Contributors (this commit): 4 | Commits (past 90d): 42 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -55,6 +55,8 @@ DEPTH_STATS = True\n DOWNLOAD_DELAY = 0\n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n+DOWNLOADER_DEBUG = False\n+\n DOWNLOADER_MIDDLEWARES = [\n     # Engine side\n     'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware',\n\n@@ -1,20 +1,18 @@\n from __future__ import with_statement\n \n import os\n-import sys\n import hashlib\n import datetime\n import urlparse\n import cPickle as pickle\n from pydispatch import dispatcher\n-from twisted.internet import defer\n \n from scrapy.core import signals\n from scrapy import log\n-from scrapy.core.engine import scrapyengine\n from scrapy.http import Response, Headers\n from scrapy.http.headers import headers_dict_to_raw\n-from scrapy.core.exceptions import UsageError, NotConfigured, HttpException, IgnoreRequest\n+from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest\n+from scrapy.utils.request import request_fingerprint\n from scrapy.conf import settings\n \n \n@@ -33,7 +31,7 @@ class CacheMiddleware(object):\n         if not is_cacheable(request):\n             return\n \n-        key = request.fingerprint()\n+        key = request_fingerprint(request)\n         domain = spider.domain_name\n         try:\n             response = self.cache.retrieve_response(domain, key)\n@@ -53,7 +51,7 @@ class CacheMiddleware(object):\n             return response\n \n         if isinstance(response, Response) and not response.cached:\n-            key = request.fingerprint()\n+            key = request_fingerprint(request)\n             domain = spider.domain_name\n             self.cache.store(domain, key, request, response)\n \n@@ -64,7 +62,7 @@ class CacheMiddleware(object):\n             return\n \n         if isinstance(exception, HttpException) and isinstance(exception.response, Response):\n-            key = request.fingerprint()\n+            key = request_fingerprint(request)\n             domain = spider.domain_name\n             self.cache.store(domain, key, request, exception.response)\n \n\n@@ -30,6 +30,7 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n \n from scrapy import log\n from scrapy.core.exceptions import HttpException\n+from scrapy.utils.request import request_fingerprint\n from scrapy.conf import settings\n \n class RetryMiddleware(object):\n@@ -44,7 +45,7 @@ class RetryMiddleware(object):\n \n     def process_exception(self, request, exception, spider):\n         if isinstance(exception, self.EXCEPTIONS_TO_RETRY) or (isinstance(exception, HttpException) and (int(exception.status) in self.retry_http_codes)):\n-            fp = request.fingerprint()\n+            fp = request_fingerprint(request)\n             self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n \n             if self.failed_count[fp] <= self.retry_times:\n\n@@ -5,6 +5,7 @@ from twisted.internet import defer\n from scrapy import log\n from scrapy.core.scheduler import Scheduler\n from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.utils.request import request_fingerprint\n \n class RulesScheduler(Scheduler):\n     \"\"\"Scheduler that uses rules to determine if we should follow links\n@@ -33,7 +34,7 @@ class RulesScheduler(Scheduler):\n         The url will only be added if we have not checked it already within\n         a specified time period.\n         \"\"\"\n-        requestid = request.fingerprint()\n+        requestid = request_fingerprint(request)\n         added = self.groupfilter.add(domain, requestid)\n \n         if request.dont_filter or added:\n\n@@ -3,6 +3,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n+from scrapy.utils.request import request_fingerprint\n from scrapy.spider import spiders\n \n \n@@ -53,7 +54,7 @@ class MediaPipeline(object):\n \n     def _enqueue(self, request, info):\n         wad = request.deferred or defer.Deferred()\n-        fp = request.fingerprint()\n+        fp = request_fingerprint(request)\n \n         # if already downloaded, return cached result.\n         if fp in info.downloaded:\n\n@@ -13,6 +13,7 @@ from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.defer import chain_deferred, mustbe_deferred\n+from scrapy.utils.request import request_info\n \n \n class SiteDetails(object):\n@@ -43,7 +44,7 @@ class SiteDetails(object):\n \n \n class Downloader(object):\n-    \"\"\"Maintain many concurrent downloads and provide an HTTP abstraction.\n+    \"\"\"Mantain many concurrent downloads and provide an HTTP abstraction.\n     It supports a limited number of connections per domain and many domains in\n     parallel.\n     \"\"\"\n@@ -59,6 +60,8 @@ class Downloader(object):\n         self.middleware = DownloaderMiddlewareManager()\n         self.middleware.download_function = self.enqueue\n         self.download_function = download_any\n+        self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n+        self.debug_mode = settings.getbool('DOWNLOADER_DEBUG')\n \n     def fetch(self, request, spider):\n         \"\"\" Main method to use to request a download\n@@ -120,13 +123,15 @@ class Downloader(object):\n             self.engine.closed_domain(domain)\n \n     def _download(self, request, spider, deferred):\n-        log.msg('Activating %s' % request.traceinfo(), log.TRACE)\n+        if self.debug_mode:\n+            log.msg('Activating %s' % request_info(request), log.DEBUG)\n         domain = spider.domain_name\n         site = self.sites.get(domain)\n         site.downloading.add(request)\n \n         def _remove(result):\n-            log.msg('Deactivating %s' % request.traceinfo(), log.TRACE)\n+            if self.debug_mode:\n+                log.msg('Deactivating %s' % request_info(request), log.DEBUG)\n             site.downloading.remove(request)\n             return result\n \n@@ -153,14 +158,16 @@ class Downloader(object):\n \n     def close_domain(self, domain):\n         \"\"\"Free any resources associated with the given domain\"\"\"\n-        log.msg(\"Downloader closing domain %s\" % domain, log.TRACE, domain=domain)\n+        if self.debug_mode:\n+            log.msg(\"Downloader closing domain %s\" % domain, log.DEBUG, domain=domain)\n         site = self.sites.get(domain)\n         if site:\n             site.closed = True\n             spider = spiders.fromdomain(domain)\n             self.process_queue(spider)\n         else:\n-            log.msg('Domain %s already closed' % domain, log.TRACE, domain=domain)\n+            if self.debug_mode:\n+                log.msg('Domain %s already closed' % domain, log.DEBUG, domain=domain)\n \n     def needs_backout(self, domain):\n         site = self.sites.get(domain)\n@@ -195,13 +202,8 @@ class Downloader(object):\n \n     def has_capacity(self):\n         \"\"\"Does the downloader have capacity to handle more domains\"\"\"\n-        return len(self.sites) < settings.getint('CONCURRENT_DOMAINS')\n+        return len(self.sites) < self.concurrent_domains\n \n     def is_idle(self):\n         return not self.sites\n \n-    # deprecated\n-    def clear_requests(self, domain):\n-        log.msg(\"Downloader clearing request for domain %s\" % domain, log.TRACE, domain=domain)\n-        self.sites[domain].queue = []\n-\n\n@@ -1,6 +1,8 @@\n \"\"\"\n-Core execution engine for the web crawling framework. This controls the \n-scheduler, downloader and spiders.\n+This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n+\n+For more information see docs/topics/architecture.rst\n+\n \"\"\"\n from datetime import datetime\n \n@@ -21,6 +23,7 @@ from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n+from scrapy.utils.request import request_info\n \n class ExecutionEngine(object):\n     \"\"\"\n@@ -247,7 +250,7 @@ class ExecutionEngine(object):\n                     signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n                     self.crawl(request=item, spider=spider, priority=priority)\n                 else:\n-                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.TRACE, domain=domain)\n+                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.DEBUG, domain=domain)\n \n             class _ResultContainer(object):\n                 def append(self, item):\n@@ -292,10 +295,10 @@ class ExecutionEngine(object):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n             if self.debug_mode: \n-                log.msg('Scheduling %s (delayed)' % request.traceinfo(), log.TRACE)\n+                log.msg('Scheduling %s (delayed)' % request_info(request), log.DEBUG)\n             return self._add_starter(request, spider, domain_priority)\n         if self.debug_mode: \n-            log.msg('Scheduling %s (now)' % request.traceinfo(), log.TRACE)\n+            log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)\n         schd = self.scheduler.enqueue_request(domain, request, priority)\n         self.next_request(spider)\n         return schd\n@@ -338,13 +341,15 @@ class ExecutionEngine(object):\n         del self.starters[domain]\n \n     def download(self, request, spider):\n-        log.msg('Downloading %s' % request.traceinfo(), log.TRACE)\n+        if self.debug_mode:\n+            log.msg('Downloading %s' % request_info(request), log.DEBUG)\n         domain = spider.domain_name\n \n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n-            log.msg(\"Requested %s\" % request.traceinfo(), level=log.TRACE, domain=domain)\n+            if self.debug_mode:\n+                log.msg(\"Requested %s\" % request_info(request), level=log.DEBUG, domain=domain)\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n                 cached = 'cached' if response.cached else 'live'\n\n@@ -8,6 +8,7 @@ from scrapy.core.scheduler.filter import GroupFilter\n from scrapy import log\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n+from scrapy.utils.request import request_fingerprint\n from scrapy.utils.defer import defer_fail\n from scrapy.conf import settings\n \n@@ -108,7 +109,7 @@ class Scheduler(object) :\n         \"\"\"\n         Add a page to be scraped for a domain that is currently being scraped.\n         \"\"\"\n-        requestid = request.fingerprint()\n+        requestid = request_fingerprint(request)\n         added = self.groupfilter.add(domain, requestid)\n \n         if request.dont_filter or added:\n@@ -123,7 +124,7 @@ class Scheduler(object) :\n         Returns True if the given Request was scheduled before for the given\n         domain\n         \"\"\"\n-        return self.groupfilter.has(domain, request.fingerprint())\n+        return self.groupfilter.has(domain, request_fingerprint(request))\n \n     def next_request(self, domain):\n         \"\"\"\n\n@@ -1,5 +1,4 @@\n import urllib\n-import hashlib\n from copy import copy\n from base64 import urlsafe_b64encode\n \n@@ -14,7 +13,7 @@ from scrapy.utils.defer import chain_deferred\n class Request(object):\n     def __init__(self, url, callback=None, context=None, method=None, body=None, headers=None, cookies=None,\n             referer=None, url_encoding='utf-8', link_text='', http_user='', http_pass='', dont_filter=None, \n-            fingerprint_params=None, domain=None):\n+            domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -43,9 +42,6 @@ class Request(object):\n         self.context = context or {}\n         # dont_filter be filtered by scheduler\n         self.dont_filter = dont_filter\n-        # fingerprint parameters\n-        self.fingerprint_params = fingerprint_params or {}\n-        self._fingerprint = None\n         # shortcut for setting referer\n         if referer is not None:\n             self.headers['referer'] = referer\n@@ -57,6 +53,9 @@ class Request(object):\n         #allows to directly specify the spider for the request\n         self.domain = domain\n \n+        # bucket to store cached data such as fingerprint and others\n+        self._cache = {}\n+        \n     def append_callback(self, callback, *args, **kwargs):\n         if isinstance(callback, defer.Deferred):\n             return chain_deferred(self.deferred, callback)\n@@ -73,7 +72,6 @@ class Request(object):\n         assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__\n         decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n         self._url = Url(safe_url_string(decoded_url, self.encoding))\n-        self._fingerprint = None # invalidate cached fingerprint\n     url = property(lambda x: x._url, set_url)\n \n     def httpauth(self, http_user, http_pass):\n@@ -92,11 +90,6 @@ class Request(object):\n         \"\"\"Return raw HTTP request size\"\"\"\n         return len(self.to_string())\n \n-    def traceinfo(self):\n-        fp = self.fingerprint()\n-        version = '%s..%s' % (fp[:4], fp[-4:])\n-        return \"<Request: %s %s (%s)>\" % (self.method, self.url, version)\n-\n     def __repr__(self):\n         d = {\n             'method': self.method,\n@@ -111,61 +104,15 @@ class Request(object):\n     def copy(self):\n         \"\"\"Clone request except `context` attribute\"\"\"\n         new = copy(self)\n+        new._cache = {}\n         for att in self.__dict__:\n-            if att not in ['context', 'url', 'deferred', '_fingerprint']:\n+            if att not in ['_cache', 'context', 'url', 'deferred']:\n                 value = getattr(self, att)\n                 setattr(new, att, copy(value))\n         new.deferred = defer.Deferred()\n         new.context = self.context # requests shares same context dictionary\n-        new._fingerprint = None # reset fingerprint\n         return new\n \n-    def fingerprint(self):\n-        \"\"\"Returns unique resource fingerprint with caching support\"\"\"\n-        if not self._fingerprint or self.fingerprint_params:\n-            self.update_fingerprint()\n-        return self._fingerprint\n-\n-    def update_fingerprint(self):\n-        \"\"\"Update request fingerprint, based on its current data. A request\n-        fingerprint is a hash which uniquely identifies the HTTP resource\"\"\"\n-\n-        headers = {}\n-        if self.fingerprint_params:\n-            if 'tamperfunc' in self.fingerprint_params:\n-                tamperfunc = self.fingerprint_params['tamperfunc']\n-                assert callable(tamperfunc)\n-                req = tamperfunc(self.copy())\n-                assert isinstance(req, Request)\n-                try:\n-                    del req.fingerprint_params['tamperfunc']\n-                except KeyError:\n-                    pass\n-                return req.fingerprint()\n-\n-            if self.headers:\n-                if 'include_headers' in self.fingerprint_params:\n-                    keys = [k.lower() for k in self.fingerprint_params['include_headers']]\n-                    headers = dict([(k, v) for k, v in self.headers.items() if k.lower() in keys])\n-                elif 'exclude_headers' in self.fingerprint_params:\n-                    keys = [k.lower() for k in self.fingerprint_params['exclude_headers']]\n-                    headers = dict([(k, v) for k, v in self.headers.items() if k.lower() not in keys])\n-\n-        # fingerprint generation\n-        fp = hashlib.sha1()\n-        fp.update(canonicalize(self.url))\n-        fp.update(self.method)\n-\n-        if self.body and self.method in ['POST', 'PUT']:\n-            fp.update(self.body)\n-\n-        if headers:\n-            for k, v in sorted([(k.lower(), v) for k, v in headers.items()]):\n-                fp.update(k)\n-                fp.update(v)\n-\n-        self._fingerprint = fp.hexdigest()\n-\n     def to_string(self):\n         \"\"\" Return raw HTTP request representation (as string). This is\n         provided only for reference since it's not the actual stream of bytes\n\n@@ -132,7 +132,6 @@ class Replay(object):\n             self.passed_new[str(item.guid)] = item\n \n     def response_received(self, response, spider):\n-        #key = response.request.fingerprint()\n         key = response.version()\n         if (self.recording or self.updating) and key:\n             self.responses_old[key] = response.copy()\n\n@@ -3,12 +3,10 @@ from scrapy.http import Request, Headers\n from scrapy.core.scheduler import GroupFilter\n \n class RequestTest(unittest.TestCase):\n-    \"\"\" c14n comparison functions \"\"\"\n \n     def test_groupfilter(self):\n-        k1 = Request(url=\"http://www.scrapy.org/path?key=val\").fingerprint()\n-        k2 = Request(url=\"http://www.scrapy.org/path?key=val&key=val\").fingerprint()\n-        self.assertEqual(k1, k2)\n+        k1 = \"id1\"\n+        k2 = \"id1\"\n \n         f = GroupFilter()\n         f.open(\"mygroup\")\n@@ -58,146 +56,6 @@ class RequestTest(unittest.TestCase):\n         set_.add(r2)\n         self.assertEqual(len(set_), 2)\n \n-    def test_fingerprint(self):\n-        url = 'http://www.scrapy.org'\n-        r = Request(url=url)\n-        urlhash = r.fingerprint()\n-\n-        # fingerprint including all initials headers\n-        r.fingerprint_params['exclude_headers'] = []\n-        fullhash = r.fingerprint()\n-        del r.fingerprint_params['exclude_headers']\n-\n-        # all headers are excluded from fingerprint by default\n-        r.headers['Accept'] = 'application/json'\n-        accepthash = r.fingerprint()\n-        r.headers['User-Agent'] = 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.8) Gecko/20071004 Iceweasel/2.0.0.8 (Debian-2.0.0.8-1)'\n-        useragenthash = r.fingerprint()\n-        self.assertEqual(urlhash, accepthash)\n-        self.assertEqual(urlhash, useragenthash)\n-        self.assertEqual(accepthash, useragenthash)\n-\n-        # take User-Agent header in count as specified in `include_headers`\n-        r.fingerprint_params['include_headers'] = ['user-agent']\n-        useragenthash = r.fingerprint()\n-        self.assertNotEqual(useragenthash, urlhash)\n-\n-        # include_headers has precedence over exclude_headers\n-        r.fingerprint_params['exclude_headers'] = ['user-agent', 'Accept']\n-        self.assertEqual(useragenthash, r.fingerprint())\n-\n-        del r.fingerprint_params['include_headers']\n-        self.assertNotEqual(useragenthash, r.fingerprint()) # exclude_headers is excluding 'User-Agent' from hash\n-        self.assertEqual(fullhash, r.fingerprint()) # all headers previously seted was excluded\n-\n-        # set more headers\n-        r.headers['Accept-Language'] = 'en-us,en;q=0.5'\n-        r.headers['SESSIONID'] = 'an ugly session id header'\n-        self.assertNotEqual(urlhash, r.fingerprint())\n-\n-        # force emtpy include_headers (ignore exclude_headers)\n-        r.fingerprint_params['include_headers'] = []\n-        self.assertEqual(urlhash, r.fingerprint())\n-\n-        # Tamper Function\n-        r.fingerprint_params['tamperfunc'] = lambda req: Request(url=req.url)\n-        self.assertEqual(urlhash, r.fingerprint())\n-\n-        # Compare request with None vs {} header\n-        r = Request(url=url, headers=None)\n-        o = Request(url=url, headers={})\n-        self.assertEqual(o.fingerprint(), r.fingerprint())\n-        self.assertNotEqual(r, o)\n-\n-        # Different ways of setting headers attribute\n-        headers = {'Accept':'gzip', 'Custom-Header':'nothing to tell you'}\n-        r = Request(url=url, headers=headers)\n-        p = Request(url=url, headers=r.headers)\n-        o = Request(url=url)\n-        o.headers = headers\n-        self.assertEqual(r.fingerprint(), o.fingerprint())\n-        self.assertEqual(r.fingerprint(), p.fingerprint())\n-        self.assertEqual(o.fingerprint(), p.fingerprint())\n-        self.assertNotEqual(r, o)\n-        self.assertNotEqual(r, p)\n-        self.assertNotEqual(o, p)\n-\n-        # same url, implicit method\n-        r1 = Request(url=url)\n-        r2 = Request(url=url, method='GET')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # same url, different method\n-        r3 = Request(url=url, method='POST')\n-        self.assertNotEqual(r1.fingerprint(), r3.fingerprint())\n-\n-        # implicit POST method\n-        r3.body = ''\n-        r4 = Request(url=url, body='')\n-        self.assertEqual(r3.fingerprint(), r4.fingerprint())\n-\n-        # body is not important in GET or DELETE\n-        r1 = Request(url=url, method='get', body='data')\n-        r2 = Request(url=url, method='get')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        r1 = Request(url=url, method='delete', body='data')\n-        r2 = Request(url=url, method='delete')\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # no body by default\n-        r1 = Request(url=url, method='POST')\n-        r2 = Request(url=url, method='POST', body=None)\n-        self.assertEqual(r1.fingerprint(), r2.fingerprint())\n-\n-        # empty body is equal to None body\n-        r3 = Request(url=url, method='POST', body='')\n-        self.assertEqual(r1.fingerprint(), r3.fingerprint())\n-\n-    def test_insensitive_request_fingerprints(self):\n-        url = 'http://www.scrapy.org'\n-        fp = {'include_headers':['Accept','Content-Type']}\n-        r1a = Request(url=url.lower())\n-        r1b = Request(url=url.upper())\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get')\n-        r1b = Request(url=url.upper(), method='GET')\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1c = Request(url=url.upper(), method='GET', body='this is not important')\n-        self.assertEqual(r1b.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp)\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp)\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', headers={'ACCEPT':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'Accept':'Black'})\n-        r1c = Request(url=url.upper(), method='gEt', fingerprint_params=fp, headers={'accept':'Black'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='get', fingerprint_params=fp, headers={'ACCEPT':'Black', 'content-type':'application/json'})\n-        r1b = Request(url=url.upper(), method='GET', fingerprint_params=fp, headers={'Accept':'Black', 'Content-Type':'application/json'})\n-        r1c = Request(url=url.upper(), method='Get', fingerprint_params=fp, headers={'accepT':'Black', 'CONTENT-TYPE':'application/json'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n-        r1a = Request(url=url.lower(), method='Post', fingerprint_params=fp, headers={'ACCEPT':'Black', 'content-type':'application/json'})\n-        r1b = Request(url=url.upper(), method='POST', fingerprint_params=fp, headers={'Accept':'Black', 'Content-Type':'application/json'})\n-        r1c = Request(url=url.upper(), method='posT', fingerprint_params=fp, headers={'accepT':'Black', 'CONTENT-TYPE':'application/json'})\n-        self.assertEqual(r1a.fingerprint(), r1b.fingerprint())\n-        self.assertEqual(r1a.fingerprint(), r1c.fingerprint())\n-\n     def test_url(self):\n         \"\"\"Request url tests\"\"\"\n         r = Request(url=\"http://www.scrapy.org/path\")\n@@ -226,5 +84,3 @@ class RequestTest(unittest.TestCase):\n \n if __name__ == \"__main__\":\n     unittest.main()\n-\n-\n\n@@ -0,0 +1,61 @@\n+import unittest\n+from scrapy.http import Request\n+from scrapy.utils.request import request_fingerprint\n+\n+class UtilsRequestTest(unittest.TestCase):\n+\n+    def test_request_fingerprint(self):\n+        url = 'http://www.scrapy.org'\n+        r = Request(url=url)\n+        urlhash = request_fingerprint(r)\n+\n+        r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n+        r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r1))\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n+\n+        # make sure caching is working\n+        self.assertEqual(request_fingerprint(r1), r1._cache['fingerprint'])\n+\n+        r1 = Request(\"http://www.example.com/members/offers.html\")\n+        r2 = Request(\"http://www.example.com/members/offers.html\")\n+        r2.headers['SESSIONID'] = \"somehash\"\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n+\n+        r1 = Request(\"http://www.example.com/\")\n+        r2 = Request(\"http://www.example.com/\")\n+        r2.headers['Accept-Language'] = 'en'\n+        r3 = Request(\"http://www.example.com/\")\n+        r3.headers['Accept-Language'] = 'en'\n+        r3.headers['SESSIONID'] = \"somehash\"\n+\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2), request_fingerprint(r3))\n+\n+        self.assertEqual(request_fingerprint(r1),\n+                         request_fingerprint(r1, include_headers=['Accept-Language']))\n+\n+        self.assertNotEqual(request_fingerprint(r1),\n+                         request_fingerprint(r2, include_headers=['Accept-Language']))\n+\n+        self.assertEqual(request_fingerprint(r3, include_headers=['accept-language', 'sessionid']),\n+                         request_fingerprint(r3, include_headers=['SESSIONID', 'Accept-Language']))\n+\n+        r1 = Request(\"http://www.example.com\")\n+        r2 = Request(\"http://www.example.com\", method='POST')\n+        r3 = Request(\"http://www.example.com\", method='POST', body='request body')\n+\n+        self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2))\n+        self.assertNotEqual(request_fingerprint(r2), request_fingerprint(r3))\n+\n+        # cached fingerprint must be cleared on request copy\n+        r1 = Request(\"http://www.example.com\")\n+        fp1 = request_fingerprint(r1)\n+        r2 = r1.copy()\n+        r2.url = \"http://www.example.com/other\"\n+        fp2 = request_fingerprint(r2)\n+        self.assertNotEqual(fp1, fp2)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n+\n\n@@ -0,0 +1,65 @@\n+\"\"\"\n+This module provides some useful functions for working with\n+scrapy.http.Request objects\n+\"\"\"\n+\n+import hashlib\n+\n+from scrapy.utils.url import canonicalize_url\n+\n+def request_fingerprint(request, include_headers=()):\n+    \"\"\"\n+    Return the request fingerprint.\n+    \n+    The request fingerprint is a hash that uniquely identifies the resource the\n+    request points to. For example, take the following two urls:\n+    \n+    http://www.example.com/query?id=111&cat=222\n+    http://www.example.com/query?cat=222&id=111\n+\n+    Even though those are two different URLs both point to the same resource\n+    and are equivalent (ie. they should return the same response).\n+\n+    Another example are cookies used to store session ids. Suppose the\n+    following page is only accesible to authenticated users:\n+    \n+    http://www.example.com/members/offers.html\n+\n+    Lot of sites use a cookie to store the session id, which adds a random\n+    component to the HTTP Request and thus should be ignored when calculating\n+    the fingerprint. \n+    \n+    For this reason, request headers are ignored by default when calculating\n+    the fingeprint. If you want to include specific headers use the\n+    include_headers argument, which is a list of Request headers to include.\n+\n+    \"\"\"\n+\n+    if include_headers:\n+        include_headers = [h.lower() for h in sorted(include_headers)]\n+        cachekey = 'fingerprint' + '_'.join(include_headers)\n+    else:\n+        cachekey = 'fingerprint'\n+\n+    try:\n+        return request._cache[cachekey]\n+    except KeyError:\n+        fp = hashlib.sha1()\n+        fp.update(request.method)\n+        fp.update(canonicalize_url(request.url))\n+        fp.update(request.body or '')\n+        for hdr in include_headers:\n+            if hdr in request.headers:\n+                fp.update(hdr)\n+                fp.update(request.headers.get(hdr, ''))\n+        fphash = fp.hexdigest()\n+        request._cache[cachekey] = fphash\n+        return fphash\n+\n+def request_info(request):\n+    \"\"\"Return a short string with request info including method, url and\n+    fingeprint. Mainly used for debugging\n+    \"\"\"\n+    fp = request_fingerprint(request)\n+    return \"<Request: %s %s (%s..)>\" % (request.method, request.url, fp[:8])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
