{"custom_id": "keras#7cd6c59789c8e469c0d3cab6bca7ae3d2d028002", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 734 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,8 @@ setup(name='Keras',\n                     'flaky',\n                     'pytest-cov',\n                     'pandas',\n-                    'requests'],\n+                    'requests',\n+                    'markdown'],\n       },\n       classifiers=[\n           'Development Status :: 5 - Production/Stable',\n\n@@ -1,3 +1,4 @@\n+from markdown import markdown\n from docs import autogen\n import pytest\n \n@@ -327,7 +328,7 @@ y = layer(x)\n \n def test_doc_lists():\n     docstring = autogen.process_docstring(test_doc1['doc'])\n-    assert docstring == test_doc1['result']\n+    assert markdown(docstring) == markdown(test_doc1['result'])\n \n \n dummy_docstring = \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7bee9a18d83899ce6dfd50c4883afc678139f4ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 77 | Files Changed: 2 | Hunks: 16 | Methods Changed: 2 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 97 | Churn Cumulative: 17216 | Contributors (this commit): 132 | Commits (past 90d): 56 | Contributors (cumulative): 158 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1197,10 +1197,10 @@ def batch_dot(x, y, axes=None):\n                          'Expected: None, int, (int, int), ' +\n                          'Provided: ' + str(axes))\n \n-    # if tuple, convert to list\n+    # if tuple, convert to list.\n     axes = list(axes)\n \n-    # convert negative indices\n+    # convert negative indices.\n     if axes[0] < 0:\n         axes[0] += x_ndim\n     if axes[1] < 0:\n@@ -1223,31 +1223,6 @@ def batch_dot(x, y, axes=None):\n                          ' with axes=' + str(axes) + '. x.shape[%d] != '\n                          'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n \n-    # There are 2 ways to perform theano's batched_tensordot in tensorflow:\n-    # 1) Elementwise multiplication followed by tf.reduce_sum. This requires\n-    # more memory but works with partial shape information.\n-    # 2) Using tf.matmul. This is more efficient but all dimensions except\n-    # batch size should be known for input with rank > 3.\n-\n-    if x_ndim > 3:\n-        if None in x_shape[1:]:\n-            x_matmullabe = False\n-        else:\n-            x_matmullabe = True\n-    else:\n-        x_matmullabe = True\n-\n-    if y_ndim > 3:\n-        if None in y_shape[1:]:\n-            y_matmullabe = False\n-        else:\n-            y_matmullabe = True\n-    else:\n-        y_matmullabe = True\n-\n-    use_matmul = x_matmullabe and y_matmullabe\n-\n-    if use_matmul:\n     # backup ndims. Need them later.\n     orig_x_ndim = x_ndim\n     orig_y_ndim = y_ndim\n@@ -1280,23 +1255,22 @@ def batch_dot(x, y, axes=None):\n     # normalize both inputs to rank 3.\n     if x_ndim > 3:\n         # squash middle dimensions of x.\n-            x_shape = list(int_shape(x))\n+        x_shape = shape(x)\n         x_mid_dims = x_shape[1:-1]\n-            x_squashed_dim = np.prod(x_mid_dims)\n-            if x_batch_size is None:\n-                x_batch_size = -1\n-            x = tf.reshape(x, [x_batch_size, x_squashed_dim, x_shape[-1]])\n+        x_squashed_dim = tf.reduce_prod(x_mid_dims)\n+        x_squashed_shape = tf.stack([x_shape[0], x_squashed_dim, x_shape[-1]])\n+        x = tf.reshape(x, x_squashed_shape)\n         x_squashed = True\n     else:\n         x_squashed = False\n+\n     if y_ndim > 3:\n-            # squash trailing dimensions of y\n-            y_shape = list(int_shape(y))\n+        # squash trailing dimensions of y.\n+        y_shape = shape(y)\n         y_trail_dims = y_shape[2:]\n-            y_squashed_dim = np.prod(y_trail_dims)\n-            if y_batch_size is None:\n-                y_batch_size = -1\n-            y = tf.reshape(y, [y_batch_size, y_shape[1], y_squashed_dim])\n+        y_squashed_dim = tf.reduce_prod(y_trail_dims)\n+        y_squashed_shape = tf.stack([y_shape[0], y_shape[1], y_squashed_dim])\n+        y = tf.reshape(y, y_squashed_shape)\n         y_squashed = True\n     else:\n         y_squashed = False\n@@ -1304,18 +1278,20 @@ def batch_dot(x, y, axes=None):\n     result = tf.matmul(x, y)\n \n     # if inputs were squashed, we have to reshape the matmul output.\n-        output_shape = list(int_shape(result))\n+    output_shape = tf.shape(result)\n     do_reshape = False\n+\n     if x_squashed:\n-            output_shape = [output_shape[0]] + x_mid_dims + [output_shape[-1]]\n+        output_shape = tf.concat([output_shape[:1],\n+                                  x_mid_dims,\n+                                  output_shape[-1:]], 0)\n         do_reshape = True\n+\n     if y_squashed:\n-            output_shape = output_shape[:-1] + y_trail_dims\n+        output_shape = tf.concat([output_shape[:-1], y_trail_dims], 0)\n         do_reshape = True\n \n     if do_reshape:\n-            if output_shape[0] is None:\n-                output_shape[0] = -1\n         result = tf.reshape(result, output_shape)\n \n     # if the inputs were originally rank 2, we remove the added 1 dim.\n@@ -1323,39 +1299,6 @@ def batch_dot(x, y, axes=None):\n         result = tf.squeeze(result, 1)\n     elif orig_y_ndim == 2:\n         result = tf.squeeze(result, -1)\n-    else:\n-\n-        # bring the dimension to be reduced to axis 1.\n-        if a0 != 1:\n-            pattern = list(range(x_ndim))\n-            for i in range(a0, 1, -1):\n-                pattern[i] = pattern[i - 1]\n-            pattern[1] = a0\n-            x = tf.transpose(x, pattern)\n-\n-        if a1 != 1:\n-            pattern = list(range(y_ndim))\n-            for i in range(a1, 1, -1):\n-                pattern[i] = pattern[i - 1]\n-            pattern[1] = a1\n-            y = tf.transpose(y, pattern)\n-\n-        # reshape to closest broadcastable shape.\n-        x_shape = tf.shape(x)\n-        y_shape = tf.shape(y)\n-\n-        new_x_shape = tf.concat([x_shape, tf.ones_like(y_shape[2:])], 0)\n-        new_y_shape = tf.concat([y_shape[:2],\n-                                tf.ones_like(x_shape[2:]),\n-                                y_shape[2:]], 0)\n-\n-        x = reshape(x, new_x_shape)\n-        y = reshape(y, new_y_shape)\n-\n-        result = tf.reduce_sum(x * y, 1)\n-\n-        if ndim(result) == 1:\n-            result = tf.expand_dims(result, -1)\n \n     return result\n \n\n@@ -362,7 +362,7 @@ class TestBackend(object):\n \n             z_shape = K.int_shape(z)\n             if z_shape is not None:\n-                assert z_shape == z_np.shape\n+                assert z_shape[1:] == z_np.shape[1:]\n \n             z = K.eval(z)\n             assert_allclose(z, z_np, atol=1e-05)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c05ef1fd95a6024155ab59656fef8dac5a45c335", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 1458 | Contributors (this commit): 25 | Commits (past 90d): 2 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -320,7 +320,7 @@ class KerasRegressor(BaseWrapper):\n                 Predictions.\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n-        return np.squeeze(self.model.predict(x, **kwargs))\n+        return np.squeeze(self.model.predict(x, **kwargs), axis=-1)\n \n     def score(self, x, y, **kwargs):\n         \"\"\"Returns the mean loss on the given test data and labels.\n\n@@ -167,6 +167,24 @@ def assert_regression_works(reg):\n     assert preds.shape == (num_test, )\n \n \n+def test_regression_predict_shape_correct_num_test_0():\n+    assert_regression_predict_shape_correct(num_test=0)\n+\n+\n+def test_regression_predict_shape_correct_num_test_1():\n+    assert_regression_predict_shape_correct(num_test=1)\n+\n+\n+def assert_regression_predict_shape_correct(num_test):\n+    reg = KerasRegressor(\n+        build_fn=build_fn_reg, hidden_dims=hidden_dims,\n+        batch_size=batch_size, epochs=epochs)\n+    reg.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n+\n+    preds = reg.predict(X_test[:num_test], batch_size=batch_size)\n+    assert preds.shape == (num_test, )\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e90e17068eb0d9620aeeb90168cc222025ea119e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 28 | Churn Cumulative: 17336 | Contributors (this commit): 133 | Commits (past 90d): 62 | Contributors (cumulative): 163 | DMM Complexity: 1.0\n\nDIFF:\n@@ -585,6 +585,14 @@ def reverse(x, axes):\n     return np.flip(x, axes)\n \n \n+py_slice = slice\n+\n+\n+def slice(x, start, size):\n+    slices = [py_slice(i, i + j) for i, j in zip(start, size)]\n+    return x[tuple(slices)]\n+\n+\n def variable(value, dtype=None, name=None, constraint=None):\n     if constraint is not None:\n         raise TypeError(\"Constraint must be None when \"\n\n@@ -2564,6 +2564,8 @@ def slice(x, start, size):\n         ```python\n         new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]\n         ```\n+\n+    {{np_implementation}}\n     \"\"\"\n     return tf.slice(x, start, size)\n \n\n@@ -1731,6 +1731,24 @@ class TestBackend(object):\n         assert np.alltrue(decode_pred_np == decode_pred)\n         assert np.allclose(log_prob_pred_np, log_prob_pred)\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='tensorflow-way slice is '\n+                        'only supported in tensorflow.')\n+    @pytest.mark.parametrize('x_size', [\n+        [1, 1, 3],\n+        [1, 2, 3],\n+        [2, 1, 3]\n+    ])\n+    def test_slice(self, x_size):\n+        npt = np.array([[[1, 1, 1], [2, 2, 2]],\n+                       [[3, 3, 3], [4, 4, 4]],\n+                       [[5, 5, 5], [6, 6, 6]]])\n+        x_start = [1, 0, 0]\n+        tft = K.constant(npt)\n+        test_input = K.eval(K.slice(tft, x_start, x_size))\n+        expected = KNP.slice(npt, x_start, x_size)\n+        assert np.allclose(test_input, expected)\n+\n     @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Beam search is only implemented with '\n                                'the TensorFlow backend.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4449be11edf1a6ac7298fd5c41c30a7c8cfdc292", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 9735 | Contributors (this commit): 60 | Commits (past 90d): 4 | Contributors (cumulative): 60 | DMM Complexity: None\n\nDIFF:\n@@ -293,7 +293,8 @@ class RNN(Layer):\n     # Output shape\n         - if `return_state`: a list of tensors. The first tensor is\n             the output. The remaining tensors are the last states,\n-            each with shape `(batch_size, units)`.\n+            each with shape `(batch_size, units)`. For example, the number of\n+            state tensors is 1 (for RNN and GRU) or 2 (for LSTM).\n         - if `return_sequences`: 3D tensor with shape\n             `(batch_size, timesteps, units)`.\n         - else, 2D tensor with shape `(batch_size, units)`.\n@@ -2096,7 +2097,8 @@ class LSTM(RNN):\n         return_sequences: Boolean. Whether to return the last output\n             in the output sequence, or the full sequence.\n         return_state: Boolean. Whether to return the last state\n-            in addition to the output.\n+            in addition to the output. The returned elements of the\n+            states list are the hidden state and the cell state, respectively.\n         go_backwards: Boolean (default False).\n             If True, process the input sequence backwards and return the\n             reversed sequence.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78e930b6f6a6a8690e0837b3198a59f3656fe44b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 2298 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -975,9 +975,6 @@ class Network(Layer):\n                     kwargs = input_data[3]\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n-                if inbound_layer_name not in created_layers:\n-                    add_unprocessed_node(layer, node_data)\n-                    return\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                     add_unprocessed_node(layer, node_data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88af7d0c97497b5c3a198ee9416b2accfbc72c36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 14 | Complexity Δ (Sum/Max): 14/5 | Churn Δ: 100 | Churn Cumulative: 150 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,3 +40,6 @@ from .mobilenet import MobileNet\n from .mobilenet_v2 import MobileNetV2\n from .densenet import DenseNet121, DenseNet169, DenseNet201\n from .nasnet import NASNetMobile, NASNetLarge\n+from .resnet import ResNet101, ResNet152\n+from .resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n+from .resnext import ResNeXt50, ResNeXt101\n\n@@ -0,0 +1,34 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+try:\n+    from keras_applications import resnet\n+except:\n+    resnet = None\n+from . import keras_modules_injection\n+\n+\n+@keras_modules_injection\n+def ResNet50(*args, **kwargs):\n+    return resnet.ResNet50(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def ResNet101(*args, **kwargs):\n+    return resnet.ResNet101(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def ResNet152(*args, **kwargs):\n+    return resnet.ResNet152(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def decode_predictions(*args, **kwargs):\n+    return resnet.decode_predictions(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def preprocess_input(*args, **kwargs):\n+    return resnet.preprocess_input(*args, **kwargs)\n\n@@ -0,0 +1,34 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+try:\n+    from keras_applications import resnet_v2\n+except:\n+    resnet_v2 = None\n+from . import keras_modules_injection\n+\n+\n+@keras_modules_injection\n+def ResNet50V2(*args, **kwargs):\n+    return resnet_v2.ResNet50V2(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def ResNet101V2(*args, **kwargs):\n+    return resnet_v2.ResNet101V2(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def ResNet152V2(*args, **kwargs):\n+    return resnet_v2.ResNet152V2(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def decode_predictions(*args, **kwargs):\n+    return resnet_v2.decode_predictions(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def preprocess_input(*args, **kwargs):\n+    return resnet_v2.preprocess_input(*args, **kwargs)\n\n@@ -0,0 +1,29 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+try:\n+    from keras_applications import resnext\n+except:\n+    resnext = None\n+from . import keras_modules_injection\n+\n+\n+@keras_modules_injection\n+def ResNeXt50(*args, **kwargs):\n+    return resnext.ResNeXt50(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def ResNeXt101(*args, **kwargs):\n+    return resnext.ResNeXt101(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def decode_predictions(*args, **kwargs):\n+    return resnext.decode_predictions(*args, **kwargs)\n+\n+\n+@keras_modules_injection\n+def preprocess_input(*args, **kwargs):\n+    return resnext.preprocess_input(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b54657ab4806b0aaef8f8eeb973edb83c3d3483", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 826 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -410,7 +410,7 @@ def weighted_masked_objective(fn):\n             score_array *= mask\n             #  the loss per batch should be proportional\n             #  to the number of unmasked samples.\n-            score_array /= K.mean(mask)\n+            score_array /= K.mean(mask) + K.epsilon()\n \n         # apply sample weighting\n         if weights is not None:\n\n@@ -8,20 +8,32 @@ from keras import losses\n from keras import backend as K\n \n \n+def create_masking_model():\n+    model = Sequential()\n+    model.add(Masking(mask_value=0, input_shape=(None, 1)))\n+    model.add(TimeDistributed(Dense(1, kernel_initializer='one')))\n+    model.compile(loss='mse', optimizer='sgd')\n+    return model\n+\n+\n def test_masking():\n     np.random.seed(1337)\n     x = np.array([[[1], [1]],\n                   [[0], [0]]])\n-    model = Sequential()\n-    model.add(Masking(mask_value=0, input_shape=(2, 1)))\n-    model.add(TimeDistributed(Dense(1, kernel_initializer='one')))\n-    model.compile(loss='mse', optimizer='sgd')\n+    model = create_masking_model()\n     y = np.array([[[1], [1]],\n                   [[1], [1]]])\n     loss = model.train_on_batch(x, y)\n     assert loss == 0\n \n \n+def test_masking_is_all_zeros():\n+    x = y = np.array([[[0], [0]]])\n+    model = create_masking_model()\n+    loss = model.train_on_batch(x, y)\n+    assert loss == 0\n+\n+\n def test_loss_masking():\n     weighted_loss = weighted_masked_objective(losses.get('mae'))\n     shape = (3, 4, 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca802e1b6b9fb47194fe62c2fd0df03de1002c84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 22 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 84 | Churn Cumulative: 6128 | Contributors (this commit): 48 | Commits (past 90d): 17 | Contributors (cumulative): 69 | DMM Complexity: 0.7727272727272727\n\nDIFF:\n@@ -568,8 +568,9 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 for i in sequence:\n                     if self.stop_signal.is_set():\n                         return\n-                    self.queue.put(\n-                        executor.apply_async(get_index, (self.uid, i)), block=True)\n+                    future = executor.apply_async(get_index, (self.uid, i))\n+                    future.idx = i\n+                    self.queue.put(future, block=True)\n \n                 # Done with the current epoch, waiting for the final batches\n                 self._wait_queue()\n@@ -594,8 +595,17 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         \"\"\"\n         try:\n             while self.is_running():\n-                inputs = self.queue.get(block=True).get()\n+                try:\n+                    future = self.queue.get(block=True)\n+                    inputs = future.get(timeout=30)\n                     self.queue.task_done()\n+                except mp.TimeoutError:\n+                    idx = future.idx\n+                    warnings.warn(\n+                        'The input {} could not be retrieved.'\n+                        ' It could be because a worker has died.'.format(idx),\n+                        UserWarning)\n+                    inputs = self.sequence[idx]\n                 if inputs is not None:\n                     yield inputs\n         except Exception:\n@@ -684,8 +694,17 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n         \"\"\"\n         try:\n             while self.is_running():\n-                inputs = self.queue.get(block=True).get()\n+                try:\n+                    future = self.queue.get(block=True)\n+                    inputs = future.get(timeout=30)\n                     self.queue.task_done()\n+                except mp.TimeoutError:\n+                    warnings.warn(\n+                        'An input could not be retrieved.'\n+                        ' It could be because a worker has died.'\n+                        'We do not have any information on the lost sample.',\n+                        UserWarning)\n+                    continue\n                 if inputs is not None:\n                     yield inputs\n         except StopIteration:\n\n@@ -466,10 +466,11 @@ def test_model_methods():\n                               epochs=5,\n                               initial_epoch=0,\n                               validation_data=val_seq,\n-                              callbacks=[tracker_cb])\n+                              callbacks=[tracker_cb],\n+                              max_queue_size=1)\n     assert trained_epochs == [0, 1, 2, 3, 4]\n     assert trained_batches == list(range(12)) * 5\n-    assert len(val_seq.logs) == 12 * 5\n+    assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.\n \n     # test for workers = 0\n     trained_epochs = []\n\n@@ -1,7 +1,9 @@\n import pytest\n import json\n import keras\n+import keras.backend as K\n import numpy as np\n+import os\n \n \n def test_dense_legacy_interface():\n@@ -806,6 +808,9 @@ def test_cropping3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n+@pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'}\n+                    and 'TRAVIS_PYTHON_VERSION' in os.environ,\n+                    reason='Generators cannot use `spawn`.')\n def test_generator_methods_interface():\n     def train_generator():\n         x = np.random.randn(2, 2)\n\n@@ -13,6 +13,8 @@ import pytest\n import six\n from six.moves.urllib.parse import urljoin\n from six.moves.urllib.request import pathname2url\n+import warnings\n+\n from flaky import flaky\n \n from keras.utils import GeneratorEnqueuer\n@@ -24,9 +26,13 @@ from keras.utils.data_utils import validate_file\n from keras import backend as K\n \n pytestmark = pytest.mark.skipif(\n-    K.backend() in {'tensorflow', 'cntk'} and 'TRAVIS_PYTHON_VERSION' in os.environ,\n+    six.PY2 and 'TRAVIS_PYTHON_VERSION' in os.environ,\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n \n+skip_generators = pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'} and\n+                                     'TRAVIS_PYTHON_VERSION' in os.environ,\n+                                     reason='Generators do not work with `spawn`.')\n+\n if sys.version_info < (3,):\n     def next(x):\n         return x.next()\n@@ -34,14 +40,17 @@ if sys.version_info < (3,):\n \n def use_spawn(func):\n     \"\"\"Decorator to test both Unix (fork) and Windows (spawn)\"\"\"\n+\n     @six.wraps(func)\n     def wrapper(*args, **kwargs):\n-        out = func(*args, **kwargs)\n         if sys.version_info > (3, 4):\n             mp.set_start_method('spawn', force=True)\n-            func(*args, **kwargs)\n+            out = func(*args, **kwargs)\n             mp.set_start_method('fork', force=True)\n+        else:\n+            out = func(*args, **kwargs)\n         return out\n+\n     return wrapper\n \n \n@@ -188,6 +197,7 @@ def test_generator_enqueuer_threads():\n     enqueuer.stop()\n \n \n+@skip_generators\n def test_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n         DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n@@ -223,6 +233,7 @@ def test_generator_enqueuer_fail_threads():\n         next(gen_output)\n \n \n+@skip_generators\n def test_generator_enqueuer_fail_processes():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n         FaultSequence()), use_multiprocessing=True)\n@@ -378,6 +389,7 @@ def test_finite_generator_enqueuer_threads():\n     enqueuer.stop()\n \n \n+@skip_generators\n def test_finite_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n         DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n@@ -391,5 +403,34 @@ def test_finite_generator_enqueuer_processes():\n     enqueuer.stop()\n \n \n+@pytest.mark.skipif('TRAVIS_PYTHON_VERSION' in os.environ,\n+                    reason='Takes 150s to run')\n+def test_missing_inputs():\n+    missing_idx = 10\n+\n+    class TimeOutSequence(DummySequence):\n+        def __getitem__(self, item):\n+            if item == missing_idx:\n+                time.sleep(120)\n+            return super(TimeOutSequence, self).__getitem__(item)\n+\n+    enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n+        TimeOutSequence([3, 2, 2, 3])), use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.warns(UserWarning, match='An input could not be retrieved.'):\n+        for _ in range(4 * missing_idx):\n+            next(gen_output)\n+\n+    enqueuer = OrderedEnqueuer(TimeOutSequence([3, 2, 2, 3]),\n+                               use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    warning_msg = \"The input {} could not be retrieved.\".format(missing_idx)\n+    with pytest.warns(UserWarning, match=warning_msg):\n+        for _ in range(11):\n+            next(gen_output)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf0f25f79a368aecc79515be6a7f20a87de2c436", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 17343 | Contributors (this commit): 133 | Commits (past 90d): 58 | Contributors (cumulative): 163 | DMM Complexity: 1.0\n\nDIFF:\n@@ -285,6 +285,10 @@ def softplus(x):\n     return np.log(1. + np.exp(x))\n \n \n+def softsign(x):\n+    return x / (1 + np.abs(x))\n+\n+\n def elu(x, alpha=1.):\n     return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0)\n \n\n@@ -3447,6 +3447,8 @@ def softsign(x):\n \n     # Returns\n         A tensor.\n+\n+    {{np_implementation}}\n     \"\"\"\n     return tf.nn.softsign(x)\n \n\n@@ -1077,6 +1077,7 @@ class TestBackend(object):\n                                       max_value=max_value, threshold=threshold)\n \n     def test_nn_operations(self):\n+        check_single_tensor_operation('softsign', (4, 10), WITH_NP)\n         check_single_tensor_operation('softplus', (4, 10), WITH_NP)\n         check_single_tensor_operation('elu', (4, 10), WITH_NP, alpha=0.5)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bfa87eb1b0ab53c5f1efab1ec1d91377678046af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 1861 | Contributors (this commit): 31 | Commits (past 90d): 10 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -418,13 +418,6 @@ def clean_module_name(name):\n     return name\n \n \n-def class_to_docs_link(cls):\n-    module_name = clean_module_name(cls.__module__)\n-    module_name = module_name[6:]\n-    link = ROOT + module_name.replace('.', '/') + '#' + cls.__name__.lower()\n-    return link\n-\n-\n def class_to_source_link(cls):\n     module_name = clean_module_name(cls.__module__)\n     path = module_name.replace('.', '/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29e740e0965be5f460f67692a284a9a2deb6bb04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 22 | Churn Cumulative: 17365 | Contributors (this commit): 133 | Commits (past 90d): 60 | Contributors (cumulative): 165 | DMM Complexity: 1.0\n\nDIFF:\n@@ -737,6 +737,10 @@ def _remove_blanks(inds, num_classes):\n     return inds[inds < (num_classes - 1)]\n \n \n+def stack(x, axis=0):\n+    return np.stack(x, axis=axis)\n+\n+\n square = np.square\n abs = np.abs\n exp = np.exp\n\n@@ -2510,6 +2510,8 @@ def stack(x, axis=0):\n \n     # Returns\n         A tensor.\n+\n+    {{np_implementation}}\n     \"\"\"\n     return tf.stack(x, axis=axis)\n \n\n@@ -1863,6 +1863,22 @@ class TestBackend(object):\n         assert k_s_d.shape == k_d.shape\n         assert_allclose(k_s_d, k_d, atol=1e-05)\n \n+    def test_stack(self):\n+        tensor_list = [np.random.randn(5, 4, 6, 10) for _ in range(5)]\n+        stack_axis = 3\n+        results = []\n+        if WITH_NP[0] == KC:\n+            check_two_tensor_operation('stack', (5, 4, 6, 10),\n+                                       (5, 4, 6, 10), WITH_NP,\n+                                       axis=stack_axis, concat_args=True)\n+        else:\n+            for k in WITH_NP:\n+                tensor_list_var = [k.variable(tensor) for tensor in tensor_list]\n+                out = k.eval(k.stack(tensor_list_var, axis=stack_axis))\n+                results.append(out)\n+\n+            assert_list_pairwise(results)\n+\n     @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n     def test_map(self):\n         x = np.random.rand(10, 3).astype(np.float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f41e41eda6e8ea96403cae5798a5a89c8bb5605", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 324 | Lines Deleted: 120 | Files Changed: 2 | Hunks: 38 | Methods Changed: 24 | Complexity Δ (Sum/Max): 23/22 | Churn Δ: 444 | Churn Cumulative: 2208 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,6 @@ import pytest\n import six\n from six.moves.urllib.parse import urljoin\n from six.moves.urllib.request import pathname2url\n-import warnings\n \n from flaky import flaky\n \n@@ -33,17 +32,15 @@ skip_generators = pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'} and\n                                      'TRAVIS_PYTHON_VERSION' in os.environ,\n                                      reason='Generators do not work with `spawn`.')\n \n-if sys.version_info < (3,):\n-    def next(x):\n-        return x.next()\n-\n \n def use_spawn(func):\n-    \"\"\"Decorator to test both Unix (fork) and Windows (spawn)\"\"\"\n+    \"\"\"Decorator which uses `spawn` when possible.\n+    This is useful on Travis to avoid memory issues.\n+    \"\"\"\n \n     @six.wraps(func)\n     def wrapper(*args, **kwargs):\n-        if sys.version_info > (3, 4):\n+        if sys.version_info > (3, 4) and os.name != 'nt':\n             mp.set_start_method('spawn', force=True)\n             out = func(*args, **kwargs)\n             mp.set_start_method('fork', force=True)\n@@ -54,6 +51,11 @@ def use_spawn(func):\n     return wrapper\n \n \n+if sys.version_info < (3,):\n+    def next(x):\n+        return x.next()\n+\n+\n @pytest.fixture\n def in_tmpdir(tmpdir):\n     \"\"\"Runs a function in a temporary directory.\n\n@@ -1,17 +1,45 @@\n from __future__ import print_function\n+\n+import multiprocessing as mp\n import os\n+import sys\n import threading\n import pytest\n import numpy as np\n+import six\n+\n from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils import Sequence\n from keras import backend as K\n \n pytestmark = pytest.mark.skipif(\n-    K.backend() in {'tensorflow', 'cntk'} and 'TRAVIS_PYTHON_VERSION' in os.environ,\n+    six.PY2 and 'TRAVIS_PYTHON_VERSION' in os.environ,\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n \n+skip_generators = pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'} and\n+                                     'TRAVIS_PYTHON_VERSION' in os.environ,\n+                                     reason='Generators do not work with `spawn`.')\n+\n+\n+def use_spawn(func):\n+    \"\"\"Decorator which uses `spawn` when possible.\n+    This is useful on Travis to avoid memory issues.\n+    \"\"\"\n+\n+    @six.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        if sys.version_info > (3, 4) and os.name != 'nt':\n+            mp.set_start_method('spawn', force=True)\n+            out = func(*args, **kwargs)\n+            mp.set_start_method('fork', force=True)\n+        else:\n+            out = func(*args, **kwargs)\n+        return out\n+\n+    return wrapper\n+\n+\n STEPS_PER_EPOCH = 100\n STEPS = 100\n WORKERS = 4 if K.backend() != 'tensorflow' else 2\n@@ -66,6 +94,7 @@ def in_tmpdir(tmpdir):\n     assert not tmpdir.listdir()\n \n \n+@skip_generators\n def test_multiprocessing_training():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -218,22 +247,6 @@ def test_multiprocessing_training():\n                         workers=0,\n                         use_multiprocessing=False)\n \n-    # - For Sequence\n-    model.fit_generator(DummySequence(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=True)\n-    model.fit_generator(DummySequence(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=False)\n-\n     # Test invalid use cases\n     @threadsafe_generator\n     def invalid_generator():\n@@ -273,7 +286,24 @@ def test_multiprocessing_training():\n                             workers=1,\n                             use_multiprocessing=False)\n \n+    # - For Sequence\n+    model.fit_generator(DummySequence(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=DummySequence(),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=True)\n+    model.fit_generator(DummySequence(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=DummySequence(),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=False)\n \n+\n+@skip_generators\n def test_multiprocessing_training_from_file(in_tmpdir):\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -325,17 +355,6 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n                             workers=WORKERS,\n                             use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=WORKERS,\n-                        use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -361,6 +380,59 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n                             workers=1,\n                             use_multiprocessing=True)\n \n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        epochs=1,\n+                        verbose=1,\n+                        validation_steps=None,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=True)\n+\n+    os.remove('data.npz')\n+\n+\n+def test_multithreading_from_file():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+    np.savez('data_threads.npz', **{'data': arr_data, 'labels': arr_labels})\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        arr = np.load('data_threads.npz')\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr['data'][start: end]\n+            y = arr['labels'][start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        epochs=1,\n+                        verbose=1,\n+                        validation_steps=None,\n+                        max_queue_size=10,\n+                        workers=WORKERS,\n+                        use_multiprocessing=False)\n+\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     model.fit_generator(custom_generator(),\n@@ -374,14 +446,6 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n \n     # - Produce and consume data without a queue on main thread\n     #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=True)\n     model.fit_generator(custom_generator(),\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         epochs=1,\n@@ -391,9 +455,10 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n                         workers=0,\n                         use_multiprocessing=False)\n \n-    os.remove('data.npz')\n+    os.remove('data_threads.npz')\n \n \n+@skip_generators\n def test_multiprocessing_predicting():\n     arr_data = np.random.randint(0, 256, (50, 2))\n \n@@ -433,14 +498,6 @@ def test_multiprocessing_predicting():\n                                 workers=WORKERS,\n                                 use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -460,6 +517,43 @@ def test_multiprocessing_predicting():\n                                 workers=1,\n                                 use_multiprocessing=True)\n \n+    # - Main thread runs the generator without a queue\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.predict_generator(custom_generator(),\n+                            steps=STEPS,\n+                            max_queue_size=10,\n+                            workers=0,\n+                            use_multiprocessing=True)\n+\n+\n+def test_multithreading_predicting():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            yield X\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    model.predict_generator(custom_generator(),\n+                            steps=STEPS,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n+                            use_multiprocessing=False)\n+\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     model.predict_generator(custom_generator(),\n@@ -470,11 +564,6 @@ def test_multiprocessing_predicting():\n \n     # - Main thread runs the generator without a queue\n     #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=True)\n     model.predict_generator(custom_generator(),\n                             steps=STEPS,\n                             max_queue_size=10,\n@@ -482,6 +571,7 @@ def test_multiprocessing_predicting():\n                             use_multiprocessing=False)\n \n \n+@skip_generators\n def test_multiprocessing_evaluating():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -524,14 +614,6 @@ def test_multiprocessing_evaluating():\n                                  workers=WORKERS,\n                                  use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=WORKERS,\n-                             use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -551,6 +633,45 @@ def test_multiprocessing_evaluating():\n                                  workers=1,\n                                  use_multiprocessing=True)\n \n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.evaluate_generator(custom_generator(),\n+                             steps=STEPS,\n+                             max_queue_size=10,\n+                             workers=0,\n+                             use_multiprocessing=True)\n+\n+\n+def test_multithreading_evaluating():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    model.evaluate_generator(custom_generator(),\n+                             steps=STEPS,\n+                             max_queue_size=10,\n+                             workers=WORKERS,\n+                             use_multiprocessing=False)\n+\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     model.evaluate_generator(custom_generator(),\n@@ -561,11 +682,6 @@ def test_multiprocessing_evaluating():\n \n     # - Produce and consume data without a queue on main thread\n     #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=0,\n-                             use_multiprocessing=True)\n     model.evaluate_generator(custom_generator(),\n                              steps=STEPS,\n                              max_queue_size=10,\n@@ -573,6 +689,7 @@ def test_multiprocessing_evaluating():\n                              use_multiprocessing=False)\n \n \n+@skip_generators\n def test_multiprocessing_fit_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -621,17 +738,6 @@ def test_multiprocessing_fit_error():\n                                 workers=WORKERS,\n                                 use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -655,6 +761,53 @@ def test_multiprocessing_fit_error():\n                                 workers=1,\n                                 use_multiprocessing=True)\n \n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=0,\n+                            use_multiprocessing=True)\n+\n+\n+def test_multithreading_fit_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+    batch_size = 10\n+    n_samples = 50\n+    good_batches = 3\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    samples = batch_size * (good_batches + 1)\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n+                            use_multiprocessing=False)\n+\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     #   - Make sure `RuntimeError` exception bubbles up\n@@ -669,13 +822,6 @@ def test_multiprocessing_fit_error():\n     # - Produce and consume data without a queue on main thread\n     #   - Make sure the value of `use_multiprocessing` is ignored\n     #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=True)\n     with pytest.raises(RuntimeError):\n         model.fit_generator(custom_generator(),\n                             steps_per_epoch=samples,\n@@ -685,6 +831,7 @@ def test_multiprocessing_fit_error():\n                             use_multiprocessing=False)\n \n \n+@skip_generators\n def test_multiprocessing_evaluate_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -729,16 +876,6 @@ def test_multiprocessing_evaluate_error():\n                                      workers=WORKERS,\n                                      use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches * WORKERS + 1,\n-                                 max_queue_size=10,\n-                                 workers=WORKERS,\n-                                 use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -760,6 +897,50 @@ def test_multiprocessing_evaluate_error():\n                                      workers=1,\n                                      use_multiprocessing=True)\n \n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches + 1,\n+                                 max_queue_size=10,\n+                                 workers=0,\n+                                 use_multiprocessing=True)\n+\n+\n+def test_multithreading_evaluate_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+    batch_size = 10\n+    n_samples = 50\n+    good_batches = 3\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches * WORKERS + 1,\n+                                 max_queue_size=10,\n+                                 workers=WORKERS,\n+                                 use_multiprocessing=False)\n+\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     #   - Make sure `RuntimeError` exception bubbles up\n@@ -773,12 +954,6 @@ def test_multiprocessing_evaluate_error():\n     # - Produce and consume data without a queue on main thread\n     #   - Make sure the value of `use_multiprocessing` is ignored\n     #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches + 1,\n-                                 max_queue_size=10,\n-                                 workers=0,\n-                                 use_multiprocessing=True)\n     with pytest.raises(RuntimeError):\n         model.evaluate_generator(custom_generator(),\n                                  steps=good_batches + 1,\n@@ -787,6 +962,7 @@ def test_multiprocessing_evaluate_error():\n                                  use_multiprocessing=False)\n \n \n+@skip_generators\n def test_multiprocessing_predict_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     good_batches = 3\n@@ -830,16 +1006,6 @@ def test_multiprocessing_predict_error():\n                                     workers=WORKERS,\n                                     use_multiprocessing=True)\n \n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches * WORKERS + 1,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=False)\n-\n     # - Produce data on 1 worker process, consume on main process:\n     #   - Worker process runs generator\n     #   - BUT on Windows, `multiprocessing` won't marshall generators across\n@@ -861,6 +1027,48 @@ def test_multiprocessing_predict_error():\n                                     workers=1,\n                                     use_multiprocessing=True)\n \n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches + 1,\n+                                max_queue_size=10,\n+                                workers=0,\n+                                use_multiprocessing=True)\n+\n+\n+def test_multithreading_predict_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    good_batches = 3\n+\n+    @threadsafe_generator\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        batch_size = 10\n+        n_samples = 50\n+\n+        for i in range(good_batches):\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            yield X\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2,)))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches * WORKERS + 1,\n+                                max_queue_size=10,\n+                                workers=WORKERS,\n+                                use_multiprocessing=False)\n     # - Produce data on 1 worker thread, consume on main thread:\n     #   - Worker thread is the only thread running the generator\n     #   - Make sure `RuntimeError` exception bubbles up\n@@ -874,12 +1082,6 @@ def test_multiprocessing_predict_error():\n     # - Produce and consume data without a queue on main thread\n     #   - Make sure the value of `use_multiprocessing` is ignored\n     #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches + 1,\n-                                max_queue_size=10,\n-                                workers=0,\n-                                use_multiprocessing=True)\n     with pytest.raises(RuntimeError):\n         model.predict_generator(custom_generator(),\n                                 steps=good_batches + 1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
