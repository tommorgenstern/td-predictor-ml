{"custom_id": "keras#58611f18319b4384c7100d3eff690ef193dcc8ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 127 | Churn Cumulative: 219 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from keras import backend\n from keras import layers\n from keras import metrics\n from keras import models\n+from keras import optimizers\n from keras import testing\n from keras.saving import object_registration\n \n@@ -29,6 +30,21 @@ class CustomModelX(models.Model):\n         return 1\n \n \n+@object_registration.register_keras_serializable(package=\"my_package\")\n+class CustomSignatureModel(models.Model):\n+    def __init__(self):\n+        super(CustomSignatureModel, self).__init__()\n+        self.v = tf.Variable(1.0)\n+\n+    @tf.function\n+    def __call__(self, x):\n+        return x * self.v\n+\n+    @tf.function(input_signature=[tf.TensorSpec([], tf.float32)])\n+    def mutate(self, new_v):\n+        self.v.assign(new_v)\n+\n+\n @pytest.mark.skipif(\n     backend.backend() != \"tensorflow\",\n     reason=\"The SavedModel test can only run with TF backend.\",\n@@ -246,3 +262,114 @@ class SavedModelTest(testing.TestCase):\n             rtol=1e-4,\n             atol=1e-4,\n         )\n+\n+    def test_fixed_signature_string_dtype(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class Adder(models.Model):\n+            @tf.function(\n+                input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)]\n+            )\n+            def concat(self, x):\n+                return x + x\n+\n+        model = Adder()\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        self.assertEqual(model.concat(\"hello\"), restored_model.concat(\"hello\"))\n+\n+    def test_non_fixed_signature_string_dtype(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class Adder(models.Model):\n+            @tf.function\n+            def concat(self, x):\n+                return x + x\n+\n+        model = Adder()\n+\n+        no_fn_path = os.path.join(self.get_temp_dir(), \"my_keras_model_no_fn\")\n+        tf.saved_model.save(model, no_fn_path)\n+        restored_model = tf.saved_model.load(no_fn_path)\n+        with self.assertRaisesRegex(ValueError, \"zero restored functions\"):\n+            _ = restored_model.concat(\"hello\")\n+\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n+        tf.saved_model.save(\n+            model,\n+            path,\n+            signatures=model.concat.get_concrete_function(\n+                tf.TensorSpec(shape=[], dtype=tf.string, name=\"string_input\")\n+            ),\n+        )\n+        restored_model = tf.saved_model.load(path)\n+        self.assertEqual(model.concat(\"hello\"), restored_model.concat(\"hello\"))\n+\n+    def test_fine_tuning(self):\n+        model = CustomSignatureModel()\n+        model_no_signatures_path = os.path.join(\n+            self.get_temp_dir(), \"model_no_signatures\"\n+        )\n+        _ = model(tf.constant(0.0))\n+\n+        tf.saved_model.save(model, model_no_signatures_path)\n+        restored_model = tf.saved_model.load(model_no_signatures_path)\n+\n+        self.assertLen(list(restored_model.signatures.keys()), 0)\n+        self.assertEqual(restored_model(tf.constant(3.0)).numpy(), 3)\n+        restored_model.mutate(tf.constant(2.0))\n+        self.assertEqual(restored_model(tf.constant(3.0)).numpy(), 6)\n+        optimizer = optimizers.SGD(0.05)\n+\n+        def train_step():\n+            with tf.GradientTape() as tape:\n+                loss = (10.0 - restored_model(tf.constant(2.0))) ** 2\n+            variables = tape.watched_variables()\n+            grads = tape.gradient(loss, variables)\n+            optimizer.apply_gradients(zip(grads, variables))\n+            return loss\n+\n+        for _ in range(10):\n+            # \"v\" approaches 5, \"loss\" approaches 0\n+            loss = train_step()\n+\n+        self.assertAllClose(loss, 0.0, rtol=1e-2, atol=1e-2)\n+        self.assertAllClose(restored_model.v.numpy(), 5.0, rtol=1e-2, atol=1e-2)\n+\n+    def test_signatures_path(self):\n+        model = CustomSignatureModel()\n+        model_with_signature_path = os.path.join(\n+            self.get_temp_dir(), \"model_with_signature\"\n+        )\n+        call = model.__call__.get_concrete_function(\n+            tf.TensorSpec(None, tf.float32)\n+        )\n+\n+        tf.saved_model.save(model, model_with_signature_path, signatures=call)\n+        restored_model = tf.saved_model.load(model_with_signature_path)\n+        self.assertEqual(\n+            list(restored_model.signatures.keys()), [\"serving_default\"]\n+        )\n+\n+    def test_multiple_signatures_dict_path(self):\n+        model = CustomSignatureModel()\n+        model_multiple_signatures_path = os.path.join(\n+            self.get_temp_dir(), \"model_with_multiple_signatures\"\n+        )\n+        call = model.__call__.get_concrete_function(\n+            tf.TensorSpec(None, tf.float32)\n+        )\n+        signatures = {\n+            \"serving_default\": call,\n+            \"array_input\": model.__call__.get_concrete_function(\n+                tf.TensorSpec([None], tf.float32)\n+            ),\n+        }\n+\n+        tf.saved_model.save(\n+            model, model_multiple_signatures_path, signatures=signatures\n+        )\n+        restored_model = tf.saved_model.load(model_multiple_signatures_path)\n+        self.assertEqual(\n+            list(restored_model.signatures.keys()),\n+            [\"serving_default\", \"array_input\"],\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1553deeca3d257ad73b246c8b51612b2b5398d8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 185 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 19/7 | Churn Δ: 185 | Churn Cumulative: 1430 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: 0.8082191780821918\n\nDIFF:\n@@ -87,6 +87,30 @@ def distribute_tensor(tensor, layout):\n         return global_value\n \n \n+def initialize(job_addresses, num_processes, process_id):\n+    if job_addresses and \",\" in job_addresses:\n+        # When user provide all the job addresses, we will split and get the\n+        # first one, which is the coordinator.\n+        job_addresses = job_addresses.split(\",\")\n+        # Do a sanity check to make sure the number of addresses also match\n+        # the num_processes.\n+        if num_processes is not None and num_processes != len(job_addresses):\n+            raise ValueError(\n+                f\"The provided job_addresses {job_addresses} has \"\n+                f\"{len(job_addresses)} jobs, but num_processes is \"\n+                f\"{num_processes}\"\n+            )\n+        corrdinator_address = job_addresses[0]\n+    else:\n+        corrdinator_address = job_addresses\n+\n+    jax.distributed.initialize(\n+        corrdinator_address=corrdinator_address,\n+        num_processes=num_processes,\n+        process_id=process_id,\n+    )\n+\n+\n def _to_jax_device(device_id):\n     device_type, index = device_id.split(\":\")\n     index = int(index)\n\n@@ -2,6 +2,7 @@\n \n import functools\n import os\n+from unittest import mock\n \n import jax\n import numpy as np\n@@ -36,6 +37,26 @@ class JaxDistributionLibTest(testing.TestCase):\n         self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n         self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n \n+    @mock.patch.object(jax.distributed, \"initialize\", return_value=None)\n+    def test_initialize_with_all_job_addresses(self, mock_jax_initialze):\n+        backend_dlib.initialize(\"10.0.0.1:1234,10.0.0.2:2345\", 2, 0)\n+        mock_jax_initialze.assert_called_once_with(\n+            corrdinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n+        )\n+\n+    def test_initialize_validate_job_and_process(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"has 2 jobs, but num_processes is 3\"\n+        ):\n+            backend_dlib.initialize(\"10.0.0.1:1234,10.0.0.2:2345\", 3, 0)\n+\n+    @mock.patch.object(jax.distributed, \"initialize\", return_value=None)\n+    def test_initialize_with_coordinater_address(self, mock_jax_initialze):\n+        backend_dlib.initialize(\"10.0.0.1:1234\", 2, 0)\n+        mock_jax_initialze.assert_called_once_with(\n+            corrdinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n+        )\n+\n     def test_distribute_tensor(self):\n         jax_mesh = jax.sharding.Mesh(\n             np.array(jax.devices()).reshape(2, 4), (\"batch\", \"model\")\n\n@@ -8,6 +8,7 @@ will be implemented in the future (via tf.dtensor API).\n \n import collections\n import contextlib\n+import os\n import re\n import warnings\n \n@@ -40,6 +41,96 @@ def list_devices(device_type=None):\n     return distribution_lib.list_devices(device_type)\n \n \n+@keras_export(\"keras.distribution.initialize\")\n+def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n+    \"\"\"Initialize the distribution system for multi-host/process setting.\n+\n+    Calling `initialize` will prepare the backend for execution on multi-host\n+    GPU or TPUs. It should be called before any computations.\n+\n+    Note that the parameters can also be injected via enviornment variables,\n+    which can be better controlled by the launch script at startup time.\n+    For certain backend that also rely on the enviornment variables to\n+    configure, Keras will properly forward them.\n+\n+    Args:\n+        job_addresses: string. Comma separated IP addresses for all the jobs\n+            that will form the whole computation cluster. Note that for JAX\n+            backend, only the address for job 0 (coodinator) is needed. For\n+            certain runtime like cloud TPU, this value can be `None`, and the\n+            backend will figure it out with the TPU enviornment variables. You\n+            can also config this value via enviornment variable\n+            `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n+        num_processes: int. The number of worker/processes that will form the\n+            whole computation cluster. For certain runtime like cloud TPU, this\n+            value can be `None`, and the backend will figure it out with the TPU\n+            enviornment variables. You can also configure this value via\n+            enviornment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+        process_id: int. The ID number of the current worker/process. The value\n+            should be ranged from `0` to `num_processes - 1`. `0` will indicate\n+            the current worker/process is the master/coordinate job. You can\n+            also configure this value via enviornment variable\n+            `KERAS_DISTRIBUTION_PROCESS_ID`.\n+\n+        Example:\n+            Suppose there are two GPU processes, and process 0 is running at\n+            address `10.0.0.1:1234`, and process 1 is running at address\n+            `10.0.0.2:2345`. To configure such cluster, you can run\n+\n+        On process 0:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=0)\n+        ```\n+\n+        On process 1:\n+        ```python\n+        keras.distribute.initialize(\n+            job_addresses=\"10.0.0.1:1234,10.0.0.2:2345\",\n+            num_processes=2,\n+            process_id=1)\n+        ```\n+\n+        or via the enviornment variables:\n+        On process 0:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"0\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        On process 1:\n+        ```python\n+        os.environ[\n+            \"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = \"2\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = \"1\"\n+        keras.distribute.initialize()\n+        ```\n+\n+        Also note that for JAX backend, the `job_addresses` can be further\n+        reduced to just the master/coordinator address, which is\n+        `10.0.0.1:1234`.\n+    \"\"\"\n+    if (\n+        job_addresses is None\n+        and \"KERAS_DISTRIBUTION_JOB_ADDRESSES\" in os.environ\n+    ):\n+        job_addresses = os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"]\n+    if (\n+        num_processes is None\n+        and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n+    ):\n+        num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n+    if proceed_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        proceed_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    distribution_lib.initialize(job_addresses, num_processes, proceed_id)\n+\n+\n @keras_export(\"keras.distribution.DeviceMesh\")\n class DeviceMesh:\n     \"\"\"A cluster of computation devices for distributed computation.\n\n@@ -1,14 +1,63 @@\n \"\"\"Test for distribution_lib.py.\"\"\"\n \n+import os\n from unittest import mock\n \n import numpy as np\n+import pytest\n \n from keras import backend\n from keras import testing\n+from keras.backend import distribution_lib as backend_dlib\n from keras.distribution import distribution_lib\n \n \n+@pytest.mark.skipif(\n+    backend.backend() != \"jax\",\n+    reason=\"Only JAX has the backend to mock at the moment\",\n+)\n+@mock.patch.object(\n+    backend_dlib,\n+    \"initialize\",\n+    return_value=None,\n+)\n+class MultiProcessInitializeTest(testing.TestCase):\n+    def tearDown(self):\n+        super().tearDown()\n+        os.environ.clear()\n+\n+    def test_initialize_with_explicit_param(self, mock_backend_initialize):\n+        job_addresses = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        num_processes = 2\n+        current_process_id = 0\n+\n+        distribution_lib.initialize(\n+            job_addresses, num_processes, current_process_id\n+        )\n+\n+        mock_backend_initialize.assert_called_once_with(\n+            job_addresses, num_processes, current_process_id\n+        )\n+\n+    def test_initialize_with_env_vars(self, mock_backend_initialize):\n+        job_addresses = \"10.0.0.1:1234,10.0.0.2:2345\"\n+        num_processes = 2\n+        current_process_id = 0\n+        os.environ[\"KERAS_DISTRIBUTION_JOB_ADDRESSES\"] = job_addresses\n+        os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"] = str(num_processes)\n+        os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"] = str(current_process_id)\n+\n+        distribution_lib.initialize()\n+        mock_backend_initialize.assert_called_once_with(\n+            job_addresses, num_processes, current_process_id\n+        )\n+\n+    def test_init_with_nones(self, mock_backend_initialize):\n+        # This is also valid case for Cloud TPU on JAX\n+        distribution_lib.initialize()\n+        mock_backend_initialize.assert_called_once_with(None, None, None)\n+\n+\n class DeviceMeshTest(testing.TestCase):\n     def test_mesh_creation(self):\n         devices = [f\"cpu:{i}\" for i in range(8)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#51e4516ab77fea6e52c23181405e3ea762b72437", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 207 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -853,7 +853,10 @@ class Layer(BackendLayer, Operation):\n         return outputs\n \n     def call(self, *args, **kwargs):\n-        raise NotImplementedError\n+        raise NotImplementedError(\n+            f\"Layer {self.__class__.__name__} does not have a `call()` \"\n+            \"method implemented.\"\n+        )\n \n     @traceback_utils.filter_traceback\n     def stateless_call(\n\n@@ -156,8 +156,11 @@ class Model(Trainer, Layer):\n         else:\n             Layer.__init__(self, *args, **kwargs)\n \n-    def call(self, inputs, training=False):\n-        raise NotImplementedError\n+    def call(self, *args, **kwargs):\n+        raise NotImplementedError(\n+            f\"Model {self.__class__.__name__} does not have a `call()` \"\n+            \"method implemented.\"\n+        )\n \n     @property\n     def layers(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc946ac2318766263e6d7b3d85ec58b413c42d19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 2598 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -260,7 +260,8 @@ def _clone_functional_model(model, input_tensors=None, clone_function=None):\n             )\n     else:\n         input_tensors = tree.map_structure(\n-            lambda x: Input(x.shape, dtype=x.dtype, name=x.name), model.input\n+            lambda x: Input(batch_shape=x.shape, dtype=x.dtype, name=x.name),\n+            model.input,\n         )\n \n     def operation_fn(layer):\n\n@@ -8,7 +8,7 @@ from keras import testing\n from keras.models.cloning import clone_model\n \n \n-def get_functional_model(shared_layers=False):\n+def get_mlp_functional_model(shared_layers=False):\n     inputs = layers.Input(shape=(3,))\n     x = layers.Dense(2)(inputs)\n     if shared_layers:\n@@ -20,6 +20,18 @@ def get_functional_model(shared_layers=False):\n     return model\n \n \n+def get_cnn_functional_model(shared_layers=False):\n+    inputs = layers.Input(shape=(7, 3))\n+    x = layers.Conv1D(2, 2, padding=\"same\")(inputs)\n+    if shared_layers:\n+        layer = layers.Conv1D(2, 2, padding=\"same\", name=\"shared\")\n+        x = layer(x)\n+        x = layer(x)\n+    outputs = layers.Conv1D(2, 2, padding=\"same\")(x)\n+    model = models.Model(inputs, outputs)\n+    return model\n+\n+\n def get_sequential_model(explicit_input=True):\n     model = models.Sequential()\n     if explicit_input:\n@@ -45,7 +57,8 @@ def get_subclassed_model():\n @pytest.mark.requires_trainable_backend\n class CloneModelTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n-        (\"functional\", get_functional_model),\n+        (\"mlp_functional\", get_mlp_functional_model),\n+        (\"cnn_functional\", get_cnn_functional_model, True),\n         (\"sequential\", get_sequential_model),\n         (\n             \"deferred_sequential\",\n@@ -53,8 +66,8 @@ class CloneModelTest(testing.TestCase, parameterized.TestCase):\n         ),\n         (\"subclassed\", get_subclassed_model),\n     )\n-    def test_cloning_correctness(self, model_fn):\n-        ref_input = np.random.random((2, 3))\n+    def test_cloning_correctness(self, model_fn, is_conv=False):\n+        ref_input = np.random.random((2, 7, 3) if is_conv else (2, 3))\n         model = model_fn()\n         new_model = clone_model(model)\n         ref_output = model(ref_input)\n@@ -64,7 +77,8 @@ class CloneModelTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(ref_output, output)\n \n     @parameterized.named_parameters(\n-        (\"functional\", get_functional_model),\n+        (\"mlp_functional\", get_mlp_functional_model),\n+        (\"cnn_functional\", get_cnn_functional_model),\n         (\"sequential\", get_sequential_model),\n     )\n     def test_custom_clone_function(self, model_fn):\n@@ -80,6 +94,6 @@ class CloneModelTest(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(l2.name, l1.name + \"_custom\")\n \n     def test_shared_layers_cloning(self):\n-        model = get_functional_model(shared_layers=True)\n+        model = get_mlp_functional_model(shared_layers=True)\n         new_model = clone_model(model)\n         self.assertLen(new_model.layers, 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d72171dd0f85c227e4628787d9c6f6dcadad524f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 47 | Churn Cumulative: 4030 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -133,7 +133,12 @@ class CategoryEncoding(TFDataLayer):\n         return outputs\n \n     def compute_output_shape(self, input_shape):\n+        if self.output_mode == \"one_hot\":\n+            if input_shape[-1] != 1:\n                 return tuple(input_shape + (self.num_tokens,))\n+            else:\n+                return tuple(input_shape[:-1] + (self.num_tokens,))\n+        return tuple(input_shape[:-1] + (self.num_tokens,))\n \n     def get_config(self):\n         config = {\n\n@@ -18,6 +18,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertEqual(expected_output_shape, int_data.shape)\n         self.assertAllClose(int_data, expected_output)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_array.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_batched_count_output(self):\n         input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n         expected_output = np.array([[0, 2, 1, 1, 0, 0], [2, 1, 0, 1, 0, 0]])\n@@ -30,6 +37,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertEqual(expected_output_shape, int_data.shape)\n         self.assertAllClose(int_data, expected_output)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_array.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_multi_hot(self):\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array([1, 1, 1, 1, 0, 0])\n@@ -44,6 +58,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_data.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_batched_multi_hot(self):\n         input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n         expected_output = np.array([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0]])\n@@ -58,6 +79,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_data.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_one_hot(self):\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array(\n@@ -79,6 +107,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_data.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_batched_one_hot(self):\n         input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n         expected_output = np.array(\n@@ -108,6 +143,13 @@ class CategoryEncodingTest(testing.TestCase):\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n \n+        # Test symbolic call.\n+        output = layer(\n+            layers.Input(batch_shape=input_data.shape, dtype=\"int32\")\n+        )\n+        self.assertEqual(expected_output_shape, output.shape)\n+        self.assertEqual(\"float32\", output.dtype)\n+\n     def test_tf_data_compatibility(self):\n         layer = layers.CategoryEncoding(\n             num_tokens=4, output_mode=\"one_hot\", dtype=\"int32\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2db9669e0262e3402dfa498687f567d942bb2a86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 261 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 261 | Churn Cumulative: 261 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,261 @@\n+\"\"\"\n+Title: Supervised Contrastive Learning\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2020/11/30\n+Last modified: 2020/11/30\n+Description: Using supervised contrastive learning for image classification.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n+(Prannay Khosla et al.) is a training methodology that outperforms\n+supervised training with crossentropy on classification tasks.\n+\n+Essentially, training an image classification model with Supervised Contrastive\n+Learning is performed in two phases:\n+\n+1. Training an encoder to learn to produce vector representations of input images such\n+that representations of images in the same class will be more similar compared to\n+representations of images in different classes.\n+2. Training a classifier on top of the frozen encoder.\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras\n+from keras import ops\n+from keras import layers\n+from keras.applications.resnet_v2 import ResNet50V2\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 10\n+input_shape = (32, 32, 3)\n+\n+# Load the train and test data splits\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+\n+# Display shapes of train and test datasets\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\n+\"\"\"\n+## Using image data augmentation\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Normalization(),\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomRotation(0.02),\n+    ]\n+)\n+\n+# Setting the state of the normalization layer.\n+data_augmentation.layers[0].adapt(x_train)\n+\n+\"\"\"\n+## Build the encoder model\n+\n+The encoder model takes the image as input and turns it into a 2048-dimensional\n+feature vector.\n+\"\"\"\n+\n+\n+def create_encoder():\n+    resnet = ResNet50V2(\n+        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n+    )\n+\n+    inputs = keras.Input(shape=input_shape)\n+    augmented = data_augmentation(inputs)\n+    outputs = resnet(augmented)\n+    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n+    return model\n+\n+\n+encoder = create_encoder()\n+encoder.summary()\n+\n+learning_rate = 0.001\n+batch_size = 265\n+hidden_units = 512\n+projection_units = 128\n+num_epochs = 50\n+dropout_rate = 0.5\n+temperature = 0.05\n+\n+\"\"\"\n+## Build the classification model\n+\n+The classification model adds a fully-connected layer on top of the encoder,\n+plus a softmax layer with the target classes.\n+\"\"\"\n+\n+\n+def create_classifier(encoder, trainable=True):\n+    for layer in encoder.layers:\n+        layer.trainable = trainable\n+\n+    inputs = keras.Input(shape=input_shape)\n+    features = encoder(inputs)\n+    features = layers.Dropout(dropout_rate)(features)\n+    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n+    features = layers.Dropout(dropout_rate)(features)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n+\n+    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n+    model.compile(\n+        optimizer=keras.optimizers.Adam(learning_rate),\n+        loss=keras.losses.SparseCategoricalCrossentropy(),\n+        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+    )\n+    return model\n+\n+\n+\"\"\"\n+## Define npairs loss function\n+\"\"\"\n+\n+def npairs_loss(y_true, y_pred):\n+    \"\"\"Computes the npairs loss between `y_true` and `y_pred`.\n+\n+    Npairs loss expects paired data where a pair is composed of samples from\n+    the same labels and each pairs in the minibatch have different labels.\n+    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n+    as logits and the remapped multi-class labels, `y_true`, as labels.\n+\n+\n+    See:\n+    http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n+\n+    Args:\n+      y_true: Ground truth values, of shape `[batch_size]` of multi-class\n+        labels.\n+      y_pred: Predicted values of shape `[batch_size, batch_size]` of\n+        similarity matrix between embedding matrices.\n+\n+    Returns:\n+      npairs_loss: float scalar.\n+    \"\"\"\n+    y_pred = ops.cast(y_pred, \"float32\")\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+    y_true = ops.cast(ops.equal(y_true, ops.transpose(y_true)), y_pred.dtype)\n+    y_true /= ops.sum(y_true, 1, keepdims=True)\n+    loss = ops.categorical_crossentropy(y_true, y_pred, from_logits=True)\n+\n+    return ops.mean(loss)\n+\n+\n+\"\"\"\n+## Experiment 1: Train the baseline classification model\n+\n+In this experiment, a baseline classifier is trained as usual, i.e., the\n+encoder and the classifier parts are trained together as a single model\n+to minimize the crossentropy loss.\n+\"\"\"\n+\n+encoder = create_encoder()\n+classifier = create_classifier(encoder)\n+classifier.summary()\n+\n+history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n+\n+accuracy = classifier.evaluate(x_test, y_test)[1]\n+print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+\n+\n+\"\"\"\n+## Experiment 2: Use supervised contrastive learning\n+\n+In this experiment, the model is trained in two phases. In the first phase,\n+the encoder is pretrained to optimize the supervised contrastive loss,\n+described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n+\n+In the second phase, the classifier is trained using the trained encoder with\n+its weights freezed; only the weights of fully-connected layers with the\n+softmax are optimized.\n+\n+### 1. Supervised contrastive learning loss function\n+\"\"\"\n+\n+\n+class SupervisedContrastiveLoss(keras.losses.Loss):\n+    def __init__(self, temperature=1, name=None):\n+        super().__init__(name=name)\n+        self.temperature = temperature\n+\n+    def __call__(self, labels, feature_vectors, sample_weight=None):\n+        # Normalize feature vectors\n+        feature_vectors_normalized = keras.utils.normalize(feature_vectors, axis=1, order=2)\n+        # Compute logits\n+        logits = ops.divide(\n+            ops.matmul(\n+                feature_vectors_normalized, ops.transpose(feature_vectors_normalized)\n+            ),\n+            self.temperature,\n+        )\n+        return npairs_loss(ops.squeeze(labels), logits)\n+\n+\n+def add_projection_head(encoder):\n+    inputs = keras.Input(shape=input_shape)\n+    features = encoder(inputs)\n+    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n+    model = keras.Model(\n+        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n+    )\n+    return model\n+\n+\n+\"\"\"\n+### 2. Pretrain the encoder\n+\"\"\"\n+\n+encoder = create_encoder()\n+\n+encoder_with_projection_head = add_projection_head(encoder)\n+encoder_with_projection_head.compile(\n+    optimizer=keras.optimizers.Adam(learning_rate),\n+    loss=SupervisedContrastiveLoss(temperature),\n+)\n+\n+encoder_with_projection_head.summary()\n+\n+history = encoder_with_projection_head.fit(\n+    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n+)\n+\n+\"\"\"\n+### 3. Train the classifier with the frozen encoder\n+\"\"\"\n+\n+classifier = create_classifier(encoder, trainable=False)\n+\n+history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n+\n+accuracy = classifier.evaluate(x_test, y_test)[1]\n+print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+\n+\"\"\"\n+We get to an improved test accuracy.\n+\"\"\"\n+\n+\"\"\"\n+## Conclusion\n+\n+As shown in the experiments, using the supervised contrastive learning technique\n+outperformed the conventional technique in terms of the test accuracy. Note that\n+the same training budget (i.e., number of epochs) was given to each technique.\n+Supervised contrastive learning pays off when the encoder involves a complex\n+architecture, like ResNet, and multi-class problems with many labels.\n+In addition, large batch sizes and multi-layer projection heads\n+improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n+paper for more details.\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d3a966b57405e15cc19426e77559d75632c27f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ from keras import ops\n from keras.api_export import keras_export\n from keras.backend import standardize_dtype\n from keras.initializers.initializer import Initializer\n+from keras.saving import serialization_lib\n \n \n @keras_export([\"keras.initializers.Constant\", \"keras.initializers.constant\"])\n@@ -36,7 +37,12 @@ class Constant(Initializer):\n         )\n \n     def get_config(self):\n-        return {\"value\": self.value}\n+        return {\"value\": serialization_lib.serialize_keras_object(self.value)}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        value = serialization_lib.deserialize_keras_object(config[\"value\"])\n+        return cls(value)\n \n \n @keras_export([\"keras.initializers.Zeros\", \"keras.initializers.zeros\"])\n\n@@ -42,6 +42,20 @@ class ConstantInitializersTest(testing.TestCase):\n \n         self.run_class_serialization_test(initializer)\n \n+    def test_constant_initializer_array_value(self):\n+        shape = (3, 3)\n+        constant_value = np.random.random((3, 3))\n+\n+        initializer = initializers.Constant(value=constant_value)\n+        values = initializer(shape=shape)\n+        self.assertEqual(values.shape, shape)\n+        np_values = backend.convert_to_numpy(values)\n+        self.assertAllClose(\n+            np_values, np.full(shape=shape, fill_value=constant_value)\n+        )\n+\n+        self.run_class_serialization_test(initializer)\n+\n     def test_identity_initializer(self):\n         shape = (3, 3)\n         gain = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f82a4d6ecbb640d4b2df34bfaff6884438ec15e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 63 | Churn Cumulative: 1493 | Contributors (this commit): 3 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,6 +111,16 @@ def initialize(job_addresses, num_processes, process_id):\n     )\n \n \n+def num_processes():\n+    \"\"\"Return the number of processes for the current distribution setting.\"\"\"\n+    return jax.process_count()\n+\n+\n+def process_id():\n+    \"\"\"Return the current process ID for the distribution setting.\"\"\"\n+    return jax.process_index()\n+\n+\n def _to_jax_device(device_id):\n     device_type, index = device_id.split(\":\")\n     index = int(index)\n\n@@ -81,6 +81,10 @@ class JaxDistributionLibTest(testing.TestCase):\n         result = distribution_lib.distribute_tensor(inputs, target_layout)\n         self.assertTrue(result.sharding.is_equivalent_to(target_layout, ndim=2))\n \n+    def test_processes(self):\n+        self.assertEqual(backend_dlib.process_id(), 0)\n+        self.assertEqual(backend_dlib.num_processes(), 1)\n+\n     def test_to_jax_mesh(self):\n         devices = [f\"cpu:{i}\" for i in range(8)]\n         shape = (4, 2)\n\n@@ -14,6 +14,9 @@ import warnings\n \n import numpy as np\n \n+# TF is used for dataset sharding.\n+import tensorflow as tf\n+\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend import distribution_lib\n@@ -325,6 +328,19 @@ class Distribution:\n     def device_mesh(self):\n         return self._device_mesh\n \n+    def distribute_dataset(self, dataset):\n+        \"\"\"Create a distributed dataset instance from the original user dataset.\n+\n+        Args:\n+            dataset: the original global dataset instance. Only\n+            `tf.data.Dataset` is supported at the moment.\n+\n+        Returns:\n+            a sharded `tf.data.Dataset` instance, which will produce data for\n+            the current local worker/process.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n \n @keras_export(\"keras.distribution.DataParallel\")\n class DataParallel(Distribution):\n@@ -359,6 +375,9 @@ class DataParallel(Distribution):\n             self._initialize_mesh_from_list_devices()\n \n         self._batch_dim_name = self.device_mesh.axis_names[0]\n+        self._num_process = distribution_lib.num_processes()\n+        self._process_id = distribution_lib.process_id()\n+        self._is_multi_process = self._num_process > 1\n \n     def _initialize_with_device_mesh(self, device_mesh):\n         if not isinstance(device_mesh, DeviceMesh):\n@@ -406,6 +425,18 @@ class DataParallel(Distribution):\n         # For data parallel training, the intermediate state is not changed.\n         return None\n \n+    def distribute_dataset(self, dataset):\n+        if not isinstance(dataset, tf.data.Dataset):\n+            raise ValueError(\n+                \"Only `tf.data.Dataset` is supported for \"\n+                f\"sharding, got {type(dataset)}\"\n+            )\n+        if self._is_multi_process:\n+            return dataset.shard(\n+                num_shards=self._num_process, index=self._process_id\n+            ).prefetch(tf.data.AUTOTUNE)\n+        return dataset\n+\n \n @keras_export(\"keras.distribution.ModelParallel\")\n class ModelParallel(Distribution):\n\n@@ -5,6 +5,7 @@ from unittest import mock\n \n import numpy as np\n import pytest\n+import tensorflow as tf\n \n from keras import backend\n from keras import testing\n@@ -162,6 +163,10 @@ class DistributionTest(testing.TestCase):\n         self.assertIsNone(distribution_lib.distribution())\n \n \n+@pytest.mark.skipif(\n+    backend.backend() != \"jax\",\n+    reason=\"Only JAX has the proper backend distribution lib\",\n+)\n class DataParallelDistributionTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n@@ -183,6 +188,10 @@ class DataParallelDistributionTest(testing.TestCase):\n         self.assertEqual(device_mesh.axis_names, [\"data\"])\n         self.assertEqual(distribution._batch_dim_name, \"data\")\n \n+        self.assertFalse(distribution._is_multi_process)\n+        self.assertEqual(distribution._process_id, 0)\n+        self.assertEqual(distribution._num_process, 1)\n+\n     def test_create_with_devices(self):\n         distribution = distribution_lib.DataParallel(devices=self.devices)\n         device_mesh = distribution.device_mesh\n@@ -233,6 +242,15 @@ class DataParallelDistributionTest(testing.TestCase):\n         tensor_layout = distribution.get_tensor_layout(path)\n         self.assertIsNone(tensor_layout)\n \n+    def test_distribute_dataset(self):\n+        # We can only verify the single worker/process case in OSS for now.\n+        dataset = tf.data.Dataset.range(8)\n+        distribution = distribution_lib.DataParallel(\n+            device_mesh=self.device_mesh\n+        )\n+        distributed_dataset = distribution.distribute_dataset(dataset)\n+        self.assertIs(dataset, distributed_dataset)\n+\n \n class ModelParallelDistributionTest(testing.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8de296183387d23edb5841f001fa1d0117ac0dd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 17 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -441,6 +441,7 @@ def rnn(\n     return last_output, outputs, new_states\n \n \n+@tf.function\n def gru(\n     inputs,\n     initial_state,\n@@ -801,6 +802,7 @@ def cudnn_ok(\n     return args_supported and _is_gpu_available()\n \n \n+@tf.function\n def lstm(\n     inputs,\n     initial_state_h,\n@@ -853,6 +855,7 @@ def lstm(\n         raise NotImplementedError\n \n \n+@tf.function(autograph=False)\n def _cudnn_lstm(\n     inputs,\n     initial_state_h,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a120c000459c237669f9696fa90d12382a35b8b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ from matplotlib import pyplot as plt\n ## Load the data\n \n We will use the [Numenta Anomaly Benchmark(NAB)](\n-https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artifical\n+https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artificial\n timeseries data containing labeled anomalous periods of behavior. Data are\n ordered, timestamped, single-valued metrics.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d026dfdbb465bd092c6ae9551e3c75c73c674d0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 1386 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ python3 pip_build.py --install\n ```\n \"\"\"\n import argparse\n+import datetime\n import glob\n import os\n import pathlib\n@@ -137,12 +138,27 @@ def create_legacy_directory():\n     shutil.rmtree(os.path.join(package, \"_legacy\"))\n \n \n-def export_version_string(__version__):\n+def export_version_string(version, is_nightly=False, rc_index=None):\n+    \"\"\"Export Version and Package Name.\"\"\"\n+    if is_nightly:\n+        date = datetime.datetime.now()\n+        version += f\".dev{date.strftime('%Y%m%d%H')}\"\n+        # Replaces `name=\"keras\"` string in `setup.py` with `keras-nightly`\n+        with open(\"setup.py\") as f:\n+            setup_contents = f.read()\n+        with open(\"setup.py\", \"w\") as f:\n+            setup_contents = setup_contents.replace(\n+                'name=\"keras\"', 'name=\"keras-nightly\"'\n+            )\n+            f.write(setup_contents)\n+    elif rc_index is not None:\n+        version += \"rc\" + str(rc_index)\n+\n     # Make sure to export the __version__ string\n     with open(os.path.join(package, \"__init__.py\")) as f:\n         init_contents = f.read()\n     with open(os.path.join(package, \"__init__.py\"), \"w\") as f:\n-        f.write(init_contents + \"\\n\\n\" + f'__version__ = \"{__version__}\"\\n')\n+        f.write(init_contents + \"\\n\\n\" + f'__version__ = \"{version}\"\\n')\n \n \n def build_and_save_output(root_path, __version__):\n@@ -167,7 +183,7 @@ def build_and_save_output(root_path, __version__):\n     return whl_path\n \n \n-def build(root_path):\n+def build(root_path, is_nightly=False, rc_index=None):\n     if os.path.exists(build_directory):\n         raise ValueError(f\"Directory already exists: {build_directory}\")\n \n@@ -177,7 +193,7 @@ def build(root_path):\n         create_legacy_directory()\n         from keras.src.version import __version__  # noqa: E402\n \n-        export_version_string(__version__)\n+        export_version_string(__version__, is_nightly, rc_index)\n         return build_and_save_output(root_path, __version__)\n     finally:\n         # Clean up: remove the build directory (no longer needed)\n@@ -196,8 +212,18 @@ if __name__ == \"__main__\":\n         action=\"store_true\",\n         help=\"Whether to install the generated wheel file.\",\n     )\n+    parser.add_argument(\n+        \"--nightly\",\n+        action=\"store_true\",\n+        help=\"Whether to generate nightly wheel file.\",\n+    )\n+    parser.add_argument(\n+        \"--rc\",\n+        type=int,\n+        help=\"Specify `[0-9] when generating RC wheels.\",\n+    )\n     args = parser.parse_args()\n     root_path = pathlib.Path(__file__).parent.resolve()\n-    whl_path = build(root_path)\n+    whl_path = build(root_path, args.nightly, args.rc)\n     if whl_path and args.install:\n         install_whl(whl_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c0d99773ed98907ea790b5a8f9dd95fe85d5b0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 32 | Files Changed: 12 | Hunks: 28 | Methods Changed: 19 | Complexity Δ (Sum/Max): 7/2 | Churn Δ: 156 | Churn Cumulative: 1454 | Contributors (this commit): 13 | Commits (past 90d): 38 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,3 +77,9 @@ def clear_session():\n             from tensorflow.python.eager import context\n \n             context.context().clear_kernel_cache()\n+    elif backend.backend() == \"torch\":\n+        import torch._dynamo as dynamo\n+\n+        # reset's torchdynamo's cache so that  cached guards, compiled fn, etc\n+        # do not persist between clear_session() calls\n+        dynamo.reset()\n\n@@ -149,6 +149,7 @@ def convert_to_tensor(x, dtype=None, sparse=False):\n         return torch.as_tensor(x, dtype=torch.int32, device=get_device())\n     if isinstance(x, float):\n         return torch.as_tensor(x, dtype=torch.float32, device=get_device())\n+\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n@@ -180,7 +181,15 @@ def convert_to_numpy(x):\n \n \n def is_tensor(x):\n-    return torch.is_tensor(x)\n+    # Using the built-in `isinstance` is recommended by pytorch\n+    # over using torch.is_tensor\n+    # see: https://pytorch.org/docs/stable/generated/torch.is_tensor.html\n+    #\n+    # Also, `torch.is_tensor()` causes issues with dynamo caching when\n+    # a torch.Tensor and numpy.ndarray of the same size, shape, and dtype\n+    # is passed, if called on a Tensor first the second call with ndarray\n+    # will return `True` and vice-versa.\n+    return isinstance(x, torch.Tensor)\n \n \n def shape(x):\n\n@@ -63,6 +63,9 @@ def mean(x, axis=None, keepdims=False):\n     if axis == () or axis == []:\n         # Torch handles the empty axis case differently from numpy.\n         return x\n+    elif isinstance(axis, int):\n+        axis = (axis,)  # see [NB] below\n+\n     ori_dtype = standardize_dtype(x.dtype)\n     # torch.mean only supports floating point inputs\n     compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n@@ -70,8 +73,22 @@ def mean(x, axis=None, keepdims=False):\n         result_dtype = compute_dtype\n     else:\n         result_dtype = ori_dtype\n+\n+    # [NB] the python torch op torch.mean() is generated into\n+    # `torch._C._VariableFunctions.pyi`, and the method\n+    # signature is overloaded.\n+    # Dynamo won't actually find the correct signature of\n+    # `torch.mean()` if arguments are passed via kwargs\n+    # So we have to pass the arguments via positional args\n+    # EXCEPT for those that are forced as kwargs via the `*`\n+    # delimiter in the overloaded method signatures.\n+    # Additionally, we have to create a singleton-tuple\n+    # when `axis` is an int to match the existing fn signature\n     result = torch.mean(\n-        x, axis=axis, keepdims=keepdims, dtype=to_torch_dtype(compute_dtype)\n+        x,\n+        axis,\n+        keepdims,\n+        dtype=to_torch_dtype(compute_dtype),\n     )\n     return cast(result, result_dtype)\n \n\n@@ -10,6 +10,9 @@ from keras.random.seed_generator import draw_seed\n from keras.random.seed_generator import make_default_seed\n \n \n+# torch.Generator not supported with dynamo\n+# see: https://github.com/pytorch/pytorch/issues/88576\n+@torch.compiler.disable()\n def torch_seed_generator(seed):\n     first_seed, second_seed = draw_seed(seed)\n     device = get_device()\n\n@@ -102,12 +102,7 @@ class TorchTrainer(base_trainer.Trainer):\n             return self.train_step(data)\n \n         if self.jit_compile:\n-            raise ValueError(\n-                \"`jit_compile` is not yet enabled for the PyTorch backend.\"\n-            )\n-            # Temporarily disabled torch compile due to failed unit tests.\n-            # TODO: Uncomment the following line when unit tests passes.\n-            # self.train_function = torch.compile(one_step_on_data)\n+            self.train_function = torch.compile(one_step_on_data)\n         else:\n             self.train_function = one_step_on_data\n \n@@ -127,6 +122,9 @@ class TorchTrainer(base_trainer.Trainer):\n             with torch.no_grad():\n                 return self.test_step(data)\n \n+        if self.jit_compile:\n+            self.test_function = torch.compile(one_step_on_data)\n+        else:\n             self.test_function = one_step_on_data\n \n     def make_predict_function(self, force=False):\n@@ -145,6 +143,9 @@ class TorchTrainer(base_trainer.Trainer):\n             with torch.no_grad():\n                 return self.predict_step(data)\n \n+        if self.jit_compile:\n+            self.predict_function = torch.compile(one_step_on_data)\n+        else:\n             self.predict_function = one_step_on_data\n \n     def _symbolic_build(self, data_batch):\n\n@@ -57,7 +57,12 @@ class Flatten(Layer):\n         non_batch_dims = input_shape[1:]\n         if len(non_batch_dims) == 0:\n             flattened_dim = 1\n-        elif None in non_batch_dims:\n+        elif any(d is None for d in non_batch_dims):\n+            # NB: we cannot use the shorter `None in non_batch_dims` here b/c\n+            # torchdynamo errors when calling `__contains__` op with\n+            # a constant (in this case `None`) operand since it assumes\n+            # that the elements in the collection are also `ConstantVariable`s\n+            # but tensor shapes can be `SymNodeVariable`s (e.g. `SymInt`)\n             flattened_dim = None\n         else:\n             flattened_dim = math.prod(non_batch_dims)\n\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n@@ -149,10 +147,6 @@ class UpSampling2D(Layer):\n         else:\n             raise ValueError(f\"Invalid `data_format` argument: {data_format}\")\n \n-        new_shape = x.shape[rows : cols + 1]\n-        new_shape *= np.array([height_factor, width_factor])\n-        new_shape = new_shape.tolist()\n-\n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 2, 3, 1])\n         # https://github.com/keras-team/keras/issues/294\n@@ -161,6 +155,17 @@ class UpSampling2D(Layer):\n             x = ops.repeat(x, height_factor, axis=1)\n             x = ops.repeat(x, width_factor, axis=2)\n         else:\n+            # multiply the height and width factor on each dim\n+            # by hand (versus using element-wise multiplication\n+            # by np.array([height_factor, width_factor]) then\n+            # list-ifying the tensor by calling `.tolist()`)\n+            # since when running under torchdynamo, `new_shape`\n+            # will be traced as a symbolic variable (specifically\n+            # a `FakeTensor`) which does not have a `tolist()` method.\n+            new_shape = (\n+                x.shape[rows] * height_factor,\n+                x.shape[cols] * width_factor,\n+            )\n             x = ops.image.resize(x, new_shape, interpolation=interpolation)\n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 3, 1, 2])\n\n@@ -11,6 +11,7 @@ from keras import ops\n from keras import utils\n from keras.backend.common import is_float_dtype\n from keras.backend.common import standardize_dtype\n+from keras.backend.common.global_state import clear_session\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n@@ -24,6 +25,12 @@ class TestCase(unittest.TestCase):\n         if traceback_utils.is_traceback_filtering_enabled():\n             traceback_utils.disable_traceback_filtering()\n \n+    def setUp(self):\n+        # clear global state so that test cases are independent\n+        # required for the jit enabled torch tests since dynamo has\n+        # a global cache for guards, compiled fn, etc\n+        clear_session()\n+\n     def get_temp_dir(self):\n         temp_dir = tempfile.mkdtemp()\n         self.addCleanup(lambda: shutil.rmtree(temp_dir))\n@@ -329,7 +336,19 @@ class TestCase(unittest.TestCase):\n                 output_data = tree.map_structure(\n                     lambda x: backend.convert_to_numpy(x), output_data\n                 )\n-                model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=True)\n+                # test the \"default\" path for each backend by setting\n+                # jit_compile=\"auto.\n+                # for tensorflow and jax backends auto is jitted\n+                # for torch backend auto is eager\n+                #\n+                # NB: for torch, jit_compile=True turns on torchdynamo\n+                #  which may not always succeed in tracing depending\n+                #  on the model. Run your program with these env vars\n+                #  to get debug traces of dynamo:\n+                #    TORCH_LOGS=\"+dynamo\"\n+                #    TORCHDYNAMO_VERBOSE=1\n+                #    TORCHDYNAMO_REPORT_GUARD_FAILURES=1\n+                model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=\"auto\")\n                 model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n\n@@ -122,7 +122,7 @@ class EpochIterator:\n             if buffer:\n                 yield step - len(buffer) + 1, buffer\n             if not self._num_batches:\n-                # Infer the number of batches returned by the data_adater.\n+                # Infer the number of batches returned by the data_adapter.\n                 # Assumed static.\n                 self._num_batches = step + 1\n         self.data_adapter.on_epoch_end()\n\n@@ -104,7 +104,7 @@ class Trainer:\n                  and to set it to `True` when debugging.\n             steps_per_execution: Int. The number of batches to run\n                 during each a single compiled function call. Running multiple\n-                batches inside a single a single compiled function call can\n+                batches inside a single compiled function call can\n                 greatly improve performance on TPUs or small models with a large\n                 Python overhead. At most, one full epoch will be run each\n                 execution. If a number larger than the size of the epoch is\n@@ -115,9 +115,12 @@ class Trainer:\n                 each compiled function execution).\n                 Not supported with the PyTorch backend.\n             jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n-                compiling a model. Not supported with the PyTorch backend.\n-                If `\"auto\"`, XLA compilation will be enabled if the\n-                the model supports it, and disabled otherwise.\n+                compiling a model. For `jax` and `tensorflow` backends,\n+                `jit_compile=\"auto\"` enables XLA compilation if the model\n+                supports it, and disabled otherwise.\n+                For `torch` backend, `\"auto\"` will default to eager\n+                execution and `jit_compile=True` will run with `torch.compile`\n+                with the `\"inductor\"` backend.\n             auto_scale_loss: Bool. If `True` and the model dtype policy is\n                 `\"mixed_float16\"`, the passed optimizer will be automatically\n                 wrapped in a `LossScaleOptimizer`, which will dynamically\n@@ -162,12 +165,7 @@ class Trainer:\n                 \"cannot also be True. Disabling `jit_compile`.\",\n                 stacklevel=2,\n             )\n-        if jit_compile and backend.backend() == \"torch\":\n-            warnings.warn(\n-                \"`jit_compile` is not yet enabled for the PyTorch backend. \"\n-                \"Proceeding with `jit_compile=False`.\"\n-            )\n-            jit_compile = False\n+\n         self.jit_compile = jit_compile\n         self.run_eagerly = run_eagerly\n         self.stop_training = False\n@@ -194,7 +192,10 @@ class Trainer:\n     def jit_compile(self):\n         if self._jit_compile is None:\n             # Value was never set. Resolve it now.\n-            jit_compile = model_supports_jit(self)\n+            # torch always runs in eager unless jit_compile is explicitly set\n+            jit_compile = (\n+                model_supports_jit(self) and backend.backend() != \"torch\"\n+            )\n             self._jit_compile = jit_compile\n         return self._jit_compile\n \n@@ -866,11 +867,11 @@ class Trainer:\n \n \n def resolve_auto_jit_compile(model):\n-    if model_supports_jit(model):\n     if backend.backend() == \"torch\":\n-            # Torch defaults to eager mode\n-            # until torch compile is reliable\n+        # jit_compile = \"auto\" with the pytorch backend defaults to eager\n         return False\n+\n+    if model_supports_jit(model):\n         return True\n     return False\n \n\n@@ -148,6 +148,18 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(len(model_weighted.metrics), 3)\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"torch\",\n+        reason=\"torch backend runs in eager mode for jit_compile='auto'\",\n+    )\n+    def test_compile_eager_vs_jit_torch(self):\n+        model = ExampleModel(units=3)\n+        model.compile(jit_compile=\"auto\")\n+        # torch trainer en/disables torch.compile only based on the value of\n+        # model.jit_compile (not model.run_eagerly)\n+        self.assertFalse(model.run_eagerly)\n+        self.assertFalse(model.jit_compile)\n+\n     @parameterized.named_parameters(\n         [\n             (\"eager\", True, False, False),\n@@ -292,6 +304,14 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         outputs = model.predict(x, batch_size=batch_size)\n         self.assertAllClose(outputs, 4 * np.ones((100, 3)))\n \n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False),\n+            (\"graph_fn\", False, False),\n+            (\"jit\", False, True),\n+        ]\n+    )\n+    def test_predict_flow_struct(self, run_eagerly, jit_compile):\n         # Test with input/output structs\n         model = StructModel(units=3)\n         model.run_eagerly = run_eagerly\n\n@@ -61,7 +61,13 @@ class NamingUtilsTest(test_case.TestCase):\n         name = \"unique_name\"\n         unique_name = naming.uniquify(name)\n         new_unique_name = naming.uniquify(unique_name)\n-        self.assertEqual(new_unique_name, unique_name)\n+\n+        # first time `name` is uniquified so returns same name\n+        self.assertEqual(name, unique_name)\n+\n+        # second time `name` is uniquified should be different\n+        # from the first output\n+        self.assertNotEqual(new_unique_name, unique_name)\n \n     def test_to_snake_case_capital_after_any_character(self):\n         name = \"myVariableNameHere\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
