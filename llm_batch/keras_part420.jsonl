{"custom_id": "keras#82d8eeeb352ea0d2128b756d9efefb26d3d7e68a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 8288 | Contributors (this commit): 24 | Commits (past 90d): 9 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3378,6 +3378,8 @@ class BaseRandomLayer(Layer):\n     super().__init__(**kwargs)\n     self._random_generator = backend.RandomGenerator(\n         seed, force_generator=force_generator)\n+    # Eagerly init the generator to avoid any issue like b/206821407\n+    self._random_generator._maybe_init()\n \n   def _trackable_children(self, save_type='checkpoint', **kwargs):\n     if save_type == 'savedmodel':\n@@ -3386,8 +3388,10 @@ class BaseRandomLayer(Layer):\n       # that any input shape changes are applied before getting the config of\n       # the model.\n       children = self._trackable_saved_model_saver.trackable_children(cache)\n+      # This method exposes the self._random_generator to SavedModel only\n+      # (not layer.weights and checkpoint).\n+      children['_random_generator'] = self._random_generator\n     else:\n       children = {}\n     children.update(super()._trackable_children(save_type, **kwargs))\n-    children['_random_generator'] = self._random_generator\n     return children\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c3a204eb6ad6883e5817f9c2a4360dcd3800564", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 62 | Files Changed: 1 | Hunks: 36 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 115 | Churn Cumulative: 3235 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,6 @@ from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.preprocessing.image import smart_resize\n-from keras.utils import control_flow_util\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -422,11 +421,9 @@ class RandomFlip(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n     inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n \n-    def random_flipped_inputs():\n+    def random_flipped_inputs(inputs):\n       flipped_outputs = inputs\n       if self.horizontal:\n         seed = self._random_generator.make_seed_for_stateless_op()\n@@ -444,12 +441,13 @@ class RandomFlip(base_layer.BaseRandomLayer):\n         else:\n           flipped_outputs = tf.image.random_flip_up_down(\n               flipped_outputs, self._random_generator.make_legacy_seed())\n+      flipped_outputs.set_shape(inputs.shape)\n       return flipped_outputs\n \n-    output = control_flow_util.smart_cond(training, random_flipped_inputs,\n-                                          lambda: inputs)\n-    output.set_shape(inputs.shape)\n-    return output\n+    if training:\n+      return random_flipped_inputs(inputs)\n+    else:\n+      return inputs\n \n   def compute_output_shape(self, input_shape):\n     return input_shape\n@@ -569,19 +567,17 @@ class RandomTranslation(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-    original_shape = inputs.shape\n-    unbatched = inputs.shape.rank == 3\n+\n+    def random_translated_inputs(inputs):\n+      \"\"\"Translated inputs with random ops.\"\"\"\n       # The transform op only accepts rank 4 inputs, so if we have an unbatched\n       # image, we need to temporarily expand dims to a batch.\n+      original_shape = inputs.shape\n+      unbatched = inputs.shape.rank == 3\n       if unbatched:\n         inputs = tf.expand_dims(inputs, 0)\n \n-    def random_translated_inputs():\n-      \"\"\"Translated inputs with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n       batch_size = inputs_shape[0]\n       img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n@@ -601,20 +597,22 @@ class RandomTranslation(base_layer.BaseRandomLayer):\n       translations = tf.cast(\n           tf.concat([width_translate, height_translate], axis=1),\n           dtype=tf.float32)\n-      return transform(\n+      output = transform(\n           inputs,\n           get_translation_matrix(translations),\n           interpolation=self.interpolation,\n           fill_mode=self.fill_mode,\n           fill_value=self.fill_value)\n-\n-    output = control_flow_util.smart_cond(training, random_translated_inputs,\n-                                          lambda: inputs)\n       if unbatched:\n         output = tf.squeeze(output, 0)\n       output.set_shape(original_shape)\n       return output\n \n+    if training:\n+      return random_translated_inputs(inputs)\n+    else:\n+      return inputs\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n@@ -874,19 +872,16 @@ class RandomRotation(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n+\n+    def random_rotated_inputs(inputs):\n+      \"\"\"Rotated inputs with random ops.\"\"\"\n       original_shape = inputs.shape\n       unbatched = inputs.shape.rank == 3\n       # The transform op only accepts rank 4 inputs, so if we have an unbatched\n       # image, we need to temporarily expand dims to a batch.\n       if unbatched:\n         inputs = tf.expand_dims(inputs, 0)\n-\n-    def random_rotated_inputs():\n-      \"\"\"Rotated inputs with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n       batch_size = inputs_shape[0]\n       img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n@@ -895,20 +890,22 @@ class RandomRotation(base_layer.BaseRandomLayer):\n       max_angle = self.upper * 2. * np.pi\n       angles = self._random_generator.random_uniform(\n           shape=[batch_size], minval=min_angle, maxval=max_angle)\n-      return transform(\n+      output = transform(\n           inputs,\n           get_rotation_matrix(angles, img_hd, img_wd),\n           fill_mode=self.fill_mode,\n           fill_value=self.fill_value,\n           interpolation=self.interpolation)\n-\n-    output = control_flow_util.smart_cond(training, random_rotated_inputs,\n-                                          lambda: inputs)\n       if unbatched:\n         output = tf.squeeze(output, 0)\n       output.set_shape(original_shape)\n       return output\n \n+    if training:\n+      return random_rotated_inputs(inputs)\n+    else:\n+      return inputs\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n@@ -1030,19 +1027,16 @@ class RandomZoom(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n+\n+    def random_zoomed_inputs(inputs):\n+      \"\"\"Zoomed inputs with random ops.\"\"\"\n       original_shape = inputs.shape\n       unbatched = inputs.shape.rank == 3\n       # The transform op only accepts rank 4 inputs, so if we have an unbatched\n       # image, we need to temporarily expand dims to a batch.\n       if unbatched:\n         inputs = tf.expand_dims(inputs, 0)\n-\n-    def random_zoomed_inputs():\n-      \"\"\"Zoomed inputs with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n       batch_size = inputs_shape[0]\n       img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n@@ -1061,20 +1055,22 @@ class RandomZoom(base_layer.BaseRandomLayer):\n       zooms = tf.cast(\n           tf.concat([width_zoom, height_zoom], axis=1),\n           dtype=tf.float32)\n-      return transform(\n+      output = transform(\n           inputs,\n           get_zoom_matrix(zooms, img_hd, img_wd),\n           fill_mode=self.fill_mode,\n           fill_value=self.fill_value,\n           interpolation=self.interpolation)\n-\n-    output = control_flow_util.smart_cond(training, random_zoomed_inputs,\n-                                          lambda: inputs)\n       if unbatched:\n         output = tf.squeeze(output, 0)\n       output.set_shape(original_shape)\n       return output\n \n+    if training:\n+      return random_zoomed_inputs(inputs)\n+    else:\n+      return inputs\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n@@ -1185,25 +1181,24 @@ class RandomContrast(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-    def random_contrasted_inputs():\n+    def random_contrasted_inputs(inputs):\n       seed = self._random_generator.make_seed_for_stateless_op()\n       if seed is not None:\n-        return tf.image.stateless_random_contrast(\n+        output = tf.image.stateless_random_contrast(\n             inputs, 1. - self.lower, 1. + self.upper, seed=seed)\n       else:\n-        return tf.image.random_contrast(\n+        output = tf.image.random_contrast(\n             inputs, 1. - self.lower, 1. + self.upper,\n             seed=self._random_generator.make_legacy_seed())\n-\n-    output = control_flow_util.smart_cond(training, random_contrasted_inputs,\n-                                          lambda: inputs)\n       output.set_shape(inputs.shape)\n       return output\n \n+    if training:\n+      return random_contrasted_inputs(inputs)\n+    else:\n+      return inputs\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n@@ -1283,12 +1278,9 @@ class RandomHeight(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs)\n \n-    def random_height_inputs():\n+    def random_height_inputs(inputs):\n       \"\"\"Inputs height-adjusted with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n       img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n@@ -1308,9 +1300,10 @@ class RandomHeight(base_layer.BaseRandomLayer):\n       output.set_shape(output_shape)\n       return output\n \n-    return control_flow_util.smart_cond(\n-        training, random_height_inputs,\n-        lambda: tf.cast(inputs, self.compute_dtype))\n+    if training:\n+      return random_height_inputs(inputs)\n+    else:\n+      return inputs\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -1391,12 +1384,9 @@ class RandomWidth(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n-\n     inputs = utils.ensure_tensor(inputs)\n \n-    def random_width_inputs():\n+    def random_width_inputs(inputs):\n       \"\"\"Inputs width-adjusted with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n       img_hd = inputs_shape[H_AXIS]\n@@ -1416,9 +1406,10 @@ class RandomWidth(base_layer.BaseRandomLayer):\n       output.set_shape(output_shape)\n       return output\n \n-    return control_flow_util.smart_cond(\n-        training, random_width_inputs,\n-        lambda: tf.cast(inputs, self.compute_dtype))\n+    if training:\n+      return random_width_inputs(inputs)\n+    else:\n+      return inputs\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cff8cc93305d1c4a54385fb623fe895dafa0845c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 713 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -151,7 +151,8 @@ def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n   # Recursively convert.\n   if not tf.nest.is_nested(nested):\n     raise ValueError(\n-        f'Received non-atomic and non-sequence element: {nested}')\n+        f'Received non-atomic and non-sequence element: {nested} '\n+        f'of type {type(nested)}')\n   if tf.__internal__.nest.is_mapping(nested):\n     values = [nested[k] for k in sorted(nested.keys())]\n   elif tf.__internal__.nest.is_attrs(nested):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4ca1a9540299e01501a8388c100fae5906733d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 75 | Churn Cumulative: 16993 | Contributors (this commit): 122 | Commits (past 90d): 32 | Contributors (cumulative): 127 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1163,6 +1163,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             in the `x` and `y` data provided, before shuffling. This argument is\n             not supported when `x` is a dataset, generator or\n             `keras.utils.Sequence` instance.\n+            If both `validation_data` and `validation_split` are provided,\n+            `validation_data` will override `validation_split`.\n             `validation_split` is not yet supported with\n             `tf.distribute.experimental.ParameterServerStrategy`.\n         validation_data: Data on which to evaluate\n@@ -1305,7 +1307,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     verbose = _get_verbosity(verbose, self.distribute_strategy)\n \n-    if validation_split:\n+    if validation_split and validation_data is None:\n       # Create the validation data using the training data. Only supported for\n       # `Tensor` and `NumPy` input.\n       (x, y, sample_weight), validation_data = (\n\n@@ -18,37 +18,90 @@ import tensorflow.compat.v2 as tf\n \n import io\n import sys\n+from unittest import mock\n \n from absl.testing import parameterized\n import numpy as np\n \n import keras\n from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.engine import data_adapter\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.layers import core\n from keras.utils import io_utils\n \n \n+def _create_dataset(num_samples, batch_size):\n+  input_data = np.random.rand(num_samples, 1)\n+  expected_data = input_data * 3\n+  dataset = tf.data.Dataset.from_tensor_slices((input_data, expected_data))\n+  return dataset.shuffle(10 * batch_size).batch(batch_size)\n+\n+\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class ValidationDatasetAndValidationSplit(test_combinations.TestCase,\n+                                          parameterized.TestCase):\n+  \"\"\"Verifies when validation_data is provided validation_split is ignored.\n+\n+  The validation_split arg can't be passed in v1 mode because\n+  training_utils_v1.py:validate_dataset_input will raise a ValueError that\n+  validation_split is not supported when input x is a dataset or a dataset\n+  iterator.\n+  \"\"\"\n+\n+  @parameterized.named_parameters((\"with_default_falsey_validation_split\", 0.),\n+                                  (\"with_non_falsey_validation_split\", 0.1))\n+  def test_ignore_validation_split_when_validation_dataset_is_present(\n+      self, validation_split):\n+    # Create a model that learns y=Mx.\n+    layers = [core.Dense(1)]\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n+\n+    train_dataset = _create_dataset(num_samples=200, batch_size=10)\n+    eval_dataset = _create_dataset(num_samples=50, batch_size=25)\n+\n+    # Make sure model.fit doesn't raise an error because of the mocking alone.\n+    mock_train_validation_split_return = ((train_dataset, None, None),\n+                                          eval_dataset)\n+\n+    with mock.patch.object(\n+        data_adapter,\n+        \"train_validation_split\",\n+        return_value=mock_train_validation_split_return\n+    ) as mock_train_validation_split:\n+      model.fit(\n+          x=train_dataset,\n+          validation_split=validation_split,\n+          validation_data=eval_dataset,\n+          epochs=2)\n+      mock_train_validation_split.assert_not_called()\n+\n+      history = model.fit(\n+          x=train_dataset, validation_data=eval_dataset, epochs=2)\n+      evaluation = model.evaluate(x=eval_dataset)\n+\n+      # See test_validation_dataset_with_no_step_arg for details.\n+      self.assertAlmostEqual(\n+          history.history[\"val_mean_absolute_error\"][-1],\n+          evaluation[-1],\n+          places=5)\n+\n+\n @test_combinations.run_with_all_model_types\n @test_combinations.run_all_keras_modes\n class ValidationDatasetNoLimitTest(test_combinations.TestCase):\n \n-  def create_dataset(self, num_samples, batch_size):\n-    input_data = np.random.rand(num_samples, 1)\n-    expected_data = input_data * 3\n-    dataset = tf.data.Dataset.from_tensor_slices((input_data,\n-                                                      expected_data))\n-    return dataset.shuffle(10 * batch_size).batch(batch_size)\n-\n   def test_validation_dataset_with_no_step_arg(self):\n     # Create a model that learns y=Mx.\n     layers = [core.Dense(1)]\n     model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n     model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n \n-    train_dataset = self.create_dataset(num_samples=200, batch_size=10)\n-    eval_dataset = self.create_dataset(num_samples=50, batch_size=25)\n+    train_dataset = _create_dataset(num_samples=200, batch_size=10)\n+    eval_dataset = _create_dataset(num_samples=50, batch_size=25)\n \n     history = model.fit(x=train_dataset, validation_data=eval_dataset, epochs=2)\n     evaluation = model.evaluate(x=eval_dataset)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86209dc46a58d6fe3ec25ca69947a26d227c3e38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 355 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -138,7 +138,7 @@ def image_dataset_from_directory(directory,\n       `(batch_size,)`.\n     - if `label_mode` is `binary`, the labels are a `float32` tensor of\n       1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorial`, the labels are a `float32` tensor\n+    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n       of shape `(batch_size, num_classes)`, representing a one-hot\n       encoding of the class index.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58c6cd769dd53e75a6954790f49463868c24890d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 13 | Churn Cumulative: 8301 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2688,19 +2688,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         self.set_weights(self._initial_weights)\n       self._initial_weights = None\n \n-  def _symbolic_call(self, inputs):\n-    input_shapes = tf.nest.map_structure(lambda x: x.shape, inputs)\n-    output_shapes = self.compute_output_shape(input_shapes)\n-    # Convert to TensorShape so that nest.map_structure will not map into\n-    # individual dim of the shape.\n-    output_shapes = tf_utils.convert_shapes(output_shapes, to_tuples=False)\n-\n-    def _make_placeholder_like(shape):\n-      ph = backend.placeholder(shape=shape, dtype=self.dtype)\n-      ph._keras_mask = None\n-      return ph\n-    return tf.nest.map_structure(_make_placeholder_like, output_shapes)\n-\n   def _get_trainable_state(self):\n     \"\"\"Get the `trainable` state of each sublayer.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0fa8ae87a6d63c54c48ed38ef0e49de07778b07d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -12,3 +12,4 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n+\"\"\"This is under development, and subject to interface/implementation changes.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef293fdb46994b0bca2571eb3d8d66aad2cfc4ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 248 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -111,7 +111,7 @@ def text_dataset_from_directory(directory,\n       `(batch_size,)`.\n     - if `label_mode` is `binary`, the labels are a `float32` tensor of\n       1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorial`, the labels are a `float32` tensor\n+    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n       of shape `(batch_size, num_classes)`, representing a one-hot\n       encoding of the class index.\n   \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4967ce6f8f0199969ba40d74eba1bddc204ca100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 16694 | Contributors (this commit): 122 | Commits (past 90d): 28 | Contributors (cumulative): 122 | DMM Complexity: None\n\nDIFF:\n@@ -503,7 +503,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     Args:\n         optimizer: String (name of optimizer) or optimizer instance. See\n           `tf.keras.optimizers`.\n-        loss: Loss function. Maybe be a string (name of loss function), or\n+        loss: Loss function. May be a string (name of loss function), or\n           a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n           function is any callable with the signature `loss = fn(y_true,\n           y_pred)`, where `y_true` are the ground truth values, and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b879229cf3ac36d961989c2e4e3761661c5467c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1882 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1167,7 +1167,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self._eval_if_tensor(opt.inner_optimizer.momentum), 0.5)\n     self.assertEqual(self.evaluate(opt.loss_scale), 2.)\n     self.assertEqual(opt.dynamic_growth_steps, 3.)\n-    self.assertTrue(opt.dynamic, 4.)\n+    self.assertEqual(opt.dynamic, 4.)\n     if lso_type in ('v1', 'v2'):\n       # Deserializing a LossScaleOptimizerV1 results in a V2 LossScaleOptimizer.\n       self.assertEqual(type(opt), loss_scale_optimizer.LossScaleOptimizer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec6d304608481a77d8eedeed3c732fd96764791a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 5706 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1143,7 +1143,9 @@ class RandomContrast(base_layer.BaseRandomLayer):\n   `(x - mean) * contrast_factor + mean`.\n \n   Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-  of interger or floating point dtype. By default, the layer will output floats.\n+  in integer or floating point dtype. By default, the layer will output floats.\n+  The output value will be clipped to the range `[0, 255]`, the valid\n+  range of RGB colors.\n \n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -1160,7 +1162,9 @@ class RandomContrast(base_layer.BaseRandomLayer):\n     factor: a positive float represented as fraction of value, or a tuple of\n       size 2 representing lower and upper bound. When represented as a single\n       float, lower = upper. The contrast factor will be randomly picked between\n-      `[1.0 - lower, 1.0 + upper]`.\n+      `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel, the output\n+      will be `(x - mean) * factor + mean` where `mean` is the mean value of the\n+      channel.\n     seed: Integer. Used to create a random seed.\n   \"\"\"\n \n@@ -1191,6 +1195,7 @@ class RandomContrast(base_layer.BaseRandomLayer):\n         output = tf.image.random_contrast(\n             inputs, 1. - self.lower, 1. + self.upper,\n             seed=self._random_generator.make_legacy_seed())\n+      output = tf.clip_by_value(output, 0, 255)\n       output.set_shape(inputs.shape)\n       return output\n \n\n@@ -667,6 +667,14 @@ class RandomContrastTest(test_combinations.TestCase):\n     layer_1 = image_preprocessing.RandomContrast.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_output_value_clip(self):\n+    input_images = np.random.random((5, 8, 3)).astype(np.float32) * 255.0\n+    # Give a factor range [1.0, 11.0] so that it will produce large contrast.\n+    layer = image_preprocessing.RandomContrast((0.0, 10.0))\n+    output = layer(input_images)\n+    self.assertLessEqual(tf.reduce_max(output), 255.0)\n+    self.assertGreaterEqual(tf.reduce_min(output), 0.0)\n+\n   def test_unbatched_image(self):\n     np.random.seed(1337)\n     mock_random = 0.2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01c906c4178db5ae03b7eb2d298a052c952a0667", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8931 | Lines Deleted: 8257 | Files Changed: 56 | Hunks: 326 | Methods Changed: 637 | Complexity Δ (Sum/Max): 820/165 | Churn Δ: 17188 | Churn Cumulative: 51759 | Contributors (this commit): 117 | Commits (past 90d): 107 | Contributors (cumulative): 252 | DMM Complexity: 0.47368421052631576\n\nDIFF:\n@@ -21,8 +21,10 @@ import numpy as np\n import keras\n from keras.testing_infra import test_utils\n from keras.distribute import keras_correctness_test_base\n-from keras.layers import recurrent as rnn_v1\n-from keras.layers import recurrent_v2 as rnn_v2\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_v1\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n from keras.mixed_precision import policy\n from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n@@ -73,9 +75,9 @@ class DistributionStrategyGruModelCorrectnessTest(\n     if tf.__internal__.tf2.enabled():\n       if not tf.executing_eagerly():\n         self.skipTest(\"GRU v2 and legacy graph mode don't work together.\")\n-      return rnn_v2.GRU\n+      return gru.GRU\n     else:\n-      return rnn_v1.GRU\n+      return gru_v1.GRU\n \n   @tf.__internal__.distribute.combinations.generate(\n       keras_correctness_test_base.test_combinations_for_embedding_model() +\n@@ -94,9 +96,9 @@ class DistributionStrategyLstmModelCorrectnessTest(\n     if tf.__internal__.tf2.enabled():\n       if not tf.executing_eagerly():\n         self.skipTest(\"LSTM v2 and legacy graph mode don't work together.\")\n-      return rnn_v2.LSTM\n+      return lstm.LSTM\n     else:\n-      return rnn_v1.LSTM\n+      return lstm_v1.LSTM\n \n   @tf.__internal__.distribute.combinations.generate(\n       keras_correctness_test_base.test_combinations_for_embedding_model() +\n\n@@ -1743,7 +1743,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n     s = sequential.Sequential()\n     s.add(layers.Dense(5, input_shape=(4,)))\n \n-    x = layers.wrappers.TimeDistributed(s)(x)\n+    x = layers.TimeDistributed(s)(x)\n     model = training_lib.Model(inputs=inputs, outputs=x)\n     model.compile(\n         optimizer='rmsprop',\n\n@@ -30,7 +30,8 @@ from keras import backend\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n from keras.layers import merging\n-from keras.layers import recurrent\n+from keras.layers.rnn import base_rnn\n+from keras.layers.rnn import simple_rnn\n \n \n class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n@@ -95,7 +96,7 @@ class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n     ctx_input = backend.repeat(ctx_input, tf.shape(seq_input)[1])\n     concatenated_input = merging.concatenate([seq_input, ctx_input])\n \n-    rnn_layer = recurrent.RNN(recurrent.SimpleRNNCell(10))\n+    rnn_layer = base_rnn.RNN(simple_rnn.SimpleRNNCell(10))\n     output = rnn_layer(concatenated_input)\n \n     with self.cached_session() as sess:\n\n@@ -196,58 +196,58 @@ from keras.layers.pooling.global_max_pooling2d import GlobalMaxPool2D\n from keras.layers.pooling.global_max_pooling3d import GlobalMaxPool3D\n \n # Recurrent layers.\n-from keras.layers.recurrent import RNN\n-from keras.layers.recurrent import AbstractRNNCell\n-from keras.layers.recurrent import StackedRNNCells\n-from keras.layers.recurrent import SimpleRNNCell\n-from keras.layers.recurrent import PeepholeLSTMCell\n-from keras.layers.recurrent import SimpleRNN\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n+from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.layers.rnn.simple_rnn import SimpleRNNCell\n+from keras.layers.rnn.lstm_v1 import PeepholeLSTMCell\n+from keras.layers.rnn.simple_rnn import SimpleRNN\n \n if tf.__internal__.tf2.enabled():\n-  from keras.layers.recurrent_v2 import GRU\n-  from keras.layers.recurrent_v2 import GRUCell\n-  from keras.layers.recurrent_v2 import LSTM\n-  from keras.layers.recurrent_v2 import LSTMCell\n-  from keras.layers.recurrent import GRU as GRUV1\n-  from keras.layers.recurrent import GRUCell as GRUCellV1\n-  from keras.layers.recurrent import LSTM as LSTMV1\n-  from keras.layers.recurrent import LSTMCell as LSTMCellV1\n+  from keras.layers.rnn.gru import GRU\n+  from keras.layers.rnn.gru import GRUCell\n+  from keras.layers.rnn.lstm import LSTM\n+  from keras.layers.rnn.lstm import LSTMCell\n+  from keras.layers.rnn.gru_v1 import GRU as GRUV1\n+  from keras.layers.rnn.gru_v1 import GRUCell as GRUCellV1\n+  from keras.layers.rnn.lstm_v1 import LSTM as LSTMV1\n+  from keras.layers.rnn.lstm_v1 import LSTMCell as LSTMCellV1\n   GRUV2 = GRU\n   GRUCellV2 = GRUCell\n   LSTMV2 = LSTM\n   LSTMCellV2 = LSTMCell\n else:\n-  from keras.layers.recurrent import GRU\n-  from keras.layers.recurrent import GRUCell\n-  from keras.layers.recurrent import LSTM\n-  from keras.layers.recurrent import LSTMCell\n-  from keras.layers.recurrent_v2 import GRU as GRUV2\n-  from keras.layers.recurrent_v2 import GRUCell as GRUCellV2\n-  from keras.layers.recurrent_v2 import LSTM as LSTMV2\n-  from keras.layers.recurrent_v2 import LSTMCell as LSTMCellV2\n+  from keras.layers.rnn.gru_v1 import GRU\n+  from keras.layers.rnn.gru_v1 import GRUCell\n+  from keras.layers.rnn.lstm_v1 import LSTM\n+  from keras.layers.rnn.lstm_v1 import LSTMCell\n+  from keras.layers.rnn.gru import GRU as GRUV2\n+  from keras.layers.rnn.gru import GRUCell as GRUCellV2\n+  from keras.layers.rnn.lstm import LSTM as LSTMV2\n+  from keras.layers.rnn.lstm import LSTMCell as LSTMCellV2\n   GRUV1 = GRU\n   GRUCellV1 = GRUCell\n   LSTMV1 = LSTM\n   LSTMCellV1 = LSTMCell\n \n # Convolutional-recurrent layers.\n-from keras.layers.convolutional_recurrent import ConvLSTM1D\n-from keras.layers.convolutional_recurrent import ConvLSTM2D\n-from keras.layers.convolutional_recurrent import ConvLSTM3D\n+from keras.layers.rnn.conv_lstm1d import ConvLSTM1D\n+from keras.layers.rnn.conv_lstm2d import ConvLSTM2D\n+from keras.layers.rnn.conv_lstm3d import ConvLSTM3D\n \n # cuDNN recurrent layers.\n-from keras.layers.cudnn_recurrent import CuDNNLSTM\n-from keras.layers.cudnn_recurrent import CuDNNGRU\n+from keras.layers.rnn.cudnn_lstm import CuDNNLSTM\n+from keras.layers.rnn.cudnn_gru import CuDNNGRU\n \n # Wrapper functions.\n-from keras.layers.wrappers import Wrapper\n-from keras.layers.wrappers import Bidirectional\n-from keras.layers.wrappers import TimeDistributed\n+from keras.layers.rnn.base_wrapper import Wrapper\n+from keras.layers.rnn.bidirectional import Bidirectional\n+from keras.layers.rnn.time_distributed import TimeDistributed\n \n # RNN Cell wrappers.\n-from keras.layers.rnn_cell_wrapper_v2 import DeviceWrapper\n-from keras.layers.rnn_cell_wrapper_v2 import DropoutWrapper\n-from keras.layers.rnn_cell_wrapper_v2 import ResidualWrapper\n+from keras.layers.rnn.cell_wrappers import DeviceWrapper\n+from keras.layers.rnn.cell_wrappers import DropoutWrapper\n+from keras.layers.rnn.cell_wrappers import ResidualWrapper\n \n # Serialization functions.\n from keras.layers import serialization\n\n@@ -1,1466 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-direct-tensorflow-import\n-\"\"\"Convolutional-recurrent layers.\"\"\"\n-\n-from keras import activations\n-from keras import backend\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine import base_layer\n-from keras.engine.input_spec import InputSpec\n-from keras.layers.recurrent import DropoutRNNCellMixin\n-from keras.layers.recurrent import RNN\n-from keras.utils import conv_utils\n-from keras.utils import generic_utils\n-from keras.utils import tf_utils\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-class ConvRNN(RNN):\n-  \"\"\"N-Dimensional Base class for convolutional-recurrent layers.\n-\n-  Args:\n-    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n-    cell: A RNN cell instance. A RNN cell is a class that has: - a\n-      `call(input_at_t, states_at_t)` method, returning `(output_at_t,\n-      states_at_t_plus_1)`. The call method of the cell can also take the\n-      optional argument `constants`, see section \"Note on passing external\n-      constants\" below. - a `state_size` attribute. This can be a single integer\n-      (single state) in which case it is the number of channels of the recurrent\n-      state (which should be the same as the number of channels of the cell\n-      output). This can also be a list/tuple of integers (one size per state).\n-      In this case, the first entry (`state_size[0]`) should be the same as the\n-      size of the cell output.\n-    return_sequences: Boolean. Whether to return the last output. in the output\n-      sequence, or the full sequence.\n-    return_state: Boolean. Whether to return the last state in addition to the\n-      output.\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    stateful: Boolean (default False). If True, the last state for each sample\n-      at index i in a batch will be used as initial state for the sample of\n-      index i in the following batch.\n-    input_shape: Use this argument to specify the shape of the input when this\n-      layer is the first one in a model.\n-  Call arguments:\n-    inputs: A (2 + `rank`)D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n-      given timestep should be masked.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is for use with cells that use dropout.\n-    initial_state: List of initial state tensors to be passed to the first call\n-      of the cell.\n-    constants: List of constant tensors to be passed to the cell at each\n-      timestep.\n-  Input shape:\n-    (3 + `rank`)D tensor with shape: `(samples, timesteps, channels,\n-      img_dimensions...)`\n-    if data_format='channels_first' or shape: `(samples, timesteps,\n-      img_dimensions..., channels)` if data_format='channels_last'.\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is the output. The\n-      remaining tensors are the last states,\n-      each (2 + `rank`)D tensor with shape: `(samples, filters,\n-        new_img_dimensions...)` if data_format='channels_first'\n-      or shape: `(samples, new_img_dimensions..., filters)` if\n-        data_format='channels_last'. img_dimension values might have changed due\n-        to padding.\n-    - If `return_sequences`: (3 + `rank`)D tensor with shape: `(samples,\n-      timesteps, filters, new_img_dimensions...)` if\n-      data_format='channels_first'\n-      or shape: `(samples, timesteps, new_img_dimensions..., filters)` if\n-        data_format='channels_last'.\n-    - Else, (2 + `rank`)D tensor with shape: `(samples, filters,\n-      new_img_dimensions...)` if data_format='channels_first'\n-      or shape: `(samples, new_img_dimensions..., filters)` if\n-        data_format='channels_last'.\n-  Masking: This layer supports masking for input data with a variable number of\n-    timesteps.\n-  Note on using statefulness in RNNs: You can set RNN layers to be 'stateful',\n-    which means that the states computed for the samples in one batch will be\n-    reused as initial states for the samples in the next batch. This assumes a\n-    one-to-one mapping between samples in different successive batches.\n-    To enable statefulness: - Specify `stateful=True` in the layer constructor.\n-      - Specify a fixed batch size for your model, by passing\n-          - If sequential model: `batch_input_shape=(...)` to the first layer in\n-            your model.\n-          - If functional model with 1 or more Input layers: `batch_shape=(...)`\n-            to all the first layers in your model. This is the expected shape of\n-            your inputs *including the batch size*. It should be a tuple of\n-            integers, e.g. `(32, 10, 100, 100, 32)`. for rank 2 convolution Note\n-            that the image dimensions should be specified too. - Specify\n-            `shuffle=False` when calling fit(). To reset the states of your\n-            model, call `.reset_states()` on either a specific layer, or on your\n-            entire model.\n-  Note on specifying the initial state of RNNs: You can specify the initial\n-    state of RNN layers symbolically by calling them with the keyword argument\n-    `initial_state`. The value of `initial_state` should be a tensor or list of\n-    tensors representing the initial state of the RNN layer. You can specify the\n-    initial state of RNN layers numerically by calling `reset_states` with the\n-    keyword argument `states`. The value of `states` should be a numpy array or\n-    list of numpy arrays representing the initial state of the RNN layer.\n-  Note on passing external constants to RNNs: You can pass \"external\" constants\n-    to the cell using the `constants` keyword argument of `RNN.__call__` (as\n-    well as `RNN.call`) method. This requires that the `cell.call` method\n-    accepts the same keyword argument `constants`. Such constants can be used to\n-    condition the cell transformation on additional static inputs (not changing\n-    over time), a.k.a. an attention mechanism.\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               cell,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               **kwargs):\n-    if unroll:\n-      raise TypeError(\n-          'Unrolling is not possible with convolutional RNNs. '\n-          f'Received: unroll={unroll}')\n-    if isinstance(cell, (list, tuple)):\n-      # The StackedConvRNN3DCells isn't implemented yet.\n-      raise TypeError('It is not possible at the moment to'\n-                      'stack convolutional cells. Only pass a single cell '\n-                      'instance as the `cell` argument. Received: '\n-                      f'cell={cell}')\n-    super(ConvRNN, self).__init__(cell, return_sequences, return_state,\n-                                  go_backwards, stateful, unroll, **kwargs)\n-    self.rank = rank\n-    self.input_spec = [InputSpec(ndim=rank + 3)]\n-    self.states = None\n-    self._num_constants = None\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-\n-    cell = self.cell\n-    if cell.data_format == 'channels_first':\n-      img_dims = input_shape[3:]\n-    elif cell.data_format == 'channels_last':\n-      img_dims = input_shape[2:-1]\n-\n-    norm_img_dims = tuple([\n-        conv_utils.conv_output_length(\n-            img_dims[idx],\n-            cell.kernel_size[idx],\n-            padding=cell.padding,\n-            stride=cell.strides[idx],\n-            dilation=cell.dilation_rate[idx]) for idx in range(len(img_dims))\n-    ])\n-\n-    if cell.data_format == 'channels_first':\n-      output_shape = input_shape[:2] + (cell.filters,) + norm_img_dims\n-    elif cell.data_format == 'channels_last':\n-      output_shape = input_shape[:2] + norm_img_dims + (cell.filters,)\n-\n-    if not self.return_sequences:\n-      output_shape = output_shape[:1] + output_shape[2:]\n-\n-    if self.return_state:\n-      output_shape = [output_shape]\n-      if cell.data_format == 'channels_first':\n-        output_shape += [\n-            (input_shape[0], cell.filters) + norm_img_dims for _ in range(2)\n-        ]\n-      elif cell.data_format == 'channels_last':\n-        output_shape += [(input_shape[0],) + norm_img_dims + (cell.filters,)\n-                         for _ in range(2)]\n-    return output_shape\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    # Note input_shape will be list of shapes of initial states and\n-    # constants if these are passed in __call__.\n-    if self._num_constants is not None:\n-      constants_shape = input_shape[-self._num_constants:]  # pylint: disable=E1130\n-    else:\n-      constants_shape = None\n-\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-\n-    batch_size = input_shape[0] if self.stateful else None\n-    self.input_spec[0] = InputSpec(\n-        shape=(batch_size, None) + input_shape[2:self.rank + 3])\n-\n-    # allow cell (if layer) to build before we set or validate state_spec\n-    if isinstance(self.cell, base_layer.Layer):\n-      step_input_shape = (input_shape[0],) + input_shape[2:]\n-      if constants_shape is not None:\n-        self.cell.build([step_input_shape] + constants_shape)\n-      else:\n-        self.cell.build(step_input_shape)\n-\n-    # set or validate state_spec\n-    if hasattr(self.cell.state_size, '__len__'):\n-      state_size = list(self.cell.state_size)\n-    else:\n-      state_size = [self.cell.state_size]\n-\n-    if self.state_spec is not None:\n-      # initial_state was passed in call, check compatibility\n-      if self.cell.data_format == 'channels_first':\n-        ch_dim = 1\n-      elif self.cell.data_format == 'channels_last':\n-        ch_dim = self.rank + 1\n-      if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n-        raise ValueError(\n-            'An `initial_state` was passed that is not compatible with '\n-            '`cell.state_size`. Received state shapes '\n-            f'{[spec.shape for spec in self.state_spec]}. '\n-            f'However `cell.state_size` is {self.cell.state_size}')\n-    else:\n-      img_dims = tuple((None for _ in range(self.rank)))\n-      if self.cell.data_format == 'channels_first':\n-        self.state_spec = [\n-            InputSpec(shape=(None, dim) + img_dims) for dim in state_size\n-        ]\n-      elif self.cell.data_format == 'channels_last':\n-        self.state_spec = [\n-            InputSpec(shape=(None,) + img_dims + (dim,)) for dim in state_size\n-        ]\n-    if self.stateful:\n-      self.reset_states()\n-    self.built = True\n-\n-  def get_initial_state(self, inputs):\n-    # (samples, timesteps, img_dims..., filters)\n-    initial_state = backend.zeros_like(inputs)\n-    # (samples, img_dims..., filters)\n-    initial_state = backend.sum(initial_state, axis=1)\n-    shape = list(self.cell.kernel_shape)\n-    shape[-1] = self.cell.filters\n-    initial_state = self.cell.input_conv(initial_state,\n-                                         tf.zeros(tuple(shape),\n-                                                         initial_state.dtype),\n-                                         padding=self.cell.padding)\n-\n-    if hasattr(self.cell.state_size, '__len__'):\n-      return [initial_state for _ in self.cell.state_size]\n-    else:\n-      return [initial_state]\n-\n-  def call(self,\n-           inputs,\n-           mask=None,\n-           training=None,\n-           initial_state=None,\n-           constants=None):\n-    # note that the .build() method of subclasses MUST define\n-    # self.input_spec and self.state_spec with complete input shapes.\n-    inputs, initial_state, constants = self._process_inputs(\n-        inputs, initial_state, constants)\n-\n-    if isinstance(mask, list):\n-      mask = mask[0]\n-    timesteps = backend.int_shape(inputs)[1]\n-\n-    kwargs = {}\n-    if generic_utils.has_arg(self.cell.call, 'training'):\n-      kwargs['training'] = training\n-\n-    if constants:\n-      if not generic_utils.has_arg(self.cell.call, 'constants'):\n-        raise ValueError(\n-            f'RNN cell {self.cell} does not support constants. '\n-            f'Received: constants={constants}')\n-\n-      def step(inputs, states):\n-        constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n-        states = states[:-self._num_constants]  # pylint: disable=invalid-unary-operand-type\n-        return self.cell.call(inputs, states, constants=constants, **kwargs)\n-    else:\n-      def step(inputs, states):\n-        return self.cell.call(inputs, states, **kwargs)\n-\n-    last_output, outputs, states = backend.rnn(step,\n-                                               inputs,\n-                                               initial_state,\n-                                               constants=constants,\n-                                               go_backwards=self.go_backwards,\n-                                               mask=mask,\n-                                               input_length=timesteps)\n-    if self.stateful:\n-      updates = [\n-          backend.update(self_state, state)\n-          for self_state, state in zip(self.states, states)\n-      ]\n-      self.add_update(updates)\n-\n-    if self.return_sequences:\n-      output = outputs\n-    else:\n-      output = last_output\n-\n-    if self.return_state:\n-      if not isinstance(states, (list, tuple)):\n-        states = [states]\n-      else:\n-        states = list(states)\n-      return [output] + states\n-    return output\n-\n-  def reset_states(self, states=None):\n-    if not self.stateful:\n-      raise AttributeError('Layer must be stateful.')\n-    input_shape = self.input_spec[0].shape\n-    state_shape = self.compute_output_shape(input_shape)\n-    if self.return_state:\n-      state_shape = state_shape[0]\n-    if self.return_sequences:\n-      state_shape = state_shape[:1].concatenate(state_shape[2:])\n-    if None in state_shape:\n-      raise ValueError('If a RNN is stateful, it needs to know '\n-                       'its batch size. Specify the batch size '\n-                       'of your input tensors: \\n'\n-                       '- If using a Sequential model, '\n-                       'specify the batch size by passing '\n-                       'a `batch_input_shape` '\n-                       'argument to your first layer.\\n'\n-                       '- If using the functional API, specify '\n-                       'the time dimension by passing a '\n-                       '`batch_shape` argument to your Input layer.\\n'\n-                       'The same thing goes for the number of rows and '\n-                       'columns.')\n-\n-    # helper function\n-    def get_tuple_shape(nb_channels):\n-      result = list(state_shape)\n-      if self.cell.data_format == 'channels_first':\n-        result[1] = nb_channels\n-      elif self.cell.data_format == 'channels_last':\n-        result[self.rank + 1] = nb_channels\n-      else:\n-        raise KeyError(\n-            'Cell data format must be one of '\n-            '{\"channels_first\", \"channels_last\"}. Received: '\n-            f'cell.data_format={self.cell.data_format}')\n-      return tuple(result)\n-\n-    # initialize state if None\n-    if self.states[0] is None:\n-      if hasattr(self.cell.state_size, '__len__'):\n-        self.states = [backend.zeros(get_tuple_shape(dim))\n-                       for dim in self.cell.state_size]\n-      else:\n-        self.states = [backend.zeros(get_tuple_shape(self.cell.state_size))]\n-    elif states is None:\n-      if hasattr(self.cell.state_size, '__len__'):\n-        for state, dim in zip(self.states, self.cell.state_size):\n-          backend.set_value(state, np.zeros(get_tuple_shape(dim)))\n-      else:\n-        backend.set_value(self.states[0],\n-                          np.zeros(get_tuple_shape(self.cell.state_size)))\n-    else:\n-      if not isinstance(states, (list, tuple)):\n-        states = [states]\n-      if len(states) != len(self.states):\n-        raise ValueError(\n-            f'Layer {self.name} expects {len(self.states)} states, '\n-            f'but it received {len(states)} state values. '\n-            f'States received: {states}')\n-      for index, (value, state) in enumerate(zip(states, self.states)):\n-        if hasattr(self.cell.state_size, '__len__'):\n-          dim = self.cell.state_size[index]\n-        else:\n-          dim = self.cell.state_size\n-        if value.shape != get_tuple_shape(dim):\n-          raise ValueError(\n-              f'State {index} is incompatible with layer {self.name}: '\n-              f'expected shape={get_tuple_shape(dim)}, '\n-              f'found shape={value.shape}')\n-        backend.set_value(state, value)\n-\n-\n-class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n-  \"\"\"Cell class for the ConvLSTM layer.\n-\n-  Args:\n-    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers, specifying the strides of\n-      the convolution. Specifying any stride value != 1 is incompatible with\n-      specifying any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding evenly to the left/right or up/down\n-      of the input such that output has the same height/width dimension as the\n-      input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      It defaults to the `image_data_format` value found in your Keras config\n-      file at `~/.keras/keras.json`. If you never set it, then it will be\n-      \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-      is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-  Call arguments:\n-    inputs: A (2+ `rank`)D tensor.\n-    states:  List of state tensors corresponding to the previous timestep.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.0,\n-               recurrent_dropout=0.0,\n-               **kwargs):\n-    super(ConvLSTMCell, self).__init__(**kwargs)\n-    self.rank = rank\n-    if self.rank > 3:\n-      raise ValueError(f'Rank {rank} convolutions are not currently '\n-                       f'implemented. Received: rank={rank}')\n-    self.filters = filters\n-    self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank,\n-                                                  'kernel_size')\n-    self.strides = conv_utils.normalize_tuple(\n-        strides, self.rank, 'strides', allow_zero=True)\n-    self.padding = conv_utils.normalize_padding(padding)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, self.rank,\n-                                                    'dilation_rate')\n-    self.activation = activations.get(activation)\n-    self.recurrent_activation = activations.get(recurrent_activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.unit_forget_bias = unit_forget_bias\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1.0, max(0.0, dropout))\n-    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n-    self.state_size = (self.filters, self.filters)\n-\n-  def build(self, input_shape):\n-\n-    if self.data_format == 'channels_first':\n-      channel_axis = 1\n-    else:\n-      channel_axis = -1\n-    if input_shape[channel_axis] is None:\n-      raise ValueError(\n-          'The channel dimension of the inputs (last axis) should be defined. '\n-          f'Found None. Full input shape received: input_shape={input_shape}')\n-    input_dim = input_shape[channel_axis]\n-    self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n-    recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n-\n-    self.kernel = self.add_weight(\n-        shape=self.kernel_shape,\n-        initializer=self.kernel_initializer,\n-        name='kernel',\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=recurrent_kernel_shape,\n-        initializer=self.recurrent_initializer,\n-        name='recurrent_kernel',\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint)\n-\n-    if self.use_bias:\n-      if self.unit_forget_bias:\n-\n-        def bias_initializer(_, *args, **kwargs):\n-          return backend.concatenate([\n-              self.bias_initializer((self.filters,), *args, **kwargs),\n-              initializers.get('ones')((self.filters,), *args, **kwargs),\n-              self.bias_initializer((self.filters * 2,), *args, **kwargs),\n-          ])\n-      else:\n-        bias_initializer = self.bias_initializer\n-      self.bias = self.add_weight(\n-          shape=(self.filters * 4,),\n-          name='bias',\n-          initializer=bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs, states, training=None):\n-    h_tm1 = states[0]  # previous memory state\n-    c_tm1 = states[1]  # previous carry state\n-\n-    # dropout matrices for input units\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n-    # dropout matrices for recurrent units\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        h_tm1, training, count=4)\n-\n-    if 0 < self.dropout < 1.:\n-      inputs_i = inputs * dp_mask[0]\n-      inputs_f = inputs * dp_mask[1]\n-      inputs_c = inputs * dp_mask[2]\n-      inputs_o = inputs * dp_mask[3]\n-    else:\n-      inputs_i = inputs\n-      inputs_f = inputs\n-      inputs_c = inputs\n-      inputs_o = inputs\n-\n-    if 0 < self.recurrent_dropout < 1.:\n-      h_tm1_i = h_tm1 * rec_dp_mask[0]\n-      h_tm1_f = h_tm1 * rec_dp_mask[1]\n-      h_tm1_c = h_tm1 * rec_dp_mask[2]\n-      h_tm1_o = h_tm1 * rec_dp_mask[3]\n-    else:\n-      h_tm1_i = h_tm1\n-      h_tm1_f = h_tm1\n-      h_tm1_c = h_tm1\n-      h_tm1_o = h_tm1\n-\n-    (kernel_i, kernel_f, kernel_c, kernel_o) = tf.split(\n-        self.kernel, 4, axis=self.rank + 1)\n-    (recurrent_kernel_i, recurrent_kernel_f, recurrent_kernel_c,\n-     recurrent_kernel_o) = tf.split(\n-         self.recurrent_kernel, 4, axis=self.rank + 1)\n-\n-    if self.use_bias:\n-      bias_i, bias_f, bias_c, bias_o = tf.split(self.bias, 4)\n-    else:\n-      bias_i, bias_f, bias_c, bias_o = None, None, None, None\n-\n-    x_i = self.input_conv(inputs_i, kernel_i, bias_i, padding=self.padding)\n-    x_f = self.input_conv(inputs_f, kernel_f, bias_f, padding=self.padding)\n-    x_c = self.input_conv(inputs_c, kernel_c, bias_c, padding=self.padding)\n-    x_o = self.input_conv(inputs_o, kernel_o, bias_o, padding=self.padding)\n-    h_i = self.recurrent_conv(h_tm1_i, recurrent_kernel_i)\n-    h_f = self.recurrent_conv(h_tm1_f, recurrent_kernel_f)\n-    h_c = self.recurrent_conv(h_tm1_c, recurrent_kernel_c)\n-    h_o = self.recurrent_conv(h_tm1_o, recurrent_kernel_o)\n-\n-    i = self.recurrent_activation(x_i + h_i)\n-    f = self.recurrent_activation(x_f + h_f)\n-    c = f * c_tm1 + i * self.activation(x_c + h_c)\n-    o = self.recurrent_activation(x_o + h_o)\n-    h = o * self.activation(c)\n-    return h, [h, c]\n-\n-  @property\n-  def _conv_func(self):\n-    if self.rank == 1:\n-      return backend.conv1d\n-    if self.rank == 2:\n-      return backend.conv2d\n-    if self.rank == 3:\n-      return backend.conv3d\n-\n-  def input_conv(self, x, w, b=None, padding='valid'):\n-    conv_out = self._conv_func(\n-        x,\n-        w,\n-        strides=self.strides,\n-        padding=padding,\n-        data_format=self.data_format,\n-        dilation_rate=self.dilation_rate)\n-    if b is not None:\n-      conv_out = backend.bias_add(conv_out, b, data_format=self.data_format)\n-    return conv_out\n-\n-  def recurrent_conv(self, x, w):\n-    strides = conv_utils.normalize_tuple(\n-        1, self.rank, 'strides', allow_zero=True)\n-    conv_out = self._conv_func(\n-        x, w, strides=strides, padding='same', data_format=self.data_format)\n-    return conv_out\n-\n-  def get_config(self):\n-    config = {\n-        'filters':\n-            self.filters,\n-        'kernel_size':\n-            self.kernel_size,\n-        'strides':\n-            self.strides,\n-        'padding':\n-            self.padding,\n-        'data_format':\n-            self.data_format,\n-        'dilation_rate':\n-            self.dilation_rate,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'unit_forget_bias':\n-            self.unit_forget_bias,\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout,\n-    }\n-    base_config = super(ConvLSTMCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class ConvLSTM(ConvRNN):\n-  \"\"\"Abstract N-D Convolutional LSTM layer. (Used as implementation base)\n-\n-  Similar to an LSTM layer, but the input transformations\n-  and recurrent transformations are both convolutional.\n-\n-  Args:\n-    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers,\n-      specifying the strides of the convolution.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, time, ..., channels)`\n-      while `channels_first` corresponds to\n-      inputs with shape `(batch, time, channels, ...)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying\n-      the dilation rate to use for dilated convolution.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any `strides` value != 1.\n-    activation: Activation function to use.\n-      By default hyperbolic tangent activation function is applied\n-      (`tanh(x)`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix,\n-      used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean.\n-      If True, add 1 to the bias of the forget gate at initialization.\n-      Use in combination with `bias_initializer=\"zeros\"`.\n-      This is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to.\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    return_sequences: Boolean. Whether to return the last output\n-      in the output sequence, or the full sequence. (default False)\n-    return_state: Boolean Whether to return the last state\n-      in addition to the output. (default False)\n-    go_backwards: Boolean (default False).\n-      If True, process the input sequence backwards.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               dropout=0.0,\n-               recurrent_dropout=0.0,\n-               **kwargs):\n-    cell = ConvLSTMCell(\n-        rank=rank,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        dtype=kwargs.get('dtype'))\n-    super(ConvLSTM, self).__init__(\n-        rank,\n-        cell,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        **kwargs)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(ConvLSTM, self).call(\n-        inputs, mask=mask, training=training, initial_state=initial_state)\n-\n-  @property\n-  def filters(self):\n-    return self.cell.filters\n-\n-  @property\n-  def kernel_size(self):\n-    return self.cell.kernel_size\n-\n-  @property\n-  def strides(self):\n-    return self.cell.strides\n-\n-  @property\n-  def padding(self):\n-    return self.cell.padding\n-\n-  @property\n-  def data_format(self):\n-    return self.cell.data_format\n-\n-  @property\n-  def dilation_rate(self):\n-    return self.cell.dilation_rate\n-\n-  @property\n-  def activation(self):\n-    return self.cell.activation\n-\n-  @property\n-  def recurrent_activation(self):\n-    return self.cell.recurrent_activation\n-\n-  @property\n-  def use_bias(self):\n-    return self.cell.use_bias\n-\n-  @property\n-  def kernel_initializer(self):\n-    return self.cell.kernel_initializer\n-\n-  @property\n-  def recurrent_initializer(self):\n-    return self.cell.recurrent_initializer\n-\n-  @property\n-  def bias_initializer(self):\n-    return self.cell.bias_initializer\n-\n-  @property\n-  def unit_forget_bias(self):\n-    return self.cell.unit_forget_bias\n-\n-  @property\n-  def kernel_regularizer(self):\n-    return self.cell.kernel_regularizer\n-\n-  @property\n-  def recurrent_regularizer(self):\n-    return self.cell.recurrent_regularizer\n-\n-  @property\n-  def bias_regularizer(self):\n-    return self.cell.bias_regularizer\n-\n-  @property\n-  def kernel_constraint(self):\n-    return self.cell.kernel_constraint\n-\n-  @property\n-  def recurrent_constraint(self):\n-    return self.cell.recurrent_constraint\n-\n-  @property\n-  def bias_constraint(self):\n-    return self.cell.bias_constraint\n-\n-  @property\n-  def dropout(self):\n-    return self.cell.dropout\n-\n-  @property\n-  def recurrent_dropout(self):\n-    return self.cell.recurrent_dropout\n-\n-  def get_config(self):\n-    config = {'filters': self.filters,\n-              'kernel_size': self.kernel_size,\n-              'strides': self.strides,\n-              'padding': self.padding,\n-              'data_format': self.data_format,\n-              'dilation_rate': self.dilation_rate,\n-              'activation': activations.serialize(self.activation),\n-              'recurrent_activation': activations.serialize(\n-                  self.recurrent_activation),\n-              'use_bias': self.use_bias,\n-              'kernel_initializer': initializers.serialize(\n-                  self.kernel_initializer),\n-              'recurrent_initializer': initializers.serialize(\n-                  self.recurrent_initializer),\n-              'bias_initializer': initializers.serialize(self.bias_initializer),\n-              'unit_forget_bias': self.unit_forget_bias,\n-              'kernel_regularizer': regularizers.serialize(\n-                  self.kernel_regularizer),\n-              'recurrent_regularizer': regularizers.serialize(\n-                  self.recurrent_regularizer),\n-              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-              'activity_regularizer': regularizers.serialize(\n-                  self.activity_regularizer),\n-              'kernel_constraint': constraints.serialize(\n-                  self.kernel_constraint),\n-              'recurrent_constraint': constraints.serialize(\n-                  self.recurrent_constraint),\n-              'bias_constraint': constraints.serialize(self.bias_constraint),\n-              'dropout': self.dropout,\n-              'recurrent_dropout': self.recurrent_dropout}\n-    base_config = super(ConvLSTM, self).get_config()\n-    del base_config['cell']\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    return cls(**config)\n-\n-\n-@keras_export('keras.layers.ConvLSTM1D')\n-class ConvLSTM1D(ConvLSTM):\n-  \"\"\"1D Convolutional LSTM.\n-\n-  Similar to an LSTM layer, but the input transformations\n-  and recurrent transformations are both convolutional.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers, specifying the strides of\n-      the convolution. Specifying any stride value != 1 is incompatible with\n-      specifying any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding evenly to the left/right or up/down\n-      of the input such that output has the same height/width dimension as the\n-      input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-      defaults to the `image_data_format` value found in your Keras config file\n-      at `~/.keras/keras.json`. If you never set it, then it will be\n-      \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. By default hyperbolic tangent\n-      activation function is applied (`tanh(x)`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-      is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    return_sequences: Boolean. Whether to return the last output in the output\n-      sequence, or the full sequence. (default False)\n-    return_state: Boolean Whether to return the last state in addition to the\n-      output. (default False)\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards.\n-    stateful: Boolean (default False). If True, the last state for each sample\n-      at index i in a batch will be used as initial state for the sample of\n-      index i in the following batch.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-  Call arguments:\n-    inputs: A 4D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n-      given timestep should be masked.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-      are set.\n-    initial_state: List of initial state tensors to be passed to the first call\n-      of the cell.\n-  Input shape: - If data_format='channels_first'\n-        4D tensor with shape: `(samples, time, channels, rows)` - If\n-          data_format='channels_last'\n-        4D tensor with shape: `(samples, time, rows, channels)`\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is the output. The\n-      remaining tensors are the last states,\n-      each 3D tensor with shape: `(samples, filters, new_rows)` if\n-        data_format='channels_first'\n-      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n-        `rows` values might have changed due to padding.\n-    - If `return_sequences`: 4D tensor with shape: `(samples, timesteps,\n-      filters, new_rows)` if data_format='channels_first'\n-      or shape: `(samples, timesteps, new_rows, filters)` if\n-        data_format='channels_last'.\n-    - Else, 3D tensor with shape: `(samples, filters, new_rows)` if\n-      data_format='channels_first'\n-      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n-\n-  Raises:\n-    ValueError: in case of invalid constructor arguments.\n-\n-  References:\n-    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n-    (the current implementation does not include the feedback loop on the\n-    cells output).\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               dropout=0.0,\n-               recurrent_dropout=0.0,\n-               **kwargs):\n-    super(ConvLSTM1D, self).__init__(\n-        rank=1,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.ConvLSTM2D')\n-class ConvLSTM2D(ConvLSTM):\n-  \"\"\"2D Convolutional LSTM.\n-\n-  Similar to an LSTM layer, but the input transformations\n-  and recurrent transformations are both convolutional.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers, specifying the strides of\n-      the convolution. Specifying any stride value != 1 is incompatible with\n-      specifying any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding evenly to the left/right or up/down\n-      of the input such that output has the same height/width dimension as the\n-      input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-      defaults to the `image_data_format` value found in your Keras config file\n-      at `~/.keras/keras.json`. If you never set it, then it will be\n-      \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. By default hyperbolic tangent\n-      activation function is applied (`tanh(x)`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-      is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    return_sequences: Boolean. Whether to return the last output in the output\n-      sequence, or the full sequence. (default False)\n-    return_state: Boolean Whether to return the last state in addition to the\n-      output. (default False)\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards.\n-    stateful: Boolean (default False). If True, the last state for each sample\n-      at index i in a batch will be used as initial state for the sample of\n-      index i in the following batch.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-  Call arguments:\n-    inputs: A 5D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n-      given timestep should be masked.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-      are set.\n-    initial_state: List of initial state tensors to be passed to the first call\n-      of the cell.\n-  Input shape: - If data_format='channels_first'\n-        5D tensor with shape: `(samples, time, channels, rows, cols)` - If\n-          data_format='channels_last'\n-        5D tensor with shape: `(samples, time, rows, cols, channels)`\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is the output. The\n-      remaining tensors are the last states,\n-      each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n-        data_format='channels_first'\n-      or shape: `(samples, new_rows, new_cols, filters)` if\n-        data_format='channels_last'. `rows` and `cols` values might have changed\n-        due to padding.\n-    - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,\n-      filters, new_rows, new_cols)` if data_format='channels_first'\n-      or shape: `(samples, timesteps, new_rows, new_cols, filters)` if\n-        data_format='channels_last'.\n-    - Else, 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n-      data_format='channels_first'\n-      or shape: `(samples, new_rows, new_cols, filters)` if\n-        data_format='channels_last'.\n-\n-  Raises:\n-    ValueError: in case of invalid constructor arguments.\n-\n-  References:\n-    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n-    (the current implementation does not include the feedback loop on the\n-    cells output).\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1),\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=(1, 1),\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               dropout=0.0,\n-               recurrent_dropout=0.0,\n-               **kwargs):\n-    super(ConvLSTM2D, self).__init__(\n-        rank=2,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.ConvLSTM3D')\n-class ConvLSTM3D(ConvLSTM):\n-  \"\"\"3D Convolutional LSTM.\n-\n-  Similar to an LSTM layer, but the input transformations\n-  and recurrent transformations are both convolutional.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers, specifying the strides of\n-      the convolution. Specifying any stride value != 1 is incompatible with\n-      specifying any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding evenly to the left/right or up/down\n-      of the input such that output has the same height/width dimension as the\n-      input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-      defaults to the `image_data_format` value found in your Keras config file\n-      at `~/.keras/keras.json`. If you never set it, then it will be\n-      \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. By default hyperbolic tangent\n-      activation function is applied (`tanh(x)`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-      is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    return_sequences: Boolean. Whether to return the last output in the output\n-      sequence, or the full sequence. (default False)\n-    return_state: Boolean Whether to return the last state in addition to the\n-      output. (default False)\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards.\n-    stateful: Boolean (default False). If True, the last state for each sample\n-      at index i in a batch will be used as initial state for the sample of\n-      index i in the following batch.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-  Call arguments:\n-    inputs: A 6D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n-      given timestep should be masked.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-      are set.\n-    initial_state: List of initial state tensors to be passed to the first call\n-      of the cell.\n-  Input shape: - If data_format='channels_first'\n-        6D tensor with shape: `(samples, time, channels, rows, cols, depth)` -\n-          If data_format='channels_last'\n-        5D tensor with shape: `(samples, time, rows, cols, depth, channels)`\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is the output. The\n-      remaining tensors are the last states,\n-      each 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n-        new_depth)` if data_format='channels_first'\n-      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n-        data_format='channels_last'. `rows`, `cols`, and `depth` values might\n-        have changed due to padding.\n-    - If `return_sequences`: 6D tensor with shape: `(samples, timesteps,\n-      filters, new_rows, new_cols, new_depth)` if data_format='channels_first'\n-      or shape: `(samples, timesteps, new_rows, new_cols, new_depth, filters)`\n-        if data_format='channels_last'.\n-    - Else, 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n-      new_depth)` if data_format='channels_first'\n-      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n-        data_format='channels_last'.\n-\n-  Raises:\n-    ValueError: in case of invalid constructor arguments.\n-\n-  References:\n-    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n-    (the current implementation does not include the feedback loop on the\n-    cells output).\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1, 1),\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=(1, 1, 1),\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               dropout=0.0,\n-               recurrent_dropout=0.0,\n-               **kwargs):\n-    super(ConvLSTM3D, self).__init__(\n-        rank=3,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        **kwargs)\n\n@@ -1,534 +0,0 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Recurrent layers backed by cuDNN.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import collections\n-from keras import backend\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine.input_spec import InputSpec\n-from keras.layers import recurrent_v2\n-from keras.layers.recurrent import RNN\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-class _CuDNNRNN(RNN):\n-  \"\"\"Private base class for CuDNNGRU and CuDNNLSTM layers.\n-\n-  Args:\n-    return_sequences: Boolean. Whether to return the last output\n-        in the output sequence, or the full sequence.\n-    return_state: Boolean. Whether to return the last state\n-        in addition to the output.\n-    go_backwards: Boolean (default False).\n-        If True, process the input sequence backwards and return the\n-        reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-        for each sample at index i in a batch will be used as initial\n-        state for the sample of index i in the following batch.\n-    time_major: Boolean (default False). If true, the inputs and outputs will be\n-        in shape `(timesteps, batch, ...)`, whereas in the False case, it will\n-        be `(batch, timesteps, ...)`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               time_major=False,\n-               **kwargs):\n-    # We invoke the base layer's initializer directly here because we do not\n-    # want to create RNN cell instance.\n-    super(RNN, self).__init__(**kwargs)  # pylint: disable=bad-super-call\n-    self.return_sequences = return_sequences\n-    self.return_state = return_state\n-    self.go_backwards = go_backwards\n-    self.stateful = stateful\n-    self.time_major = time_major\n-    self.supports_masking = False\n-    self.input_spec = [InputSpec(ndim=3)]\n-    if hasattr(self.cell.state_size, '__len__'):\n-      state_size = self.cell.state_size\n-    else:\n-      state_size = [self.cell.state_size]\n-    self.state_spec = [InputSpec(shape=(None, dim)) for dim in state_size]\n-    self.constants_spec = None\n-    self._states = None\n-    self._num_constants = 0\n-    self._vector_shape = tf.constant([-1])\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    if isinstance(mask, list):\n-      mask = mask[0]\n-    if mask is not None:\n-      raise ValueError('Masking is not supported for CuDNN RNNs.')\n-\n-    # input shape: `(samples, time (padded with zeros), input_dim)`\n-    # note that the .build() method of subclasses MUST define\n-    # self.input_spec and self.state_spec with complete input shapes.\n-    if isinstance(inputs, list):\n-      initial_state = inputs[1:]\n-      inputs = inputs[0]\n-    elif initial_state is not None:\n-      pass\n-    elif self.stateful:\n-      initial_state = self.states\n-    else:\n-      initial_state = self.get_initial_state(inputs)\n-\n-    if len(initial_state) != len(self.states):\n-      raise ValueError('Layer has ' + str(len(self.states)) +\n-                       ' states but was passed ' + str(len(initial_state)) +\n-                       ' initial states.')\n-\n-    if self.go_backwards:\n-      # Reverse time axis.\n-      inputs = backend.reverse(inputs, 1)\n-    output, states = self._process_batch(inputs, initial_state)\n-\n-    if self.stateful:\n-      updates = [\n-          tf.compat.v1.assign(self_state, state)\n-          for self_state, state in zip(self.states, states)\n-      ]\n-      self.add_update(updates)\n-\n-    if self.return_state:\n-      return [output] + states\n-    else:\n-      return output\n-\n-  def get_config(self):\n-    config = {\n-        'return_sequences': self.return_sequences,\n-        'return_state': self.return_state,\n-        'go_backwards': self.go_backwards,\n-        'stateful': self.stateful,\n-        'time_major': self.time_major,\n-    }\n-    base_config = super(  # pylint: disable=bad-super-call\n-        RNN, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    return cls(**config)\n-\n-  @property\n-  def trainable_weights(self):\n-    if self.trainable and self.built:\n-      return [self.kernel, self.recurrent_kernel, self.bias]\n-    return []\n-\n-  @property\n-  def non_trainable_weights(self):\n-    if not self.trainable and self.built:\n-      return [self.kernel, self.recurrent_kernel, self.bias]\n-    return []\n-\n-  @property\n-  def losses(self):\n-    return super(RNN, self).losses\n-\n-  def get_losses_for(self, inputs=None):\n-    return super(  # pylint: disable=bad-super-call\n-        RNN, self).get_losses_for(inputs=inputs)\n-\n-\n-@keras_export(v1=['keras.layers.CuDNNGRU'])\n-class CuDNNGRU(_CuDNNRNN):\n-  \"\"\"Fast GRU implementation backed by cuDNN.\n-\n-  More information about cuDNN can be found on the [NVIDIA\n-  developer website](https://developer.nvidia.com/cudnn).\n-  Can only be run on GPU.\n-\n-  Args:\n-      units: Positive integer, dimensionality of the output space.\n-      kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-        the linear transformation of the inputs.\n-      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-        matrix, used for the linear transformation of the recurrent state.\n-      bias_initializer: Initializer for the bias vector.\n-      kernel_regularizer: Regularizer function applied to the `kernel` weights\n-        matrix.\n-      recurrent_regularizer: Regularizer function applied to the\n-        `recurrent_kernel` weights matrix.\n-      bias_regularizer: Regularizer function applied to the bias vector.\n-      activity_regularizer: Regularizer function applied to the output of the\n-        layer (its \"activation\").\n-      kernel_constraint: Constraint function applied to the `kernel` weights\n-        matrix.\n-      recurrent_constraint: Constraint function applied to the\n-        `recurrent_kernel` weights matrix.\n-      bias_constraint: Constraint function applied to the bias vector.\n-      return_sequences: Boolean. Whether to return the last output in the output\n-        sequence, or the full sequence.\n-      return_state: Boolean. Whether to return the last state in addition to the\n-        output.\n-      go_backwards: Boolean (default False). If True, process the input sequence\n-        backwards and return the reversed sequence.\n-      stateful: Boolean (default False). If True, the last state for each sample\n-        at index i in a batch will be used as initial state for the sample of\n-        index i in the following batch.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               **kwargs):\n-    self.units = units\n-    cell_spec = collections.namedtuple('cell', 'state_size')\n-    self._cell = cell_spec(state_size=self.units)\n-    super(CuDNNGRU, self).__init__(\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        **kwargs)\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-  @property\n-  def cell(self):\n-    return self._cell\n-\n-  def build(self, input_shape):\n-    super(CuDNNGRU, self).build(input_shape)\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-    input_dim = int(input_shape[-1])\n-\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 3),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint)\n-\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 3),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint)\n-\n-    self.bias = self.add_weight(\n-        shape=(self.units * 6,),\n-        name='bias',\n-        initializer=self.bias_initializer,\n-        regularizer=self.bias_regularizer,\n-        constraint=self.bias_constraint)\n-\n-    self.built = True\n-\n-  def _process_batch(self, inputs, initial_state):\n-    if not self.time_major:\n-      inputs = tf.transpose(inputs, perm=(1, 0, 2))\n-    input_h = initial_state[0]\n-    input_h = tf.expand_dims(input_h, axis=0)\n-\n-    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access\n-        weights=[\n-            self.kernel[:, self.units:self.units * 2],\n-            self.kernel[:, :self.units],\n-            self.kernel[:, self.units * 2:],\n-            self.recurrent_kernel[:, self.units:self.units * 2],\n-            self.recurrent_kernel[:, :self.units],\n-            self.recurrent_kernel[:, self.units * 2:],\n-        ],\n-        biases=[\n-            self.bias[self.units:self.units * 2],\n-            self.bias[:self.units],\n-            self.bias[self.units * 2:self.units * 3],\n-            self.bias[self.units * 4:self.units * 5],\n-            self.bias[self.units * 3:self.units * 4],\n-            self.bias[self.units * 5:],\n-        ],\n-        shape=self._vector_shape)\n-\n-    args = {\n-        'input': inputs,\n-        'input_h': input_h,\n-        'input_c': 0,\n-        'params': params,\n-        'is_training': True,\n-        'rnn_mode': 'gru',\n-    }\n-\n-    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV2(**args)\n-\n-    if self.stateful or self.return_state:\n-      h = h[0]\n-    if self.return_sequences:\n-      if self.time_major:\n-        output = outputs\n-      else:\n-        output = tf.transpose(outputs, perm=(1, 0, 2))\n-    else:\n-      output = outputs[-1]\n-    return output, [h]\n-\n-  def get_config(self):\n-    config = {\n-        'units': self.units,\n-        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer': initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint': constraints.serialize(self.bias_constraint)\n-    }\n-    base_config = super(CuDNNGRU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(v1=['keras.layers.CuDNNLSTM'])\n-class CuDNNLSTM(_CuDNNRNN):\n-  \"\"\"Fast LSTM implementation backed by cuDNN.\n-\n-  More information about cuDNN can be found on the [NVIDIA\n-  developer website](https://developer.nvidia.com/cudnn).\n-  Can only be run on GPU.\n-\n-  Args:\n-      units: Positive integer, dimensionality of the output space.\n-      kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-        the linear transformation of the inputs.\n-      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n-        at initialization. Setting it to true will also force\n-        `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n-        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-        matrix, used for the linear transformation of the recurrent state.\n-      bias_initializer: Initializer for the bias vector.\n-      kernel_regularizer: Regularizer function applied to the `kernel` weights\n-        matrix.\n-      recurrent_regularizer: Regularizer function applied to the\n-        `recurrent_kernel` weights matrix.\n-      bias_regularizer: Regularizer function applied to the bias vector.\n-      activity_regularizer: Regularizer function applied to the output of the\n-        layer (its \"activation\").\n-      kernel_constraint: Constraint function applied to the `kernel` weights\n-        matrix.\n-      recurrent_constraint: Constraint function applied to the\n-        `recurrent_kernel` weights matrix.\n-      bias_constraint: Constraint function applied to the bias vector.\n-      return_sequences: Boolean. Whether to return the last output. in the\n-        output sequence, or the full sequence.\n-      return_state: Boolean. Whether to return the last state in addition to the\n-        output.\n-      go_backwards: Boolean (default False). If True, process the input sequence\n-        backwards and return the reversed sequence.\n-      stateful: Boolean (default False). If True, the last state for each sample\n-        at index i in a batch will be used as initial state for the sample of\n-        index i in the following batch.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               **kwargs):\n-    self.units = units\n-    cell_spec = collections.namedtuple('cell', 'state_size')\n-    self._cell = cell_spec(state_size=(self.units, self.units))\n-    super(CuDNNLSTM, self).__init__(\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        **kwargs)\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.unit_forget_bias = unit_forget_bias\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-  @property\n-  def cell(self):\n-    return self._cell\n-\n-  def build(self, input_shape):\n-    super(CuDNNLSTM, self).build(input_shape)\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-    input_dim = int(input_shape[-1])\n-\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 4),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint)\n-\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 4),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint)\n-\n-    if self.unit_forget_bias:\n-\n-      def bias_initializer(_, *args, **kwargs):\n-        return tf.concat([\n-            self.bias_initializer((self.units * 5,), *args, **kwargs),\n-            tf.compat.v1.ones_initializer()((self.units,), *args, **kwargs),\n-            self.bias_initializer((self.units * 2,), *args, **kwargs),\n-        ], axis=0)\n-    else:\n-      bias_initializer = self.bias_initializer\n-    self.bias = self.add_weight(\n-        shape=(self.units * 8,),\n-        name='bias',\n-        initializer=bias_initializer,\n-        regularizer=self.bias_regularizer,\n-        constraint=self.bias_constraint)\n-\n-    self.built = True\n-\n-  def _process_batch(self, inputs, initial_state):\n-    if not self.time_major:\n-      inputs = tf.transpose(inputs, perm=(1, 0, 2))\n-    input_h = initial_state[0]\n-    input_c = initial_state[1]\n-    input_h = tf.expand_dims(input_h, axis=0)\n-    input_c = tf.expand_dims(input_c, axis=0)\n-\n-    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access\n-        weights=[\n-            self.kernel[:, :self.units],\n-            self.kernel[:, self.units:self.units * 2],\n-            self.kernel[:, self.units * 2:self.units * 3],\n-            self.kernel[:, self.units * 3:],\n-            self.recurrent_kernel[:, :self.units],\n-            self.recurrent_kernel[:, self.units:self.units * 2],\n-            self.recurrent_kernel[:, self.units * 2:self.units * 3],\n-            self.recurrent_kernel[:, self.units * 3:],\n-        ],\n-        biases=[\n-            self.bias[:self.units],\n-            self.bias[self.units:self.units * 2],\n-            self.bias[self.units * 2:self.units * 3],\n-            self.bias[self.units * 3:self.units * 4],\n-            self.bias[self.units * 4:self.units * 5],\n-            self.bias[self.units * 5:self.units * 6],\n-            self.bias[self.units * 6:self.units * 7],\n-            self.bias[self.units * 7:],\n-        ],\n-        shape=self._vector_shape)\n-\n-    args = {\n-        'input': inputs,\n-        'input_h': input_h,\n-        'input_c': input_c,\n-        'params': params,\n-        'is_training': True,\n-    }\n-\n-    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV2(**args)\n-\n-    if self.stateful or self.return_state:\n-      h = h[0]\n-      c = c[0]\n-    if self.return_sequences:\n-      if self.time_major:\n-        output = outputs\n-      else:\n-        output = tf.transpose(outputs, perm=(1, 0, 2))\n-    else:\n-      output = outputs[-1]\n-    return output, [h, c]\n-\n-  def get_config(self):\n-    config = {\n-        'units': self.units,\n-        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer': initializers.serialize(self.bias_initializer),\n-        'unit_forget_bias': self.unit_forget_bias,\n-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint': constraints.serialize(self.bias_constraint)\n-    }\n-    base_config = super(CuDNNLSTM, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,3122 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n-\"\"\"Recurrent layers and their base classes.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import collections\n-import functools\n-import warnings\n-\n-import numpy as np\n-from keras import activations\n-from keras import backend\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine import base_layer\n-from keras.engine.input_spec import InputSpec\n-from keras.saving.saved_model import layer_serialization\n-from keras.utils import control_flow_util\n-from keras.utils import generic_utils\n-from keras.utils import tf_utils\n-from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.tools.docs import doc_controls\n-\n-\n-RECURRENT_DROPOUT_WARNING_MSG = (\n-    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n-    'Using `implementation=1`.')\n-\n-\n-@keras_export('keras.layers.StackedRNNCells')\n-class StackedRNNCells(base_layer.Layer):\n-  \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n-\n-  Used to implement efficient stacked RNNs.\n-\n-  Args:\n-    cells: List of RNN cell instances.\n-\n-  Examples:\n-\n-  ```python\n-  batch_size = 3\n-  sentence_max_length = 5\n-  n_features = 2\n-  new_shape = (batch_size, sentence_max_length, n_features)\n-  x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)\n-\n-  rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]\n-  stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)\n-  lstm_layer = tf.keras.layers.RNN(stacked_lstm)\n-\n-  result = lstm_layer(x)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self, cells, **kwargs):\n-    for cell in cells:\n-      if 'call' not in dir(cell):\n-        raise ValueError('All cells must have a `call` method. '\n-                         f'Received cell without a `call` method: {cell}')\n-      if 'state_size' not in dir(cell):\n-        raise ValueError('All cells must have a `state_size` attribute. '\n-                         f'Received cell without a `state_size`: {cell}')\n-    self.cells = cells\n-    # reverse_state_order determines whether the state size will be in a reverse\n-    # order of the cells' state. User might want to set this to True to keep the\n-    # existing behavior. This is only useful when use RNN(return_state=True)\n-    # since the state will be returned as the same order of state_size.\n-    self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n-    if self.reverse_state_order:\n-      logging.warning('reverse_state_order=True in StackedRNNCells will soon '\n-                      'be deprecated. Please update the code to work with the '\n-                      'natural order of states if you rely on the RNN states, '\n-                      'eg RNN(return_state=True).')\n-    super(StackedRNNCells, self).__init__(**kwargs)\n-\n-  @property\n-  def state_size(self):\n-    return tuple(c.state_size for c in\n-                 (self.cells[::-1] if self.reverse_state_order else self.cells))\n-\n-  @property\n-  def output_size(self):\n-    if getattr(self.cells[-1], 'output_size', None) is not None:\n-      return self.cells[-1].output_size\n-    elif _is_multiple_state(self.cells[-1].state_size):\n-      return self.cells[-1].state_size[0]\n-    else:\n-      return self.cells[-1].state_size\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    initial_states = []\n-    for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n-      get_initial_state_fn = getattr(cell, 'get_initial_state', None)\n-      if get_initial_state_fn:\n-        initial_states.append(get_initial_state_fn(\n-            inputs=inputs, batch_size=batch_size, dtype=dtype))\n-      else:\n-        initial_states.append(_generate_zero_filled_state_for_cell(\n-            cell, inputs, batch_size, dtype))\n-\n-    return tuple(initial_states)\n-\n-  def call(self, inputs, states, constants=None, training=None, **kwargs):\n-    # Recover per-cell states.\n-    state_size = (self.state_size[::-1]\n-                  if self.reverse_state_order else self.state_size)\n-    nested_states = tf.nest.pack_sequence_as(state_size, tf.nest.flatten(states))\n-\n-    # Call the cells in order and store the returned states.\n-    new_nested_states = []\n-    for cell, states in zip(self.cells, nested_states):\n-      states = states if tf.nest.is_nested(states) else [states]\n-      # TF cell does not wrap the state into list when there is only one state.\n-      is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\n-      states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n-      if generic_utils.has_arg(cell.call, 'training'):\n-        kwargs['training'] = training\n-      else:\n-        kwargs.pop('training', None)\n-      # Use the __call__ function for callable objects, eg layers, so that it\n-      # will have the proper name scopes for the ops, etc.\n-      cell_call_fn = cell.__call__ if callable(cell) else cell.call\n-      if generic_utils.has_arg(cell.call, 'constants'):\n-        inputs, states = cell_call_fn(inputs, states,\n-                                      constants=constants, **kwargs)\n-      else:\n-        inputs, states = cell_call_fn(inputs, states, **kwargs)\n-      new_nested_states.append(states)\n-\n-    return inputs, tf.nest.pack_sequence_as(state_size,\n-                                         tf.nest.flatten(new_nested_states))\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-\n-    def get_batch_input_shape(batch_size, dim):\n-      shape = tf.TensorShape(dim).as_list()\n-      return tuple([batch_size] + shape)\n-\n-    for cell in self.cells:\n-      if isinstance(cell, base_layer.Layer) and not cell.built:\n-        with backend.name_scope(cell.name):\n-          cell.build(input_shape)\n-          cell.built = True\n-      if getattr(cell, 'output_size', None) is not None:\n-        output_dim = cell.output_size\n-      elif _is_multiple_state(cell.state_size):\n-        output_dim = cell.state_size[0]\n-      else:\n-        output_dim = cell.state_size\n-      batch_size = tf.nest.flatten(input_shape)[0]\n-      if tf.nest.is_nested(output_dim):\n-        input_shape = tf.nest.map_structure(\n-            functools.partial(get_batch_input_shape, batch_size), output_dim)\n-        input_shape = tuple(input_shape)\n-      else:\n-        input_shape = tuple([batch_size] + tf.TensorShape(output_dim).as_list())\n-    self.built = True\n-\n-  def get_config(self):\n-    cells = []\n-    for cell in self.cells:\n-      cells.append(generic_utils.serialize_keras_object(cell))\n-    config = {'cells': cells}\n-    base_config = super(StackedRNNCells, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n-    cells = []\n-    for cell_config in config.pop('cells'):\n-      cells.append(\n-          deserialize_layer(cell_config, custom_objects=custom_objects))\n-    return cls(cells, **config)\n-\n-\n-@keras_export('keras.layers.RNN')\n-class RNN(base_layer.Layer):\n-  \"\"\"Base class for recurrent layers.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  Args:\n-    cell: A RNN cell instance or a list of RNN cell instances.\n-      A RNN cell is a class that has:\n-      - A `call(input_at_t, states_at_t)` method, returning\n-        `(output_at_t, states_at_t_plus_1)`. The call method of the\n-        cell can also take the optional argument `constants`, see\n-        section \"Note on passing external constants\" below.\n-      - A `state_size` attribute. This can be a single integer\n-        (single state) in which case it is the size of the recurrent\n-        state. This can also be a list/tuple of integers (one size per state).\n-        The `state_size` can also be TensorShape or tuple/list of\n-        TensorShape, to represent high dimension state.\n-      - A `output_size` attribute. This can be a single integer or a\n-        TensorShape, which represent the shape of the output. For backward\n-        compatible reason, if this attribute is not available for the\n-        cell, the value will be inferred by the first element of the\n-        `state_size`.\n-      - A `get_initial_state(inputs=None, batch_size=None, dtype=None)`\n-        method that creates a tensor meant to be fed to `call()` as the\n-        initial state, if the user didn't specify any initial state via other\n-        means. The returned initial state should have a shape of\n-        [batch_size, cell.state_size]. The cell might choose to create a\n-        tensor full of zeros, or full of other values based on the cell's\n-        implementation.\n-        `inputs` is the input tensor to the RNN layer, which should\n-        contain the batch size as its shape[0], and also dtype. Note that\n-        the shape[0] might be `None` during the graph construction. Either\n-        the `inputs` or the pair of `batch_size` and `dtype` are provided.\n-        `batch_size` is a scalar tensor that represents the batch size\n-        of the inputs. `dtype` is `tf.DType` that represents the dtype of\n-        the inputs.\n-        For backward compatibility, if this method is not implemented\n-        by the cell, the RNN layer will create a zero filled tensor with the\n-        size of [batch_size, cell.state_size].\n-      In the case that `cell` is a list of RNN cell instances, the cells\n-      will be stacked on top of each other in the RNN, resulting in an\n-      efficient stacked RNN.\n-    return_sequences: Boolean (default `False`). Whether to return the last\n-      output in the output sequence, or the full sequence.\n-    return_state: Boolean (default `False`). Whether to return the last state\n-      in addition to the output.\n-    go_backwards: Boolean (default `False`).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default `False`). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    unroll: Boolean (default `False`).\n-      If True, the network will be unrolled, else a symbolic loop will be used.\n-      Unrolling can speed-up a RNN, although it tends to be more\n-      memory-intensive. Unrolling is only suitable for short sequences.\n-    time_major: The shape format of the `inputs` and `outputs` tensors.\n-      If True, the inputs and outputs will be in shape\n-      `(timesteps, batch, ...)`, whereas in the False case, it will be\n-      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n-      efficient because it avoids transposes at the beginning and end of the\n-      RNN calculation. However, most TensorFlow data is batch-major, so by\n-      default this function accepts input and emits output in batch-major\n-      form.\n-    zero_output_for_mask: Boolean (default `False`).\n-      Whether the output should use zeros for the masked timesteps. Note that\n-      this field is only used when `return_sequences` is True and mask is\n-      provided. It can useful if you want to reuse the raw output sequence of\n-      the RNN without interference from the masked timesteps, eg, merging\n-      bidirectional RNNs.\n-\n-  Call arguments:\n-    inputs: Input tensor.\n-    mask: Binary tensor of shape `[batch_size, timesteps]` indicating whether\n-      a given timestep should be masked. An individual `True` entry indicates\n-      that the corresponding timestep should be utilized, while a `False`\n-      entry indicates that the corresponding timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is for use with cells that use dropout.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n-    constants: List of constant tensors to be passed to the cell at each\n-      timestep.\n-\n-  Input shape:\n-    N-D tensor with shape `[batch_size, timesteps, ...]` or\n-    `[timesteps, batch_size, ...]` when time_major is True.\n-\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is\n-      the output. The remaining tensors are the last states,\n-      each with shape `[batch_size, state_size]`, where `state_size` could\n-      be a high dimension tensor shape.\n-    - If `return_sequences`: N-D tensor with shape\n-      `[batch_size, timesteps, output_size]`, where `output_size` could\n-      be a high dimension tensor shape, or\n-      `[timesteps, batch_size, output_size]` when `time_major` is True.\n-    - Else, N-D tensor with shape `[batch_size, output_size]`, where\n-      `output_size` could be a high dimension tensor shape.\n-\n-  Masking:\n-    This layer supports masking for input data with a variable number\n-    of timesteps. To introduce masks to your data,\n-    use an [tf.keras.layers.Embedding] layer with the `mask_zero` parameter\n-    set to `True`.\n-\n-  Note on using statefulness in RNNs:\n-    You can set RNN layers to be 'stateful', which means that the states\n-    computed for the samples in one batch will be reused as initial states\n-    for the samples in the next batch. This assumes a one-to-one mapping\n-    between samples in different successive batches.\n-\n-    To enable statefulness:\n-      - Specify `stateful=True` in the layer constructor.\n-      - Specify a fixed batch size for your model, by passing\n-        If sequential model:\n-          `batch_input_shape=(...)` to the first layer in your model.\n-        Else for functional model with 1 or more Input layers:\n-          `batch_shape=(...)` to all the first layers in your model.\n-        This is the expected shape of your inputs\n-        *including the batch size*.\n-        It should be a tuple of integers, e.g. `(32, 10, 100)`.\n-      - Specify `shuffle=False` when calling `fit()`.\n-\n-    To reset the states of your model, call `.reset_states()` on either\n-    a specific layer, or on your entire model.\n-\n-  Note on specifying the initial state of RNNs:\n-    You can specify the initial state of RNN layers symbolically by\n-    calling them with the keyword argument `initial_state`. The value of\n-    `initial_state` should be a tensor or list of tensors representing\n-    the initial state of the RNN layer.\n-\n-    You can specify the initial state of RNN layers numerically by\n-    calling `reset_states` with the keyword argument `states`. The value of\n-    `states` should be a numpy array or list of numpy arrays representing\n-    the initial state of the RNN layer.\n-\n-  Note on passing external constants to RNNs:\n-    You can pass \"external\" constants to the cell using the `constants`\n-    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n-    requires that the `cell.call` method accepts the same keyword argument\n-    `constants`. Such constants can be used to condition the cell\n-    transformation on additional static inputs (not changing over time),\n-    a.k.a. an attention mechanism.\n-\n-  Examples:\n-\n-  ```python\n-  # First, let's define a RNN Cell, as a layer subclass.\n-\n-  class MinimalRNNCell(keras.layers.Layer):\n-\n-      def __init__(self, units, **kwargs):\n-          self.units = units\n-          self.state_size = units\n-          super(MinimalRNNCell, self).__init__(**kwargs)\n-\n-      def build(self, input_shape):\n-          self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n-                                        initializer='uniform',\n-                                        name='kernel')\n-          self.recurrent_kernel = self.add_weight(\n-              shape=(self.units, self.units),\n-              initializer='uniform',\n-              name='recurrent_kernel')\n-          self.built = True\n-\n-      def call(self, inputs, states):\n-          prev_output = states[0]\n-          h = backend.dot(inputs, self.kernel)\n-          output = h + backend.dot(prev_output, self.recurrent_kernel)\n-          return output, [output]\n-\n-  # Let's use this cell in a RNN layer:\n-\n-  cell = MinimalRNNCell(32)\n-  x = keras.Input((None, 5))\n-  layer = RNN(cell)\n-  y = layer(x)\n-\n-  # Here's how to use the cell to build a stacked RNN:\n-\n-  cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n-  x = keras.Input((None, 5))\n-  layer = RNN(cells)\n-  y = layer(x)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               cell,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               time_major=False,\n-               **kwargs):\n-    if isinstance(cell, (list, tuple)):\n-      cell = StackedRNNCells(cell)\n-    if 'call' not in dir(cell):\n-      raise ValueError('Argument `cell` should have a `call` method. '\n-                       f'The RNN was passed: cell={cell}')\n-    if 'state_size' not in dir(cell):\n-      raise ValueError('The RNN cell should have a `state_size` attribute '\n-                       '(tuple of integers, one integer per RNN state). '\n-                       f'Received: cell={cell}')\n-    # If True, the output for masked timestep will be zeros, whereas in the\n-    # False case, output from previous timestep is returned for masked timestep.\n-    self.zero_output_for_mask = kwargs.pop('zero_output_for_mask', False)\n-\n-    if 'input_shape' not in kwargs and (\n-        'input_dim' in kwargs or 'input_length' in kwargs):\n-      input_shape = (kwargs.pop('input_length', None),\n-                     kwargs.pop('input_dim', None))\n-      kwargs['input_shape'] = input_shape\n-\n-    super(RNN, self).__init__(**kwargs)\n-    self.cell = cell\n-    self.return_sequences = return_sequences\n-    self.return_state = return_state\n-    self.go_backwards = go_backwards\n-    self.stateful = stateful\n-    self.unroll = unroll\n-    self.time_major = time_major\n-\n-    self.supports_masking = True\n-    # The input shape is unknown yet, it could have nested tensor inputs, and\n-    # the input spec will be the list of specs for nested inputs, the structure\n-    # of the input_spec will be the same as the input.\n-    self.input_spec = None\n-    self.state_spec = None\n-    self._states = None\n-    self.constants_spec = None\n-    self._num_constants = 0\n-\n-    if stateful:\n-      if tf.distribute.has_strategy():\n-        raise ValueError('Stateful RNNs (created with `stateful=True`) '\n-                         'are not yet supported with tf.distribute.Strategy.')\n-\n-  @property\n-  def _use_input_spec_as_call_signature(self):\n-    if self.unroll:\n-      # When the RNN layer is unrolled, the time step shape cannot be unknown.\n-      # The input spec does not define the time step (because this layer can be\n-      # called with any time step value, as long as it is not None), so it\n-      # cannot be used as the call function signature when saving to SavedModel.\n-      return False\n-    return super(RNN, self)._use_input_spec_as_call_signature\n-\n-  @property\n-  def states(self):\n-    if self._states is None:\n-      state = tf.nest.map_structure(lambda _: None, self.cell.state_size)\n-      return state if tf.nest.is_nested(self.cell.state_size) else [state]\n-    return self._states\n-\n-  @states.setter\n-  # Automatic tracking catches \"self._states\" which adds an extra weight and\n-  # breaks HDF5 checkpoints.\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def states(self, states):\n-    self._states = states\n-\n-  def compute_output_shape(self, input_shape):\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-    # Check whether the input shape contains any nested shapes. It could be\n-    # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n-    # inputs.\n-    try:\n-      input_shape = tf.TensorShape(input_shape)\n-    except (ValueError, TypeError):\n-      # A nested tensor input\n-      input_shape = tf.nest.flatten(input_shape)[0]\n-\n-    batch = input_shape[0]\n-    time_step = input_shape[1]\n-    if self.time_major:\n-      batch, time_step = time_step, batch\n-\n-    if _is_multiple_state(self.cell.state_size):\n-      state_size = self.cell.state_size\n-    else:\n-      state_size = [self.cell.state_size]\n-\n-    def _get_output_shape(flat_output_size):\n-      output_dim = tf.TensorShape(flat_output_size).as_list()\n-      if self.return_sequences:\n-        if self.time_major:\n-          output_shape = tf.TensorShape(\n-              [time_step, batch] + output_dim)\n-        else:\n-          output_shape = tf.TensorShape(\n-              [batch, time_step] + output_dim)\n-      else:\n-        output_shape = tf.TensorShape([batch] + output_dim)\n-      return output_shape\n-\n-    if getattr(self.cell, 'output_size', None) is not None:\n-      # cell.output_size could be nested structure.\n-      output_shape = tf.nest.flatten(tf.nest.map_structure(\n-          _get_output_shape, self.cell.output_size))\n-      output_shape = output_shape[0] if len(output_shape) == 1 else output_shape\n-    else:\n-      # Note that state_size[0] could be a tensor_shape or int.\n-      output_shape = _get_output_shape(state_size[0])\n-\n-    if self.return_state:\n-      def _get_state_shape(flat_state):\n-        state_shape = [batch] + tf.TensorShape(flat_state).as_list()\n-        return tf.TensorShape(state_shape)\n-      state_shape = tf.nest.map_structure(_get_state_shape, state_size)\n-      return generic_utils.to_list(output_shape) + tf.nest.flatten(state_shape)\n-    else:\n-      return output_shape\n-\n-  def compute_mask(self, inputs, mask):\n-    # Time step masks must be the same for each input.\n-    # This is because the mask for an RNN is of size [batch, time_steps, 1],\n-    # and specifies which time steps should be skipped, and a time step\n-    # must be skipped for all inputs.\n-    # TODO(scottzhu): Should we accept multiple different masks?\n-    mask = tf.nest.flatten(mask)[0]\n-    output_mask = mask if self.return_sequences else None\n-    if self.return_state:\n-      state_mask = [None for _ in self.states]\n-      return [output_mask] + state_mask\n-    else:\n-      return output_mask\n-\n-  def build(self, input_shape):\n-    if isinstance(input_shape, list):\n-      input_shape = input_shape[0]\n-      # The input_shape here could be a nest structure.\n-\n-    # do the tensor_shape to shapes here. The input could be single tensor, or a\n-    # nested structure of tensors.\n-    def get_input_spec(shape):\n-      \"\"\"Convert input shape to InputSpec.\"\"\"\n-      if isinstance(shape, tf.TensorShape):\n-        input_spec_shape = shape.as_list()\n-      else:\n-        input_spec_shape = list(shape)\n-      batch_index, time_step_index = (1, 0) if self.time_major else (0, 1)\n-      if not self.stateful:\n-        input_spec_shape[batch_index] = None\n-      input_spec_shape[time_step_index] = None\n-      return InputSpec(shape=tuple(input_spec_shape))\n-\n-    def get_step_input_shape(shape):\n-      if isinstance(shape, tf.TensorShape):\n-        shape = tuple(shape.as_list())\n-      # remove the timestep from the input_shape\n-      return shape[1:] if self.time_major else (shape[0],) + shape[2:]\n-\n-    def get_state_spec(shape):\n-      state_spec_shape = tf.TensorShape(shape).as_list()\n-      # append batch dim\n-      state_spec_shape = [None] + state_spec_shape\n-      return InputSpec(shape=tuple(state_spec_shape))\n-\n-    # Check whether the input shape contains any nested shapes. It could be\n-    # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n-    # inputs.\n-    try:\n-      input_shape = tf.TensorShape(input_shape)\n-    except (ValueError, TypeError):\n-      # A nested tensor input\n-      pass\n-\n-    if not tf.nest.is_nested(input_shape):\n-      # This indicates the there is only one input.\n-      if self.input_spec is not None:\n-        self.input_spec[0] = get_input_spec(input_shape)\n-      else:\n-        self.input_spec = [get_input_spec(input_shape)]\n-      step_input_shape = get_step_input_shape(input_shape)\n-    else:\n-      if self.input_spec is not None:\n-        self.input_spec[0] = tf.nest.map_structure(get_input_spec, input_shape)\n-      else:\n-        self.input_spec = generic_utils.to_list(\n-            tf.nest.map_structure(get_input_spec, input_shape))\n-      step_input_shape = tf.nest.map_structure(get_step_input_shape, input_shape)\n-\n-    # allow cell (if layer) to build before we set or validate state_spec.\n-    if isinstance(self.cell, base_layer.Layer) and not self.cell.built:\n-      with backend.name_scope(self.cell.name):\n-        self.cell.build(step_input_shape)\n-        self.cell.built = True\n-\n-    # set or validate state_spec\n-    if _is_multiple_state(self.cell.state_size):\n-      state_size = list(self.cell.state_size)\n-    else:\n-      state_size = [self.cell.state_size]\n-\n-    if self.state_spec is not None:\n-      # initial_state was passed in call, check compatibility\n-      self._validate_state_spec(state_size, self.state_spec)\n-    else:\n-      if tf.nest.is_nested(state_size):\n-        self.state_spec = tf.nest.map_structure(get_state_spec, state_size)\n-      else:\n-        self.state_spec = [\n-            InputSpec(shape=[None] + tf.TensorShape(dim).as_list())\n-            for dim in state_size\n-        ]\n-      # ensure the generated state_spec is correct.\n-      self._validate_state_spec(state_size, self.state_spec)\n-    if self.stateful:\n-      self.reset_states()\n-    self.built = True\n-\n-  @staticmethod\n-  def _validate_state_spec(cell_state_sizes, init_state_specs):\n-    \"\"\"Validate the state spec between the initial_state and the state_size.\n-\n-    Args:\n-      cell_state_sizes: list, the `state_size` attribute from the cell.\n-      init_state_specs: list, the `state_spec` from the initial_state that is\n-        passed in `call()`.\n-\n-    Raises:\n-      ValueError: When initial state spec is not compatible with the state size.\n-    \"\"\"\n-    validation_error = ValueError(\n-        'An `initial_state` was passed that is not compatible with '\n-        '`cell.state_size`. Received `state_spec`={}; '\n-        'however `cell.state_size` is '\n-        '{}'.format(init_state_specs, cell_state_sizes))\n-    flat_cell_state_sizes = tf.nest.flatten(cell_state_sizes)\n-    flat_state_specs = tf.nest.flatten(init_state_specs)\n-\n-    if len(flat_cell_state_sizes) != len(flat_state_specs):\n-      raise validation_error\n-    for cell_state_spec, cell_state_size in zip(flat_state_specs,\n-                                                flat_cell_state_sizes):\n-      if not tf.TensorShape(\n-          # Ignore the first axis for init_state which is for batch\n-          cell_state_spec.shape[1:]).is_compatible_with(\n-              tf.TensorShape(cell_state_size)):\n-        raise validation_error\n-\n-  @doc_controls.do_not_doc_inheritable\n-  def get_initial_state(self, inputs):\n-    get_initial_state_fn = getattr(self.cell, 'get_initial_state', None)\n-\n-    if tf.nest.is_nested(inputs):\n-      # The input are nested sequences. Use the first element in the seq to get\n-      # batch size and dtype.\n-      inputs = tf.nest.flatten(inputs)[0]\n-\n-    input_shape = tf.shape(inputs)\n-    batch_size = input_shape[1] if self.time_major else input_shape[0]\n-    dtype = inputs.dtype\n-    if get_initial_state_fn:\n-      init_state = get_initial_state_fn(\n-          inputs=None, batch_size=batch_size, dtype=dtype)\n-    else:\n-      init_state = _generate_zero_filled_state(batch_size, self.cell.state_size,\n-                                               dtype)\n-    # Keras RNN expect the states in a list, even if it's a single state tensor.\n-    if not tf.nest.is_nested(init_state):\n-      init_state = [init_state]\n-    # Force the state to be a list in case it is a namedtuple eg LSTMStateTuple.\n-    return list(init_state)\n-\n-  def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n-    inputs, initial_state, constants = _standardize_args(inputs,\n-                                                         initial_state,\n-                                                         constants,\n-                                                         self._num_constants)\n-\n-    if initial_state is None and constants is None:\n-      return super(RNN, self).__call__(inputs, **kwargs)\n-\n-    # If any of `initial_state` or `constants` are specified and are Keras\n-    # tensors, then add them to the inputs and temporarily modify the\n-    # input_spec to include them.\n-\n-    additional_inputs = []\n-    additional_specs = []\n-    if initial_state is not None:\n-      additional_inputs += initial_state\n-      self.state_spec = tf.nest.map_structure(\n-          lambda s: InputSpec(shape=backend.int_shape(s)), initial_state)\n-      additional_specs += self.state_spec\n-    if constants is not None:\n-      additional_inputs += constants\n-      self.constants_spec = [\n-          InputSpec(shape=backend.int_shape(constant)) for constant in constants\n-      ]\n-      self._num_constants = len(constants)\n-      additional_specs += self.constants_spec\n-    # additional_inputs can be empty if initial_state or constants are provided\n-    # but empty (e.g. the cell is stateless).\n-    flat_additional_inputs = tf.nest.flatten(additional_inputs)\n-    is_keras_tensor = backend.is_keras_tensor(\n-        flat_additional_inputs[0]) if flat_additional_inputs else True\n-    for tensor in flat_additional_inputs:\n-      if backend.is_keras_tensor(tensor) != is_keras_tensor:\n-        raise ValueError(\n-            'The initial state or constants of an RNN layer cannot be '\n-            'specified via a mix of Keras tensors and non-Keras tensors '\n-            '(a \"Keras tensor\" is a tensor that was returned by a Keras layer '\n-            ' or by `Input` during Functional model construction). '\n-            f'Received: initial_state={initial_state}, constants={constants}')\n-\n-    if is_keras_tensor:\n-      # Compute the full input spec, including state and constants\n-      full_input = [inputs] + additional_inputs\n-      if self.built:\n-        # Keep the input_spec since it has been populated in build() method.\n-        full_input_spec = self.input_spec + additional_specs\n-      else:\n-        # The original input_spec is None since there could be a nested tensor\n-        # input. Update the input_spec to match the inputs.\n-        full_input_spec = generic_utils.to_list(\n-            tf.nest.map_structure(lambda _: None, inputs)) + additional_specs\n-      # Perform the call with temporarily replaced input_spec\n-      self.input_spec = full_input_spec\n-      output = super(RNN, self).__call__(full_input, **kwargs)\n-      # Remove the additional_specs from input spec and keep the rest. It is\n-      # important to keep since the input spec was populated by build(), and\n-      # will be reused in the stateful=True.\n-      self.input_spec = self.input_spec[:-len(additional_specs)]\n-      return output\n-    else:\n-      if initial_state is not None:\n-        kwargs['initial_state'] = initial_state\n-      if constants is not None:\n-        kwargs['constants'] = constants\n-      return super(RNN, self).__call__(inputs, **kwargs)\n-\n-  def call(self,\n-           inputs,\n-           mask=None,\n-           training=None,\n-           initial_state=None,\n-           constants=None):\n-    # The input should be dense, padded with zeros. If a ragged input is fed\n-    # into the layer, it is padded and the row lengths are used for masking.\n-    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n-    is_ragged_input = (row_lengths is not None)\n-    self._validate_args_if_ragged(is_ragged_input, mask)\n-\n-    inputs, initial_state, constants = self._process_inputs(\n-        inputs, initial_state, constants)\n-\n-    self._maybe_reset_cell_dropout_mask(self.cell)\n-    if isinstance(self.cell, StackedRNNCells):\n-      for cell in self.cell.cells:\n-        self._maybe_reset_cell_dropout_mask(cell)\n-\n-    if mask is not None:\n-      # Time step masks must be the same for each input.\n-      # TODO(scottzhu): Should we accept multiple different masks?\n-      mask = tf.nest.flatten(mask)[0]\n-\n-    if tf.nest.is_nested(inputs):\n-      # In the case of nested input, use the first element for shape check.\n-      input_shape = backend.int_shape(tf.nest.flatten(inputs)[0])\n-    else:\n-      input_shape = backend.int_shape(inputs)\n-    timesteps = input_shape[0] if self.time_major else input_shape[1]\n-    if self.unroll and timesteps is None:\n-      raise ValueError('Cannot unroll a RNN if the '\n-                       'time dimension is undefined. \\n'\n-                       '- If using a Sequential model, '\n-                       'specify the time dimension by passing '\n-                       'an `input_shape` or `batch_input_shape` '\n-                       'argument to your first layer. If your '\n-                       'first layer is an Embedding, you can '\n-                       'also use the `input_length` argument.\\n'\n-                       '- If using the functional API, specify '\n-                       'the time dimension by passing a `shape` '\n-                       'or `batch_shape` argument to your Input layer.')\n-\n-    kwargs = {}\n-    if generic_utils.has_arg(self.cell.call, 'training'):\n-      kwargs['training'] = training\n-\n-    # TF RNN cells expect single tensor as state instead of list wrapped tensor.\n-    is_tf_rnn_cell = getattr(self.cell, '_is_tf_rnn_cell', None) is not None\n-    # Use the __call__ function for callable objects, eg layers, so that it\n-    # will have the proper name scopes for the ops, etc.\n-    cell_call_fn = self.cell.__call__ if callable(self.cell) else self.cell.call\n-    if constants:\n-      if not generic_utils.has_arg(self.cell.call, 'constants'):\n-        raise ValueError(\n-            f'RNN cell {self.cell} does not support constants. '\n-            f'Received: constants={constants}')\n-\n-      def step(inputs, states):\n-        constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n-        states = states[:-self._num_constants]  # pylint: disable=invalid-unary-operand-type\n-\n-        states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n-        output, new_states = cell_call_fn(\n-            inputs, states, constants=constants, **kwargs)\n-        if not tf.nest.is_nested(new_states):\n-          new_states = [new_states]\n-        return output, new_states\n-    else:\n-\n-      def step(inputs, states):\n-        states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n-        output, new_states = cell_call_fn(inputs, states, **kwargs)\n-        if not tf.nest.is_nested(new_states):\n-          new_states = [new_states]\n-        return output, new_states\n-    last_output, outputs, states = backend.rnn(\n-        step,\n-        inputs,\n-        initial_state,\n-        constants=constants,\n-        go_backwards=self.go_backwards,\n-        mask=mask,\n-        unroll=self.unroll,\n-        input_length=row_lengths if row_lengths is not None else timesteps,\n-        time_major=self.time_major,\n-        zero_output_for_mask=self.zero_output_for_mask)\n-\n-    if self.stateful:\n-      updates = [\n-          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n-          for self_state, state in zip(\n-              tf.nest.flatten(self.states), tf.nest.flatten(states))\n-      ]\n-      self.add_update(updates)\n-\n-    if self.return_sequences:\n-      output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n-    else:\n-      output = last_output\n-\n-    if self.return_state:\n-      if not isinstance(states, (list, tuple)):\n-        states = [states]\n-      else:\n-        states = list(states)\n-      return generic_utils.to_list(output) + states\n-    else:\n-      return output\n-\n-  def _process_inputs(self, inputs, initial_state, constants):\n-    # input shape: `(samples, time (padded with zeros), input_dim)`\n-    # note that the .build() method of subclasses MUST define\n-    # self.input_spec and self.state_spec with complete input shapes.\n-    if (isinstance(inputs, collections.abc.Sequence)\n-        and not isinstance(inputs, tuple)):\n-      # get initial_state from full input spec\n-      # as they could be copied to multiple GPU.\n-      if not self._num_constants:\n-        initial_state = inputs[1:]\n-      else:\n-        initial_state = inputs[1:-self._num_constants]\n-        constants = inputs[-self._num_constants:]\n-      if len(initial_state) == 0:\n-        initial_state = None\n-      inputs = inputs[0]\n-\n-    if self.stateful:\n-      if initial_state is not None:\n-        # When layer is stateful and initial_state is provided, check if the\n-        # recorded state is same as the default value (zeros). Use the recorded\n-        # state if it is not same as the default.\n-        non_zero_count = tf.add_n([tf.math.count_nonzero(s)\n-                                   for s in tf.nest.flatten(self.states)])\n-        # Set strict = True to keep the original structure of the state.\n-        initial_state = tf.compat.v1.cond(non_zero_count > 0,\n-                                          true_fn=lambda: self.states,\n-                                          false_fn=lambda: initial_state,\n-                                          strict=True)\n-      else:\n-        initial_state = self.states\n-      initial_state = tf.nest.map_structure(\n-          # When the layer has a inferred dtype, use the dtype from the cell.\n-          lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype),\n-          initial_state\n-      )\n-    elif initial_state is None:\n-      initial_state = self.get_initial_state(inputs)\n-\n-    if len(initial_state) != len(self.states):\n-      raise ValueError(f'Layer has {len(self.states)} '\n-                       f'states but was passed {len(initial_state)} initial '\n-                       f'states. Received: initial_state={initial_state}')\n-    return inputs, initial_state, constants\n-\n-  def _validate_args_if_ragged(self, is_ragged_input, mask):\n-    if not is_ragged_input:\n-      return\n-\n-    if mask is not None:\n-      raise ValueError(f'The mask that was passed in was {mask}, which '\n-                       'cannot be applied to RaggedTensor inputs. Please '\n-                       'make sure that there is no mask injected by upstream '\n-                       'layers.')\n-    if self.unroll:\n-      raise ValueError('The input received contains RaggedTensors and does '\n-                       'not support unrolling. Disable unrolling by passing '\n-                       '`unroll=False` in the RNN Layer constructor.')\n-\n-  def _maybe_reset_cell_dropout_mask(self, cell):\n-    if isinstance(cell, DropoutRNNCellMixin):\n-      cell.reset_dropout_mask()\n-      cell.reset_recurrent_dropout_mask()\n-\n-  def reset_states(self, states=None):\n-    \"\"\"Reset the recorded states for the stateful RNN layer.\n-\n-    Can only be used when RNN layer is constructed with `stateful` = `True`.\n-    Args:\n-      states: Numpy arrays that contains the value for the initial state, which\n-        will be feed to cell at the first time step. When the value is None,\n-        zero filled numpy array will be created based on the cell state size.\n-\n-    Raises:\n-      AttributeError: When the RNN layer is not stateful.\n-      ValueError: When the batch size of the RNN layer is unknown.\n-      ValueError: When the input numpy array is not compatible with the RNN\n-        layer state, either size wise or dtype wise.\n-    \"\"\"\n-    if not self.stateful:\n-      raise AttributeError('Layer must be stateful.')\n-    spec_shape = None\n-    if self.input_spec is not None:\n-      spec_shape = tf.nest.flatten(self.input_spec[0])[0].shape\n-    if spec_shape is None:\n-      # It is possible to have spec shape to be None, eg when construct a RNN\n-      # with a custom cell, or standard RNN layers (LSTM/GRU) which we only know\n-      # it has 3 dim input, but not its full shape spec before build().\n-      batch_size = None\n-    else:\n-      batch_size = spec_shape[1] if self.time_major else spec_shape[0]\n-    if not batch_size:\n-      raise ValueError('If a RNN is stateful, it needs to know '\n-                       'its batch size. Specify the batch size '\n-                       'of your input tensors: \\n'\n-                       '- If using a Sequential model, '\n-                       'specify the batch size by passing '\n-                       'a `batch_input_shape` '\n-                       'argument to your first layer.\\n'\n-                       '- If using the functional API, specify '\n-                       'the batch size by passing a '\n-                       '`batch_shape` argument to your Input layer.')\n-    # initialize state if None\n-    if tf.nest.flatten(self.states)[0] is None:\n-      if getattr(self.cell, 'get_initial_state', None):\n-        flat_init_state_values = tf.nest.flatten(self.cell.get_initial_state(\n-            inputs=None, batch_size=batch_size,\n-            # Use variable_dtype instead of compute_dtype, since the state is\n-            # stored in a variable\n-            dtype=self.variable_dtype or backend.floatx()))\n-      else:\n-        flat_init_state_values = tf.nest.flatten(_generate_zero_filled_state(\n-            batch_size, self.cell.state_size,\n-            self.variable_dtype or backend.floatx()))\n-      flat_states_variables = tf.nest.map_structure(\n-          backend.variable, flat_init_state_values)\n-      self.states = tf.nest.pack_sequence_as(self.cell.state_size,\n-                                             flat_states_variables)\n-      if not tf.nest.is_nested(self.states):\n-        self.states = [self.states]\n-    elif states is None:\n-      for state, size in zip(tf.nest.flatten(self.states),\n-                             tf.nest.flatten(self.cell.state_size)):\n-        backend.set_value(\n-            state,\n-            np.zeros([batch_size] + tf.TensorShape(size).as_list()))\n-    else:\n-      flat_states = tf.nest.flatten(self.states)\n-      flat_input_states = tf.nest.flatten(states)\n-      if len(flat_input_states) != len(flat_states):\n-        raise ValueError(f'Layer {self.name} expects {len(flat_states)} '\n-                         f'states, but it received {len(flat_input_states)} '\n-                         f'state values. States received: {states}')\n-      set_value_tuples = []\n-      for i, (value, state) in enumerate(zip(flat_input_states,\n-                                             flat_states)):\n-        if value.shape != state.shape:\n-          raise ValueError(\n-              f'State {i} is incompatible with layer {self.name}: '\n-              f'expected shape={(batch_size, state)} '\n-              f'but found shape={value.shape}')\n-        set_value_tuples.append((state, value))\n-      backend.batch_set_value(set_value_tuples)\n-\n-  def get_config(self):\n-    config = {\n-        'return_sequences': self.return_sequences,\n-        'return_state': self.return_state,\n-        'go_backwards': self.go_backwards,\n-        'stateful': self.stateful,\n-        'unroll': self.unroll,\n-        'time_major': self.time_major\n-    }\n-    if self._num_constants:\n-      config['num_constants'] = self._num_constants\n-    if self.zero_output_for_mask:\n-      config['zero_output_for_mask'] = self.zero_output_for_mask\n-\n-    config['cell'] = generic_utils.serialize_keras_object(self.cell)\n-    base_config = super(RNN, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n-    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\n-    num_constants = config.pop('num_constants', 0)\n-    layer = cls(cell, **config)\n-    layer._num_constants = num_constants\n-    return layer\n-\n-  @property\n-  def _trackable_saved_model_saver(self):\n-    return layer_serialization.RNNSavedModelSaver(self)\n-\n-\n-@keras_export('keras.layers.AbstractRNNCell')\n-class AbstractRNNCell(base_layer.Layer):\n-  \"\"\"Abstract object representing an RNN cell.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  This is the base class for implementing RNN cells with custom behavior.\n-\n-  Every `RNNCell` must have the properties below and implement `call` with\n-  the signature `(output, next_state) = call(input, state)`.\n-\n-  Examples:\n-\n-  ```python\n-    class MinimalRNNCell(AbstractRNNCell):\n-\n-      def __init__(self, units, **kwargs):\n-        self.units = units\n-        super(MinimalRNNCell, self).__init__(**kwargs)\n-\n-      @property\n-      def state_size(self):\n-        return self.units\n-\n-      def build(self, input_shape):\n-        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n-                                      initializer='uniform',\n-                                      name='kernel')\n-        self.recurrent_kernel = self.add_weight(\n-            shape=(self.units, self.units),\n-            initializer='uniform',\n-            name='recurrent_kernel')\n-        self.built = True\n-\n-      def call(self, inputs, states):\n-        prev_output = states[0]\n-        h = backend.dot(inputs, self.kernel)\n-        output = h + backend.dot(prev_output, self.recurrent_kernel)\n-        return output, output\n-  ```\n-\n-  This definition of cell differs from the definition used in the literature.\n-  In the literature, 'cell' refers to an object with a single scalar output.\n-  This definition refers to a horizontal array of such units.\n-\n-  An RNN cell, in the most abstract setting, is anything that has\n-  a state and performs some operation that takes a matrix of inputs.\n-  This operation results in an output matrix with `self.output_size` columns.\n-  If `self.state_size` is an integer, this operation also results in a new\n-  state matrix with `self.state_size` columns.  If `self.state_size` is a\n-  (possibly nested tuple of) TensorShape object(s), then it should return a\n-  matching structure of Tensors having shape `[batch_size].concatenate(s)`\n-  for each `s` in `self.batch_size`.\n-  \"\"\"\n-\n-  def call(self, inputs, states):\n-    \"\"\"The function that contains the logic for one RNN step calculation.\n-\n-    Args:\n-      inputs: the input tensor, which is a slide from the overall RNN input by\n-        the time dimension (usually the second dimension).\n-      states: the state tensor from previous step, which has the same shape\n-        as `(batch, state_size)`. In the case of timestep 0, it will be the\n-        initial state user specified, or zero filled tensor otherwise.\n-\n-    Returns:\n-      A tuple of two tensors:\n-        1. output tensor for the current timestep, with size `output_size`.\n-        2. state tensor for next step, which has the shape of `state_size`.\n-    \"\"\"\n-    raise NotImplementedError\n-\n-  @property\n-  def state_size(self):\n-    \"\"\"size(s) of state(s) used by this cell.\n-\n-    It can be represented by an Integer, a TensorShape or a tuple of Integers\n-    or TensorShapes.\n-    \"\"\"\n-    raise NotImplementedError\n-\n-  @property\n-  def output_size(self):\n-    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n-    raise NotImplementedError\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n-\n-\n-@doc_controls.do_not_generate_docs\n-class DropoutRNNCellMixin:\n-  \"\"\"Object that hold dropout related fields for RNN Cell.\n-\n-  This class is not a standalone RNN cell. It suppose to be used with a RNN cell\n-  by multiple inheritance. Any cell that mix with class should have following\n-  fields:\n-    dropout: a float number within range [0, 1). The ratio that the input\n-      tensor need to dropout.\n-    recurrent_dropout: a float number within range [0, 1). The ratio that the\n-      recurrent state weights need to dropout.\n-    _random_generator: A backend.RandomGenerator instance, which will be used\n-      to produce outputs based on the inputs and dropout rate.\n-  This object will create and cache created dropout masks, and reuse them for\n-  the incoming data, so that the same mask is used for every batch input.\n-  \"\"\"\n-\n-  def __init__(self, *args, **kwargs):\n-    self._create_non_trackable_mask_cache()\n-    super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def _create_non_trackable_mask_cache(self):\n-    \"\"\"Create the cache for dropout and recurrent dropout mask.\n-\n-    Note that the following two masks will be used in \"graph function\" mode,\n-    e.g. these masks are symbolic tensors. In eager mode, the `eager_*_mask`\n-    tensors will be generated differently than in the \"graph function\" case,\n-    and they will be cached.\n-\n-    Also note that in graph mode, we still cache those masks only because the\n-    RNN could be created with `unroll=True`. In that case, the `cell.call()`\n-    function will be invoked multiple times, and we want to ensure same mask\n-    is used every time.\n-\n-    Also the caches are created without tracking. Since they are not picklable\n-    by python when deepcopy, we don't want `layer._obj_reference_counts_dict`\n-    to track it by default.\n-    \"\"\"\n-    self._dropout_mask_cache = backend.ContextValueCache(\n-        self._create_dropout_mask)\n-    self._recurrent_dropout_mask_cache = backend.ContextValueCache(\n-        self._create_recurrent_dropout_mask)\n-\n-  def reset_dropout_mask(self):\n-    \"\"\"Reset the cached dropout masks if any.\n-\n-    This is important for the RNN layer to invoke this in it `call()` method so\n-    that the cached mask is cleared before calling the `cell.call()`. The mask\n-    should be cached across the timestep within the same batch, but shouldn't\n-    be cached between batches. Otherwise it will introduce unreasonable bias\n-    against certain index of data within the batch.\n-    \"\"\"\n-    self._dropout_mask_cache.clear()\n-\n-  def reset_recurrent_dropout_mask(self):\n-    \"\"\"Reset the cached recurrent dropout masks if any.\n-\n-    This is important for the RNN layer to invoke this in it call() method so\n-    that the cached mask is cleared before calling the cell.call(). The mask\n-    should be cached across the timestep within the same batch, but shouldn't\n-    be cached between batches. Otherwise it will introduce unreasonable bias\n-    against certain index of data within the batch.\n-    \"\"\"\n-    self._recurrent_dropout_mask_cache.clear()\n-\n-  def _create_dropout_mask(self, inputs, training, count=1):\n-    return _generate_dropout_mask(\n-        self._random_generator,\n-        tf.ones_like(inputs),\n-        self.dropout,\n-        training=training,\n-        count=count)\n-\n-  def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n-    return _generate_dropout_mask(\n-        self._random_generator,\n-        tf.ones_like(inputs),\n-        self.recurrent_dropout,\n-        training=training,\n-        count=count)\n-\n-  def get_dropout_mask_for_cell(self, inputs, training, count=1):\n-    \"\"\"Get the dropout mask for RNN cell's input.\n-\n-    It will create mask based on context if there isn't any existing cached\n-    mask. If a new mask is generated, it will update the cache in the cell.\n-\n-    Args:\n-      inputs: The input tensor whose shape will be used to generate dropout\n-        mask.\n-      training: Boolean tensor, whether its in training mode, dropout will be\n-        ignored in non-training mode.\n-      count: Int, how many dropout mask will be generated. It is useful for cell\n-        that has internal weights fused together.\n-    Returns:\n-      List of mask tensor, generated or cached mask based on context.\n-    \"\"\"\n-    if self.dropout == 0:\n-      return None\n-    init_kwargs = dict(inputs=inputs, training=training, count=count)\n-    return self._dropout_mask_cache.setdefault(kwargs=init_kwargs)\n-\n-  def get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1):\n-    \"\"\"Get the recurrent dropout mask for RNN cell.\n-\n-    It will create mask based on context if there isn't any existing cached\n-    mask. If a new mask is generated, it will update the cache in the cell.\n-\n-    Args:\n-      inputs: The input tensor whose shape will be used to generate dropout\n-        mask.\n-      training: Boolean tensor, whether its in training mode, dropout will be\n-        ignored in non-training mode.\n-      count: Int, how many dropout mask will be generated. It is useful for cell\n-        that has internal weights fused together.\n-    Returns:\n-      List of mask tensor, generated or cached mask based on context.\n-    \"\"\"\n-    if self.recurrent_dropout == 0:\n-      return None\n-    init_kwargs = dict(inputs=inputs, training=training, count=count)\n-    return self._recurrent_dropout_mask_cache.setdefault(kwargs=init_kwargs)\n-\n-  def __getstate__(self):\n-    # Used for deepcopy. The caching can't be pickled by python, since it will\n-    # contain tensor and graph.\n-    state = super(DropoutRNNCellMixin, self).__getstate__()\n-    state.pop('_dropout_mask_cache', None)\n-    state.pop('_recurrent_dropout_mask_cache', None)\n-    return state\n-\n-  def __setstate__(self, state):\n-    state['_dropout_mask_cache'] = backend.ContextValueCache(\n-        self._create_dropout_mask)\n-    state['_recurrent_dropout_mask_cache'] = backend.ContextValueCache(\n-        self._create_recurrent_dropout_mask)\n-    super(DropoutRNNCellMixin, self).__setstate__(state)\n-\n-\n-@keras_export('keras.layers.SimpleRNNCell')\n-class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n-  \"\"\"Cell class for SimpleRNN.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  This class processes one step within the whole time sequence input, whereas\n-  `tf.keras.layer.SimpleRNN` processes the whole sequence.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs. Default:\n-      `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix, used for the linear transformation of the recurrent state.\n-      Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix. Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs. Default: 0.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state. Default: 0.\n-\n-  Call arguments:\n-    inputs: A 2D tensor, with shape of `[batch, feature]`.\n-    states: A 2D tensor with shape of `[batch, units]`, which is the state from\n-      the previous time step. For timestep 0, the initial state provided by user\n-      will be feed to cell.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-\n-  Examples:\n-\n-  ```python\n-  inputs = np.random.random([32, 10, 8]).astype(np.float32)\n-  rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))\n-\n-  output = rnn(inputs)  # The output has shape `[32, 4]`.\n-\n-  rnn = tf.keras.layers.RNN(\n-      tf.keras.layers.SimpleRNNCell(4),\n-      return_sequences=True,\n-      return_state=True)\n-\n-  # whole_sequence_output has shape `[32, 10, 4]`.\n-  # final_state has shape `[32, 4]`.\n-  whole_sequence_output, final_state = rnn(inputs)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               **kwargs):\n-    if units < 0:\n-      raise ValueError(f'Received an invalid value for argument `units`, '\n-                       f'expected a positive integer, got {units}.')\n-    # By default use cached variable under v2 mode, see b/143699808.\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n-    else:\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(SimpleRNNCell, self).__init__(**kwargs)\n-    self.units = units\n-    self.activation = activations.get(activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-    self.state_size = self.units\n-    self.output_size = self.units\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    default_caching_device = _caching_device(self)\n-    self.kernel = self.add_weight(\n-        shape=(input_shape[-1], self.units),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        caching_device=default_caching_device)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint,\n-        caching_device=default_caching_device)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          shape=(self.units,),\n-          name='bias',\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          caching_device=default_caching_device)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs, states, training=None):\n-    prev_output = states[0] if tf.nest.is_nested(states) else states\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        prev_output, training)\n-\n-    if dp_mask is not None:\n-      h = backend.dot(inputs * dp_mask, self.kernel)\n-    else:\n-      h = backend.dot(inputs, self.kernel)\n-    if self.bias is not None:\n-      h = backend.bias_add(h, self.bias)\n-\n-    if rec_dp_mask is not None:\n-      prev_output = prev_output * rec_dp_mask\n-    output = h + backend.dot(prev_output, self.recurrent_kernel)\n-    if self.activation is not None:\n-      output = self.activation(output)\n-\n-    new_state = [output] if tf.nest.is_nested(states) else output\n-    return output, new_state\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout\n-    }\n-    config.update(_config_for_enable_caching_device(self))\n-    base_config = super(SimpleRNNCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.SimpleRNN')\n-class SimpleRNN(RNN):\n-  \"\"\"Fully-connected RNN where the output is to be fed back to input.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass None, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs. Default:\n-      `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix, used for the linear transformation of the recurrent state.\n-      Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\"). Default: `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix.  Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for the linear transformation of the inputs.\n-      Default: 0.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for the linear transformation of the\n-      recurrent state. Default: 0.\n-    return_sequences: Boolean. Whether to return the last output\n-      in the output sequence, or the full sequence. Default: `False`.\n-    return_state: Boolean. Whether to return the last state\n-      in addition to the output. Default: `False`\n-    go_backwards: Boolean (default False).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    unroll: Boolean (default False).\n-      If True, the network will be unrolled,\n-      else a symbolic loop will be used.\n-      Unrolling can speed-up a RNN,\n-      although it tends to be more memory-intensive.\n-      Unrolling is only suitable for short sequences.\n-\n-  Call arguments:\n-    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n-    mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n-      a given timestep should be masked. An individual `True` entry indicates\n-      that the corresponding timestep should be utilized, while a `False` entry\n-      indicates that the corresponding timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or\n-      `recurrent_dropout` is used.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n-\n-  Examples:\n-\n-  ```python\n-  inputs = np.random.random([32, 10, 8]).astype(np.float32)\n-  simple_rnn = tf.keras.layers.SimpleRNN(4)\n-\n-  output = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n-\n-  simple_rnn = tf.keras.layers.SimpleRNN(\n-      4, return_sequences=True, return_state=True)\n-\n-  # whole_sequence_output has shape `[32, 10, 4]`.\n-  # final_state has shape `[32, 4]`.\n-  whole_sequence_output, final_state = simple_rnn(inputs)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               **kwargs):\n-    if 'implementation' in kwargs:\n-      kwargs.pop('implementation')\n-      logging.warning('The `implementation` argument '\n-                      'in `SimpleRNN` has been deprecated. '\n-                      'Please remove it from your layer call.')\n-    if 'enable_caching_device' in kwargs:\n-      cell_kwargs = {'enable_caching_device':\n-                     kwargs.pop('enable_caching_device')}\n-    else:\n-      cell_kwargs = {}\n-    cell = SimpleRNNCell(\n-        units,\n-        activation=activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        dtype=kwargs.get('dtype'),\n-        trainable=kwargs.get('trainable', True),\n-        **cell_kwargs)\n-    super(SimpleRNN, self).__init__(\n-        cell,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        unroll=unroll,\n-        **kwargs)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-    self.input_spec = [InputSpec(ndim=3)]\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(SimpleRNN, self).call(\n-        inputs, mask=mask, training=training, initial_state=initial_state)\n-\n-  @property\n-  def units(self):\n-    return self.cell.units\n-\n-  @property\n-  def activation(self):\n-    return self.cell.activation\n-\n-  @property\n-  def use_bias(self):\n-    return self.cell.use_bias\n-\n-  @property\n-  def kernel_initializer(self):\n-    return self.cell.kernel_initializer\n-\n-  @property\n-  def recurrent_initializer(self):\n-    return self.cell.recurrent_initializer\n-\n-  @property\n-  def bias_initializer(self):\n-    return self.cell.bias_initializer\n-\n-  @property\n-  def kernel_regularizer(self):\n-    return self.cell.kernel_regularizer\n-\n-  @property\n-  def recurrent_regularizer(self):\n-    return self.cell.recurrent_regularizer\n-\n-  @property\n-  def bias_regularizer(self):\n-    return self.cell.bias_regularizer\n-\n-  @property\n-  def kernel_constraint(self):\n-    return self.cell.kernel_constraint\n-\n-  @property\n-  def recurrent_constraint(self):\n-    return self.cell.recurrent_constraint\n-\n-  @property\n-  def bias_constraint(self):\n-    return self.cell.bias_constraint\n-\n-  @property\n-  def dropout(self):\n-    return self.cell.dropout\n-\n-  @property\n-  def recurrent_dropout(self):\n-    return self.cell.recurrent_dropout\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout\n-    }\n-    base_config = super(SimpleRNN, self).get_config()\n-    config.update(_config_for_enable_caching_device(self.cell))\n-    del base_config['cell']\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    if 'implementation' in config:\n-      config.pop('implementation')\n-    return cls(**config)\n-\n-\n-@keras_export(v1=['keras.layers.GRUCell'])\n-class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n-  \"\"\"Cell class for the GRU layer.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass None, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-      Default: hard sigmoid (`hard_sigmoid`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix,\n-      used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-    reset_after: GRU convention (whether to apply reset gate after or\n-      before matrix multiplication). False = \"before\" (default),\n-      True = \"after\" (cuDNN compatible).\n-\n-  Call arguments:\n-    inputs: A 2D tensor.\n-    states: List of state tensors corresponding to the previous timestep.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               reset_after=False,\n-               **kwargs):\n-    if units < 0:\n-      raise ValueError(f'Received an invalid value for argument `units`, '\n-                       f'expected a positive integer, got {units}.')\n-    # By default use cached variable under v2 mode, see b/143699808.\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n-    else:\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(GRUCell, self).__init__(**kwargs)\n-    self.units = units\n-    self.activation = activations.get(activation)\n-    self.recurrent_activation = activations.get(recurrent_activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-\n-    implementation = kwargs.pop('implementation', 1)\n-    if self.recurrent_dropout != 0 and implementation != 1:\n-      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n-      self.implementation = 1\n-    else:\n-      self.implementation = implementation\n-    self.reset_after = reset_after\n-    self.state_size = self.units\n-    self.output_size = self.units\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    input_dim = input_shape[-1]\n-    default_caching_device = _caching_device(self)\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 3),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        caching_device=default_caching_device)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 3),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint,\n-        caching_device=default_caching_device)\n-\n-    if self.use_bias:\n-      if not self.reset_after:\n-        bias_shape = (3 * self.units,)\n-      else:\n-        # separate biases for input and recurrent kernels\n-        # Note: the shape is intentionally different from CuDNNGRU biases\n-        # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n-        # when loading and converting saved weights.\n-        bias_shape = (2, 3 * self.units)\n-      self.bias = self.add_weight(shape=bias_shape,\n-                                  name='bias',\n-                                  initializer=self.bias_initializer,\n-                                  regularizer=self.bias_regularizer,\n-                                  constraint=self.bias_constraint,\n-                                  caching_device=default_caching_device)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs, states, training=None):\n-    h_tm1 = states[0] if tf.nest.is_nested(states) else states  # previous memory\n-\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        h_tm1, training, count=3)\n-\n-    if self.use_bias:\n-      if not self.reset_after:\n-        input_bias, recurrent_bias = self.bias, None\n-      else:\n-        input_bias, recurrent_bias = tf.unstack(self.bias)\n-\n-    if self.implementation == 1:\n-      if 0. < self.dropout < 1.:\n-        inputs_z = inputs * dp_mask[0]\n-        inputs_r = inputs * dp_mask[1]\n-        inputs_h = inputs * dp_mask[2]\n-      else:\n-        inputs_z = inputs\n-        inputs_r = inputs\n-        inputs_h = inputs\n-\n-      x_z = backend.dot(inputs_z, self.kernel[:, :self.units])\n-      x_r = backend.dot(inputs_r, self.kernel[:, self.units:self.units * 2])\n-      x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2:])\n-\n-      if self.use_bias:\n-        x_z = backend.bias_add(x_z, input_bias[:self.units])\n-        x_r = backend.bias_add(x_r, input_bias[self.units: self.units * 2])\n-        x_h = backend.bias_add(x_h, input_bias[self.units * 2:])\n-\n-      if 0. < self.recurrent_dropout < 1.:\n-        h_tm1_z = h_tm1 * rec_dp_mask[0]\n-        h_tm1_r = h_tm1 * rec_dp_mask[1]\n-        h_tm1_h = h_tm1 * rec_dp_mask[2]\n-      else:\n-        h_tm1_z = h_tm1\n-        h_tm1_r = h_tm1\n-        h_tm1_h = h_tm1\n-\n-      recurrent_z = backend.dot(h_tm1_z, self.recurrent_kernel[:, :self.units])\n-      recurrent_r = backend.dot(\n-          h_tm1_r, self.recurrent_kernel[:, self.units:self.units * 2])\n-      if self.reset_after and self.use_bias:\n-        recurrent_z = backend.bias_add(recurrent_z, recurrent_bias[:self.units])\n-        recurrent_r = backend.bias_add(\n-            recurrent_r, recurrent_bias[self.units:self.units * 2])\n-\n-      z = self.recurrent_activation(x_z + recurrent_z)\n-      r = self.recurrent_activation(x_r + recurrent_r)\n-\n-      # reset gate applied after/before matrix multiplication\n-      if self.reset_after:\n-        recurrent_h = backend.dot(\n-            h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n-        if self.use_bias:\n-          recurrent_h = backend.bias_add(\n-              recurrent_h, recurrent_bias[self.units * 2:])\n-        recurrent_h = r * recurrent_h\n-      else:\n-        recurrent_h = backend.dot(\n-            r * h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n-\n-      hh = self.activation(x_h + recurrent_h)\n-    else:\n-      if 0. < self.dropout < 1.:\n-        inputs = inputs * dp_mask[0]\n-\n-      # inputs projected by all gate matrices at once\n-      matrix_x = backend.dot(inputs, self.kernel)\n-      if self.use_bias:\n-        # biases: bias_z_i, bias_r_i, bias_h_i\n-        matrix_x = backend.bias_add(matrix_x, input_bias)\n-\n-      x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n-\n-      if self.reset_after:\n-        # hidden state projected by all gate matrices at once\n-        matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n-        if self.use_bias:\n-          matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n-      else:\n-        # hidden state projected separately for update/reset and new\n-        matrix_inner = backend.dot(\n-            h_tm1, self.recurrent_kernel[:, :2 * self.units])\n-\n-      recurrent_z, recurrent_r, recurrent_h = tf.split(\n-          matrix_inner, [self.units, self.units, -1], axis=-1)\n-\n-      z = self.recurrent_activation(x_z + recurrent_z)\n-      r = self.recurrent_activation(x_r + recurrent_r)\n-\n-      if self.reset_after:\n-        recurrent_h = r * recurrent_h\n-      else:\n-        recurrent_h = backend.dot(\n-            r * h_tm1, self.recurrent_kernel[:, 2 * self.units:])\n-\n-      hh = self.activation(x_h + recurrent_h)\n-    # previous and candidate state mixed by update gate\n-    h = z * h_tm1 + (1 - z) * hh\n-    new_state = [h] if tf.nest.is_nested(states) else h\n-    return h, new_state\n-\n-  def get_config(self):\n-    config = {\n-        'units': self.units,\n-        'activation': activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias': self.use_bias,\n-        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer': initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint': constraints.serialize(self.bias_constraint),\n-        'dropout': self.dropout,\n-        'recurrent_dropout': self.recurrent_dropout,\n-        'implementation': self.implementation,\n-        'reset_after': self.reset_after\n-    }\n-    config.update(_config_for_enable_caching_device(self))\n-    base_config = super(GRUCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n-\n-\n-@keras_export(v1=['keras.layers.GRU'])\n-class GRU(RNN):\n-  \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n-\n-  There are two variants. The default one is based on 1406.1078v3 and\n-  has reset gate applied to hidden state before matrix multiplication. The\n-  other one is based on original 1406.1078v1 and has the order reversed.\n-\n-  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n-  inference on CPU. Thus it has separate biases for `kernel` and\n-  `recurrent_kernel`. Use `'reset_after'=True` and\n-  `recurrent_activation='sigmoid'`.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-      Default: hard sigmoid (`hard_sigmoid`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix, used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\")..\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-    return_sequences: Boolean. Whether to return the last output\n-      in the output sequence, or the full sequence.\n-    return_state: Boolean. Whether to return the last state\n-      in addition to the output.\n-    go_backwards: Boolean (default False).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    unroll: Boolean (default False).\n-      If True, the network will be unrolled,\n-      else a symbolic loop will be used.\n-      Unrolling can speed-up a RNN,\n-      although it tends to be more memory-intensive.\n-      Unrolling is only suitable for short sequences.\n-    time_major: The shape format of the `inputs` and `outputs` tensors.\n-      If True, the inputs and outputs will be in shape\n-      `(timesteps, batch, ...)`, whereas in the False case, it will be\n-      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n-      efficient because it avoids transposes at the beginning and end of the\n-      RNN calculation. However, most TensorFlow data is batch-major, so by\n-      default this function accepts input and emits output in batch-major\n-      form.\n-    reset_after: GRU convention (whether to apply reset gate after or\n-      before matrix multiplication). False = \"before\" (default),\n-      True = \"after\" (cuDNN compatible).\n-\n-  Call arguments:\n-    inputs: A 3D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked. An individual `True` entry indicates\n-      that the corresponding timestep should be utilized, while a `False`\n-      entry indicates that the corresponding timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or\n-      `recurrent_dropout` is used.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               reset_after=False,\n-               **kwargs):\n-    implementation = kwargs.pop('implementation', 1)\n-    if implementation == 0:\n-      logging.warning('`implementation=0` has been deprecated, '\n-                      'and now defaults to `implementation=1`.'\n-                      'Please update your layer call.')\n-    if 'enable_caching_device' in kwargs:\n-      cell_kwargs = {'enable_caching_device':\n-                     kwargs.pop('enable_caching_device')}\n-    else:\n-      cell_kwargs = {}\n-    cell = GRUCell(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=implementation,\n-        reset_after=reset_after,\n-        dtype=kwargs.get('dtype'),\n-        trainable=kwargs.get('trainable', True),\n-        **cell_kwargs)\n-    super(GRU, self).__init__(\n-        cell,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        unroll=unroll,\n-        **kwargs)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-    self.input_spec = [InputSpec(ndim=3)]\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(GRU, self).call(\n-        inputs, mask=mask, training=training, initial_state=initial_state)\n-\n-  @property\n-  def units(self):\n-    return self.cell.units\n-\n-  @property\n-  def activation(self):\n-    return self.cell.activation\n-\n-  @property\n-  def recurrent_activation(self):\n-    return self.cell.recurrent_activation\n-\n-  @property\n-  def use_bias(self):\n-    return self.cell.use_bias\n-\n-  @property\n-  def kernel_initializer(self):\n-    return self.cell.kernel_initializer\n-\n-  @property\n-  def recurrent_initializer(self):\n-    return self.cell.recurrent_initializer\n-\n-  @property\n-  def bias_initializer(self):\n-    return self.cell.bias_initializer\n-\n-  @property\n-  def kernel_regularizer(self):\n-    return self.cell.kernel_regularizer\n-\n-  @property\n-  def recurrent_regularizer(self):\n-    return self.cell.recurrent_regularizer\n-\n-  @property\n-  def bias_regularizer(self):\n-    return self.cell.bias_regularizer\n-\n-  @property\n-  def kernel_constraint(self):\n-    return self.cell.kernel_constraint\n-\n-  @property\n-  def recurrent_constraint(self):\n-    return self.cell.recurrent_constraint\n-\n-  @property\n-  def bias_constraint(self):\n-    return self.cell.bias_constraint\n-\n-  @property\n-  def dropout(self):\n-    return self.cell.dropout\n-\n-  @property\n-  def recurrent_dropout(self):\n-    return self.cell.recurrent_dropout\n-\n-  @property\n-  def implementation(self):\n-    return self.cell.implementation\n-\n-  @property\n-  def reset_after(self):\n-    return self.cell.reset_after\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout,\n-        'implementation':\n-            self.implementation,\n-        'reset_after':\n-            self.reset_after\n-    }\n-    config.update(_config_for_enable_caching_device(self.cell))\n-    base_config = super(GRU, self).get_config()\n-    del base_config['cell']\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    if 'implementation' in config and config['implementation'] == 0:\n-      config['implementation'] = 1\n-    return cls(**config)\n-\n-\n-@keras_export(v1=['keras.layers.LSTMCell'])\n-class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n-  \"\"\"Cell class for the LSTM layer.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-      Default: hard sigmoid (`hard_sigmoid`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix,\n-      used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean.\n-      If True, add 1 to the bias of the forget gate at initialization.\n-      Setting it to true will also force `bias_initializer=\"zeros\"`.\n-      This is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-\n-  Call arguments:\n-    inputs: A 2D tensor.\n-    states: List of state tensors corresponding to the previous timestep.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               **kwargs):\n-    if units < 0:\n-      raise ValueError(f'Received an invalid value for argument `units`, '\n-                       f'expected a positive integer, got {units}.')\n-    # By default use cached variable under v2 mode, see b/143699808.\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n-    else:\n-      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(LSTMCell, self).__init__(**kwargs)\n-    self.units = units\n-    self.activation = activations.get(activation)\n-    self.recurrent_activation = activations.get(recurrent_activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.recurrent_initializer = initializers.get(recurrent_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.unit_forget_bias = unit_forget_bias\n-\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.recurrent_constraint = constraints.get(recurrent_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-\n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n-    implementation = kwargs.pop('implementation', 1)\n-    if self.recurrent_dropout != 0 and implementation != 1:\n-      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n-      self.implementation = 1\n-    else:\n-      self.implementation = implementation\n-    self.state_size = [self.units, self.units]\n-    self.output_size = self.units\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    default_caching_device = _caching_device(self)\n-    input_dim = input_shape[-1]\n-    self.kernel = self.add_weight(\n-        shape=(input_dim, self.units * 4),\n-        name='kernel',\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        caching_device=default_caching_device)\n-    self.recurrent_kernel = self.add_weight(\n-        shape=(self.units, self.units * 4),\n-        name='recurrent_kernel',\n-        initializer=self.recurrent_initializer,\n-        regularizer=self.recurrent_regularizer,\n-        constraint=self.recurrent_constraint,\n-        caching_device=default_caching_device)\n-\n-    if self.use_bias:\n-      if self.unit_forget_bias:\n-\n-        def bias_initializer(_, *args, **kwargs):\n-          return backend.concatenate([\n-              self.bias_initializer((self.units,), *args, **kwargs),\n-              initializers.get('ones')((self.units,), *args, **kwargs),\n-              self.bias_initializer((self.units * 2,), *args, **kwargs),\n-          ])\n-      else:\n-        bias_initializer = self.bias_initializer\n-      self.bias = self.add_weight(\n-          shape=(self.units * 4,),\n-          name='bias',\n-          initializer=bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          caching_device=default_caching_device)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n-    \"\"\"Computes carry and output using split kernels.\"\"\"\n-    x_i, x_f, x_c, x_o = x\n-    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n-    i = self.recurrent_activation(\n-        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n-    f = self.recurrent_activation(x_f + backend.dot(\n-        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n-    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n-        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n-    o = self.recurrent_activation(\n-        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n-    return c, o\n-\n-  def _compute_carry_and_output_fused(self, z, c_tm1):\n-    \"\"\"Computes carry and output using fused kernels.\"\"\"\n-    z0, z1, z2, z3 = z\n-    i = self.recurrent_activation(z0)\n-    f = self.recurrent_activation(z1)\n-    c = f * c_tm1 + i * self.activation(z2)\n-    o = self.recurrent_activation(z3)\n-    return c, o\n-\n-  def call(self, inputs, states, training=None):\n-    h_tm1 = states[0]  # previous memory state\n-    c_tm1 = states[1]  # previous carry state\n-\n-    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n-    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n-        h_tm1, training, count=4)\n-\n-    if self.implementation == 1:\n-      if 0 < self.dropout < 1.:\n-        inputs_i = inputs * dp_mask[0]\n-        inputs_f = inputs * dp_mask[1]\n-        inputs_c = inputs * dp_mask[2]\n-        inputs_o = inputs * dp_mask[3]\n-      else:\n-        inputs_i = inputs\n-        inputs_f = inputs\n-        inputs_c = inputs\n-        inputs_o = inputs\n-      k_i, k_f, k_c, k_o = tf.split(\n-          self.kernel, num_or_size_splits=4, axis=1)\n-      x_i = backend.dot(inputs_i, k_i)\n-      x_f = backend.dot(inputs_f, k_f)\n-      x_c = backend.dot(inputs_c, k_c)\n-      x_o = backend.dot(inputs_o, k_o)\n-      if self.use_bias:\n-        b_i, b_f, b_c, b_o = tf.split(\n-            self.bias, num_or_size_splits=4, axis=0)\n-        x_i = backend.bias_add(x_i, b_i)\n-        x_f = backend.bias_add(x_f, b_f)\n-        x_c = backend.bias_add(x_c, b_c)\n-        x_o = backend.bias_add(x_o, b_o)\n-\n-      if 0 < self.recurrent_dropout < 1.:\n-        h_tm1_i = h_tm1 * rec_dp_mask[0]\n-        h_tm1_f = h_tm1 * rec_dp_mask[1]\n-        h_tm1_c = h_tm1 * rec_dp_mask[2]\n-        h_tm1_o = h_tm1 * rec_dp_mask[3]\n-      else:\n-        h_tm1_i = h_tm1\n-        h_tm1_f = h_tm1\n-        h_tm1_c = h_tm1\n-        h_tm1_o = h_tm1\n-      x = (x_i, x_f, x_c, x_o)\n-      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n-      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n-    else:\n-      if 0. < self.dropout < 1.:\n-        inputs = inputs * dp_mask[0]\n-      z = backend.dot(inputs, self.kernel)\n-      z += backend.dot(h_tm1, self.recurrent_kernel)\n-      if self.use_bias:\n-        z = backend.bias_add(z, self.bias)\n-\n-      z = tf.split(z, num_or_size_splits=4, axis=1)\n-      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n-\n-    h = o * self.activation(c)\n-    return h, [h, c]\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'unit_forget_bias':\n-            self.unit_forget_bias,\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout,\n-        'implementation':\n-            self.implementation\n-    }\n-    config.update(_config_for_enable_caching_device(self))\n-    base_config = super(LSTMCell, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n-    return list(_generate_zero_filled_state_for_cell(\n-        self, inputs, batch_size, dtype))\n-\n-\n-@keras_export('keras.experimental.PeepholeLSTMCell')\n-class PeepholeLSTMCell(LSTMCell):\n-  \"\"\"Equivalent to LSTMCell class but adds peephole connections.\n-\n-  Peephole connections allow the gates to utilize the previous internal state as\n-  well as the previous hidden state (which is what LSTMCell is limited to).\n-  This allows PeepholeLSTMCell to better learn precise timings over LSTMCell.\n-\n-  From [Gers et al., 2002](\n-    http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf):\n-\n-  \"We find that LSTM augmented by 'peephole connections' from its internal\n-  cells to its multiplicative gates can learn the fine distinction between\n-  sequences of spikes spaced either 50 or 49 time steps apart without the help\n-  of any short training exemplars.\"\n-\n-  The peephole implementation is based on:\n-\n-  [Sak et al., 2014](https://research.google.com/pubs/archive/43905.pdf)\n-\n-  Example:\n-\n-  ```python\n-  # Create 2 PeepholeLSTMCells\n-  peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]\n-  # Create a layer composed sequentially of the peephole LSTM cells.\n-  layer = RNN(peephole_lstm_cells)\n-  input = keras.Input((timesteps, input_dim))\n-  output = layer(input)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               **kwargs):\n-    warnings.warn(\n-        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '\n-        'and will be removed in a future version. '\n-        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '\n-        'instead.',\n-        stacklevel=2)\n-    super(PeepholeLSTMCell, self).__init__(\n-        units=units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 1),\n-        **kwargs)\n-\n-  def build(self, input_shape):\n-    super(PeepholeLSTMCell, self).build(input_shape)\n-    # The following are the weight matrices for the peephole connections. These\n-    # are multiplied with the previous internal state during the computation of\n-    # carry and output.\n-    self.input_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='input_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-    self.forget_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='forget_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-    self.output_gate_peephole_weights = self.add_weight(\n-        shape=(self.units,),\n-        name='output_gate_peephole_weights',\n-        initializer=self.kernel_initializer)\n-\n-  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n-    x_i, x_f, x_c, x_o = x\n-    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n-    i = self.recurrent_activation(\n-        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]) +\n-        self.input_gate_peephole_weights * c_tm1)\n-    f = self.recurrent_activation(x_f + backend.dot(\n-        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]) +\n-                                  self.forget_gate_peephole_weights * c_tm1)\n-    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n-        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n-    o = self.recurrent_activation(\n-        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]) +\n-        self.output_gate_peephole_weights * c)\n-    return c, o\n-\n-  def _compute_carry_and_output_fused(self, z, c_tm1):\n-    z0, z1, z2, z3 = z\n-    i = self.recurrent_activation(z0 +\n-                                  self.input_gate_peephole_weights * c_tm1)\n-    f = self.recurrent_activation(z1 +\n-                                  self.forget_gate_peephole_weights * c_tm1)\n-    c = f * c_tm1 + i * self.activation(z2)\n-    o = self.recurrent_activation(z3 + self.output_gate_peephole_weights * c)\n-    return c, o\n-\n-\n-@keras_export(v1=['keras.layers.LSTM'])\n-class LSTM(RNN):\n-  \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n-\n-   Note that this cell is not optimized for performance on GPU. Please use\n-  `tf.compat.v1.keras.layers.CuDNNLSTM` for better performance on GPU.\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-      Default: hard sigmoid (`hard_sigmoid`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs..\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix,\n-      used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean.\n-      If True, add 1 to the bias of the forget gate at initialization.\n-      Setting it to true will also force `bias_initializer=\"zeros\"`.\n-      This is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\").\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n-      the linear transformation of the recurrent state.\n-    return_sequences: Boolean. Whether to return the last output\n-      in the output sequence, or the full sequence.\n-    return_state: Boolean. Whether to return the last state\n-      in addition to the output.\n-    go_backwards: Boolean (default False).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    unroll: Boolean (default False).\n-      If True, the network will be unrolled,\n-      else a symbolic loop will be used.\n-      Unrolling can speed-up a RNN,\n-      although it tends to be more memory-intensive.\n-      Unrolling is only suitable for short sequences.\n-    time_major: The shape format of the `inputs` and `outputs` tensors.\n-      If True, the inputs and outputs will be in shape\n-      `(timesteps, batch, ...)`, whereas in the False case, it will be\n-      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n-      efficient because it avoids transposes at the beginning and end of the\n-      RNN calculation. However, most TensorFlow data is batch-major, so by\n-      default this function accepts input and emits output in batch-major\n-      form.\n-\n-  Call arguments:\n-    inputs: A 3D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked. An individual `True` entry indicates\n-      that the corresponding timestep should be utilized, while a `False`\n-      entry indicates that the corresponding timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or\n-      `recurrent_dropout` is used.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='hard_sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               **kwargs):\n-    implementation = kwargs.pop('implementation', 1)\n-    if implementation == 0:\n-      logging.warning('`implementation=0` has been deprecated, '\n-                      'and now defaults to `implementation=1`.'\n-                      'Please update your layer call.')\n-    if 'enable_caching_device' in kwargs:\n-      cell_kwargs = {'enable_caching_device':\n-                     kwargs.pop('enable_caching_device')}\n-    else:\n-      cell_kwargs = {}\n-    cell = LSTMCell(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=implementation,\n-        dtype=kwargs.get('dtype'),\n-        trainable=kwargs.get('trainable', True),\n-        **cell_kwargs)\n-    super(LSTM, self).__init__(\n-        cell,\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        unroll=unroll,\n-        **kwargs)\n-    self.activity_regularizer = regularizers.get(activity_regularizer)\n-    self.input_spec = [InputSpec(ndim=3)]\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(LSTM, self).call(\n-        inputs, mask=mask, training=training, initial_state=initial_state)\n-\n-  @property\n-  def units(self):\n-    return self.cell.units\n-\n-  @property\n-  def activation(self):\n-    return self.cell.activation\n-\n-  @property\n-  def recurrent_activation(self):\n-    return self.cell.recurrent_activation\n-\n-  @property\n-  def use_bias(self):\n-    return self.cell.use_bias\n-\n-  @property\n-  def kernel_initializer(self):\n-    return self.cell.kernel_initializer\n-\n-  @property\n-  def recurrent_initializer(self):\n-    return self.cell.recurrent_initializer\n-\n-  @property\n-  def bias_initializer(self):\n-    return self.cell.bias_initializer\n-\n-  @property\n-  def unit_forget_bias(self):\n-    return self.cell.unit_forget_bias\n-\n-  @property\n-  def kernel_regularizer(self):\n-    return self.cell.kernel_regularizer\n-\n-  @property\n-  def recurrent_regularizer(self):\n-    return self.cell.recurrent_regularizer\n-\n-  @property\n-  def bias_regularizer(self):\n-    return self.cell.bias_regularizer\n-\n-  @property\n-  def kernel_constraint(self):\n-    return self.cell.kernel_constraint\n-\n-  @property\n-  def recurrent_constraint(self):\n-    return self.cell.recurrent_constraint\n-\n-  @property\n-  def bias_constraint(self):\n-    return self.cell.bias_constraint\n-\n-  @property\n-  def dropout(self):\n-    return self.cell.dropout\n-\n-  @property\n-  def recurrent_dropout(self):\n-    return self.cell.recurrent_dropout\n-\n-  @property\n-  def implementation(self):\n-    return self.cell.implementation\n-\n-  def get_config(self):\n-    config = {\n-        'units':\n-            self.units,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'recurrent_activation':\n-            activations.serialize(self.recurrent_activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'recurrent_initializer':\n-            initializers.serialize(self.recurrent_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'unit_forget_bias':\n-            self.unit_forget_bias,\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'recurrent_regularizer':\n-            regularizers.serialize(self.recurrent_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'recurrent_constraint':\n-            constraints.serialize(self.recurrent_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint),\n-        'dropout':\n-            self.dropout,\n-        'recurrent_dropout':\n-            self.recurrent_dropout,\n-        'implementation':\n-            self.implementation\n-    }\n-    config.update(_config_for_enable_caching_device(self.cell))\n-    base_config = super(LSTM, self).get_config()\n-    del base_config['cell']\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    if 'implementation' in config and config['implementation'] == 0:\n-      config['implementation'] = 1\n-    return cls(**config)\n-\n-\n-def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n-  def dropped_inputs():\n-    return generator.dropout(ones, rate)\n-\n-  if count > 1:\n-    return [\n-        backend.in_train_phase(dropped_inputs, ones, training=training)\n-        for _ in range(count)\n-    ]\n-  return backend.in_train_phase(dropped_inputs, ones, training=training)\n-\n-\n-def _standardize_args(inputs, initial_state, constants, num_constants):\n-  \"\"\"Standardizes `__call__` to a single list of tensor inputs.\n-\n-  When running a model loaded from a file, the input tensors\n-  `initial_state` and `constants` can be passed to `RNN.__call__()` as part\n-  of `inputs` instead of by the dedicated keyword arguments. This method\n-  makes sure the arguments are separated and that `initial_state` and\n-  `constants` are lists of tensors (or None).\n-\n-  Args:\n-    inputs: Tensor or list/tuple of tensors. which may include constants\n-      and initial states. In that case `num_constant` must be specified.\n-    initial_state: Tensor or list of tensors or None, initial states.\n-    constants: Tensor or list of tensors or None, constant tensors.\n-    num_constants: Expected number of constants (if constants are passed as\n-      part of the `inputs` list.\n-\n-  Returns:\n-    inputs: Single tensor or tuple of tensors.\n-    initial_state: List of tensors or None.\n-    constants: List of tensors or None.\n-  \"\"\"\n-  if isinstance(inputs, list):\n-    # There are several situations here:\n-    # In the graph mode, __call__ will be only called once. The initial_state\n-    # and constants could be in inputs (from file loading).\n-    # In the eager mode, __call__ will be called twice, once during\n-    # rnn_layer(inputs=input_t, constants=c_t, ...), and second time will be\n-    # model.fit/train_on_batch/predict with real np data. In the second case,\n-    # the inputs will contain initial_state and constants as eager tensor.\n-    #\n-    # For either case, the real input is the first item in the list, which\n-    # could be a nested structure itself. Then followed by initial_states, which\n-    # could be a list of items, or list of list if the initial_state is complex\n-    # structure, and finally followed by constants which is a flat list.\n-    assert initial_state is None and constants is None\n@@ -1,1786 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n-\"\"\"Recurrent layers for TF 2.\"\"\"\n-\n-\n-import uuid\n-\n-from keras import activations\n-from keras import backend\n-from keras.engine import base_layer\n-from keras.engine.input_spec import InputSpec\n-from keras.layers import recurrent\n-\n-import tensorflow.compat.v2 as tf\n-from tensorflow.python.eager.context import get_device_name\n-from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-# The following string constants are used by Defun approach for unified backend\n-# of LSTM and GRU.\n-_FUNCTION_API_NAME_ATTRIBUTE = 'api_implements'\n-_FUNCTION_DEVICE_ATTRIBUTE = 'api_preferred_device'\n-_CPU_DEVICE_NAME = 'CPU'\n-_GPU_DEVICE_NAME = 'GPU'\n-\n-# The following number constants are used to represent the runtime of the defun\n-# backend function. Since the CPU/GPU implementation are mathematically same, we\n-# need some signal for the function to indicate which function is executed. This\n-# is for testing purpose to verify the correctness of swapping backend function.\n-_RUNTIME_UNKNOWN = 0\n-_RUNTIME_CPU = 1\n-_RUNTIME_GPU = 2\n-\n-_CUDNN_AVAILABLE_MSG = 'Layer %s will use cuDNN kernels when running on GPU.'\n-_CUDNN_NOT_AVAILABLE_MSG = ('Layer %s will not use cuDNN kernels since it '\n-                            'doesn\\'t meet the criteria. It will '\n-                            'use a generic GPU kernel as fallback when running '\n-                            'on GPU.')\n-\n-\n-def _use_new_code():\n-  return False\n-\n-\n-# TODO(b/169707691): The wrapper can be removed if TFLite doesn't need to rely\n-# on supportive attributes from LSTM/GRU.\n-class _DefunWrapper:\n-  \"\"\"A wrapper with no deep copy of the Defun in LSTM/GRU layer.\"\"\"\n-\n-  def __init__(self, time_major, go_backwards, layer_name):\n-    self.time_major = time_major\n-    self.go_backwards = go_backwards\n-    self.layer_name = layer_name\n-    if self.layer_name not in ['lstm', 'gru']:\n-      raise ValueError('Defun wrapper only applies to LSTM and GRU layer, '\n-                       'but given {}'.format(self.layer_name))\n-    # The first two attributes are added to support TFLite use case.\n-    supportive_attributes = {\n-        'time_major': self.time_major,\n-        'go_backwards': self.go_backwards,\n-        _FUNCTION_API_NAME_ATTRIBUTE: self.layer_name + '_' + str(uuid.uuid4())\n-    }\n-    if self.layer_name == 'lstm':\n-      layer_func = lstm_with_backend_selection\n-    else:\n-      layer_func = gru_with_backend_selection\n-\n-    self.defun_layer = tf.__internal__.function.defun_with_attributes(\n-        layer_func,\n-        attributes=supportive_attributes,\n-        autograph=False)\n-\n-  def __deepcopy__(self, memo):\n-    new_wrapper = type(self)(\n-        self.time_major, self.go_backwards, self.layer_name)\n-    memo[id(self)] = new_wrapper\n-    return new_wrapper\n-\n-\n-@keras_export('keras.layers.GRUCell', v1=[])\n-class GRUCell(recurrent.GRUCell):\n-  \"\"\"Cell class for the GRU layer.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  This class processes one step within the whole time sequence input, whereas\n-  `tf.keras.layer.GRU` processes the whole sequence.\n-\n-  For example:\n-\n-  >>> inputs = tf.random.normal([32, 10, 8])\n-  >>> rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(4))\n-  >>> output = rnn(inputs)\n-  >>> print(output.shape)\n-  (32, 4)\n-  >>> rnn = tf.keras.layers.RNN(\n-  ...    tf.keras.layers.GRUCell(4),\n-  ...    return_sequences=True,\n-  ...    return_state=True)\n-  >>> whole_sequence_output, final_state = rnn(inputs)\n-  >>> print(whole_sequence_output.shape)\n-  (32, 10, 4)\n-  >>> print(final_state.shape)\n-  (32, 4)\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use. Default: hyperbolic tangent\n-      (`tanh`). If you pass None, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n-      applied (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs. Default:\n-      `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix, used for the linear transformation of the recurrent state.\n-      Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix. Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the\n-      linear transformation of the inputs. Default: 0.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state. Default: 0.\n-    reset_after: GRU convention (whether to apply reset gate after or\n-      before matrix multiplication). False = \"before\",\n-      True = \"after\" (default and cuDNN compatible).\n-\n-  Call arguments:\n-    inputs: A 2D tensor, with shape of `[batch, feature]`.\n-    states: A 2D tensor with shape of `[batch, units]`, which is the state from\n-      the previous time step. For timestep 0, the initial state provided by user\n-      will be feed to cell.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               reset_after=True,\n-               **kwargs):\n-    super(GRUCell, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        reset_after=reset_after,\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.GRU', v1=[])\n-class GRU(recurrent.DropoutRNNCellMixin,\n-          recurrent.GRU, base_layer.BaseRandomLayer):\n-  \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  Based on available runtime hardware and constraints, this layer\n-  will choose different implementations (cuDNN-based or pure-TensorFlow)\n-  to maximize the performance. If a GPU is available and all\n-  the arguments to the layer meet the requirement of the cuDNN kernel\n-  (see below for details), the layer will use a fast cuDNN implementation.\n-\n-  The requirements to use the cuDNN implementation are:\n-\n-  1. `activation` == `tanh`\n-  2. `recurrent_activation` == `sigmoid`\n-  3. `recurrent_dropout` == 0\n-  4. `unroll` is `False`\n-  5. `use_bias` is `True`\n-  6. `reset_after` is `True`\n-  7. Inputs, if use masking, are strictly right-padded.\n-  8. Eager execution is enabled in the outermost context.\n-\n-  There are two variants of the GRU implementation. The default one is based on\n-  [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n-  state before matrix multiplication. The other one is based on\n-  [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n-\n-  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n-  inference on CPU. Thus it has separate biases for `kernel` and\n-  `recurrent_kernel`. To use this variant, set `reset_after=True` and\n-  `recurrent_activation='sigmoid'`.\n-\n-  For example:\n-\n-  >>> inputs = tf.random.normal([32, 10, 8])\n-  >>> gru = tf.keras.layers.GRU(4)\n-  >>> output = gru(inputs)\n-  >>> print(output.shape)\n-  (32, 4)\n-  >>> gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n-  >>> whole_sequence_output, final_state = gru(inputs)\n-  >>> print(whole_sequence_output.shape)\n-  (32, 10, 4)\n-  >>> print(final_state.shape)\n-  (32, 4)\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n-      Default: sigmoid (`sigmoid`).\n-      If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs. Default:\n-      `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-       weights matrix, used for the linear transformation of the recurrent\n-       state. Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\"). Default: `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix. Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs. Default: 0.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state. Default: 0.\n-    return_sequences: Boolean. Whether to return the last output\n-      in the output sequence, or the full sequence. Default: `False`.\n-    return_state: Boolean. Whether to return the last state in addition to the\n-      output. Default: `False`.\n-    go_backwards: Boolean (default `False`).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    unroll: Boolean (default False).\n-      If True, the network will be unrolled,\n-      else a symbolic loop will be used.\n-      Unrolling can speed-up a RNN,\n-      although it tends to be more memory-intensive.\n-      Unrolling is only suitable for short sequences.\n-    time_major: The shape format of the `inputs` and `outputs` tensors.\n-      If True, the inputs and outputs will be in shape\n-      `[timesteps, batch, feature]`, whereas in the False case, it will be\n-      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n-      efficient because it avoids transposes at the beginning and end of the\n-      RNN calculation. However, most TensorFlow data is batch-major, so by\n-      default this function accepts input and emits output in batch-major\n-      form.\n-    reset_after: GRU convention (whether to apply reset gate after or\n-      before matrix multiplication). False = \"before\",\n-      True = \"after\" (default and cuDNN compatible).\n-\n-  Call arguments:\n-    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n-    mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n-      a given timestep should be masked  (optional, defaults to `None`).\n-      An individual `True` entry indicates that the corresponding timestep\n-      should be utilized, while a `False` entry indicates that the\n-      corresponding timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or\n-      `recurrent_dropout` is used  (optional, defaults to `None`).\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell  (optional, defaults to `None` which causes creation\n-      of zero-filled initial state tensors).\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               unroll=False,\n-               time_major=False,\n-               reset_after=True,\n-               **kwargs):\n-    # return_runtime is a flag for testing, which shows the real backend\n-    # implementation chosen by grappler in graph mode.\n-    self._return_runtime = kwargs.pop('return_runtime', False)\n-\n-    super(GRU, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        unroll=unroll,\n-        time_major=time_major,\n-        reset_after=reset_after,\n-        **kwargs)\n-    # GPU kernel uses following setting by default and not configurable.\n-    self._could_use_gpu_kernel = (\n-        self.activation in (activations.tanh, tf.tanh) and\n-        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and\n-        recurrent_dropout == 0 and not unroll and use_bias and\n-        reset_after and tf.compat.v1.executing_eagerly_outside_functions())\n-    if tf.config.list_logical_devices('GPU'):\n-      # Only show the message when there is GPU available, user will not care\n-      # about the cuDNN if there isn't any GPU.\n-      if self._could_use_gpu_kernel:\n-        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)\n-      else:\n-        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)\n-\n-    if _use_new_code():\n-      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    # The input should be dense, padded with zeros. If a ragged input is fed\n-    # into the layer, it is padded and the row lengths are used for masking.\n-    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n-    is_ragged_input = (row_lengths is not None)\n-    self._validate_args_if_ragged(is_ragged_input, mask)\n-\n-    # GRU does not support constants. Ignore it during process.\n-    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n-\n-    if isinstance(mask, list):\n-      mask = mask[0]\n-\n-    input_shape = backend.int_shape(inputs)\n-    timesteps = input_shape[0] if self.time_major else input_shape[1]\n-\n-    if not self._could_use_gpu_kernel:\n-      kwargs = {'training': training}\n-      self._maybe_reset_cell_dropout_mask(self.cell)\n-\n-      def step(cell_inputs, cell_states):\n-        return self.cell(cell_inputs, cell_states, **kwargs)\n-\n-      last_output, outputs, states = backend.rnn(\n-          step,\n-          inputs,\n-          initial_state,\n-          constants=None,\n-          go_backwards=self.go_backwards,\n-          mask=mask,\n-          unroll=self.unroll,\n-          input_length=row_lengths if row_lengths is not None else timesteps,\n-          time_major=self.time_major,\n-          zero_output_for_mask=self.zero_output_for_mask)\n-      # This is a dummy tensor for testing purpose.\n-      runtime = _runtime(_RUNTIME_UNKNOWN)\n-    else:\n-      last_output, outputs, runtime, states = self._defun_gru_call(\n-          inputs, initial_state, training, mask, row_lengths)\n-\n-    if self.stateful:\n-      updates = [tf.compat.v1.assign(self.states[0],\n-                                     tf.cast(states[0], self.states[0].dtype))]\n-      self.add_update(updates)\n-\n-    if self.return_sequences:\n-      output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n-    else:\n-      output = last_output\n-\n-    if self.return_state:\n-      return [output] + list(states)\n-    elif self._return_runtime:\n-      return output, runtime\n-    else:\n-      return output\n-\n-  def _defun_gru_call(self, inputs, initial_state, training, mask,\n-                      sequence_lengths):\n-    # Use the new defun approach for backend implementation swap.\n-    # Note that different implementations need to have same function\n-    # signature, eg, the tensor parameters need to have same shape and dtypes.\n-\n-    self.reset_dropout_mask()\n-    dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n-    if dropout_mask is not None:\n-      inputs = inputs * dropout_mask[0]\n-\n-    if _use_new_code():\n-      gru_kwargs = {\n-          'inputs': inputs,\n-          'init_h': _read_variable_value(initial_state[0]),\n-          'kernel': _read_variable_value(self.cell.kernel),\n-          'recurrent_kernel': _read_variable_value(self.cell.recurrent_kernel),\n-          'bias': _read_variable_value(self.cell.bias),\n-          'mask': mask,\n-          'time_major': self.time_major,\n-          'go_backwards': self.go_backwards,\n-          'sequence_lengths': sequence_lengths,\n-          'zero_output_for_mask': self.zero_output_for_mask\n-      }\n-      (last_output, outputs, new_h,\n-       runtime) = self._defun_wrapper.defun_layer(**gru_kwargs)\n-    else:\n-      gpu_gru_kwargs = {\n-          'inputs': inputs,\n-          'init_h': _read_variable_value(initial_state[0]),\n-          'kernel': _read_variable_value(self.cell.kernel),\n-          'recurrent_kernel': _read_variable_value(self.cell.recurrent_kernel),\n-          'bias': _read_variable_value(self.cell.bias),\n-          'mask': mask,\n-          'time_major': self.time_major,\n-          'go_backwards': self.go_backwards,\n-          'sequence_lengths': sequence_lengths\n-      }\n-      normal_gru_kwargs = gpu_gru_kwargs.copy()\n-      normal_gru_kwargs.update({\n-          'zero_output_for_mask': self.zero_output_for_mask,\n-      })\n-\n-      if tf.executing_eagerly():\n-        device_type = _get_context_device_type()\n-        can_use_gpu = (\n-            # Either user specified GPU or unspecified but GPU is available.\n-            (device_type == _GPU_DEVICE_NAME or\n-             (device_type is None and tf.config.list_logical_devices('GPU'))) and\n-            (mask is None or is_cudnn_supported_inputs(mask, self.time_major)))\n-        # Under eager context, check the device placement and prefer the\n-        if can_use_gpu:\n-          last_output, outputs, new_h, runtime = gpu_gru(**gpu_gru_kwargs)\n-        else:\n-          last_output, outputs, new_h, runtime = standard_gru(\n-              **normal_gru_kwargs)\n-      else:\n-        last_output, outputs, new_h, runtime = gru_with_backend_selection(\n-            **normal_gru_kwargs)\n-\n-    states = [new_h]\n-    return last_output, outputs, runtime, states\n-\n-\n-def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n-                 time_major, go_backwards, sequence_lengths,\n-                 zero_output_for_mask):\n-  \"\"\"GRU with standard kernel implementation.\n-\n-  This implementation can be run on all types of hardware.\n-\n-  This implementation lifts out all the layer weights and make them function\n-  parameters. It has same number of tensor input params as the cuDNN\n-  counterpart. The RNN step logic has been simplified, eg dropout and mask is\n-  removed since cuDNN implementation does not support that.\n-\n-  Args:\n-    inputs: Input tensor of GRU layer.\n-    init_h: Initial state tensor for the cell output.\n-    kernel: Weights for cell kernel.\n-    recurrent_kernel: Weights for cell recurrent kernel.\n-    bias: Weights for cell kernel bias and recurrent bias. The bias contains the\n-      combined input_bias and recurrent_bias.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked. An individual `True` entry indicates\n-      that the corresponding timestep should be utilized, while a `False` entry\n-      indicates that the corresponding timestep should be ignored.\n-    time_major: Boolean, whether the inputs are in the format of\n-      [time, batch, feature] or [batch, time, feature].\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    sequence_lengths: The lengths of all sequences coming from a variable length\n-      input, such as ragged tensors. If the input has a fixed timestep size,\n-      this should be None.\n-    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-\n-  Returns:\n-    last_output: output tensor for the last timestep, which has shape\n-      [batch, units].\n-    outputs: output tensor for all timesteps, which has shape\n-      [batch, time, units].\n-    state_0: the cell output, which has same shape as init_h.\n-    runtime: constant string tensor which indicate real runtime hardware. This\n-      value is for testing purpose and should be used by user.\n-  \"\"\"\n-  input_shape = backend.int_shape(inputs)\n-  timesteps = input_shape[0] if time_major else input_shape[1]\n-\n-  input_bias, recurrent_bias = tf.unstack(bias)\n-\n-  def step(cell_inputs, cell_states):\n-    \"\"\"Step function that will be used by Keras RNN backend.\"\"\"\n-    h_tm1 = cell_states[0]\n-\n-    # inputs projected by all gate matrices at once\n-    matrix_x = backend.dot(cell_inputs, kernel)\n-    matrix_x = backend.bias_add(matrix_x, input_bias)\n-\n-    x_z, x_r, x_h = tf.split(matrix_x, 3, axis=1)\n-\n-    # hidden state projected by all gate matrices at once\n-    matrix_inner = backend.dot(h_tm1, recurrent_kernel)\n-    matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n-\n-    recurrent_z, recurrent_r, recurrent_h = tf.split(matrix_inner, 3,\n-                                                            axis=1)\n-    z = tf.sigmoid(x_z + recurrent_z)\n-    r = tf.sigmoid(x_r + recurrent_r)\n-    hh = tf.tanh(x_h + r * recurrent_h)\n-\n-    # previous and candidate state mixed by update gate\n-    h = z * h_tm1 + (1 - z) * hh\n-    return h, [h]\n-\n-  last_output, outputs, new_states = backend.rnn(\n-      step,\n-      inputs, [init_h],\n-      constants=None,\n-      unroll=False,\n-      time_major=time_major,\n-      mask=mask,\n-      go_backwards=go_backwards,\n-      input_length=sequence_lengths\n-      if sequence_lengths is not None else timesteps,\n-      zero_output_for_mask=zero_output_for_mask)\n-  return last_output, outputs, new_states[0], _runtime(_RUNTIME_CPU)\n-\n-\n-def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n-            go_backwards, sequence_lengths):\n-  \"\"\"GRU with cuDNN implementation which is only available for GPU.\"\"\"\n-  if mask is not None:\n-    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-\n-  if not time_major and sequence_lengths is None:\n-    inputs = tf.transpose(inputs, perm=(1, 0, 2))\n-    seq_axis, batch_axis = (0, 1)\n-  else:\n-    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n-  # For init_h, cuDNN expects one more dim of num_layers before or after batch\n-  # dim for time major or batch major inputs respectively\n-  init_h = tf.expand_dims(init_h, axis=seq_axis)\n-\n-  weights = tf.split(kernel, 3, axis=1)\n-  weights += tf.split(recurrent_kernel, 3, axis=1)\n-  # Note that the bias was initialized as shape (2, 3 * units), flat it into\n-  # (6 * units)\n-  bias = tf.split(backend.flatten(bias), 6)\n-\n-  if tf.sysconfig.get_build_info()['is_cuda_build']:\n-    # Note that the gate order for cuDNN is different from the canonical format.\n-    # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need\n-    # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.\n-    # z is update gate weights.\n-    # r is reset gate weights.\n-    # h is output gate weights.\n-    weights[0], weights[1] = weights[1], weights[0]\n-    weights[3], weights[4] = weights[4], weights[3]\n-    bias[0], bias[1] = bias[1], bias[0]\n-    bias[3], bias[4] = bias[4], bias[3]\n-\n-  params = _canonical_to_params(\n-      weights=weights,\n-      biases=bias,\n-      shape=tf.constant([-1]),\n-      transpose_weights=True)\n-\n-  if sequence_lengths is not None:\n-    if go_backwards:\n-      # Three reversals are required. E.g.,\n-      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n-      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]\n-      # output_from_cudnn = [6, 5, 4, 0, 0]\n-      # expected_output = [0, 0, 6, 5 ,4]\n-      inputs = tf.reverse_sequence(\n-          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n-    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV3(\n-        input=inputs,\n-        input_h=init_h,\n-        input_c=0,\n-        params=params,\n-        is_training=True,\n-        rnn_mode='gru',\n-        sequence_lengths=sequence_lengths,\n-        time_major=time_major)\n-    if go_backwards:\n-      outputs = tf.reverse_sequence(\n-          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n-      outputs = tf.reverse(outputs, axis=[seq_axis])\n-  else:\n-    if go_backwards:\n-      # Reverse axis 0 since the input is already convert to time major.\n-      inputs = tf.reverse(inputs, axis=[0])\n-    outputs, h, _, _ = tf.raw_ops.CudnnRNN(\n-        input=inputs, input_h=init_h, input_c=0, params=params,\n-        is_training=True, rnn_mode='gru')\n-\n-  last_output = outputs[-1]\n-  if not time_major and sequence_lengths is None:\n-    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n-  h = tf.squeeze(h, axis=seq_axis)\n-\n-  # In the case of variable length input, the cudnn kernel will fill zeros for\n-  # the output, whereas the default keras behavior is to bring over the previous\n-  # output for t-1, so that in the return_sequence=False case, user can quickly\n-  # get the final effect output instead just 0s at the last timestep.\n-  # In order to mimic the default keras behavior, we copy the final h state as\n-  # the last_output, since it is numerically same as the output.\n-  if sequence_lengths is not None:\n-    last_output = h\n-\n-  return last_output, outputs, h, _runtime(_RUNTIME_GPU)\n-\n-\n-def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n-                               mask, time_major, go_backwards, sequence_lengths,\n-                               zero_output_for_mask):\n-  \"\"\"Call the GRU with optimized backend kernel selection.\n-\n-  Under the hood, this function will create two TF function, one with the most\n-  generic kernel and can run on all device condition, and the second one with\n-  cuDNN specific kernel, which can only run on GPU.\n-\n-  The first function will be called with normal_lstm_params, while the second\n-  function is not called, but only registered in the graph. The Grappler will\n-  do the proper graph rewrite and swap the optimized TF function based on the\n-  device placement.\n-\n-  Args:\n-    inputs: Input tensor of GRU layer.\n-    init_h: Initial state tensor for the cell output.\n-    kernel: Weights for cell kernel.\n-    recurrent_kernel: Weights for cell recurrent kernel.\n-    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n-      is used in this case.\n-    mask: Boolean tensor for mask out the steps within sequence.\n-      An individual `True` entry indicates that the corresponding timestep\n-      should be utilized, while a `False` entry indicates that the corresponding\n-      timestep should be ignored.\n-    time_major: Boolean, whether the inputs are in the format of\n-      [time, batch, feature] or [batch, time, feature].\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    sequence_lengths: The lengths of all sequences coming from a variable length\n-      input, such as ragged tensors. If the input has a fixed timestep size,\n-      this should be None.\n-    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-\n-  Returns:\n-    List of output tensors, same as standard_gru.\n-  \"\"\"\n-  params = {\n-      'inputs': inputs,\n-      'init_h': init_h,\n-      'kernel': kernel,\n-      'recurrent_kernel': recurrent_kernel,\n-      'bias': bias,\n-      'mask': mask,\n-      'time_major': time_major,\n-      'go_backwards': go_backwards,\n-      'sequence_lengths': sequence_lengths,\n-      'zero_output_for_mask': zero_output_for_mask,\n-  }\n-\n-  def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,\n-                            mask, time_major, go_backwards, sequence_lengths,\n-                            zero_output_for_mask):\n-    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n-    if mask is None:\n-      return gpu_gru(\n-          inputs=inputs,\n-          init_h=init_h,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n-\n-    def cudnn_gru_fn():\n-      return gpu_gru(\n-          inputs=inputs,\n-          init_h=init_h,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n-\n-    def standard_gru_fn():\n-      return standard_gru(\n-          inputs=inputs,\n-          init_h=init_h,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths,\n-          zero_output_for_mask=zero_output_for_mask)\n-\n-    return tf.cond(\n-        is_cudnn_supported_inputs(mask, time_major),\n-        true_fn=cudnn_gru_fn,\n-        false_fn=standard_gru_fn)\n-\n-  if _use_new_code():\n-    # Chooses the implementation dynamically based on the running device.\n-    (last_output, outputs, new_h,\n-     runtime) = tf.__internal__.execute_fn_for_device(\n-         {\n-             _CPU_DEVICE_NAME: lambda: standard_gru(**params),\n-             _GPU_DEVICE_NAME: lambda: gpu_gru_with_fallback(**params)\n-         }, lambda: standard_gru(**params))\n-  else:\n-    # Each time a `tf.function` is called, we will give it a unique\n-    # identifiable API name, so that Grappler won't get confused when it\n-    # sees multiple GRU layers added into same graph, and it will be able\n-    # to pair up the different implementations across them.\n-    api_name = 'gru_' + str(uuid.uuid4())\n-    supportive_attribute = {\n-        'time_major': time_major,\n-        'go_backwards': go_backwards,\n-    }\n-    defun_standard_gru = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,\n-                                                 standard_gru,\n-                                                 supportive_attribute)\n-    defun_gpu_gru = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,\n-                                            gpu_gru_with_fallback,\n-                                            supportive_attribute)\n-\n-    # Call the normal GRU impl and register the cuDNN impl function. The\n-    # grappler will kick in during session execution to optimize the graph.\n-    last_output, outputs, new_h, runtime = defun_standard_gru(**params)\n-    _function_register(defun_gpu_gru, **params)\n-\n-  return last_output, outputs, new_h, runtime\n-\n-\n-@keras_export('keras.layers.LSTMCell', v1=[])\n-class LSTMCell(recurrent.LSTMCell):\n-  \"\"\"Cell class for the LSTM layer.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  This class processes one step within the whole time sequence input, whereas\n-  `tf.keras.layer.LSTM` processes the whole sequence.\n-\n-  For example:\n-\n-  >>> inputs = tf.random.normal([32, 10, 8])\n-  >>> rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n-  >>> output = rnn(inputs)\n-  >>> print(output.shape)\n-  (32, 4)\n-  >>> rnn = tf.keras.layers.RNN(\n-  ...    tf.keras.layers.LSTMCell(4),\n-  ...    return_sequences=True,\n-  ...    return_state=True)\n-  >>> whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)\n-  >>> print(whole_seq_output.shape)\n-  (32, 10, 4)\n-  >>> print(final_memory_state.shape)\n-  (32, 4)\n-  >>> print(final_carry_state.shape)\n-  (32, 4)\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use. Default: hyperbolic tangent\n-      (`tanh`). If you pass `None`, no activation is applied (ie. \"linear\"\n-      activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is applied\n-      (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs. Default: `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-      Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n-      the forget gate at initialization. Setting it to true will also force\n-      `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n-        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix. Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs. Default: 0.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state. Default: 0.\n-\n-  Call arguments:\n-    inputs: A 2D tensor, with shape of `[batch, feature]`.\n-    states: List of 2 tensors that corresponding to the cell's units. Both of\n-      them have shape `[batch, units]`, the first tensor is the memory state\n-      from previous time step, the second tensor is the carry state from\n-      previous time step. For timestep 0, the initial state provided by user\n-      will be feed to cell.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. Only relevant when `dropout` or\n-      `recurrent_dropout` is used.\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               **kwargs):\n-    super(LSTMCell, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.LSTM', v1=[])\n-class LSTM(recurrent.DropoutRNNCellMixin,\n-           recurrent.LSTM, base_layer.BaseRandomLayer):\n-  \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n-\n-  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n-  for details about the usage of RNN API.\n-\n-  Based on available runtime hardware and constraints, this layer\n-  will choose different implementations (cuDNN-based or pure-TensorFlow)\n-  to maximize the performance. If a GPU is available and all\n-  the arguments to the layer meet the requirement of the cuDNN kernel\n-  (see below for details), the layer will use a fast cuDNN implementation.\n-\n-  The requirements to use the cuDNN implementation are:\n-\n-  1. `activation` == `tanh`\n-  2. `recurrent_activation` == `sigmoid`\n-  3. `recurrent_dropout` == 0\n-  4. `unroll` is `False`\n-  5. `use_bias` is `True`\n-  6. Inputs, if use masking, are strictly right-padded.\n-  7. Eager execution is enabled in the outermost context.\n-\n-  For example:\n-\n-  >>> inputs = tf.random.normal([32, 10, 8])\n-  >>> lstm = tf.keras.layers.LSTM(4)\n-  >>> output = lstm(inputs)\n-  >>> print(output.shape)\n-  (32, 4)\n-  >>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n-  >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n-  >>> print(whole_seq_output.shape)\n-  (32, 10, 4)\n-  >>> print(final_memory_state.shape)\n-  (32, 4)\n-  >>> print(final_carry_state.shape)\n-  (32, 4)\n-\n-  Args:\n-    units: Positive integer, dimensionality of the output space.\n-    activation: Activation function to use.\n-      Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation\n-      is applied (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use for the recurrent step.\n-      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n-      applied (ie. \"linear\" activation: `a(x) = x`).\n-    use_bias: Boolean (default `True`), whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-      the linear transformation of the inputs. Default: `glorot_uniform`.\n-    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n-      matrix, used for the linear transformation of the recurrent state.\n-      Default: `orthogonal`.\n-    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n-    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n-      the forget gate at initialization. Setting it to true will also force\n-      `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n-          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_regularizer: Regularizer function applied to the\n-      `recurrent_kernel` weights matrix. Default: `None`.\n-    bias_regularizer: Regularizer function applied to the bias vector. Default:\n-      `None`.\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\"). Default: `None`.\n-    kernel_constraint: Constraint function applied to the `kernel` weights\n-      matrix. Default: `None`.\n-    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-      weights matrix. Default: `None`.\n-    bias_constraint: Constraint function applied to the bias vector. Default:\n-      `None`.\n-    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-      transformation of the inputs. Default: 0.\n-    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-      the linear transformation of the recurrent state. Default: 0.\n-    return_sequences: Boolean. Whether to return the last output. in the output\n-      sequence, or the full sequence. Default: `False`.\n-    return_state: Boolean. Whether to return the last state in addition to the\n-      output. Default: `False`.\n-    go_backwards: Boolean (default `False`). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    stateful: Boolean (default `False`). If True, the last state for each sample\n-      at index i in a batch will be used as initial state for the sample of\n-      index i in the following batch.\n-    time_major: The shape format of the `inputs` and `outputs` tensors.\n-      If True, the inputs and outputs will be in shape\n-      `[timesteps, batch, feature]`, whereas in the False case, it will be\n-      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n-      efficient because it avoids transposes at the beginning and end of the\n-      RNN calculation. However, most TensorFlow data is batch-major, so by\n-      default this function accepts input and emits output in batch-major\n-      form.\n-    unroll: Boolean (default `False`). If True, the network will be unrolled,\n-      else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n-      it tends to be more memory-intensive. Unrolling is only suitable for short\n-      sequences.\n-\n-  Call arguments:\n-    inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n-    mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n-      a given timestep should be masked (optional, defaults to `None`).\n-      An individual `True` entry indicates that the corresponding timestep\n-      should be utilized, while a `False` entry indicates that the corresponding\n-      timestep should be ignored.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or\n-      `recurrent_dropout` is used (optional, defaults to `None`).\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell (optional, defaults to `None` which causes creation\n-      of zero-filled initial state tensors).\n-  \"\"\"\n-\n-  def __init__(self,\n-               units,\n-               activation='tanh',\n-               recurrent_activation='sigmoid',\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               recurrent_initializer='orthogonal',\n-               bias_initializer='zeros',\n-               unit_forget_bias=True,\n-               kernel_regularizer=None,\n-               recurrent_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               recurrent_constraint=None,\n-               bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n-               return_sequences=False,\n-               return_state=False,\n-               go_backwards=False,\n-               stateful=False,\n-               time_major=False,\n-               unroll=False,\n-               **kwargs):\n-    # return_runtime is a flag for testing, which shows the real backend\n-    # implementation chosen by grappler in graph mode.\n-    self.return_runtime = kwargs.pop('return_runtime', False)\n-\n-    super(LSTM, self).__init__(\n-        units,\n-        activation=activation,\n-        recurrent_activation=recurrent_activation,\n-        use_bias=use_bias,\n-        kernel_initializer=kernel_initializer,\n-        recurrent_initializer=recurrent_initializer,\n-        bias_initializer=bias_initializer,\n-        unit_forget_bias=unit_forget_bias,\n-        kernel_regularizer=kernel_regularizer,\n-        recurrent_regularizer=recurrent_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        kernel_constraint=kernel_constraint,\n-        recurrent_constraint=recurrent_constraint,\n-        bias_constraint=bias_constraint,\n-        dropout=dropout,\n-        recurrent_dropout=recurrent_dropout,\n-        implementation=kwargs.pop('implementation', 2),\n-        return_sequences=return_sequences,\n-        return_state=return_state,\n-        go_backwards=go_backwards,\n-        stateful=stateful,\n-        time_major=time_major,\n-        unroll=unroll,\n-        **kwargs)\n-\n-    self.state_spec = [\n-        InputSpec(shape=(None, dim)) for dim in (self.units, self.units)\n-    ]\n-    self._could_use_gpu_kernel = (\n-        self.activation in (activations.tanh, tf.tanh) and\n-        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and\n-        recurrent_dropout == 0 and not unroll and use_bias and\n-        tf.compat.v1.executing_eagerly_outside_functions())\n-    if tf.config.list_logical_devices('GPU'):\n-      # Only show the message when there is GPU available, user will not care\n-      # about the cuDNN if there isn't any GPU.\n-      if self._could_use_gpu_kernel:\n-        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)\n-      else:\n-        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)\n-\n-    if _use_new_code():\n-      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'lstm')\n-\n-  def call(self, inputs, mask=None, training=None, initial_state=None):\n-    # The input should be dense, padded with zeros. If a ragged input is fed\n-    # into the layer, it is padded and the row lengths are used for masking.\n-    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n-    is_ragged_input = (row_lengths is not None)\n-    self._validate_args_if_ragged(is_ragged_input, mask)\n-\n-    # LSTM does not support constants. Ignore it during process.\n-    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n-\n-    if isinstance(mask, list):\n-      mask = mask[0]\n-\n-    input_shape = backend.int_shape(inputs)\n-    timesteps = input_shape[0] if self.time_major else input_shape[1]\n-\n-    if not self._could_use_gpu_kernel:\n-      # Fall back to use the normal LSTM.\n-      kwargs = {'training': training}\n-      self._maybe_reset_cell_dropout_mask(self.cell)\n-\n-      def step(inputs, states):\n-        return self.cell(inputs, states, **kwargs)\n-\n-      last_output, outputs, states = backend.rnn(\n-          step,\n-          inputs,\n-          initial_state,\n-          constants=None,\n-          go_backwards=self.go_backwards,\n-          mask=mask,\n-          unroll=self.unroll,\n-          input_length=row_lengths if row_lengths is not None else timesteps,\n-          time_major=self.time_major,\n-          zero_output_for_mask=self.zero_output_for_mask)\n-      runtime = _runtime(_RUNTIME_UNKNOWN)\n-    else:\n-      # Use the new defun approach for backend implementation swap.\n-      # Note that different implementations need to have same function\n-      # signature, eg, the tensor parameters need to have same shape and dtypes.\n-      # Since the cuDNN has an extra set of bias, those bias will be passed to\n-      # both normal and cuDNN implementations.\n-      self.reset_dropout_mask()\n-      dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n-      if dropout_mask is not None:\n-        inputs = inputs * dropout_mask[0]\n-      if _use_new_code():\n-        lstm_kwargs = {\n-            'inputs':\n-                inputs,\n-            'init_h':\n-                _read_variable_value(initial_state[0]),\n-            'init_c':\n-                _read_variable_value(initial_state[1]),\n-            'kernel':\n-                _read_variable_value(self.cell.kernel),\n-            'recurrent_kernel':\n-                _read_variable_value(self.cell.recurrent_kernel),\n-            'bias':\n-                _read_variable_value(self.cell.bias),\n-            'mask':\n-                mask,\n-            'time_major':\n-                self.time_major,\n-            'go_backwards':\n-                self.go_backwards,\n-            'sequence_lengths':\n-                row_lengths,\n-            'zero_output_for_mask':\n-                self.zero_output_for_mask,\n-        }\n-        (last_output, outputs, new_h, new_c,\n-         runtime) = self._defun_wrapper.defun_layer(**lstm_kwargs)\n-      else:\n-        gpu_lstm_kwargs = {\n-            'inputs':\n-                inputs,\n-            'init_h':\n-                _read_variable_value(initial_state[0]),\n-            'init_c':\n-                _read_variable_value(initial_state[1]),\n-            'kernel':\n-                _read_variable_value(self.cell.kernel),\n-            'recurrent_kernel':\n-                _read_variable_value(self.cell.recurrent_kernel),\n-            'bias':\n-                _read_variable_value(self.cell.bias),\n-            'mask':\n-                mask,\n-            'time_major':\n-                self.time_major,\n-            'go_backwards':\n-                self.go_backwards,\n-            'sequence_lengths':\n-                row_lengths\n-        }\n-        normal_lstm_kwargs = gpu_lstm_kwargs.copy()\n-        normal_lstm_kwargs.update({\n-            'zero_output_for_mask': self.zero_output_for_mask,\n-        })\n-\n-        if tf.executing_eagerly():\n-          device_type = _get_context_device_type()\n-          can_use_gpu = (\n-              # Either user specified GPU or unspecified but GPU is available.\n-              (device_type == _GPU_DEVICE_NAME or\n-               (device_type is None\n-                and tf.config.list_logical_devices('GPU'))) and\n-              (mask is None or\n-               is_cudnn_supported_inputs(mask, self.time_major)))\n-          # Under eager context, check the device placement and prefer the\n-          # GPU implementation when GPU is available.\n-          if can_use_gpu:\n-            last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n-                **gpu_lstm_kwargs)\n-          else:\n-            last_output, outputs, new_h, new_c, runtime = standard_lstm(\n-                **normal_lstm_kwargs)\n-        else:\n-          (last_output, outputs, new_h, new_c,\n-           runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\n-\n-      states = [new_h, new_c]\n-\n-    if self.stateful:\n-      updates = [\n-          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n-          for self_state, state in zip(self.states, states)\n-      ]\n-      self.add_update(updates)\n-\n-    if self.return_sequences:\n-      output = backend.maybe_convert_to_ragged(\n-          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n-    else:\n-      output = last_output\n-\n-    if self.return_state:\n-      return [output] + list(states)\n-    elif self.return_runtime:\n-      return output, runtime\n-    else:\n-      return output\n-\n-\n-def _canonical_to_params(weights, biases, shape, transpose_weights=False):\n-  \"\"\"Utility function convert variable to cuDNN compatible parameter.\n-\n-  Note that Keras weights for kernels are different from the cuDNN format. Eg.:\n-\n-  ```\n-    Keras                 cuDNN\n-    [[0, 1, 2],  <--->  [[0, 2, 4],\n-     [3, 4, 5]]          [1, 3, 5]]\n-  ```\n-\n-  If the input weights need to be in a unified format, then set\n-  `transpose_weights=True` to convert the weights.\n-\n-  Args:\n-    weights: list of weights for the individual kernels and recurrent kernels.\n-    biases: list of biases for individual gate.\n-    shape: the shape for the converted variables that will be feed to cuDNN.\n-    transpose_weights: boolean, whether to transpose the weights.\n-\n-  Returns:\n-    The converted weights that can be feed to cuDNN ops as param.\n-  \"\"\"\n-  def convert(w):\n-    return tf.transpose(w) if transpose_weights else w\n-\n-  weights = [tf.reshape(convert(x), shape) for x in weights]\n-  biases = [tf.reshape(x, shape) for x in biases]\n-  return tf.concat(weights + biases, axis=0)\n-\n-\n-def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n-                  mask, time_major, go_backwards, sequence_lengths,\n-                  zero_output_for_mask):\n-  \"\"\"LSTM with standard kernel implementation.\n-\n-  This implementation can be run on all types for hardware.\n-\n-  This implementation lifts out all the layer weights and make them function\n-  parameters. It has same number of tensor input params as the cuDNN\n-  counterpart. The RNN step logic has been simplified, eg dropout and mask is\n-  removed since cuDNN implementation does not support that.\n-\n-  Note that the first half of the bias tensor should be ignored by this impl.\n-  The cuDNN impl need an extra set of input gate bias. In order to make the both\n-  function take same shape of parameter, that extra set of bias is also feed\n-  here.\n-\n-  Args:\n-    inputs: input tensor of LSTM layer.\n-    init_h: initial state tensor for the cell output.\n-    init_c: initial state tensor for the cell hidden state.\n-    kernel: weights for cell kernel.\n-    recurrent_kernel: weights for cell recurrent kernel.\n-    bias: weights for cell kernel bias and recurrent bias. Only recurrent bias\n-      is used in this case.\n-    mask: Boolean tensor for mask out the steps within sequence.\n-      An individual `True` entry indicates that the corresponding timestep\n-      should be utilized, while a `False` entry indicates that the corresponding\n-      timestep should be ignored.\n-    time_major: boolean, whether the inputs are in the format of\n-      [time, batch, feature] or [batch, time, feature].\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    sequence_lengths: The lengths of all sequences coming from a variable length\n-      input, such as ragged tensors. If the input has a fixed timestep size,\n-      this should be None.\n-    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-\n-  Returns:\n-    last_output: output tensor for the last timestep, which has shape\n-      [batch, units].\n-    outputs: output tensor for all timesteps, which has shape\n-      [batch, time, units].\n-    state_0: the cell output, which has same shape as init_h.\n-    state_1: the cell hidden state, which has same shape as init_c.\n-    runtime: constant string tensor which indicate real runtime hardware. This\n-      value is for testing purpose and should be used by user.\n-  \"\"\"\n-  input_shape = backend.int_shape(inputs)\n-  timesteps = input_shape[0] if time_major else input_shape[1]\n-\n-  def step(cell_inputs, cell_states):\n-    \"\"\"Step function that will be used by Keras RNN backend.\"\"\"\n-    h_tm1 = cell_states[0]  # previous memory state\n-    c_tm1 = cell_states[1]  # previous carry state\n-\n-    z = backend.dot(cell_inputs, kernel)\n-    z += backend.dot(h_tm1, recurrent_kernel)\n-    z = backend.bias_add(z, bias)\n-\n-    z0, z1, z2, z3 = tf.split(z, 4, axis=1)\n-\n-    i = tf.sigmoid(z0)\n-    f = tf.sigmoid(z1)\n-    c = f * c_tm1 + i * tf.tanh(z2)\n-    o = tf.sigmoid(z3)\n-\n-    h = o * tf.tanh(c)\n-    return h, [h, c]\n-\n-  last_output, outputs, new_states = backend.rnn(\n-      step,\n-      inputs, [init_h, init_c],\n-      constants=None,\n-      unroll=False,\n-      time_major=time_major,\n-      mask=mask,\n-      go_backwards=go_backwards,\n-      input_length=(sequence_lengths\n-                    if sequence_lengths is not None else timesteps),\n-      zero_output_for_mask=zero_output_for_mask)\n-  return (last_output, outputs, new_states[0], new_states[1],\n-          _runtime(_RUNTIME_CPU))\n-\n-\n-def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n-             time_major, go_backwards, sequence_lengths):\n-  \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for GPU.\n-\n-  Note that currently only right padded data is supported, or the result will be\n-  polluted by the unmasked data which should be filtered.\n-\n-  Args:\n-    inputs: Input tensor of LSTM layer.\n-    init_h: Initial state tensor for the cell output.\n-    init_c: Initial state tensor for the cell hidden state.\n-    kernel: Weights for cell kernel.\n-    recurrent_kernel: Weights for cell recurrent kernel.\n-    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n-      is used in this case.\n-    mask: Boolean tensor for mask out the steps within sequence. An individual\n-      `True` entry indicates that the corresponding timestep should be utilized,\n-      while a `False` entry indicates that the corresponding timestep should be\n-      ignored.\n-    time_major: Boolean, whether the inputs are in the format of [time, batch,\n-      feature] or [batch, time, feature].\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    sequence_lengths: The lengths of all sequences coming from a variable length\n-      input, such as ragged tensors. If the input has a fixed timestep size,\n-      this should be None.\n-\n-  Returns:\n-    last_output: Output tensor for the last timestep, which has shape\n-      [batch, units].\n-    outputs: Output tensor for all timesteps, which has shape\n-      [batch, time, units].\n-    state_0: The cell output, which has same shape as init_h.\n-    state_1: The cell hidden state, which has same shape as init_c.\n-    runtime: Constant string tensor which indicate real runtime hardware. This\n-      value is for testing purpose and should not be used by user.\n-  \"\"\"\n-  if mask is not None:\n-    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-\n-  if not time_major and sequence_lengths is None:\n-    inputs = tf.transpose(inputs, perm=(1, 0, 2))\n-    seq_axis, batch_axis = (0, 1)\n-  else:\n-    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n-  # For init_h and init_c, cuDNN expects one more dim of num_layers before or\n-  # after batch dim for time major or batch major inputs respectively\n-  init_h = tf.expand_dims(init_h, axis=seq_axis)\n-  init_c = tf.expand_dims(init_c, axis=seq_axis)\n-\n-  weights = tf.split(kernel, 4, axis=1)\n-  weights += tf.split(recurrent_kernel, 4, axis=1)\n-  # cuDNN has an extra set of bias for inputs, we disable them (setting to 0),\n-  # so that mathematically it is same as the canonical LSTM implementation.\n-  full_bias = tf.concat((tf.zeros_like(bias), bias), 0)\n-\n-  if tf.sysconfig.get_build_info()['is_rocm_build']:\n-    # ROCm MIOpen's weight sequence for LSTM is different from both canonical\n-    # and Cudnn format\n-    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]\n-    # i is input gate weights.\n-    # f is forget gate weights.\n-    # o is output gate weights.\n-    # c is cell gate weights.\n-    weights = [weights[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]\n-    # full_bias is a tensor of shape (8*n,)\n-    full_bias = tf.split(full_bias, 8, axis=0)\n-    full_bias = [full_bias[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]\n-\n-  params = _canonical_to_params(\n-      weights=weights,\n-      biases=tf.split(full_bias, 8),\n-      shape=tf.constant([-1]),\n-      transpose_weights=True)\n-\n-  if sequence_lengths is not None:\n-    if go_backwards:\n-      # Three reversals are required. E.g.,\n-      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n-      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]\n-      # output_from_cudnn = [6, 5, 4, 0, 0]\n-      # expected_output = [0, 0, 6, 5 ,4]\n-      inputs = tf.reverse_sequence(\n-          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n-    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV3(\n-        input=inputs,\n-        input_h=init_h,\n-        input_c=init_c,\n-        params=params,\n-        is_training=True,\n-        rnn_mode='lstm',\n-        sequence_lengths=sequence_lengths,\n-        time_major=time_major)\n-    if go_backwards:\n-      outputs = tf.reverse_sequence(\n-          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n-      outputs = tf.reverse(outputs, axis=[seq_axis])\n-  else:\n-    # # Fill the array with shape [batch] with value of max timesteps.\n-    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],\n-    #                                  array_ops.shape(inputs)[0])\n-    if go_backwards:\n-      # Reverse axis 0 since the input is already convert to time major.\n-      inputs = tf.reverse(inputs, axis=[0])\n-    outputs, h, c, _ = tf.raw_ops.CudnnRNN(\n-        input=inputs, input_h=init_h, input_c=init_c, params=params,\n-        is_training=True, rnn_mode='lstm')\n-\n-  last_output = outputs[-1]\n-  if not time_major and sequence_lengths is None:\n-    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n-  h = tf.squeeze(h, axis=seq_axis)\n-  c = tf.squeeze(c, axis=seq_axis)\n-\n-  # In the case of variable length input, the cudnn kernel will fill zeros for\n-  # the output, whereas the default keras behavior is to bring over the previous\n-  # output for t-1, so that in the return_sequence=False case, user can quickly\n-  # get the final effect output instead just 0s at the last timestep.\n-  # In order to mimic the default keras behavior, we copy the final h state as\n-  # the last_output, since it is numerically same as the output.\n-  if sequence_lengths is not None:\n-    last_output = h\n-  return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)\n-\n-\n-def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n-                                recurrent_kernel, bias, mask, time_major,\n-                                go_backwards, sequence_lengths,\n-                                zero_output_for_mask):\n-  \"\"\"Call the LSTM with optimized backend kernel selection.\n-\n-  Under the hood, this function will create two TF function, one with the most\n-  generic kernel and can run on all device condition, and the second one with\n-  cuDNN specific kernel, which can only run on GPU.\n-\n-  The first function will be called with normal_lstm_params, while the second\n-  function is not called, but only registered in the graph. The Grappler will\n-  do the proper graph rewrite and swap the optimized TF function based on the\n-  device placement.\n-\n-  Args:\n-    inputs: Input tensor of LSTM layer.\n-    init_h: Initial state tensor for the cell output.\n-    init_c: Initial state tensor for the cell hidden state.\n-    kernel: Weights for cell kernel.\n-    recurrent_kernel: Weights for cell recurrent kernel.\n-    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n-      is used in this case.\n-    mask: Boolean tensor for mask out the steps within sequence.\n-      An individual `True` entry indicates that the corresponding timestep\n-      should be utilized, while a `False` entry indicates that the corresponding\n-      timestep should be ignored.\n-    time_major: Boolean, whether the inputs are in the format of\n-      [time, batch, feature] or [batch, time, feature].\n-    go_backwards: Boolean (default False). If True, process the input sequence\n-      backwards and return the reversed sequence.\n-    sequence_lengths: The lengths of all sequences coming from a variable length\n-      input, such as ragged tensors. If the input has a fixed timestep size,\n-      this should be None.\n-    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-\n-  Returns:\n-    List of output tensors, same as standard_lstm.\n-  \"\"\"\n-  params = {\n-      'inputs': inputs,\n-      'init_h': init_h,\n-      'init_c': init_c,\n-      'kernel': kernel,\n-      'recurrent_kernel': recurrent_kernel,\n-      'bias': bias,\n-      'mask': mask,\n-      'time_major': time_major,\n-      'go_backwards': go_backwards,\n-      'sequence_lengths': sequence_lengths,\n-      'zero_output_for_mask': zero_output_for_mask,\n-  }\n-\n-  def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,\n-                             bias, mask, time_major, go_backwards,\n-                             sequence_lengths, zero_output_for_mask):\n-    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n-    if mask is None:\n-      return gpu_lstm(\n-          inputs=inputs,\n-          init_h=init_h,\n-          init_c=init_c,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n-\n-    def cudnn_lstm_fn():\n-      return gpu_lstm(\n-          inputs=inputs,\n-          init_h=init_h,\n-          init_c=init_c,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n-\n-    def stardard_lstm_fn():\n-      return standard_lstm(\n-          inputs=inputs,\n-          init_h=init_h,\n-          init_c=init_c,\n-          kernel=kernel,\n-          recurrent_kernel=recurrent_kernel,\n-          bias=bias,\n-          mask=mask,\n-          time_major=time_major,\n-          go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths,\n-          zero_output_for_mask=zero_output_for_mask)\n-\n-    return tf.cond(\n-        is_cudnn_supported_inputs(mask, time_major),\n-        true_fn=cudnn_lstm_fn,\n-        false_fn=stardard_lstm_fn)\n-\n-  if _use_new_code():\n-    # Chooses the implementation dynamically based on the running device.\n-    (last_output, outputs, new_h, new_c,\n-     runtime) = tf.__internal__.execute_fn_for_device(\n-         {\n-             _CPU_DEVICE_NAME: lambda: standard_lstm(**params),\n-             _GPU_DEVICE_NAME: lambda: gpu_lstm_with_fallback(**params)\n-         }, lambda: standard_lstm(**params))\n-  else:\n-    # Each time a `tf.function` is called, we will give it a unique\n-    # identifiable API name, so that Grappler won't get confused when it\n-    # sees multiple LSTM layers added into same graph, and it will be able\n-    # to pair up the different implementations across them.\n-    api_name = 'lstm_' + str(uuid.uuid4())\n-    supportive_attribute = {\n-        'time_major': time_major,\n-        'go_backwards': go_backwards,\n-    }\n-    defun_standard_lstm = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,\n-                                                  standard_lstm,\n-                                                  supportive_attribute)\n-    defun_gpu_lstm = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,\n-                                             gpu_lstm_with_fallback,\n-                                             supportive_attribute)\n-\n-    # Call the normal LSTM impl and register the cuDNN impl function. The\n-    # grappler will kick in during session execution to optimize the graph.\n-    last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)\n-    _function_register(defun_gpu_lstm, **params)\n-\n-  return last_output, outputs, new_h, new_c, runtime\n-\n-\n-def is_sequence_right_padded(mask):\n-  \"\"\"Check the mask tensor and see if it right padded.\n-\n-  For cuDNN kernel, it uses the sequence length param to skip the tailing\n-  timestep. If the data is left padded, or not a strict right padding (has\n-  masked value in the middle of the sequence), then cuDNN kernel won't be work\n-  properly in those cases.\n-\n-  Left padded data: [[False, False, True, True, True]].\n-  Right padded data: [[True, True, True, False, False]].\n-  Mixture of mask/unmasked data: [[True, False, True, False, False]].\n-\n-  Note that for the mixed data example above, the actually data RNN should see\n-  are those 2 Trues (index 0 and 2), the index 1 False should be ignored and not\n-  pollute the internal states.\n-\n-  Args:\n-    mask: the Boolean tensor with shape [batch, timestep]\n-\n-  Returns:\n-    boolean scalar tensor, whether the mask is strictly right padded.\n-  \"\"\"\n-  max_seq_length = tf.shape(mask)[1]\n-  count_of_true = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n-  right_padded_mask = tf.sequence_mask(\n-      count_of_true, maxlen=max_seq_length)\n-  return tf.reduce_all(tf.equal(mask, right_padded_mask))\n-\n-\n-def has_fully_masked_sequence(mask):\n-  # See https://github.com/tensorflow/tensorflow/issues/33148 for more details.\n-  # Cudnn kernel will error out if the input sequence contains any fully masked\n-  # data. We walk around this issue by rerouting the computation to standard\n-  # kernel, until the issue on cudnn side has been fixed.\n-  # For a fully masked sequence, it will contain all Falses. To make it easy to\n-  # check, we inverse the boolean, check if any of the sequence has all True.\n-  return tf.reduce_any(\n-      tf.reduce_all(\n-          tf.logical_not(mask),\n-          axis=1))\n-\n-\n-def is_cudnn_supported_inputs(mask, time_major):\n-  if time_major:\n-    mask = tf.transpose(mask)\n-\n-  return tf.logical_and(\n-      is_sequence_right_padded(mask),\n-      tf.logical_not(has_fully_masked_sequence(mask)))\n-\n-\n-def calculate_sequence_by_mask(mask, time_major):\n-  \"\"\"Calculate the sequence length tensor (1-D) based on the masking tensor.\n-\n-  The masking tensor is a 2D boolean tensor with shape [batch, timestep]. For\n-  any timestep that should be masked, the corresponding field will be False.\n-  Consider the following example:\n-    a = [[True, True, False, False],\n-         [True, True, True, False]]\n-  It is a (2, 4) tensor, and the corresponding sequence length result should be\n-  1D tensor with value [2, 3]. Note that the masking tensor must be right\n-  padded that could be checked by, e.g., `is_sequence_right_padded()`.\n-\n-  Args:\n-    mask: Boolean tensor with shape [batch, timestep] or [timestep, batch] if\n-      time_major=True.\n-    time_major: Boolean, which indicates whether the mask is time major or batch\n-      major.\n-  Returns:\n-    sequence_length: 1D int32 tensor.\n-  \"\"\"\n-  timestep_index = 0 if time_major else 1\n-  return tf.reduce_sum(tf.cast(mask, tf.int32),\n-                             axis=timestep_index)\n-\n-\n-def _generate_defun_backend(unique_api_name, preferred_device, func,\n-                            supportive_attributes):\n-  function_attributes = {\n-      _FUNCTION_API_NAME_ATTRIBUTE: unique_api_name,\n-      _FUNCTION_DEVICE_ATTRIBUTE: preferred_device,\n-  }\n-  function_attributes.update(supportive_attributes)\n-  return tf.__internal__.function.defun_with_attributes(func=func,\n-                                        attributes=function_attributes,\n-                                        autograph=False)\n-\n-\n-def _get_context_device_type():\n-  \"\"\"Parse the current context and return the device type, eg CPU/GPU.\"\"\"\n-  current_device = get_device_name()\n-  if current_device is None:\n-    return None\n-  return tf.compat.v1.DeviceSpec.from_string(current_device).device_type\n-\n-\n-def _runtime(runtime_name):\n-  with tf.device('/cpu:0'):\n-    return tf.constant(\n-        runtime_name, dtype=tf.float32, name='runtime')\n-\n-\n-def _read_variable_value(v):\n-  \"\"\"Read the value of a variable if it is variable.\"\"\"\n-  if isinstance(v, tf.Variable):\n-    return v.read_value()\n-  return v\n-\n-\n-def _function_register(func, *args, **kwargs):\n-  \"\"\"Register a specialization of a `Function` into the graph.\n-\n-  This won't actually call the function with the inputs, and only put the\n-  function definition into graph. Register function with different input param\n-  will result into multiple version of functions registered in graph.\n-\n-  Args:\n-    func: the `Function` instance that generated by a @defun\n-    *args: input arguments for the Python function.\n-    **kwargs: input keyword arguments for the Python function.\n-\n-  Returns:\n-    a `ConcreteFunction` object specialized to inputs and execution context.\n-\n-  Raises:\n-    ValueError: When the input function is not a defun wrapped python function.\n-  \"\"\"\n-  concrete_func = func.get_concrete_function(*args, **kwargs)\n-  concrete_func.add_to_graph()\n-  concrete_func.add_gradient_functions_to_graph()\n-  return concrete_func\n\n@@ -0,0 +1,72 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras recurrent layers.\"\"\"\n+# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,disable=g-import-not-at-top\n+\n+import tensorflow.compat.v2 as tf\n+\n+# Recurrent layers.\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n+from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.layers.rnn.simple_rnn import SimpleRNNCell\n+from keras.layers.rnn.lstm_v1 import PeepholeLSTMCell\n+from keras.layers.rnn.simple_rnn import SimpleRNN\n+\n+if tf.__internal__.tf2.enabled():\n+  from keras.layers.rnn.gru import GRU\n+  from keras.layers.rnn.gru import GRUCell\n+  from keras.layers.rnn.lstm import LSTM\n+  from keras.layers.rnn.lstm import LSTMCell\n+  from keras.layers.rnn.gru_v1 import GRU as GRUV1\n+  from keras.layers.rnn.gru_v1 import GRUCell as GRUCellV1\n+  from keras.layers.rnn.lstm_v1 import LSTM as LSTMV1\n+  from keras.layers.rnn.lstm_v1 import LSTMCell as LSTMCellV1\n+  GRUV2 = GRU\n+  GRUCellV2 = GRUCell\n+  LSTMV2 = LSTM\n+  LSTMCellV2 = LSTMCell\n+else:\n+  from keras.layers.rnn.gru_v1 import GRU\n+  from keras.layers.rnn.gru_v1 import GRUCell\n+  from keras.layers.rnn.lstm_v1 import LSTM\n+  from keras.layers.rnn.lstm_v1 import LSTMCell\n+  from keras.layers.rnn.gru import GRU as GRUV2\n+  from keras.layers.rnn.gru import GRUCell as GRUCellV2\n+  from keras.layers.rnn.lstm import LSTM as LSTMV2\n+  from keras.layers.rnn.lstm import LSTMCell as LSTMCellV2\n+  GRUV1 = GRU\n+  GRUCellV1 = GRUCell\n+  LSTMV1 = LSTM\n+  LSTMCellV1 = LSTMCell\n+\n+# Convolutional-recurrent layers.\n+from keras.layers.rnn.conv_lstm1d import ConvLSTM1D\n+from keras.layers.rnn.conv_lstm2d import ConvLSTM2D\n+from keras.layers.rnn.conv_lstm3d import ConvLSTM3D\n+\n+# cuDNN recurrent layers.\n+from keras.layers.rnn.cudnn_lstm import CuDNNLSTM\n+from keras.layers.rnn.cudnn_gru import CuDNNGRU\n+\n+# Wrapper functions.\n+from keras.layers.rnn.base_wrapper import Wrapper\n+from keras.layers.rnn.bidirectional import Bidirectional\n+from keras.layers.rnn.time_distributed import TimeDistributed\n+\n+# RNN Cell wrappers.\n+from keras.layers.rnn.cell_wrappers import DeviceWrapper\n+from keras.layers.rnn.cell_wrappers import DropoutWrapper\n+from keras.layers.rnn.cell_wrappers import ResidualWrapper\n\n@@ -0,0 +1,113 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for RNN cells.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine import base_layer\n+from keras.layers.rnn import rnn_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.AbstractRNNCell')\n+class AbstractRNNCell(base_layer.Layer):\n+  \"\"\"Abstract object representing an RNN cell.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  This is the base class for implementing RNN cells with custom behavior.\n+\n+  Every `RNNCell` must have the properties below and implement `call` with\n+  the signature `(output, next_state) = call(input, state)`.\n+\n+  Examples:\n+\n+  ```python\n+    class MinimalRNNCell(AbstractRNNCell):\n+\n+      def __init__(self, units, **kwargs):\n+        self.units = units\n+        super(MinimalRNNCell, self).__init__(**kwargs)\n+\n+      @property\n+      def state_size(self):\n+        return self.units\n+\n+      def build(self, input_shape):\n+        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n+                                      initializer='uniform',\n+                                      name='kernel')\n+        self.recurrent_kernel = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer='uniform',\n+            name='recurrent_kernel')\n+        self.built = True\n+\n+      def call(self, inputs, states):\n+        prev_output = states[0]\n+        h = backend.dot(inputs, self.kernel)\n+        output = h + backend.dot(prev_output, self.recurrent_kernel)\n+        return output, output\n+  ```\n+\n+  This definition of cell differs from the definition used in the literature.\n+  In the literature, 'cell' refers to an object with a single scalar output.\n+  This definition refers to a horizontal array of such units.\n+\n+  An RNN cell, in the most abstract setting, is anything that has\n+  a state and performs some operation that takes a matrix of inputs.\n+  This operation results in an output matrix with `self.output_size` columns.\n+  If `self.state_size` is an integer, this operation also results in a new\n+  state matrix with `self.state_size` columns.  If `self.state_size` is a\n+  (possibly nested tuple of) TensorShape object(s), then it should return a\n+  matching structure of Tensors having shape `[batch_size].concatenate(s)`\n+  for each `s` in `self.batch_size`.\n+  \"\"\"\n+\n+  def call(self, inputs, states):\n+    \"\"\"The function that contains the logic for one RNN step calculation.\n+\n+    Args:\n+      inputs: the input tensor, which is a slide from the overall RNN input by\n+        the time dimension (usually the second dimension).\n+      states: the state tensor from previous step, which has the same shape\n+        as `(batch, state_size)`. In the case of timestep 0, it will be the\n+        initial state user specified, or zero filled tensor otherwise.\n+\n+    Returns:\n+      A tuple of two tensors:\n+        1. output tensor for the current timestep, with size `output_size`.\n+        2. state tensor for next step, which has the shape of `state_size`.\n+    \"\"\"\n+    raise NotImplementedError\n+\n+  @property\n+  def state_size(self):\n+    \"\"\"size(s) of state(s) used by this cell.\n+\n+    It can be represented by an Integer, a TensorShape or a tuple of Integers\n+    or TensorShapes.\n+    \"\"\"\n+    raise NotImplementedError\n+\n+  @property\n+  def output_size(self):\n+    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n+    raise NotImplementedError\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype)\n\n@@ -12,19 +12,20 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Module contains the implementation of RNN cell wrappers.\"\"\"\n+\"\"\"Module contains the base implementation of RNN cell wrappers.\"\"\"\n+\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n-\n import hashlib\n import numbers\n import sys\n import types as python_types\n import warnings\n+\n from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n \n \n class DropoutWrapperBase:\n@@ -494,7 +495,7 @@ def _parse_config_to_function(config, custom_objects, func_attr_name,\n \n \n def _default_dropout_state_filter_visitor(substate):\n-  from keras.layers.legacy_rnn.rnn_cell_impl import LSTMStateTuple  # pylint: disable=g-import-not-at-top\n+  from keras.layers.rnn.legacy_cells import LSTMStateTuple  # pylint: disable=g-import-not-at-top\n   if isinstance(substate, LSTMStateTuple):\n     # Do not perform dropout on the memory state.\n     return LSTMStateTuple(c=False, h=True)\n@@ -511,5 +512,6 @@ def _enumerated_map_structure_up_to(shallow_structure, map_fn, *args, **kwargs):\n     ix[0] += 1\n     return r\n \n-  return tf.__internal__.nest.map_structure_up_to(shallow_structure, enumerated_fn, *args,\n+  return tf.__internal__.nest.map_structure_up_to(shallow_structure,\n+                                                  enumerated_fn, *args,\n                                                   **kwargs)\n\n@@ -0,0 +1,600 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for N-D convolutional LSTM layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine import base_layer\n+from keras.layers.rnn.base_conv_rnn import ConvRNN\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+  \"\"\"Cell class for the ConvLSTM layer.\n+\n+  Args:\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      It defaults to the `image_data_format` value found in your Keras config\n+      file at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A (2+ `rank`)D tensor.\n+    states:  List of state tensors corresponding to the previous timestep.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTMCell, self).__init__(**kwargs)\n+    self.rank = rank\n+    if self.rank > 3:\n+      raise ValueError(f'Rank {rank} convolutions are not currently '\n+                       f'implemented. Received: rank={rank}')\n+    self.filters = filters\n+    self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank,\n+                                                  'kernel_size')\n+    self.strides = conv_utils.normalize_tuple(\n+        strides, self.rank, 'strides', allow_zero=True)\n+    self.padding = conv_utils.normalize_padding(padding)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, self.rank,\n+                                                    'dilation_rate')\n+    self.activation = activations.get(activation)\n+    self.recurrent_activation = activations.get(recurrent_activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.unit_forget_bias = unit_forget_bias\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1.0, max(0.0, dropout))\n+    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n+    self.state_size = (self.filters, self.filters)\n+\n+  def build(self, input_shape):\n+\n+    if self.data_format == 'channels_first':\n+      channel_axis = 1\n+    else:\n+      channel_axis = -1\n+    if input_shape[channel_axis] is None:\n+      raise ValueError(\n+          'The channel dimension of the inputs (last axis) should be defined. '\n+          f'Found None. Full input shape received: input_shape={input_shape}')\n+    input_dim = input_shape[channel_axis]\n+    self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n+    recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n+\n+    self.kernel = self.add_weight(\n+        shape=self.kernel_shape,\n+        initializer=self.kernel_initializer,\n+        name='kernel',\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=recurrent_kernel_shape,\n+        initializer=self.recurrent_initializer,\n+        name='recurrent_kernel',\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint)\n+\n+    if self.use_bias:\n+      if self.unit_forget_bias:\n+\n+        def bias_initializer(_, *args, **kwargs):\n+          return backend.concatenate([\n+              self.bias_initializer((self.filters,), *args, **kwargs),\n+              initializers.get('ones')((self.filters,), *args, **kwargs),\n+              self.bias_initializer((self.filters * 2,), *args, **kwargs),\n+          ])\n+      else:\n+        bias_initializer = self.bias_initializer\n+      self.bias = self.add_weight(\n+          shape=(self.filters * 4,),\n+          name='bias',\n+          initializer=bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs, states, training=None):\n+    h_tm1 = states[0]  # previous memory state\n+    c_tm1 = states[1]  # previous carry state\n+\n+    # dropout matrices for input units\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n+    # dropout matrices for recurrent units\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        h_tm1, training, count=4)\n+\n+    if 0 < self.dropout < 1.:\n+      inputs_i = inputs * dp_mask[0]\n+      inputs_f = inputs * dp_mask[1]\n+      inputs_c = inputs * dp_mask[2]\n+      inputs_o = inputs * dp_mask[3]\n+    else:\n+      inputs_i = inputs\n+      inputs_f = inputs\n+      inputs_c = inputs\n+      inputs_o = inputs\n+\n+    if 0 < self.recurrent_dropout < 1.:\n+      h_tm1_i = h_tm1 * rec_dp_mask[0]\n+      h_tm1_f = h_tm1 * rec_dp_mask[1]\n+      h_tm1_c = h_tm1 * rec_dp_mask[2]\n+      h_tm1_o = h_tm1 * rec_dp_mask[3]\n+    else:\n+      h_tm1_i = h_tm1\n+      h_tm1_f = h_tm1\n+      h_tm1_c = h_tm1\n+      h_tm1_o = h_tm1\n+\n+    (kernel_i, kernel_f, kernel_c, kernel_o) = tf.split(\n+        self.kernel, 4, axis=self.rank + 1)\n+    (recurrent_kernel_i, recurrent_kernel_f, recurrent_kernel_c,\n+     recurrent_kernel_o) = tf.split(\n+         self.recurrent_kernel, 4, axis=self.rank + 1)\n+\n+    if self.use_bias:\n+      bias_i, bias_f, bias_c, bias_o = tf.split(self.bias, 4)\n+    else:\n+      bias_i, bias_f, bias_c, bias_o = None, None, None, None\n+\n+    x_i = self.input_conv(inputs_i, kernel_i, bias_i, padding=self.padding)\n+    x_f = self.input_conv(inputs_f, kernel_f, bias_f, padding=self.padding)\n+    x_c = self.input_conv(inputs_c, kernel_c, bias_c, padding=self.padding)\n+    x_o = self.input_conv(inputs_o, kernel_o, bias_o, padding=self.padding)\n+    h_i = self.recurrent_conv(h_tm1_i, recurrent_kernel_i)\n+    h_f = self.recurrent_conv(h_tm1_f, recurrent_kernel_f)\n+    h_c = self.recurrent_conv(h_tm1_c, recurrent_kernel_c)\n+    h_o = self.recurrent_conv(h_tm1_o, recurrent_kernel_o)\n+\n+    i = self.recurrent_activation(x_i + h_i)\n+    f = self.recurrent_activation(x_f + h_f)\n+    c = f * c_tm1 + i * self.activation(x_c + h_c)\n+    o = self.recurrent_activation(x_o + h_o)\n+    h = o * self.activation(c)\n+    return h, [h, c]\n+\n+  @property\n+  def _conv_func(self):\n+    if self.rank == 1:\n+      return backend.conv1d\n+    if self.rank == 2:\n+      return backend.conv2d\n+    if self.rank == 3:\n+      return backend.conv3d\n+\n+  def input_conv(self, x, w, b=None, padding='valid'):\n+    conv_out = self._conv_func(\n+        x,\n+        w,\n+        strides=self.strides,\n+        padding=padding,\n+        data_format=self.data_format,\n+        dilation_rate=self.dilation_rate)\n+    if b is not None:\n+      conv_out = backend.bias_add(conv_out, b, data_format=self.data_format)\n+    return conv_out\n+\n+  def recurrent_conv(self, x, w):\n+    strides = conv_utils.normalize_tuple(\n+        1, self.rank, 'strides', allow_zero=True)\n+    conv_out = self._conv_func(\n+        x, w, strides=strides, padding='same', data_format=self.data_format)\n+    return conv_out\n+\n+  def get_config(self):\n+    config = {\n+        'filters':\n+            self.filters,\n+        'kernel_size':\n+            self.kernel_size,\n+        'strides':\n+            self.strides,\n+        'padding':\n+            self.padding,\n+        'data_format':\n+            self.data_format,\n+        'dilation_rate':\n+            self.dilation_rate,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+    }\n+    base_config = super(ConvLSTMCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class ConvLSTM(ConvRNN):\n+  \"\"\"Abstract N-D Convolutional LSTM layer (used as implementation base).\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers,\n+      specifying the strides of the convolution.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, time, ..., channels)`\n+      while `channels_first` corresponds to\n+      inputs with shape `(batch, time, channels, ...)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying\n+      the dilation rate to use for dilated convolution.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any `strides` value != 1.\n+    activation: Activation function to use.\n+      By default hyperbolic tangent activation function is applied\n+      (`tanh(x)`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix,\n+      used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean.\n+      If True, add 1 to the bias of the forget gate at initialization.\n+      Use in combination with `bias_initializer=\"zeros\"`.\n+      This is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    recurrent_constraint: Constraint function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output\n+      in the output sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state\n+      in addition to the output. (default False)\n+    go_backwards: Boolean (default False).\n+      If True, process the input sequence backwards.\n+    stateful: Boolean (default False). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    cell = ConvLSTMCell(\n+        rank=rank,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        dtype=kwargs.get('dtype'))\n+    super(ConvLSTM, self).__init__(\n+        rank,\n+        cell,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        **kwargs)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    return super(ConvLSTM, self).call(\n+        inputs, mask=mask, training=training, initial_state=initial_state)\n+\n+  @property\n+  def filters(self):\n+    return self.cell.filters\n+\n+  @property\n+  def kernel_size(self):\n+    return self.cell.kernel_size\n+\n+  @property\n+  def strides(self):\n+    return self.cell.strides\n+\n+  @property\n+  def padding(self):\n+    return self.cell.padding\n+\n+  @property\n+  def data_format(self):\n+    return self.cell.data_format\n+\n+  @property\n+  def dilation_rate(self):\n+    return self.cell.dilation_rate\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def recurrent_activation(self):\n+    return self.cell.recurrent_activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def unit_forget_bias(self):\n+    return self.cell.unit_forget_bias\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  def get_config(self):\n+    config = {'filters': self.filters,\n+              'kernel_size': self.kernel_size,\n+              'strides': self.strides,\n+              'padding': self.padding,\n+              'data_format': self.data_format,\n+              'dilation_rate': self.dilation_rate,\n+              'activation': activations.serialize(self.activation),\n+              'recurrent_activation': activations.serialize(\n+                  self.recurrent_activation),\n+              'use_bias': self.use_bias,\n+              'kernel_initializer': initializers.serialize(\n+                  self.kernel_initializer),\n+              'recurrent_initializer': initializers.serialize(\n+                  self.recurrent_initializer),\n+              'bias_initializer': initializers.serialize(self.bias_initializer),\n+              'unit_forget_bias': self.unit_forget_bias,\n+              'kernel_regularizer': regularizers.serialize(\n+                  self.kernel_regularizer),\n+              'recurrent_regularizer': regularizers.serialize(\n+                  self.recurrent_regularizer),\n+              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+              'activity_regularizer': regularizers.serialize(\n+                  self.activity_regularizer),\n+              'kernel_constraint': constraints.serialize(\n+                  self.kernel_constraint),\n+              'recurrent_constraint': constraints.serialize(\n+                  self.recurrent_constraint),\n+              'bias_constraint': constraints.serialize(self.bias_constraint),\n+              'dropout': self.dropout,\n+              'recurrent_dropout': self.recurrent_dropout}\n+    base_config = super(ConvLSTM, self).get_config()\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    return cls(**config)\n\n@@ -0,0 +1,388 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for convolutional-recurrent layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.utils import conv_utils\n+from keras.utils import generic_utils\n+from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+class ConvRNN(RNN):\n+  \"\"\"N-Dimensional Base class for convolutional-recurrent layers.\n+\n+  Args:\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n+    cell: A RNN cell instance. A RNN cell is a class that has: - a\n+      `call(input_at_t, states_at_t)` method, returning `(output_at_t,\n+      states_at_t_plus_1)`. The call method of the cell can also take the\n+      optional argument `constants`, see section \"Note on passing external\n+      constants\" below. - a `state_size` attribute. This can be a single integer\n+      (single state) in which case it is the number of channels of the recurrent\n+      state (which should be the same as the number of channels of the cell\n+      output). This can also be a list/tuple of integers (one size per state).\n+      In this case, the first entry (`state_size[0]`) should be the same as the\n+      size of the cell output.\n+    return_sequences: Boolean. Whether to return the last output. in the output\n+      sequence, or the full sequence.\n+    return_state: Boolean. Whether to return the last state in addition to the\n+      output.\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    input_shape: Use this argument to specify the shape of the input when this\n+      layer is the first one in a model.\n+  Call arguments:\n+    inputs: A (2 + `rank`)D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is for use with cells that use dropout.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+    constants: List of constant tensors to be passed to the cell at each\n+      timestep.\n+  Input shape:\n+    (3 + `rank`)D tensor with shape: `(samples, timesteps, channels,\n+      img_dimensions...)`\n+    if data_format='channels_first' or shape: `(samples, timesteps,\n+      img_dimensions..., channels)` if data_format='channels_last'.\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each (2 + `rank`)D tensor with shape: `(samples, filters,\n+        new_img_dimensions...)` if data_format='channels_first'\n+      or shape: `(samples, new_img_dimensions..., filters)` if\n+        data_format='channels_last'. img_dimension values might have changed due\n+        to padding.\n+    - If `return_sequences`: (3 + `rank`)D tensor with shape: `(samples,\n+      timesteps, filters, new_img_dimensions...)` if\n+      data_format='channels_first'\n+      or shape: `(samples, timesteps, new_img_dimensions..., filters)` if\n+        data_format='channels_last'.\n+    - Else, (2 + `rank`)D tensor with shape: `(samples, filters,\n+      new_img_dimensions...)` if data_format='channels_first'\n+      or shape: `(samples, new_img_dimensions..., filters)` if\n+        data_format='channels_last'.\n+  Masking: This layer supports masking for input data with a variable number of\n+    timesteps.\n+  Note on using statefulness in RNNs: You can set RNN layers to be 'stateful',\n+    which means that the states computed for the samples in one batch will be\n+    reused as initial states for the samples in the next batch. This assumes a\n+    one-to-one mapping between samples in different successive batches.\n+    To enable statefulness: - Specify `stateful=True` in the layer constructor.\n+      - Specify a fixed batch size for your model, by passing\n+          - If sequential model: `batch_input_shape=(...)` to the first layer in\n+            your model.\n+          - If functional model with 1 or more Input layers: `batch_shape=(...)`\n+            to all the first layers in your model. This is the expected shape of\n+            your inputs *including the batch size*. It should be a tuple of\n+            integers, e.g. `(32, 10, 100, 100, 32)`. for rank 2 convolution Note\n+            that the image dimensions should be specified too. - Specify\n+            `shuffle=False` when calling fit(). To reset the states of your\n+            model, call `.reset_states()` on either a specific layer, or on your\n+            entire model.\n+  Note on specifying the initial state of RNNs: You can specify the initial\n+    state of RNN layers symbolically by calling them with the keyword argument\n+    `initial_state`. The value of `initial_state` should be a tensor or list of\n+    tensors representing the initial state of the RNN layer. You can specify the\n+    initial state of RNN layers numerically by calling `reset_states` with the\n+    keyword argument `states`. The value of `states` should be a numpy array or\n+    list of numpy arrays representing the initial state of the RNN layer.\n+  Note on passing external constants to RNNs: You can pass \"external\" constants\n+    to the cell using the `constants` keyword argument of `RNN.__call__` (as\n+    well as `RNN.call`) method. This requires that the `cell.call` method\n+    accepts the same keyword argument `constants`. Such constants can be used to\n+    condition the cell transformation on additional static inputs (not changing\n+    over time), a.k.a. an attention mechanism.\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               cell,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               **kwargs):\n+    if unroll:\n+      raise TypeError(\n+          'Unrolling is not possible with convolutional RNNs. '\n+          f'Received: unroll={unroll}')\n+    if isinstance(cell, (list, tuple)):\n+      # The StackedConvRNN3DCells isn't implemented yet.\n+      raise TypeError('It is not possible at the moment to'\n+                      'stack convolutional cells. Only pass a single cell '\n+                      'instance as the `cell` argument. Received: '\n+                      f'cell={cell}')\n+    super(ConvRNN, self).__init__(cell, return_sequences, return_state,\n+                                  go_backwards, stateful, unroll, **kwargs)\n+    self.rank = rank\n+    self.input_spec = [InputSpec(ndim=rank + 3)]\n+    self.states = None\n+    self._num_constants = None\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+\n+    cell = self.cell\n+    if cell.data_format == 'channels_first':\n+      img_dims = input_shape[3:]\n+    elif cell.data_format == 'channels_last':\n+      img_dims = input_shape[2:-1]\n+\n+    norm_img_dims = tuple([\n+        conv_utils.conv_output_length(  # pylint: disable=g-complex-comprehension\n+            img_dims[idx],\n+            cell.kernel_size[idx],\n+            padding=cell.padding,\n+            stride=cell.strides[idx],\n+            dilation=cell.dilation_rate[idx]) for idx in range(len(img_dims))\n+    ])\n+\n+    if cell.data_format == 'channels_first':\n+      output_shape = input_shape[:2] + (cell.filters,) + norm_img_dims\n+    elif cell.data_format == 'channels_last':\n+      output_shape = input_shape[:2] + norm_img_dims + (cell.filters,)\n+\n+    if not self.return_sequences:\n+      output_shape = output_shape[:1] + output_shape[2:]\n+\n+    if self.return_state:\n+      output_shape = [output_shape]\n+      if cell.data_format == 'channels_first':\n+        output_shape += [\n+            (input_shape[0], cell.filters) + norm_img_dims for _ in range(2)\n+        ]\n+      elif cell.data_format == 'channels_last':\n+        output_shape += [(input_shape[0],) + norm_img_dims + (cell.filters,)\n+                         for _ in range(2)]\n+    return output_shape\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    # Note input_shape will be list of shapes of initial states and\n+    # constants if these are passed in __call__.\n+    if self._num_constants is not None:\n+      constants_shape = input_shape[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n+    else:\n+      constants_shape = None\n+\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+\n+    batch_size = input_shape[0] if self.stateful else None\n+    self.input_spec[0] = InputSpec(\n+        shape=(batch_size, None) + input_shape[2:self.rank + 3])\n+\n+    # allow cell (if layer) to build before we set or validate state_spec\n+    if isinstance(self.cell, base_layer.Layer):\n+      step_input_shape = (input_shape[0],) + input_shape[2:]\n+      if constants_shape is not None:\n+        self.cell.build([step_input_shape] + constants_shape)\n+      else:\n+        self.cell.build(step_input_shape)\n+\n+    # set or validate state_spec\n+    if hasattr(self.cell.state_size, '__len__'):\n+      state_size = list(self.cell.state_size)\n+    else:\n+      state_size = [self.cell.state_size]\n+\n+    if self.state_spec is not None:\n+      # initial_state was passed in call, check compatibility\n+      if self.cell.data_format == 'channels_first':\n+        ch_dim = 1\n+      elif self.cell.data_format == 'channels_last':\n+        ch_dim = self.rank + 1\n+      if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n+        raise ValueError(\n+            'An `initial_state` was passed that is not compatible with '\n+            '`cell.state_size`. Received state shapes '\n+            f'{[spec.shape for spec in self.state_spec]}. '\n+            f'However `cell.state_size` is {self.cell.state_size}')\n+    else:\n+      img_dims = tuple((None for _ in range(self.rank)))\n+      if self.cell.data_format == 'channels_first':\n+        self.state_spec = [\n+            InputSpec(shape=(None, dim) + img_dims) for dim in state_size\n+        ]\n+      elif self.cell.data_format == 'channels_last':\n+        self.state_spec = [\n+            InputSpec(shape=(None,) + img_dims + (dim,)) for dim in state_size\n+        ]\n+    if self.stateful:\n+      self.reset_states()\n+    self.built = True\n+\n+  def get_initial_state(self, inputs):\n+    # (samples, timesteps, img_dims..., filters)\n+    initial_state = backend.zeros_like(inputs)\n+    # (samples, img_dims..., filters)\n+    initial_state = backend.sum(initial_state, axis=1)\n+    shape = list(self.cell.kernel_shape)\n+    shape[-1] = self.cell.filters\n+    initial_state = self.cell.input_conv(initial_state,\n+                                         tf.zeros(tuple(shape),\n+                                                  initial_state.dtype),\n+                                         padding=self.cell.padding)\n+\n+    if hasattr(self.cell.state_size, '__len__'):\n+      return [initial_state for _ in self.cell.state_size]\n+    else:\n+      return [initial_state]\n+\n+  def call(self,\n+           inputs,\n+           mask=None,\n+           training=None,\n+           initial_state=None,\n+           constants=None):\n+    # note that the .build() method of subclasses MUST define\n+    # self.input_spec and self.state_spec with complete input shapes.\n+    inputs, initial_state, constants = self._process_inputs(\n+        inputs, initial_state, constants)\n+\n+    if isinstance(mask, list):\n+      mask = mask[0]\n+    timesteps = backend.int_shape(inputs)[1]\n+\n+    kwargs = {}\n+    if generic_utils.has_arg(self.cell.call, 'training'):\n+      kwargs['training'] = training\n+\n+    if constants:\n+      if not generic_utils.has_arg(self.cell.call, 'constants'):\n+        raise ValueError(\n+            f'RNN cell {self.cell} does not support constants. '\n+            f'Received: constants={constants}')\n+\n+      def step(inputs, states):\n+        constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n+        states = states[:-self._num_constants]  # pylint: disable=invalid-unary-operand-type\n+        return self.cell.call(inputs, states, constants=constants, **kwargs)\n+    else:\n+      def step(inputs, states):\n+        return self.cell.call(inputs, states, **kwargs)\n+\n+    last_output, outputs, states = backend.rnn(step,\n+                                               inputs,\n+                                               initial_state,\n+                                               constants=constants,\n+                                               go_backwards=self.go_backwards,\n+                                               mask=mask,\n+                                               input_length=timesteps)\n+    if self.stateful:\n+      updates = [\n+          backend.update(self_state, state)\n+          for self_state, state in zip(self.states, states)\n+      ]\n+      self.add_update(updates)\n+\n+    if self.return_sequences:\n+      output = outputs\n+    else:\n+      output = last_output\n+\n+    if self.return_state:\n+      if not isinstance(states, (list, tuple)):\n+        states = [states]\n+      else:\n+        states = list(states)\n+      return [output] + states\n+    return output\n+\n+  def reset_states(self, states=None):\n+    if not self.stateful:\n+      raise AttributeError('Layer must be stateful.')\n+    input_shape = self.input_spec[0].shape\n+    state_shape = self.compute_output_shape(input_shape)\n+    if self.return_state:\n+      state_shape = state_shape[0]\n+    if self.return_sequences:\n+      state_shape = state_shape[:1].concatenate(state_shape[2:])\n+    if None in state_shape:\n+      raise ValueError('If a RNN is stateful, it needs to know '\n+                       'its batch size. Specify the batch size '\n+                       'of your input tensors: \\n'\n+                       '- If using a Sequential model, '\n+                       'specify the batch size by passing '\n+                       'a `batch_input_shape` '\n+                       'argument to your first layer.\\n'\n+                       '- If using the functional API, specify '\n+                       'the time dimension by passing a '\n+                       '`batch_shape` argument to your Input layer.\\n'\n+                       'The same thing goes for the number of rows and '\n+                       'columns.')\n+\n+    # helper function\n+    def get_tuple_shape(nb_channels):\n+      result = list(state_shape)\n+      if self.cell.data_format == 'channels_first':\n+        result[1] = nb_channels\n+      elif self.cell.data_format == 'channels_last':\n+        result[self.rank + 1] = nb_channels\n+      else:\n+        raise KeyError(\n+            'Cell data format must be one of '\n+            '{\"channels_first\", \"channels_last\"}. Received: '\n+            f'cell.data_format={self.cell.data_format}')\n+      return tuple(result)\n+\n+    # initialize state if None\n+    if self.states[0] is None:\n+      if hasattr(self.cell.state_size, '__len__'):\n+        self.states = [backend.zeros(get_tuple_shape(dim))\n+                       for dim in self.cell.state_size]\n+      else:\n+        self.states = [backend.zeros(get_tuple_shape(self.cell.state_size))]\n+    elif states is None:\n+      if hasattr(self.cell.state_size, '__len__'):\n+        for state, dim in zip(self.states, self.cell.state_size):\n+          backend.set_value(state, np.zeros(get_tuple_shape(dim)))\n+      else:\n+        backend.set_value(self.states[0],\n+                          np.zeros(get_tuple_shape(self.cell.state_size)))\n+    else:\n+      if not isinstance(states, (list, tuple)):\n+        states = [states]\n+      if len(states) != len(self.states):\n+        raise ValueError(\n+            f'Layer {self.name} expects {len(self.states)} states, '\n+            f'but it received {len(states)} state values. '\n+            f'States received: {states}')\n+      for index, (value, state) in enumerate(zip(states, self.states)):\n+        if hasattr(self.cell.state_size, '__len__'):\n+          dim = self.cell.state_size[index]\n+        else:\n+          dim = self.cell.state_size\n+        if value.shape != get_tuple_shape(dim):\n+          raise ValueError(\n+              f'State {index} is incompatible with layer {self.name}: '\n+              f'expected shape={get_tuple_shape(dim)}, '\n+              f'found shape={value.shape}')\n+        backend.set_value(state, value)\n\n@@ -0,0 +1,145 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for recurrent layers backed by cuDNN.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import backend\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn.base_rnn import RNN\n+import tensorflow.compat.v2 as tf\n+\n+\n+class _CuDNNRNN(RNN):\n+  \"\"\"Private base class for CuDNNGRU and CuDNNLSTM layers.\n+\n+  Args:\n+    return_sequences: Boolean. Whether to return the last output\n+        in the output sequence, or the full sequence.\n+    return_state: Boolean. Whether to return the last state\n+        in addition to the output.\n+    go_backwards: Boolean (default False).\n+        If True, process the input sequence backwards and return the\n+        reversed sequence.\n+    stateful: Boolean (default False). If True, the last state\n+        for each sample at index i in a batch will be used as initial\n+        state for the sample of index i in the following batch.\n+    time_major: Boolean (default False). If true, the inputs and outputs will be\n+        in shape `(timesteps, batch, ...)`, whereas in the False case, it will\n+        be `(batch, timesteps, ...)`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               time_major=False,\n+               **kwargs):\n+    # We invoke the base layer's initializer directly here because we do not\n+    # want to create RNN cell instance.\n+    super(RNN, self).__init__(**kwargs)  # pylint: disable=bad-super-call\n+    self.return_sequences = return_sequences\n+    self.return_state = return_state\n+    self.go_backwards = go_backwards\n+    self.stateful = stateful\n+    self.time_major = time_major\n+    self.supports_masking = False\n+    self.input_spec = [InputSpec(ndim=3)]\n+    if hasattr(self.cell.state_size, '__len__'):\n+      state_size = self.cell.state_size\n+    else:\n+      state_size = [self.cell.state_size]\n+    self.state_spec = [InputSpec(shape=(None, dim)) for dim in state_size]\n+    self.constants_spec = None\n+    self._states = None\n+    self._num_constants = 0\n+    self._vector_shape = tf.constant([-1])\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    if isinstance(mask, list):\n+      mask = mask[0]\n+    if mask is not None:\n+      raise ValueError('Masking is not supported for CuDNN RNNs.')\n+\n+    # input shape: `(samples, time (padded with zeros), input_dim)`\n+    # note that the .build() method of subclasses MUST define\n+    # self.input_spec and self.state_spec with complete input shapes.\n+    if isinstance(inputs, list):\n+      initial_state = inputs[1:]\n+      inputs = inputs[0]\n+    elif initial_state is not None:\n+      pass\n+    elif self.stateful:\n+      initial_state = self.states\n+    else:\n+      initial_state = self.get_initial_state(inputs)\n+\n+    if len(initial_state) != len(self.states):\n+      raise ValueError('Layer has ' + str(len(self.states)) +\n+                       ' states but was passed ' + str(len(initial_state)) +\n+                       ' initial states.')\n+\n+    if self.go_backwards:\n+      # Reverse time axis.\n+      inputs = backend.reverse(inputs, 1)\n+    output, states = self._process_batch(inputs, initial_state)\n+\n+    if self.stateful:\n+      updates = [\n+          tf.compat.v1.assign(self_state, state)\n+          for self_state, state in zip(self.states, states)\n+      ]\n+      self.add_update(updates)\n+\n+    if self.return_state:\n+      return [output] + states\n+    else:\n+      return output\n+\n+  def get_config(self):\n+    config = {\n+        'return_sequences': self.return_sequences,\n+        'return_state': self.return_state,\n+        'go_backwards': self.go_backwards,\n+        'stateful': self.stateful,\n+        'time_major': self.time_major,\n+    }\n+    base_config = super(  # pylint: disable=bad-super-call\n+        RNN, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    return cls(**config)\n+\n+  @property\n+  def trainable_weights(self):\n+    if self.trainable and self.built:\n+      return [self.kernel, self.recurrent_kernel, self.bias]\n+    return []\n+\n+  @property\n+  def non_trainable_weights(self):\n+    if not self.trainable and self.built:\n+      return [self.kernel, self.recurrent_kernel, self.bias]\n+    return []\n+\n+  @property\n+  def losses(self):\n+    return super(RNN, self).losses\n+\n+  def get_losses_for(self, inputs=None):\n+    return super(  # pylint: disable=bad-super-call\n+        RNN, self).get_losses_for(inputs=inputs)\n\n@@ -0,0 +1,861 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for recurrent layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import collections\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.saving.saved_model import layer_serialization\n+from keras.utils import generic_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls\n+\n+\n+@keras_export('keras.layers.RNN')\n+class RNN(base_layer.Layer):\n+  \"\"\"Base class for recurrent layers.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  Args:\n+    cell: A RNN cell instance or a list of RNN cell instances.\n+      A RNN cell is a class that has:\n+      - A `call(input_at_t, states_at_t)` method, returning\n+        `(output_at_t, states_at_t_plus_1)`. The call method of the\n+        cell can also take the optional argument `constants`, see\n+        section \"Note on passing external constants\" below.\n+      - A `state_size` attribute. This can be a single integer\n+        (single state) in which case it is the size of the recurrent\n+        state. This can also be a list/tuple of integers (one size per state).\n+        The `state_size` can also be TensorShape or tuple/list of\n+        TensorShape, to represent high dimension state.\n+      - A `output_size` attribute. This can be a single integer or a\n+        TensorShape, which represent the shape of the output. For backward\n+        compatible reason, if this attribute is not available for the\n+        cell, the value will be inferred by the first element of the\n+        `state_size`.\n+      - A `get_initial_state(inputs=None, batch_size=None, dtype=None)`\n+        method that creates a tensor meant to be fed to `call()` as the\n+        initial state, if the user didn't specify any initial state via other\n+        means. The returned initial state should have a shape of\n+        [batch_size, cell.state_size]. The cell might choose to create a\n+        tensor full of zeros, or full of other values based on the cell's\n+        implementation.\n+        `inputs` is the input tensor to the RNN layer, which should\n+        contain the batch size as its shape[0], and also dtype. Note that\n+        the shape[0] might be `None` during the graph construction. Either\n+        the `inputs` or the pair of `batch_size` and `dtype` are provided.\n+        `batch_size` is a scalar tensor that represents the batch size\n+        of the inputs. `dtype` is `tf.DType` that represents the dtype of\n+        the inputs.\n+        For backward compatibility, if this method is not implemented\n+        by the cell, the RNN layer will create a zero filled tensor with the\n+        size of [batch_size, cell.state_size].\n+      In the case that `cell` is a list of RNN cell instances, the cells\n+      will be stacked on top of each other in the RNN, resulting in an\n+      efficient stacked RNN.\n+    return_sequences: Boolean (default `False`). Whether to return the last\n+      output in the output sequence, or the full sequence.\n+    return_state: Boolean (default `False`). Whether to return the last state\n+      in addition to the output.\n+    go_backwards: Boolean (default `False`).\n+      If True, process the input sequence backwards and return the\n+      reversed sequence.\n+    stateful: Boolean (default `False`). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    unroll: Boolean (default `False`).\n+      If True, the network will be unrolled, else a symbolic loop will be used.\n+      Unrolling can speed-up a RNN, although it tends to be more\n+      memory-intensive. Unrolling is only suitable for short sequences.\n+    time_major: The shape format of the `inputs` and `outputs` tensors.\n+      If True, the inputs and outputs will be in shape\n+      `(timesteps, batch, ...)`, whereas in the False case, it will be\n+      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n+      efficient because it avoids transposes at the beginning and end of the\n+      RNN calculation. However, most TensorFlow data is batch-major, so by\n+      default this function accepts input and emits output in batch-major\n+      form.\n+    zero_output_for_mask: Boolean (default `False`).\n+      Whether the output should use zeros for the masked timesteps. Note that\n+      this field is only used when `return_sequences` is True and mask is\n+      provided. It can useful if you want to reuse the raw output sequence of\n+      the RNN without interference from the masked timesteps, eg, merging\n+      bidirectional RNNs.\n+\n+  Call arguments:\n+    inputs: Input tensor.\n+    mask: Binary tensor of shape `[batch_size, timesteps]` indicating whether\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is for use with cells that use dropout.\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell.\n+    constants: List of constant tensors to be passed to the cell at each\n+      timestep.\n+\n+  Input shape:\n+    N-D tensor with shape `[batch_size, timesteps, ...]` or\n+    `[timesteps, batch_size, ...]` when time_major is True.\n+\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is\n+      the output. The remaining tensors are the last states,\n+      each with shape `[batch_size, state_size]`, where `state_size` could\n+      be a high dimension tensor shape.\n+    - If `return_sequences`: N-D tensor with shape\n+      `[batch_size, timesteps, output_size]`, where `output_size` could\n+      be a high dimension tensor shape, or\n+      `[timesteps, batch_size, output_size]` when `time_major` is True.\n+    - Else, N-D tensor with shape `[batch_size, output_size]`, where\n+      `output_size` could be a high dimension tensor shape.\n+\n+  Masking:\n+    This layer supports masking for input data with a variable number\n+    of timesteps. To introduce masks to your data,\n+    use an [tf.keras.layers.Embedding] layer with the `mask_zero` parameter\n+    set to `True`.\n+\n+  Note on using statefulness in RNNs:\n+    You can set RNN layers to be 'stateful', which means that the states\n+    computed for the samples in one batch will be reused as initial states\n+    for the samples in the next batch. This assumes a one-to-one mapping\n+    between samples in different successive batches.\n+\n+    To enable statefulness:\n+      - Specify `stateful=True` in the layer constructor.\n+      - Specify a fixed batch size for your model, by passing\n+        If sequential model:\n+          `batch_input_shape=(...)` to the first layer in your model.\n+        Else for functional model with 1 or more Input layers:\n+          `batch_shape=(...)` to all the first layers in your model.\n+        This is the expected shape of your inputs\n+        *including the batch size*.\n+        It should be a tuple of integers, e.g. `(32, 10, 100)`.\n+      - Specify `shuffle=False` when calling `fit()`.\n+\n+    To reset the states of your model, call `.reset_states()` on either\n+    a specific layer, or on your entire model.\n+\n+  Note on specifying the initial state of RNNs:\n+    You can specify the initial state of RNN layers symbolically by\n+    calling them with the keyword argument `initial_state`. The value of\n+    `initial_state` should be a tensor or list of tensors representing\n+    the initial state of the RNN layer.\n+\n+    You can specify the initial state of RNN layers numerically by\n+    calling `reset_states` with the keyword argument `states`. The value of\n+    `states` should be a numpy array or list of numpy arrays representing\n+    the initial state of the RNN layer.\n+\n+  Note on passing external constants to RNNs:\n+    You can pass \"external\" constants to the cell using the `constants`\n+    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n+    requires that the `cell.call` method accepts the same keyword argument\n+    `constants`. Such constants can be used to condition the cell\n+    transformation on additional static inputs (not changing over time),\n+    a.k.a. an attention mechanism.\n+\n+  Examples:\n+\n+  ```python\n+  # First, let's define a RNN Cell, as a layer subclass.\n+\n+  class MinimalRNNCell(keras.layers.Layer):\n+\n+      def __init__(self, units, **kwargs):\n+          self.units = units\n+          self.state_size = units\n+          super(MinimalRNNCell, self).__init__(**kwargs)\n+\n+      def build(self, input_shape):\n+          self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n+                                        initializer='uniform',\n+                                        name='kernel')\n+          self.recurrent_kernel = self.add_weight(\n+              shape=(self.units, self.units),\n+              initializer='uniform',\n+              name='recurrent_kernel')\n+          self.built = True\n+\n+      def call(self, inputs, states):\n+          prev_output = states[0]\n+          h = backend.dot(inputs, self.kernel)\n+          output = h + backend.dot(prev_output, self.recurrent_kernel)\n+          return output, [output]\n+\n+  # Let's use this cell in a RNN layer:\n+\n+  cell = MinimalRNNCell(32)\n+  x = keras.Input((None, 5))\n+  layer = RNN(cell)\n+  y = layer(x)\n+\n+  # Here's how to use the cell to build a stacked RNN:\n+\n+  cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n+  x = keras.Input((None, 5))\n+  layer = RNN(cells)\n+  y = layer(x)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               cell,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               time_major=False,\n+               **kwargs):\n+    if isinstance(cell, (list, tuple)):\n+      cell = StackedRNNCells(cell)\n+    if 'call' not in dir(cell):\n+      raise ValueError('Argument `cell` should have a `call` method. '\n+                       f'The RNN was passed: cell={cell}')\n+    if 'state_size' not in dir(cell):\n+      raise ValueError('The RNN cell should have a `state_size` attribute '\n+                       '(tuple of integers, one integer per RNN state). '\n+                       f'Received: cell={cell}')\n+    # If True, the output for masked timestep will be zeros, whereas in the\n+    # False case, output from previous timestep is returned for masked timestep.\n+    self.zero_output_for_mask = kwargs.pop('zero_output_for_mask', False)\n+\n+    if 'input_shape' not in kwargs and (\n+        'input_dim' in kwargs or 'input_length' in kwargs):\n+      input_shape = (kwargs.pop('input_length', None),\n+                     kwargs.pop('input_dim', None))\n+      kwargs['input_shape'] = input_shape\n+\n+    super(RNN, self).__init__(**kwargs)\n+    self.cell = cell\n+    self.return_sequences = return_sequences\n+    self.return_state = return_state\n+    self.go_backwards = go_backwards\n+    self.stateful = stateful\n+    self.unroll = unroll\n+    self.time_major = time_major\n+\n+    self.supports_masking = True\n+    # The input shape is unknown yet, it could have nested tensor inputs, and\n+    # the input spec will be the list of specs for nested inputs, the structure\n+    # of the input_spec will be the same as the input.\n+    self.input_spec = None\n+    self.state_spec = None\n+    self._states = None\n+    self.constants_spec = None\n+    self._num_constants = 0\n+\n+    if stateful:\n+      if tf.distribute.has_strategy():\n+        raise ValueError('Stateful RNNs (created with `stateful=True`) '\n+                         'are not yet supported with tf.distribute.Strategy.')\n+\n+  @property\n+  def _use_input_spec_as_call_signature(self):\n+    if self.unroll:\n+      # When the RNN layer is unrolled, the time step shape cannot be unknown.\n+      # The input spec does not define the time step (because this layer can be\n+      # called with any time step value, as long as it is not None), so it\n+      # cannot be used as the call function signature when saving to SavedModel.\n+      return False\n+    return super(RNN, self)._use_input_spec_as_call_signature\n+\n+  @property\n+  def states(self):\n+    if self._states is None:\n+      state = tf.nest.map_structure(lambda _: None, self.cell.state_size)\n+      return state if tf.nest.is_nested(self.cell.state_size) else [state]\n+    return self._states\n+\n+  @states.setter\n+  # Automatic tracking catches \"self._states\" which adds an extra weight and\n+  # breaks HDF5 checkpoints.\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def states(self, states):\n+    self._states = states\n+\n+  def compute_output_shape(self, input_shape):\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+    # Check whether the input shape contains any nested shapes. It could be\n+    # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n+    # inputs.\n+    try:\n+      input_shape = tf.TensorShape(input_shape)\n+    except (ValueError, TypeError):\n+      # A nested tensor input\n+      input_shape = tf.nest.flatten(input_shape)[0]\n+\n+    batch = input_shape[0]\n+    time_step = input_shape[1]\n+    if self.time_major:\n+      batch, time_step = time_step, batch\n+\n+    if rnn_utils.is_multiple_state(self.cell.state_size):\n+      state_size = self.cell.state_size\n+    else:\n+      state_size = [self.cell.state_size]\n+\n+    def _get_output_shape(flat_output_size):\n+      output_dim = tf.TensorShape(flat_output_size).as_list()\n+      if self.return_sequences:\n+        if self.time_major:\n+          output_shape = tf.TensorShape(\n+              [time_step, batch] + output_dim)\n+        else:\n+          output_shape = tf.TensorShape(\n+              [batch, time_step] + output_dim)\n+      else:\n+        output_shape = tf.TensorShape([batch] + output_dim)\n+      return output_shape\n+\n+    if getattr(self.cell, 'output_size', None) is not None:\n+      # cell.output_size could be nested structure.\n+      output_shape = tf.nest.flatten(tf.nest.map_structure(\n+          _get_output_shape, self.cell.output_size))\n+      output_shape = output_shape[0] if len(output_shape) == 1 else output_shape\n+    else:\n+      # Note that state_size[0] could be a tensor_shape or int.\n+      output_shape = _get_output_shape(state_size[0])\n+\n+    if self.return_state:\n+      def _get_state_shape(flat_state):\n+        state_shape = [batch] + tf.TensorShape(flat_state).as_list()\n+        return tf.TensorShape(state_shape)\n+      state_shape = tf.nest.map_structure(_get_state_shape, state_size)\n+      return generic_utils.to_list(output_shape) + tf.nest.flatten(state_shape)\n+    else:\n+      return output_shape\n+\n+  def compute_mask(self, inputs, mask):\n+    # Time step masks must be the same for each input.\n+    # This is because the mask for an RNN is of size [batch, time_steps, 1],\n+    # and specifies which time steps should be skipped, and a time step\n+    # must be skipped for all inputs.\n+    # TODO(scottzhu): Should we accept multiple different masks?\n+    mask = tf.nest.flatten(mask)[0]\n+    output_mask = mask if self.return_sequences else None\n+    if self.return_state:\n+      state_mask = [None for _ in self.states]\n+      return [output_mask] + state_mask\n+    else:\n+      return output_mask\n+\n+  def build(self, input_shape):\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+      # The input_shape here could be a nest structure.\n+\n+    # do the tensor_shape to shapes here. The input could be single tensor, or a\n+    # nested structure of tensors.\n+    def get_input_spec(shape):\n+      \"\"\"Convert input shape to InputSpec.\"\"\"\n+      if isinstance(shape, tf.TensorShape):\n+        input_spec_shape = shape.as_list()\n+      else:\n+        input_spec_shape = list(shape)\n+      batch_index, time_step_index = (1, 0) if self.time_major else (0, 1)\n+      if not self.stateful:\n+        input_spec_shape[batch_index] = None\n+      input_spec_shape[time_step_index] = None\n+      return InputSpec(shape=tuple(input_spec_shape))\n+\n+    def get_step_input_shape(shape):\n+      if isinstance(shape, tf.TensorShape):\n+        shape = tuple(shape.as_list())\n+      # remove the timestep from the input_shape\n+      return shape[1:] if self.time_major else (shape[0],) + shape[2:]\n+\n+    def get_state_spec(shape):\n+      state_spec_shape = tf.TensorShape(shape).as_list()\n+      # append batch dim\n+      state_spec_shape = [None] + state_spec_shape\n+      return InputSpec(shape=tuple(state_spec_shape))\n+\n+    # Check whether the input shape contains any nested shapes. It could be\n+    # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n+    # inputs.\n+    try:\n+      input_shape = tf.TensorShape(input_shape)\n+    except (ValueError, TypeError):\n+      # A nested tensor input\n+      pass\n+\n+    if not tf.nest.is_nested(input_shape):\n+      # This indicates the there is only one input.\n+      if self.input_spec is not None:\n+        self.input_spec[0] = get_input_spec(input_shape)\n+      else:\n+        self.input_spec = [get_input_spec(input_shape)]\n+      step_input_shape = get_step_input_shape(input_shape)\n+    else:\n+      if self.input_spec is not None:\n+        self.input_spec[0] = tf.nest.map_structure(get_input_spec, input_shape)\n+      else:\n+        self.input_spec = generic_utils.to_list(\n+            tf.nest.map_structure(get_input_spec, input_shape))\n+      step_input_shape = tf.nest.map_structure(get_step_input_shape,\n+                                               input_shape)\n+\n+    # allow cell (if layer) to build before we set or validate state_spec.\n+    if isinstance(self.cell, base_layer.Layer) and not self.cell.built:\n+      with backend.name_scope(self.cell.name):\n+        self.cell.build(step_input_shape)\n+        self.cell.built = True\n+\n+    # set or validate state_spec\n+    if rnn_utils.is_multiple_state(self.cell.state_size):\n+      state_size = list(self.cell.state_size)\n+    else:\n+      state_size = [self.cell.state_size]\n+\n+    if self.state_spec is not None:\n+      # initial_state was passed in call, check compatibility\n+      self._validate_state_spec(state_size, self.state_spec)\n+    else:\n+      if tf.nest.is_nested(state_size):\n+        self.state_spec = tf.nest.map_structure(get_state_spec, state_size)\n+      else:\n+        self.state_spec = [\n+            InputSpec(shape=[None] + tf.TensorShape(dim).as_list())\n+            for dim in state_size\n+        ]\n+      # ensure the generated state_spec is correct.\n+      self._validate_state_spec(state_size, self.state_spec)\n+    if self.stateful:\n+      self.reset_states()\n+    self.built = True\n+\n+  @staticmethod\n+  def _validate_state_spec(cell_state_sizes, init_state_specs):\n+    \"\"\"Validate the state spec between the initial_state and the state_size.\n+\n+    Args:\n+      cell_state_sizes: list, the `state_size` attribute from the cell.\n+      init_state_specs: list, the `state_spec` from the initial_state that is\n+        passed in `call()`.\n+\n+    Raises:\n+      ValueError: When initial state spec is not compatible with the state size.\n+    \"\"\"\n+    validation_error = ValueError(\n+        'An `initial_state` was passed that is not compatible with '\n+        '`cell.state_size`. Received `state_spec`={}; '\n+        'however `cell.state_size` is '\n+        '{}'.format(init_state_specs, cell_state_sizes))\n+    flat_cell_state_sizes = tf.nest.flatten(cell_state_sizes)\n+    flat_state_specs = tf.nest.flatten(init_state_specs)\n+\n+    if len(flat_cell_state_sizes) != len(flat_state_specs):\n+      raise validation_error\n+    for cell_state_spec, cell_state_size in zip(flat_state_specs,\n+                                                flat_cell_state_sizes):\n+      if not tf.TensorShape(\n+          # Ignore the first axis for init_state which is for batch\n+          cell_state_spec.shape[1:]).is_compatible_with(\n+              tf.TensorShape(cell_state_size)):\n+        raise validation_error\n+\n+  @doc_controls.do_not_doc_inheritable\n+  def get_initial_state(self, inputs):\n+    get_initial_state_fn = getattr(self.cell, 'get_initial_state', None)\n+\n+    if tf.nest.is_nested(inputs):\n+      # The input are nested sequences. Use the first element in the seq to get\n+      # batch size and dtype.\n+      inputs = tf.nest.flatten(inputs)[0]\n+\n+    input_shape = tf.shape(inputs)\n+    batch_size = input_shape[1] if self.time_major else input_shape[0]\n+    dtype = inputs.dtype\n+    if get_initial_state_fn:\n+      init_state = get_initial_state_fn(\n+          inputs=None, batch_size=batch_size, dtype=dtype)\n+    else:\n+      init_state = rnn_utils.generate_zero_filled_state(\n+          batch_size, self.cell.state_size, dtype)\n+    # Keras RNN expect the states in a list, even if it's a single state tensor.\n+    if not tf.nest.is_nested(init_state):\n+      init_state = [init_state]\n+    # Force the state to be a list in case it is a namedtuple eg LSTMStateTuple.\n+    return list(init_state)\n+\n+  def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n+    inputs, initial_state, constants = rnn_utils.standardize_args(\n+        inputs, initial_state, constants, self._num_constants)\n+\n+    if initial_state is None and constants is None:\n+      return super(RNN, self).__call__(inputs, **kwargs)\n+\n+    # If any of `initial_state` or `constants` are specified and are Keras\n+    # tensors, then add them to the inputs and temporarily modify the\n+    # input_spec to include them.\n+\n+    additional_inputs = []\n+    additional_specs = []\n+    if initial_state is not None:\n+      additional_inputs += initial_state\n+      self.state_spec = tf.nest.map_structure(\n+          lambda s: InputSpec(shape=backend.int_shape(s)), initial_state)\n+      additional_specs += self.state_spec\n+    if constants is not None:\n+      additional_inputs += constants\n+      self.constants_spec = [\n+          InputSpec(shape=backend.int_shape(constant)) for constant in constants\n+      ]\n+      self._num_constants = len(constants)\n+      additional_specs += self.constants_spec\n+    # additional_inputs can be empty if initial_state or constants are provided\n+    # but empty (e.g. the cell is stateless).\n+    flat_additional_inputs = tf.nest.flatten(additional_inputs)\n+    is_keras_tensor = backend.is_keras_tensor(\n+        flat_additional_inputs[0]) if flat_additional_inputs else True\n+    for tensor in flat_additional_inputs:\n+      if backend.is_keras_tensor(tensor) != is_keras_tensor:\n+        raise ValueError(\n+            'The initial state or constants of an RNN layer cannot be '\n+            'specified via a mix of Keras tensors and non-Keras tensors '\n+            '(a \"Keras tensor\" is a tensor that was returned by a Keras layer '\n+            ' or by `Input` during Functional model construction). '\n+            f'Received: initial_state={initial_state}, constants={constants}')\n+\n+    if is_keras_tensor:\n+      # Compute the full input spec, including state and constants\n+      full_input = [inputs] + additional_inputs\n+      if self.built:\n+        # Keep the input_spec since it has been populated in build() method.\n+        full_input_spec = self.input_spec + additional_specs\n+      else:\n+        # The original input_spec is None since there could be a nested tensor\n+        # input. Update the input_spec to match the inputs.\n+        full_input_spec = generic_utils.to_list(\n+            tf.nest.map_structure(lambda _: None, inputs)) + additional_specs\n+      # Perform the call with temporarily replaced input_spec\n+      self.input_spec = full_input_spec\n+      output = super(RNN, self).__call__(full_input, **kwargs)\n+      # Remove the additional_specs from input spec and keep the rest. It is\n+      # important to keep since the input spec was populated by build(), and\n+      # will be reused in the stateful=True.\n+      self.input_spec = self.input_spec[:-len(additional_specs)]\n+      return output\n+    else:\n+      if initial_state is not None:\n+        kwargs['initial_state'] = initial_state\n+      if constants is not None:\n+        kwargs['constants'] = constants\n+      return super(RNN, self).__call__(inputs, **kwargs)\n+\n+  def call(self,\n+           inputs,\n+           mask=None,\n+           training=None,\n+           initial_state=None,\n+           constants=None):\n+    # The input should be dense, padded with zeros. If a ragged input is fed\n+    # into the layer, it is padded and the row lengths are used for masking.\n+    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n+    is_ragged_input = (row_lengths is not None)\n+    self._validate_args_if_ragged(is_ragged_input, mask)\n+\n+    inputs, initial_state, constants = self._process_inputs(\n+        inputs, initial_state, constants)\n+\n+    self._maybe_reset_cell_dropout_mask(self.cell)\n+    if isinstance(self.cell, StackedRNNCells):\n+      for cell in self.cell.cells:\n+        self._maybe_reset_cell_dropout_mask(cell)\n+\n+    if mask is not None:\n+      # Time step masks must be the same for each input.\n+      # TODO(scottzhu): Should we accept multiple different masks?\n+      mask = tf.nest.flatten(mask)[0]\n+\n+    if tf.nest.is_nested(inputs):\n+      # In the case of nested input, use the first element for shape check.\n+      input_shape = backend.int_shape(tf.nest.flatten(inputs)[0])\n+    else:\n+      input_shape = backend.int_shape(inputs)\n+    timesteps = input_shape[0] if self.time_major else input_shape[1]\n+    if self.unroll and timesteps is None:\n+      raise ValueError('Cannot unroll a RNN if the '\n+                       'time dimension is undefined. \\n'\n+                       '- If using a Sequential model, '\n+                       'specify the time dimension by passing '\n+                       'an `input_shape` or `batch_input_shape` '\n+                       'argument to your first layer. If your '\n+                       'first layer is an Embedding, you can '\n+                       'also use the `input_length` argument.\\n'\n+                       '- If using the functional API, specify '\n+                       'the time dimension by passing a `shape` '\n+                       'or `batch_shape` argument to your Input layer.')\n+\n+    kwargs = {}\n+    if generic_utils.has_arg(self.cell.call, 'training'):\n+      kwargs['training'] = training\n+\n+    # TF RNN cells expect single tensor as state instead of list wrapped tensor.\n+    is_tf_rnn_cell = getattr(self.cell, '_is_tf_rnn_cell', None) is not None\n+    # Use the __call__ function for callable objects, eg layers, so that it\n+    # will have the proper name scopes for the ops, etc.\n+    cell_call_fn = self.cell.__call__ if callable(self.cell) else self.cell.call\n+    if constants:\n+      if not generic_utils.has_arg(self.cell.call, 'constants'):\n+        raise ValueError(\n+            f'RNN cell {self.cell} does not support constants. '\n+            f'Received: constants={constants}')\n+\n+      def step(inputs, states):\n+        constants = states[-self._num_constants:]  # pylint: disable=invalid-unary-operand-type\n+        states = states[:-self._num_constants]  # pylint: disable=invalid-unary-operand-type\n+\n+        states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n+        output, new_states = cell_call_fn(\n+            inputs, states, constants=constants, **kwargs)\n+        if not tf.nest.is_nested(new_states):\n+          new_states = [new_states]\n+        return output, new_states\n+    else:\n+\n+      def step(inputs, states):\n+        states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n+        output, new_states = cell_call_fn(inputs, states, **kwargs)\n+        if not tf.nest.is_nested(new_states):\n+          new_states = [new_states]\n+        return output, new_states\n+    last_output, outputs, states = backend.rnn(\n+        step,\n+        inputs,\n+        initial_state,\n+        constants=constants,\n+        go_backwards=self.go_backwards,\n+        mask=mask,\n+        unroll=self.unroll,\n+        input_length=row_lengths if row_lengths is not None else timesteps,\n+        time_major=self.time_major,\n+        zero_output_for_mask=self.zero_output_for_mask)\n+\n+    if self.stateful:\n+      updates = [\n+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n+          for self_state, state in zip(\n+              tf.nest.flatten(self.states), tf.nest.flatten(states))\n+      ]\n+      self.add_update(updates)\n+\n+    if self.return_sequences:\n+      output = backend.maybe_convert_to_ragged(\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n+    else:\n+      output = last_output\n+\n+    if self.return_state:\n+      if not isinstance(states, (list, tuple)):\n+        states = [states]\n+      else:\n+        states = list(states)\n+      return generic_utils.to_list(output) + states\n+    else:\n+      return output\n+\n+  def _process_inputs(self, inputs, initial_state, constants):\n+    # input shape: `(samples, time (padded with zeros), input_dim)`\n+    # note that the .build() method of subclasses MUST define\n+    # self.input_spec and self.state_spec with complete input shapes.\n+    if (isinstance(inputs, collections.abc.Sequence)\n+        and not isinstance(inputs, tuple)):\n+      # get initial_state from full input spec\n+      # as they could be copied to multiple GPU.\n+      if not self._num_constants:\n+        initial_state = inputs[1:]\n+      else:\n+        initial_state = inputs[1:-self._num_constants]\n+        constants = inputs[-self._num_constants:]\n+      if len(initial_state) == 0:\n+        initial_state = None\n+      inputs = inputs[0]\n+\n+    if self.stateful:\n+      if initial_state is not None:\n+        # When layer is stateful and initial_state is provided, check if the\n+        # recorded state is same as the default value (zeros). Use the recorded\n+        # state if it is not same as the default.\n+        non_zero_count = tf.add_n([tf.math.count_nonzero(s)\n+                                   for s in tf.nest.flatten(self.states)])\n+        # Set strict = True to keep the original structure of the state.\n+        initial_state = tf.compat.v1.cond(non_zero_count > 0,\n+                                          true_fn=lambda: self.states,\n+                                          false_fn=lambda: initial_state,\n+                                          strict=True)\n+      else:\n+        initial_state = self.states\n+      initial_state = tf.nest.map_structure(\n+          # When the layer has a inferred dtype, use the dtype from the cell.\n+          lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype),\n+          initial_state\n+      )\n+    elif initial_state is None:\n+      initial_state = self.get_initial_state(inputs)\n+\n+    if len(initial_state) != len(self.states):\n+      raise ValueError(f'Layer has {len(self.states)} '\n+                       f'states but was passed {len(initial_state)} initial '\n+                       f'states. Received: initial_state={initial_state}')\n+    return inputs, initial_state, constants\n+\n+  def _validate_args_if_ragged(self, is_ragged_input, mask):\n+    if not is_ragged_input:\n+      return\n+\n+    if mask is not None:\n+      raise ValueError(f'The mask that was passed in was {mask}, which '\n+                       'cannot be applied to RaggedTensor inputs. Please '\n+                       'make sure that there is no mask injected by upstream '\n+                       'layers.')\n+    if self.unroll:\n+      raise ValueError('The input received contains RaggedTensors and does '\n+                       'not support unrolling. Disable unrolling by passing '\n+                       '`unroll=False` in the RNN Layer constructor.')\n+\n+  def _maybe_reset_cell_dropout_mask(self, cell):\n+    if isinstance(cell, DropoutRNNCellMixin):\n+      cell.reset_dropout_mask()\n+      cell.reset_recurrent_dropout_mask()\n+\n+  def reset_states(self, states=None):\n+    \"\"\"Reset the recorded states for the stateful RNN layer.\n+\n+    Can only be used when RNN layer is constructed with `stateful` = `True`.\n+    Args:\n+      states: Numpy arrays that contains the value for the initial state, which\n+        will be feed to cell at the first time step. When the value is None,\n+        zero filled numpy array will be created based on the cell state size.\n+\n+    Raises:\n+      AttributeError: When the RNN layer is not stateful.\n+      ValueError: When the batch size of the RNN layer is unknown.\n+      ValueError: When the input numpy array is not compatible with the RNN\n+        layer state, either size wise or dtype wise.\n+    \"\"\"\n+    if not self.stateful:\n+      raise AttributeError('Layer must be stateful.')\n+    spec_shape = None\n+    if self.input_spec is not None:\n+      spec_shape = tf.nest.flatten(self.input_spec[0])[0].shape\n+    if spec_shape is None:\n+      # It is possible to have spec shape to be None, eg when construct a RNN\n+      # with a custom cell, or standard RNN layers (LSTM/GRU) which we only know\n+      # it has 3 dim input, but not its full shape spec before build().\n+      batch_size = None\n+    else:\n+      batch_size = spec_shape[1] if self.time_major else spec_shape[0]\n+    if not batch_size:\n+      raise ValueError('If a RNN is stateful, it needs to know '\n+                       'its batch size. Specify the batch size '\n+                       'of your input tensors: \\n'\n+                       '- If using a Sequential model, '\n+                       'specify the batch size by passing '\n+                       'a `batch_input_shape` '\n+                       'argument to your first layer.\\n'\n+                       '- If using the functional API, specify '\n+                       'the batch size by passing a '\n+                       '`batch_shape` argument to your Input layer.')\n+    # initialize state if None\n+    if tf.nest.flatten(self.states)[0] is None:\n+      if getattr(self.cell, 'get_initial_state', None):\n+        flat_init_state_values = tf.nest.flatten(self.cell.get_initial_state(\n+            inputs=None, batch_size=batch_size,\n+            # Use variable_dtype instead of compute_dtype, since the state is\n+            # stored in a variable\n+            dtype=self.variable_dtype or backend.floatx()))\n+      else:\n+        flat_init_state_values = tf.nest.flatten(\n+            rnn_utils.generate_zero_filled_state(\n+                batch_size, self.cell.state_size, self.variable_dtype or\n+                backend.floatx()))\n+      flat_states_variables = tf.nest.map_structure(\n+          backend.variable, flat_init_state_values)\n+      self.states = tf.nest.pack_sequence_as(self.cell.state_size,\n+                                             flat_states_variables)\n+      if not tf.nest.is_nested(self.states):\n+        self.states = [self.states]\n+    elif states is None:\n+      for state, size in zip(tf.nest.flatten(self.states),\n+                             tf.nest.flatten(self.cell.state_size)):\n+        backend.set_value(\n+            state,\n+            np.zeros([batch_size] + tf.TensorShape(size).as_list()))\n+    else:\n+      flat_states = tf.nest.flatten(self.states)\n+      flat_input_states = tf.nest.flatten(states)\n+      if len(flat_input_states) != len(flat_states):\n+        raise ValueError(f'Layer {self.name} expects {len(flat_states)} '\n+                         f'states, but it received {len(flat_input_states)} '\n+                         f'state values. States received: {states}')\n+      set_value_tuples = []\n+      for i, (value, state) in enumerate(zip(flat_input_states,\n+                                             flat_states)):\n+        if value.shape != state.shape:\n+          raise ValueError(\n+              f'State {i} is incompatible with layer {self.name}: '\n+              f'expected shape={(batch_size, state)} '\n+              f'but found shape={value.shape}')\n+        set_value_tuples.append((state, value))\n+      backend.batch_set_value(set_value_tuples)\n+\n+  def get_config(self):\n+    config = {\n+        'return_sequences': self.return_sequences,\n+        'return_state': self.return_state,\n+        'go_backwards': self.go_backwards,\n+        'stateful': self.stateful,\n+        'unroll': self.unroll,\n+        'time_major': self.time_major\n+    }\n+    if self._num_constants:\n+      config['num_constants'] = self._num_constants\n+    if self.zero_output_for_mask:\n+      config['zero_output_for_mask'] = self.zero_output_for_mask\n+\n+    config['cell'] = generic_utils.serialize_keras_object(self.cell)\n+    base_config = super(RNN, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n+    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\n+    num_constants = config.pop('num_constants', 0)\n+    layer = cls(cell, **config)\n+    layer._num_constants = num_constants  # pylint: disable=protected-access\n+    return layer\n+\n+  @property\n+  def _trackable_saved_model_saver(self):\n+    return layer_serialization.RNNSavedModelSaver(self)\n\n@@ -16,24 +16,27 @@\n \n See also: lstm_test.py, gru_test.py, simplernn_test.py.\n \"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n \n import collections\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n+from keras.engine import base_layer_utils\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_v1\n+from keras.layers.rnn import legacy_cells\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.engine import base_layer_utils\n-from keras.layers import recurrent as rnn_v1\n-from keras.layers import recurrent_v2 as rnn_v2\n-from keras.layers.legacy_rnn import rnn_cell_impl\n from keras.utils import generic_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.training.tracking import util as trackable_util\n \n+\n # Used for nested input/output/state RNN test.\n NestedInput = collections.namedtuple('NestedInput', ['t1', 't2'])\n NestedState = collections.namedtuple('NestedState', ['s1', 's2'])\n@@ -246,8 +249,8 @@ class RNNTest(test_combinations.TestCase):\n     y = layer(x)\n     model = keras.models.Model(x, y)\n     model.compile(\n-        optimizer=\"rmsprop\",\n-        loss=\"mse\",\n+        optimizer='rmsprop',\n+        loss='mse',\n         run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n@@ -405,10 +408,10 @@ class RNNTest(test_combinations.TestCase):\n     self.assertAllClose(y_np, y_np_3, atol=1e-4)\n \n     # Test stacking.\n-    cells = [keras.layers.recurrent.GRUCell(8),\n+    cells = [gru_v1.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n-    layer = keras.layers.recurrent.RNN(cells)\n+    layer = keras.layers.RNN(cells)\n     y = layer(x, constants=c)\n     model = keras.models.Model([x, c], y)\n     model.compile(\n@@ -423,8 +426,8 @@ class RNNTest(test_combinations.TestCase):\n     # Test GRUCell reset_after property.\n     x = keras.Input((None, 5))\n     c = keras.Input((3,))\n-    cells = [keras.layers.recurrent.GRUCell(32, reset_after=True)]\n-    layer = keras.layers.recurrent.RNN(cells)\n+    cells = [gru_v1.GRUCell(32, reset_after=True)]\n+    layer = keras.layers.RNN(cells)\n     y = layer(x, constants=c)\n     model = keras.models.Model([x, c], y)\n     model.compile(\n@@ -443,7 +446,7 @@ class RNNTest(test_combinations.TestCase):\n     weights = model.get_weights()\n     config = layer.get_config()\n     with generic_utils.CustomObjectScope(custom_objects):\n-      layer = keras.layers.recurrent.RNN.from_config(config.copy())\n+      layer = keras.layers.RNN.from_config(config.copy())\n     y = layer(x, constants=c)\n     model = keras.models.Model([x, c], y)\n     model.set_weights(weights)\n@@ -466,10 +469,10 @@ class RNNTest(test_combinations.TestCase):\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n-    cells = [keras.layers.recurrent.GRUCell(8),\n+    cells = [gru_v1.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n-    layer = keras.layers.recurrent.RNN(cells)\n+    layer = keras.layers.RNN(cells)\n     y = layer(x, constants=c)\n     model = keras.models.Model(x, y)\n     model.compile(\n@@ -543,10 +546,10 @@ class RNNTest(test_combinations.TestCase):\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n-    cells = [keras.layers.recurrent.GRUCell(8),\n+    cells = [gru_v1.GRUCell(8),\n              RNNCellWithConstants(12, constant_size=3),\n              RNNCellWithConstants(32, constant_size=3)]\n-    layer = keras.layers.recurrent.RNN(cells)\n+    layer = keras.layers.RNN(cells)\n     s = [tf.zeros([6, 8], dtype=tf.float32),\n          tf.zeros([6, 12], dtype=tf.float32),\n          tf.zeros([6, 32], dtype=tf.float32)]\n@@ -735,8 +738,10 @@ class RNNTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       *test_utils.generate_combinations_with_testcase_name(\n-          layer=[rnn_v1.SimpleRNN, rnn_v1.GRU, rnn_v1.LSTM,\n-                 rnn_v2.GRU, rnn_v2.LSTM],\n+          layer=[\n+              keras.layers.SimpleRNN, gru_v1.GRU, lstm_v1.LSTM, gru.GRU,\n+              lstm.LSTM\n+          ],\n           unroll=[True, False]))\n   def test_rnn_dropout(self, layer, unroll):\n     rnn_layer = layer(3, dropout=0.1, recurrent_dropout=0.1, unroll=unroll)\n@@ -1348,7 +1353,7 @@ class RNNTest(test_combinations.TestCase):\n         recurrent_activation='sigmoid',\n         implementation=2)\n     tf_lstm_cell_output = _run_cell(\n-        rnn_cell_impl.LSTMCell,\n+        legacy_cells.LSTMCell,\n         use_peepholes=True,\n         initializer=tf.compat.v1.ones_initializer)\n     self.assertNotAllClose(first_implementation_output, no_peephole_output)\n@@ -1635,7 +1640,7 @@ class RNNTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       *test_utils.generate_combinations_with_testcase_name(layer=[\n-          rnn_v1.SimpleRNN, rnn_v1.GRU, rnn_v1.LSTM, rnn_v2.GRU, rnn_v2.LSTM\n+          keras.layers.SimpleRNN, gru_v1.GRU, lstm_v1.LSTM, gru.GRU, lstm.LSTM\n       ]))\n   def test_rnn_with_ragged_input(self, layer):\n     ragged_data = tf.ragged.constant(\n@@ -1800,7 +1805,7 @@ class RNNTest(test_combinations.TestCase):\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 5)))\n \n   @parameterized.parameters(\n-      [rnn_v1.SimpleRNN, rnn_v1.GRU, rnn_v1.LSTM, rnn_v2.GRU, rnn_v2.LSTM])\n+      [keras.layers.SimpleRNN, gru_v1.GRU, lstm_v1.LSTM, gru.GRU, lstm.LSTM])\n   def test_for_enable_caching_device_for_layer(self, layer_cls):\n     expected_caching_device = tf.compat.v1.executing_eagerly_outside_functions()\n     layer = layer_cls(1)\n@@ -1817,8 +1822,8 @@ class RNNTest(test_combinations.TestCase):\n     self.assertEqual(config['enable_caching_device'], non_default_value)\n \n   @parameterized.parameters(\n-      [rnn_v1.SimpleRNNCell, rnn_v1.GRUCell, rnn_v1.LSTMCell, rnn_v2.GRUCell,\n-       rnn_v2.LSTMCell])\n+      [keras.layers.SimpleRNNCell, gru_v1.GRUCell, lstm_v1.LSTMCell,\n+       gru.GRUCell, lstm.LSTMCell])\n   def test_for_enable_caching_device_for_cell(self, cell_cls):\n     expected_caching_device = tf.compat.v1.executing_eagerly_outside_functions()\n     cell = cell_cls(1)\n@@ -1906,15 +1911,15 @@ class Minimal2DRNNCell(keras.layers.Layer):\n     prev_output = states[0]\n     h = tf.einsum('bij,ijkl->bkl', inputs, self.kernel)\n     h += tf.expand_dims(self.bias, axis=0)\n-    output = h + tf.einsum('bij,ijkl->bkl', prev_output,\n-                                         self.recurring_kernel)\n+    output = h + tf.einsum('bij,ijkl->bkl', prev_output, self.recurring_kernel)\n     return output, [output]\n \n \n class PlusOneRNNCell(keras.layers.Layer):\n   \"\"\"Add one to the input and state.\n \n-  This cell is used for testing state_size and output_size.\"\"\"\n+  This cell is used for testing state_size and output_size.\n+  \"\"\"\n \n   def __init__(self, num_unit, **kwargs):\n     self.state_size = num_unit\n@@ -1966,8 +1971,7 @@ class NestedCell(keras.layers.Layer):\n     s1, s2 = states\n \n     output_1 = tf.matmul(flatten_inputs[0], self.kernel_1)\n-    output_2_3 = tf.einsum('bij,ijkl->bkl', flatten_inputs[1],\n-                                         self.kernel_2_3)\n+    output_2_3 = tf.einsum('bij,ijkl->bkl', flatten_inputs[1], self.kernel_2_3)\n     state_1 = s1 + output_1\n     state_2_3 = s2 + output_2_3\n \n\n@@ -0,0 +1,71 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Base class for wrapper layers.\n+\n+Wrappers are layers that augment the functionality of another layer.\n+\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import copy\n+\n+from keras.engine.base_layer import Layer\n+from keras.utils import generic_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Wrapper')\n+class Wrapper(Layer):\n+  \"\"\"Abstract wrapper base class.\n+\n+  Wrappers take another layer and augment it in various ways.\n+  Do not use this class as a layer, it is only an abstract base class.\n+  Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n+\n+  Args:\n+    layer: The layer to be wrapped.\n+  \"\"\"\n+\n+  def __init__(self, layer, **kwargs):\n+    assert isinstance(layer, Layer)\n+    self.layer = layer\n+    super(Wrapper, self).__init__(**kwargs)\n+\n+  def build(self, input_shape=None):\n+    if not self.layer.built:\n+      self.layer.build(input_shape)\n+      self.layer.built = True\n+    self.built = True\n+\n+  @property\n+  def activity_regularizer(self):\n+    if hasattr(self.layer, 'activity_regularizer'):\n+      return self.layer.activity_regularizer\n+    else:\n+      return None\n+\n+  def get_config(self):\n+    config = {'layer': generic_utils.serialize_keras_object(self.layer)}\n+    base_config = super(Wrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n+    # Avoid mutating the input dict\n+    config = copy.deepcopy(config)\n+    layer = deserialize_layer(\n+        config.pop('layer'), custom_objects=custom_objects)\n+    return cls(layer, **config)\n\n@@ -0,0 +1,44 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for the Wrapper base class.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+import tensorflow.compat.v2 as tf\n+\n+\n+class ExampleWrapper(keras.layers.Wrapper):\n+  \"\"\"Simple Wrapper subclass.\"\"\"\n+\n+  def call(self, inputs, *args, **kwargs):\n+    return self.layer(inputs, *args, **kwargs)\n+\n+\n+class WrapperTest(parameterized.TestCase):\n+\n+  def test_wrapper_from_config_no_mutation(self):\n+    wrapper = ExampleWrapper(keras.layers.Dense(1))\n+    config = wrapper.get_config()\n+    config_copy = config.copy()\n+    self.assertEqual(config, config_copy)\n+\n+    wrapper_from_config = ExampleWrapper.from_config(config)\n+    new_config = wrapper_from_config.get_config()\n+    self.assertEqual(new_config, config)\n+    self.assertEqual(new_config, config_copy)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -12,365 +12,24 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n-\"\"\"Wrapper layers: layers that augment the functionality of another layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Bidirectional wrapper for RNNs.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n import copy\n+\n from keras import backend\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n-from keras.layers.recurrent import _standardize_args\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_wrapper import Wrapper\n from keras.utils import generic_utils\n-from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.layers.Wrapper')\n-class Wrapper(Layer):\n-  \"\"\"Abstract wrapper base class.\n-\n-  Wrappers take another layer and augment it in various ways.\n-  Do not use this class as a layer, it is only an abstract base class.\n-  Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n-\n-  Args:\n-    layer: The layer to be wrapped.\n-  \"\"\"\n-\n-  def __init__(self, layer, **kwargs):\n-    assert isinstance(layer, Layer)\n-    self.layer = layer\n-    super(Wrapper, self).__init__(**kwargs)\n-\n-  def build(self, input_shape=None):\n-    if not self.layer.built:\n-      self.layer.build(input_shape)\n-      self.layer.built = True\n-    self.built = True\n-\n-  @property\n-  def activity_regularizer(self):\n-    if hasattr(self.layer, 'activity_regularizer'):\n-      return self.layer.activity_regularizer\n-    else:\n-      return None\n-\n-  def get_config(self):\n-    config = {'layer': generic_utils.serialize_keras_object(self.layer)}\n-    base_config = super(Wrapper, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n-    # Avoid mutating the input dict\n-    config = copy.deepcopy(config)\n-    layer = deserialize_layer(\n-        config.pop('layer'), custom_objects=custom_objects)\n-    return cls(layer, **config)\n-\n-\n-@keras_export('keras.layers.TimeDistributed')\n-class TimeDistributed(Wrapper):\n-  \"\"\"This wrapper allows to apply a layer to every temporal slice of an input.\n-\n-  Every input should be at least 3D, and the dimension of index one of the\n-  first input will be considered to be the temporal dimension.\n-\n-  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image\n-  with `channels_last` data format, across 10 timesteps.\n-  The batch input shape is `(32, 10, 128, 128, 3)`.\n-\n-  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each\n-  of the 10 timesteps, independently:\n-\n-  >>> inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n-  >>> conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\n-  >>> outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\n-  >>> outputs.shape\n-  TensorShape([None, 10, 126, 126, 64])\n-\n-  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the\n-  timestamps, the same set of weights are used at each timestamp.\n-\n-  Args:\n-    layer: a `tf.keras.layers.Layer` instance.\n-\n-  Call arguments:\n-    inputs: Input tensor of shape (batch, time, ...) or nested tensors,\n-      and each of which has shape (batch, time, ...).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the\n-      wrapped layer (only if the layer supports this argument).\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked. This argument is passed to the\n-      wrapped layer (only if the layer supports this argument).\n-\n-  Raises:\n-    ValueError: If not initialized with a `tf.keras.layers.Layer` instance.\n-  \"\"\"\n-\n-  def __init__(self, layer, **kwargs):\n-    if not isinstance(layer, Layer):\n-      raise ValueError(\n-          'Please initialize `TimeDistributed` layer with a '\n-          f'`tf.keras.layers.Layer` instance. Received: {layer}')\n-    super(TimeDistributed, self).__init__(layer, **kwargs)\n-    self.supports_masking = True\n-\n-    # It is safe to use the fast, reshape-based approach with all of our\n-    # built-in Layers.\n-    self._always_use_reshape = (\n-        layer_utils.is_builtin_layer(layer) and\n-        not getattr(layer, 'stateful', False))\n-\n-  def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n-    \"\"\"Finds non-specific dimensions in the static shapes.\n-\n-    The static shapes are replaced with the corresponding dynamic shapes of the\n-    tensor.\n-    Args:\n-      init_tuple: a tuple, the first part of the output shape\n-      tensor: the tensor from which to get the (static and dynamic) shapes\n-        as the last part of the output shape\n-      start_idx: int, which indicate the first dimension to take from\n-        the static shape of the tensor\n-      int_shape: an alternative static shape to take as the last part\n-        of the output shape\n-    Returns:\n-      The new int_shape with the first part from init_tuple\n-      and the last part from either `int_shape` (if provided)\n-      or `tensor.shape`, where every `None` is replaced by\n-      the corresponding dimension from `tf.shape(tensor)`.\n-    \"\"\"\n-    # replace all None in int_shape by backend.shape\n-    if int_shape is None:\n-      int_shape = backend.int_shape(tensor)[start_idx:]\n-    if isinstance(int_shape, tf.TensorShape):\n-      int_shape = int_shape.as_list()\n-    if not any(not s for s in int_shape):\n-      return init_tuple + tuple(int_shape)\n-    shape = backend.shape(tensor)\n-    int_shape = list(int_shape)\n-    for i, s in enumerate(int_shape):\n-      if not s:\n-        int_shape[i] = shape[start_idx + i]\n-    return init_tuple + tuple(int_shape)\n-\n-  def _remove_timesteps(self, dims):\n-    dims = dims.as_list()\n-    return tf.TensorShape([dims[0]] + dims[2:])\n-\n-  def build(self, input_shape):\n-    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n-    input_dims = tf.nest.flatten(\n-        tf.nest.map_structure(lambda x: x.ndims, input_shape))\n-    if any(dim < 3 for dim in input_dims):\n-      raise ValueError(\n-          '`TimeDistributed` Layer should be passed an `input_shape ` '\n-          f'with at least 3 dimensions, received: {input_shape}')\n-    # Don't enforce the batch or time dimension.\n-    self.input_spec = tf.nest.map_structure(\n-        lambda x: InputSpec(shape=[None, None] + x.as_list()[2:]), input_shape)\n-    child_input_shape = tf.nest.map_structure(self._remove_timesteps, input_shape)\n-    child_input_shape = tf_utils.convert_shapes(child_input_shape)\n-    super(TimeDistributed, self).build(tuple(child_input_shape))\n-    self.built = True\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n-\n-    child_input_shape = tf.nest.map_structure(self._remove_timesteps, input_shape)\n-    child_output_shape = self.layer.compute_output_shape(child_input_shape)\n-    child_output_shape = tf_utils.convert_shapes(\n-        child_output_shape, to_tuples=False)\n-    timesteps = tf_utils.convert_shapes(input_shape)\n-    timesteps = tf.nest.flatten(timesteps)[1]\n-\n-    def insert_timesteps(dims):\n-      dims = dims.as_list()\n-      return tf.TensorShape([dims[0], timesteps] + dims[1:])\n-\n-    return tf.nest.map_structure(insert_timesteps, child_output_shape)\n-\n-  def call(self, inputs, training=None, mask=None):\n-    kwargs = {}\n-    if generic_utils.has_arg(self.layer.call, 'training'):\n-      kwargs['training'] = training\n-\n-    input_shape = tf.nest.map_structure(\n-        lambda x: tf.TensorShape(backend.int_shape(x)), inputs)\n-    batch_size = tf_utils.convert_shapes(input_shape)\n-    batch_size = tf.nest.flatten(batch_size)[0]\n-    if batch_size and not self._always_use_reshape:\n-      inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n-      is_ragged_input = row_lengths is not None\n-      input_length = tf_utils.convert_shapes(input_shape)\n-      input_length = tf.nest.flatten(input_length)[1]\n-\n-      # batch size matters, use rnn-based implementation\n-      def step(x, _):\n-        output = self.layer(x, **kwargs)\n-        return output, []\n-\n-      _, outputs, _ = backend.rnn(\n-          step,\n-          inputs,\n-          initial_states=[],\n-          input_length=row_lengths[0] if is_ragged_input else input_length,\n-          mask=mask,\n-          unroll=False)\n-      # pylint: disable=g-long-lambda\n-      y = tf.nest.map_structure(\n-          lambda output: backend.maybe_convert_to_ragged(\n-              is_ragged_input, output, row_lengths), outputs)\n-    else:\n-      # No batch size specified, therefore the layer will be able\n-      # to process batches of any size.\n-      # We can go with reshape-based implementation for performance.\n-      is_ragged_input = tf.nest.map_structure(\n-          lambda x: isinstance(x, tf.RaggedTensor), inputs)\n-      is_ragged_input = tf.nest.flatten(is_ragged_input)\n-      if all(is_ragged_input):\n-        input_values = tf.nest.map_structure(lambda x: x.values, inputs)\n-        input_row_lenghts = tf.nest.map_structure(\n-            lambda x: x.nested_row_lengths()[0], inputs)\n-        y = self.layer(input_values, **kwargs)\n-        y = tf.nest.map_structure(tf.RaggedTensor.from_row_lengths, y,\n-                                  input_row_lenghts)\n-      elif any(is_ragged_input):\n-        raise ValueError('All inputs has to be either ragged or not, '\n-                         f'but not mixed. Received: {inputs}')\n-      else:\n-        input_length = tf_utils.convert_shapes(input_shape)\n-        input_length = tf.nest.flatten(input_length)[1]\n-        if not input_length:\n-          input_length = tf.nest.map_structure(lambda x: tf.shape(x)[1], inputs)\n-          input_length = generic_utils.to_list(tf.nest.flatten(input_length))[0]\n-\n-        inner_input_shape = tf.nest.map_structure(\n-            lambda x: self._get_shape_tuple((-1,), x, 2), inputs)\n-        # Shape: (num_samples * timesteps, ...). And track the\n-        # transformation in self._input_map.\n-        inputs = tf.__internal__.nest.map_structure_up_to(inputs, tf.reshape, inputs,\n-                                          inner_input_shape)\n-        # (num_samples * timesteps, ...)\n-        if generic_utils.has_arg(self.layer.call, 'mask') and mask is not None:\n-          inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n-          kwargs['mask'] = backend.reshape(mask, inner_mask_shape)\n-\n-        y = self.layer(inputs, **kwargs)\n-\n-        # Shape: (num_samples, timesteps, ...)\n-        output_shape = self.compute_output_shape(input_shape)\n-        # pylint: disable=g-long-lambda\n-        output_shape = tf.nest.map_structure(\n-            lambda tensor, int_shape: self._get_shape_tuple(\n-                (-1, input_length), tensor, 1, int_shape[2:]), y, output_shape)\n-        y = tf.__internal__.nest.map_structure_up_to(y, tf.reshape, y, output_shape)\n-        if not tf.executing_eagerly():\n-          # Set the static shape for the result since it might be lost during\n-          # array_ops reshape, eg, some `None` dim in the result could be\n-          # inferred.\n-          tf.__internal__.nest.map_structure_up_to(\n-              y, lambda tensor, shape: tensor.set_shape(shape), y,\n-              self.compute_output_shape(input_shape))\n-\n-    return y\n-\n-  def compute_mask(self, inputs, mask=None):\n-    \"\"\"Computes an output mask tensor for Embedding layer.\n-\n-    This is based on the inputs, mask, and the inner layer.\n-    If batch size is specified:\n-    Simply return the input `mask`. (An rnn-based implementation with\n-    more than one rnn inputs is required but not supported in tf.keras yet.)\n-    Otherwise we call `compute_mask` of the inner layer at each time step.\n-    If the output mask at each time step is not `None`:\n-    (E.g., inner layer is Masking or RNN)\n-    Concatenate all of them and return the concatenation.\n-    If the output mask at each time step is `None` and the input mask is not\n-    `None`:(E.g., inner layer is Dense)\n-    Reduce the input_mask to 2 dimensions and return it.\n-    Otherwise (both the output mask and the input mask are `None`):\n-    (E.g., `mask` is not used at all)\n-    Return `None`.\n-\n-    Args:\n-      inputs: Tensor with shape [batch size, timesteps, ...] indicating the\n-        input to TimeDistributed. If static shape information is available for\n-        \"batch size\", `mask` is returned unmodified.\n-      mask: Either None (indicating no masking) or a Tensor indicating the\n-        input mask for TimeDistributed. The shape can be static or dynamic.\n-\n-    Returns:\n-      Either None (no masking), or a [batch size, timesteps, ...] Tensor with\n-      an output mask for the TimeDistributed layer with the shape beyond the\n-      second dimension being the value of the input mask shape(if the computed\n-      output mask is none), an output mask with the shape beyond the first\n-      dimension being the value of the mask shape(if mask is not None) or\n-      output mask with the shape beyond the first dimension being the\n-      value of the computed output shape.\n-\n-    \"\"\"\n-    # cases need to call the layer.compute_mask when input_mask is None:\n-    # Masking layer and Embedding layer with mask_zero\n-    input_shape = tf.nest.map_structure(\n-        lambda x: tf.TensorShape(backend.int_shape(x)), inputs)\n-    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n-    batch_size = tf_utils.convert_shapes(input_shape)\n-    batch_size = tf.nest.flatten(batch_size)[0]\n-    is_ragged_input = tf.nest.map_structure(\n-        lambda x: isinstance(x, tf.RaggedTensor), inputs)\n-    is_ragged_input = generic_utils.to_list(tf.nest.flatten(is_ragged_input))\n-    if batch_size and not self._always_use_reshape or any(is_ragged_input):\n-      # batch size matters, we currently do not handle mask explicitly, or if\n-      # the layer always uses reshape approach, or the input is a ragged tensor.\n-      return mask\n-    inner_mask = mask\n-    if inner_mask is not None:\n-      inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n-      inner_mask = backend.reshape(inner_mask, inner_mask_shape)\n-    inner_input_shape = tf.nest.map_structure(\n-        lambda tensor: self._get_shape_tuple((-1,), tensor, 2), inputs)\n-    inner_inputs = tf.__internal__.nest.map_structure_up_to(inputs, tf.reshape, inputs,\n-                                            inner_input_shape)\n-    output_mask = self.layer.compute_mask(inner_inputs, inner_mask)\n-    if output_mask is None:\n-      if mask is None:\n-        return None\n-      # input_mask is not None, and output_mask is None:\n-      # we should return a not-None mask\n-      output_mask = mask\n-      for _ in range(2, len(backend.int_shape(mask))):\n-        output_mask = backend.any(output_mask, axis=-1)\n-    else:\n-      # output_mask is not None. We need to reshape it\n-      input_length = tf_utils.convert_shapes(input_shape)\n-      input_length = tf.nest.flatten(input_length)[1]\n-      if not input_length:\n-        input_length = tf.nest.map_structure(lambda x: backend.shape(x)[1], inputs)\n-        input_length = tf.nest.flatten(input_length)[0]\n-      output_mask_int_shape = backend.int_shape(output_mask)\n-      if output_mask_int_shape is None:\n-        # if the output_mask does not have a static shape,\n-        # its shape must be the same as mask's\n-        if mask is not None:\n-          output_mask_int_shape = backend.int_shape(mask)\n-        else:\n-          input_shape = generic_utils.to_list(tf.nest.flatten(input_shape))[0]\n-          output_mask_int_shape = backend.compute_output_shape(input_shape)[:-1]\n-      output_mask_shape = self._get_shape_tuple(\n-          (-1, input_length), output_mask, 1, output_mask_int_shape[1:])\n-      output_mask = backend.reshape(output_mask, output_mask_shape)\n-    return output_mask\n-\n-\n @keras_export('keras.layers.Bidirectional')\n class Bidirectional(Wrapper):\n   \"\"\"Bidirectional wrapper for RNNs.\n@@ -510,7 +169,7 @@ class Bidirectional(Wrapper):\n \n   @property\n   def _use_input_spec_as_call_signature(self):\n-    return self.layer._use_input_spec_as_call_signature\n+    return self.layer._use_input_spec_as_call_signature  # pylint: disable=protected-access\n \n   def _verify_layer_config(self):\n     \"\"\"Ensure the forward and backward layers have valid common property.\"\"\"\n@@ -578,7 +237,7 @@ class Bidirectional(Wrapper):\n \n   def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n     \"\"\"`Bidirectional.__call__` implements the same API as the wrapped `RNN`.\"\"\"\n-    inputs, initial_state, constants = _standardize_args(\n+    inputs, initial_state, constants = rnn_utils.standardize_args(\n         inputs, initial_state, constants, self._num_constants)\n \n     if isinstance(inputs, list):\n@@ -804,5 +463,5 @@ class Bidirectional(Wrapper):\n       config['backward_layer'] = backward_layer\n     # Instantiate the wrapper, adjust it and return it.\n     layer = cls(**config)\n-    layer._num_constants = num_constants\n+    layer._num_constants = num_constants  # pylint: disable=protected-access\n     return layer\n\n@@ -12,23 +12,23 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for layer wrappers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for Bidirectional wrapper.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n \n import copy\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n-from tensorflow.python.framework import test_util as tf_test_util\n-from keras.testing_infra import test_combinations\n-from keras.testing_infra import test_utils\n from keras.engine import base_layer_utils\n from keras.layers import core\n-from keras.layers.rnn_cell_wrapper_v2 import ResidualWrapper\n+from keras.layers.rnn.cell_wrappers import ResidualWrapper\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.framework import test_util as tf_test_util\n from tensorflow.python.training.tracking import util as trackable_util\n \n \n@@ -95,451 +95,6 @@ class _AddOneCell(keras.layers.AbstractRNNCell):\n     return outputs, state\n \n \n-class TimeDistributedTest(test_combinations.TestCase):\n-\n-  @test_combinations.generate(\n-      test_combinations.combine(mode=['graph', 'eager']))\n-  def test_timedistributed_dense(self):\n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.TimeDistributed(\n-            keras.layers.Dense(2), input_shape=(3, 4)))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(\n-        np.random.random((10, 3, 4)),\n-        np.random.random((10, 3, 2)),\n-        epochs=1,\n-        batch_size=10)\n-\n-    # test config\n-    model.get_config()\n-\n-    # check whether the model variables are present in the\n-    # trackable list of objects\n-    checkpointed_object_ids = {\n-        id(o) for o in trackable_util.list_objects(model)\n-    }\n-    for v in model.variables:\n-      self.assertIn(id(v), checkpointed_object_ids)\n-\n-  def test_timedistributed_static_batch_size(self):\n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.TimeDistributed(\n-            keras.layers.Dense(2), input_shape=(3, 4), batch_size=10))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(\n-        np.random.random((10, 3, 4)),\n-        np.random.random((10, 3, 2)),\n-        epochs=1,\n-        batch_size=10)\n-\n-  def test_timedistributed_invalid_init(self):\n-    x = tf.constant(np.zeros((1, 1)).astype('float32'))\n-    with self.assertRaisesRegex(\n-        ValueError, 'Please initialize `TimeDistributed` layer with a '\n-        '`tf.keras.layers.Layer` instance.'):\n-      keras.layers.TimeDistributed(x)\n-\n-  def test_timedistributed_conv2d(self):\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(\n-          keras.layers.TimeDistributed(\n-              keras.layers.Conv2D(5, (2, 2), padding='same'),\n-              input_shape=(2, 4, 4, 3)))\n-      model.add(keras.layers.Activation('relu'))\n-      model.compile(optimizer='rmsprop', loss='mse')\n-      model.train_on_batch(\n-          np.random.random((1, 2, 4, 4, 3)), np.random.random((1, 2, 4, 4, 5)))\n-\n-      model = keras.models.model_from_json(model.to_json())\n-      model.summary()\n-\n-  def test_timedistributed_stacked(self):\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(\n-          keras.layers.TimeDistributed(\n-              keras.layers.Dense(2), input_shape=(3, 4)))\n-      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-      model.add(keras.layers.Activation('relu'))\n-      model.compile(optimizer='rmsprop', loss='mse')\n-\n-      model.fit(\n-          np.random.random((10, 3, 4)),\n-          np.random.random((10, 3, 3)),\n-          epochs=1,\n-          batch_size=10)\n-\n-  def test_regularizers(self):\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(\n-          keras.layers.TimeDistributed(\n-              keras.layers.Dense(2, kernel_regularizer='l1',\n-                                 activity_regularizer='l1'),\n-              input_shape=(3, 4)))\n-      model.add(keras.layers.Activation('relu'))\n-      model.compile(optimizer='rmsprop', loss='mse')\n-      self.assertEqual(len(model.losses), 2)\n-\n-  def test_TimeDistributed_learning_phase(self):\n-    with self.cached_session():\n-      # test layers that need learning_phase to be set\n-      np.random.seed(1234)\n-      x = keras.layers.Input(shape=(3, 2))\n-      y = keras.layers.TimeDistributed(keras.layers.Dropout(.999))(\n-          x, training=True)\n-      model = keras.models.Model(x, y)\n-      y = model.predict(np.random.random((10, 3, 2)))\n-      self.assertAllClose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n-\n-  def test_TimeDistributed_batchnorm(self):\n-    with self.cached_session():\n-      # test that wrapped BN updates still work.\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.TimeDistributed(\n-          keras.layers.BatchNormalization(center=True, scale=True),\n-          name='bn',\n-          input_shape=(10, 2)))\n-      model.compile(optimizer='rmsprop', loss='mse')\n-      # Assert that mean and variance are 0 and 1.\n-      td = model.layers[0]\n-      self.assertAllClose(td.get_weights()[2], np.array([0, 0]))\n-      assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n-      # Train\n-      model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n-                           np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n-      # Assert that mean and variance changed.\n-      assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n-      assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n-\n-  def test_TimeDistributed_trainable(self):\n-    # test layers that need learning_phase to be set\n-    x = keras.layers.Input(shape=(3, 2))\n-    layer = keras.layers.TimeDistributed(keras.layers.BatchNormalization())\n-    _ = layer(x)\n-    self.assertEqual(len(layer.trainable_weights), 2)\n-    layer.trainable = False\n-    assert not layer.trainable_weights\n-    layer.trainable = True\n-    assert len(layer.trainable_weights) == 2\n-\n-  def test_TimeDistributed_with_masked_embedding_and_unspecified_shape(self):\n-    with self.cached_session():\n-      # test with unspecified shape and Embeddings with mask_zero\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.TimeDistributed(\n-          keras.layers.Embedding(5, 6, mask_zero=True),\n-          input_shape=(None, None)))  # N by t_1 by t_2 by 6\n-      model.add(keras.layers.TimeDistributed(\n-          keras.layers.SimpleRNN(7, return_sequences=True)))\n-      model.add(keras.layers.TimeDistributed(\n-          keras.layers.SimpleRNN(8, return_sequences=False)))\n-      model.add(keras.layers.SimpleRNN(1, return_sequences=False))\n-      model.compile(optimizer='rmsprop', loss='mse')\n-      model_input = np.random.randint(low=1, high=5, size=(10, 3, 4),\n-                                      dtype='int32')\n-      for i in range(4):\n-        model_input[i, i:, i:] = 0\n-      model.fit(model_input,\n-                np.random.random((10, 1)), epochs=1, batch_size=10)\n-      mask_outputs = [model.layers[0].compute_mask(model.input)]\n-      for layer in model.layers[1:]:\n-        mask_outputs.append(layer.compute_mask(layer.input, mask_outputs[-1]))\n-      func = keras.backend.function([model.input], mask_outputs[:-1])\n-      mask_outputs_val = func([model_input])\n-      ref_mask_val_0 = model_input > 0         # embedding layer\n-      ref_mask_val_1 = ref_mask_val_0          # first RNN layer\n-      ref_mask_val_2 = np.any(ref_mask_val_1, axis=-1)     # second RNN layer\n-      ref_mask_val = [ref_mask_val_0, ref_mask_val_1, ref_mask_val_2]\n-      for i in range(3):\n-        self.assertAllEqual(mask_outputs_val[i], ref_mask_val[i])\n-      self.assertIs(mask_outputs[-1], None)  # final layer\n-\n-  @test_combinations.generate(\n-      test_combinations.combine(mode=['graph', 'eager']))\n-  def test_TimeDistributed_with_masking_layer(self):\n-    # test with Masking layer\n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.TimeDistributed(\n-            keras.layers.Masking(mask_value=0.,), input_shape=(None, 4)))\n-    model.add(keras.layers.TimeDistributed(keras.layers.Dense(5)))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4))\n-    for i in range(4):\n-      model_input[i, i:, :] = 0.\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(model_input, np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n-    mask_outputs = [model.layers[0].compute_mask(model.input)]\n-    mask_outputs += [\n-        model.layers[1].compute_mask(model.layers[1].input, mask_outputs[-1])\n-    ]\n-    func = keras.backend.function([model.input], mask_outputs)\n-    mask_outputs_val = func([model_input])\n-    self.assertEqual((mask_outputs_val[0]).all(), model_input.all())\n-    self.assertEqual((mask_outputs_val[1]).all(), model_input.all())\n-\n-  def test_TimeDistributed_with_different_time_shapes(self):\n-    time_dist = keras.layers.TimeDistributed(keras.layers.Dense(5))\n-    ph_1 = keras.backend.placeholder(shape=(None, 10, 13))\n-    out_1 = time_dist(ph_1)\n-    self.assertEqual(out_1.shape.as_list(), [None, 10, 5])\n-\n-    ph_2 = keras.backend.placeholder(shape=(None, 1, 13))\n-    out_2 = time_dist(ph_2)\n-    self.assertEqual(out_2.shape.as_list(), [None, 1, 5])\n-\n-    ph_3 = keras.backend.placeholder(shape=(None, 1, 18))\n-    with self.assertRaisesRegex(ValueError, 'is incompatible with'):\n-      time_dist(ph_3)\n-\n-  def test_TimeDistributed_with_invalid_dimensions(self):\n-    time_dist = keras.layers.TimeDistributed(keras.layers.Dense(5))\n-    ph = keras.backend.placeholder(shape=(None, 10))\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        '`TimeDistributed` Layer should be passed an `input_shape `'):\n-      time_dist(ph)\n-\n-  @test_combinations.generate(\n-      test_combinations.combine(mode=['graph', 'eager']))\n-  def test_TimeDistributed_reshape(self):\n-\n-    class NoReshapeLayer(keras.layers.Layer):\n-\n-      def call(self, inputs):\n-        return inputs\n-\n-    # Built-in layers that aren't stateful use the reshape implementation.\n-    td1 = keras.layers.TimeDistributed(keras.layers.Dense(5))\n-    self.assertTrue(td1._always_use_reshape)\n-\n-    # Built-in layers that are stateful don't use the reshape implementation.\n-    td2 = keras.layers.TimeDistributed(\n-        keras.layers.RNN(keras.layers.SimpleRNNCell(10), stateful=True))\n-    self.assertFalse(td2._always_use_reshape)\n-\n-    # Custom layers are not allowlisted for the fast reshape implementation.\n-    td3 = keras.layers.TimeDistributed(NoReshapeLayer())\n-    self.assertFalse(td3._always_use_reshape)\n-\n-  @test_combinations.generate(\n-      test_combinations.combine(mode=['graph', 'eager']))\n-  def test_TimeDistributed_output_shape_return_types(self):\n-\n-    class TestLayer(keras.layers.Layer):\n-\n-      def call(self, inputs):\n-        return tf.concat([inputs, inputs], axis=-1)\n-\n-      def compute_output_shape(self, input_shape):\n-        output_shape = tf.TensorShape(input_shape).as_list()\n-        output_shape[-1] = output_shape[-1] * 2\n-        output_shape = tf.TensorShape(output_shape)\n-        return output_shape\n-\n-    class TestListLayer(TestLayer):\n-\n-      def compute_output_shape(self, input_shape):\n-        shape = super(TestListLayer, self).compute_output_shape(input_shape)\n-        return shape.as_list()\n-\n-    class TestTupleLayer(TestLayer):\n-\n-      def compute_output_shape(self, input_shape):\n-        shape = super(TestTupleLayer, self).compute_output_shape(input_shape)\n-        return tuple(shape.as_list())\n-\n-    # Layers can specify output shape as list/tuple/TensorShape\n-    test_layers = [TestLayer, TestListLayer, TestTupleLayer]\n-    for layer in test_layers:\n-      input_layer = keras.layers.TimeDistributed(layer())\n-      inputs = keras.backend.placeholder(shape=(None, 2, 4))\n-      output = input_layer(inputs)\n-      self.assertEqual(output.shape.as_list(), [None, 2, 8])\n-      self.assertEqual(\n-          input_layer.compute_output_shape([None, 2, 4]).as_list(),\n-          [None, 2, 8])\n-\n-  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  # TODO(scottzhu): check why v1 session failed.\n-  def test_TimeDistributed_with_mask_first_implementation(self):\n-    np.random.seed(100)\n-    rnn_layer = keras.layers.LSTM(4, return_sequences=True, stateful=True)\n-\n-    data = np.array([[[[1.0], [1.0]], [[0.0], [1.0]]],\n-                     [[[1.0], [0.0]], [[1.0], [1.0]]],\n-                     [[[1.0], [0.0]], [[1.0], [1.0]]]])\n-    x = keras.layers.Input(shape=(2, 2, 1), batch_size=3)\n-    x_masking = keras.layers.Masking()(x)\n-    y = keras.layers.TimeDistributed(rnn_layer)(x_masking)\n-    model_1 = keras.models.Model(x, y)\n-    model_1.compile(\n-        'rmsprop',\n-        'mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    output_with_mask = model_1.predict(data, steps=1)\n-\n-    y = keras.layers.TimeDistributed(rnn_layer)(x)\n-    model_2 = keras.models.Model(x, y)\n-    model_2.compile(\n-        'rmsprop',\n-        'mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    output = model_2.predict(data, steps=1)\n-\n-    self.assertNotAllClose(output_with_mask, output, atol=1e-7)\n-\n-  @test_combinations.run_all_keras_modes\n-  @parameterized.named_parameters(\n-      *test_utils.generate_combinations_with_testcase_name(\n-          layer=[keras.layers.LSTM,\n-                 keras.layers.Dense]))\n-  def test_TimeDistributed_with_ragged_input(self, layer):\n-    if tf.executing_eagerly():\n-      self.skipTest('b/143103634')\n-    np.random.seed(100)\n-    layer = layer(4)\n-    ragged_data = tf.ragged.constant(\n-        [[[[1.0], [1.0]], [[2.0], [2.0]]],\n-         [[[4.0], [4.0]], [[5.0], [5.0]], [[6.0], [6.0]]],\n-         [[[7.0], [7.0]], [[8.0], [8.0]], [[9.0], [9.0]]]],\n-        ragged_rank=1)\n-\n-    x_ragged = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n-    y_ragged = keras.layers.TimeDistributed(layer)(x_ragged)\n-    model_1 = keras.models.Model(x_ragged, y_ragged)\n-    model_1._run_eagerly = test_utils.should_run_eagerly()\n-    output_ragged = model_1.predict(ragged_data, steps=1)\n-\n-    x_dense = keras.Input(shape=(None, 2, 1), dtype='float32')\n-    masking = keras.layers.Masking()(x_dense)\n-    y_dense = keras.layers.TimeDistributed(layer)(masking)\n-    model_2 = keras.models.Model(x_dense, y_dense)\n-    dense_data = ragged_data.to_tensor()\n-    model_2._run_eagerly = test_utils.should_run_eagerly()\n-    output_dense = model_2.predict(dense_data, steps=1)\n-\n-    output_ragged = convert_ragged_tensor_value(output_ragged)\n-    self.assertAllEqual(output_ragged.to_tensor(), output_dense)\n-\n-  @test_combinations.run_all_keras_modes\n-  def test_TimeDistributed_with_ragged_input_with_batch_size(self):\n-    np.random.seed(100)\n-    layer = keras.layers.Dense(16)\n-\n-    ragged_data = tf.ragged.constant(\n-        [[[[1.0], [1.0]], [[2.0], [2.0]]],\n-         [[[4.0], [4.0]], [[5.0], [5.0]], [[6.0], [6.0]]],\n-         [[[7.0], [7.0]], [[8.0], [8.0]], [[9.0], [9.0]]]],\n-        ragged_rank=1)\n-\n-    # Use the first implementation by specifying batch_size\n-    x_ragged = keras.Input(shape=(None, 2, 1), batch_size=3, dtype='float32',\n-                           ragged=True)\n-    y_ragged = keras.layers.TimeDistributed(layer)(x_ragged)\n-    model_1 = keras.models.Model(x_ragged, y_ragged)\n-    output_ragged = model_1.predict(ragged_data, steps=1)\n-\n-    x_dense = keras.Input(shape=(None, 2, 1), batch_size=3, dtype='float32')\n-    masking = keras.layers.Masking()(x_dense)\n-    y_dense = keras.layers.TimeDistributed(layer)(masking)\n-    model_2 = keras.models.Model(x_dense, y_dense)\n-    dense_data = ragged_data.to_tensor()\n-    output_dense = model_2.predict(dense_data, steps=1)\n-\n-    output_ragged = convert_ragged_tensor_value(output_ragged)\n-    self.assertAllEqual(output_ragged.to_tensor(), output_dense)\n-\n-  def test_TimeDistributed_set_static_shape(self):\n-    layer = keras.layers.TimeDistributed(keras.layers.Conv2D(16, (3, 3)))\n-    inputs = keras.Input(batch_shape=(1, None, 32, 32, 1))\n-    outputs = layer(inputs)\n-    # Make sure the batch dim is not lost after array_ops.reshape.\n-    self.assertListEqual(outputs.shape.as_list(), [1, None, 30, 30, 16])\n-\n-  @test_combinations.run_all_keras_modes\n-  def test_TimeDistributed_with_mimo(self):\n-    dense_1 = keras.layers.Dense(8)\n-    dense_2 = keras.layers.Dense(16)\n-\n-    class TestLayer(keras.layers.Layer):\n-\n-      def __init__(self):\n-        super(TestLayer, self).__init__()\n-        self.dense_1 = dense_1\n-        self.dense_2 = dense_2\n-\n-      def call(self, inputs):\n-        return self.dense_1(inputs[0]), self.dense_2(inputs[1])\n-\n-      def compute_output_shape(self, input_shape):\n-        output_shape_1 = self.dense_1.compute_output_shape(input_shape[0])\n-        output_shape_2 = self.dense_2.compute_output_shape(input_shape[1])\n-        return output_shape_1, output_shape_2\n-\n-    np.random.seed(100)\n-    layer = TestLayer()\n-\n-    data_1 = tf.constant([[[[1.0], [1.0]], [[2.0], [2.0]]],\n-                                 [[[4.0], [4.0]], [[5.0], [5.0]]],\n-                                 [[[7.0], [7.0]], [[8.0], [8.0]]]])\n-\n-    data_2 = tf.constant([[[[1.0], [1.0]], [[2.0], [2.0]]],\n-                                 [[[4.0], [4.0]], [[5.0], [5.0]]],\n-                                 [[[7.0], [7.0]], [[8.0], [8.0]]]])\n-\n-    x1 = keras.Input(shape=(None, 2, 1), dtype='float32')\n-    x2 = keras.Input(shape=(None, 2, 1), dtype='float32')\n-    y1, y2 = keras.layers.TimeDistributed(layer)([x1, x2])\n-    model_1 = keras.models.Model([x1, x2], [y1, y2])\n-    model_1.compile(\n-        optimizer='rmsprop',\n-        loss='mse',\n-        run_eagerly=test_utils.should_run_eagerly())\n-    output_1 = model_1.predict((data_1, data_2), steps=1)\n-\n-    y1 = dense_1(x1)\n-    y2 = dense_2(x2)\n-    model_2 = keras.models.Model([x1, x2], [y1, y2])\n-    output_2 = model_2.predict((data_1, data_2), steps=1)\n-\n-    self.assertAllClose(output_1, output_2)\n-\n-    model_1.fit(\n-        x=[np.random.random((10, 2, 2, 1)),\n-           np.random.random((10, 2, 2, 1))],\n-        y=[np.random.random((10, 2, 2, 8)),\n-           np.random.random((10, 2, 2, 16))],\n-        epochs=1,\n-        batch_size=3)\n-\n-  def test_TimeDistributed_Attention(self):\n-    query_input = keras.layers.Input(shape=(None, 1, 10), dtype='float32')\n-    value_input = keras.layers.Input(shape=(None, 4, 10), dtype='float32')\n-\n-    # Query-value attention of shape [batch_size, Tq, filters].\n-    query_value_attention_seq = keras.layers.TimeDistributed(\n-        keras.layers.Attention())([query_input, value_input])\n-    model = keras.models.Model([query_input, value_input],\n-                               query_value_attention_seq)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(\n-        [np.random.random((10, 8, 1, 10)),\n-         np.random.random((10, 8, 4, 10))],\n-        np.random.random((10, 8, 1, 10)),\n-        epochs=1,\n-        batch_size=10)\n-\n-    # test config and serialization/deserialization\n-    model.get_config()\n-    model = keras.models.model_from_json(model.to_json())\n-    model.summary()\n-\n-\n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n \n@@ -1362,27 +917,6 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertNotAllClose(output1, output2)\n \n \n-class ExampleWrapper(keras.layers.Wrapper):\n-  \"\"\"Simple Wrapper subclass.\"\"\"\n-\n-  def call(self, inputs, *args, **kwargs):\n-    return self.layer(inputs, *args, **kwargs)\n-\n-\n-class WrapperTest(parameterized.TestCase):\n-\n-  def test_wrapper_from_config_no_mutation(self):\n-    wrapper = ExampleWrapper(keras.layers.Dense(1))\n-    config = wrapper.get_config()\n-    config_copy = config.copy()\n-    self.assertEqual(config, config_copy)\n-\n-    wrapper_from_config = ExampleWrapper.from_config(config)\n-    new_config = wrapper.get_config()\n-    self.assertEqual(new_config, config_copy)\n-    self.assertEqual(config, config_copy)\n-\n-\n def _to_list(ls):\n   if isinstance(ls, list):\n     return ls\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Module implementing for RNN wrappers for TF v2.\"\"\"\n+\"\"\"Module implementing RNN wrappers.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n \n # Note that all the APIs under this module are exported as tf.nn.*. This is due\n # to the fact that those APIs were from tf.nn.rnn_cell_impl. They are ported\n@@ -21,13 +22,15 @@\n # existing Keras RNN API.\n \n \n-from keras.layers import recurrent\n-from keras.layers.legacy_rnn import rnn_cell_wrapper_impl\n+from keras.layers.rnn import base_cell_wrappers\n+from keras.layers.rnn import lstm_v1\n+from keras.layers.rnn.abstract_rnn_cell import AbstractRNNCell\n from keras.utils import tf_inspect\n+\n from tensorflow.python.util.tf_export import tf_export\n \n \n-class _RNNCellWrapperV2(recurrent.AbstractRNNCell):\n+class _RNNCellWrapper(AbstractRNNCell):\n   \"\"\"Base class for cells wrappers V2 compatibility.\n \n   This class along with `rnn_cell_impl._RNNCellWrapperV1` allows to define\n@@ -36,7 +39,7 @@ class _RNNCellWrapperV2(recurrent.AbstractRNNCell):\n   \"\"\"\n \n   def __init__(self, cell, *args, **kwargs):\n-    super(_RNNCellWrapperV2, self).__init__(*args, **kwargs)\n+    super(_RNNCellWrapper, self).__init__(*args, **kwargs)\n     self.cell = cell\n     cell_call_spec = tf_inspect.getfullargspec(cell.call)\n     self._expects_training_arg = (\"training\" in cell_call_spec.args) or (\n@@ -79,7 +82,7 @@ class _RNNCellWrapperV2(recurrent.AbstractRNNCell):\n             \"config\": self.cell.get_config()\n         },\n     }\n-    base_config = super(_RNNCellWrapperV2, self).get_config()\n+    base_config = super(_RNNCellWrapper, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -91,37 +94,35 @@ class _RNNCellWrapperV2(recurrent.AbstractRNNCell):\n \n \n @tf_export(\"nn.RNNCellDropoutWrapper\", v1=[])\n-class DropoutWrapper(rnn_cell_wrapper_impl.DropoutWrapperBase,\n-                     _RNNCellWrapperV2):\n+class DropoutWrapper(base_cell_wrappers.DropoutWrapperBase, _RNNCellWrapper):\n   \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n \n   def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n     super(DropoutWrapper, self).__init__(*args, **kwargs)\n-    if isinstance(self.cell, recurrent.LSTMCell):\n+    if isinstance(self.cell, lstm_v1.LSTMCell):\n       raise ValueError(\"keras LSTM cell does not work with DropoutWrapper. \"\n                        \"Please use LSTMCell(dropout=x, recurrent_dropout=y) \"\n                        \"instead.\")\n \n-  __init__.__doc__ = rnn_cell_wrapper_impl.DropoutWrapperBase.__init__.__doc__\n+  __init__.__doc__ = base_cell_wrappers.DropoutWrapperBase.__init__.__doc__\n \n \n @tf_export(\"nn.RNNCellResidualWrapper\", v1=[])\n-class ResidualWrapper(rnn_cell_wrapper_impl.ResidualWrapperBase,\n-                      _RNNCellWrapperV2):\n+class ResidualWrapper(base_cell_wrappers.ResidualWrapperBase,\n+                      _RNNCellWrapper):\n   \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n \n   def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n     super(ResidualWrapper, self).__init__(*args, **kwargs)\n \n-  __init__.__doc__ = rnn_cell_wrapper_impl.ResidualWrapperBase.__init__.__doc__\n+  __init__.__doc__ = base_cell_wrappers.ResidualWrapperBase.__init__.__doc__\n \n \n @tf_export(\"nn.RNNCellDeviceWrapper\", v1=[])\n-class DeviceWrapper(rnn_cell_wrapper_impl.DeviceWrapperBase,\n-                    _RNNCellWrapperV2):\n+class DeviceWrapper(base_cell_wrappers.DeviceWrapperBase, _RNNCellWrapper):\n   \"\"\"Operator that ensures an RNNCell runs on a particular device.\"\"\"\n \n   def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n     super(DeviceWrapper, self).__init__(*args, **kwargs)\n \n-  __init__.__doc__ = rnn_cell_wrapper_impl.DeviceWrapperBase.__init__.__doc__\n+  __init__.__doc__ = base_cell_wrappers.DeviceWrapperBase.__init__.__doc__\n\n@@ -12,30 +12,29 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for RNN cell wrapper v2 implementation.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for RNN cell wrappers.\"\"\"\n \n from absl.testing import parameterized\n-import numpy as np\n-from keras.testing_infra import test_combinations\n from keras import layers\n-from keras.layers import rnn_cell_wrapper_v2\n-from keras.layers.legacy_rnn import rnn_cell_impl\n+from keras.layers.rnn import cell_wrappers\n+from keras.layers.rnn import legacy_cells\n from keras.legacy_tf_layers import base as legacy_base_layer\n+from keras.testing_infra import test_combinations\n from keras.utils import generic_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testResidualWrapper(self):\n-    wrapper_type = rnn_cell_wrapper_v2.ResidualWrapper\n+    wrapper_type = cell_wrappers.ResidualWrapper\n     x = tf.convert_to_tensor(\n         np.array([[1., 1., 1.]]), dtype=\"float32\")\n     m = tf.convert_to_tensor(\n         np.array([[0.1, 0.1, 0.1]]), dtype=\"float32\")\n-    base_cell = rnn_cell_impl.GRUCell(\n+    base_cell = legacy_cells.GRUCell(\n         3, kernel_initializer=tf.compat.v1.constant_initializer(0.5),\n         bias_initializer=tf.compat.v1.constant_initializer(0.5))\n     g, m_new = base_cell(x, m)\n@@ -53,12 +52,12 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(res[2], res[3])\n \n   def testResidualWrapperWithSlice(self):\n-    wrapper_type = rnn_cell_wrapper_v2.ResidualWrapper\n+    wrapper_type = cell_wrappers.ResidualWrapper\n     x = tf.convert_to_tensor(\n         np.array([[1., 1., 1., 1., 1.]]), dtype=\"float32\")\n     m = tf.convert_to_tensor(\n         np.array([[0.1, 0.1, 0.1]]), dtype=\"float32\")\n-    base_cell = rnn_cell_impl.GRUCell(\n+    base_cell = legacy_cells.GRUCell(\n         3, kernel_initializer=tf.compat.v1.constant_initializer(0.5),\n         bias_initializer=tf.compat.v1.constant_initializer(0.5))\n     g, m_new = base_cell(x, m)\n@@ -78,10 +77,10 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(res_m_new, res_m_new_res)\n \n   def testDeviceWrapper(self):\n-    wrapper_type = rnn_cell_wrapper_v2.DeviceWrapper\n+    wrapper_type = cell_wrappers.DeviceWrapper\n     x = tf.zeros([1, 3])\n     m = tf.zeros([1, 3])\n-    cell = rnn_cell_impl.GRUCell(3)\n+    cell = legacy_cells.GRUCell(3)\n     wrapped_cell = wrapper_type(cell, \"/cpu:0\")\n     self.assertDictEqual({\"cell\": cell},\n                          wrapped_cell._trackable_children())\n@@ -91,25 +90,20 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIn(\"cpu:0\", outputs.device.lower())\n \n   @parameterized.parameters(\n-      [[rnn_cell_impl.DropoutWrapper, rnn_cell_wrapper_v2.DropoutWrapper],\n-       [rnn_cell_impl.ResidualWrapper, rnn_cell_wrapper_v2.ResidualWrapper]])\n-  def testWrapperKerasStyle(self, wrapper, wrapper_v2):\n+      [cell_wrappers.DropoutWrapper, cell_wrappers.ResidualWrapper])\n+  def testWrapperKerasStyle(self, wrapper):\n     \"\"\"Tests if wrapper cell is instantiated in keras style scope.\"\"\"\n-    wrapped_cell_v2 = wrapper_v2(rnn_cell_impl.BasicRNNCell(1))\n-    self.assertIsNone(getattr(wrapped_cell_v2, \"_keras_style\", None))\n-\n-    wrapped_cell = wrapper(rnn_cell_impl.BasicRNNCell(1))\n-    self.assertFalse(wrapped_cell._keras_style)\n+    wrapped_cell = wrapper(legacy_cells.BasicRNNCell(1))\n+    self.assertIsNone(getattr(wrapped_cell, \"_keras_style\", None))\n \n   @parameterized.parameters(\n-      [rnn_cell_wrapper_v2.DropoutWrapper, rnn_cell_wrapper_v2.ResidualWrapper])\n+      [cell_wrappers.DropoutWrapper, cell_wrappers.ResidualWrapper])\n   def testWrapperWeights(self, wrapper):\n     \"\"\"Tests that wrapper weights contain wrapped cells weights.\"\"\"\n     base_cell = layers.SimpleRNNCell(1, name=\"basic_rnn_cell\")\n     rnn_cell = wrapper(base_cell)\n     rnn_layer = layers.RNN(rnn_cell)\n-    inputs = tf.convert_to_tensor([[[1]]],\n-                                                    dtype=tf.float32)\n+    inputs = tf.convert_to_tensor([[[1]]], dtype=tf.float32)\n     rnn_layer(inputs)\n \n     wrapper_name = generic_utils.to_snake_case(wrapper.__name__)\n@@ -125,13 +119,13 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n                           expected_weights)\n \n   @parameterized.parameters(\n-      [rnn_cell_wrapper_v2.DropoutWrapper, rnn_cell_wrapper_v2.ResidualWrapper])\n+      [cell_wrappers.DropoutWrapper, cell_wrappers.ResidualWrapper])\n   def testWrapperV2Caller(self, wrapper):\n     \"\"\"Tests that wrapper V2 is using the LayerRNNCell's caller.\"\"\"\n \n     with legacy_base_layer.keras_style_scope():\n-      base_cell = rnn_cell_impl.MultiRNNCell(\n-          [rnn_cell_impl.BasicRNNCell(1) for _ in range(2)])\n+      base_cell = legacy_cells.MultiRNNCell(\n+          [legacy_cells.BasicRNNCell(1) for _ in range(2)])\n     rnn_cell = wrapper(base_cell)\n     inputs = tf.convert_to_tensor([[1]], dtype=tf.float32)\n     state = tf.convert_to_tensor([[1]], dtype=tf.float32)\n@@ -141,15 +135,15 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertTrue(all(\"_wrapper\" in v.name for v in weights))\n \n   @parameterized.parameters(\n-      [rnn_cell_wrapper_v2.DropoutWrapper, rnn_cell_wrapper_v2.ResidualWrapper])\n+      [cell_wrappers.DropoutWrapper, cell_wrappers.ResidualWrapper])\n   def testWrapperV2Build(self, wrapper):\n-    cell = rnn_cell_impl.LSTMCell(10)\n+    cell = legacy_cells.LSTMCell(10)\n     wrapper = wrapper(cell)\n     wrapper.build((1,))\n     self.assertTrue(cell.built)\n \n   def testDeviceWrapperSerialization(self):\n-    wrapper_cls = rnn_cell_wrapper_v2.DeviceWrapper\n+    wrapper_cls = cell_wrappers.DeviceWrapper\n     cell = layers.LSTMCell(10)\n     wrapper = wrapper_cls(cell, \"/cpu:0\")\n     config = wrapper.get_config()\n@@ -159,7 +153,7 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIsInstance(reconstructed_wrapper, wrapper_cls)\n \n   def testResidualWrapperSerialization(self):\n-    wrapper_cls = rnn_cell_wrapper_v2.ResidualWrapper\n+    wrapper_cls = cell_wrappers.ResidualWrapper\n     cell = layers.LSTMCell(10)\n     wrapper = wrapper_cls(cell)\n     config = wrapper.get_config()\n@@ -186,7 +180,7 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(reconstructed_wrapper._residual_fn(1, 2), 5)\n \n   def testDropoutWrapperSerialization(self):\n-    wrapper_cls = rnn_cell_wrapper_v2.DropoutWrapper\n+    wrapper_cls = cell_wrappers.DropoutWrapper\n     cell = layers.GRUCell(10)\n     wrapper = wrapper_cls(cell)\n     config = wrapper.get_config()\n@@ -212,7 +206,7 @@ class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertFalse(reconstructed_wrapper._dropout_state_filter(None))\n \n   def testDropoutWrapperWithKerasLSTMCell(self):\n-    wrapper_cls = rnn_cell_wrapper_v2.DropoutWrapper\n+    wrapper_cls = cell_wrappers.DropoutWrapper\n     cell = layers.LSTMCell(10)\n \n     with self.assertRaisesRegex(ValueError, \"does not work with \"):\n\n@@ -0,0 +1,184 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"1D Convolutional LSTM layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n+from keras.layers.rnn.base_conv_lstm import ConvLSTM\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ConvLSTM1D')\n+class ConvLSTM1D(ConvLSTM):\n+  \"\"\"1D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 4D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        4D tensor with shape: `(samples, time, channels, rows)` - If\n+          data_format='channels_last'\n+        4D tensor with shape: `(samples, time, rows, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 3D tensor with shape: `(samples, filters, new_rows)` if\n+        data_format='channels_first'\n+      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n+        `rows` values might have changed due to padding.\n+    - If `return_sequences`: 4D tensor with shape: `(samples, timesteps,\n+      filters, new_rows)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, filters)` if\n+        data_format='channels_last'.\n+    - Else, 3D tensor with shape: `(samples, filters, new_rows)` if\n+      data_format='channels_first'\n+      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM1D, self).__init__(\n+        rank=1,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n\n@@ -0,0 +1,186 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"2D Convolutional LSTM layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n+from keras.layers.rnn.base_conv_lstm import ConvLSTM\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ConvLSTM2D')\n+class ConvLSTM2D(ConvLSTM):\n+  \"\"\"2D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 5D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        5D tensor with shape: `(samples, time, channels, rows, cols)` - If\n+          data_format='channels_last'\n+        5D tensor with shape: `(samples, time, rows, cols, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n+        data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, filters)` if\n+        data_format='channels_last'. `rows` and `cols` values might have changed\n+        due to padding.\n+    - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,\n+      filters, new_rows, new_cols)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, new_cols, filters)` if\n+        data_format='channels_last'.\n+    - Else, 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n+      data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, filters)` if\n+        data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM2D, self).__init__(\n+        rank=2,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n\n@@ -0,0 +1,186 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"3D Convolutional LSTM layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n+from keras.layers.rnn.base_conv_lstm import ConvLSTM\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ConvLSTM3D')\n+class ConvLSTM3D(ConvLSTM):\n+  \"\"\"3D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 6D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        6D tensor with shape: `(samples, time, channels, rows, cols, depth)` -\n+          If data_format='channels_last'\n+        5D tensor with shape: `(samples, time, rows, cols, depth, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n+        new_depth)` if data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n+        data_format='channels_last'. `rows`, `cols`, and `depth` values might\n+        have changed due to padding.\n+    - If `return_sequences`: 6D tensor with shape: `(samples, timesteps,\n+      filters, new_rows, new_cols, new_depth)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, new_cols, new_depth, filters)`\n+        if data_format='channels_last'.\n+    - Else, 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n+      new_depth)` if data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n+        data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1, 1),\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM3D, self).__init__(\n+        rank=3,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n\n@@ -14,14 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for convolutional recurrent layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.run_all_keras_modes\n\n@@ -0,0 +1,207 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Fast GRU layer backed by cuDNN.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import collections\n+\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn.base_cudnn_rnn import _CuDNNRNN\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(v1=['keras.layers.CuDNNGRU'])\n+class CuDNNGRU(_CuDNNRNN):\n+  \"\"\"Fast GRU implementation backed by cuDNN.\n+\n+  More information about cuDNN can be found on the [NVIDIA\n+  developer website](https://developer.nvidia.com/cudnn).\n+  Can only be run on GPU.\n+\n+  Args:\n+      units: Positive integer, dimensionality of the output space.\n+      kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+        the linear transformation of the inputs.\n+      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+        matrix, used for the linear transformation of the recurrent state.\n+      bias_initializer: Initializer for the bias vector.\n+      kernel_regularizer: Regularizer function applied to the `kernel` weights\n+        matrix.\n+      recurrent_regularizer: Regularizer function applied to the\n+        `recurrent_kernel` weights matrix.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+      activity_regularizer: Regularizer function applied to the output of the\n+        layer (its \"activation\").\n+      kernel_constraint: Constraint function applied to the `kernel` weights\n+        matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n+      bias_constraint: Constraint function applied to the bias vector.\n+      return_sequences: Boolean. Whether to return the last output in the output\n+        sequence, or the full sequence.\n+      return_state: Boolean. Whether to return the last state in addition to the\n+        output.\n+      go_backwards: Boolean (default False). If True, process the input sequence\n+        backwards and return the reversed sequence.\n+      stateful: Boolean (default False). If True, the last state for each sample\n+        at index i in a batch will be used as initial state for the sample of\n+        index i in the following batch.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               **kwargs):\n+    self.units = units\n+    cell_spec = collections.namedtuple('cell', 'state_size')\n+    self._cell = cell_spec(state_size=self.units)\n+    super(CuDNNGRU, self).__init__(\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        **kwargs)\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+  @property\n+  def cell(self):\n+    return self._cell\n+\n+  def build(self, input_shape):\n+    super(CuDNNGRU, self).build(input_shape)\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+    input_dim = int(input_shape[-1])\n+\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 3),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint)\n+\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 3),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint)\n+\n+    self.bias = self.add_weight(\n+        shape=(self.units * 6,),\n+        name='bias',\n+        initializer=self.bias_initializer,\n+        regularizer=self.bias_regularizer,\n+        constraint=self.bias_constraint)\n+\n+    self.built = True\n+\n+  def _process_batch(self, inputs, initial_state):\n+    if not self.time_major:\n+      inputs = tf.transpose(inputs, perm=(1, 0, 2))\n+    input_h = initial_state[0]\n+    input_h = tf.expand_dims(input_h, axis=0)\n+\n+    params = gru_lstm_utils.canonical_to_params(\n+        weights=[\n+            self.kernel[:, self.units:self.units * 2],\n+            self.kernel[:, :self.units],\n+            self.kernel[:, self.units * 2:],\n+            self.recurrent_kernel[:, self.units:self.units * 2],\n+            self.recurrent_kernel[:, :self.units],\n+            self.recurrent_kernel[:, self.units * 2:],\n+        ],\n+        biases=[\n+            self.bias[self.units:self.units * 2],\n+            self.bias[:self.units],\n+            self.bias[self.units * 2:self.units * 3],\n+            self.bias[self.units * 4:self.units * 5],\n+            self.bias[self.units * 3:self.units * 4],\n+            self.bias[self.units * 5:],\n+        ],\n+        shape=self._vector_shape)\n+\n+    args = {\n+        'input': inputs,\n+        'input_h': input_h,\n+        'input_c': 0,\n+        'params': params,\n+        'is_training': True,\n+        'rnn_mode': 'gru',\n+    }\n+\n+    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV2(**args)\n+\n+    if self.stateful or self.return_state:\n+      h = h[0]\n+    if self.return_sequences:\n+      if self.time_major:\n+        output = outputs\n+      else:\n+        output = tf.transpose(outputs, perm=(1, 0, 2))\n+    else:\n+      output = outputs[-1]\n+    return output, [h]\n+\n+  def get_config(self):\n+    config = {\n+        'units': self.units,\n+        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer': initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint': constraints.serialize(self.bias_constraint)\n+    }\n+    base_config = super(CuDNNGRU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,230 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Fast LSTM layer backed by cuDNN.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import collections\n+\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn.base_cudnn_rnn import _CuDNNRNN\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(v1=['keras.layers.CuDNNLSTM'])\n+class CuDNNLSTM(_CuDNNRNN):\n+  \"\"\"Fast LSTM implementation backed by cuDNN.\n+\n+  More information about cuDNN can be found on the [NVIDIA\n+  developer website](https://developer.nvidia.com/cudnn).\n+  Can only be run on GPU.\n+\n+  Args:\n+      units: Positive integer, dimensionality of the output space.\n+      kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+        the linear transformation of the inputs.\n+      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n+        at initialization. Setting it to true will also force\n+        `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n+        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+        matrix, used for the linear transformation of the recurrent state.\n+      bias_initializer: Initializer for the bias vector.\n+      kernel_regularizer: Regularizer function applied to the `kernel` weights\n+        matrix.\n+      recurrent_regularizer: Regularizer function applied to the\n+        `recurrent_kernel` weights matrix.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+      activity_regularizer: Regularizer function applied to the output of the\n+        layer (its \"activation\").\n+      kernel_constraint: Constraint function applied to the `kernel` weights\n+        matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n+      bias_constraint: Constraint function applied to the bias vector.\n+      return_sequences: Boolean. Whether to return the last output. in the\n+        output sequence, or the full sequence.\n+      return_state: Boolean. Whether to return the last state in addition to the\n+        output.\n+      go_backwards: Boolean (default False). If True, process the input sequence\n+        backwards and return the reversed sequence.\n+      stateful: Boolean (default False). If True, the last state for each sample\n+        at index i in a batch will be used as initial state for the sample of\n+        index i in the following batch.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               **kwargs):\n+    self.units = units\n+    cell_spec = collections.namedtuple('cell', 'state_size')\n+    self._cell = cell_spec(state_size=(self.units, self.units))\n+    super(CuDNNLSTM, self).__init__(\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        **kwargs)\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.unit_forget_bias = unit_forget_bias\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+  @property\n+  def cell(self):\n+    return self._cell\n+\n+  def build(self, input_shape):\n+    super(CuDNNLSTM, self).build(input_shape)\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+    input_dim = int(input_shape[-1])\n+\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 4),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint)\n+\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 4),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint)\n+\n+    if self.unit_forget_bias:\n+\n+      def bias_initializer(_, *args, **kwargs):\n+        return tf.concat([\n+            self.bias_initializer((self.units * 5,), *args, **kwargs),\n+            tf.compat.v1.ones_initializer()((self.units,), *args, **kwargs),\n+            self.bias_initializer((self.units * 2,), *args, **kwargs),\n+        ], axis=0)\n+    else:\n+      bias_initializer = self.bias_initializer\n+    self.bias = self.add_weight(\n+        shape=(self.units * 8,),\n+        name='bias',\n+        initializer=bias_initializer,\n+        regularizer=self.bias_regularizer,\n+        constraint=self.bias_constraint)\n+\n+    self.built = True\n+\n+  def _process_batch(self, inputs, initial_state):\n+    if not self.time_major:\n+      inputs = tf.transpose(inputs, perm=(1, 0, 2))\n+    input_h = initial_state[0]\n+    input_c = initial_state[1]\n+    input_h = tf.expand_dims(input_h, axis=0)\n+    input_c = tf.expand_dims(input_c, axis=0)\n+\n+    params = gru_lstm_utils.canonical_to_params(\n+        weights=[\n+            self.kernel[:, :self.units],\n+            self.kernel[:, self.units:self.units * 2],\n+            self.kernel[:, self.units * 2:self.units * 3],\n+            self.kernel[:, self.units * 3:],\n+            self.recurrent_kernel[:, :self.units],\n+            self.recurrent_kernel[:, self.units:self.units * 2],\n+            self.recurrent_kernel[:, self.units * 2:self.units * 3],\n+            self.recurrent_kernel[:, self.units * 3:],\n+        ],\n+        biases=[\n+            self.bias[:self.units],\n+            self.bias[self.units:self.units * 2],\n+            self.bias[self.units * 2:self.units * 3],\n+            self.bias[self.units * 3:self.units * 4],\n+            self.bias[self.units * 4:self.units * 5],\n+            self.bias[self.units * 5:self.units * 6],\n+            self.bias[self.units * 6:self.units * 7],\n+            self.bias[self.units * 7:],\n+        ],\n+        shape=self._vector_shape)\n+\n+    args = {\n+        'input': inputs,\n+        'input_h': input_h,\n+        'input_c': input_c,\n+        'params': params,\n+        'is_training': True,\n+    }\n+\n+    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV2(**args)\n+\n+    if self.stateful or self.return_state:\n+      h = h[0]\n+      c = c[0]\n+    if self.return_sequences:\n+      if self.time_major:\n+        output = outputs\n+      else:\n+        output = tf.transpose(outputs, perm=(1, 0, 2))\n+    else:\n+      output = outputs[-1]\n+    return output, [h, c]\n+\n+  def get_config(self):\n+    config = {\n+        'units': self.units,\n+        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer': initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias': self.unit_forget_bias,\n+        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint': constraints.serialize(self.bias_constraint)\n+    }\n+    base_config = super(CuDNNLSTM, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n\n@@ -0,0 +1,173 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Mixin holding dropout fields for RNN cells.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras import backend\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.tools.docs import doc_controls\n+\n+\n+@doc_controls.do_not_generate_docs\n+class DropoutRNNCellMixin:\n+  \"\"\"Object that hold dropout related fields for RNN Cell.\n+\n+  This class is not a standalone RNN cell. It suppose to be used with a RNN cell\n+  by multiple inheritance. Any cell that mix with class should have following\n+  fields:\n+    dropout: a float number within range [0, 1). The ratio that the input\n+      tensor need to dropout.\n+    recurrent_dropout: a float number within range [0, 1). The ratio that the\n+      recurrent state weights need to dropout.\n+    _random_generator: A backend.RandomGenerator instance, which will be used\n+      to produce outputs based on the inputs and dropout rate.\n+  This object will create and cache created dropout masks, and reuse them for\n+  the incoming data, so that the same mask is used for every batch input.\n+  \"\"\"\n+\n+  def __init__(self, *args, **kwargs):\n+    self._create_non_trackable_mask_cache()\n+    super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def _create_non_trackable_mask_cache(self):\n+    \"\"\"Create the cache for dropout and recurrent dropout mask.\n+\n+    Note that the following two masks will be used in \"graph function\" mode,\n+    e.g. these masks are symbolic tensors. In eager mode, the `eager_*_mask`\n+    tensors will be generated differently than in the \"graph function\" case,\n+    and they will be cached.\n+\n+    Also note that in graph mode, we still cache those masks only because the\n+    RNN could be created with `unroll=True`. In that case, the `cell.call()`\n+    function will be invoked multiple times, and we want to ensure same mask\n+    is used every time.\n+\n+    Also the caches are created without tracking. Since they are not picklable\n+    by python when deepcopy, we don't want `layer._obj_reference_counts_dict`\n+    to track it by default.\n+    \"\"\"\n+    self._dropout_mask_cache = backend.ContextValueCache(\n+        self._create_dropout_mask)\n+    self._recurrent_dropout_mask_cache = backend.ContextValueCache(\n+        self._create_recurrent_dropout_mask)\n+\n+  def reset_dropout_mask(self):\n+    \"\"\"Reset the cached dropout masks if any.\n+\n+    This is important for the RNN layer to invoke this in it `call()` method so\n+    that the cached mask is cleared before calling the `cell.call()`. The mask\n+    should be cached across the timestep within the same batch, but shouldn't\n+    be cached between batches. Otherwise it will introduce unreasonable bias\n+    against certain index of data within the batch.\n+    \"\"\"\n+    self._dropout_mask_cache.clear()\n+\n+  def reset_recurrent_dropout_mask(self):\n+    \"\"\"Reset the cached recurrent dropout masks if any.\n+\n+    This is important for the RNN layer to invoke this in it call() method so\n+    that the cached mask is cleared before calling the cell.call(). The mask\n+    should be cached across the timestep within the same batch, but shouldn't\n+    be cached between batches. Otherwise it will introduce unreasonable bias\n+    against certain index of data within the batch.\n+    \"\"\"\n+    self._recurrent_dropout_mask_cache.clear()\n+\n+  def _create_dropout_mask(self, inputs, training, count=1):\n+    return _generate_dropout_mask(\n+        self._random_generator,\n+        tf.ones_like(inputs),\n+        self.dropout,\n+        training=training,\n+        count=count)\n+\n+  def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n+    return _generate_dropout_mask(\n+        self._random_generator,\n+        tf.ones_like(inputs),\n+        self.recurrent_dropout,\n+        training=training,\n+        count=count)\n+\n+  def get_dropout_mask_for_cell(self, inputs, training, count=1):\n+    \"\"\"Get the dropout mask for RNN cell's input.\n+\n+    It will create mask based on context if there isn't any existing cached\n+    mask. If a new mask is generated, it will update the cache in the cell.\n+\n+    Args:\n+      inputs: The input tensor whose shape will be used to generate dropout\n+        mask.\n+      training: Boolean tensor, whether its in training mode, dropout will be\n+        ignored in non-training mode.\n+      count: Int, how many dropout mask will be generated. It is useful for cell\n+        that has internal weights fused together.\n+    Returns:\n+      List of mask tensor, generated or cached mask based on context.\n+    \"\"\"\n+    if self.dropout == 0:\n+      return None\n+    init_kwargs = dict(inputs=inputs, training=training, count=count)\n+    return self._dropout_mask_cache.setdefault(kwargs=init_kwargs)\n+\n+  def get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1):\n+    \"\"\"Get the recurrent dropout mask for RNN cell.\n+\n+    It will create mask based on context if there isn't any existing cached\n+    mask. If a new mask is generated, it will update the cache in the cell.\n+\n+    Args:\n+      inputs: The input tensor whose shape will be used to generate dropout\n+        mask.\n+      training: Boolean tensor, whether its in training mode, dropout will be\n+        ignored in non-training mode.\n+      count: Int, how many dropout mask will be generated. It is useful for cell\n+        that has internal weights fused together.\n+    Returns:\n+      List of mask tensor, generated or cached mask based on context.\n+    \"\"\"\n+    if self.recurrent_dropout == 0:\n+      return None\n+    init_kwargs = dict(inputs=inputs, training=training, count=count)\n+    return self._recurrent_dropout_mask_cache.setdefault(kwargs=init_kwargs)\n+\n+  def __getstate__(self):\n+    # Used for deepcopy. The caching can't be pickled by python, since it will\n+    # contain tensor and graph.\n+    state = super(DropoutRNNCellMixin, self).__getstate__()\n+    state.pop('_dropout_mask_cache', None)\n+    state.pop('_recurrent_dropout_mask_cache', None)\n+    return state\n+\n+  def __setstate__(self, state):\n+    state['_dropout_mask_cache'] = backend.ContextValueCache(\n+        self._create_dropout_mask)\n+    state['_recurrent_dropout_mask_cache'] = backend.ContextValueCache(\n+        self._create_recurrent_dropout_mask)\n+    super(DropoutRNNCellMixin, self).__setstate__(state)\n+\n+\n+def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n+  def dropped_inputs():\n+    return generator.dropout(ones, rate)\n+\n+  if count > 1:\n+    return [\n+        backend.in_train_phase(dropped_inputs, ones, training=training)\n+        for _ in range(count)\n+    ]\n+  return backend.in_train_phase(dropped_inputs, ones, training=training)\n\n@@ -0,0 +1,789 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Gated Recurrent Unit layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import uuid\n+\n+from keras import activations\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn import gru_v1\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GRUCell', v1=[])\n+class GRUCell(gru_v1.GRUCell):\n+  \"\"\"Cell class for the GRU layer.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  This class processes one step within the whole time sequence input, whereas\n+  `tf.keras.layer.GRU` processes the whole sequence.\n+\n+  For example:\n+\n+  >>> inputs = tf.random.normal([32, 10, 8])\n+  >>> rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(4))\n+  >>> output = rnn(inputs)\n+  >>> print(output.shape)\n+  (32, 4)\n+  >>> rnn = tf.keras.layers.RNN(\n+  ...    tf.keras.layers.GRUCell(4),\n+  ...    return_sequences=True,\n+  ...    return_state=True)\n+  >>> whole_sequence_output, final_state = rnn(inputs)\n+  >>> print(whole_sequence_output.shape)\n+  (32, 10, 4)\n+  >>> print(final_state.shape)\n+  (32, 4)\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use. Default: hyperbolic tangent\n+      (`tanh`). If you pass None, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n+      applied (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs. Default:\n+      `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix, used for the linear transformation of the recurrent state.\n+      Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix. Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the\n+      linear transformation of the inputs. Default: 0.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state. Default: 0.\n+    reset_after: GRU convention (whether to apply reset gate after or\n+      before matrix multiplication). False = \"before\",\n+      True = \"after\" (default and cuDNN compatible).\n+\n+  Call arguments:\n+    inputs: A 2D tensor, with shape of `[batch, feature]`.\n+    states: A 2D tensor with shape of `[batch, units]`, which is the state from\n+      the previous time step. For timestep 0, the initial state provided by user\n+      will be feed to cell.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               reset_after=True,\n+               **kwargs):\n+    super(GRUCell, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 2),\n+        reset_after=reset_after,\n+        **kwargs)\n+\n+\n+@keras_export('keras.layers.GRU', v1=[])\n+class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n+  \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  Based on available runtime hardware and constraints, this layer\n+  will choose different implementations (cuDNN-based or pure-TensorFlow)\n+  to maximize the performance. If a GPU is available and all\n+  the arguments to the layer meet the requirement of the cuDNN kernel\n+  (see below for details), the layer will use a fast cuDNN implementation.\n+\n+  The requirements to use the cuDNN implementation are:\n+\n+  1. `activation` == `tanh`\n+  2. `recurrent_activation` == `sigmoid`\n+  3. `recurrent_dropout` == 0\n+  4. `unroll` is `False`\n+  5. `use_bias` is `True`\n+  6. `reset_after` is `True`\n+  7. Inputs, if use masking, are strictly right-padded.\n+  8. Eager execution is enabled in the outermost context.\n+\n+  There are two variants of the GRU implementation. The default one is based on\n+  [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n+  state before matrix multiplication. The other one is based on\n+  [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n+\n+  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n+  inference on CPU. Thus it has separate biases for `kernel` and\n+  `recurrent_kernel`. To use this variant, set `reset_after=True` and\n+  `recurrent_activation='sigmoid'`.\n+\n+  For example:\n+\n+  >>> inputs = tf.random.normal([32, 10, 8])\n+  >>> gru = tf.keras.layers.GRU(4)\n+  >>> output = gru(inputs)\n+  >>> print(output.shape)\n+  (32, 4)\n+  >>> gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)\n+  >>> whole_sequence_output, final_state = gru(inputs)\n+  >>> print(whole_sequence_output.shape)\n+  (32, 10, 4)\n+  >>> print(final_state.shape)\n+  (32, 4)\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+      Default: sigmoid (`sigmoid`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs. Default:\n+      `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+       weights matrix, used for the linear transformation of the recurrent\n+       state. Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its \"activation\"). Default: `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix. Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs. Default: 0.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state. Default: 0.\n+    return_sequences: Boolean. Whether to return the last output\n+      in the output sequence, or the full sequence. Default: `False`.\n+    return_state: Boolean. Whether to return the last state in addition to the\n+      output. Default: `False`.\n+    go_backwards: Boolean (default `False`).\n+      If True, process the input sequence backwards and return the\n+      reversed sequence.\n+    stateful: Boolean (default False). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    unroll: Boolean (default False).\n+      If True, the network will be unrolled,\n+      else a symbolic loop will be used.\n+      Unrolling can speed-up a RNN,\n+      although it tends to be more memory-intensive.\n+      Unrolling is only suitable for short sequences.\n+    time_major: The shape format of the `inputs` and `outputs` tensors.\n+      If True, the inputs and outputs will be in shape\n+      `[timesteps, batch, feature]`, whereas in the False case, it will be\n+      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n+      efficient because it avoids transposes at the beginning and end of the\n+      RNN calculation. However, most TensorFlow data is batch-major, so by\n+      default this function accepts input and emits output in batch-major\n+      form.\n+    reset_after: GRU convention (whether to apply reset gate after or\n+      before matrix multiplication). False = \"before\",\n+      True = \"after\" (default and cuDNN compatible).\n+\n+  Call arguments:\n+    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n+    mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n+      a given timestep should be masked  (optional, defaults to `None`).\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the\n+      corresponding timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or\n+      `recurrent_dropout` is used  (optional, defaults to `None`).\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell  (optional, defaults to `None` which causes creation\n+      of zero-filled initial state tensors).\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               time_major=False,\n+               reset_after=True,\n+               **kwargs):\n+    # return_runtime is a flag for testing, which shows the real backend\n+    # implementation chosen by grappler in graph mode.\n+    self._return_runtime = kwargs.pop('return_runtime', False)\n+\n+    super(GRU, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 2),\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        unroll=unroll,\n+        time_major=time_major,\n+        reset_after=reset_after,\n+        **kwargs)\n+    # GPU kernel uses following setting by default and not configurable.\n+    self._could_use_gpu_kernel = (\n+        self.activation in (activations.tanh, tf.tanh) and\n+        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and\n+        recurrent_dropout == 0 and not unroll and use_bias and\n+        reset_after and tf.compat.v1.executing_eagerly_outside_functions())\n+    if tf.config.list_logical_devices('GPU'):\n+      # Only show the message when there is GPU available, user will not care\n+      # about the cuDNN if there isn't any GPU.\n+      if self._could_use_gpu_kernel:\n+        logging.debug(gru_lstm_utils.CUDNN_AVAILABLE_MSG % self.name)\n+      else:\n+        logging.warning(gru_lstm_utils.CUDNN_NOT_AVAILABLE_MSG % self.name)\n+\n+    if gru_lstm_utils.use_new_gru_lstm_impl():\n+      self._defun_wrapper = gru_lstm_utils.DefunWrapper(\n+          time_major, go_backwards, 'gru')\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    # The input should be dense, padded with zeros. If a ragged input is fed\n+    # into the layer, it is padded and the row lengths are used for masking.\n+    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n+    is_ragged_input = (row_lengths is not None)\n+    self._validate_args_if_ragged(is_ragged_input, mask)\n+\n+    # GRU does not support constants. Ignore it during process.\n+    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n+\n+    if isinstance(mask, list):\n+      mask = mask[0]\n+\n+    input_shape = backend.int_shape(inputs)\n+    timesteps = input_shape[0] if self.time_major else input_shape[1]\n+\n+    if not self._could_use_gpu_kernel:\n+      kwargs = {'training': training}\n+      self._maybe_reset_cell_dropout_mask(self.cell)\n+\n+      def step(cell_inputs, cell_states):\n+        return self.cell(cell_inputs, cell_states, **kwargs)\n+\n+      last_output, outputs, states = backend.rnn(\n+          step,\n+          inputs,\n+          initial_state,\n+          constants=None,\n+          go_backwards=self.go_backwards,\n+          mask=mask,\n+          unroll=self.unroll,\n+          input_length=row_lengths if row_lengths is not None else timesteps,\n+          time_major=self.time_major,\n+          zero_output_for_mask=self.zero_output_for_mask)\n+      # This is a dummy tensor for testing purpose.\n+      runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n+    else:\n+      last_output, outputs, runtime, states = self._defun_gru_call(\n+          inputs, initial_state, training, mask, row_lengths)\n+\n+    if self.stateful:\n+      updates = [tf.compat.v1.assign(self.states[0],\n+                                     tf.cast(states[0], self.states[0].dtype))]\n+      self.add_update(updates)\n+\n+    if self.return_sequences:\n+      output = backend.maybe_convert_to_ragged(\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n+    else:\n+      output = last_output\n+\n+    if self.return_state:\n+      return [output] + list(states)\n+    elif self._return_runtime:\n+      return output, runtime\n+    else:\n+      return output\n+\n+  def _defun_gru_call(self, inputs, initial_state, training, mask,\n+                      sequence_lengths):\n+    # Use the new defun approach for backend implementation swap.\n+    # Note that different implementations need to have same function\n+    # signature, eg, the tensor parameters need to have same shape and dtypes.\n+\n+    self.reset_dropout_mask()\n+    dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n+    if dropout_mask is not None:\n+      inputs = inputs * dropout_mask[0]\n+\n+    if gru_lstm_utils.use_new_gru_lstm_impl():\n+      gru_kwargs = {\n+          'inputs':\n+              inputs,\n+          'init_h':\n+              gru_lstm_utils.read_variable_value(initial_state[0]),\n+          'kernel':\n+              gru_lstm_utils.read_variable_value(self.cell.kernel),\n+          'recurrent_kernel':\n+              gru_lstm_utils.read_variable_value(self.cell.recurrent_kernel),\n+          'bias':\n+              gru_lstm_utils.read_variable_value(self.cell.bias),\n+          'mask':\n+              mask,\n+          'time_major':\n+              self.time_major,\n+          'go_backwards':\n+              self.go_backwards,\n+          'sequence_lengths':\n+              sequence_lengths,\n+          'zero_output_for_mask':\n+              self.zero_output_for_mask\n+      }\n+      (last_output, outputs, new_h,\n+       runtime) = self._defun_wrapper.defun_layer(**gru_kwargs)\n+    else:\n+      gpu_gru_kwargs = {\n+          'inputs':\n+              inputs,\n+          'init_h':\n+              gru_lstm_utils.read_variable_value(initial_state[0]),\n+          'kernel':\n+              gru_lstm_utils.read_variable_value(self.cell.kernel),\n+          'recurrent_kernel':\n+              gru_lstm_utils.read_variable_value(self.cell.recurrent_kernel),\n+          'bias':\n+              gru_lstm_utils.read_variable_value(self.cell.bias),\n+          'mask':\n+              mask,\n+          'time_major':\n+              self.time_major,\n+          'go_backwards':\n+              self.go_backwards,\n+          'sequence_lengths':\n+              sequence_lengths\n+      }\n+      normal_gru_kwargs = gpu_gru_kwargs.copy()\n+      normal_gru_kwargs.update({\n+          'zero_output_for_mask': self.zero_output_for_mask,\n+      })\n+\n+      if tf.executing_eagerly():\n+        device_type = gru_lstm_utils.get_context_device_type()\n+        can_use_gpu = (\n+            # Either user specified GPU or unspecified but GPU is available.\n+            (device_type == gru_lstm_utils.GPU_DEVICE_NAME or\n+             (device_type is None and tf.config.list_logical_devices('GPU')))\n+            and\n+            (mask is None or\n+             gru_lstm_utils.is_cudnn_supported_inputs(mask, self.time_major)))\n+        # Under eager context, check the device placement and prefer the\n+        if can_use_gpu:\n+          last_output, outputs, new_h, runtime = gpu_gru(**gpu_gru_kwargs)\n+        else:\n+          last_output, outputs, new_h, runtime = standard_gru(\n+              **normal_gru_kwargs)\n+      else:\n+        last_output, outputs, new_h, runtime = gru_with_backend_selection(\n+            **normal_gru_kwargs)\n+\n+    states = [new_h]\n+    return last_output, outputs, runtime, states\n+\n+\n+def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n+                 time_major, go_backwards, sequence_lengths,\n+                 zero_output_for_mask):\n+  \"\"\"GRU with standard kernel implementation.\n+\n+  This implementation can be run on all types of hardware.\n+\n+  This implementation lifts out all the layer weights and make them function\n+  parameters. It has same number of tensor input params as the cuDNN\n+  counterpart. The RNN step logic has been simplified, eg dropout and mask is\n+  removed since cuDNN implementation does not support that.\n+\n+  Args:\n+    inputs: Input tensor of GRU layer.\n+    init_h: Initial state tensor for the cell output.\n+    kernel: Weights for cell kernel.\n+    recurrent_kernel: Weights for cell recurrent kernel.\n+    bias: Weights for cell kernel bias and recurrent bias. The bias contains the\n+      combined input_bias and recurrent_bias.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False` entry\n+      indicates that the corresponding timestep should be ignored.\n+    time_major: Boolean, whether the inputs are in the format of\n+      [time, batch, feature] or [batch, time, feature].\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    sequence_lengths: The lengths of all sequences coming from a variable length\n+      input, such as ragged tensors. If the input has a fixed timestep size,\n+      this should be None.\n+    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+\n+  Returns:\n+    last_output: output tensor for the last timestep, which has shape\n+      [batch, units].\n+    outputs: output tensor for all timesteps, which has shape\n+      [batch, time, units].\n+    state_0: the cell output, which has same shape as init_h.\n+    runtime: constant string tensor which indicate real runtime hardware. This\n+      value is for testing purpose and should be used by user.\n+  \"\"\"\n+  input_shape = backend.int_shape(inputs)\n+  timesteps = input_shape[0] if time_major else input_shape[1]\n+\n+  input_bias, recurrent_bias = tf.unstack(bias)\n+\n+  def step(cell_inputs, cell_states):\n+    \"\"\"Step function that will be used by Keras RNN backend.\"\"\"\n+    h_tm1 = cell_states[0]\n+\n+    # inputs projected by all gate matrices at once\n+    matrix_x = backend.dot(cell_inputs, kernel)\n+    matrix_x = backend.bias_add(matrix_x, input_bias)\n+\n+    x_z, x_r, x_h = tf.split(matrix_x, 3, axis=1)\n+\n+    # hidden state projected by all gate matrices at once\n+    matrix_inner = backend.dot(h_tm1, recurrent_kernel)\n+    matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n+\n+    recurrent_z, recurrent_r, recurrent_h = tf.split(matrix_inner, 3, axis=1)\n+    z = tf.sigmoid(x_z + recurrent_z)\n+    r = tf.sigmoid(x_r + recurrent_r)\n+    hh = tf.tanh(x_h + r * recurrent_h)\n+\n+    # previous and candidate state mixed by update gate\n+    h = z * h_tm1 + (1 - z) * hh\n+    return h, [h]\n+\n+  last_output, outputs, new_states = backend.rnn(\n+      step,\n+      inputs, [init_h],\n+      constants=None,\n+      unroll=False,\n+      time_major=time_major,\n+      mask=mask,\n+      go_backwards=go_backwards,\n+      input_length=sequence_lengths\n+      if sequence_lengths is not None else timesteps,\n+      zero_output_for_mask=zero_output_for_mask)\n+  return last_output, outputs, new_states[0], gru_lstm_utils.runtime(\n+      gru_lstm_utils.RUNTIME_CPU)\n+\n+\n+def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n+            go_backwards, sequence_lengths):\n+  \"\"\"GRU with cuDNN implementation which is only available for GPU.\"\"\"\n+  if mask is not None:\n+    sequence_lengths = gru_lstm_utils.calculate_sequence_by_mask(\n+        mask, time_major)\n+\n+  if not time_major and sequence_lengths is None:\n+    inputs = tf.transpose(inputs, perm=(1, 0, 2))\n+    seq_axis, batch_axis = (0, 1)\n+  else:\n+    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n+  # For init_h, cuDNN expects one more dim of num_layers before or after batch\n+  # dim for time major or batch major inputs respectively\n+  init_h = tf.expand_dims(init_h, axis=seq_axis)\n+\n+  weights = tf.split(kernel, 3, axis=1)\n+  weights += tf.split(recurrent_kernel, 3, axis=1)\n+  # Note that the bias was initialized as shape (2, 3 * units), flat it into\n+  # (6 * units)\n+  bias = tf.split(backend.flatten(bias), 6)\n+\n+  if tf.sysconfig.get_build_info()['is_cuda_build']:\n+    # Note that the gate order for cuDNN is different from the canonical format.\n+    # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need\n+    # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.\n+    # z is update gate weights.\n+    # r is reset gate weights.\n+    # h is output gate weights.\n+    weights[0], weights[1] = weights[1], weights[0]\n+    weights[3], weights[4] = weights[4], weights[3]\n+    bias[0], bias[1] = bias[1], bias[0]\n+    bias[3], bias[4] = bias[4], bias[3]\n+\n+  params = gru_lstm_utils.canonical_to_params(\n+      weights=weights,\n+      biases=bias,\n+      shape=tf.constant([-1]),\n+      transpose_weights=True)\n+\n+  if sequence_lengths is not None:\n+    if go_backwards:\n+      # Three reversals are required. E.g.,\n+      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n+      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]\n+      # output_from_cudnn = [6, 5, 4, 0, 0]\n+      # expected_output = [0, 0, 6, 5 ,4]\n+      inputs = tf.reverse_sequence(\n+          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n+    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV3(\n+        input=inputs,\n+        input_h=init_h,\n+        input_c=0,\n+        params=params,\n+        is_training=True,\n+        rnn_mode='gru',\n+        sequence_lengths=sequence_lengths,\n+        time_major=time_major)\n+    if go_backwards:\n+      outputs = tf.reverse_sequence(\n+          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n+      outputs = tf.reverse(outputs, axis=[seq_axis])\n+  else:\n+    if go_backwards:\n+      # Reverse axis 0 since the input is already convert to time major.\n+      inputs = tf.reverse(inputs, axis=[0])\n+    outputs, h, _, _ = tf.raw_ops.CudnnRNN(\n+        input=inputs, input_h=init_h, input_c=0, params=params,\n+        is_training=True, rnn_mode='gru')\n+\n+  last_output = outputs[-1]\n+  if not time_major and sequence_lengths is None:\n+    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n+  h = tf.squeeze(h, axis=seq_axis)\n+\n+  # In the case of variable length input, the cudnn kernel will fill zeros for\n+  # the output, whereas the default keras behavior is to bring over the previous\n+  # output for t-1, so that in the return_sequence=False case, user can quickly\n+  # get the final effect output instead just 0s at the last timestep.\n+  # In order to mimic the default keras behavior, we copy the final h state as\n+  # the last_output, since it is numerically same as the output.\n+  if sequence_lengths is not None:\n+    last_output = h\n+\n+  return last_output, outputs, h, gru_lstm_utils.runtime(\n+      gru_lstm_utils.RUNTIME_GPU)\n+\n+\n+def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n+                               mask, time_major, go_backwards, sequence_lengths,\n+                               zero_output_for_mask):\n+  \"\"\"Call the GRU with optimized backend kernel selection.\n+\n+  Under the hood, this function will create two TF function, one with the most\n+  generic kernel and can run on all device condition, and the second one with\n+  cuDNN specific kernel, which can only run on GPU.\n+\n+  The first function will be called with normal_lstm_params, while the second\n+  function is not called, but only registered in the graph. The Grappler will\n+  do the proper graph rewrite and swap the optimized TF function based on the\n+  device placement.\n+\n+  Args:\n+    inputs: Input tensor of GRU layer.\n+    init_h: Initial state tensor for the cell output.\n+    kernel: Weights for cell kernel.\n+    recurrent_kernel: Weights for cell recurrent kernel.\n+    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n+      is used in this case.\n+    mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n+    time_major: Boolean, whether the inputs are in the format of\n+      [time, batch, feature] or [batch, time, feature].\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    sequence_lengths: The lengths of all sequences coming from a variable length\n+      input, such as ragged tensors. If the input has a fixed timestep size,\n+      this should be None.\n+    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+\n+  Returns:\n+    List of output tensors, same as standard_gru.\n+  \"\"\"\n+  params = {\n+      'inputs': inputs,\n+      'init_h': init_h,\n+      'kernel': kernel,\n+      'recurrent_kernel': recurrent_kernel,\n+      'bias': bias,\n+      'mask': mask,\n+      'time_major': time_major,\n+      'go_backwards': go_backwards,\n+      'sequence_lengths': sequence_lengths,\n+      'zero_output_for_mask': zero_output_for_mask,\n+  }\n+\n+  def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,\n+                            mask, time_major, go_backwards, sequence_lengths,\n+                            zero_output_for_mask):\n+    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n+    if mask is None:\n+      return gpu_gru(\n+          inputs=inputs,\n+          init_h=init_h,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths)\n+\n+    def cudnn_gru_fn():\n+      return gpu_gru(\n+          inputs=inputs,\n+          init_h=init_h,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths)\n+\n+    def standard_gru_fn():\n+      return standard_gru(\n+          inputs=inputs,\n+          init_h=init_h,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths,\n+          zero_output_for_mask=zero_output_for_mask)\n+\n+    return tf.cond(\n+        gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n+        true_fn=cudnn_gru_fn,\n+        false_fn=standard_gru_fn)\n+\n+  if gru_lstm_utils.use_new_gru_lstm_impl():\n+    # Chooses the implementation dynamically based on the running device.\n+    (last_output, outputs, new_h,\n+     runtime) = tf.__internal__.execute_fn_for_device(\n+         {\n+             gru_lstm_utils.CPU_DEVICE_NAME:\n+                 lambda: standard_gru(**params),\n+             gru_lstm_utils.GPU_DEVICE_NAME:\n+                 lambda: gpu_gru_with_fallback(**params)\n+         }, lambda: standard_gru(**params))\n+  else:\n+    # Each time a `tf.function` is called, we will give it a unique\n+    # identifiable API name, so that Grappler won't get confused when it\n+    # sees multiple GRU layers added into same graph, and it will be able\n+    # to pair up the different implementations across them.\n+    api_name = 'gru_' + str(uuid.uuid4())\n+    supportive_attribute = {\n+        'time_major': time_major,\n+        'go_backwards': go_backwards,\n+    }\n+    defun_standard_gru = gru_lstm_utils.generate_defun_backend(\n+        api_name, gru_lstm_utils.CPU_DEVICE_NAME, standard_gru,\n+        supportive_attribute)\n+    defun_gpu_gru = gru_lstm_utils.generate_defun_backend(\n+        api_name, gru_lstm_utils.GPU_DEVICE_NAME, gpu_gru_with_fallback,\n+        supportive_attribute)\n+\n+    # Call the normal GRU impl and register the cuDNN impl function. The\n+    # grappler will kick in during session execution to optimize the graph.\n+    last_output, outputs, new_h, runtime = defun_standard_gru(**params)\n+    gru_lstm_utils.function_register(defun_gpu_gru, **params)\n+\n+  return last_output, outputs, new_h, runtime\n\n@@ -12,28 +12,28 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for recurrent v2 layers functionality other than GRU, LSTM.\n+\"\"\"Tests that are common for GRU and LSTM.\n \n-See also: lstm_v2_test.py, gru_v2_test.py.\n+See also: lstm_test.py, gru_test.py.\n \"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import os\n-from absl.testing import parameterized\n-import numpy as np\n \n+from absl.testing import parameterized\n import keras\n+from keras.layers import embeddings\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import lstm\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.layers import embeddings\n-from keras.layers import recurrent_v2 as rnn_v2\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.run_all_keras_modes\n class RNNV2Test(test_combinations.TestCase):\n \n-  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n   def test_device_placement(self, layer):\n     if not tf.test.is_gpu_available():\n       self.skipTest('Need GPU for testing.')\n@@ -60,7 +60,7 @@ class RNNV2Test(test_combinations.TestCase):\n           run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, shuffle=False)\n \n-  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n   def test_reset_dropout_mask_between_batch(self, layer):\n     # See https://github.com/tensorflow/tensorflow/issues/29187 for more details\n     batch_size = 8\n@@ -87,7 +87,7 @@ class RNNV2Test(test_combinations.TestCase):\n                                self.evaluate(recurrent_dropout))\n         previous_recurrent_dropout = recurrent_dropout\n \n-  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n   def test_recurrent_dropout_with_stateful_RNN(self, layer):\n     # See https://github.com/tensorflow/tensorflow/issues/27829 for details.\n     # The issue was caused by using inplace mul for a variable, which was a\n@@ -97,11 +97,12 @@ class RNNV2Test(test_combinations.TestCase):\n               batch_input_shape=[32, None, 5], recurrent_dropout=0.2)\n     ])\n \n-  def test_recurrent_dropout_saved_model(self):\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n+  def test_recurrent_dropout_saved_model(self, layer):\n     if not tf.executing_eagerly():\n       self.skipTest('v2-only test')\n     inputs = keras.Input(shape=(784, 3), name='digits')\n-    x = keras.layers.GRU(64, activation='relu', name='GRU', dropout=0.1)(inputs)\n+    x = layer(64, activation='relu', name='RNN', dropout=0.1)(inputs)\n     x = keras.layers.Dense(64, activation='relu', name='dense')(x)\n     outputs = keras.layers.Dense(\n         10, activation='softmax', name='predictions')(\n@@ -109,17 +110,17 @@ class RNNV2Test(test_combinations.TestCase):\n     model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer')\n     model.save(os.path.join(self.get_temp_dir(), 'model'), save_format='tf')\n \n-  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n   def test_ragged(self, layer):\n     vocab_size = 100\n     inputs = tf.ragged.constant(\n         np.random.RandomState(0).randint(0, vocab_size, [128, 25]))\n     embedder = embeddings.Embedding(input_dim=vocab_size, output_dim=16)\n     embedded_inputs = embedder(inputs)\n-    lstm = layer(32)\n-    lstm(embedded_inputs)\n+    layer = layer(32)\n+    layer(embedded_inputs)\n \n-  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @parameterized.parameters([lstm.LSTM, gru.GRU])\n   @test_utils.run_v2_only\n   def test_compare_ragged_with_masks(self, layer):\n     vocab_size = 100\n\n@@ -0,0 +1,247 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utilities used by both the GRU and LSTM classes.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+import uuid\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.eager.context import get_device_name\n+\n+\n+# The following string constants are used by Defun approach for unified backend\n+# of LSTM and GRU.\n+_FUNCTION_API_NAME_ATTRIBUTE = 'api_implements'\n+_FUNCTION_DEVICE_ATTRIBUTE = 'api_preferred_device'\n+CPU_DEVICE_NAME = 'CPU'\n+GPU_DEVICE_NAME = 'GPU'\n+\n+# The following number constants are used to represent the runtime of the defun\n+# backend function. Since the CPU/GPU implementation are mathematically same, we\n+# need some signal for the function to indicate which function is executed. This\n+# is for testing purpose to verify the correctness of swapping backend function.\n+RUNTIME_UNKNOWN = 0\n+RUNTIME_CPU = 1\n+RUNTIME_GPU = 2\n+\n+CUDNN_AVAILABLE_MSG = 'Layer %s will use cuDNN kernels when running on GPU.'\n+CUDNN_NOT_AVAILABLE_MSG = ('Layer %s will not use cuDNN kernels since it '\n+                           'doesn\\'t meet the criteria. It will '\n+                           'use a generic GPU kernel as fallback when running '\n+                           'on GPU.')\n+\n+\n+def use_new_gru_lstm_impl():\n+  return False\n+\n+\n+# TODO(b/169707691): The wrapper can be removed if TFLite doesn't need to rely\n+# on supportive attributes from LSTM/GRU.\n+class DefunWrapper:\n+  \"\"\"A wrapper with no deep copy of the Defun in LSTM/GRU layer.\"\"\"\n+\n+  def __init__(self, time_major, go_backwards, layer_name):\n+    self.time_major = time_major\n+    self.go_backwards = go_backwards\n+    self.layer_name = layer_name\n+    if self.layer_name not in ['lstm', 'gru']:\n+      raise ValueError('Defun wrapper only applies to LSTM and GRU layer, '\n+                       'but given {}'.format(self.layer_name))\n+    # The first two attributes are added to support TFLite use case.\n+    supportive_attributes = {\n+        'time_major': self.time_major,\n+        'go_backwards': self.go_backwards,\n+        _FUNCTION_API_NAME_ATTRIBUTE: self.layer_name + '_' + str(uuid.uuid4())\n+    }\n+    if self.layer_name == 'lstm':\n+      from keras.layers.rnn import lstm  # pylint: disable=g-import-not-at-top\n+      layer_func = lstm.lstm_with_backend_selection\n+    else:\n+      from keras.layers.rnn import gru  # pylint: disable=g-import-not-at-top\n+      layer_func = gru.gru_with_backend_selection\n+\n+    self.defun_layer = tf.__internal__.function.defun_with_attributes(\n+        layer_func,\n+        attributes=supportive_attributes,\n+        autograph=False)\n+\n+  def __deepcopy__(self, memo):\n+    new_wrapper = type(self)(\n+        self.time_major, self.go_backwards, self.layer_name)\n+    memo[id(self)] = new_wrapper\n+    return new_wrapper\n+\n+\n+def canonical_to_params(weights, biases, shape, transpose_weights=False):\n+  \"\"\"Utility function convert variable to cuDNN compatible parameter.\n+\n+  Note that Keras weights for kernels are different from the cuDNN format. Eg.:\n+\n+  ```\n+    Keras                 cuDNN\n+    [[0, 1, 2],  <--->  [[0, 2, 4],\n+     [3, 4, 5]]          [1, 3, 5]]\n+  ```\n+\n+  If the input weights need to be in a unified format, then set\n+  `transpose_weights=True` to convert the weights.\n+\n+  Args:\n+    weights: list of weights for the individual kernels and recurrent kernels.\n+    biases: list of biases for individual gate.\n+    shape: the shape for the converted variables that will be feed to cuDNN.\n+    transpose_weights: boolean, whether to transpose the weights.\n+\n+  Returns:\n+    The converted weights that can be feed to cuDNN ops as param.\n+  \"\"\"\n+  def convert(w):\n+    return tf.transpose(w) if transpose_weights else w\n+\n+  weights = [tf.reshape(convert(x), shape) for x in weights]\n+  biases = [tf.reshape(x, shape) for x in biases]\n+  return tf.concat(weights + biases, axis=0)\n+\n+\n+def is_sequence_right_padded(mask):\n+  \"\"\"Check the mask tensor and see if it right padded.\n+\n+  For cuDNN kernel, it uses the sequence length param to skip the tailing\n+  timestep. If the data is left padded, or not a strict right padding (has\n+  masked value in the middle of the sequence), then cuDNN kernel won't be work\n+  properly in those cases.\n+\n+  Left padded data: [[False, False, True, True, True]].\n+  Right padded data: [[True, True, True, False, False]].\n+  Mixture of mask/unmasked data: [[True, False, True, False, False]].\n+\n+  Note that for the mixed data example above, the actually data RNN should see\n+  are those 2 Trues (index 0 and 2), the index 1 False should be ignored and not\n+  pollute the internal states.\n+\n+  Args:\n+    mask: the Boolean tensor with shape [batch, timestep]\n+\n+  Returns:\n+    boolean scalar tensor, whether the mask is strictly right padded.\n+  \"\"\"\n+  max_seq_length = tf.shape(mask)[1]\n+  count_of_true = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n+  right_padded_mask = tf.sequence_mask(\n+      count_of_true, maxlen=max_seq_length)\n+  return tf.reduce_all(tf.equal(mask, right_padded_mask))\n+\n+\n+def has_fully_masked_sequence(mask):\n+  # See https://github.com/tensorflow/tensorflow/issues/33148 for more details.\n+  # Cudnn kernel will error out if the input sequence contains any fully masked\n+  # data. We walk around this issue by rerouting the computation to standard\n+  # kernel, until the issue on cudnn side has been fixed.\n+  # For a fully masked sequence, it will contain all Falses. To make it easy to\n+  # check, we inverse the boolean, check if any of the sequence has all True.\n+  return tf.reduce_any(\n+      tf.reduce_all(\n+          tf.logical_not(mask),\n+          axis=1))\n+\n+\n+def is_cudnn_supported_inputs(mask, time_major):\n+  if time_major:\n+    mask = tf.transpose(mask)\n+\n+  return tf.logical_and(\n+      is_sequence_right_padded(mask),\n+      tf.logical_not(has_fully_masked_sequence(mask)))\n+\n+\n+def calculate_sequence_by_mask(mask, time_major):\n+  \"\"\"Calculate the sequence length tensor (1-D) based on the masking tensor.\n+\n+  The masking tensor is a 2D boolean tensor with shape [batch, timestep]. For\n+  any timestep that should be masked, the corresponding field will be False.\n+  Consider the following example:\n+    a = [[True, True, False, False],\n+         [True, True, True, False]]\n+  It is a (2, 4) tensor, and the corresponding sequence length result should be\n+  1D tensor with value [2, 3]. Note that the masking tensor must be right\n+  padded that could be checked by, e.g., `is_sequence_right_padded()`.\n+\n+  Args:\n+    mask: Boolean tensor with shape [batch, timestep] or [timestep, batch] if\n+      time_major=True.\n+    time_major: Boolean, which indicates whether the mask is time major or batch\n+      major.\n+  Returns:\n+    sequence_length: 1D int32 tensor.\n+  \"\"\"\n+  timestep_index = 0 if time_major else 1\n+  return tf.reduce_sum(tf.cast(mask, tf.int32), axis=timestep_index)\n+\n+\n+def generate_defun_backend(unique_api_name, preferred_device, func,\n+                           supportive_attributes):\n+  function_attributes = {\n+      _FUNCTION_API_NAME_ATTRIBUTE: unique_api_name,\n+      _FUNCTION_DEVICE_ATTRIBUTE: preferred_device,\n+  }\n+  function_attributes.update(supportive_attributes)\n+  return tf.__internal__.function.defun_with_attributes(\n+      func=func, attributes=function_attributes, autograph=False)\n+\n+\n+def get_context_device_type():\n+  \"\"\"Parse the current context and return the device type, eg CPU/GPU.\"\"\"\n+  current_device = get_device_name()\n+  if current_device is None:\n+    return None\n+  return tf.compat.v1.DeviceSpec.from_string(current_device).device_type\n+\n+\n+def runtime(runtime_name):\n+  with tf.device('/cpu:0'):\n+    return tf.constant(\n+        runtime_name, dtype=tf.float32, name='runtime')\n+\n+\n+def read_variable_value(v):\n+  \"\"\"Read the value of a variable if it is variable.\"\"\"\n+  if isinstance(v, tf.Variable):\n+    return v.read_value()\n+  return v\n+\n+\n+def function_register(func, *args, **kwargs):\n+  \"\"\"Register a specialization of a `Function` into the graph.\n+\n+  This won't actually call the function with the inputs, and only put the\n+  function definition into graph. Register function with different input param\n+  will result into multiple version of functions registered in graph.\n+\n+  Args:\n+    func: the `Function` instance that generated by a @defun\n+    *args: input arguments for the Python function.\n+    **kwargs: input keyword arguments for the Python function.\n+\n+  Returns:\n+    a `ConcreteFunction` object specialized to inputs and execution context.\n+\n+  Raises:\n+    ValueError: When the input function is not a defun wrapped python function.\n+  \"\"\"\n+  concrete_func = func.get_concrete_function(*args, **kwargs)\n+  concrete_func.add_to_graph()\n+  concrete_func.add_gradient_functions_to_graph()\n+  return concrete_func\n\n@@ -12,24 +12,26 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for GRU V2 layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for GRU layer.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n \n import copy\n import os\n import shutil\n \n from absl.testing import parameterized\n-import numpy as np\n-from tensorflow.core.protobuf import rewriter_config_pb2\n import keras\n-from tensorflow.python.framework import test_util as tf_test_util\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn import gru_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.layers import recurrent as rnn_v1\n-from keras.layers import recurrent_v2 as rnn\n from keras.utils import np_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.core.protobuf import rewriter_config_pb2\n+from tensorflow.python.framework import test_util as tf_test_util\n \n \n # Global config for grappler setting that is used for graph mode test.\n@@ -55,7 +57,7 @@ class GRUV2Test(test_combinations.TestCase):\n   def test_could_use_defun_backend(self, activation, recurrent_activation,\n                                    recurrent_dropout, unroll, use_bias,\n                                    reset_after):\n-    layer = rnn.GRU(1,\n+    layer = gru.GRU(1,\n                     activation=activation,\n                     recurrent_activation=recurrent_activation,\n                     recurrent_dropout=recurrent_dropout,\n@@ -66,10 +68,10 @@ class GRUV2Test(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n-    layer = rnn.GRU(1, activation=tf.tanh)\n+    layer = gru.GRU(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n-    layer = rnn.GRU(1, recurrent_activation=tf.sigmoid)\n+    layer = gru.GRU(1, recurrent_activation=tf.sigmoid)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n   def test_keras_model_with_gru(self):\n@@ -87,7 +89,7 @@ class GRUV2Test(test_combinations.TestCase):\n         num_classes=output_shape)\n     y_train = np_utils.to_categorical(y_train, output_shape)\n \n-    layer = rnn.GRU(rnn_state_size)\n+    layer = gru.GRU(rnn_state_size)\n \n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n@@ -104,7 +106,7 @@ class GRUV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer = rnn.GRU(units, input_shape=(None, embedding_dim))\n+    layer = gru.GRU(units, input_shape=(None, embedding_dim))\n     model = keras.models.Sequential()\n     model.add(layer)\n     model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')\n@@ -117,15 +119,15 @@ class GRUV2Test(test_combinations.TestCase):\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n-    model.add(rnn.GRU(10, return_sequences=True, unroll=False))\n-    model.add(rnn.GRU(5, return_sequences=True, unroll=False))\n+    model.add(gru.GRU(10, return_sequences=True, unroll=False))\n+    model.add(gru.GRU(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   def test_from_config_GRU(self):\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     for stateful in (False, True):\n       l1 = layer_class(units=1, stateful=stateful)\n       l2 = layer_class.from_config(l1.get_config())\n@@ -156,7 +158,7 @@ class GRUV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n     masked_input = keras.layers.Masking()(inputs)\n-    gru_layer = rnn_v1.GRU(rnn_state_size,\n+    gru_layer = gru_v1.GRU(rnn_state_size,\n                            recurrent_activation='sigmoid',\n                            reset_after=True)\n     output = gru_layer(masked_input)\n@@ -168,7 +170,7 @@ class GRUV2Test(test_combinations.TestCase):\n     y_2 = gru_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      cudnn_layer = rnn.GRU(rnn_state_size,\n+      cudnn_layer = gru.GRU(rnn_state_size,\n                             recurrent_activation='sigmoid',\n                             reset_after=True)\n       cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n@@ -202,7 +204,7 @@ class GRUV2Test(test_combinations.TestCase):\n     def build_model():\n       inputs = keras.layers.Input(\n           shape=[timestep, input_dim], dtype=tf.float32)\n-      layer = rnn.GRU(\n+      layer = gru.GRU(\n           units,\n           use_bias=use_bias,\n           bias_initializer=bias_initializer)\n@@ -231,14 +233,14 @@ class GRUV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n     with test_utils.device(should_use_gpu=False):\n-      layer = rnn.GRU(rnn_state_size)\n+      layer = gru.GRU(rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n       y_1 = cpu_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      layer = rnn.GRU(rnn_state_size)\n+      layer = gru.GRU(rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n       gpu_model.set_weights(weights)\n@@ -248,7 +250,7 @@ class GRUV2Test(test_combinations.TestCase):\n     # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve\n     # the same output.\n     with test_utils.device(should_use_gpu=True):\n-      layer = rnn_v1.GRU(rnn_state_size,\n+      layer = gru_v1.GRU(rnn_state_size,\n                          recurrent_activation='sigmoid',\n                          reset_after=True)\n       output = layer(inputs)\n@@ -293,11 +295,11 @@ class GRUV2Test(test_combinations.TestCase):\n         outputs = layer(inputs)\n       return keras.models.Model(inputs, outputs)\n \n-    gru_model = build_model(rnn_v1.GRU)\n+    gru_model = build_model(gru_v1.GRU)\n     y_ref = gru_model.predict(x_train)\n     weights = gru_model.get_weights()\n \n-    gru_v2_model = build_model(rnn.GRU)\n+    gru_v2_model = build_model(gru.GRU)\n     gru_v2_model.set_weights(weights)\n     y = gru_v2_model.predict(x_train)\n \n@@ -307,7 +309,7 @@ class GRUV2Test(test_combinations.TestCase):\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_with_masking_layer_GRU(self):\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -327,8 +329,8 @@ class GRUV2Test(test_combinations.TestCase):\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n     model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(rnn.GRU(10, return_sequences=True, unroll=False))\n-    model.add(rnn.GRU(5, return_sequences=True, unroll=False))\n+    model.add(gru.GRU(10, return_sequences=True, unroll=False))\n+    model.add(gru.GRU(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n@@ -340,7 +342,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.GRU,\n+        gru.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n@@ -355,7 +357,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.GRU,\n+        gru.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True,\n                 'dtype': 'float64'},\n@@ -366,7 +368,7 @@ class GRUV2Test(test_combinations.TestCase):\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_return_states_GRU(self):\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     x = np.random.random((2, 3, 4))\n     y = np.abs(np.random.random((2, 5)))\n     s = np.abs(np.random.random((2, 5)))\n@@ -386,7 +388,7 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.GRU,\n+        gru.GRU,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n                 'recurrent_dropout': 0.1},\n@@ -394,7 +396,7 @@ class GRUV2Test(test_combinations.TestCase):\n \n   def test_constraints_GRU(self):\n     embedding_dim = 4\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     k_constraint = keras.constraints.max_norm(0.01)\n     r_constraint = keras.constraints.max_norm(0.01)\n     b_constraint = keras.constraints.max_norm(0.01)\n@@ -418,14 +420,14 @@ class GRUV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.GRU,\n+        gru.GRU,\n         kwargs={'units': units,\n                 'implementation': implementation_mode},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n   def test_regularizers_GRU(self):\n     embedding_dim = 4\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     layer = layer_class(\n         5,\n         return_sequences=False,\n@@ -453,7 +455,7 @@ class GRUV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer_class = rnn.GRU\n+    layer_class = gru.GRU\n     model = keras.models.Sequential()\n     model.add(\n         keras.layers.Embedding(\n@@ -534,7 +536,7 @@ class GRUV2Test(test_combinations.TestCase):\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, embedding_dim,\n                                batch_input_shape=[batch_size, timestep]),\n-        rnn.GRU(units, return_sequences=True, stateful=True),\n+        gru.GRU(units, return_sequences=True, stateful=True),\n         keras.layers.Dense(vocab_size)\n     ])\n     model.compile(\n@@ -558,14 +560,14 @@ class GRUV2Test(test_combinations.TestCase):\n     mask[:, masksteps:] = 0\n \n     # Test for V1 behavior.\n-    lstm_v1 = rnn_v1.GRU(units, return_sequences=True, go_backwards=True)\n+    lstm_v1 = gru_v1.GRU(units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n       outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))\n       outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n \n     # Test for V2 behavior.\n-    lstm = rnn.GRU(units, return_sequences=True, go_backwards=True)\n+    lstm = gru.GRU(units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n       outputs_masked = lstm(inputs, mask=tf.constant(mask))\n       outputs_trimmed = lstm(inputs[:, :masksteps])\n@@ -581,7 +583,7 @@ class GRUV2Test(test_combinations.TestCase):\n           (x, y)).shuffle(100).batch(32)\n \n       inp = keras.layers.Input(shape=(4, 8))\n-      layer = rnn.GRU(1)(inp)\n+      layer = gru.GRU(1)(inp)\n       layer = keras.layers.Dense(1)(layer)\n \n       model = keras.models.Model(inp, layer)\n@@ -608,7 +610,7 @@ class GRUV2Test(test_combinations.TestCase):\n             mask_zero=True,\n             input_length=timestep,\n             batch_input_shape=(num_samples, timestep)))\n-    layer = rnn.GRU(units)\n+    layer = gru.GRU(units)\n     model.add(layer)\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n@@ -621,14 +623,14 @@ class GRUV2Test(test_combinations.TestCase):\n   def test_deepcopy(self):\n     if not tf.executing_eagerly():\n       self.skipTest('v2-only test')\n-    original_layer = rnn.GRU(5)\n+    original_layer = gru.GRU(5)\n     copied_layer = copy.deepcopy(original_layer)\n     self.assertEqual(copied_layer.units, 5)\n     self.assertEqual(original_layer.get_config(), original_layer.get_config())\n \n     # Copy layer before layer call on inputs without weight initialization.\n     inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)\n-    original_layer = rnn.GRU(4)\n+    original_layer = gru.GRU(4)\n     copied_layer = copy.deepcopy(original_layer)\n     outputs = original_layer(inputs)\n     copied_outputs = copied_layer(inputs)\n@@ -636,7 +638,7 @@ class GRUV2Test(test_combinations.TestCase):\n         self.evaluate(outputs), self.evaluate(copied_outputs))\n \n     # Copy layer after layer call on inputs with weight initialization.\n-    original_layer = rnn.GRU(4)\n+    original_layer = gru.GRU(4)\n     outputs = original_layer(inputs)\n     copied_layer = copy.deepcopy(original_layer)\n     copied_outputs = copied_layer(inputs)\n@@ -653,7 +655,8 @@ class GRULayerGradientTapeTest(test_combinations.TestCase):\n       embedding_size = 11\n       gru_unit_size = 12\n \n-      gru = rnn.GRU(gru_unit_size,\n+      gru_layer = gru.GRU(\n+          gru_unit_size,\n           return_sequences=True,\n           return_state=True,\n           recurrent_activation='sigmoid',\n@@ -664,11 +667,11 @@ class GRULayerGradientTapeTest(test_combinations.TestCase):\n \n       with tf.GradientTape() as tape:\n         hidden_state = tf.zeros([1, gru_unit_size], dtype=tf.float32)\n-        _, state = gru(x, initial_state=hidden_state)\n+        _, state = gru_layer(x, initial_state=hidden_state)\n \n         loss = tf.reduce_mean(tf.square(state - y))\n \n-      tape.gradient(loss, gru.variables)\n+      tape.gradient(loss, gru_layer.variables)\n \n \n @test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n@@ -704,13 +707,13 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n   @test_utils.run_v2_only\n   def test_GRU_runtime(self):\n-    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -731,7 +734,7 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n   def test_GRU_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n-    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -762,38 +765,38 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n     # Verify unpadded data.\n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n     # Update x/y to be right padded by setting the last timestep to 0\n     x_train[:, -1, :] = 0\n     y_train[:, -1] = 0\n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n     # Further update x/y to be mix padded (masks in the middle), and verify\n     # only cpu kernel can be selected.\n     x_train[:, -3, :] = 0\n     y_train[:, -3] = 0\n     _, runtime_value = model.predict(x_train)\n-    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+    self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n   @test_utils.run_v2_only\n   def test_GRU_runtime_with_cond(self):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n     # states.\n-    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)\n+    layer = gru.GRU(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n \n     zeros = tf.zeros([self.batch, self.output_shape])\n-    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)\n+    dummy_runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n     a = tf.constant(0)\n     b = tf.constant(1)\n     # Will always run the GRU layer.\n\n@@ -0,0 +1,599 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Gated Recurrent Unit V1 layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+RECURRENT_DROPOUT_WARNING_MSG = (\n+    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n+    'Using `implementation=1`.')\n+\n+\n+@keras_export(v1=['keras.layers.GRUCell'])\n+class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+  \"\"\"Cell class for the GRU layer.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass None, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+      Default: hard sigmoid (`hard_sigmoid`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix,\n+      used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    recurrent_constraint: Constraint function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for the linear transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+    reset_after: GRU convention (whether to apply reset gate after or\n+      before matrix multiplication). False = \"before\" (default),\n+      True = \"after\" (cuDNN compatible).\n+\n+  Call arguments:\n+    inputs: A 2D tensor.\n+    states: List of state tensors corresponding to the previous timestep.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               reset_after=False,\n+               **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n+    # By default use cached variable under v2 mode, see b/143699808.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n+    else:\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n+    super(GRUCell, self).__init__(**kwargs)\n+    self.units = units\n+    self.activation = activations.get(activation)\n+    self.recurrent_activation = activations.get(recurrent_activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1., max(0., dropout))\n+    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+\n+    implementation = kwargs.pop('implementation', 1)\n+    if self.recurrent_dropout != 0 and implementation != 1:\n+      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n+      self.implementation = 1\n+    else:\n+      self.implementation = implementation\n+    self.reset_after = reset_after\n+    self.state_size = self.units\n+    self.output_size = self.units\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    input_dim = input_shape[-1]\n+    default_caching_device = rnn_utils.caching_device(self)\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 3),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        caching_device=default_caching_device)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 3),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint,\n+        caching_device=default_caching_device)\n+\n+    if self.use_bias:\n+      if not self.reset_after:\n+        bias_shape = (3 * self.units,)\n+      else:\n+        # separate biases for input and recurrent kernels\n+        # Note: the shape is intentionally different from CuDNNGRU biases\n+        # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n+        # when loading and converting saved weights.\n+        bias_shape = (2, 3 * self.units)\n+      self.bias = self.add_weight(shape=bias_shape,\n+                                  name='bias',\n+                                  initializer=self.bias_initializer,\n+                                  regularizer=self.bias_regularizer,\n+                                  constraint=self.bias_constraint,\n+                                  caching_device=default_caching_device)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs, states, training=None):\n+    h_tm1 = states[0] if tf.nest.is_nested(\n+        states) else states  # previous memory\n+\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        h_tm1, training, count=3)\n+\n+    if self.use_bias:\n+      if not self.reset_after:\n+        input_bias, recurrent_bias = self.bias, None\n+      else:\n+        input_bias, recurrent_bias = tf.unstack(self.bias)\n+\n+    if self.implementation == 1:\n+      if 0. < self.dropout < 1.:\n+        inputs_z = inputs * dp_mask[0]\n+        inputs_r = inputs * dp_mask[1]\n+        inputs_h = inputs * dp_mask[2]\n+      else:\n+        inputs_z = inputs\n+        inputs_r = inputs\n+        inputs_h = inputs\n+\n+      x_z = backend.dot(inputs_z, self.kernel[:, :self.units])\n+      x_r = backend.dot(inputs_r, self.kernel[:, self.units:self.units * 2])\n+      x_h = backend.dot(inputs_h, self.kernel[:, self.units * 2:])\n+\n+      if self.use_bias:\n+        x_z = backend.bias_add(x_z, input_bias[:self.units])\n+        x_r = backend.bias_add(x_r, input_bias[self.units: self.units * 2])\n+        x_h = backend.bias_add(x_h, input_bias[self.units * 2:])\n+\n+      if 0. < self.recurrent_dropout < 1.:\n+        h_tm1_z = h_tm1 * rec_dp_mask[0]\n+        h_tm1_r = h_tm1 * rec_dp_mask[1]\n+        h_tm1_h = h_tm1 * rec_dp_mask[2]\n+      else:\n+        h_tm1_z = h_tm1\n+        h_tm1_r = h_tm1\n+        h_tm1_h = h_tm1\n+\n+      recurrent_z = backend.dot(h_tm1_z, self.recurrent_kernel[:, :self.units])\n+      recurrent_r = backend.dot(\n+          h_tm1_r, self.recurrent_kernel[:, self.units:self.units * 2])\n+      if self.reset_after and self.use_bias:\n+        recurrent_z = backend.bias_add(recurrent_z, recurrent_bias[:self.units])\n+        recurrent_r = backend.bias_add(\n+            recurrent_r, recurrent_bias[self.units:self.units * 2])\n+\n+      z = self.recurrent_activation(x_z + recurrent_z)\n+      r = self.recurrent_activation(x_r + recurrent_r)\n+\n+      # reset gate applied after/before matrix multiplication\n+      if self.reset_after:\n+        recurrent_h = backend.dot(\n+            h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n+        if self.use_bias:\n+          recurrent_h = backend.bias_add(\n+              recurrent_h, recurrent_bias[self.units * 2:])\n+        recurrent_h = r * recurrent_h\n+      else:\n+        recurrent_h = backend.dot(\n+            r * h_tm1_h, self.recurrent_kernel[:, self.units * 2:])\n+\n+      hh = self.activation(x_h + recurrent_h)\n+    else:\n+      if 0. < self.dropout < 1.:\n+        inputs = inputs * dp_mask[0]\n+\n+      # inputs projected by all gate matrices at once\n+      matrix_x = backend.dot(inputs, self.kernel)\n+      if self.use_bias:\n+        # biases: bias_z_i, bias_r_i, bias_h_i\n+        matrix_x = backend.bias_add(matrix_x, input_bias)\n+\n+      x_z, x_r, x_h = tf.split(matrix_x, 3, axis=-1)\n+\n+      if self.reset_after:\n+        # hidden state projected by all gate matrices at once\n+        matrix_inner = backend.dot(h_tm1, self.recurrent_kernel)\n+        if self.use_bias:\n+          matrix_inner = backend.bias_add(matrix_inner, recurrent_bias)\n+      else:\n+        # hidden state projected separately for update/reset and new\n+        matrix_inner = backend.dot(\n+            h_tm1, self.recurrent_kernel[:, :2 * self.units])\n+\n+      recurrent_z, recurrent_r, recurrent_h = tf.split(\n+          matrix_inner, [self.units, self.units, -1], axis=-1)\n+\n+      z = self.recurrent_activation(x_z + recurrent_z)\n+      r = self.recurrent_activation(x_r + recurrent_r)\n+\n+      if self.reset_after:\n+        recurrent_h = r * recurrent_h\n+      else:\n+        recurrent_h = backend.dot(\n+            r * h_tm1, self.recurrent_kernel[:, 2 * self.units:])\n+\n+      hh = self.activation(x_h + recurrent_h)\n+    # previous and candidate state mixed by update gate\n+    h = z * h_tm1 + (1 - z) * hh\n+    new_state = [h] if tf.nest.is_nested(states) else h\n+    return h, new_state\n+\n+  def get_config(self):\n+    config = {\n+        'units': self.units,\n+        'activation': activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias': self.use_bias,\n+        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer': initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint': constraints.serialize(self.bias_constraint),\n+        'dropout': self.dropout,\n+        'recurrent_dropout': self.recurrent_dropout,\n+        'implementation': self.implementation,\n+        'reset_after': self.reset_after\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self))\n+    base_config = super(GRUCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype)\n+\n+\n+@keras_export(v1=['keras.layers.GRU'])\n+class GRU(RNN):\n+  \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n+\n+  There are two variants. The default one is based on 1406.1078v3 and\n+  has reset gate applied to hidden state before matrix multiplication. The\n+  other one is based on original 1406.1078v1 and has the order reversed.\n+\n+  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n+  inference on CPU. Thus it has separate biases for `kernel` and\n+  `recurrent_kernel`. Use `'reset_after'=True` and\n+  `recurrent_activation='sigmoid'`.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+      Default: hard sigmoid (`hard_sigmoid`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\")..\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    recurrent_constraint: Constraint function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+    return_sequences: Boolean. Whether to return the last output\n+      in the output sequence, or the full sequence.\n+    return_state: Boolean. Whether to return the last state\n+      in addition to the output.\n+    go_backwards: Boolean (default False).\n+      If True, process the input sequence backwards and return the\n+      reversed sequence.\n+    stateful: Boolean (default False). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    unroll: Boolean (default False).\n+      If True, the network will be unrolled,\n+      else a symbolic loop will be used.\n+      Unrolling can speed-up a RNN,\n+      although it tends to be more memory-intensive.\n+      Unrolling is only suitable for short sequences.\n+    time_major: The shape format of the `inputs` and `outputs` tensors.\n+      If True, the inputs and outputs will be in shape\n+      `(timesteps, batch, ...)`, whereas in the False case, it will be\n+      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n+      efficient because it avoids transposes at the beginning and end of the\n+      RNN calculation. However, most TensorFlow data is batch-major, so by\n+      default this function accepts input and emits output in batch-major\n+      form.\n+    reset_after: GRU convention (whether to apply reset gate after or\n+      before matrix multiplication). False = \"before\" (default),\n+      True = \"after\" (cuDNN compatible).\n+\n+  Call arguments:\n+    inputs: A 3D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or\n+      `recurrent_dropout` is used.\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               reset_after=False,\n+               **kwargs):\n+    implementation = kwargs.pop('implementation', 1)\n+    if implementation == 0:\n+      logging.warning('`implementation=0` has been deprecated, '\n+                      'and now defaults to `implementation=1`.'\n+                      'Please update your layer call.')\n+    if 'enable_caching_device' in kwargs:\n+      cell_kwargs = {'enable_caching_device':\n+                     kwargs.pop('enable_caching_device')}\n+    else:\n+      cell_kwargs = {}\n+    cell = GRUCell(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=implementation,\n+        reset_after=reset_after,\n+        dtype=kwargs.get('dtype'),\n+        trainable=kwargs.get('trainable', True),\n+        **cell_kwargs)\n+    super(GRU, self).__init__(\n+        cell,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        unroll=unroll,\n+        **kwargs)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+    self.input_spec = [InputSpec(ndim=3)]\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    return super(GRU, self).call(\n+        inputs, mask=mask, training=training, initial_state=initial_state)\n+\n+  @property\n+  def units(self):\n+    return self.cell.units\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def recurrent_activation(self):\n+    return self.cell.recurrent_activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  @property\n+  def implementation(self):\n+    return self.cell.implementation\n+\n+  @property\n+  def reset_after(self):\n+    return self.cell.reset_after\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation,\n+        'reset_after':\n+            self.reset_after\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n+    base_config = super(GRU, self).get_config()\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    if 'implementation' in config and config['implementation'] == 0:\n+      config['implementation'] = 1\n+    return cls(**config)\n\n@@ -12,25 +12,23 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for GRU layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for GRU V1 layer.\"\"\"\n \n import copy\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.run_all_keras_modes\n class GRULayerTest(test_combinations.TestCase):\n \n-  def test_return_sequences_GRU(self):\n+  def test_return_sequences_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -45,7 +43,7 @@ class GRULayerTest(test_combinations.TestCase):\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Double type is not yet supported in ROCm')\n   @test_utils.run_v2_only\n-  def test_float64_GRU(self):\n+  def test_float64_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -58,7 +56,7 @@ class GRULayerTest(test_combinations.TestCase):\n         input_shape=(num_samples, timesteps, embedding_dim),\n         input_dtype='float64')\n \n-  def test_dynamic_behavior_GRU(self):\n+  def test_dynamic_behavior_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -74,7 +72,7 @@ class GRULayerTest(test_combinations.TestCase):\n     y = np.random.random((num_samples, units))\n     model.train_on_batch(x, y)\n \n-  def test_dropout_GRU(self):\n+  def test_dropout_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -92,7 +90,7 @@ class GRULayerTest(test_combinations.TestCase):\n     self.assertEqual(layer.implementation, 1)\n \n   @parameterized.parameters([0, 1, 2])\n-  def test_implementation_mode_GRU(self, implementation_mode):\n+  def test_implementation_mode_gru(self, implementation_mode):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -103,7 +101,7 @@ class GRULayerTest(test_combinations.TestCase):\n                 'implementation': implementation_mode},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n-  def test_reset_after_GRU(self):\n+  def test_reset_after_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -131,7 +129,7 @@ class GRULayerTest(test_combinations.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='MIOpen only supports packed input output')\n-  def test_with_masking_layer_GRU(self):\n+  def test_with_masking_layer_gru(self):\n     layer_class = keras.layers.GRU\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n@@ -148,7 +146,7 @@ class GRULayerTest(test_combinations.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='MIOpen only supports packed input output')\n-  def test_statefulness_GRU(self):\n+  def test_statefulness_gru(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -225,7 +223,7 @@ class GRULayerTest(test_combinations.TestCase):\n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class GRULayerGenericTest(tf.test.TestCase):\n \n-  def test_constraints_GRU(self):\n+  def test_constraints_gru(self):\n     embedding_dim = 4\n     layer_class = keras.layers.GRU\n     k_constraint = keras.constraints.max_norm(0.01)\n@@ -244,20 +242,20 @@ class GRULayerGenericTest(tf.test.TestCase):\n     self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n     self.assertEqual(layer.cell.bias.constraint, b_constraint)\n \n-  def test_from_config_GRU(self):\n+  def test_from_config_gru(self):\n     layer_class = keras.layers.GRU\n     for stateful in (False, True):\n       l1 = layer_class(units=1, stateful=stateful)\n       l2 = layer_class.from_config(l1.get_config())\n       assert l1.get_config() == l2.get_config()\n \n-  def test_deep_copy_GRU(self):\n+  def test_deep_copy_gru(self):\n     cell = keras.layers.GRUCell(5)\n     copied_cell = copy.deepcopy(cell)\n     self.assertEqual(copied_cell.units, 5)\n     self.assertEqual(cell.get_config(), copied_cell.get_config())\n \n-  def test_regularizers_GRU(self):\n+  def test_regularizers_gru(self):\n     embedding_dim = 4\n     layer_class = keras.layers.GRU\n     layer = layer_class(\n@@ -270,14 +268,14 @@ class GRULayerGenericTest(tf.test.TestCase):\n         bias_regularizer='l2',\n         activity_regularizer='l1')\n     layer.build((None, None, 2))\n-    self.assertEqual(len(layer.losses), 3)\n+    self.assertLen(layer.losses, 3)\n \n     x = keras.backend.variable(np.ones((2, 3, 2)))\n     layer(x)\n     if tf.executing_eagerly():\n-      self.assertEqual(len(layer.losses), 4)\n+      self.assertLen(layer.losses, 4)\n     else:\n-      self.assertEqual(len(layer.get_losses_for(x)), 1)\n+      self.assertLen(layer.get_losses_for(x), 1)\n \n \n if __name__ == '__main__':\n\n@@ -0,0 +1,190 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Module implementing the V1 version of RNN cell wrappers.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from keras.layers.rnn import base_cell_wrappers\n+from keras.layers.rnn.legacy_cells import RNNCell\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.python.util.tf_export import tf_export\n+\n+\n+# This can be used with self.assertRaisesRegexp for assert_like_rnncell.\n+ASSERT_LIKE_RNNCELL_ERROR_REGEXP = \"is not an RNNCell\"\n+\n+\n+def _hasattr(obj, attr_name):\n+  try:\n+    getattr(obj, attr_name)\n+  except AttributeError:\n+    return False\n+  else:\n+    return True\n+\n+\n+def assert_like_rnncell(cell_name, cell):\n+  \"\"\"Raises a TypeError if cell is not like an RNNCell.\n+\n+  NOTE: Do not rely on the error message (in particular in tests) which can be\n+  subject to change to increase readability. Use\n+  ASSERT_LIKE_RNNCELL_ERROR_REGEXP.\n+\n+  Args:\n+    cell_name: A string to give a meaningful error referencing to the name of\n+      the functionargument.\n+    cell: The object which should behave like an RNNCell.\n+\n+  Raises:\n+    TypeError: A human-friendly exception.\n+  \"\"\"\n+  conditions = [\n+      _hasattr(cell, \"output_size\"),\n+      _hasattr(cell, \"state_size\"),\n+      _hasattr(cell, \"get_initial_state\") or _hasattr(cell, \"zero_state\"),\n+      callable(cell),\n+  ]\n+  errors = [\n+      \"'output_size' property is missing\", \"'state_size' property is missing\",\n+      \"either 'zero_state' or 'get_initial_state' method is required\",\n+      \"is not callable\"\n+  ]\n+\n+  if not all(conditions):\n+\n+    errors = [error for error, cond in zip(errors, conditions) if not cond]\n+    raise TypeError(\"The argument {!r} ({}) is not an RNNCell: {}.\".format(\n+        cell_name, cell, \", \".join(errors)))\n+\n+\n+class _RNNCellWrapperV1(RNNCell):\n+  \"\"\"Base class for cells wrappers V1 compatibility.\n+\n+  This class along with `_RNNCellWrapperV2` allows to define cells wrappers that\n+  are compatible with V1 and V2, and defines helper methods for this purpose.\n+  \"\"\"\n+\n+  def __init__(self, cell, *args, **kwargs):\n+    super(_RNNCellWrapperV1, self).__init__(*args, **kwargs)\n+    assert_like_rnncell(\"cell\", cell)\n+    self.cell = cell\n+    if isinstance(cell, tf.__internal__.tracking.Trackable):\n+      self._track_trackable(self.cell, name=\"cell\")\n+\n+  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n+    \"\"\"Calls the wrapped cell and performs the wrapping logic.\n+\n+    This method is called from the wrapper's `call` or `__call__` methods.\n+\n+    Args:\n+      inputs: A tensor with wrapped cell's input.\n+      state: A tensor or tuple of tensors with wrapped cell's state.\n+      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n+        `__call__` or 'call' method).\n+      **kwargs: Additional arguments.\n+\n+    Returns:\n+      A pair containing:\n+      - Output: A tensor with cell's output.\n+      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+    \"\"\"\n+    raise NotImplementedError\n+\n+  def __call__(self, inputs, state, scope=None):\n+    \"\"\"Runs the RNN cell step computation.\n+\n+    We assume that the wrapped RNNCell is being built within its `__call__`\n+    method. We directly use the wrapped cell's `__call__` in the overridden\n+    wrapper `__call__` method.\n+\n+    This allows to use the wrapped cell and the non-wrapped cell equivalently\n+    when using `__call__`.\n+\n+    Args:\n+      inputs: A tensor with wrapped cell's input.\n+      state: A tensor or tuple of tensors with wrapped cell's state.\n+      scope: VariableScope for the subgraph created in the wrapped cells'\n+        `__call__`.\n+\n+    Returns:\n+      A pair containing:\n+\n+      - Output: A tensor with cell's output.\n+      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+    \"\"\"\n+    return self._call_wrapped_cell(\n+        inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n+\n+  def get_config(self):\n+    config = {\n+        \"cell\": {\n+            \"class_name\": self.cell.__class__.__name__,\n+            \"config\": self.cell.get_config()\n+        },\n+    }\n+    base_config = super(_RNNCellWrapperV1, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    config = config.copy()\n+    cell = config.pop(\"cell\")\n+    try:\n+      assert_like_rnncell(\"cell\", cell)\n+      return cls(cell, **config)\n+    except TypeError:\n+      raise ValueError(\"RNNCellWrapper cannot reconstruct the wrapped cell. \"\n+                       \"Please overwrite the cell in the config with a RNNCell \"\n+                       \"instance.\")\n+\n+\n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DropoutWrapper\"])\n+@tf_export(v1=[\"nn.rnn_cell.DropoutWrapper\"])\n+class DropoutWrapper(base_cell_wrappers.DropoutWrapperBase,\n+                     _RNNCellWrapperV1):\n+  \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n+\n+  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n+    super(DropoutWrapper, self).__init__(*args, **kwargs)\n+\n+  __init__.__doc__ = base_cell_wrappers.DropoutWrapperBase.__init__.__doc__\n+\n+\n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.ResidualWrapper\"])\n+@tf_export(v1=[\"nn.rnn_cell.ResidualWrapper\"])\n+class ResidualWrapper(base_cell_wrappers.ResidualWrapperBase,\n+                      _RNNCellWrapperV1):\n+  \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n+\n+  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n+    super(ResidualWrapper, self).__init__(*args, **kwargs)\n+\n+  __init__.__doc__ = base_cell_wrappers.ResidualWrapperBase.__init__.__doc__\n+\n+\n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DeviceWrapper\"])\n+@tf_export(v1=[\"nn.rnn_cell.DeviceWrapper\"])\n+class DeviceWrapper(base_cell_wrappers.DeviceWrapperBase,\n+                    _RNNCellWrapperV1):\n+\n+  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n+    super(DeviceWrapper, self).__init__(*args, **kwargs)\n+\n+  __init__.__doc__ = base_cell_wrappers.DeviceWrapperBase.__init__.__doc__\n\n@@ -0,0 +1,37 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for RNN cell wrappers v1 implementation.\"\"\"\n+\n+from absl.testing import parameterized\n+from keras.layers.rnn import legacy_cell_wrappers\n+from keras.layers.rnn import legacy_cells\n+from keras.testing_infra import test_combinations\n+import tensorflow.compat.v2 as tf\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class RNNCellWrapperV1Test(tf.test.TestCase, parameterized.TestCase):\n+\n+  @parameterized.parameters([\n+      legacy_cell_wrappers.DropoutWrapper, legacy_cell_wrappers.ResidualWrapper\n+  ])\n+  def testWrapperKerasStyle(self, wrapper):\n+    \"\"\"Tests if wrapper cell is instantiated in keras style scope.\"\"\"\n+    wrapped_cell = wrapper(legacy_cells.BasicRNNCell(1))\n+    self.assertFalse(wrapped_cell._keras_style)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -12,8 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n-\"\"\"Module implementing RNN Cells.\n+\"\"\"Legacy module implementing RNN Cells.\n \n This module provides a number of basic commonly used RNN cells, such as LSTM\n (Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of\n@@ -21,32 +20,32 @@ operators that allow adding dropouts, projections, or embeddings for inputs.\n Constructing multi-layer cells is supported by the class `MultiRNNCell`, or by\n calling the `rnn` ops several times.\n \"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n-\n import collections\n import warnings\n+\n from keras import activations\n from keras import backend\n from keras import initializers\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n-from keras.layers.legacy_rnn import rnn_cell_wrapper_impl\n from keras.legacy_tf_layers import base as base_layer\n from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.python.util.tf_export import tf_export\n \n+\n _BIAS_VARIABLE_NAME = \"bias\"\n _WEIGHTS_VARIABLE_NAME = \"kernel\"\n \n-# This can be used with self.assertRaisesRegexp for assert_like_rnncell.\n-ASSERT_LIKE_RNNCELL_ERROR_REGEXP = \"is not an RNNCell\"\n-\n \n def _hasattr(obj, attr_name):\n   try:\n@@ -57,40 +56,6 @@ def _hasattr(obj, attr_name):\n     return True\n \n \n-def assert_like_rnncell(cell_name, cell):\n-  \"\"\"Raises a TypeError if cell is not like an RNNCell.\n-\n-  NOTE: Do not rely on the error message (in particular in tests) which can be\n-  subject to change to increase readability. Use\n-  ASSERT_LIKE_RNNCELL_ERROR_REGEXP.\n-\n-  Args:\n-    cell_name: A string to give a meaningful error referencing to the name of\n-      the functionargument.\n-    cell: The object which should behave like an RNNCell.\n-\n-  Raises:\n-    TypeError: A human-friendly exception.\n-  \"\"\"\n-  conditions = [\n-      _hasattr(cell, \"output_size\"),\n-      _hasattr(cell, \"state_size\"),\n-      _hasattr(cell, \"get_initial_state\") or _hasattr(cell, \"zero_state\"),\n-      callable(cell),\n-  ]\n-  errors = [\n-      \"'output_size' property is missing\", \"'state_size' property is missing\",\n-      \"either 'zero_state' or 'get_initial_state' method is required\",\n-      \"is not callable\"\n-  ]\n-\n-  if not all(conditions):\n-\n-    errors = [error for error, cond in zip(errors, conditions) if not cond]\n-    raise TypeError(\"The argument {!r} ({}) is not an RNNCell: {}.\".format(\n-        cell_name, cell, \", \".join(errors)))\n-\n-\n def _concat(prefix, suffix, static=False):\n   \"\"\"Concat that enables int, Tensor, or TensorShape values.\n \n@@ -230,7 +195,8 @@ class RNNCell(base_layer.Layer):\n       scope = getattr(self, scope_attrname, None)\n       if scope is None:\n         scope = tf.compat.v1.variable_scope(\n-            tf.compat.v1.get_variable_scope(), custom_getter=self._rnn_get_variable)\n+            tf.compat.v1.get_variable_scope(),\n+            custom_getter=self._rnn_get_variable)\n         setattr(self, scope_attrname, scope)\n       with scope:\n         return super(RNNCell, self).__call__(inputs, state)\n@@ -1113,122 +1079,6 @@ class LSTMCell(LayerRNNCell):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-class _RNNCellWrapperV1(RNNCell):\n-  \"\"\"Base class for cells wrappers V1 compatibility.\n-\n-  This class along with `_RNNCellWrapperV2` allows to define cells wrappers that\n-  are compatible with V1 and V2, and defines helper methods for this purpose.\n-  \"\"\"\n-\n-  def __init__(self, cell, *args, **kwargs):\n-    super(_RNNCellWrapperV1, self).__init__(*args, **kwargs)\n-    assert_like_rnncell(\"cell\", cell)\n-    self.cell = cell\n-    if isinstance(cell, tf.__internal__.tracking.Trackable):\n-      self._track_trackable(self.cell, name=\"cell\")\n-\n-  def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-    \"\"\"Calls the wrapped cell and performs the wrapping logic.\n-\n-    This method is called from the wrapper's `call` or `__call__` methods.\n-\n-    Args:\n-      inputs: A tensor with wrapped cell's input.\n-      state: A tensor or tuple of tensors with wrapped cell's state.\n-      cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-        `__call__` or 'call' method).\n-      **kwargs: Additional arguments.\n-\n-    Returns:\n-      A pair containing:\n-      - Output: A tensor with cell's output.\n-      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n-    \"\"\"\n-    raise NotImplementedError\n-\n-  def __call__(self, inputs, state, scope=None):\n-    \"\"\"Runs the RNN cell step computation.\n-\n-    We assume that the wrapped RNNCell is being built within its `__call__`\n-    method. We directly use the wrapped cell's `__call__` in the overridden\n-    wrapper `__call__` method.\n-\n-    This allows to use the wrapped cell and the non-wrapped cell equivalently\n-    when using `__call__`.\n-\n-    Args:\n-      inputs: A tensor with wrapped cell's input.\n-      state: A tensor or tuple of tensors with wrapped cell's state.\n-      scope: VariableScope for the subgraph created in the wrapped cells'\n-        `__call__`.\n-\n-    Returns:\n-      A pair containing:\n-\n-      - Output: A tensor with cell's output.\n-      - New state: A tensor or tuple of tensors with new wrapped cell's state.\n-    \"\"\"\n-    return self._call_wrapped_cell(\n-        inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n-\n-  def get_config(self):\n-    config = {\n-        \"cell\": {\n-            \"class_name\": self.cell.__class__.__name__,\n-            \"config\": self.cell.get_config()\n-        },\n-    }\n-    base_config = super(_RNNCellWrapperV1, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    config = config.copy()\n-    cell = config.pop(\"cell\")\n-    try:\n-      assert_like_rnncell(\"cell\", cell)\n-      return cls(cell, **config)\n-    except TypeError:\n-      raise ValueError(\"RNNCellWrapper cannot reconstruct the wrapped cell. \"\n-                       \"Please overwrite the cell in the config with a RNNCell \"\n-                       \"instance.\")\n-\n-\n-@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DropoutWrapper\"])\n-@tf_export(v1=[\"nn.rnn_cell.DropoutWrapper\"])\n-class DropoutWrapper(rnn_cell_wrapper_impl.DropoutWrapperBase,\n-                     _RNNCellWrapperV1):\n-  \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n-\n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DropoutWrapper, self).__init__(*args, **kwargs)\n-\n-  __init__.__doc__ = rnn_cell_wrapper_impl.DropoutWrapperBase.__init__.__doc__\n-\n-\n-@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.ResidualWrapper\"])\n-@tf_export(v1=[\"nn.rnn_cell.ResidualWrapper\"])\n-class ResidualWrapper(rnn_cell_wrapper_impl.ResidualWrapperBase,\n-                      _RNNCellWrapperV1):\n-  \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n-\n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(ResidualWrapper, self).__init__(*args, **kwargs)\n-\n-  __init__.__doc__ = rnn_cell_wrapper_impl.ResidualWrapperBase.__init__.__doc__\n-\n-\n-@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DeviceWrapper\"])\n-@tf_export(v1=[\"nn.rnn_cell.DeviceWrapper\"])\n-class DeviceWrapper(rnn_cell_wrapper_impl.DeviceWrapperBase,\n-                    _RNNCellWrapperV1):\n-\n-  def __init__(self, *args, **kwargs):  # pylint: disable=useless-super-delegation\n-    super(DeviceWrapper, self).__init__(*args, **kwargs)\n-\n-  __init__.__doc__ = rnn_cell_wrapper_impl.DeviceWrapperBase.__init__.__doc__\n-\n-\n @keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.MultiRNNCell\"])\n @tf_export(v1=[\"nn.rnn_cell.MultiRNNCell\"])\n class MultiRNNCell(RNNCell):\n@@ -1343,8 +1193,7 @@ class MultiRNNCell(RNNCell):\n                 f\", but received: {state}\")\n           cur_state = state[i]\n         else:\n-          cur_state = tf.slice(state, [0, cur_state_pos],\n-                                      [-1, cell.state_size])\n+          cur_state = tf.slice(state, [0, cur_state_pos], [-1, cell.state_size])\n           cur_state_pos += cell.state_size\n         cur_inp, new_state = cell(cur_inp, cur_state)\n         new_states.append(new_state)\n\n@@ -0,0 +1,830 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Long Short-Term Memory layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import uuid\n+\n+from keras import activations\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn import lstm_v1\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.LSTMCell', v1=[])\n+class LSTMCell(lstm_v1.LSTMCell):\n+  \"\"\"Cell class for the LSTM layer.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  This class processes one step within the whole time sequence input, whereas\n+  `tf.keras.layer.LSTM` processes the whole sequence.\n+\n+  For example:\n+\n+  >>> inputs = tf.random.normal([32, 10, 8])\n+  >>> rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))\n+  >>> output = rnn(inputs)\n+  >>> print(output.shape)\n+  (32, 4)\n+  >>> rnn = tf.keras.layers.RNN(\n+  ...    tf.keras.layers.LSTMCell(4),\n+  ...    return_sequences=True,\n+  ...    return_state=True)\n+  >>> whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)\n+  >>> print(whole_seq_output.shape)\n+  (32, 10, 4)\n+  >>> print(final_memory_state.shape)\n+  (32, 4)\n+  >>> print(final_carry_state.shape)\n+  (32, 4)\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use. Default: hyperbolic tangent\n+      (`tanh`). If you pass `None`, no activation is applied (ie. \"linear\"\n+      activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs. Default: `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+      Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n+      the forget gate at initialization. Setting it to true will also force\n+      `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n+        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix. Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs. Default: 0.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state. Default: 0.\n+\n+  Call arguments:\n+    inputs: A 2D tensor, with shape of `[batch, feature]`.\n+    states: List of 2 tensors that corresponding to the cell's units. Both of\n+      them have shape `[batch, units]`, the first tensor is the memory state\n+      from previous time step, the second tensor is the carry state from\n+      previous time step. For timestep 0, the initial state provided by user\n+      will be feed to cell.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               **kwargs):\n+    super(LSTMCell, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 2),\n+        **kwargs)\n+\n+\n+@keras_export('keras.layers.LSTM', v1=[])\n+class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n+  \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  Based on available runtime hardware and constraints, this layer\n+  will choose different implementations (cuDNN-based or pure-TensorFlow)\n+  to maximize the performance. If a GPU is available and all\n+  the arguments to the layer meet the requirement of the cuDNN kernel\n+  (see below for details), the layer will use a fast cuDNN implementation.\n+\n+  The requirements to use the cuDNN implementation are:\n+\n+  1. `activation` == `tanh`\n+  2. `recurrent_activation` == `sigmoid`\n+  3. `recurrent_dropout` == 0\n+  4. `unroll` is `False`\n+  5. `use_bias` is `True`\n+  6. Inputs, if use masking, are strictly right-padded.\n+  7. Eager execution is enabled in the outermost context.\n+\n+  For example:\n+\n+  >>> inputs = tf.random.normal([32, 10, 8])\n+  >>> lstm = tf.keras.layers.LSTM(4)\n+  >>> output = lstm(inputs)\n+  >>> print(output.shape)\n+  (32, 4)\n+  >>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n+  >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n+  >>> print(whole_seq_output.shape)\n+  (32, 10, 4)\n+  >>> print(final_memory_state.shape)\n+  (32, 4)\n+  >>> print(final_carry_state.shape)\n+  (32, 4)\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation\n+      is applied (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n+      applied (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs. Default: `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+      Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n+      the forget gate at initialization. Setting it to true will also force\n+      `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n+          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its \"activation\"). Default: `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix. Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs. Default: 0.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state. Default: 0.\n+    return_sequences: Boolean. Whether to return the last output. in the output\n+      sequence, or the full sequence. Default: `False`.\n+    return_state: Boolean. Whether to return the last state in addition to the\n+      output. Default: `False`.\n+    go_backwards: Boolean (default `False`). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    stateful: Boolean (default `False`). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    time_major: The shape format of the `inputs` and `outputs` tensors.\n+      If True, the inputs and outputs will be in shape\n+      `[timesteps, batch, feature]`, whereas in the False case, it will be\n+      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n+      efficient because it avoids transposes at the beginning and end of the\n+      RNN calculation. However, most TensorFlow data is batch-major, so by\n+      default this function accepts input and emits output in batch-major\n+      form.\n+    unroll: Boolean (default `False`). If True, the network will be unrolled,\n+      else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n+      it tends to be more memory-intensive. Unrolling is only suitable for short\n+      sequences.\n+\n+  Call arguments:\n+    inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n+    mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n+      a given timestep should be masked (optional, defaults to `None`).\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or\n+      `recurrent_dropout` is used (optional, defaults to `None`).\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell (optional, defaults to `None` which causes creation\n+      of zero-filled initial state tensors).\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               time_major=False,\n+               unroll=False,\n+               **kwargs):\n+    # return_runtime is a flag for testing, which shows the real backend\n+    # implementation chosen by grappler in graph mode.\n+    self.return_runtime = kwargs.pop('return_runtime', False)\n+\n+    super(LSTM, self).__init__(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 2),\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        time_major=time_major,\n+        unroll=unroll,\n+        **kwargs)\n+\n+    self.state_spec = [\n+        InputSpec(shape=(None, dim)) for dim in (self.units, self.units)\n+    ]\n+    self._could_use_gpu_kernel = (\n+        self.activation in (activations.tanh, tf.tanh) and\n+        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and\n+        recurrent_dropout == 0 and not unroll and use_bias and\n+        tf.compat.v1.executing_eagerly_outside_functions())\n+    if tf.config.list_logical_devices('GPU'):\n+      # Only show the message when there is GPU available, user will not care\n+      # about the cuDNN if there isn't any GPU.\n+      if self._could_use_gpu_kernel:\n+        logging.debug(gru_lstm_utils.CUDNN_AVAILABLE_MSG % self.name)\n+      else:\n+        logging.warning(gru_lstm_utils.CUDNN_NOT_AVAILABLE_MSG % self.name)\n+\n+    if gru_lstm_utils.use_new_gru_lstm_impl():\n+      self._defun_wrapper = gru_lstm_utils.DefunWrapper(\n+          time_major, go_backwards, 'lstm')\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    # The input should be dense, padded with zeros. If a ragged input is fed\n+    # into the layer, it is padded and the row lengths are used for masking.\n+    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n+    is_ragged_input = (row_lengths is not None)\n+    self._validate_args_if_ragged(is_ragged_input, mask)\n+\n+    # LSTM does not support constants. Ignore it during process.\n+    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n+\n+    if isinstance(mask, list):\n+      mask = mask[0]\n+\n+    input_shape = backend.int_shape(inputs)\n+    timesteps = input_shape[0] if self.time_major else input_shape[1]\n+\n+    if not self._could_use_gpu_kernel:\n+      # Fall back to use the normal LSTM.\n+      kwargs = {'training': training}\n+      self._maybe_reset_cell_dropout_mask(self.cell)\n+\n+      def step(inputs, states):\n+        return self.cell(inputs, states, **kwargs)\n+\n+      last_output, outputs, states = backend.rnn(\n+          step,\n+          inputs,\n+          initial_state,\n+          constants=None,\n+          go_backwards=self.go_backwards,\n+          mask=mask,\n+          unroll=self.unroll,\n+          input_length=row_lengths if row_lengths is not None else timesteps,\n+          time_major=self.time_major,\n+          zero_output_for_mask=self.zero_output_for_mask)\n+      runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n+    else:\n+      # Use the new defun approach for backend implementation swap.\n+      # Note that different implementations need to have same function\n+      # signature, eg, the tensor parameters need to have same shape and dtypes.\n+      # Since the cuDNN has an extra set of bias, those bias will be passed to\n+      # both normal and cuDNN implementations.\n+      self.reset_dropout_mask()\n+      dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n+      if dropout_mask is not None:\n+        inputs = inputs * dropout_mask[0]\n+      if gru_lstm_utils.use_new_gru_lstm_impl():\n+        lstm_kwargs = {\n+            'inputs':\n+                inputs,\n+            'init_h':\n+                gru_lstm_utils.read_variable_value(initial_state[0]),\n+            'init_c':\n+                gru_lstm_utils.read_variable_value(initial_state[1]),\n+            'kernel':\n+                gru_lstm_utils.read_variable_value(self.cell.kernel),\n+            'recurrent_kernel':\n+                gru_lstm_utils.read_variable_value(self.cell.recurrent_kernel),\n+            'bias':\n+                gru_lstm_utils.read_variable_value(self.cell.bias),\n+            'mask':\n+                mask,\n+            'time_major':\n+                self.time_major,\n+            'go_backwards':\n+                self.go_backwards,\n+            'sequence_lengths':\n+                row_lengths,\n+            'zero_output_for_mask':\n+                self.zero_output_for_mask,\n+        }\n+        (last_output, outputs, new_h, new_c,\n+         runtime) = self._defun_wrapper.defun_layer(**lstm_kwargs)\n+      else:\n+        gpu_lstm_kwargs = {\n+            'inputs':\n+                inputs,\n+            'init_h':\n+                gru_lstm_utils.read_variable_value(initial_state[0]),\n+            'init_c':\n+                gru_lstm_utils.read_variable_value(initial_state[1]),\n+            'kernel':\n+                gru_lstm_utils.read_variable_value(self.cell.kernel),\n+            'recurrent_kernel':\n+                gru_lstm_utils.read_variable_value(self.cell.recurrent_kernel),\n+            'bias':\n+                gru_lstm_utils.read_variable_value(self.cell.bias),\n+            'mask':\n+                mask,\n+            'time_major':\n+                self.time_major,\n+            'go_backwards':\n+                self.go_backwards,\n+            'sequence_lengths':\n+                row_lengths\n+        }\n+        normal_lstm_kwargs = gpu_lstm_kwargs.copy()\n+        normal_lstm_kwargs.update({\n+            'zero_output_for_mask': self.zero_output_for_mask,\n+        })\n+\n+        if tf.executing_eagerly():\n+          device_type = gru_lstm_utils.get_context_device_type()\n+          can_use_gpu = (\n+              # Either user specified GPU or unspecified but GPU is available.\n+              (device_type == gru_lstm_utils.GPU_DEVICE_NAME or\n+               (device_type is None\n+                and tf.config.list_logical_devices('GPU'))) and\n+              (mask is None or\n+               gru_lstm_utils.is_cudnn_supported_inputs(mask, self.time_major)))\n+          # Under eager context, check the device placement and prefer the\n+          # GPU implementation when GPU is available.\n+          if can_use_gpu:\n+            last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n+                **gpu_lstm_kwargs)\n+          else:\n+            last_output, outputs, new_h, new_c, runtime = standard_lstm(\n+                **normal_lstm_kwargs)\n+        else:\n+          (last_output, outputs, new_h, new_c,\n+           runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\n+\n+      states = [new_h, new_c]\n+\n+    if self.stateful:\n+      updates = [\n+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))\n+          for self_state, state in zip(self.states, states)\n+      ]\n+      self.add_update(updates)\n+\n+    if self.return_sequences:\n+      output = backend.maybe_convert_to_ragged(\n+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)\n+    else:\n+      output = last_output\n+\n+    if self.return_state:\n+      return [output] + list(states)\n+    elif self.return_runtime:\n+      return output, runtime\n+    else:\n+      return output\n+\n+\n+def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n+                  mask, time_major, go_backwards, sequence_lengths,\n+                  zero_output_for_mask):\n+  \"\"\"LSTM with standard kernel implementation.\n+\n+  This implementation can be run on all types for hardware.\n+\n+  This implementation lifts out all the layer weights and make them function\n+  parameters. It has same number of tensor input params as the cuDNN\n+  counterpart. The RNN step logic has been simplified, eg dropout and mask is\n+  removed since cuDNN implementation does not support that.\n+\n+  Note that the first half of the bias tensor should be ignored by this impl.\n+  The cuDNN impl need an extra set of input gate bias. In order to make the both\n+  function take same shape of parameter, that extra set of bias is also feed\n+  here.\n+\n+  Args:\n+    inputs: input tensor of LSTM layer.\n+    init_h: initial state tensor for the cell output.\n+    init_c: initial state tensor for the cell hidden state.\n+    kernel: weights for cell kernel.\n+    recurrent_kernel: weights for cell recurrent kernel.\n+    bias: weights for cell kernel bias and recurrent bias. Only recurrent bias\n+      is used in this case.\n+    mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n+    time_major: boolean, whether the inputs are in the format of\n+      [time, batch, feature] or [batch, time, feature].\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    sequence_lengths: The lengths of all sequences coming from a variable length\n+      input, such as ragged tensors. If the input has a fixed timestep size,\n+      this should be None.\n+    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+\n+  Returns:\n+    last_output: output tensor for the last timestep, which has shape\n+      [batch, units].\n+    outputs: output tensor for all timesteps, which has shape\n+      [batch, time, units].\n+    state_0: the cell output, which has same shape as init_h.\n+    state_1: the cell hidden state, which has same shape as init_c.\n+    runtime: constant string tensor which indicate real runtime hardware. This\n+      value is for testing purpose and should be used by user.\n+  \"\"\"\n+  input_shape = backend.int_shape(inputs)\n+  timesteps = input_shape[0] if time_major else input_shape[1]\n+\n+  def step(cell_inputs, cell_states):\n+    \"\"\"Step function that will be used by Keras RNN backend.\"\"\"\n+    h_tm1 = cell_states[0]  # previous memory state\n+    c_tm1 = cell_states[1]  # previous carry state\n+\n+    z = backend.dot(cell_inputs, kernel)\n+    z += backend.dot(h_tm1, recurrent_kernel)\n+    z = backend.bias_add(z, bias)\n+\n+    z0, z1, z2, z3 = tf.split(z, 4, axis=1)\n+\n+    i = tf.sigmoid(z0)\n+    f = tf.sigmoid(z1)\n+    c = f * c_tm1 + i * tf.tanh(z2)\n+    o = tf.sigmoid(z3)\n+\n+    h = o * tf.tanh(c)\n+    return h, [h, c]\n+\n+  last_output, outputs, new_states = backend.rnn(\n+      step,\n+      inputs, [init_h, init_c],\n+      constants=None,\n+      unroll=False,\n+      time_major=time_major,\n+      mask=mask,\n+      go_backwards=go_backwards,\n+      input_length=(sequence_lengths\n+                    if sequence_lengths is not None else timesteps),\n+      zero_output_for_mask=zero_output_for_mask)\n+  return (last_output, outputs, new_states[0], new_states[1],\n+          gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_CPU))\n+\n+\n+def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n+             time_major, go_backwards, sequence_lengths):\n+  \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for GPU.\n+\n+  Note that currently only right padded data is supported, or the result will be\n+  polluted by the unmasked data which should be filtered.\n+\n+  Args:\n+    inputs: Input tensor of LSTM layer.\n+    init_h: Initial state tensor for the cell output.\n+    init_c: Initial state tensor for the cell hidden state.\n+    kernel: Weights for cell kernel.\n+    recurrent_kernel: Weights for cell recurrent kernel.\n+    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n+      is used in this case.\n+    mask: Boolean tensor for mask out the steps within sequence. An individual\n+      `True` entry indicates that the corresponding timestep should be utilized,\n+      while a `False` entry indicates that the corresponding timestep should be\n+      ignored.\n+    time_major: Boolean, whether the inputs are in the format of [time, batch,\n+      feature] or [batch, time, feature].\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    sequence_lengths: The lengths of all sequences coming from a variable length\n+      input, such as ragged tensors. If the input has a fixed timestep size,\n+      this should be None.\n+\n+  Returns:\n+    last_output: Output tensor for the last timestep, which has shape\n+      [batch, units].\n+    outputs: Output tensor for all timesteps, which has shape\n+      [batch, time, units].\n+    state_0: The cell output, which has same shape as init_h.\n+    state_1: The cell hidden state, which has same shape as init_c.\n+    runtime: Constant string tensor which indicate real runtime hardware. This\n+      value is for testing purpose and should not be used by user.\n+  \"\"\"\n+  if mask is not None:\n+    sequence_lengths = gru_lstm_utils.calculate_sequence_by_mask(\n+        mask, time_major)\n+\n+  if not time_major and sequence_lengths is None:\n+    inputs = tf.transpose(inputs, perm=(1, 0, 2))\n+    seq_axis, batch_axis = (0, 1)\n+  else:\n+    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n+  # For init_h and init_c, cuDNN expects one more dim of num_layers before or\n+  # after batch dim for time major or batch major inputs respectively\n+  init_h = tf.expand_dims(init_h, axis=seq_axis)\n+  init_c = tf.expand_dims(init_c, axis=seq_axis)\n+\n+  weights = tf.split(kernel, 4, axis=1)\n+  weights += tf.split(recurrent_kernel, 4, axis=1)\n+  # cuDNN has an extra set of bias for inputs, we disable them (setting to 0),\n+  # so that mathematically it is same as the canonical LSTM implementation.\n+  full_bias = tf.concat((tf.zeros_like(bias), bias), 0)\n+\n+  if tf.sysconfig.get_build_info()['is_rocm_build']:\n+    # ROCm MIOpen's weight sequence for LSTM is different from both canonical\n+    # and Cudnn format\n+    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]\n+    # i is input gate weights.\n+    # f is forget gate weights.\n+    # o is output gate weights.\n+    # c is cell gate weights.\n+    weights = [weights[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]\n+    # full_bias is a tensor of shape (8*n,)\n+    full_bias = tf.split(full_bias, 8, axis=0)\n+    full_bias = [full_bias[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]\n+\n+  params = gru_lstm_utils.canonical_to_params(\n+      weights=weights,\n+      biases=tf.split(full_bias, 8),\n+      shape=tf.constant([-1]),\n+      transpose_weights=True)\n+\n+  if sequence_lengths is not None:\n+    if go_backwards:\n+      # Three reversals are required. E.g.,\n+      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n+      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]\n+      # output_from_cudnn = [6, 5, 4, 0, 0]\n+      # expected_output = [0, 0, 6, 5 ,4]\n+      inputs = tf.reverse_sequence(\n+          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n+    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV3(\n+        input=inputs,\n+        input_h=init_h,\n+        input_c=init_c,\n+        params=params,\n+        is_training=True,\n+        rnn_mode='lstm',\n+        sequence_lengths=sequence_lengths,\n+        time_major=time_major)\n+    if go_backwards:\n+      outputs = tf.reverse_sequence(\n+          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)\n+      outputs = tf.reverse(outputs, axis=[seq_axis])\n+  else:\n+    # # Fill the array with shape [batch] with value of max timesteps.\n+    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],\n+    #                                  array_ops.shape(inputs)[0])\n+    if go_backwards:\n+      # Reverse axis 0 since the input is already convert to time major.\n+      inputs = tf.reverse(inputs, axis=[0])\n+    outputs, h, c, _ = tf.raw_ops.CudnnRNN(\n+        input=inputs, input_h=init_h, input_c=init_c, params=params,\n+        is_training=True, rnn_mode='lstm')\n+\n+  last_output = outputs[-1]\n+  if not time_major and sequence_lengths is None:\n+    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n+  h = tf.squeeze(h, axis=seq_axis)\n+  c = tf.squeeze(c, axis=seq_axis)\n+\n+  # In the case of variable length input, the cudnn kernel will fill zeros for\n+  # the output, whereas the default keras behavior is to bring over the previous\n+  # output for t-1, so that in the return_sequence=False case, user can quickly\n+  # get the final effect output instead just 0s at the last timestep.\n+  # In order to mimic the default keras behavior, we copy the final h state as\n+  # the last_output, since it is numerically same as the output.\n+  if sequence_lengths is not None:\n+    last_output = h\n+  return last_output, outputs, h, c, gru_lstm_utils.runtime(\n+      gru_lstm_utils.RUNTIME_GPU)\n+\n+\n+def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n+                                recurrent_kernel, bias, mask, time_major,\n+                                go_backwards, sequence_lengths,\n+                                zero_output_for_mask):\n+  \"\"\"Call the LSTM with optimized backend kernel selection.\n+\n+  Under the hood, this function will create two TF function, one with the most\n+  generic kernel and can run on all device condition, and the second one with\n+  cuDNN specific kernel, which can only run on GPU.\n+\n+  The first function will be called with normal_lstm_params, while the second\n+  function is not called, but only registered in the graph. The Grappler will\n+  do the proper graph rewrite and swap the optimized TF function based on the\n+  device placement.\n+\n+  Args:\n+    inputs: Input tensor of LSTM layer.\n+    init_h: Initial state tensor for the cell output.\n+    init_c: Initial state tensor for the cell hidden state.\n+    kernel: Weights for cell kernel.\n+    recurrent_kernel: Weights for cell recurrent kernel.\n+    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n+      is used in this case.\n+    mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n+    time_major: Boolean, whether the inputs are in the format of\n+      [time, batch, feature] or [batch, time, feature].\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    sequence_lengths: The lengths of all sequences coming from a variable length\n+      input, such as ragged tensors. If the input has a fixed timestep size,\n+      this should be None.\n+    zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+\n+  Returns:\n+    List of output tensors, same as standard_lstm.\n+  \"\"\"\n+  params = {\n+      'inputs': inputs,\n+      'init_h': init_h,\n+      'init_c': init_c,\n+      'kernel': kernel,\n+      'recurrent_kernel': recurrent_kernel,\n+      'bias': bias,\n+      'mask': mask,\n+      'time_major': time_major,\n+      'go_backwards': go_backwards,\n+      'sequence_lengths': sequence_lengths,\n+      'zero_output_for_mask': zero_output_for_mask,\n+  }\n+\n+  def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,\n+                             bias, mask, time_major, go_backwards,\n+                             sequence_lengths, zero_output_for_mask):\n+    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n+    if mask is None:\n+      return gpu_lstm(\n+          inputs=inputs,\n+          init_h=init_h,\n+          init_c=init_c,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths)\n+\n+    def cudnn_lstm_fn():\n+      return gpu_lstm(\n+          inputs=inputs,\n+          init_h=init_h,\n+          init_c=init_c,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths)\n+\n+    def stardard_lstm_fn():\n+      return standard_lstm(\n+          inputs=inputs,\n+          init_h=init_h,\n+          init_c=init_c,\n+          kernel=kernel,\n+          recurrent_kernel=recurrent_kernel,\n+          bias=bias,\n+          mask=mask,\n+          time_major=time_major,\n+          go_backwards=go_backwards,\n+          sequence_lengths=sequence_lengths,\n+          zero_output_for_mask=zero_output_for_mask)\n+\n+    return tf.cond(\n+        gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n+        true_fn=cudnn_lstm_fn,\n+        false_fn=stardard_lstm_fn)\n+\n+  if gru_lstm_utils.use_new_gru_lstm_impl():\n+    # Chooses the implementation dynamically based on the running device.\n+    (last_output, outputs, new_h, new_c,\n+     runtime) = tf.__internal__.execute_fn_for_device(\n+         {\n+             gru_lstm_utils.CPU_DEVICE_NAME:\n+                 lambda: standard_lstm(**params),\n+             gru_lstm_utils.GPU_DEVICE_NAME:\n+                 lambda: gpu_lstm_with_fallback(**params)\n+         }, lambda: standard_lstm(**params))\n+  else:\n+    # Each time a `tf.function` is called, we will give it a unique\n+    # identifiable API name, so that Grappler won't get confused when it\n+    # sees multiple LSTM layers added into same graph, and it will be able\n+    # to pair up the different implementations across them.\n+    api_name = 'lstm_' + str(uuid.uuid4())\n+    supportive_attribute = {\n+        'time_major': time_major,\n+        'go_backwards': go_backwards,\n+    }\n+    defun_standard_lstm = gru_lstm_utils.generate_defun_backend(\n+        api_name, gru_lstm_utils.CPU_DEVICE_NAME, standard_lstm,\n+        supportive_attribute)\n+    defun_gpu_lstm = gru_lstm_utils.generate_defun_backend(\n+        api_name, gru_lstm_utils.GPU_DEVICE_NAME, gpu_lstm_with_fallback,\n+        supportive_attribute)\n+\n+    # Call the normal LSTM impl and register the cuDNN impl function. The\n+    # grappler will kick in during session execution to optimize the graph.\n+    last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)\n+    gru_lstm_utils.function_register(defun_gpu_lstm, **params)\n+\n+  return last_output, outputs, new_h, new_c, runtime\n\n@@ -12,9 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for V2 LSTM layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for LSTM layer.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n \n import copy\n import os\n@@ -22,15 +21,18 @@ import shutil\n import time\n \n from absl.testing import parameterized\n-import numpy as np\n-from tensorflow.core.protobuf import rewriter_config_pb2\n import keras\n-from tensorflow.python.framework import test_util as tf_test_util\n+from keras.layers.rnn import gru_lstm_utils\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.layers import recurrent as rnn_v1\n-from keras.layers import recurrent_v2 as rnn\n from keras.utils import np_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.core.protobuf import rewriter_config_pb2\n+from tensorflow.python.framework import test_util as tf_test_util\n from tensorflow.python.platform import tf_logging as logging\n \n \n@@ -54,7 +56,7 @@ class LSTMV2Test(test_combinations.TestCase):\n   )\n   def test_could_use_defun_backend(self, activation, recurrent_activation,\n                                    recurrent_dropout, unroll, use_bias):\n-    layer = rnn.LSTM(\n+    layer = lstm.LSTM(\n         1,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n@@ -65,10 +67,10 @@ class LSTMV2Test(test_combinations.TestCase):\n \n   @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n-    layer = rnn.LSTM(1, activation=tf.tanh)\n+    layer = lstm.LSTM(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n-    layer = rnn.LSTM(1, recurrent_activation=tf.sigmoid)\n+    layer = lstm.LSTM(1, recurrent_activation=tf.sigmoid)\n     self.assertTrue(layer._could_use_gpu_kernel)\n \n   def test_static_shape_inference_LSTM(self):\n@@ -81,7 +83,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Dense(\n         embedding_dim, input_shape=(timesteps, embedding_dim))\n     model.add(inputs)\n-    layer = rnn.LSTM(units, return_sequences=True)\n+    layer = lstm.LSTM(units, return_sequences=True)\n     model.add(layer)\n     outputs = model.layers[-1].output\n     self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])\n@@ -91,7 +93,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer = rnn.LSTM(units, input_shape=(None, embedding_dim))\n+    layer = lstm.LSTM(units, input_shape=(None, embedding_dim))\n     model = keras.models.Sequential()\n     model.add(layer)\n     model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')\n@@ -104,15 +106,15 @@ class LSTMV2Test(test_combinations.TestCase):\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n-    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))\n-    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))\n+    model.add(lstm.LSTM(10, return_sequences=True, unroll=False))\n+    model.add(lstm.LSTM(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   def test_from_config_LSTM(self):\n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n     for stateful in (False, True):\n       l1 = layer_class(units=1, stateful=stateful)\n       l2 = layer_class.from_config(l1.get_config())\n@@ -128,7 +130,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     # Test with Keras tensor\n     inputs = keras.Input((timesteps, embedding_dim))\n     initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    layer = rnn.LSTM(units)\n+    layer = lstm.LSTM(units)\n     if len(initial_state) == 1:\n       output = layer(inputs, initial_state=initial_state[0])\n     else:\n@@ -162,7 +164,7 @@ class LSTMV2Test(test_combinations.TestCase):\n         keras.backend.random_normal_variable((num_samples, units), 0, 1)\n         for _ in range(num_states)\n     ]\n-    layer = rnn.LSTM(units)\n+    layer = lstm.LSTM(units)\n     output = layer(inputs, initial_state=initial_state)\n \n     model = keras.models.Model(inputs, output)\n@@ -181,7 +183,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     units = 3\n     num_samples = 2\n \n-    layer = rnn.LSTM(units, stateful=True)\n+    layer = lstm.LSTM(units, stateful=True)\n     layer.build((num_samples, timesteps, embedding_dim))\n     initial_weight_count = len(layer.weights)\n     layer.reset_states()\n@@ -221,7 +223,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.Input((timesteps, embedding_dim))\n     _ = keras.layers.Masking()(inputs)\n     initial_state = [keras.Input((units,)) for _ in range(num_states)]\n-    output = rnn.LSTM(units)(\n+    output = lstm.LSTM(units)(\n         inputs, initial_state=initial_state)\n \n     model = keras.models.Model([inputs] + initial_state, output)\n@@ -248,7 +250,7 @@ class LSTMV2Test(test_combinations.TestCase):\n \n     inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n     masked = keras.layers.Masking()(inputs)\n-    layer = rnn.LSTM(units, return_state=True, stateful=True)\n+    layer = lstm.LSTM(units, return_state=True, stateful=True)\n     outputs = layer(masked)\n     state = outputs[1:]\n     assert len(state) == num_states\n@@ -265,11 +267,11 @@ class LSTMV2Test(test_combinations.TestCase):\n     num_samples = 2\n \n     inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = rnn.LSTM(\n+    layer = lstm.LSTM(\n         units, return_state=True, return_sequences=True)\n     outputs = layer(inputs)\n     output, state = outputs[0], outputs[1:]\n-    output = rnn.LSTM(units)(output, initial_state=state)\n+    output = lstm.LSTM(units)(output, initial_state=state)\n     model = keras.models.Model(inputs, output)\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n@@ -281,7 +283,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     units = 3\n     num_samples = 2\n     num_states = 2\n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n \n     # Test with Keras tensor\n     main_inputs = keras.Input((timesteps, embedding_dim))\n@@ -331,8 +333,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n     masked_input = keras.layers.Masking()(inputs)\n-    lstm_layer = rnn_v1.LSTM(rnn_state_size,\n-                             recurrent_activation='sigmoid')\n+    lstm_layer = lstm_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n     output = lstm_layer(masked_input)\n     lstm_model = keras.models.Model(inputs, output)\n     weights = lstm_model.get_weights()\n@@ -342,7 +343,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     y_2 = lstm_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      cudnn_layer = rnn.LSTM(rnn_state_size)\n+      cudnn_layer = lstm.LSTM(rnn_state_size)\n       cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n     cudnn_model.set_weights(weights)\n     y_3 = cudnn_model.predict(x_train)\n@@ -363,14 +364,14 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.LSTM,\n+        lstm.LSTM,\n         kwargs={\n             'units': units,\n             'implementation': implementation_mode\n         },\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n     k_constraint = keras.constraints.max_norm(0.01)\n     r_constraint = keras.constraints.max_norm(0.01)\n     b_constraint = keras.constraints.max_norm(0.01)\n@@ -387,7 +388,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)\n     self.assertEqual(layer.cell.bias.constraint, b_constraint)\n \n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -408,8 +409,8 @@ class LSTMV2Test(test_combinations.TestCase):\n     targets /= targets.sum(axis=-1, keepdims=True)\n     model = keras.models.Sequential()\n     model.add(keras.layers.Masking(input_shape=(3, 4)))\n-    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))\n-    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))\n+    model.add(lstm.LSTM(10, return_sequences=True, unroll=False))\n+    model.add(lstm.LSTM(5, return_sequences=True, unroll=False))\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n@@ -448,11 +449,11 @@ class LSTMV2Test(test_combinations.TestCase):\n         outputs = layer(inputs)\n       return keras.models.Model(inputs, outputs)\n \n-    lstm_model = build_model(rnn_v1.LSTM)\n+    lstm_model = build_model(lstm_v1.LSTM)\n     y_ref = lstm_model.predict(x_train)\n     weights = lstm_model.get_weights()\n \n-    lstm_v2_model = build_model(rnn.LSTM)\n+    lstm_v2_model = build_model(lstm.LSTM)\n     lstm_v2_model.set_weights(weights)\n     y = lstm_v2_model.predict(x_train)\n \n@@ -472,7 +473,7 @@ class LSTMV2Test(test_combinations.TestCase):\n         num_classes=output_shape)\n     y_train = np_utils.to_categorical(y_train, output_shape)\n \n-    layer = rnn.LSTM(rnn_state_size)\n+    layer = lstm.LSTM(rnn_state_size)\n \n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n@@ -505,7 +506,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     def build_model():\n       inputs = keras.layers.Input(\n           shape=[timestep, input_dim], dtype=tf.float32)\n-      layer = rnn.LSTM(\n+      layer = lstm.LSTM(\n           units,\n           use_bias=use_bias,\n           bias_initializer=bias_initializer)\n@@ -534,14 +535,14 @@ class LSTMV2Test(test_combinations.TestCase):\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n     with test_utils.device(should_use_gpu=False):\n-      layer = rnn.LSTM(rnn_state_size)\n+      layer = lstm.LSTM(rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n     y_1 = cpu_model.predict(x_train)\n \n     with test_utils.device(should_use_gpu=True):\n-      layer = rnn.LSTM(rnn_state_size)\n+      layer = lstm.LSTM(rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n       gpu_model.set_weights(weights)\n@@ -551,7 +552,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve\n     # the same output.\n     with test_utils.device(should_use_gpu=True):\n-      layer = rnn_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n+      layer = lstm_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n       output = layer(inputs)\n       canonical_model = keras.models.Model(inputs, output)\n       # Remove the extra cudnn bias since canonical lstm will not use it.\n@@ -567,7 +568,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.LSTM,\n+        lstm.LSTM,\n         kwargs={\n             'units': units,\n             'return_sequences': True\n@@ -584,7 +585,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.LSTM,\n+        lstm.LSTM,\n         kwargs={\n             'units': units,\n             'return_sequences': True,\n@@ -595,7 +596,7 @@ class LSTMV2Test(test_combinations.TestCase):\n \n   def test_regularizers_LSTM(self):\n     embedding_dim = 4\n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n     layer = layer_class(\n         5,\n         return_sequences=False,\n@@ -622,7 +623,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    layer_class = rnn.LSTM\n+    layer_class = lstm.LSTM\n     model = keras.models.Sequential()\n     model.add(\n         keras.layers.Embedding(\n@@ -703,7 +704,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, embedding_dim,\n                                batch_input_shape=[batch_size, timestep]),\n-        rnn.LSTM(units, return_sequences=True, stateful=True),\n+        lstm.LSTM(units, return_sequences=True, stateful=True),\n         keras.layers.Dense(vocab_size)\n     ])\n     model.compile(\n@@ -718,7 +719,7 @@ class LSTMV2Test(test_combinations.TestCase):\n     embedding_dim = 4\n     units = 2\n     test_utils.layer_test(\n-        rnn.LSTM,\n+        lstm.LSTM,\n         kwargs={\n             'units': units,\n             'dropout': 0.1,\n@@ -732,9 +733,9 @@ class LSTMV2Test(test_combinations.TestCase):\n     vocab_size = 1000\n     model = keras.Sequential([\n         keras.layers.Embedding(vocab_size, 64),\n-        keras.layers.Bidirectional(rnn.LSTM(\n+        keras.layers.Bidirectional(lstm.LSTM(\n             64, return_sequences=True)),\n-        keras.layers.Bidirectional(rnn.LSTM(32)),\n+        keras.layers.Bidirectional(lstm.LSTM(32)),\n         keras.layers.Dense(64, activation='relu'),\n         keras.layers.Dense(1, activation='sigmoid')\n     ])\n@@ -764,17 +765,18 @@ class LSTMV2Test(test_combinations.TestCase):\n     mask[:, masksteps:] = 0\n \n     # Test for V1 behavior.\n-    lstm_v1 = rnn_v1.LSTM(units, return_sequences=True, go_backwards=True)\n+    lstm_v1_layer = lstm_v1.LSTM(\n+        units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n-      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))\n-      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])\n+      outputs_masked_v1 = lstm_v1_layer(inputs, mask=tf.constant(mask))\n+      outputs_trimmed_v1 = lstm_v1_layer(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n \n     # Test for V2 behavior.\n-    lstm = rnn.LSTM(units, return_sequences=True, go_backwards=True)\n+    lstm_layer = lstm.LSTM(units, return_sequences=True, go_backwards=True)\n     with test_utils.device(should_use_gpu=True):\n-      outputs_masked = lstm(inputs, mask=tf.constant(mask))\n-      outputs_trimmed = lstm(inputs[:, :masksteps])\n+      outputs_masked = lstm_layer(inputs, mask=tf.constant(mask))\n+      outputs_trimmed = lstm_layer(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)\n \n   @tf_test_util.enable_output_all_intermediates\n@@ -787,7 +789,7 @@ class LSTMV2Test(test_combinations.TestCase):\n           (x, y)).shuffle(100).batch(32)\n \n       inp = keras.layers.Input(shape=(4, 8))\n-      layer = rnn.LSTM(1)(inp)\n+      layer = lstm.LSTM(1)(inp)\n       layer = keras.layers.Dense(1)(layer)\n \n       model = keras.models.Model(inp, layer)\n@@ -814,7 +816,7 @@ class LSTMV2Test(test_combinations.TestCase):\n             mask_zero=True,\n             input_length=timestep,\n             batch_input_shape=(num_samples, timestep)))\n-    layer = rnn.LSTM(units)\n+    layer = lstm.LSTM(units)\n     model.add(layer)\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n@@ -827,14 +829,14 @@ class LSTMV2Test(test_combinations.TestCase):\n   def test_deepcopy(self):\n     if not tf.executing_eagerly():\n       self.skipTest('v2-only test')\n-    original_layer = rnn.LSTM(5)\n+    original_layer = lstm.LSTM(5)\n     copied_layer = copy.deepcopy(original_layer)\n     self.assertEqual(copied_layer.units, 5)\n     self.assertEqual(original_layer.get_config(), original_layer.get_config())\n \n     # Copy layer before layer call on inputs without weight initialization.\n     inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)\n-    original_layer = rnn.LSTM(4)\n+    original_layer = lstm.LSTM(4)\n     copied_layer = copy.deepcopy(original_layer)\n     outputs = original_layer(inputs)\n     copied_outputs = copied_layer(inputs)\n@@ -842,7 +844,7 @@ class LSTMV2Test(test_combinations.TestCase):\n         self.evaluate(outputs), self.evaluate(copied_outputs))\n \n     # Copy layer after layer call on inputs with weight initialization.\n-    original_layer = rnn.LSTM(4)\n+    original_layer = lstm.LSTM(4)\n     outputs = original_layer(inputs)\n     copied_layer = copy.deepcopy(original_layer)\n     copied_outputs = copied_layer(inputs)\n@@ -883,13 +885,13 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n   @test_utils.run_v2_only\n   def test_LSTM_runtime(self):\n-    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -910,7 +912,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n   def test_LSTM_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n-    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n@@ -941,38 +943,38 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n     # Verify unpadded data.\n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n     # Update x/y to be right padded by setting the last timestep to 0\n     x_train[:, -1, :] = 0\n     y_train[:, -1] = 0\n     _, runtime_value = model.predict(x_train)\n     if tf.test.is_gpu_available():\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n     else:\n-      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+      self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n     # Further update x/y to be mix padded (masks in the middle), and verify\n     # only cpu kernel can be selected.\n     x_train[:, -3, :] = 0\n     y_train[:, -3] = 0\n     _, runtime_value = model.predict(x_train)\n-    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n+    self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_CPU)\n \n   @test_utils.run_v2_only\n   def test_LSTM_runtime_with_cond(self):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n     # states.\n-    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)\n+    layer = lstm.LSTM(self.rnn_state_size, return_runtime=True)\n \n     inputs = keras.layers.Input(\n         shape=[self.timestep, self.input_shape], dtype=tf.float32)\n \n     zeros = tf.zeros([self.batch, self.output_shape])\n-    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)\n+    dummy_runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n     a = tf.constant(0)\n     b = tf.constant(1)\n     # Will always run the lstm layer.\n@@ -1031,7 +1033,7 @@ class LSTMPerformanceTest(tf.test.Benchmark):\n     rnn_state_size = test_config['rnn_state_size']\n     timestep = test_config['timestep']\n \n-    layer = rnn.LSTM(rnn_state_size)\n+    layer = lstm.LSTM(rnn_state_size)\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n \n@@ -1052,7 +1054,7 @@ class LSTMPerformanceTest(tf.test.Benchmark):\n     rnn_state_size = test_config['rnn_state_size']\n     timestep = test_config['timestep']\n \n-    layer = rnn_v1.LSTM(rnn_state_size)\n+    layer = lstm_v1.LSTM(rnn_state_size)\n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n \n\n@@ -0,0 +1,714 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Long Short-Term Memory V1 layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import warnings\n+\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+RECURRENT_DROPOUT_WARNING_MSG = (\n+    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\n+    'Using `implementation=1`.')\n+\n+\n+@keras_export(v1=['keras.layers.LSTMCell'])\n+class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+  \"\"\"Cell class for the LSTM layer.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+      Default: hard sigmoid (`hard_sigmoid`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix,\n+      used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean.\n+      If True, add 1 to the bias of the forget gate at initialization.\n+      Setting it to true will also force `bias_initializer=\"zeros\"`.\n+      This is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    recurrent_constraint: Constraint function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+\n+  Call arguments:\n+    inputs: A 2D tensor.\n+    states: List of state tensors corresponding to the previous timestep.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n+    # By default use cached variable under v2 mode, see b/143699808.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n+    else:\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n+    super(LSTMCell, self).__init__(**kwargs)\n+    self.units = units\n+    self.activation = activations.get(activation)\n+    self.recurrent_activation = activations.get(recurrent_activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.unit_forget_bias = unit_forget_bias\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1., max(0., dropout))\n+    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+    implementation = kwargs.pop('implementation', 1)\n+    if self.recurrent_dropout != 0 and implementation != 1:\n+      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n+      self.implementation = 1\n+    else:\n+      self.implementation = implementation\n+    self.state_size = [self.units, self.units]\n+    self.output_size = self.units\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    default_caching_device = rnn_utils.caching_device(self)\n+    input_dim = input_shape[-1]\n+    self.kernel = self.add_weight(\n+        shape=(input_dim, self.units * 4),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        caching_device=default_caching_device)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units * 4),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint,\n+        caching_device=default_caching_device)\n+\n+    if self.use_bias:\n+      if self.unit_forget_bias:\n+\n+        def bias_initializer(_, *args, **kwargs):\n+          return backend.concatenate([\n+              self.bias_initializer((self.units,), *args, **kwargs),\n+              initializers.get('ones')((self.units,), *args, **kwargs),\n+              self.bias_initializer((self.units * 2,), *args, **kwargs),\n+          ])\n+      else:\n+        bias_initializer = self.bias_initializer\n+      self.bias = self.add_weight(\n+          shape=(self.units * 4,),\n+          name='bias',\n+          initializer=bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          caching_device=default_caching_device)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n+    \"\"\"Computes carry and output using split kernels.\"\"\"\n+    x_i, x_f, x_c, x_o = x\n+    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n+    i = self.recurrent_activation(\n+        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n+    f = self.recurrent_activation(x_f + backend.dot(\n+        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n+    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n+        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n+    o = self.recurrent_activation(\n+        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n+    return c, o\n+\n+  def _compute_carry_and_output_fused(self, z, c_tm1):\n+    \"\"\"Computes carry and output using fused kernels.\"\"\"\n+    z0, z1, z2, z3 = z\n+    i = self.recurrent_activation(z0)\n+    f = self.recurrent_activation(z1)\n+    c = f * c_tm1 + i * self.activation(z2)\n+    o = self.recurrent_activation(z3)\n+    return c, o\n+\n+  def call(self, inputs, states, training=None):\n+    h_tm1 = states[0]  # previous memory state\n+    c_tm1 = states[1]  # previous carry state\n+\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        h_tm1, training, count=4)\n+\n+    if self.implementation == 1:\n+      if 0 < self.dropout < 1.:\n+        inputs_i = inputs * dp_mask[0]\n+        inputs_f = inputs * dp_mask[1]\n+        inputs_c = inputs * dp_mask[2]\n+        inputs_o = inputs * dp_mask[3]\n+      else:\n+        inputs_i = inputs\n+        inputs_f = inputs\n+        inputs_c = inputs\n+        inputs_o = inputs\n+      k_i, k_f, k_c, k_o = tf.split(\n+          self.kernel, num_or_size_splits=4, axis=1)\n+      x_i = backend.dot(inputs_i, k_i)\n+      x_f = backend.dot(inputs_f, k_f)\n+      x_c = backend.dot(inputs_c, k_c)\n+      x_o = backend.dot(inputs_o, k_o)\n+      if self.use_bias:\n+        b_i, b_f, b_c, b_o = tf.split(\n+            self.bias, num_or_size_splits=4, axis=0)\n+        x_i = backend.bias_add(x_i, b_i)\n+        x_f = backend.bias_add(x_f, b_f)\n+        x_c = backend.bias_add(x_c, b_c)\n+        x_o = backend.bias_add(x_o, b_o)\n+\n+      if 0 < self.recurrent_dropout < 1.:\n+        h_tm1_i = h_tm1 * rec_dp_mask[0]\n+        h_tm1_f = h_tm1 * rec_dp_mask[1]\n+        h_tm1_c = h_tm1 * rec_dp_mask[2]\n+        h_tm1_o = h_tm1 * rec_dp_mask[3]\n+      else:\n+        h_tm1_i = h_tm1\n+        h_tm1_f = h_tm1\n+        h_tm1_c = h_tm1\n+        h_tm1_o = h_tm1\n+      x = (x_i, x_f, x_c, x_o)\n+      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n+      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n+    else:\n+      if 0. < self.dropout < 1.:\n+        inputs = inputs * dp_mask[0]\n+      z = backend.dot(inputs, self.kernel)\n+      z += backend.dot(h_tm1, self.recurrent_kernel)\n+      if self.use_bias:\n+        z = backend.bias_add(z, self.bias)\n+\n+      z = tf.split(z, num_or_size_splits=4, axis=1)\n+      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n+\n+    h = o * self.activation(c)\n+    return h, [h, c]\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self))\n+    base_config = super(LSTMCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return list(rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype))\n+\n+\n+@keras_export('keras.experimental.PeepholeLSTMCell')\n+class PeepholeLSTMCell(LSTMCell):\n+  \"\"\"Equivalent to LSTMCell class but adds peephole connections.\n+\n+  Peephole connections allow the gates to utilize the previous internal state as\n+  well as the previous hidden state (which is what LSTMCell is limited to).\n+  This allows PeepholeLSTMCell to better learn precise timings over LSTMCell.\n+\n+  From [Gers et al., 2002](\n+    http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf):\n+\n+  \"We find that LSTM augmented by 'peephole connections' from its internal\n+  cells to its multiplicative gates can learn the fine distinction between\n+  sequences of spikes spaced either 50 or 49 time steps apart without the help\n+  of any short training exemplars.\"\n+\n+  The peephole implementation is based on:\n+\n+  [Sak et al., 2014](https://research.google.com/pubs/archive/43905.pdf)\n+\n+  Example:\n+\n+  ```python\n+  # Create 2 PeepholeLSTMCells\n+  peephole_lstm_cells = [PeepholeLSTMCell(size) for size in [128, 256]]\n+  # Create a layer composed sequentially of the peephole LSTM cells.\n+  layer = RNN(peephole_lstm_cells)\n+  input = keras.Input((timesteps, input_dim))\n+  output = layer(input)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               **kwargs):\n+    warnings.warn(\n+        '`tf.keras.experimental.PeepholeLSTMCell` is deprecated '\n+        'and will be removed in a future version. '\n+        'Please use tensorflow_addons.rnn.PeepholeLSTMCell '\n+        'instead.',\n+        stacklevel=2)\n+    super(PeepholeLSTMCell, self).__init__(\n+        units=units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=kwargs.pop('implementation', 1),\n+        **kwargs)\n+\n+  def build(self, input_shape):\n+    super(PeepholeLSTMCell, self).build(input_shape)\n+    # The following are the weight matrices for the peephole connections. These\n+    # are multiplied with the previous internal state during the computation of\n+    # carry and output.\n+    self.input_gate_peephole_weights = self.add_weight(\n+        shape=(self.units,),\n+        name='input_gate_peephole_weights',\n+        initializer=self.kernel_initializer)\n+    self.forget_gate_peephole_weights = self.add_weight(\n+        shape=(self.units,),\n+        name='forget_gate_peephole_weights',\n+        initializer=self.kernel_initializer)\n+    self.output_gate_peephole_weights = self.add_weight(\n+        shape=(self.units,),\n+        name='output_gate_peephole_weights',\n+        initializer=self.kernel_initializer)\n+\n+  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n+    x_i, x_f, x_c, x_o = x\n+    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n+    i = self.recurrent_activation(\n+        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]) +\n+        self.input_gate_peephole_weights * c_tm1)\n+    f = self.recurrent_activation(x_f + backend.dot(\n+        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]) +\n+                                  self.forget_gate_peephole_weights * c_tm1)\n+    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n+        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n+    o = self.recurrent_activation(\n+        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]) +\n+        self.output_gate_peephole_weights * c)\n+    return c, o\n+\n+  def _compute_carry_and_output_fused(self, z, c_tm1):\n+    z0, z1, z2, z3 = z\n+    i = self.recurrent_activation(z0 +\n+                                  self.input_gate_peephole_weights * c_tm1)\n+    f = self.recurrent_activation(z1 +\n+                                  self.forget_gate_peephole_weights * c_tm1)\n+    c = f * c_tm1 + i * self.activation(z2)\n+    o = self.recurrent_activation(z3 + self.output_gate_peephole_weights * c)\n+    return c, o\n+\n+\n+@keras_export(v1=['keras.layers.LSTM'])\n+class LSTM(RNN):\n+  \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n+\n+   Note that this cell is not optimized for performance on GPU. Please use\n+  `tf.compat.v1.keras.layers.CuDNNLSTM` for better performance on GPU.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    recurrent_activation: Activation function to use\n+      for the recurrent step.\n+      Default: hard sigmoid (`hard_sigmoid`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs..\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix,\n+      used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean.\n+      If True, add 1 to the bias of the forget gate at initialization.\n+      Setting it to true will also force `bias_initializer=\"zeros\"`.\n+      This is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix.\n+    recurrent_regularizer: Regularizer function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\").\n+    kernel_constraint: Constraint function applied to\n+      the `kernel` weights matrix.\n+    recurrent_constraint: Constraint function applied to\n+      the `recurrent_kernel` weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+    return_sequences: Boolean. Whether to return the last output\n+      in the output sequence, or the full sequence.\n+    return_state: Boolean. Whether to return the last state\n+      in addition to the output.\n+    go_backwards: Boolean (default False).\n+      If True, process the input sequence backwards and return the\n+      reversed sequence.\n+    stateful: Boolean (default False). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    unroll: Boolean (default False).\n+      If True, the network will be unrolled,\n+      else a symbolic loop will be used.\n+      Unrolling can speed-up a RNN,\n+      although it tends to be more memory-intensive.\n+      Unrolling is only suitable for short sequences.\n+    time_major: The shape format of the `inputs` and `outputs` tensors.\n+      If True, the inputs and outputs will be in shape\n+      `(timesteps, batch, ...)`, whereas in the False case, it will be\n+      `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n+      efficient because it avoids transposes at the beginning and end of the\n+      RNN calculation. However, most TensorFlow data is batch-major, so by\n+      default this function accepts input and emits output in batch-major\n+      form.\n+\n+  Call arguments:\n+    inputs: A 3D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or\n+      `recurrent_dropout` is used.\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell.\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               **kwargs):\n+    implementation = kwargs.pop('implementation', 1)\n+    if implementation == 0:\n+      logging.warning('`implementation=0` has been deprecated, '\n+                      'and now defaults to `implementation=1`.'\n+                      'Please update your layer call.')\n+    if 'enable_caching_device' in kwargs:\n+      cell_kwargs = {'enable_caching_device':\n+                     kwargs.pop('enable_caching_device')}\n+    else:\n+      cell_kwargs = {}\n+    cell = LSTMCell(\n+        units,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        implementation=implementation,\n+        dtype=kwargs.get('dtype'),\n+        trainable=kwargs.get('trainable', True),\n+        **cell_kwargs)\n+    super(LSTM, self).__init__(\n+        cell,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        unroll=unroll,\n+        **kwargs)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+    self.input_spec = [InputSpec(ndim=3)]\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    return super(LSTM, self).call(\n+        inputs, mask=mask, training=training, initial_state=initial_state)\n+\n+  @property\n+  def units(self):\n+    return self.cell.units\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def recurrent_activation(self):\n+    return self.cell.recurrent_activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def unit_forget_bias(self):\n+    return self.cell.unit_forget_bias\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  @property\n+  def implementation(self):\n+    return self.cell.implementation\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+        'implementation':\n+            self.implementation\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n+    base_config = super(LSTM, self).get_config()\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    if 'implementation' in config and config['implementation'] == 0:\n+      config['implementation'] = 1\n+    return cls(**config)\n\n@@ -12,18 +12,16 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for LSTM layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for LSTM V1 layer.\"\"\"\n \n import copy\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.run_all_keras_modes\n\n@@ -0,0 +1,183 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utilities for RNN cells and layers.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import,protected-access\n+\n+from keras.utils import control_flow_util\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+\n+\n+def standardize_args(inputs, initial_state, constants, num_constants):\n+  \"\"\"Standardizes `__call__` to a single list of tensor inputs.\n+\n+  When running a model loaded from a file, the input tensors\n+  `initial_state` and `constants` can be passed to `RNN.__call__()` as part\n+  of `inputs` instead of by the dedicated keyword arguments. This method\n+  makes sure the arguments are separated and that `initial_state` and\n+  `constants` are lists of tensors (or None).\n+\n+  Args:\n+    inputs: Tensor or list/tuple of tensors. which may include constants\n+      and initial states. In that case `num_constant` must be specified.\n+    initial_state: Tensor or list of tensors or None, initial states.\n+    constants: Tensor or list of tensors or None, constant tensors.\n+    num_constants: Expected number of constants (if constants are passed as\n+      part of the `inputs` list.\n+\n+  Returns:\n+    inputs: Single tensor or tuple of tensors.\n+    initial_state: List of tensors or None.\n+    constants: List of tensors or None.\n+  \"\"\"\n+  if isinstance(inputs, list):\n+    # There are several situations here:\n+    # In the graph mode, __call__ will be only called once. The initial_state\n+    # and constants could be in inputs (from file loading).\n+    # In the eager mode, __call__ will be called twice, once during\n+    # rnn_layer(inputs=input_t, constants=c_t, ...), and second time will be\n+    # model.fit/train_on_batch/predict with real np data. In the second case,\n+    # the inputs will contain initial_state and constants as eager tensor.\n+    #\n+    # For either case, the real input is the first item in the list, which\n+    # could be a nested structure itself. Then followed by initial_states, which\n+    # could be a list of items, or list of list if the initial_state is complex\n+    # structure, and finally followed by constants which is a flat list.\n+    assert initial_state is None and constants is None\n+    if num_constants:\n+      constants = inputs[-num_constants:]\n+      inputs = inputs[:-num_constants]\n+    if len(inputs) > 1:\n+      initial_state = inputs[1:]\n+      inputs = inputs[:1]\n+\n+    if len(inputs) > 1:\n+      inputs = tuple(inputs)\n+    else:\n+      inputs = inputs[0]\n+\n+  def to_list_or_none(x):\n+    if x is None or isinstance(x, list):\n+      return x\n+    if isinstance(x, tuple):\n+      return list(x)\n+    return [x]\n+\n+  initial_state = to_list_or_none(initial_state)\n+  constants = to_list_or_none(constants)\n+\n+  return inputs, initial_state, constants\n+\n+\n+def is_multiple_state(state_size):\n+  \"\"\"Check whether the state_size contains multiple states.\"\"\"\n+  return (hasattr(state_size, '__len__') and\n+          not isinstance(state_size, tf.TensorShape))\n+\n+\n+def generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\n+  if inputs is not None:\n+    batch_size = tf.shape(inputs)[0]\n+    dtype = inputs.dtype\n+  return generate_zero_filled_state(batch_size, cell.state_size, dtype)\n+\n+\n+def generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n+  \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\n+  if batch_size_tensor is None or dtype is None:\n+    raise ValueError(\n+        'batch_size and dtype cannot be None while constructing initial state. '\n+        f'Received: batch_size={batch_size_tensor}, dtype={dtype}')\n+\n+  def create_zeros(unnested_state_size):\n+    flat_dims = tf.TensorShape(unnested_state_size).as_list()\n+    init_state_size = [batch_size_tensor] + flat_dims\n+    return tf.zeros(init_state_size, dtype=dtype)\n+\n+  if tf.nest.is_nested(state_size):\n+    return tf.nest.map_structure(create_zeros, state_size)\n+  else:\n+    return create_zeros(state_size)\n+\n+\n+def caching_device(rnn_cell):\n+  \"\"\"Returns the caching device for the RNN variable.\n+\n+  This is useful for distributed training, when variable is not located as same\n+  device as the training worker. By enabling the device cache, this allows\n+  worker to read the variable once and cache locally, rather than read it every\n+  time step from remote when it is needed.\n+\n+  Note that this is assuming the variable that cell needs for each time step is\n+  having the same value in the forward path, and only gets updated in the\n+  backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If the\n+  cell body relies on any variable that gets updated every time step, then\n+  caching device will cause it to read the stall value.\n+\n+  Args:\n+    rnn_cell: the rnn cell instance.\n+  \"\"\"\n+  if tf.executing_eagerly():\n+    # caching_device is not supported in eager mode.\n+    return None\n+  if not getattr(rnn_cell, '_enable_caching_device', False):\n+    return None\n+  # Don't set a caching device when running in a loop, since it is possible that\n+  # train steps could be wrapped in a tf.while_loop. In that scenario caching\n+  # prevents forward computations in loop iterations from re-reading the\n+  # updated weights.\n+  if control_flow_util.IsInWhileLoop(tf.compat.v1.get_default_graph()):\n+    logging.warning(\n+        'Variable read device caching has been disabled because the '\n+        'RNN is in tf.while_loop loop context, which will cause '\n+        'reading stalled value in forward path. This could slow down '\n+        'the training due to duplicated variable reads. Please '\n+        'consider updating your code to remove tf.while_loop if possible.')\n+    return None\n+  if (rnn_cell._dtype_policy.compute_dtype !=\n+      rnn_cell._dtype_policy.variable_dtype):\n+    logging.warning(\n+        'Variable read device caching has been disabled since it '\n+        'doesn\\'t work with the mixed precision API. This is '\n+        'likely to cause a slowdown for RNN training due to '\n+        'duplicated read of variable for each timestep, which '\n+        'will be significant in a multi remote worker setting. '\n+        'Please consider disabling mixed precision API if '\n+        'the performance has been affected.')\n+    return None\n+  # Cache the value on the device that access the variable.\n+  return lambda op: op.device\n+\n+\n+def config_for_enable_caching_device(rnn_cell):\n+  \"\"\"Return the dict config for RNN cell wrt to enable_caching_device field.\n+\n+  Since enable_caching_device is a internal implementation detail for speed up\n+  the RNN variable read when running on the multi remote worker setting, we\n+  don't want this config to be serialized constantly in the JSON. We will only\n+  serialize this field when a none default value is used to create the cell.\n+  Args:\n+    rnn_cell: the RNN cell for serialize.\n+\n+  Returns:\n+    A dict which contains the JSON config for enable_caching_device value or\n+    empty dict if the enable_caching_device value is same as the default value.\n+  \"\"\"\n+  default_enable_caching_device = tf.compat.v1.executing_eagerly_outside_functions(\n+  )\n+  if rnn_cell._enable_caching_device != default_enable_caching_device:\n+    return {'enable_caching_device': rnn_cell._enable_caching_device}\n+  return {}\n\n@@ -0,0 +1,492 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Fully connected RNN layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine import base_layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn import rnn_utils\n+from keras.layers.rnn.base_rnn import RNN\n+from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SimpleRNNCell')\n+class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+  \"\"\"Cell class for SimpleRNN.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  This class processes one step within the whole time sequence input, whereas\n+  `tf.keras.layer.SimpleRNN` processes the whole sequence.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass `None`, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs. Default:\n+      `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix, used for the linear transformation of the recurrent state.\n+      Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix. Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs. Default: 0.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state. Default: 0.\n+\n+  Call arguments:\n+    inputs: A 2D tensor, with shape of `[batch, feature]`.\n+    states: A 2D tensor with shape of `[batch, units]`, which is the state from\n+      the previous time step. For timestep 0, the initial state provided by user\n+      will be feed to cell.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. Only relevant when `dropout` or\n+      `recurrent_dropout` is used.\n+\n+  Examples:\n+\n+  ```python\n+  inputs = np.random.random([32, 10, 8]).astype(np.float32)\n+  rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))\n+\n+  output = rnn(inputs)  # The output has shape `[32, 4]`.\n+\n+  rnn = tf.keras.layers.RNN(\n+      tf.keras.layers.SimpleRNNCell(4),\n+      return_sequences=True,\n+      return_state=True)\n+\n+  # whole_sequence_output has shape `[32, 10, 4]`.\n+  # final_state has shape `[32, 4]`.\n+  whole_sequence_output, final_state = rnn(inputs)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for argument `units`, '\n+                       f'expected a positive integer, got {units}.')\n+    # By default use cached variable under v2 mode, see b/143699808.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n+    else:\n+      self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n+    super(SimpleRNNCell, self).__init__(**kwargs)\n+    self.units = units\n+    self.activation = activations.get(activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.recurrent_initializer = initializers.get(recurrent_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.recurrent_constraint = constraints.get(recurrent_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+\n+    self.dropout = min(1., max(0., dropout))\n+    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+    self.state_size = self.units\n+    self.output_size = self.units\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    default_caching_device = rnn_utils.caching_device(self)\n+    self.kernel = self.add_weight(\n+        shape=(input_shape[-1], self.units),\n+        name='kernel',\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        caching_device=default_caching_device)\n+    self.recurrent_kernel = self.add_weight(\n+        shape=(self.units, self.units),\n+        name='recurrent_kernel',\n+        initializer=self.recurrent_initializer,\n+        regularizer=self.recurrent_regularizer,\n+        constraint=self.recurrent_constraint,\n+        caching_device=default_caching_device)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          shape=(self.units,),\n+          name='bias',\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          caching_device=default_caching_device)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs, states, training=None):\n+    prev_output = states[0] if tf.nest.is_nested(states) else states\n+    dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n+    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n+        prev_output, training)\n+\n+    if dp_mask is not None:\n+      h = backend.dot(inputs * dp_mask, self.kernel)\n+    else:\n+      h = backend.dot(inputs, self.kernel)\n+    if self.bias is not None:\n+      h = backend.bias_add(h, self.bias)\n+\n+    if rec_dp_mask is not None:\n+      prev_output = prev_output * rec_dp_mask\n+    output = h + backend.dot(prev_output, self.recurrent_kernel)\n+    if self.activation is not None:\n+      output = self.activation(output)\n+\n+    new_state = [output] if tf.nest.is_nested(states) else output\n+    return output, new_state\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    return rnn_utils.generate_zero_filled_state_for_cell(\n+        self, inputs, batch_size, dtype)\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout\n+    }\n+    config.update(rnn_utils.config_for_enable_caching_device(self))\n+    base_config = super(SimpleRNNCell, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export('keras.layers.SimpleRNN')\n+class SimpleRNN(RNN):\n+  \"\"\"Fully-connected RNN where the output is to be fed back to input.\n+\n+  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n+  for details about the usage of RNN API.\n+\n+  Args:\n+    units: Positive integer, dimensionality of the output space.\n+    activation: Activation function to use.\n+      Default: hyperbolic tangent (`tanh`).\n+      If you pass None, no activation is applied\n+      (ie. \"linear\" activation: `a(x) = x`).\n+    use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix,\n+      used for the linear transformation of the inputs. Default:\n+      `glorot_uniform`.\n+    recurrent_initializer: Initializer for the `recurrent_kernel`\n+      weights matrix, used for the linear transformation of the recurrent state.\n+      Default: `orthogonal`.\n+    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix. Default: `None`.\n+    bias_regularizer: Regularizer function applied to the bias vector. Default:\n+      `None`.\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its \"activation\"). Default: `None`.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix. Default: `None`.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.  Default: `None`.\n+    bias_constraint: Constraint function applied to the bias vector. Default:\n+      `None`.\n+    dropout: Float between 0 and 1.\n+      Fraction of the units to drop for the linear transformation of the inputs.\n+      Default: 0.\n+    recurrent_dropout: Float between 0 and 1.\n+      Fraction of the units to drop for the linear transformation of the\n+      recurrent state. Default: 0.\n+    return_sequences: Boolean. Whether to return the last output\n+      in the output sequence, or the full sequence. Default: `False`.\n+    return_state: Boolean. Whether to return the last state\n+      in addition to the output. Default: `False`\n+    go_backwards: Boolean (default False).\n+      If True, process the input sequence backwards and return the\n+      reversed sequence.\n+    stateful: Boolean (default False). If True, the last state\n+      for each sample at index i in a batch will be used as initial\n+      state for the sample of index i in the following batch.\n+    unroll: Boolean (default False).\n+      If True, the network will be unrolled,\n+      else a symbolic loop will be used.\n+      Unrolling can speed-up a RNN,\n+      although it tends to be more memory-intensive.\n+      Unrolling is only suitable for short sequences.\n+\n+  Call arguments:\n+    inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n+    mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False` entry\n+      indicates that the corresponding timestep should be ignored.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or\n+      `recurrent_dropout` is used.\n+    initial_state: List of initial state tensors to be passed to the first\n+      call of the cell.\n+\n+  Examples:\n+\n+  ```python\n+  inputs = np.random.random([32, 10, 8]).astype(np.float32)\n+  simple_rnn = tf.keras.layers.SimpleRNN(4)\n+\n+  output = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n+\n+  simple_rnn = tf.keras.layers.SimpleRNN(\n+      4, return_sequences=True, return_state=True)\n+\n+  # whole_sequence_output has shape `[32, 10, 4]`.\n+  # final_state has shape `[32, 4]`.\n+  whole_sequence_output, final_state = simple_rnn(inputs)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               units,\n+               activation='tanh',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               dropout=0.,\n+               recurrent_dropout=0.,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               unroll=False,\n+               **kwargs):\n+    if 'implementation' in kwargs:\n+      kwargs.pop('implementation')\n+      logging.warning('The `implementation` argument '\n+                      'in `SimpleRNN` has been deprecated. '\n+                      'Please remove it from your layer call.')\n+    if 'enable_caching_device' in kwargs:\n+      cell_kwargs = {'enable_caching_device':\n+                     kwargs.pop('enable_caching_device')}\n+    else:\n+      cell_kwargs = {}\n+    cell = SimpleRNNCell(\n+        units,\n+        activation=activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        dtype=kwargs.get('dtype'),\n+        trainable=kwargs.get('trainable', True),\n+        **cell_kwargs)\n+    super(SimpleRNN, self).__init__(\n+        cell,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        unroll=unroll,\n+        **kwargs)\n+    self.activity_regularizer = regularizers.get(activity_regularizer)\n+    self.input_spec = [InputSpec(ndim=3)]\n+\n+  def call(self, inputs, mask=None, training=None, initial_state=None):\n+    return super(SimpleRNN, self).call(\n+        inputs, mask=mask, training=training, initial_state=initial_state)\n+\n+  @property\n+  def units(self):\n+    return self.cell.units\n+\n+  @property\n+  def activation(self):\n+    return self.cell.activation\n+\n+  @property\n+  def use_bias(self):\n+    return self.cell.use_bias\n+\n+  @property\n+  def kernel_initializer(self):\n+    return self.cell.kernel_initializer\n+\n+  @property\n+  def recurrent_initializer(self):\n+    return self.cell.recurrent_initializer\n+\n+  @property\n+  def bias_initializer(self):\n+    return self.cell.bias_initializer\n+\n+  @property\n+  def kernel_regularizer(self):\n+    return self.cell.kernel_regularizer\n+\n+  @property\n+  def recurrent_regularizer(self):\n+    return self.cell.recurrent_regularizer\n+\n+  @property\n+  def bias_regularizer(self):\n+    return self.cell.bias_regularizer\n+\n+  @property\n+  def kernel_constraint(self):\n+    return self.cell.kernel_constraint\n+\n+  @property\n+  def recurrent_constraint(self):\n+    return self.cell.recurrent_constraint\n+\n+  @property\n+  def bias_constraint(self):\n+    return self.cell.bias_constraint\n+\n+  @property\n+  def dropout(self):\n+    return self.cell.dropout\n+\n+  @property\n+  def recurrent_dropout(self):\n+    return self.cell.recurrent_dropout\n+\n+  def get_config(self):\n+    config = {\n+        'units':\n+            self.units,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout\n+    }\n+    base_config = super(SimpleRNN, self).get_config()\n+    config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n+    del base_config['cell']\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    if 'implementation' in config:\n+      config.pop('implementation')\n+    return cls(**config)\n\n\n@@ -0,0 +1,180 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import functools\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.layers.rnn import rnn_utils\n+from keras.utils import generic_utils\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.StackedRNNCells')\n+class StackedRNNCells(base_layer.Layer):\n+  \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n+\n+  Used to implement efficient stacked RNNs.\n+\n+  Args:\n+    cells: List of RNN cell instances.\n+\n+  Examples:\n+\n+  ```python\n+  batch_size = 3\n+  sentence_max_length = 5\n+  n_features = 2\n+  new_shape = (batch_size, sentence_max_length, n_features)\n+  x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)\n+\n+  rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]\n+  stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)\n+  lstm_layer = tf.keras.layers.RNN(stacked_lstm)\n+\n+  result = lstm_layer(x)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, cells, **kwargs):\n+    for cell in cells:\n+      if 'call' not in dir(cell):\n+        raise ValueError('All cells must have a `call` method. '\n+                         f'Received cell without a `call` method: {cell}')\n+      if 'state_size' not in dir(cell):\n+        raise ValueError('All cells must have a `state_size` attribute. '\n+                         f'Received cell without a `state_size`: {cell}')\n+    self.cells = cells\n+    # reverse_state_order determines whether the state size will be in a reverse\n+    # order of the cells' state. User might want to set this to True to keep the\n+    # existing behavior. This is only useful when use RNN(return_state=True)\n+    # since the state will be returned as the same order of state_size.\n+    self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n+    if self.reverse_state_order:\n+      logging.warning('reverse_state_order=True in StackedRNNCells will soon '\n+                      'be deprecated. Please update the code to work with the '\n+                      'natural order of states if you rely on the RNN states, '\n+                      'eg RNN(return_state=True).')\n+    super(StackedRNNCells, self).__init__(**kwargs)\n+\n+  @property\n+  def state_size(self):\n+    return tuple(c.state_size for c in\n+                 (self.cells[::-1] if self.reverse_state_order else self.cells))\n+\n+  @property\n+  def output_size(self):\n+    if getattr(self.cells[-1], 'output_size', None) is not None:\n+      return self.cells[-1].output_size\n+    elif rnn_utils.is_multiple_state(self.cells[-1].state_size):\n+      return self.cells[-1].state_size[0]\n+    else:\n+      return self.cells[-1].state_size\n+\n+  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n+    initial_states = []\n+    for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n+      get_initial_state_fn = getattr(cell, 'get_initial_state', None)\n+      if get_initial_state_fn:\n+        initial_states.append(get_initial_state_fn(\n+            inputs=inputs, batch_size=batch_size, dtype=dtype))\n+      else:\n+        initial_states.append(rnn_utils.generate_zero_filled_state_for_cell(\n+            cell, inputs, batch_size, dtype))\n+\n+    return tuple(initial_states)\n+\n+  def call(self, inputs, states, constants=None, training=None, **kwargs):\n+    # Recover per-cell states.\n+    state_size = (self.state_size[::-1]\n+                  if self.reverse_state_order else self.state_size)\n+    nested_states = tf.nest.pack_sequence_as(state_size,\n+                                             tf.nest.flatten(states))\n+\n+    # Call the cells in order and store the returned states.\n+    new_nested_states = []\n+    for cell, states in zip(self.cells, nested_states):\n+      states = states if tf.nest.is_nested(states) else [states]\n+      # TF cell does not wrap the state into list when there is only one state.\n+      is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None\n+      states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\n+      if generic_utils.has_arg(cell.call, 'training'):\n+        kwargs['training'] = training\n+      else:\n+        kwargs.pop('training', None)\n+      # Use the __call__ function for callable objects, eg layers, so that it\n+      # will have the proper name scopes for the ops, etc.\n+      cell_call_fn = cell.__call__ if callable(cell) else cell.call\n+      if generic_utils.has_arg(cell.call, 'constants'):\n+        inputs, states = cell_call_fn(inputs, states,\n+                                      constants=constants, **kwargs)\n+      else:\n+        inputs, states = cell_call_fn(inputs, states, **kwargs)\n+      new_nested_states.append(states)\n+\n+    return inputs, tf.nest.pack_sequence_as(state_size,\n+                                            tf.nest.flatten(new_nested_states))\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    if isinstance(input_shape, list):\n+      input_shape = input_shape[0]\n+\n+    def get_batch_input_shape(batch_size, dim):\n+      shape = tf.TensorShape(dim).as_list()\n+      return tuple([batch_size] + shape)\n+\n+    for cell in self.cells:\n+      if isinstance(cell, base_layer.Layer) and not cell.built:\n+        with backend.name_scope(cell.name):\n+          cell.build(input_shape)\n+          cell.built = True\n+      if getattr(cell, 'output_size', None) is not None:\n+        output_dim = cell.output_size\n+      elif rnn_utils.is_multiple_state(cell.state_size):\n+        output_dim = cell.state_size[0]\n+      else:\n+        output_dim = cell.state_size\n+      batch_size = tf.nest.flatten(input_shape)[0]\n+      if tf.nest.is_nested(output_dim):\n+        input_shape = tf.nest.map_structure(\n+            functools.partial(get_batch_input_shape, batch_size), output_dim)\n+        input_shape = tuple(input_shape)\n+      else:\n+        input_shape = tuple([batch_size] + tf.TensorShape(output_dim).as_list())\n+    self.built = True\n+\n+  def get_config(self):\n+    cells = []\n+    for cell in self.cells:\n+      cells.append(generic_utils.serialize_keras_object(cell))\n+    config = {'cells': cells}\n+    base_config = super(StackedRNNCells, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config, custom_objects=None):\n+    from keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\n+    cells = []\n+    for cell_config in config.pop('cells'):\n+      cells.append(\n+          deserialize_layer(cell_config, custom_objects=custom_objects))\n+    return cls(cells, **config)\n\n@@ -0,0 +1,327 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Wrapper layer to apply every temporal slice of an input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.rnn.base_wrapper import Wrapper\n+from keras.utils import generic_utils\n+from keras.utils import layer_utils\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.TimeDistributed')\n+class TimeDistributed(Wrapper):\n+  \"\"\"This wrapper allows to apply a layer to every temporal slice of an input.\n+\n+  Every input should be at least 3D, and the dimension of index one of the\n+  first input will be considered to be the temporal dimension.\n+\n+  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image\n+  with `channels_last` data format, across 10 timesteps.\n+  The batch input shape is `(32, 10, 128, 128, 3)`.\n+\n+  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each\n+  of the 10 timesteps, independently:\n+\n+  >>> inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n+  >>> conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\n+  >>> outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\n+  >>> outputs.shape\n+  TensorShape([None, 10, 126, 126, 64])\n+\n+  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the\n+  timestamps, the same set of weights are used at each timestamp.\n+\n+  Args:\n+    layer: a `tf.keras.layers.Layer` instance.\n+\n+  Call arguments:\n+    inputs: Input tensor of shape (batch, time, ...) or nested tensors,\n+      and each of which has shape (batch, time, ...).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the\n+      wrapped layer (only if the layer supports this argument).\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n+      a given timestep should be masked. This argument is passed to the\n+      wrapped layer (only if the layer supports this argument).\n+\n+  Raises:\n+    ValueError: If not initialized with a `tf.keras.layers.Layer` instance.\n+  \"\"\"\n+\n+  def __init__(self, layer, **kwargs):\n+    if not isinstance(layer, Layer):\n+      raise ValueError(\n+          'Please initialize `TimeDistributed` layer with a '\n+          f'`tf.keras.layers.Layer` instance. Received: {layer}')\n+    super(TimeDistributed, self).__init__(layer, **kwargs)\n+    self.supports_masking = True\n+\n+    # It is safe to use the fast, reshape-based approach with all of our\n+    # built-in Layers.\n+    self._always_use_reshape = (\n+        layer_utils.is_builtin_layer(layer) and\n+        not getattr(layer, 'stateful', False))\n+\n+  def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n+    \"\"\"Finds non-specific dimensions in the static shapes.\n+\n+    The static shapes are replaced with the corresponding dynamic shapes of the\n+    tensor.\n+    Args:\n+      init_tuple: a tuple, the first part of the output shape\n+      tensor: the tensor from which to get the (static and dynamic) shapes\n+        as the last part of the output shape\n+      start_idx: int, which indicate the first dimension to take from\n+        the static shape of the tensor\n+      int_shape: an alternative static shape to take as the last part\n+        of the output shape\n+    Returns:\n+      The new int_shape with the first part from init_tuple\n+      and the last part from either `int_shape` (if provided)\n+      or `tensor.shape`, where every `None` is replaced by\n+      the corresponding dimension from `tf.shape(tensor)`.\n+    \"\"\"\n+    # replace all None in int_shape by backend.shape\n+    if int_shape is None:\n+      int_shape = backend.int_shape(tensor)[start_idx:]\n+    if isinstance(int_shape, tf.TensorShape):\n+      int_shape = int_shape.as_list()\n+    if not any(not s for s in int_shape):\n+      return init_tuple + tuple(int_shape)\n+    shape = backend.shape(tensor)\n+    int_shape = list(int_shape)\n+    for i, s in enumerate(int_shape):\n+      if not s:\n+        int_shape[i] = shape[start_idx + i]\n+    return init_tuple + tuple(int_shape)\n+\n+  def _remove_timesteps(self, dims):\n+    dims = dims.as_list()\n+    return tf.TensorShape([dims[0]] + dims[2:])\n+\n+  def build(self, input_shape):\n+    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n+    input_dims = tf.nest.flatten(\n+        tf.nest.map_structure(lambda x: x.ndims, input_shape))\n+    if any(dim < 3 for dim in input_dims):\n+      raise ValueError(\n+          '`TimeDistributed` Layer should be passed an `input_shape ` '\n+          f'with at least 3 dimensions, received: {input_shape}')\n+    # Don't enforce the batch or time dimension.\n+    self.input_spec = tf.nest.map_structure(\n+        lambda x: InputSpec(shape=[None, None] + x.as_list()[2:]), input_shape)\n+    child_input_shape = tf.nest.map_structure(self._remove_timesteps,\n+                                              input_shape)\n+    child_input_shape = tf_utils.convert_shapes(child_input_shape)\n+    super(TimeDistributed, self).build(tuple(child_input_shape))\n+    self.built = True\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n+\n+    child_input_shape = tf.nest.map_structure(self._remove_timesteps,\n+                                              input_shape)\n+    child_output_shape = self.layer.compute_output_shape(child_input_shape)\n+    child_output_shape = tf_utils.convert_shapes(\n+        child_output_shape, to_tuples=False)\n+    timesteps = tf_utils.convert_shapes(input_shape)\n+    timesteps = tf.nest.flatten(timesteps)[1]\n+\n+    def insert_timesteps(dims):\n+      dims = dims.as_list()\n+      return tf.TensorShape([dims[0], timesteps] + dims[1:])\n+\n+    return tf.nest.map_structure(insert_timesteps, child_output_shape)\n+\n+  def call(self, inputs, training=None, mask=None):\n+    kwargs = {}\n+    if generic_utils.has_arg(self.layer.call, 'training'):\n+      kwargs['training'] = training\n+\n+    input_shape = tf.nest.map_structure(\n+        lambda x: tf.TensorShape(backend.int_shape(x)), inputs)\n+    batch_size = tf_utils.convert_shapes(input_shape)\n+    batch_size = tf.nest.flatten(batch_size)[0]\n+    if batch_size and not self._always_use_reshape:\n+      inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)\n+      is_ragged_input = row_lengths is not None\n+      input_length = tf_utils.convert_shapes(input_shape)\n+      input_length = tf.nest.flatten(input_length)[1]\n+\n+      # batch size matters, use rnn-based implementation\n+      def step(x, _):\n+        output = self.layer(x, **kwargs)\n+        return output, []\n+\n+      _, outputs, _ = backend.rnn(\n+          step,\n+          inputs,\n+          initial_states=[],\n+          input_length=row_lengths[0] if is_ragged_input else input_length,\n+          mask=mask,\n+          unroll=False)\n+      # pylint: disable=g-long-lambda\n+      y = tf.nest.map_structure(\n+          lambda output: backend.maybe_convert_to_ragged(\n+              is_ragged_input, output, row_lengths), outputs)\n+    else:\n+      # No batch size specified, therefore the layer will be able\n+      # to process batches of any size.\n+      # We can go with reshape-based implementation for performance.\n+      is_ragged_input = tf.nest.map_structure(\n+          lambda x: isinstance(x, tf.RaggedTensor), inputs)\n+      is_ragged_input = tf.nest.flatten(is_ragged_input)\n+      if all(is_ragged_input):\n+        input_values = tf.nest.map_structure(lambda x: x.values, inputs)\n+        input_row_lenghts = tf.nest.map_structure(\n+            lambda x: x.nested_row_lengths()[0], inputs)\n+        y = self.layer(input_values, **kwargs)\n+        y = tf.nest.map_structure(tf.RaggedTensor.from_row_lengths, y,\n+                                  input_row_lenghts)\n+      elif any(is_ragged_input):\n+        raise ValueError('All inputs has to be either ragged or not, '\n+                         f'but not mixed. Received: {inputs}')\n+      else:\n+        input_length = tf_utils.convert_shapes(input_shape)\n+        input_length = tf.nest.flatten(input_length)[1]\n+        if not input_length:\n+          input_length = tf.nest.map_structure(lambda x: tf.shape(x)[1], inputs)\n+          input_length = generic_utils.to_list(tf.nest.flatten(input_length))[0]\n+\n+        inner_input_shape = tf.nest.map_structure(\n+            lambda x: self._get_shape_tuple((-1,), x, 2), inputs)\n+        # Shape: (num_samples * timesteps, ...). And track the\n+        # transformation in self._input_map.\n+        inputs = tf.__internal__.nest.map_structure_up_to(\n+            inputs, tf.reshape, inputs, inner_input_shape)\n+        # (num_samples * timesteps, ...)\n+        if generic_utils.has_arg(self.layer.call, 'mask') and mask is not None:\n+          inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n+          kwargs['mask'] = backend.reshape(mask, inner_mask_shape)\n+\n+        y = self.layer(inputs, **kwargs)\n+\n+        # Shape: (num_samples, timesteps, ...)\n+        output_shape = self.compute_output_shape(input_shape)\n+        # pylint: disable=g-long-lambda\n+        output_shape = tf.nest.map_structure(\n+            lambda tensor, int_shape: self._get_shape_tuple(\n+                (-1, input_length), tensor, 1, int_shape[2:]), y, output_shape)\n+        y = tf.__internal__.nest.map_structure_up_to(y, tf.reshape, y,\n+                                                     output_shape)\n+        if not tf.executing_eagerly():\n+          # Set the static shape for the result since it might be lost during\n+          # array_ops reshape, eg, some `None` dim in the result could be\n+          # inferred.\n+          tf.__internal__.nest.map_structure_up_to(\n+              y, lambda tensor, shape: tensor.set_shape(shape), y,\n+              self.compute_output_shape(input_shape))\n+\n+    return y\n+\n+  def compute_mask(self, inputs, mask=None):\n+    \"\"\"Computes an output mask tensor for Embedding layer.\n+\n+    This is based on the inputs, mask, and the inner layer.\n+    If batch size is specified:\n+    Simply return the input `mask`. (An rnn-based implementation with\n+    more than one rnn inputs is required but not supported in tf.keras yet.)\n+    Otherwise we call `compute_mask` of the inner layer at each time step.\n+    If the output mask at each time step is not `None`:\n+    (E.g., inner layer is Masking or RNN)\n+    Concatenate all of them and return the concatenation.\n+    If the output mask at each time step is `None` and the input mask is not\n+    `None`:(E.g., inner layer is Dense)\n+    Reduce the input_mask to 2 dimensions and return it.\n+    Otherwise (both the output mask and the input mask are `None`):\n+    (E.g., `mask` is not used at all)\n+    Return `None`.\n+\n+    Args:\n+      inputs: Tensor with shape [batch size, timesteps, ...] indicating the\n+        input to TimeDistributed. If static shape information is available for\n+        \"batch size\", `mask` is returned unmodified.\n+      mask: Either None (indicating no masking) or a Tensor indicating the\n+        input mask for TimeDistributed. The shape can be static or dynamic.\n+\n+    Returns:\n+      Either None (no masking), or a [batch size, timesteps, ...] Tensor with\n+      an output mask for the TimeDistributed layer with the shape beyond the\n+      second dimension being the value of the input mask shape(if the computed\n+      output mask is none), an output mask with the shape beyond the first\n+      dimension being the value of the mask shape(if mask is not None) or\n+      output mask with the shape beyond the first dimension being the\n+      value of the computed output shape.\n+\n+    \"\"\"\n+    # cases need to call the layer.compute_mask when input_mask is None:\n+    # Masking layer and Embedding layer with mask_zero\n+    input_shape = tf.nest.map_structure(\n+        lambda x: tf.TensorShape(backend.int_shape(x)), inputs)\n+    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n+    batch_size = tf_utils.convert_shapes(input_shape)\n+    batch_size = tf.nest.flatten(batch_size)[0]\n+    is_ragged_input = tf.nest.map_structure(\n+        lambda x: isinstance(x, tf.RaggedTensor), inputs)\n+    is_ragged_input = generic_utils.to_list(tf.nest.flatten(is_ragged_input))\n+    if batch_size and not self._always_use_reshape or any(is_ragged_input):\n+      # batch size matters, we currently do not handle mask explicitly, or if\n+      # the layer always uses reshape approach, or the input is a ragged tensor.\n+      return mask\n+    inner_mask = mask\n+    if inner_mask is not None:\n+      inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n+      inner_mask = backend.reshape(inner_mask, inner_mask_shape)\n+    inner_input_shape = tf.nest.map_structure(\n+        lambda tensor: self._get_shape_tuple((-1,), tensor, 2), inputs)\n+    inner_inputs = tf.__internal__.nest.map_structure_up_to(\n+        inputs, tf.reshape, inputs, inner_input_shape)\n+    output_mask = self.layer.compute_mask(inner_inputs, inner_mask)\n+    if output_mask is None:\n+      if mask is None:\n+        return None\n+      # input_mask is not None, and output_mask is None:\n+      # we should return a not-None mask\n+      output_mask = mask\n+      for _ in range(2, len(backend.int_shape(mask))):\n+        output_mask = backend.any(output_mask, axis=-1)\n+    else:\n+      # output_mask is not None. We need to reshape it\n+      input_length = tf_utils.convert_shapes(input_shape)\n+      input_length = tf.nest.flatten(input_length)[1]\n+      if not input_length:\n+        input_length = tf.nest.map_structure(lambda x: backend.shape(x)[1],\n+                                             inputs)\n+        input_length = tf.nest.flatten(input_length)[0]\n+      output_mask_int_shape = backend.int_shape(output_mask)\n+      if output_mask_int_shape is None:\n+        # if the output_mask does not have a static shape,\n+        # its shape must be the same as mask's\n+        if mask is not None:\n+          output_mask_int_shape = backend.int_shape(mask)\n+        else:\n+          input_shape = generic_utils.to_list(tf.nest.flatten(input_shape))[0]\n+          output_mask_int_shape = backend.compute_output_shape(input_shape)[:-1]\n+      output_mask_shape = self._get_shape_tuple(\n+          (-1, input_length), output_mask, 1, output_mask_int_shape[1:])\n+      output_mask = backend.reshape(output_mask, output_mask_shape)\n+    return output_mask\n\n@@ -0,0 +1,484 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for TimeDistributed wrapper.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from absl.testing import parameterized\n+import keras\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.training.tracking import util as trackable_util\n+\n+\n+class TimeDistributedTest(test_combinations.TestCase):\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_timedistributed_dense(self):\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.TimeDistributed(\n+            keras.layers.Dense(2), input_shape=(3, 4)))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(\n+        np.random.random((10, 3, 4)),\n+        np.random.random((10, 3, 2)),\n+        epochs=1,\n+        batch_size=10)\n+\n+    # test config\n+    model.get_config()\n+\n+    # check whether the model variables are present in the\n+    # trackable list of objects\n+    checkpointed_object_ids = {\n+        id(o) for o in trackable_util.list_objects(model)\n+    }\n+    for v in model.variables:\n+      self.assertIn(id(v), checkpointed_object_ids)\n+\n+  def test_timedistributed_static_batch_size(self):\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.TimeDistributed(\n+            keras.layers.Dense(2), input_shape=(3, 4), batch_size=10))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(\n+        np.random.random((10, 3, 4)),\n+        np.random.random((10, 3, 2)),\n+        epochs=1,\n+        batch_size=10)\n+\n+  def test_timedistributed_invalid_init(self):\n+    x = tf.constant(np.zeros((1, 1)).astype('float32'))\n+    with self.assertRaisesRegex(\n+        ValueError, 'Please initialize `TimeDistributed` layer with a '\n+        '`tf.keras.layers.Layer` instance.'):\n+      keras.layers.TimeDistributed(x)\n+\n+  def test_timedistributed_conv2d(self):\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(\n+          keras.layers.TimeDistributed(\n+              keras.layers.Conv2D(5, (2, 2), padding='same'),\n+              input_shape=(2, 4, 4, 3)))\n+      model.add(keras.layers.Activation('relu'))\n+      model.compile(optimizer='rmsprop', loss='mse')\n+      model.train_on_batch(\n+          np.random.random((1, 2, 4, 4, 3)), np.random.random((1, 2, 4, 4, 5)))\n+\n+      model = keras.models.model_from_json(model.to_json())\n+      model.summary()\n+\n+  def test_timedistributed_stacked(self):\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(\n+          keras.layers.TimeDistributed(\n+              keras.layers.Dense(2), input_shape=(3, 4)))\n+      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n+      model.add(keras.layers.Activation('relu'))\n+      model.compile(optimizer='rmsprop', loss='mse')\n+\n+      model.fit(\n+          np.random.random((10, 3, 4)),\n+          np.random.random((10, 3, 3)),\n+          epochs=1,\n+          batch_size=10)\n+\n+  def test_regularizers(self):\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(\n+          keras.layers.TimeDistributed(\n+              keras.layers.Dense(2, kernel_regularizer='l1',\n+                                 activity_regularizer='l1'),\n+              input_shape=(3, 4)))\n+      model.add(keras.layers.Activation('relu'))\n+      model.compile(optimizer='rmsprop', loss='mse')\n+      self.assertEqual(len(model.losses), 2)\n+\n+  def test_TimeDistributed_learning_phase(self):\n+    with self.cached_session():\n+      # test layers that need learning_phase to be set\n+      np.random.seed(1234)\n+      x = keras.layers.Input(shape=(3, 2))\n+      y = keras.layers.TimeDistributed(keras.layers.Dropout(.999))(\n+          x, training=True)\n+      model = keras.models.Model(x, y)\n+      y = model.predict(np.random.random((10, 3, 2)))\n+      self.assertAllClose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n+\n+  def test_TimeDistributed_batchnorm(self):\n+    with self.cached_session():\n+      # test that wrapped BN updates still work.\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.TimeDistributed(\n+          keras.layers.BatchNormalization(center=True, scale=True),\n+          name='bn',\n+          input_shape=(10, 2)))\n+      model.compile(optimizer='rmsprop', loss='mse')\n+      # Assert that mean and variance are 0 and 1.\n+      td = model.layers[0]\n+      self.assertAllClose(td.get_weights()[2], np.array([0, 0]))\n+      assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n+      # Train\n+      model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n+                           np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n+      # Assert that mean and variance changed.\n+      assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n+      assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n+\n+  def test_TimeDistributed_trainable(self):\n+    # test layers that need learning_phase to be set\n+    x = keras.layers.Input(shape=(3, 2))\n+    layer = keras.layers.TimeDistributed(keras.layers.BatchNormalization())\n+    _ = layer(x)\n+    self.assertEqual(len(layer.trainable_weights), 2)\n+    layer.trainable = False\n+    assert not layer.trainable_weights\n+    layer.trainable = True\n+    assert len(layer.trainable_weights) == 2\n+\n+  def test_TimeDistributed_with_masked_embedding_and_unspecified_shape(self):\n+    with self.cached_session():\n+      # test with unspecified shape and Embeddings with mask_zero\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.TimeDistributed(\n+          keras.layers.Embedding(5, 6, mask_zero=True),\n+          input_shape=(None, None)))  # N by t_1 by t_2 by 6\n+      model.add(keras.layers.TimeDistributed(\n+          keras.layers.SimpleRNN(7, return_sequences=True)))\n+      model.add(keras.layers.TimeDistributed(\n+          keras.layers.SimpleRNN(8, return_sequences=False)))\n+      model.add(keras.layers.SimpleRNN(1, return_sequences=False))\n+      model.compile(optimizer='rmsprop', loss='mse')\n+      model_input = np.random.randint(low=1, high=5, size=(10, 3, 4),\n+                                      dtype='int32')\n+      for i in range(4):\n+        model_input[i, i:, i:] = 0\n+      model.fit(model_input,\n+                np.random.random((10, 1)), epochs=1, batch_size=10)\n+      mask_outputs = [model.layers[0].compute_mask(model.input)]\n+      for layer in model.layers[1:]:\n+        mask_outputs.append(layer.compute_mask(layer.input, mask_outputs[-1]))\n+      func = keras.backend.function([model.input], mask_outputs[:-1])\n+      mask_outputs_val = func([model_input])\n+      ref_mask_val_0 = model_input > 0         # embedding layer\n+      ref_mask_val_1 = ref_mask_val_0          # first RNN layer\n+      ref_mask_val_2 = np.any(ref_mask_val_1, axis=-1)     # second RNN layer\n+      ref_mask_val = [ref_mask_val_0, ref_mask_val_1, ref_mask_val_2]\n+      for i in range(3):\n+        self.assertAllEqual(mask_outputs_val[i], ref_mask_val[i])\n+      self.assertIs(mask_outputs[-1], None)  # final layer\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_TimeDistributed_with_masking_layer(self):\n+    # test with Masking layer\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.TimeDistributed(\n+            keras.layers.Masking(mask_value=0.,), input_shape=(None, 4)))\n+    model.add(keras.layers.TimeDistributed(keras.layers.Dense(5)))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4))\n+    for i in range(4):\n+      model_input[i, i:, :] = 0.\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(model_input, np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n+    mask_outputs = [model.layers[0].compute_mask(model.input)]\n+    mask_outputs += [\n+        model.layers[1].compute_mask(model.layers[1].input, mask_outputs[-1])\n+    ]\n+    func = keras.backend.function([model.input], mask_outputs)\n+    mask_outputs_val = func([model_input])\n+    self.assertEqual((mask_outputs_val[0]).all(), model_input.all())\n+    self.assertEqual((mask_outputs_val[1]).all(), model_input.all())\n+\n+  def test_TimeDistributed_with_different_time_shapes(self):\n+    time_dist = keras.layers.TimeDistributed(keras.layers.Dense(5))\n+    ph_1 = keras.backend.placeholder(shape=(None, 10, 13))\n+    out_1 = time_dist(ph_1)\n+    self.assertEqual(out_1.shape.as_list(), [None, 10, 5])\n+\n+    ph_2 = keras.backend.placeholder(shape=(None, 1, 13))\n+    out_2 = time_dist(ph_2)\n+    self.assertEqual(out_2.shape.as_list(), [None, 1, 5])\n+\n+    ph_3 = keras.backend.placeholder(shape=(None, 1, 18))\n+    with self.assertRaisesRegex(ValueError, 'is incompatible with'):\n+      time_dist(ph_3)\n+\n+  def test_TimeDistributed_with_invalid_dimensions(self):\n+    time_dist = keras.layers.TimeDistributed(keras.layers.Dense(5))\n+    ph = keras.backend.placeholder(shape=(None, 10))\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        '`TimeDistributed` Layer should be passed an `input_shape `'):\n+      time_dist(ph)\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_TimeDistributed_reshape(self):\n+\n+    class NoReshapeLayer(keras.layers.Layer):\n+\n+      def call(self, inputs):\n+        return inputs\n+\n+    # Built-in layers that aren't stateful use the reshape implementation.\n+    td1 = keras.layers.TimeDistributed(keras.layers.Dense(5))\n+    self.assertTrue(td1._always_use_reshape)\n+\n+    # Built-in layers that are stateful don't use the reshape implementation.\n+    td2 = keras.layers.TimeDistributed(\n+        keras.layers.RNN(keras.layers.SimpleRNNCell(10), stateful=True))\n+    self.assertFalse(td2._always_use_reshape)\n+\n+    # Custom layers are not allowlisted for the fast reshape implementation.\n+    td3 = keras.layers.TimeDistributed(NoReshapeLayer())\n+    self.assertFalse(td3._always_use_reshape)\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_TimeDistributed_output_shape_return_types(self):\n+\n+    class TestLayer(keras.layers.Layer):\n+\n+      def call(self, inputs):\n+        return tf.concat([inputs, inputs], axis=-1)\n+\n+      def compute_output_shape(self, input_shape):\n+        output_shape = tf.TensorShape(input_shape).as_list()\n+        output_shape[-1] = output_shape[-1] * 2\n+        output_shape = tf.TensorShape(output_shape)\n+        return output_shape\n+\n+    class TestListLayer(TestLayer):\n+\n+      def compute_output_shape(self, input_shape):\n+        shape = super(TestListLayer, self).compute_output_shape(input_shape)\n+        return shape.as_list()\n+\n+    class TestTupleLayer(TestLayer):\n+\n+      def compute_output_shape(self, input_shape):\n+        shape = super(TestTupleLayer, self).compute_output_shape(input_shape)\n+        return tuple(shape.as_list())\n+\n+    # Layers can specify output shape as list/tuple/TensorShape\n+    test_layers = [TestLayer, TestListLayer, TestTupleLayer]\n+    for layer in test_layers:\n+      input_layer = keras.layers.TimeDistributed(layer())\n+      inputs = keras.backend.placeholder(shape=(None, 2, 4))\n+      output = input_layer(inputs)\n+      self.assertEqual(output.shape.as_list(), [None, 2, 8])\n+      self.assertEqual(\n+          input_layer.compute_output_shape([None, 2, 4]).as_list(),\n+          [None, 2, 8])\n+\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  # TODO(scottzhu): check why v1 session failed.\n+  def test_TimeDistributed_with_mask_first_implementation(self):\n+    np.random.seed(100)\n+    rnn_layer = keras.layers.LSTM(4, return_sequences=True, stateful=True)\n+\n+    data = np.array([[[[1.0], [1.0]], [[0.0], [1.0]]],\n+                     [[[1.0], [0.0]], [[1.0], [1.0]]],\n+                     [[[1.0], [0.0]], [[1.0], [1.0]]]])\n+    x = keras.layers.Input(shape=(2, 2, 1), batch_size=3)\n+    x_masking = keras.layers.Masking()(x)\n+    y = keras.layers.TimeDistributed(rnn_layer)(x_masking)\n+    model_1 = keras.models.Model(x, y)\n+    model_1.compile(\n+        'rmsprop',\n+        'mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    output_with_mask = model_1.predict(data, steps=1)\n+\n+    y = keras.layers.TimeDistributed(rnn_layer)(x)\n+    model_2 = keras.models.Model(x, y)\n+    model_2.compile(\n+        'rmsprop',\n+        'mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    output = model_2.predict(data, steps=1)\n+\n+    self.assertNotAllClose(output_with_mask, output, atol=1e-7)\n+\n+  @test_combinations.run_all_keras_modes\n+  @parameterized.named_parameters(\n+      *test_utils.generate_combinations_with_testcase_name(\n+          layer=[keras.layers.LSTM,\n+                 keras.layers.Dense]))\n+  def test_TimeDistributed_with_ragged_input(self, layer):\n+    if tf.executing_eagerly():\n+      self.skipTest('b/143103634')\n+    np.random.seed(100)\n+    layer = layer(4)\n+    ragged_data = tf.ragged.constant(\n+        [[[[1.0], [1.0]], [[2.0], [2.0]]],\n+         [[[4.0], [4.0]], [[5.0], [5.0]], [[6.0], [6.0]]],\n+         [[[7.0], [7.0]], [[8.0], [8.0]], [[9.0], [9.0]]]],\n+        ragged_rank=1)\n+\n+    x_ragged = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n+    y_ragged = keras.layers.TimeDistributed(layer)(x_ragged)\n+    model_1 = keras.models.Model(x_ragged, y_ragged)\n+    model_1._run_eagerly = test_utils.should_run_eagerly()\n+    output_ragged = model_1.predict(ragged_data, steps=1)\n+\n+    x_dense = keras.Input(shape=(None, 2, 1), dtype='float32')\n+    masking = keras.layers.Masking()(x_dense)\n+    y_dense = keras.layers.TimeDistributed(layer)(masking)\n+    model_2 = keras.models.Model(x_dense, y_dense)\n+    dense_data = ragged_data.to_tensor()\n+    model_2._run_eagerly = test_utils.should_run_eagerly()\n+    output_dense = model_2.predict(dense_data, steps=1)\n+\n+    output_ragged = convert_ragged_tensor_value(output_ragged)\n+    self.assertAllEqual(output_ragged.to_tensor(), output_dense)\n+\n+  @test_combinations.run_all_keras_modes\n+  def test_TimeDistributed_with_ragged_input_with_batch_size(self):\n+    np.random.seed(100)\n+    layer = keras.layers.Dense(16)\n+\n+    ragged_data = tf.ragged.constant(\n+        [[[[1.0], [1.0]], [[2.0], [2.0]]],\n+         [[[4.0], [4.0]], [[5.0], [5.0]], [[6.0], [6.0]]],\n+         [[[7.0], [7.0]], [[8.0], [8.0]], [[9.0], [9.0]]]],\n+        ragged_rank=1)\n+\n+    # Use the first implementation by specifying batch_size\n+    x_ragged = keras.Input(shape=(None, 2, 1), batch_size=3, dtype='float32',\n+                           ragged=True)\n+    y_ragged = keras.layers.TimeDistributed(layer)(x_ragged)\n+    model_1 = keras.models.Model(x_ragged, y_ragged)\n+    output_ragged = model_1.predict(ragged_data, steps=1)\n+\n+    x_dense = keras.Input(shape=(None, 2, 1), batch_size=3, dtype='float32')\n+    masking = keras.layers.Masking()(x_dense)\n+    y_dense = keras.layers.TimeDistributed(layer)(masking)\n+    model_2 = keras.models.Model(x_dense, y_dense)\n+    dense_data = ragged_data.to_tensor()\n+    output_dense = model_2.predict(dense_data, steps=1)\n+\n+    output_ragged = convert_ragged_tensor_value(output_ragged)\n+    self.assertAllEqual(output_ragged.to_tensor(), output_dense)\n+\n+  def test_TimeDistributed_set_static_shape(self):\n+    layer = keras.layers.TimeDistributed(keras.layers.Conv2D(16, (3, 3)))\n+    inputs = keras.Input(batch_shape=(1, None, 32, 32, 1))\n+    outputs = layer(inputs)\n+    # Make sure the batch dim is not lost after array_ops.reshape.\n+    self.assertListEqual(outputs.shape.as_list(), [1, None, 30, 30, 16])\n+\n+  @test_combinations.run_all_keras_modes\n+  def test_TimeDistributed_with_mimo(self):\n+    dense_1 = keras.layers.Dense(8)\n+    dense_2 = keras.layers.Dense(16)\n+\n+    class TestLayer(keras.layers.Layer):\n+\n+      def __init__(self):\n+        super(TestLayer, self).__init__()\n+        self.dense_1 = dense_1\n+        self.dense_2 = dense_2\n+\n+      def call(self, inputs):\n+        return self.dense_1(inputs[0]), self.dense_2(inputs[1])\n+\n+      def compute_output_shape(self, input_shape):\n+        output_shape_1 = self.dense_1.compute_output_shape(input_shape[0])\n+        output_shape_2 = self.dense_2.compute_output_shape(input_shape[1])\n+        return output_shape_1, output_shape_2\n+\n+    np.random.seed(100)\n+    layer = TestLayer()\n+\n+    data_1 = tf.constant([[[[1.0], [1.0]], [[2.0], [2.0]]],\n+                          [[[4.0], [4.0]], [[5.0], [5.0]]],\n+                          [[[7.0], [7.0]], [[8.0], [8.0]]]])\n+\n+    data_2 = tf.constant([[[[1.0], [1.0]], [[2.0], [2.0]]],\n+                          [[[4.0], [4.0]], [[5.0], [5.0]]],\n+                          [[[7.0], [7.0]], [[8.0], [8.0]]]])\n+\n+    x1 = keras.Input(shape=(None, 2, 1), dtype='float32')\n+    x2 = keras.Input(shape=(None, 2, 1), dtype='float32')\n+    y1, y2 = keras.layers.TimeDistributed(layer)([x1, x2])\n+    model_1 = keras.models.Model([x1, x2], [y1, y2])\n+    model_1.compile(\n+        optimizer='rmsprop',\n+        loss='mse',\n+        run_eagerly=test_utils.should_run_eagerly())\n+    output_1 = model_1.predict((data_1, data_2), steps=1)\n+\n+    y1 = dense_1(x1)\n+    y2 = dense_2(x2)\n+    model_2 = keras.models.Model([x1, x2], [y1, y2])\n+    output_2 = model_2.predict((data_1, data_2), steps=1)\n+\n+    self.assertAllClose(output_1, output_2)\n+\n+    model_1.fit(\n+        x=[np.random.random((10, 2, 2, 1)),\n+           np.random.random((10, 2, 2, 1))],\n+        y=[np.random.random((10, 2, 2, 8)),\n+           np.random.random((10, 2, 2, 16))],\n+        epochs=1,\n+        batch_size=3)\n+\n+  def test_TimeDistributed_Attention(self):\n+    query_input = keras.layers.Input(shape=(None, 1, 10), dtype='float32')\n+    value_input = keras.layers.Input(shape=(None, 4, 10), dtype='float32')\n+\n+    # Query-value attention of shape [batch_size, Tq, filters].\n+    query_value_attention_seq = keras.layers.TimeDistributed(\n+        keras.layers.Attention())([query_input, value_input])\n+    model = keras.models.Model([query_input, value_input],\n+                               query_value_attention_seq)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(\n+        [np.random.random((10, 8, 1, 10)),\n+         np.random.random((10, 8, 4, 10))],\n+        np.random.random((10, 8, 1, 10)),\n+        epochs=1,\n+        batch_size=10)\n+\n+    # test config and serialization/deserialization\n+    model.get_config()\n+    model = keras.models.model_from_json(model.to_json())\n+    model.summary()\n+\n+\n+def convert_ragged_tensor_value(inputs):\n+  if isinstance(inputs, tf.compat.v1.ragged.RaggedTensorValue):\n+    flat_values = tf.convert_to_tensor(\n+        value=inputs.flat_values,\n+        name='flat_values')\n+    return tf.RaggedTensor.from_nested_row_splits(\n+        flat_values, inputs.nested_row_splits, validate=False)\n+  return inputs\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -24,21 +24,19 @@ from keras.engine import input_spec\n from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n-from keras.layers import convolutional_recurrent\n from keras.layers import core\n-from keras.layers import cudnn_recurrent\n from keras.layers import einsum_dense\n from keras.layers import embeddings\n from keras.layers import locally_connected\n from keras.layers import merging\n from keras.layers import noise\n from keras.layers import pooling\n-from keras.layers import recurrent\n-from keras.layers import recurrent_v2\n from keras.layers import regularization\n from keras.layers import reshaping\n-from keras.layers import rnn_cell_wrapper_v2\n-from keras.layers import wrappers\n+from keras.layers import rnn\n+from keras.layers.rnn import cell_wrappers\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import lstm\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import batch_normalization_v1\n from keras.layers.normalization import layer_normalization\n@@ -58,15 +56,14 @@ from keras.utils import tf_inspect as inspect\n from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, activation, attention, convolutional,\n-               convolutional_recurrent, core, cudnn_recurrent, embeddings,\n-               einsum_dense, locally_connected, merging, batch_normalization_v1,\n-               layer_normalization, unit_normalization, pooling,\n-               image_preprocessing, recurrent, regularization, reshaping,\n-               wrappers, hashing, hashed_crossing, category_encoding,\n-               discretization, integer_lookup, preprocessing_normalization,\n-               string_lookup, text_vectorization)\n-ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n-                  recurrent_v2)\n+               core, embeddings, einsum_dense, locally_connected, merging,\n+               batch_normalization_v1, layer_normalization, unit_normalization,\n+               pooling, image_preprocessing, regularization, reshaping, rnn,\n+               hashing, hashed_crossing, category_encoding, discretization,\n+               integer_lookup, preprocessing_normalization, string_lookup,\n+               text_vectorization)\n+ALL_V2_MODULES = (batch_normalization, layer_normalization, cell_wrappers, gru,\n+                  lstm)\n # ALL_OBJECTS is meant to be a global mutable. Hence we need to make it\n # thread-local to avoid concurrent mutations.\n LOCAL = threading.local()\n\n@@ -20,8 +20,10 @@ from absl.testing import parameterized\n \n import keras\n from keras.testing_infra import test_combinations\n-from keras.layers import recurrent as rnn_v1\n-from keras.layers import recurrent_v2 as rnn_v2\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_v1\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n from keras.layers.normalization import batch_normalization as batchnorm_v2\n from keras.layers.normalization import batch_normalization_v1 as batchnorm_v1\n \n@@ -133,33 +135,33 @@ class LayerSerializationTest(parameterized.TestCase, tf.test.TestCase):\n     self.assertEqual(new_layer.gamma_regularizer.__class__,\n                      keras.regularizers.L2)\n \n-  @parameterized.parameters([rnn_v1.LSTM, rnn_v2.LSTM])\n+  @parameterized.parameters([lstm_v1.LSTM, lstm.LSTM])\n   def test_serialize_deserialize_lstm(self, layer):\n-    lstm = layer(5, return_sequences=True)\n-    config = keras.layers.serialize(lstm)\n+    lstm_layer = layer(5, return_sequences=True)\n+    config = keras.layers.serialize(lstm_layer)\n     self.assertEqual(config['class_name'], 'LSTM')\n     new_layer = keras.layers.deserialize(config)\n     self.assertEqual(new_layer.units, 5)\n     self.assertEqual(new_layer.return_sequences, True)\n     if tf.__internal__.tf2.enabled():\n-      self.assertIsInstance(new_layer, rnn_v2.LSTM)\n+      self.assertIsInstance(new_layer, lstm.LSTM)\n     else:\n-      self.assertIsInstance(new_layer, rnn_v1.LSTM)\n-      self.assertNotIsInstance(new_layer, rnn_v2.LSTM)\n+      self.assertIsInstance(new_layer, lstm_v1.LSTM)\n+      self.assertNotIsInstance(new_layer, lstm.LSTM)\n \n-  @parameterized.parameters([rnn_v1.GRU, rnn_v2.GRU])\n+  @parameterized.parameters([gru_v1.GRU, gru.GRU])\n   def test_serialize_deserialize_gru(self, layer):\n-    gru = layer(5, return_sequences=True)\n-    config = keras.layers.serialize(gru)\n+    gru_layer = layer(5, return_sequences=True)\n+    config = keras.layers.serialize(gru_layer)\n     self.assertEqual(config['class_name'], 'GRU')\n     new_layer = keras.layers.deserialize(config)\n     self.assertEqual(new_layer.units, 5)\n     self.assertEqual(new_layer.return_sequences, True)\n     if tf.__internal__.tf2.enabled():\n-      self.assertIsInstance(new_layer, rnn_v2.GRU)\n+      self.assertIsInstance(new_layer, gru.GRU)\n     else:\n-      self.assertIsInstance(new_layer, rnn_v1.GRU)\n-      self.assertNotIsInstance(new_layer, rnn_v2.GRU)\n+      self.assertIsInstance(new_layer, gru_v1.GRU)\n+      self.assertNotIsInstance(new_layer, gru.GRU)\n \n \n if __name__ == '__main__':\n\n@@ -25,17 +25,21 @@ from keras.testing_infra import test_utils\n from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n-from keras.layers import convolutional_recurrent\n from keras.layers import core\n from keras.layers import embeddings\n from keras.layers import locally_connected\n from keras.layers import merging\n from keras.layers import pooling\n-from keras.layers import recurrent\n-from keras.layers import recurrent_v2\n from keras.layers import regularization\n from keras.layers import reshaping\n-from keras.layers import wrappers\n+from keras.layers.rnn import bidirectional\n+from keras.layers.rnn import conv_lstm2d\n+from keras.layers.rnn import simple_rnn\n+from keras.layers.rnn import gru\n+from keras.layers.rnn import gru_v1\n+from keras.layers.rnn import lstm\n+from keras.layers.rnn import lstm_v1\n+from keras.layers.rnn import time_distributed\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import layer_normalization\n from keras.layers.preprocessing import image_preprocessing\n@@ -103,8 +107,7 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n       ('ZeroPadding2D', reshaping.ZeroPadding2D, (2, 2, 2, 1)),\n       ('Cropping2D', reshaping.Cropping2D, (2, 3, 3, 1)),\n       ('ConvLSTM2D',\n-       lambda: convolutional_recurrent.ConvLSTM2D(4, kernel_size=(2, 2)),\n-       (4, 4, 4, 4, 4)),\n+       lambda: conv_lstm2d.ConvLSTM2D(4, kernel_size=(2, 2)), (4, 4, 4, 4, 4)),\n       ('Dense', lambda: core.Dense(2), (2, 2)),\n       ('Dropout', lambda: regularization.Dropout(0.5), (2, 2)),\n       ('SpatialDropout2D',\n@@ -142,23 +145,24 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n       ('AveragePooling2D', pooling.AveragePooling2D, (2, 2, 2, 1)),\n       ('GlobalMaxPooling2D', pooling.GlobalMaxPooling2D, (2, 2, 2, 1)),\n       ('GlobalAveragePooling2D', pooling.GlobalAveragePooling2D, (2, 2, 2, 1)),\n-      ('SimpleRNN', lambda: recurrent.SimpleRNN(units=4),\n+      ('SimpleRNN', lambda: simple_rnn.SimpleRNN(units=4),\n        (4, 4, 4), 1e-2, 1e-2),\n       ('SimpleRNN_stateful',\n-       lambda: recurrent.SimpleRNN(units=4, stateful=True),\n+       lambda: simple_rnn.SimpleRNN(units=4, stateful=True),\n        (4, 4, 4), 1e-2, 1e-2),\n-      ('GRU', lambda: recurrent.GRU(units=4), (4, 4, 4)),\n-      ('LSTM', lambda: recurrent.LSTM(units=4), (4, 4, 4)),\n-      ('GRUV2', lambda: recurrent_v2.GRU(units=4), (4, 4, 4)),\n-      ('GRUV2_stateful', lambda: recurrent_v2.GRU(units=4, stateful=True),\n+      ('GRU', lambda: gru_v1.GRU(units=4), (4, 4, 4)),\n+      ('LSTM', lambda: lstm_v1.LSTM(units=4), (4, 4, 4)),\n+      ('GRUV2', lambda: gru.GRU(units=4), (4, 4, 4)),\n+      ('GRUV2_stateful', lambda: gru.GRU(units=4, stateful=True),\n        (4, 4, 4)),\n-      ('LSTMV2', lambda: recurrent_v2.LSTM(units=4), (4, 4, 4)),\n-      ('LSTMV2_stateful', lambda: recurrent_v2.LSTM(units=4, stateful=True),\n+      ('LSTMV2', lambda: lstm.LSTM(units=4), (4, 4, 4)),\n+      ('LSTMV2_stateful', lambda: lstm.LSTM(units=4, stateful=True),\n        (4, 4, 4)),\n-      ('TimeDistributed', lambda: wrappers.TimeDistributed(core.Dense(2)),\n-       (2, 2, 2)),\n+      ('TimeDistributed',\n+       lambda: time_distributed.TimeDistributed(core.Dense(2)), (2, 2, 2)),\n       ('Bidirectional',\n-       lambda: wrappers.Bidirectional(recurrent.SimpleRNN(units=4)), (2, 2, 2)),\n+       lambda: bidirectional.Bidirectional(simple_rnn.SimpleRNN(units=4)),\n+       (2, 2, 2)),\n       ('AttentionLayerCausal', lambda: attention.Attention(causal=True), [\n           (2, 2, 3), (2, 3, 3), (2, 3, 3)\n       ]),\n\n@@ -64,9 +64,9 @@ training_lib_v1 = LazyLoader(\n     \"keras.engine.training_v1\")\n metrics = LazyLoader(\"metrics\", globals(),\n                      \"keras.metrics\")\n-recurrent = LazyLoader(\n-    \"recurrent\", globals(),\n-    \"keras.layers.recurrent\")\n+base_rnn = LazyLoader(\n+    \"base_rnn\", globals(),\n+    \"keras.layers.rnn.base_rnn\")\n # pylint:enable=g-inconsistent-quotes\n \n \n@@ -923,7 +923,7 @@ def _finalize_config_layers(layers):\n     _restore_layer_metrics(layer)\n \n     # Restore RNN layer states.\n-    if (isinstance(layer, recurrent.RNN) and\n+    if (isinstance(layer, base_rnn.RNN) and\n         layer.stateful and\n         hasattr(_get_keras_attr(layer), 'states')):\n       layer.states = getattr(_get_keras_attr(layer), 'states', None)\n\n@@ -32,9 +32,9 @@ training_lib = LazyLoader(\n     \"keras.engine.training\")\n metrics = LazyLoader(\"metrics\", globals(),\n                      \"keras.metrics\")\n-recurrent = LazyLoader(\n-    \"recurrent\", globals(),\n-    \"keras.layers.recurrent\")\n+base_rnn = LazyLoader(\n+    \"base_rnn\", globals(),\n+    \"keras.layers.rnn.base_rnn\")\n # pylint:enable=g-inconsistent-quotes\n \n \n@@ -147,7 +147,7 @@ class SerializedAttributes:\n       return ModelAttributes()\n     elif isinstance(obj, metrics.Metric):\n       return MetricAttributes()\n-    elif isinstance(obj, recurrent.RNN):\n+    elif isinstance(obj, base_rnn.RNN):\n       return RNNAttributes()\n     elif isinstance(obj, base_layer.Layer):\n       return LayerAttributes()\n\n@@ -24,7 +24,7 @@ import numpy as np\n import keras\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.layers.legacy_rnn import rnn_cell_impl as rnn_cell\n+from keras.layers.rnn import legacy_cells\n from keras.legacy_tf_layers import base as base_layer\n from keras.utils import np_utils\n \n@@ -215,9 +215,10 @@ class TimeseriesClassificationIntegrationTest(test_combinations.TestCase):\n \n     with base_layer.keras_style_scope():\n       model = keras.models.Sequential()\n-      model.add(keras.layers.RNN(rnn_cell.LSTMCell(5), return_sequences=True,\n+      model.add(keras.layers.RNN(legacy_cells.LSTMCell(5),\n+                                 return_sequences=True,\n                                  input_shape=x_train.shape[1:]))\n-      model.add(keras.layers.RNN(rnn_cell.GRUCell(y_train.shape[-1],\n+      model.add(keras.layers.RNN(legacy_cells.GRUCell(y_train.shape[-1],\n                                                       activation='softmax',\n                                                       dtype=tf.float32)))\n       model.compile(\n\n@@ -56,8 +56,8 @@ def check_pydot():\n \n def is_wrapped_model(layer):\n   from keras.engine import functional\n-  from keras.layers import wrappers\n-  return (isinstance(layer, wrappers.Wrapper) and\n+  from keras.layers import Wrapper\n+  return (isinstance(layer, Wrapper) and\n           isinstance(layer.layer, functional.Functional))\n \n \n@@ -144,7 +144,7 @@ def model_to_dot(model,\n                      'Build the model first by calling `build()` or by calling '\n                      'the model on a batch of data.')\n \n-  from keras.layers import wrappers\n+  from keras.layers import Wrapper\n   from keras.engine import sequential\n   from keras.engine import functional\n \n@@ -214,7 +214,7 @@ def model_to_dot(model,\n     layer_name = layer.name\n     class_name = layer.__class__.__name__\n \n-    if isinstance(layer, wrappers.Wrapper):\n+    if isinstance(layer, Wrapper):\n       if expand_nested and isinstance(layer.layer,\n                                       functional.Functional):\n         submodel_wrapper = model_to_dot(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
