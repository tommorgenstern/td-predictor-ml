{"custom_id": "keras#b3d3c919f72229deda2ef83406485eb2ae999f34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 97 | Churn Cumulative: 162 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 0.9142857142857143\n\nDIFF:\n@@ -3,6 +3,7 @@ from keras.src import layers\n from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.layers.layer import Layer\n+from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.src.saving import saving_lib\n from keras.src.saving import serialization_lib\n from keras.src.utils import backend_utils\n@@ -277,12 +278,8 @@ class FeatureSpace(Layer):\n \n     @classmethod\n     def float(cls, name=None):\n-        from keras.src.layers.core import identity\n-\n         name = name or auto_name(\"float\")\n-        preprocessor = identity.Identity(\n-            dtype=\"float32\", name=f\"{name}_preprocessor\"\n-        )\n+        preprocessor = TFDIdentity(dtype=\"float32\", name=f\"{name}_preprocessor\")\n         return Feature(\n             dtype=\"float32\", preprocessor=preprocessor, output_mode=\"float\"\n         )\n@@ -667,7 +664,7 @@ class FeatureSpace(Layer):\n                 output_dict[name] = feature\n \n         if self.output_mode == \"concat\":\n-            self.concat = layers.Concatenate(axis=-1)\n+            self.concat = TFDConcat(axis=-1)\n             return self.concat(features_to_concat)\n         else:\n             return output_dict\n@@ -734,7 +731,10 @@ class FeatureSpace(Layer):\n         if rebatched:\n             if self.output_mode == \"concat\":\n                 assert merged_data.shape[0] == 1\n-                if backend.backend() != \"tensorflow\":\n+                if (\n+                    backend.backend() != \"tensorflow\"\n+                    and not backend_utils.in_tf_graph()\n+                ):\n                     merged_data = backend.convert_to_numpy(merged_data)\n                 merged_data = tf.squeeze(merged_data, axis=0)\n             else:\n@@ -796,3 +796,17 @@ class FeatureSpace(Layer):\n \n     def load_own_variables(self, store):\n         return\n+\n+\n+class TFDConcat(TFDataLayer):\n+    def __init__(self, axis, **kwargs):\n+        super().__init__(**kwargs)\n+        self.axis = axis\n+\n+    def call(self, xs):\n+        return self.backend.numpy.concatenate(xs, axis=self.axis)\n+\n+\n+class TFDIdentity(TFDataLayer):\n+    def call(self, x):\n+        return x\n\n@@ -365,9 +365,6 @@ class FeatureSpaceTest(testing.TestCase):\n         ds = self._get_train_data_dict(as_dataset=True)\n         model.predict(ds.batch(4))\n \n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\", reason=\"TODO: debug it\"\n-    )\n     def test_tf_data_async_processing(self):\n         fs = feature_space.FeatureSpace(\n             features={\n\n@@ -5,12 +5,12 @@ import numpy as np\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n-from keras.src.layers.layer import Layer\n+from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n @keras_export(\"keras.layers.Normalization\")\n-class Normalization(Layer):\n+class Normalization(TFDataLayer):\n     \"\"\"A preprocessing layer that normalizes continuous features.\n \n     This layer will shift and scale inputs into a distribution centered around\n@@ -106,17 +106,19 @@ class Normalization(Layer):\n             axis = tuple(axis)\n         self.axis = axis\n \n+        self.input_mean = mean\n+        self.input_variance = variance\n+        self.invert = invert\n+        self.supports_masking = True\n+        self._build_input_shape = None\n+        self.mean = None\n+\n         # Set `mean` and `variance` if passed.\n         if (mean is not None) != (variance is not None):\n             raise ValueError(\n                 \"When setting values directly, both `mean` and `variance` \"\n                 f\"must be set. Received: mean={mean} and variance={variance}\"\n             )\n-        self.input_mean = mean\n-        self.input_variance = variance\n-        self.invert = invert\n-        self.supports_masking = True\n-        self._build_input_shape = None\n \n     def build(self, input_shape):\n         if input_shape is None:\n@@ -292,19 +294,35 @@ class Normalization(Layer):\n         self.variance = ops.cast(self.variance, self.compute_dtype)\n \n     def call(self, inputs):\n-        inputs = backend.convert_to_tensor(inputs, dtype=self.compute_dtype)\n+        # This layer can be called in tf.data\n+        # even with another backend after it has been adapted.\n+        # However it must use backend-native logic for adapt().\n+        if self.mean is None:\n+            # May happen when in tf.data when mean/var was passed explicitly\n+            raise ValueError(\n+                \"You must call `.build(input_shape)` \"\n+                \"on the layer before using it.\"\n+            )\n+        inputs = self.backend.core.convert_to_tensor(\n+            inputs, dtype=self.compute_dtype\n+        )\n         if self.invert:\n-            return ops.add(\n+            return self.backend.numpy.add(\n                 self.mean,\n-                ops.multiply(\n+                self.backend.numpy.multiply(\n                     inputs,\n-                    ops.maximum(ops.sqrt(self.variance), backend.epsilon()),\n+                    self.backend.numpy.maximum(\n+                        self.backend.numpy.sqrt(self.variance),\n+                        backend.epsilon(),\n+                    ),\n                 ),\n             )\n         else:\n-            return ops.divide(\n-                ops.subtract(inputs, self.mean),\n-                ops.maximum(ops.sqrt(self.variance), backend.epsilon()),\n+            return self.backend.numpy.divide(\n+                self.backend.numpy.subtract(inputs, self.mean),\n+                self.backend.numpy.maximum(\n+                    self.backend.numpy.sqrt(self.variance), backend.epsilon()\n+                ),\n             )\n \n     def compute_output_shape(self, input_shape):\n\n@@ -144,3 +144,23 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         new_shape_data = np.random.random((10, 3))\n         with self.assertRaisesRegex(ValueError, \"an incompatible shape\"):\n             layer.adapt(new_shape_data)\n+\n+    def test_tf_data_compatibility(self):\n+        x = np.random.random((32, 3))\n+        ds = tf_data.Dataset.from_tensor_slices(x).batch(1)\n+\n+        # With built-in values\n+        layer = layers.Normalization(\n+            mean=[0.1, 0.2, 0.3], variance=[0.1, 0.2, 0.3], axis=-1\n+        )\n+        layer.build((None, 3))\n+        for output in ds.map(layer).take(1):\n+            output.numpy()\n+\n+        # With adapt flow\n+        layer = layers.Normalization(axis=-1)\n+        layer.adapt(\n+            np.random.random((32, 3)),\n+        )\n+        for output in ds.map(layer).take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#64415ad3c8cae95e39d9c276646bf20c092b0ee2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1059 | Lines Deleted: 946 | Files Changed: 9 | Hunks: 410 | Methods Changed: 114 | Complexity Δ (Sum/Max): -45/10 | Churn Δ: 2005 | Churn Cumulative: 3262 | Contributors (this commit): 4 | Commits (past 90d): 36 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,32 +15,33 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=None):\n+def rgb_to_grayscale(images, data_format=None):\n+    # return images\n+    images = convert_to_tensor(images)\n     data_format = backend.standardize_data_format(data_format)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            image = jnp.transpose(image, (0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n-            image = jnp.transpose(image, (1, 2, 0))\n-        else:\n+    channels_axis = -1 if data_format == \"channels_last\" else -3\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n-    red, green, blue = image[..., 0], image[..., 1], image[..., 2]\n-    grayscale_image = 0.2989 * red + 0.5870 * green + 0.1140 * blue\n-    grayscale_image = jnp.expand_dims(grayscale_image, axis=-1)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            grayscale_image = jnp.transpose(grayscale_image, (0, 3, 1, 2))\n-        elif len(image.shape) == 3:\n-            grayscale_image = jnp.transpose(grayscale_image, (2, 0, 1))\n-    return jnp.array(grayscale_image)\n+    # Convert to floats\n+    original_dtype = images.dtype\n+    compute_dtype = backend.result_type(images.dtype, float)\n+    images = backend.cast(images, compute_dtype)\n+\n+    # Ref: tf.image.rgb_to_grayscale\n+    rgb_weights = convert_to_tensor(\n+        [0.2989, 0.5870, 0.1140], dtype=images.dtype\n+    )\n+    images = jnp.tensordot(images, rgb_weights, axes=(channels_axis, -1))\n+    images = jnp.expand_dims(images, axis=channels_axis)\n+    return backend.cast(images, original_dtype)\n \n \n def resize(\n-    image,\n+    images,\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n@@ -73,26 +74,26 @@ def resize(\n         )\n     size = tuple(size)\n     target_height, target_width = size\n-    if len(image.shape) == 4:\n+    if len(images.shape) == 4:\n         if data_format == \"channels_last\":\n-            size = (image.shape[0],) + size + (image.shape[-1],)\n+            size = (images.shape[0],) + size + (images.shape[-1],)\n         else:\n-            size = (image.shape[0], image.shape[1]) + size\n-        batch_size = image.shape[0]\n-    elif len(image.shape) == 3:\n+            size = (images.shape[0], images.shape[1]) + size\n+        batch_size = images.shape[0]\n+    elif len(images.shape) == 3:\n         if data_format == \"channels_last\":\n-            size = size + (image.shape[-1],)\n+            size = size + (images.shape[-1],)\n         else:\n-            size = (image.shape[0],) + size\n+            size = (images.shape[0],) + size\n     else:\n         raise ValueError(\n-            \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n \n     if crop_to_aspect_ratio:\n-        shape = image.shape\n+        shape = images.shape\n         if data_format == \"channels_last\":\n             height, width = shape[-3], shape[-2]\n         else:\n@@ -104,35 +105,35 @@ def resize(\n         crop_box_hstart = int(float(height - crop_height) / 2)\n         crop_box_wstart = int(float(width - crop_width) / 2)\n         if data_format == \"channels_last\":\n-            if len(image.shape) == 4:\n-                image = image[\n+            if len(images.shape) == 4:\n+                images = images[\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                     :,\n                 ]\n             else:\n-                image = image[\n+                images = images[\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                     :,\n                 ]\n         else:\n-            if len(image.shape) == 4:\n-                image = image[\n+            if len(images.shape) == 4:\n+                images = images[\n                     :,\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n             else:\n-                image = image[\n+                images = images[\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n     elif pad_to_aspect_ratio:\n-        shape = image.shape\n+        shape = images.shape\n         if data_format == \"channels_last\":\n             height, width, channels = shape[-3], shape[-2], shape[-1]\n         else:\n@@ -146,18 +147,18 @@ def resize(\n         img_box_wstart = int(float(pad_width - width) / 2)\n         if data_format == \"channels_last\":\n             if img_box_hstart > 0:\n-                if len(image.shape) == 4:\n+                if len(images.shape) == 4:\n                     padded_img = jnp.concatenate(\n                         [\n                             jnp.ones(\n                                 (batch_size, img_box_hstart, width, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (batch_size, img_box_hstart, width, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n                         ],\n@@ -168,31 +169,31 @@ def resize(\n                         [\n                             jnp.ones(\n                                 (img_box_hstart, width, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (img_box_hstart, width, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n                         ],\n                         axis=0,\n                     )\n             elif img_box_wstart > 0:\n-                if len(image.shape) == 4:\n+                if len(images.shape) == 4:\n                     padded_img = jnp.concatenate(\n                         [\n                             jnp.ones(\n                                 (batch_size, height, img_box_wstart, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (batch_size, height, img_box_wstart, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n                         ],\n@@ -203,30 +204,30 @@ def resize(\n                         [\n                             jnp.ones(\n                                 (height, img_box_wstart, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (height, img_box_wstart, channels),\n-                                dtype=image.dtype,\n+                                dtype=images.dtype,\n                             )\n                             * fill_value,\n                         ],\n                         axis=1,\n                     )\n             else:\n-                padded_img = image\n+                padded_img = images\n         else:\n             if img_box_hstart > 0:\n-                if len(image.shape) == 4:\n+                if len(images.shape) == 4:\n                     padded_img = jnp.concatenate(\n                         [\n                             jnp.ones(\n                                 (batch_size, channels, img_box_hstart, width)\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (batch_size, channels, img_box_hstart, width)\n                             )\n@@ -239,21 +240,21 @@ def resize(\n                         [\n                             jnp.ones((channels, img_box_hstart, width))\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones((channels, img_box_hstart, width))\n                             * fill_value,\n                         ],\n                         axis=1,\n                     )\n             elif img_box_wstart > 0:\n-                if len(image.shape) == 4:\n+                if len(images.shape) == 4:\n                     padded_img = jnp.concatenate(\n                         [\n                             jnp.ones(\n                                 (batch_size, channels, height, img_box_wstart)\n                             )\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones(\n                                 (batch_size, channels, height, img_box_wstart)\n                             )\n@@ -266,18 +267,18 @@ def resize(\n                         [\n                             jnp.ones((channels, height, img_box_wstart))\n                             * fill_value,\n-                            image,\n+                            images,\n                             jnp.ones((channels, height, img_box_wstart))\n                             * fill_value,\n                         ],\n                         axis=2,\n                     )\n             else:\n-                padded_img = image\n-        image = padded_img\n+                padded_img = images\n+        images = padded_img\n \n     return jax.image.resize(\n-        image, size, method=interpolation, antialias=antialias\n+        images, size, method=interpolation, antialias=antialias\n     )\n \n \n@@ -295,7 +296,7 @@ AFFINE_TRANSFORM_FILL_MODES = {\n \n \n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n@@ -317,11 +318,11 @@ def affine_transform(\n \n     transform = convert_to_tensor(transform)\n \n-    if len(image.shape) not in (3, 4):\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-            \"Invalid image rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n     if len(transform.shape) not in (1, 2):\n         raise ValueError(\n@@ -332,20 +333,20 @@ def affine_transform(\n \n     # unbatched case\n     need_squeeze = False\n-    if len(image.shape) == 3:\n-        image = jnp.expand_dims(image, axis=0)\n+    if len(images.shape) == 3:\n+        images = jnp.expand_dims(images, axis=0)\n         need_squeeze = True\n     if len(transform.shape) == 1:\n         transform = jnp.expand_dims(transform, axis=0)\n \n     if data_format == \"channels_first\":\n-        image = jnp.transpose(image, (0, 2, 3, 1))\n+        images = jnp.transpose(images, (0, 2, 3, 1))\n \n-    batch_size = image.shape[0]\n+    batch_size = images.shape[0]\n \n     # get indices\n     meshgrid = jnp.meshgrid(\n-        *[jnp.arange(size) for size in image.shape[1:]], indexing=\"ij\"\n+        *[jnp.arange(size) for size in images.shape[1:]], indexing=\"ij\"\n     )\n     indices = jnp.concatenate(\n         [jnp.expand_dims(x, axis=-1) for x in meshgrid], axis=-1\n@@ -383,7 +384,7 @@ def affine_transform(\n         mode=fill_mode,\n         cval=fill_value,\n     )\n-    affined = jax.vmap(_map_coordinates)(image, coordinates)\n+    affined = jax.vmap(_map_coordinates)(images, coordinates)\n \n     if data_format == \"channels_first\":\n         affined = jnp.transpose(affined, (0, 3, 1, 2))\n@@ -402,8 +403,22 @@ MAP_COORDINATES_FILL_MODES = {\n \n \n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n ):\n+    inputs = convert_to_tensor(inputs)\n+    coordinates = convert_to_tensor(coordinates)\n+    if coordinates.shape[0] != len(inputs.shape):\n+        raise ValueError(\n+            \"First dim of `coordinates` must be the same as the rank of \"\n+            \"`inputs`. \"\n+            f\"Received inputs with shape: {inputs.shape} and coordinate \"\n+            f\"leading dim of {coordinates.shape[0]}\"\n+        )\n+    if len(coordinates.shape) < 2:\n+        raise ValueError(\n+            \"Invalid coordinates rank: expected at least rank 2.\"\n+            f\" Received input with shape: {coordinates.shape}\"\n+        )\n     if fill_mode not in MAP_COORDINATES_FILL_MODES:\n         raise ValueError(\n             \"Invalid value for argument `fill_mode`. Expected one of \"\n@@ -416,5 +431,5 @@ def map_coordinates(\n             f\"{[0, 1]}. Received: order={order}\"\n         )\n     return jax.scipy.ndimage.map_coordinates(\n-        input, coordinates, order, fill_mode, fill_value\n+        inputs, coordinates, order, fill_mode, fill_value\n     )\n\n@@ -14,32 +14,30 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=None):\n+def rgb_to_grayscale(images, data_format=None):\n+    images = convert_to_tensor(images)\n     data_format = backend.standardize_data_format(data_format)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            image = np.transpose(image, (0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n-            image = np.transpose(image, (1, 2, 0))\n-        else:\n+    channels_axis = -1 if data_format == \"channels_last\" else -3\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n-    red, green, blue = image[..., 0], image[..., 1], image[..., 2]\n-    grayscale_image = 0.2989 * red + 0.5870 * green + 0.1140 * blue\n-    grayscale_image = np.expand_dims(grayscale_image, axis=-1)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            grayscale_image = np.transpose(grayscale_image, (0, 3, 1, 2))\n-        elif len(image.shape) == 3:\n-            grayscale_image = np.transpose(grayscale_image, (2, 0, 1))\n-    return np.array(grayscale_image)\n+    # Convert to floats\n+    original_dtype = images.dtype\n+    compute_dtype = backend.result_type(images.dtype, float)\n+    images = images.astype(compute_dtype)\n+\n+    # Ref: tf.image.rgb_to_grayscale\n+    rgb_weights = np.array([0.2989, 0.5870, 0.1140], dtype=images.dtype)\n+    grayscales = np.tensordot(images, rgb_weights, axes=(channels_axis, -1))\n+    grayscales = np.expand_dims(grayscales, axis=channels_axis)\n+    return grayscales.astype(original_dtype)\n \n \n def resize(\n-    image,\n+    images,\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n@@ -72,25 +70,25 @@ def resize(\n         )\n     size = tuple(size)\n     target_height, target_width = size\n-    if len(image.shape) == 4:\n+    if len(images.shape) == 4:\n         if data_format == \"channels_last\":\n-            size = (image.shape[0],) + size + (image.shape[-1],)\n+            size = (images.shape[0],) + size + (images.shape[-1],)\n         else:\n-            size = (image.shape[0], image.shape[1]) + size\n-    elif len(image.shape) == 3:\n+            size = (images.shape[0], images.shape[1]) + size\n+    elif len(images.shape) == 3:\n         if data_format == \"channels_last\":\n-            size = size + (image.shape[-1],)\n+            size = size + (images.shape[-1],)\n         else:\n-            size = (image.shape[0],) + size\n+            size = (images.shape[0],) + size\n     else:\n         raise ValueError(\n-            \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n \n     if crop_to_aspect_ratio:\n-        shape = image.shape\n+        shape = images.shape\n         if data_format == \"channels_last\":\n             height, width = shape[-3], shape[-2]\n         else:\n@@ -102,36 +100,36 @@ def resize(\n         crop_box_hstart = int(float(height - crop_height) / 2)\n         crop_box_wstart = int(float(width - crop_width) / 2)\n         if data_format == \"channels_last\":\n-            if len(image.shape) == 4:\n-                image = image[\n+            if len(images.shape) == 4:\n+                images = images[\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                     :,\n                 ]\n             else:\n-                image = image[\n+                images = images[\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                     :,\n                 ]\n         else:\n-            if len(image.shape) == 4:\n-                image = image[\n+            if len(images.shape) == 4:\n+                images = images[\n                     :,\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n             else:\n-                image = image[\n+                images = images[\n                     :,\n                     crop_box_hstart : crop_box_hstart + crop_height,\n                     crop_box_wstart : crop_box_wstart + crop_width,\n                 ]\n     elif pad_to_aspect_ratio:\n-        shape = image.shape\n-        batch_size = image.shape[0]\n+        shape = images.shape\n+        batch_size = images.shape[0]\n         if data_format == \"channels_last\":\n             height, width, channels = shape[-3], shape[-2], shape[-1]\n         else:\n@@ -143,7 +141,7 @@ def resize(\n         img_box_hstart = int(float(pad_height - height) / 2)\n         img_box_wstart = int(float(pad_width - width) / 2)\n         if data_format == \"channels_last\":\n-            if len(image.shape) == 4:\n+            if len(images.shape) == 4:\n                 padded_img = (\n                     np.ones(\n                         (\n@@ -152,7 +150,7 @@ def resize(\n                             pad_width + width,\n                             channels,\n                         ),\n-                        dtype=image.dtype,\n+                        dtype=images.dtype,\n                     )\n                     * fill_value\n                 )\n@@ -161,12 +159,12 @@ def resize(\n                     img_box_hstart : img_box_hstart + height,\n                     img_box_wstart : img_box_wstart + width,\n                     :,\n-                ] = image\n+                ] = images\n             else:\n                 padded_img = (\n                     np.ones(\n                         (pad_height + height, pad_width + width, channels),\n-                        dtype=image.dtype,\n+                        dtype=images.dtype,\n                     )\n                     * fill_value\n                 )\n@@ -174,9 +172,9 @@ def resize(\n                     img_box_hstart : img_box_hstart + height,\n                     img_box_wstart : img_box_wstart + width,\n                     :,\n-                ] = image\n+                ] = images\n         else:\n-            if len(image.shape) == 4:\n+            if len(images.shape) == 4:\n                 padded_img = (\n                     np.ones(\n                         (\n@@ -185,7 +183,7 @@ def resize(\n                             pad_height + height,\n                             pad_width + width,\n                         ),\n-                        dtype=image.dtype,\n+                        dtype=images.dtype,\n                     )\n                     * fill_value\n                 )\n@@ -194,12 +192,12 @@ def resize(\n                     :,\n                     img_box_hstart : img_box_hstart + height,\n                     img_box_wstart : img_box_wstart + width,\n-                ] = image\n+                ] = images\n             else:\n                 padded_img = (\n                     np.ones(\n                         (channels, pad_height + height, pad_width + width),\n-                        dtype=image.dtype,\n+                        dtype=images.dtype,\n                     )\n                     * fill_value\n                 )\n@@ -207,11 +205,13 @@ def resize(\n                     :,\n                     img_box_hstart : img_box_hstart + height,\n                     img_box_wstart : img_box_wstart + width,\n-                ] = image\n-        image = padded_img\n+                ] = images\n+        images = padded_img\n \n     return np.array(\n-        jax.image.resize(image, size, method=interpolation, antialias=antialias)\n+        jax.image.resize(\n+            images, size, method=interpolation, antialias=antialias\n+        )\n     )\n \n \n@@ -229,7 +229,7 @@ AFFINE_TRANSFORM_FILL_MODES = {\n \n \n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n@@ -251,11 +251,11 @@ def affine_transform(\n \n     transform = convert_to_tensor(transform)\n \n-    if len(image.shape) not in (3, 4):\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-            \"Invalid image rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n     if len(transform.shape) not in (1, 2):\n         raise ValueError(\n@@ -265,26 +265,26 @@ def affine_transform(\n         )\n \n     # scipy.ndimage.map_coordinates lacks support for half precision.\n-    input_dtype = image.dtype\n+    input_dtype = images.dtype\n     if input_dtype == \"float16\":\n-        image = image.astype(\"float32\")\n+        images = images.astype(\"float32\")\n \n     # unbatched case\n     need_squeeze = False\n-    if len(image.shape) == 3:\n-        image = np.expand_dims(image, axis=0)\n+    if len(images.shape) == 3:\n+        images = np.expand_dims(images, axis=0)\n         need_squeeze = True\n     if len(transform.shape) == 1:\n         transform = np.expand_dims(transform, axis=0)\n \n     if data_format == \"channels_first\":\n-        image = np.transpose(image, (0, 2, 3, 1))\n+        images = np.transpose(images, (0, 2, 3, 1))\n \n-    batch_size = image.shape[0]\n+    batch_size = images.shape[0]\n \n     # get indices\n     meshgrid = np.meshgrid(\n-        *[np.arange(size) for size in image.shape[1:]], indexing=\"ij\"\n+        *[np.arange(size) for size in images.shape[1:]], indexing=\"ij\"\n     )\n     indices = np.concatenate(\n         [np.expand_dims(x, axis=-1) for x in meshgrid], axis=-1\n@@ -317,7 +317,7 @@ def affine_transform(\n     affined = np.stack(\n         [\n             map_coordinates(\n-                image[i],\n+                images[i],\n                 coordinates[i],\n                 order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n                 fill_mode=fill_mode,\n@@ -347,8 +347,22 @@ MAP_COORDINATES_FILL_MODES = {\n \n \n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n ):\n+    inputs = convert_to_tensor(inputs)\n+    coordinates = convert_to_tensor(coordinates)\n+    if coordinates.shape[0] != len(inputs.shape):\n+        raise ValueError(\n+            \"First dim of `coordinates` must be the same as the rank of \"\n+            \"`inputs`. \"\n+            f\"Received inputs with shape: {inputs.shape} and coordinate \"\n+            f\"leading dim of {coordinates.shape[0]}\"\n+        )\n+    if len(coordinates.shape) < 2:\n+        raise ValueError(\n+            \"Invalid coordinates rank: expected at least rank 2.\"\n+            f\" Received input with shape: {coordinates.shape}\"\n+        )\n     if fill_mode not in MAP_COORDINATES_FILL_MODES:\n         raise ValueError(\n             \"Invalid value for argument `fill_mode`. Expected one of \"\n@@ -369,7 +383,7 @@ def map_coordinates(\n             max(-np.floor(c.min()).astype(int) + 1, 0),\n             max(np.ceil(c.max()).astype(int) + 1 - size, 0),\n         )\n-        for c, size in zip(coordinates, input.shape)\n+        for c, size in zip(coordinates, inputs.shape)\n     ]\n     shifted_coords = [c + p[0] for p, c in zip(padding, coordinates)]\n     pad_mode = {\n@@ -379,10 +393,10 @@ def map_coordinates(\n     }.get(fill_mode, fill_mode)\n     if fill_mode == \"constant\":\n         padded = np.pad(\n-            input, padding, mode=pad_mode, constant_values=fill_value\n+            inputs, padding, mode=pad_mode, constant_values=fill_value\n         )\n     else:\n-        padded = np.pad(input, padding, mode=pad_mode)\n+        padded = np.pad(inputs, padding, mode=pad_mode)\n     result = scipy.ndimage.map_coordinates(\n         padded, shifted_coords, order=order, mode=fill_mode, cval=fill_value\n     )\n\n@@ -16,30 +16,32 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=None):\n+def rgb_to_grayscale(images, data_format=None):\n+    images = convert_to_tensor(images)\n     data_format = backend.standardize_data_format(data_format)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            image = tf.transpose(image, (0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n-            image = tf.transpose(image, (1, 2, 0))\n-        else:\n+    channels_axis = -1 if data_format == \"channels_last\" else -3\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n-    grayscale_image = tf.image.rgb_to_grayscale(image)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            grayscale_image = tf.transpose(grayscale_image, (0, 3, 1, 2))\n-        elif len(image.shape) == 3:\n-            grayscale_image = tf.transpose(grayscale_image, (2, 0, 1))\n-    return tf.cast(grayscale_image, image.dtype)\n+    # Convert to floats\n+    original_dtype = images.dtype\n+    compute_dtype = backend.result_type(images.dtype, float)\n+    images = backend.cast(images, compute_dtype)\n+\n+    # Ref: tf.image.rgb_to_grayscale\n+    rgb_weights = convert_to_tensor(\n+        [0.2989, 0.5870, 0.1140], dtype=images.dtype\n+    )\n+    images = tf.tensordot(images, rgb_weights, axes=(channels_axis, -1))\n+    images = tf.expand_dims(images, axis=channels_axis)\n+    return tf.cast(images, original_dtype)\n \n \n def resize(\n-    image,\n+    images,\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n@@ -71,19 +73,20 @@ def resize(\n             f\"(height, width). Received: size={size}\"\n         )\n     size = tuple(size)\n-    if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n-            image = tf.transpose(image, (0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n-            image = tf.transpose(image, (1, 2, 0))\n-        else:\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n+    if data_format == \"channels_first\":\n+        if len(images.shape) == 4:\n+            images = tf.transpose(images, (0, 2, 3, 1))\n+        else:\n+            images = tf.transpose(images, (1, 2, 0))\n+\n     if crop_to_aspect_ratio:\n-        shape = tf.shape(image)\n+        shape = tf.shape(images)\n         height, width = shape[-3], shape[-2]\n         target_height, target_width = size\n         crop_height = tf.cast(\n@@ -105,21 +108,21 @@ def resize(\n         crop_box_wstart = tf.cast(\n             tf.cast(width - crop_width, \"float32\") / 2, \"int32\"\n         )\n-        if len(image.shape) == 4:\n-            image = image[\n+        if len(images.shape) == 4:\n+            images = images[\n                 :,\n                 crop_box_hstart : crop_box_hstart + crop_height,\n                 crop_box_wstart : crop_box_wstart + crop_width,\n                 :,\n             ]\n         else:\n-            image = image[\n+            images = images[\n                 crop_box_hstart : crop_box_hstart + crop_height,\n                 crop_box_wstart : crop_box_wstart + crop_width,\n                 :,\n             ]\n     elif pad_to_aspect_ratio:\n-        shape = tf.shape(image)\n+        shape = tf.shape(images)\n         height, width = shape[-3], shape[-2]\n         target_height, target_width = size\n         pad_height = tf.cast(\n@@ -141,28 +144,28 @@ def resize(\n         img_box_wstart = tf.cast(\n             tf.cast(pad_width - width, \"float32\") / 2, \"int32\"\n         )\n-        if len(image.shape) == 4:\n-            batch_size = tf.shape(image)[0]\n-            channels = tf.shape(image)[3]\n+        if len(images.shape) == 4:\n+            batch_size = tf.shape(images)[0]\n+            channels = tf.shape(images)[3]\n             padded_img = tf.cond(\n                 img_box_hstart > 0,\n                 lambda: tf.concat(\n                     [\n                         tf.ones(\n                             (batch_size, img_box_hstart, width, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n-                        image,\n+                        images,\n                         tf.ones(\n                             (batch_size, img_box_hstart, width, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                     ],\n                     axis=1,\n                 ),\n-                lambda: image,\n+                lambda: images,\n             )\n             padded_img = tf.cond(\n                 img_box_wstart > 0,\n@@ -170,13 +173,13 @@ def resize(\n                     [\n                         tf.ones(\n                             (batch_size, height, img_box_wstart, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                         padded_img,\n                         tf.ones(\n                             (batch_size, height, img_box_wstart, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                     ],\n@@ -185,26 +188,26 @@ def resize(\n                 lambda: padded_img,\n             )\n         else:\n-            channels = tf.shape(image)[2]\n+            channels = tf.shape(images)[2]\n             padded_img = tf.cond(\n                 img_box_hstart > 0,\n                 lambda: tf.concat(\n                     [\n                         tf.ones(\n                             (img_box_hstart, width, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n-                        image,\n+                        images,\n                         tf.ones(\n                             (img_box_hstart, width, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                     ],\n                     axis=0,\n                 ),\n-                lambda: image,\n+                lambda: images,\n             )\n             padded_img = tf.cond(\n                 img_box_wstart > 0,\n@@ -212,13 +215,13 @@ def resize(\n                     [\n                         tf.ones(\n                             (height, img_box_wstart, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                         padded_img,\n                         tf.ones(\n                             (height, img_box_wstart, channels),\n-                            dtype=image.dtype,\n+                            dtype=images.dtype,\n                         )\n                         * fill_value,\n                     ],\n@@ -226,17 +229,17 @@ def resize(\n                 ),\n                 lambda: padded_img,\n             )\n-        image = padded_img\n+        images = padded_img\n \n     resized = tf.image.resize(\n-        image, size, method=interpolation, antialias=antialias\n+        images, size, method=interpolation, antialias=antialias\n     )\n     if data_format == \"channels_first\":\n-        if len(image.shape) == 4:\n+        if len(images.shape) == 4:\n             resized = tf.transpose(resized, (0, 3, 1, 2))\n-        elif len(image.shape) == 3:\n+        elif len(images.shape) == 3:\n             resized = tf.transpose(resized, (2, 0, 1))\n-    return tf.cast(resized, image.dtype)\n+    return tf.cast(resized, images.dtype)\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = (\n@@ -253,7 +256,7 @@ AFFINE_TRANSFORM_FILL_MODES = (\n \n \n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n@@ -272,11 +275,11 @@ def affine_transform(\n             \"Invalid value for argument `fill_mode`. Expected of one \"\n             f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n         )\n-    if len(image.shape) not in (3, 4):\n+    if len(images.shape) not in (3, 4):\n         raise ValueError(\n-            \"Invalid image rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n     if len(transform.shape) not in (1, 2):\n         raise ValueError(\n@@ -286,24 +289,24 @@ def affine_transform(\n         )\n     # unbatched case\n     need_squeeze = False\n-    if len(image.shape) == 3:\n-        image = tf.expand_dims(image, axis=0)\n+    if len(images.shape) == 3:\n+        images = tf.expand_dims(images, axis=0)\n         need_squeeze = True\n     if len(transform.shape) == 1:\n         transform = tf.expand_dims(transform, axis=0)\n \n     if data_format == \"channels_first\":\n-        image = tf.transpose(image, (0, 2, 3, 1))\n+        images = tf.transpose(images, (0, 2, 3, 1))\n \n     affined = tf.raw_ops.ImageProjectiveTransformV3(\n-        images=image,\n+        images=images,\n         transforms=tf.cast(transform, dtype=tf.float32),\n-        output_shape=tf.shape(image)[1:-1],\n+        output_shape=tf.shape(images)[1:-1],\n         fill_value=fill_value,\n         interpolation=interpolation.upper(),\n         fill_mode=fill_mode.upper(),\n     )\n-    affined = tf.ensure_shape(affined, image.shape)\n+    affined = tf.ensure_shape(affined, images.shape)\n \n     if data_format == \"channels_first\":\n         affined = tf.transpose(affined, (0, 3, 1, 2))\n@@ -351,20 +354,28 @@ def _linear_indices_and_weights(coordinate):\n \n \n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n ):\n-    input_arr = convert_to_tensor(input)\n+    input_arr = convert_to_tensor(inputs)\n     coordinate_arrs = convert_to_tensor(coordinates)\n+\n+    if coordinate_arrs.shape[0] != len(input_arr.shape):\n+        raise ValueError(\n+            \"First dim of `coordinates` must be the same as the rank of \"\n+            \"`inputs`. \"\n+            f\"Received inputs with shape: {input_arr.shape} and coordinate \"\n+            f\"leading dim of {coordinate_arrs.shape[0]}\"\n+        )\n+    if len(coordinate_arrs.shape) < 2:\n+        raise ValueError(\n+            \"Invalid coordinates rank: expected at least rank 2.\"\n+            f\" Received input with shape: {coordinate_arrs.shape}\"\n+        )\n+\n     # unstack into a list of tensors for following operations\n     coordinate_arrs = tf.unstack(coordinate_arrs, axis=0)\n     fill_value = convert_to_tensor(tf.cast(fill_value, input_arr.dtype))\n \n-    if len(coordinates) != len(input_arr.shape):\n-        raise ValueError(\n-            \"coordinates must be a sequence of length input.shape, but \"\n-            f\"{len(coordinates)} != {len(input_arr.shape)}\"\n-        )\n-\n     index_fixer = _INDEX_FIXERS.get(fill_mode)\n     if index_fixer is None:\n         raise ValueError(\n\n@@ -6,6 +6,7 @@ import torch\n \n from keras.src import backend\n from keras.src.backend.torch.core import convert_to_tensor\n+from keras.src.utils.module_utils import torchvision\n \n RESIZE_INTERPOLATIONS = {}  # populated after torchvision import\n \n@@ -15,41 +16,31 @@ UNSUPPORTED_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=None):\n+def rgb_to_grayscale(images, data_format=None):\n+    images = convert_to_tensor(images)\n     data_format = backend.standardize_data_format(data_format)\n-    try:\n-        import torchvision\n-    except:\n-        raise ImportError(\n-            \"The torchvision package is necessary to use \"\n-            \"`rgb_to_grayscale` with the torch backend. \"\n-            \"Please install torchvision. \"\n-        )\n-    image = convert_to_tensor(image)\n     if data_format == \"channels_last\":\n-        if image.ndim == 4:\n-            image = image.permute((0, 3, 1, 2))\n-        elif image.ndim == 3:\n-            image = image.permute((2, 0, 1))\n+        if images.ndim == 4:\n+            images = images.permute((0, 3, 1, 2))\n+        elif images.ndim == 3:\n+            images = images.permute((2, 0, 1))\n         else:\n             raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+                \"Invalid images rank: expected rank 3 (single image) \"\n                 \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n-            )\n-    grayscale_image = torchvision.transforms.functional.rgb_to_grayscale(\n-        img=image,\n+                f\"images.shape={images.shape}\"\n             )\n+    images = torchvision.transforms.functional.rgb_to_grayscale(img=images)\n     if data_format == \"channels_last\":\n-        if len(image.shape) == 4:\n-            grayscale_image = grayscale_image.permute((0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n-            grayscale_image = grayscale_image.permute((1, 2, 0))\n-    return grayscale_image\n+        if len(images.shape) == 4:\n+            images = images.permute((0, 2, 3, 1))\n+        elif len(images.shape) == 3:\n+            images = images.permute((1, 2, 0))\n+    return images\n \n \n def resize(\n-    image,\n+    images,\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n@@ -60,22 +51,13 @@ def resize(\n     data_format=None,\n ):\n     data_format = backend.standardize_data_format(data_format)\n-    try:\n-        import torchvision\n-        from torchvision.transforms import InterpolationMode as im\n-\n     RESIZE_INTERPOLATIONS.update(\n         {\n-                \"bilinear\": im.BILINEAR,\n-                \"nearest\": im.NEAREST_EXACT,\n-                \"bicubic\": im.BICUBIC,\n+            \"bilinear\": torchvision.transforms.InterpolationMode.BILINEAR,\n+            \"nearest\": torchvision.transforms.InterpolationMode.NEAREST_EXACT,\n+            \"bicubic\": torchvision.transforms.InterpolationMode.BICUBIC,\n         }\n     )\n-    except:\n-        raise ImportError(\n-            \"The torchvision package is necessary to use `resize` with the \"\n-            \"torch backend. Please install torchvision.\"\n-        )\n     if interpolation in UNSUPPORTED_INTERPOLATIONS:\n         raise ValueError(\n             \"Resizing with Lanczos interpolation is \"\n@@ -103,21 +85,21 @@ def resize(\n             f\"(height, width). Received: size={size}\"\n         )\n     size = tuple(size)\n-    image = convert_to_tensor(image)\n-    if data_format == \"channels_last\":\n-        if image.ndim == 4:\n-            image = image.permute((0, 3, 1, 2))\n-        elif image.ndim == 3:\n-            image = image.permute((2, 0, 1))\n-        else:\n+    images = convert_to_tensor(images)\n+    if images.ndim not in (3, 4):\n         raise ValueError(\n-                \"Invalid input rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n+    if data_format == \"channels_last\":\n+        if images.ndim == 4:\n+            images = images.permute((0, 3, 1, 2))\n+        else:\n+            images = images.permute((2, 0, 1))\n \n     if crop_to_aspect_ratio:\n-        shape = image.shape\n+        shape = images.shape\n         height, width = shape[-2], shape[-1]\n         target_height, target_width = size\n         crop_height = int(float(width * target_height) / target_width)\n@@ -126,21 +108,21 @@ def resize(\n         crop_width = min(width, crop_width)\n         crop_box_hstart = int(float(height - crop_height) / 2)\n         crop_box_wstart = int(float(width - crop_width) / 2)\n-        if len(image.shape) == 4:\n-            image = image[\n+        if len(images.shape) == 4:\n+            images = images[\n                 :,\n                 :,\n                 crop_box_hstart : crop_box_hstart + crop_height,\n                 crop_box_wstart : crop_box_wstart + crop_width,\n             ]\n         else:\n-            image = image[\n+            images = images[\n                 :,\n                 crop_box_hstart : crop_box_hstart + crop_height,\n                 crop_box_wstart : crop_box_wstart + crop_width,\n             ]\n     elif pad_to_aspect_ratio:\n-        shape = image.shape\n+        shape = images.shape\n         height, width = shape[-2], shape[-1]\n         target_height, target_width = size\n         pad_height = int(float(width * target_height) / target_width)\n@@ -149,9 +131,9 @@ def resize(\n         pad_width = max(width, pad_width)\n         img_box_hstart = int(float(pad_height - height) / 2)\n         img_box_wstart = int(float(pad_width - width) / 2)\n-        if len(image.shape) == 4:\n-            batch_size = image.shape[0]\n-            channels = image.shape[1]\n+        if len(images.shape) == 4:\n+            batch_size = images.shape[0]\n+            channels = images.shape[1]\n             padded_img = (\n                 torch.ones(\n                     (\n@@ -160,7 +142,7 @@ def resize(\n                         pad_height + height,\n                         pad_width + width,\n                     ),\n-                    dtype=image.dtype,\n+                    dtype=images.dtype,\n                 )\n                 * fill_value\n             )\n@@ -169,13 +151,13 @@ def resize(\n                 :,\n                 img_box_hstart : img_box_hstart + height,\n                 img_box_wstart : img_box_wstart + width,\n-            ] = image\n+            ] = images\n         else:\n-            channels = image.shape[0]\n+            channels = images.shape[0]\n             padded_img = (\n                 torch.ones(\n                     (channels, pad_height + height, pad_width + width),\n-                    dtype=image.dtype,\n+                    dtype=images.dtype,\n                 )\n                 * fill_value\n             )\n@@ -183,19 +165,19 @@ def resize(\n                 :,\n                 img_box_hstart : img_box_hstart + height,\n                 img_box_wstart : img_box_wstart + width,\n-            ] = image\n-        image = padded_img\n+            ] = images\n+        images = padded_img\n \n     resized = torchvision.transforms.functional.resize(\n-        img=image,\n+        img=images,\n         size=size,\n         interpolation=RESIZE_INTERPOLATIONS[interpolation],\n         antialias=antialias,\n     )\n     if data_format == \"channels_last\":\n-        if len(image.shape) == 4:\n+        if len(images.shape) == 4:\n             resized = resized.permute((0, 2, 3, 1))\n-        elif len(image.shape) == 3:\n+        elif len(images.shape) == 3:\n             resized = resized.permute((1, 2, 0))\n     return resized\n \n@@ -214,7 +196,7 @@ AFFINE_TRANSFORM_FILL_MODES = {\n \n \n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n@@ -234,14 +216,14 @@ def affine_transform(\n             f\"{AFFINE_TRANSFORM_FILL_MODES}. Received: fill_mode={fill_mode}\"\n         )\n \n-    image = convert_to_tensor(image)\n+    images = convert_to_tensor(images)\n     transform = convert_to_tensor(transform)\n \n-    if image.ndim not in (3, 4):\n+    if images.ndim not in (3, 4):\n         raise ValueError(\n-            \"Invalid image rank: expected rank 3 (single image) \"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n             \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+            f\"images.shape={images.shape}\"\n         )\n     if transform.ndim not in (1, 2):\n         raise ValueError(\n@@ -252,22 +234,22 @@ def affine_transform(\n \n     # unbatched case\n     need_squeeze = False\n-    if image.ndim == 3:\n-        image = image.unsqueeze(dim=0)\n+    if images.ndim == 3:\n+        images = images.unsqueeze(dim=0)\n         need_squeeze = True\n     if transform.ndim == 1:\n         transform = transform.unsqueeze(dim=0)\n \n     if data_format == \"channels_first\":\n-        image = image.permute((0, 2, 3, 1))\n+        images = images.permute((0, 2, 3, 1))\n \n-    batch_size = image.shape[0]\n+    batch_size = images.shape[0]\n \n     # get indices\n     meshgrid = torch.meshgrid(\n         *[\n             torch.arange(size, dtype=transform.dtype, device=transform.device)\n-            for size in image.shape[1:]\n+            for size in images.shape[1:]\n         ],\n         indexing=\"ij\",\n     )\n@@ -304,13 +286,13 @@ def affine_transform(\n     affined = torch.stack(\n         [\n             map_coordinates(\n-                image[i],\n+                images[i],\n                 coordinates[i],\n                 order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n                 fill_mode=fill_mode,\n                 fill_value=fill_value,\n             )\n-            for i in range(len(image))\n+            for i in range(len(images))\n         ],\n     )\n \n@@ -366,17 +348,33 @@ def _linear_indices_and_weights(coordinate):\n \n \n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n ):\n-    input_arr = convert_to_tensor(input)\n+    input_arr = convert_to_tensor(inputs)\n     coordinate_arrs = [convert_to_tensor(c) for c in coordinates]\n+\n+    if len(coordinate_arrs) != len(input_arr.shape):\n+        raise ValueError(\n+            \"First dim of `coordinates` must be the same as the rank of \"\n+            \"`inputs`. \"\n+            f\"Received inputs with shape: {input_arr.shape} and coordinate \"\n+            f\"leading dim of {len(coordinate_arrs)}\"\n+        )\n+    if len(coordinate_arrs[0].shape) < 1:\n+        dim = len(coordinate_arrs)\n+        shape = (dim,) + coordinate_arrs[0].shape\n+        raise ValueError(\n+            \"Invalid coordinates rank: expected at least rank 2.\"\n+            f\" Received input with shape: {shape}\"\n+        )\n+\n     # skip tensor creation as possible\n     if isinstance(fill_value, (int, float)) and _is_integer(input_arr):\n         fill_value = int(fill_value)\n \n     if len(coordinates) != len(input_arr.shape):\n         raise ValueError(\n-            \"coordinates must be a sequence of length input.shape, but \"\n+            \"coordinates must be a sequence of length inputs.shape, but \"\n             f\"{len(coordinates)} != {len(input_arr.shape)}\"\n         )\n \n\n@@ -365,6 +365,7 @@ class FeatureSpaceTest(testing.TestCase):\n         ds = self._get_train_data_dict(as_dataset=True)\n         model.predict(ds.batch(4))\n \n+    @pytest.mark.requires_trainable_backend\n     def test_tf_data_async_processing(self):\n         fs = feature_space.FeatureSpace(\n             features={\n\n@@ -226,7 +226,7 @@ class RandomRotation(TFDataLayer):\n         if training:\n             rotation_matrix = self._get_rotation_matrix(inputs)\n             transformed_image = self.backend.image.affine_transform(\n-                image=inputs,\n+                images=inputs,\n                 transform=rotation_matrix,\n                 interpolation=self.interpolation,\n                 fill_mode=self.fill_mode,\n\n@@ -12,52 +12,35 @@ class RGBToGrayscale(Operation):\n         super().__init__()\n         self.data_format = backend.standardize_data_format(data_format)\n \n-    def call(self, image):\n+    def call(self, images):\n         return backend.image.rgb_to_grayscale(\n-            image, data_format=self.data_format\n+            images, data_format=self.data_format\n         )\n \n-    def compute_output_spec(self, image):\n-        if len(image.shape) not in (3, 4):\n+    def compute_output_spec(self, images):\n+        images_shape = list(images.shape)\n+        if len(images_shape) not in (3, 4):\n             raise ValueError(\n-                \"Invalid image rank: expected rank 3 (single image) \"\n-                \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+                \"Invalid images rank: expected rank 3 (single image) \"\n+                \"or rank 4 (batch of images). \"\n+                f\"Received: images.shape={images_shape}\"\n             )\n-\n-        if len(image.shape) == 3:\n         if self.data_format == \"channels_last\":\n-                return KerasTensor(image.shape[:-1] + (1,), dtype=image.dtype)\n+            images_shape[-1] = 1\n         else:\n-                return KerasTensor((1,) + image.shape[1:], dtype=image.dtype)\n-        elif len(image.shape) == 4:\n-            if self.data_format == \"channels_last\":\n-                return KerasTensor(\n-                    (image.shape[0],) + image.shape[1:-1] + (1,),\n-                    dtype=image.dtype,\n-                )\n-            else:\n-                return KerasTensor(\n-                    (\n-                        image.shape[0],\n-                        1,\n-                    )\n-                    + image.shape[2:],\n-                    dtype=image.dtype,\n-                )\n+            images_shape[-3] = 1\n+        return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.rgb_to_grayscale\")\n-def rgb_to_grayscale(image, data_format=None):\n+def rgb_to_grayscale(images, data_format=None):\n     \"\"\"Convert RGB images to grayscale.\n \n     This function converts RGB images to grayscale images. It supports both\n     3D and 4D tensors, where the last dimension represents channels.\n \n     Args:\n-        image: Input RGB image or batch of RGB images. Must be a 3D tensor\n-            with shape `(height, width, channels)` or a 4D tensor with shape\n-            `(batch, height, width, channels)`.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         data_format: A string specifying the data format of the input tensor.\n             It can be either `\"channels_last\"` or `\"channels_first\"`.\n             `\"channels_last\"` corresponds to inputs with shape\n@@ -88,14 +71,9 @@ def rgb_to_grayscale(image, data_format=None):\n     >>> y.shape\n     (2, 1, 4, 4)\n     \"\"\"\n-    if any_symbolic_tensors((image,)):\n-        return RGBToGrayscale(\n-            data_format=data_format,\n-        ).symbolic_call(image)\n-    return backend.image.rgb_to_grayscale(\n-        image,\n-        data_format=data_format,\n-    )\n+    if any_symbolic_tensors((images,)):\n+        return RGBToGrayscale(data_format=data_format).symbolic_call(images)\n+    return backend.image.rgb_to_grayscale(images, data_format=data_format)\n \n \n class Resize(Operation):\n@@ -120,9 +98,9 @@ class Resize(Operation):\n         self.fill_value = fill_value\n         self.data_format = backend.standardize_data_format(data_format)\n \n-    def call(self, image):\n+    def call(self, images):\n         return backend.image.resize(\n-            image,\n+            images,\n             self.size,\n             interpolation=self.interpolation,\n             antialias=self.antialias,\n@@ -133,32 +111,26 @@ class Resize(Operation):\n             fill_value=self.fill_value,\n         )\n \n-    def compute_output_spec(self, image):\n-        if len(image.shape) == 3:\n-            return KerasTensor(\n-                self.size + (image.shape[-1],), dtype=image.dtype\n-            )\n-        elif len(image.shape) == 4:\n-            if self.data_format == \"channels_last\":\n-                return KerasTensor(\n-                    (image.shape[0],) + self.size + (image.shape[-1],),\n-                    dtype=image.dtype,\n-                )\n-            else:\n-                return KerasTensor(\n-                    (image.shape[0], image.shape[1]) + self.size,\n-                    dtype=image.dtype,\n-                )\n+    def compute_output_spec(self, images):\n+        images_shape = list(images.shape)\n+        if len(images_shape) not in (3, 4):\n             raise ValueError(\n-            \"Invalid input rank: expected rank 3 (single image) \"\n+                \"Invalid images rank: expected rank 3 (single image) \"\n                 \"or rank 4 (batch of images). Received input with shape: \"\n-            f\"image.shape={image.shape}\"\n+                f\"images.shape={images.shape}\"\n             )\n+        if self.data_format == \"channels_last\":\n+            height_axis, width_axis = -3, -2\n+        else:\n+            height_axis, width_axis = -2, -1\n+        images_shape[height_axis] = self.size[0]\n+        images_shape[width_axis] = self.size[1]\n+        return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.resize\")\n def resize(\n-    image,\n+    images,\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n@@ -171,7 +143,7 @@ def resize(\n     \"\"\"Resize images to size using the specified interpolation method.\n \n     Args:\n-        image: Input image or batch of images. Must be 3D or 4D.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         size: Size of output image in `(height, width)` format.\n         interpolation: Interpolation method. Available methods are `\"nearest\"`,\n             `\"bilinear\"`, and `\"bicubic\"`. Defaults to `\"bilinear\"`.\n@@ -227,18 +199,18 @@ def resize(\n             \"Expected `size` to be a tuple of 2 integers. \"\n             f\"Received: size={size}\"\n         )\n-    if len(image.shape) < 3 or len(image.shape) > 4:\n+    if len(images.shape) < 3 or len(images.shape) > 4:\n         raise ValueError(\n-            \"Expected an image array with shape `(height, width, \"\n-            \"channels)`, or `(batch_size, height, width, channels)`, but \"\n-            f\"got input with incorrect rank, of shape {image.shape}.\"\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n         )\n     if pad_to_aspect_ratio and crop_to_aspect_ratio:\n         raise ValueError(\n             \"Only one of `pad_to_aspect_ratio` & `crop_to_aspect_ratio` \"\n             \"can be `True`.\"\n         )\n-    if any_symbolic_tensors((image,)):\n+    if any_symbolic_tensors((images,)):\n         return Resize(\n             size,\n             interpolation=interpolation,\n@@ -248,9 +220,9 @@ def resize(\n             pad_to_aspect_ratio=pad_to_aspect_ratio,\n             fill_mode=fill_mode,\n             fill_value=fill_value,\n-        ).symbolic_call(image)\n+        ).symbolic_call(images)\n     return backend.image.resize(\n-        image,\n+        images,\n         size,\n         interpolation=interpolation,\n         antialias=antialias,\n@@ -276,9 +248,9 @@ class AffineTransform(Operation):\n         self.fill_value = fill_value\n         self.data_format = backend.standardize_data_format(data_format)\n \n-    def call(self, image, transform):\n+    def call(self, images, transform):\n         return backend.image.affine_transform(\n-            image,\n+            images,\n             transform,\n             interpolation=self.interpolation,\n             fill_mode=self.fill_mode,\n@@ -286,12 +258,12 @@ class AffineTransform(Operation):\n             data_format=self.data_format,\n         )\n \n-    def compute_output_spec(self, image, transform):\n-        if len(image.shape) not in (3, 4):\n+    def compute_output_spec(self, images, transform):\n+        if len(images.shape) not in (3, 4):\n             raise ValueError(\n-                \"Invalid image rank: expected rank 3 (single image) \"\n+                \"Invalid images rank: expected rank 3 (single image) \"\n                 \"or rank 4 (batch of images). Received input with shape: \"\n-                f\"image.shape={image.shape}\"\n+                f\"images.shape={images.shape}\"\n             )\n         if len(transform.shape) not in (1, 2):\n             raise ValueError(\n@@ -299,12 +271,12 @@ class AffineTransform(Operation):\n                 \"or rank 2 (batch of transforms). Received input with shape: \"\n                 f\"transform.shape={transform.shape}\"\n             )\n-        return KerasTensor(image.shape, dtype=image.dtype)\n+        return KerasTensor(images.shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.affine_transform\")\n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n@@ -314,7 +286,7 @@ def affine_transform(\n     \"\"\"Applies the given transform(s) to the image(s).\n \n     Args:\n-        image: Input image or batch of images. Must be 3D or 4D.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         transform: Projective transform matrix/matrices. A vector of length 8 or\n             tensor of size N x 8. If one row of transform is\n             `[a0, a1, a2, b0, b1, b2, c0, c1]`, then it maps the output point\n@@ -385,15 +357,15 @@ def affine_transform(\n     >>> y.shape\n     (2, 3, 64, 80)\n     \"\"\"\n-    if any_symbolic_tensors((image, transform)):\n+    if any_symbolic_tensors((images, transform)):\n         return AffineTransform(\n             interpolation=interpolation,\n             fill_mode=fill_mode,\n             fill_value=fill_value,\n             data_format=data_format,\n-        ).symbolic_call(image, transform)\n+        ).symbolic_call(images, transform)\n     return backend.image.affine_transform(\n-        image,\n+        images,\n         transform,\n         interpolation=interpolation,\n         fill_mode=fill_mode,\n@@ -420,9 +392,9 @@ class ExtractPatches(Operation):\n         self.padding = padding\n         self.data_format = backend.standardize_data_format(data_format)\n \n-    def call(self, image):\n+    def call(self, images):\n         return _extract_patches(\n-            image=image,\n+            images=images,\n             size=self.size,\n             strides=self.strides,\n             dilation_rate=self.dilation_rate,\n@@ -430,20 +402,21 @@ class ExtractPatches(Operation):\n             data_format=self.data_format,\n         )\n \n-    def compute_output_spec(self, image):\n-        image_shape = image.shape\n+    def compute_output_spec(self, images):\n+        images_shape = list(images.shape)\n+        original_ndim = len(images_shape)\n         if not self.strides:\n             strides = (self.size[0], self.size[1])\n         if self.data_format == \"channels_last\":\n-            channels_in = image.shape[-1]\n+            channels_in = images_shape[-1]\n         else:\n-            channels_in = image.shape[-3]\n-        if len(image.shape) == 3:\n-            image_shape = (1,) + image_shape\n+            channels_in = images_shape[-3]\n+        if original_ndim == 3:\n+            images_shape = [1] + images_shape\n         filters = self.size[0] * self.size[1] * channels_in\n         kernel_size = (self.size[0], self.size[1])\n         out_shape = compute_conv_output_shape(\n-            image_shape,\n+            images_shape,\n             filters,\n             kernel_size,\n             strides=strides,\n@@ -451,14 +424,14 @@ class ExtractPatches(Operation):\n             data_format=self.data_format,\n             dilation_rate=self.dilation_rate,\n         )\n-        if len(image.shape) == 3:\n+        if original_ndim == 3:\n             out_shape = out_shape[1:]\n-        return KerasTensor(shape=out_shape, dtype=image.dtype)\n+        return KerasTensor(shape=out_shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.extract_patches\")\n def extract_patches(\n-    image,\n+    images,\n     size,\n     strides=None,\n     dilation_rate=1,\n@@ -468,7 +441,7 @@ def extract_patches(\n     \"\"\"Extracts patches from the image(s).\n \n     Args:\n-        image: Input image or batch of images. Must be 3D or 4D.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         size: Patch size int or tuple (patch_height, patch_widht)\n         strides: strides along height and width. If not specified, or\n             if `None`, it defaults to the same value as `size`.\n@@ -501,22 +474,22 @@ def extract_patches(\n     >>> patches.shape\n     (18, 18, 27)\n     \"\"\"\n-    if any_symbolic_tensors((image,)):\n+    if any_symbolic_tensors((images,)):\n         return ExtractPatches(\n             size=size,\n             strides=strides,\n             dilation_rate=dilation_rate,\n             padding=padding,\n             data_format=data_format,\n-        ).symbolic_call(image)\n+        ).symbolic_call(images)\n \n     return _extract_patches(\n-        image, size, strides, dilation_rate, padding, data_format=data_format\n+        images, size, strides, dilation_rate, padding, data_format=data_format\n     )\n \n \n def _extract_patches(\n-    image,\n+    images,\n     size,\n     strides=None,\n     dilation_rate=1,\n@@ -534,22 +507,22 @@ def _extract_patches(\n         )\n     data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_last\":\n-        channels_in = image.shape[-1]\n+        channels_in = images.shape[-1]\n     elif data_format == \"channels_first\":\n-        channels_in = image.shape[-3]\n+        channels_in = images.shape[-3]\n     if not strides:\n         strides = size\n     out_dim = patch_h * patch_w * channels_in\n-    kernel = backend.numpy.eye(out_dim, dtype=image.dtype)\n+    kernel = backend.numpy.eye(out_dim, dtype=images.dtype)\n     kernel = backend.numpy.reshape(\n         kernel, (patch_h, patch_w, channels_in, out_dim)\n     )\n     _unbatched = False\n-    if len(image.shape) == 3:\n+    if len(images.shape) == 3:\n         _unbatched = True\n-        image = backend.numpy.expand_dims(image, axis=0)\n+        images = backend.numpy.expand_dims(images, axis=0)\n     patches = backend.nn.conv(\n-        inputs=image,\n+        inputs=images,\n         kernel=kernel,\n         strides=strides,\n         padding=padding,\n@@ -562,27 +535,27 @@ def _extract_patches(\n \n \n class MapCoordinates(Operation):\n-    def __init__(self, order=1, fill_mode=\"constant\", fill_value=0):\n+    def __init__(self, order, fill_mode=\"constant\", fill_value=0):\n         super().__init__()\n         self.order = order\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n \n-    def call(self, image, coordinates):\n+    def call(self, inputs, coordinates):\n         return backend.image.map_coordinates(\n-            image,\n+            inputs,\n             coordinates,\n             order=self.order,\n             fill_mode=self.fill_mode,\n             fill_value=self.fill_value,\n         )\n \n-    def compute_output_spec(self, image, coordinates):\n-        if coordinates.shape[0] != len(image.shape):\n+    def compute_output_spec(self, inputs, coordinates):\n+        if coordinates.shape[0] != len(inputs.shape):\n             raise ValueError(\n                 \"First dim of `coordinates` must be the same as the rank of \"\n-                \"`image`. \"\n-                f\"Received image with shape: {image.shape} and coordinate \"\n+                \"`inputs`. \"\n+                f\"Received inputs with shape: {inputs.shape} and coordinate \"\n                 f\"leading dim of {coordinates.shape[0]}\"\n             )\n         if len(coordinates.shape) < 2:\n@@ -590,57 +563,57 @@ class MapCoordinates(Operation):\n                 \"Invalid coordinates rank: expected at least rank 2.\"\n                 f\" Received input with shape: {coordinates.shape}\"\n             )\n-        return KerasTensor(coordinates.shape[1:], dtype=image.dtype)\n+        return KerasTensor(coordinates.shape[1:], dtype=inputs.dtype)\n \n \n @keras_export(\"keras.ops.image.map_coordinates\")\n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0\n ):\n-    \"\"\"Map the input array to new coordinates by interpolation..\n+    \"\"\"Map the input array to new coordinates by interpolation.\n \n     Note that interpolation near boundaries differs from the scipy function,\n     because we fixed an outstanding bug\n     [scipy/issues/2640](https://github.com/scipy/scipy/issues/2640).\n \n     Args:\n-        input: The input array.\n-        coordinates: The coordinates at which input is evaluated.\n+        inputs: The input array.\n+        coordinates: The coordinates at which inputs is evaluated.\n         order: The order of the spline interpolation. The order must be `0` or\n             `1`. `0` indicates the nearest neighbor and `1` indicates the linear\n             interpolation.\n-        fill_mode: Points outside the boundaries of the input are filled\n+        fill_mode: Points outside the boundaries of the inputs are filled\n             according to the given mode. Available methods are `\"constant\"`,\n             `\"nearest\"`, `\"wrap\"` and `\"mirror\"` and `\"reflect\"`. Defaults to\n             `\"constant\"`.\n             - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n-                The input is extended by filling all values beyond\n+                The inputs is extended by filling all values beyond\n                 the edge with the same constant value k specified by\n                 `fill_value`.\n             - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n-                The input is extended by the nearest pixel.\n+                The inputs is extended by the nearest pixel.\n             - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n-                The input is extended by wrapping around to the opposite edge.\n+                The inputs is extended by wrapping around to the opposite edge.\n             - `\"mirror\"`: `(c d c b | a b c d | c b a b)`\n-                The input is extended by mirroring about the edge.\n+                The inputs is extended by mirroring about the edge.\n             - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n-                The input is extended by reflecting about the edge of the last\n+                The inputs is extended by reflecting about the edge of the last\n                 pixel.\n-        fill_value: Value used for points outside the boundaries of the input if\n-            `fill_mode=\"constant\"`. Defaults to `0`.\n+        fill_value: Value used for points outside the boundaries of the inputs\n+            if `fill_mode=\"constant\"`. Defaults to `0`.\n \n     Returns:\n-        Output image or batch of images.\n+        Output input or batch of inputs.\n \n     \"\"\"\n-    if any_symbolic_tensors((input, coordinates)):\n+    if any_symbolic_tensors((inputs, coordinates)):\n         return MapCoordinates(\n             order,\n             fill_mode,\n             fill_value,\n-        ).symbolic_call(input, coordinates)\n+        ).symbolic_call(inputs, coordinates)\n     return backend.image.map_coordinates(\n-        input,\n+        inputs,\n         coordinates,\n         order,\n         fill_mode,\n@@ -652,8 +625,8 @@ class PadImages(Operation):\n     def __init__(\n         self,\n         top_padding=None,\n-        bottom_padding=None,\n         left_padding=None,\n+        bottom_padding=None,\n         right_padding=None,\n         target_height=None,\n         target_width=None,\n@@ -661,8 +634,8 @@ class PadImages(Operation):\n     ):\n         super().__init__()\n         self.top_padding = top_padding\n-        self.bottom_padding = bottom_padding\n         self.left_padding = left_padding\n+        self.bottom_padding = bottom_padding\n         self.right_padding = right_padding\n         self.target_height = target_height\n         self.target_width = target_width\n@@ -672,8 +645,8 @@ class PadImages(Operation):\n         return _pad_images(\n             images,\n             self.top_padding,\n-            self.bottom_padding,\n             self.left_padding,\n+            self.bottom_padding,\n             self.right_padding,\n             self.target_height,\n             self.target_width,\n@@ -690,13 +663,15 @@ class PadImages(Operation):\n             height_axis, width_axis = -2, -1\n             height, width = images_shape[height_axis], images_shape[width_axis]\n \n-        if self.target_height is None and height is not None:\n-            self.target_height = self.top_padding + height + self.bottom_padding\n-        if self.target_width is None and width is not None:\n-            self.target_width = self.left_padding + width + self.right_padding\n+        target_height = self.target_height\n+        if target_height is None and height is not None:\n+            target_height = self.top_padding + height + self.bottom_padding\n+        target_width = self.target_width\n+        if target_width is None and width is not None:\n+            target_width = self.left_padding + width + self.right_padding\n \n-        images_shape[height_axis] = self.target_height\n-        images_shape[width_axis] = self.target_width\n+        images_shape[height_axis] = target_height\n+        images_shape[width_axis] = target_width\n         return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n@@ -714,11 +689,10 @@ def pad_images(\n     \"\"\"Pad `images` with zeros to the specified `height` and `width`.\n \n     Args:\n-        images: 4D Tensor of shape `(batch, height, width, channels)` or 3D\n-            Tensor of shape `(height, width, channels)`.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         top_padding: Number of rows of zeros to add on top.\n-        bottom_padding: Number of rows of zeros to add at the bottom.\n         left_padding: Number of columns of zeros to add on the left.\n+        bottom_padding: Number of rows of zeros to add at the bottom.\n         right_padding: Number of columns of zeros to add on the right.\n         target_height: Height of output images.\n         target_width: Width of output images.\n@@ -752,8 +726,8 @@ def pad_images(\n     if any_symbolic_tensors((images,)):\n         return PadImages(\n             top_padding,\n-            bottom_padding,\n             left_padding,\n+            bottom_padding,\n             right_padding,\n             target_height,\n             target_width,\n@@ -763,8 +737,8 @@ def pad_images(\n     return _pad_images(\n         images,\n         top_padding,\n-        bottom_padding,\n         left_padding,\n+        bottom_padding,\n         right_padding,\n         target_height,\n         target_width,\n@@ -775,8 +749,8 @@ def pad_images(\n def _pad_images(\n     images,\n     top_padding,\n-    bottom_padding,\n     left_padding,\n+    bottom_padding,\n     right_padding,\n     target_height,\n     target_width,\n@@ -863,8 +837,8 @@ class CropImages(Operation):\n     def __init__(\n         self,\n         top_cropping,\n-        bottom_cropping,\n         left_cropping,\n+        bottom_cropping,\n         right_cropping,\n         target_height,\n         target_width,\n@@ -883,8 +857,8 @@ class CropImages(Operation):\n         return _crop_images(\n             images,\n             self.top_cropping,\n-            self.bottom_cropping,\n             self.left_cropping,\n+            self.bottom_cropping,\n             self.right_cropping,\n             self.target_height,\n             self.target_width,\n@@ -896,7 +870,6 @@ class CropImages(Operation):\n \n         if self.data_format == \"channels_last\":\n             height_axis, width_axis = -3, -2\n-            height, width = images_shape[height_axis], images_shape[width_axis]\n         else:\n             height_axis, width_axis = -2, -1\n         height, width = images_shape[height_axis], images_shape[width_axis]\n@@ -916,15 +889,15 @@ class CropImages(Operation):\n                 f\"target_width={self.target_width}\"\n             )\n \n-        if self.target_height is None:\n-            self.target_height = (\n-                height - self.top_cropping - self.bottom_cropping\n-            )\n-        if self.target_width is None:\n-            self.target_width = width - self.left_cropping - self.right_cropping\n+        target_height = self.target_height\n+        if target_height is None:\n+            target_height = height - self.top_cropping - self.bottom_cropping\n+        target_width = self.target_width\n+        if target_width is None:\n+            target_width = width - self.left_cropping - self.right_cropping\n \n-        images_shape[height_axis] = self.target_height\n-        images_shape[width_axis] = self.target_width\n+        images_shape[height_axis] = target_height\n+        images_shape[width_axis] = target_width\n         return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n@@ -942,11 +915,10 @@ def crop_images(\n     \"\"\"Crop `images` to a specified `height` and `width`.\n \n     Args:\n-        images: 4-D batch of images of shape `(batch, height, width, channels)`\n-             or 3-D single image of shape `(height, width, channels)`.\n+        images: Input image or batch of images. Must be 3D or 4D.\n         top_cropping: Number of columns to crop from the top.\n-        bottom_cropping: Number of columns to crop from the bottom.\n         left_cropping: Number of columns to crop from the left.\n+        bottom_cropping: Number of columns to crop from the bottom.\n         right_cropping: Number of columns to crop from the right.\n         target_height: Height of the output images.\n         target_width: Width of the output images.\n@@ -976,8 +948,8 @@ def crop_images(\n     if any_symbolic_tensors((images,)):\n         return CropImages(\n             top_cropping,\n-            bottom_cropping,\n             left_cropping,\n+            bottom_cropping,\n             right_cropping,\n             target_height,\n             target_width,\n@@ -987,8 +959,8 @@ def crop_images(\n     return _crop_images(\n         images,\n         top_cropping,\n-        bottom_cropping,\n         left_cropping,\n+        bottom_cropping,\n         right_cropping,\n         target_height,\n         target_width,\n@@ -999,8 +971,8 @@ def crop_images(\n def _crop_images(\n     images,\n     top_cropping,\n-    bottom_cropping,\n     left_cropping,\n+    bottom_cropping,\n     right_cropping,\n     target_height,\n     target_width,\n\n@@ -10,6 +10,7 @@ from keras.src import backend\n from keras.src import testing\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.ops import image as kimage\n+from keras.src.testing.test_utils import named_product\n \n \n class ImageOpsDynamicShapeTest(testing.TestCase):\n@@ -24,11 +25,19 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         return super().tearDown()\n \n     def test_rgb_to_grayscale(self):\n+        # Test channels_last\n         x = KerasTensor([None, 20, 20, 3])\n         out = kimage.rgb_to_grayscale(x)\n         self.assertEqual(out.shape, (None, 20, 20, 1))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        out = kimage.rgb_to_grayscale(x)\n+        self.assertEqual(out.shape, (None, 1, 20, 20))\n+\n     def test_resize(self):\n+        # Test channels_last\n         x = KerasTensor([None, 20, 20, 3])\n         out = kimage.resize(x, size=(15, 15))\n         self.assertEqual(out.shape, (None, 15, 15, 3))\n@@ -37,13 +46,32 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.resize(x, size=(15, 15))\n         self.assertEqual(out.shape, (15, 15, 3))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        out = kimage.resize(x, size=(15, 15))\n+        self.assertEqual(out.shape, (None, 3, 15, 15))\n+\n+        x = KerasTensor([3, None, None])\n+        out = kimage.resize(x, size=(15, 15))\n+        self.assertEqual(out.shape, (3, 15, 15))\n+\n     def test_affine_transform(self):\n+        # Test channels_last\n         x = KerasTensor([None, 20, 20, 3])\n         transform = KerasTensor([None, 8])\n         out = kimage.affine_transform(x, transform)\n         self.assertEqual(out.shape, (None, 20, 20, 3))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        transform = KerasTensor([None, 8])\n+        out = kimage.affine_transform(x, transform)\n+        self.assertEqual(out.shape, (None, 3, 20, 20))\n+\n     def test_extract_patches(self):\n+        # Test channels_last\n         x = KerasTensor([None, 20, 20, 3])\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n@@ -51,6 +79,15 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.extract_patches(x, 5)\n         self.assertEqual(out.shape, (None, 4, 4, 75))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        p_h, p_w = 5, 5\n+        out = kimage.extract_patches(x, (p_h, p_w))\n+        self.assertEqual(out.shape, (None, 75, 4, 4))\n+        out = kimage.extract_patches(x, 5)\n+        self.assertEqual(out.shape, (None, 75, 4, 4))\n+\n     def test_map_coordinates(self):\n         input = KerasTensor([20, 20, None])\n         coordinates = KerasTensor([3, 15, 15, None])\n@@ -58,6 +95,7 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(out.shape, coordinates.shape[1:])\n \n     def test_pad_images(self):\n+        # Test channels_last\n         x = KerasTensor([None, 15, 25, 3])\n         out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (None, 20, 30, 3))\n@@ -71,31 +109,18 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.pad_images(x, 2, 3, 2, 3)\n         self.assertEqual(out.shape, (None, None, 3))\n \n-        # Test channels_first and batched\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n         x = KerasTensor([None, 3, 15, 25])\n-        out = kimage.pad_images(\n-            x,\n-            2,\n-            3,\n-            target_height=20,\n-            target_width=30,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (None, 3, 20, 30))\n \n-        # Test channels_first and unbatched\n         x = KerasTensor([3, None, None])\n-        out = kimage.pad_images(\n-            x,\n-            2,\n-            3,\n-            target_height=20,\n-            target_width=30,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (3, 20, 30))\n \n     def test_crop_images(self):\n+        # Test channels_last\n         x = KerasTensor([None, 15, 25, 3])\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (None, 10, 20, 3))\n@@ -104,28 +129,14 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (10, 20, 3))\n \n-        # Test channels_first and batched\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n         x = KerasTensor([None, 3, 15, 25])\n-        out = kimage.crop_images(\n-            x,\n-            2,\n-            3,\n-            target_height=10,\n-            target_width=20,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (None, 3, 10, 20))\n \n-        # Test channels_first and unbatched\n         x = KerasTensor([3, None, None])\n-        out = kimage.crop_images(\n-            x,\n-            2,\n-            3,\n-            target_height=10,\n-            target_width=20,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (3, 10, 20))\n \n \n@@ -141,22 +152,45 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         return super().tearDown()\n \n     def test_rgb_to_grayscale(self):\n+        # Test channels_last\n         x = KerasTensor([20, 20, 3])\n         out = kimage.rgb_to_grayscale(x)\n         self.assertEqual(out.shape, (20, 20, 1))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        out = kimage.rgb_to_grayscale(x)\n+        self.assertEqual(out.shape, (1, 20, 20))\n+\n     def test_resize(self):\n+        # Test channels_last\n         x = KerasTensor([20, 20, 3])\n         out = kimage.resize(x, size=(15, 15))\n         self.assertEqual(out.shape, (15, 15, 3))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        out = kimage.resize(x, size=(15, 15))\n+        self.assertEqual(out.shape, (3, 15, 15))\n+\n     def test_affine_transform(self):\n+        # Test channels_last\n         x = KerasTensor([20, 20, 3])\n         transform = KerasTensor([8])\n         out = kimage.affine_transform(x, transform)\n         self.assertEqual(out.shape, (20, 20, 3))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        transform = KerasTensor([8])\n+        out = kimage.affine_transform(x, transform)\n+        self.assertEqual(out.shape, (3, 20, 20))\n+\n     def test_extract_patches(self):\n+        # Test channels_last\n         x = KerasTensor([20, 20, 3])\n         p_h, p_w = 5, 5\n         out = kimage.extract_patches(x, (p_h, p_w))\n@@ -164,6 +198,15 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         out = kimage.extract_patches(x, 5)\n         self.assertEqual(out.shape, (4, 4, 75))\n \n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        p_h, p_w = 5, 5\n+        out = kimage.extract_patches(x, (p_h, p_w))\n+        self.assertEqual(out.shape, (75, 4, 4))\n+        out = kimage.extract_patches(x, 5)\n+        self.assertEqual(out.shape, (75, 4, 4))\n+\n     def test_map_coordinates(self):\n         input = KerasTensor([20, 20, 3])\n         coordinates = KerasTensor([3, 15, 15, 3])\n@@ -171,6 +214,7 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(out.shape, coordinates.shape[1:])\n \n     def test_pad_images(self):\n+        # Test channels_last\n         x = KerasTensor([15, 25, 3])\n         out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (20, 30, 3))\n@@ -181,31 +225,20 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(out_batch.shape, (2, 20, 30, 3))\n \n-        # Test channels_first and unbatched\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n         x = KerasTensor([3, 15, 25])\n-        out = kimage.pad_images(\n-            x,\n-            2,\n-            3,\n-            target_height=20,\n-            target_width=30,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (3, 20, 30))\n \n-        # Test channels_first and batched\n         x_batch = KerasTensor([2, 3, 15, 25])\n         out_batch = kimage.pad_images(\n-            x_batch,\n-            2,\n-            3,\n-            target_height=20,\n-            target_width=30,\n-            data_format=\"channels_first\",\n+            x_batch, 2, 3, target_height=20, target_width=30\n         )\n         self.assertEqual(out_batch.shape, (2, 3, 20, 30))\n \n     def test_crop_images(self):\n+        # Test channels_last\n         x = KerasTensor([15, 25, 3])\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (10, 20, 3))\n@@ -216,27 +249,16 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(out_batch.shape, (2, 10, 20, 3))\n \n-        # Test channels_first and unbatched\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n         x = KerasTensor([3, 15, 25])\n-        out = kimage.crop_images(\n-            x,\n-            2,\n-            3,\n-            target_height=10,\n-            target_width=20,\n-            data_format=\"channels_first\",\n-        )\n+        out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (3, 10, 20))\n \n         # Test channels_first and batched\n         x_batch = KerasTensor([2, 3, 15, 25])\n         out_batch = kimage.crop_images(\n-            x_batch,\n-            2,\n-            3,\n-            target_height=10,\n-            target_width=20,\n-            data_format=\"channels_first\",\n+            x_batch, 2, 3, target_height=10, target_width=20\n         )\n         self.assertEqual(out_batch.shape, (2, 3, 10, 20))\n \n@@ -248,6 +270,8 @@ AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n \n \n def _compute_affine_transform_coordinates(image, transform):\n+    image = image.copy()\n+    transform = transform.copy()\n     need_squeeze = False\n     if len(image.shape) == 3:  # unbatched\n         need_squeeze = True\n@@ -328,67 +352,53 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         backend.set_image_data_format(self.data_format)\n         return super().tearDown()\n \n-    @parameterized.parameters(\n-        [\n-            (\"channels_last\"),\n-            (\"channels_first\"),\n-        ]\n-    )\n-    def test_rgb_to_grayscale(self, data_format):\n-        # Unbatched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((3, 50, 50)) * 255\n-        else:\n-            x = np.random.random((50, 50, 3)) * 255\n-        out = kimage.rgb_to_grayscale(\n-            x,\n-            data_format=data_format,\n-        )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (1, 2, 0))\n-        ref_out = tf.image.rgb_to_grayscale(\n-            x,\n-        )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (2, 0, 1))\n+    def test_rgb_to_grayscale(self):\n+        # Test channels_last\n+        x = np.random.random((50, 50, 3)).astype(\"float32\") * 255\n+        out = kimage.rgb_to_grayscale(x)\n+        ref_out = tf.image.rgb_to_grayscale(x)\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n-        # Batched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((2, 3, 50, 50)) * 255\n-        else:\n-            x = np.random.random((2, 50, 50, 3)) * 255\n-        out = kimage.rgb_to_grayscale(\n-            x,\n-            data_format=data_format,\n-        )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (0, 2, 3, 1))\n-        ref_out = tf.image.rgb_to_grayscale(\n-            x,\n-        )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (0, 3, 1, 2))\n+        x = np.random.random((2, 50, 50, 3)).astype(\"float32\") * 255\n+        out = kimage.rgb_to_grayscale(x)\n+        ref_out = tf.image.rgb_to_grayscale(x)\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n-    @parameterized.parameters(\n-        [\n-            (\"bilinear\", True, \"channels_last\"),\n-            (\"nearest\", True, \"channels_last\"),\n-            (\"lanczos3\", True, \"channels_last\"),\n-            (\"lanczos5\", True, \"channels_last\"),\n-            (\"bicubic\", True, \"channels_last\"),\n-            (\"bilinear\", False, \"channels_last\"),\n-            (\"nearest\", False, \"channels_last\"),\n-            (\"lanczos3\", False, \"channels_last\"),\n-            (\"lanczos5\", False, \"channels_last\"),\n-            (\"bicubic\", False, \"channels_last\"),\n-            (\"bilinear\", True, \"channels_first\"),\n-        ]\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.random((3, 50, 50)).astype(\"float32\") * 255\n+        out = kimage.rgb_to_grayscale(x)\n+        ref_out = tf.image.rgb_to_grayscale(np.transpose(x, [1, 2, 0]))\n+        ref_out = tf.transpose(ref_out, [2, 0, 1])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+        x = np.random.random((2, 3, 50, 50)).astype(\"float32\") * 255\n+        out = kimage.rgb_to_grayscale(x)\n+        ref_out = tf.image.rgb_to_grayscale(np.transpose(x, [0, 2, 3, 1]))\n+        ref_out = tf.transpose(ref_out, [0, 3, 1, 2])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+        # Test class\n+        out = kimage.RGBToGrayscale()(x)\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            interpolation=[\n+                \"bilinear\",\n+                \"nearest\",\n+                \"lanczos3\",\n+                \"lanczos5\",\n+                \"bicubic\",\n+            ],\n+            antialias=[True, False],\n         )\n-    def test_resize(self, interpolation, antialias, data_format):\n+    )\n+    def test_resize(self, interpolation, antialias):\n         if backend.backend() == \"torch\":\n             if \"lanczos\" in interpolation:\n                 self.skipTest(\n@@ -404,144 +414,142 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                     f\"Received: interpolation={interpolation}, \"\n                     f\"antialias={antialias}.\"\n                 )\n-        # Unbatched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((3, 50, 50)) * 255\n-        else:\n-            x = np.random.random((50, 50, 3)) * 255\n+        # Test channels_last\n+        x = np.random.random((30, 30, 3)).astype(\"float32\") * 255\n         out = kimage.resize(\n             x,\n-            size=(25, 25),\n+            size=(15, 15),\n             interpolation=interpolation,\n             antialias=antialias,\n-            data_format=data_format,\n         )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (1, 2, 0))\n         ref_out = tf.image.resize(\n-            x, size=(25, 25), method=interpolation, antialias=antialias\n+            x,\n+            size=(15, 15),\n+            method=interpolation,\n+            antialias=antialias,\n         )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (2, 0, 1))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n-        # Batched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((2, 3, 50, 50)) * 255\n-        else:\n-            x = np.random.random((2, 50, 50, 3)) * 255\n+        x = np.random.random((2, 30, 30, 3)).astype(\"float32\") * 255\n         out = kimage.resize(\n             x,\n-            size=(25, 25),\n+            size=(15, 15),\n             interpolation=interpolation,\n             antialias=antialias,\n-            data_format=data_format,\n         )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (0, 2, 3, 1))\n         ref_out = tf.image.resize(\n-            x, size=(25, 25), method=interpolation, antialias=antialias\n+            x,\n+            size=(15, 15),\n+            method=interpolation,\n+            antialias=antialias,\n         )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (0, 3, 1, 2))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n-    @parameterized.parameters(\n-        [\n-            (\"channels_last\",),\n-            (\"channels_first\",),\n-        ]\n-    )\n-    def test_resize_with_crop(self, data_format):\n-        if data_format == \"channels_first\":\n-            x = np.random.random((3, 60, 50)) * 255\n-        else:\n-            x = np.random.random((60, 50, 3)) * 255\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.random((3, 30, 30)).astype(\"float32\") * 255\n         out = kimage.resize(\n             x,\n-            size=(25, 25),\n-            crop_to_aspect_ratio=True,\n-            data_format=data_format,\n+            size=(15, 15),\n+            interpolation=interpolation,\n+            antialias=antialias,\n         )\n-        if data_format == \"channels_first\":\n-            self.assertEqual(out.shape, (3, 25, 25))\n-        else:\n+        ref_out = tf.image.resize(\n+            np.transpose(x, [1, 2, 0]),\n+            size=(15, 15),\n+            method=interpolation,\n+            antialias=antialias,\n+        )\n+        ref_out = tf.transpose(ref_out, [2, 0, 1])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+        x = np.random.random((2, 3, 30, 30)).astype(\"float32\") * 255\n+        out = kimage.resize(\n+            x,\n+            size=(15, 15),\n+            interpolation=interpolation,\n+            antialias=antialias,\n+        )\n+        ref_out = tf.image.resize(\n+            np.transpose(x, [0, 2, 3, 1]),\n+            size=(15, 15),\n+            method=interpolation,\n+            antialias=antialias,\n+        )\n+        ref_out = tf.transpose(ref_out, [0, 3, 1, 2])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+        # Test class\n+        out = kimage.Resize(\n+            size=(15, 15),\n+            interpolation=interpolation,\n+            antialias=antialias,\n+        )(x)\n+        self.assertAllClose(ref_out, out, atol=0.3)\n+\n+    def test_resize_with_crop(self):\n+        # Test channels_last\n+        x = np.random.random((60, 50, 3)).astype(\"float32\") * 255\n+        out = kimage.resize(x, size=(25, 25), crop_to_aspect_ratio=True)\n         self.assertEqual(out.shape, (25, 25, 3))\n \n-        # Batched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((2, 3, 50, 60)) * 255\n-        else:\n-            x = np.random.random((2, 50, 60, 3)) * 255\n-        out = kimage.resize(\n-            x,\n-            size=(25, 25),\n-            crop_to_aspect_ratio=True,\n-            data_format=data_format,\n-        )\n-        if data_format == \"channels_first\":\n-            self.assertEqual(out.shape, (2, 3, 25, 25))\n-        else:\n+        x = np.random.random((2, 50, 60, 3)).astype(\"float32\") * 255\n+        out = kimage.resize(x, size=(25, 25), crop_to_aspect_ratio=True)\n         self.assertEqual(out.shape, (2, 25, 25, 3))\n \n-    @parameterized.parameters(\n-        [\n-            (\"channels_last\", 2.0),\n-            (\"channels_first\", 2.0),\n-        ]\n-    )\n-    def test_resize_with_pad(self, data_format, fill_value):\n-        if data_format == \"channels_first\":\n-            x = np.random.random((3, 60, 50)) * 255\n-        else:\n-            x = np.random.random((60, 50, 3)) * 255\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.random((3, 60, 50)).astype(\"float32\") * 255\n+        out = kimage.resize(x, size=(25, 25), crop_to_aspect_ratio=True)\n+        self.assertEqual(out.shape, (3, 25, 25))\n+\n+        x = np.random.random((2, 3, 50, 60)).astype(\"float32\") * 255\n+        out = kimage.resize(x, size=(25, 25), crop_to_aspect_ratio=True)\n+        self.assertEqual(out.shape, (2, 3, 25, 25))\n+\n+    @parameterized.named_parameters(named_product(fill_value=[1.0, 2.0]))\n+    def test_resize_with_pad(self, fill_value):\n+        # Test channels_last\n+        x = np.random.random((60, 50, 3)).astype(\"float32\") * 255\n         out = kimage.resize(\n             x,\n             size=(25, 25),\n             pad_to_aspect_ratio=True,\n-            data_format=data_format,\n             fill_value=fill_value,\n         )\n-        if data_format == \"channels_first\":\n-            self.assertEqual(out.shape, (3, 25, 25))\n-        else:\n         self.assertEqual(out.shape, (25, 25, 3))\n \n-        # Batched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((2, 3, 50, 60)) * 255\n-        else:\n-            x = np.random.random((2, 50, 60, 3)) * 255\n+        x = np.random.random((2, 50, 60, 3)).astype(\"float32\") * 255\n         out = kimage.resize(\n-            x,\n-            size=(25, 25),\n-            pad_to_aspect_ratio=True,\n-            data_format=data_format,\n-            fill_value=fill_value,\n+            x, size=(25, 25), pad_to_aspect_ratio=True, fill_value=fill_value\n         )\n-        if data_format == \"channels_first\":\n-            self.assertEqual(out.shape, (2, 3, 25, 25))\n-        else:\n         self.assertEqual(out.shape, (2, 25, 25, 3))\n \n-    @parameterized.parameters(\n-        [\n-            (\"bilinear\", \"constant\", \"channels_last\"),\n-            (\"nearest\", \"constant\", \"channels_last\"),\n-            (\"bilinear\", \"nearest\", \"channels_last\"),\n-            (\"nearest\", \"nearest\", \"channels_last\"),\n-            (\"bilinear\", \"wrap\", \"channels_last\"),\n-            (\"nearest\", \"wrap\", \"channels_last\"),\n-            (\"bilinear\", \"mirror\", \"channels_last\"),\n-            (\"nearest\", \"mirror\", \"channels_last\"),\n-            (\"bilinear\", \"reflect\", \"channels_last\"),\n-            (\"nearest\", \"reflect\", \"channels_last\"),\n-            (\"bilinear\", \"constant\", \"channels_first\"),\n-        ]\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.random((3, 60, 50)).astype(\"float32\") * 255\n+        out = kimage.resize(\n+            x, size=(25, 25), pad_to_aspect_ratio=True, fill_value=fill_value\n         )\n-    def test_affine_transform(self, interpolation, fill_mode, data_format):\n+        self.assertEqual(out.shape, (3, 25, 25))\n+\n+        x = np.random.random((2, 3, 50, 60)).astype(\"float32\") * 255\n+        out = kimage.resize(\n+            x, size=(25, 25), pad_to_aspect_ratio=True, fill_value=fill_value\n+        )\n+        self.assertEqual(out.shape, (2, 3, 25, 25))\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            interpolation=[\"bilinear\", \"nearest\"],\n+            fill_mode=[\"constant\", \"nearest\", \"wrap\", \"mirror\", \"reflect\"],\n+        )\n+    )\n+    def test_affine_transform(self, interpolation, fill_mode):\n         if backend.backend() == \"tensorflow\" and fill_mode == \"mirror\":\n             self.skipTest(\n                 \"In tensorflow backend, applying affine_transform with \"\n@@ -563,22 +571,14 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 \"leads test failure\"\n             )\n \n-        # Unbatched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((3, 50, 50)).astype(\"float32\") * 255\n-        else:\n-            x = np.random.random((50, 50, 3)).astype(\"float32\") * 255\n-        transform = np.random.random(size=(6)).astype(\"float32\")\n+        # Test channels_last\n+        np.random.seed(42)\n+        x = np.random.uniform(size=(50, 50, 3)).astype(\"float32\") * 255\n+        transform = np.random.uniform(size=(6)).astype(\"float32\")\n         transform = np.pad(transform, (0, 2))  # makes c0, c1 always 0\n         out = kimage.affine_transform(\n-            x,\n-            transform,\n-            interpolation=interpolation,\n-            fill_mode=fill_mode,\n-            data_format=data_format,\n+            x, transform, interpolation=interpolation, fill_mode=fill_mode\n         )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (1, 2, 0))\n         coordinates = _compute_affine_transform_coordinates(x, transform)\n         ref_out = _fixed_map_coordinates(\n             x,\n@@ -586,27 +586,18 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n             fill_mode=fill_mode,\n         )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (2, 0, 1))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n-        self.assertAllClose(ref_out, out, atol=1e-3, rtol=1e-3)\n+        self.assertAllClose(ref_out, out, atol=1e-2)\n \n-        # Batched case\n-        if data_format == \"channels_first\":\n-            x = np.random.random((2, 3, 50, 50)).astype(\"float32\") * 255\n-        else:\n-            x = np.random.random((2, 50, 50, 3)).astype(\"float32\") * 255\n-        transform = np.random.random(size=(2, 6)).astype(\"float32\")\n+        x = np.random.uniform(size=(2, 50, 50, 3)).astype(\"float32\") * 255\n+        transform = np.random.uniform(size=(2, 6)).astype(\"float32\")\n         transform = np.pad(transform, [(0, 0), (0, 2)])  # makes c0, c1 always 0\n         out = kimage.affine_transform(\n             x,\n             transform,\n             interpolation=interpolation,\n             fill_mode=fill_mode,\n-            data_format=data_format,\n         )\n-        if data_format == \"channels_first\":\n-            x = np.transpose(x, (0, 2, 3, 1))\n         coordinates = _compute_affine_transform_coordinates(x, transform)\n         ref_out = np.stack(\n             [\n@@ -620,65 +611,95 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             ],\n             axis=0,\n         )\n-        if data_format == \"channels_first\":\n-            ref_out = np.transpose(ref_out, (0, 3, 1, 2))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n-        self.assertAllClose(ref_out, out, atol=1e-3, rtol=1e-3)\n+        self.assertAllClose(ref_out, out, atol=1e-2)\n \n-    @parameterized.parameters(\n-        [\n-            ((5, 5), None, 1, \"valid\", \"channels_last\"),\n-            ((3, 3), (2, 2), 1, \"valid\", \"channels_last\"),\n-            ((5, 5), None, 1, \"valid\", \"channels_first\"),\n-            ((3, 3), (2, 2), 1, \"valid\", \"channels_first\"),\n-            ((5, 5), None, 1, \"same\", \"channels_last\"),\n-            ((3, 3), (2, 2), 1, \"same\", \"channels_last\"),\n-            ((5, 5), None, 1, \"same\", \"channels_first\"),\n-            ((3, 3), (2, 2), 1, \"same\", \"channels_first\"),\n-            ((5, 5), (1, 1), 3, \"same\", \"channels_first\"),\n-            ((5, 5), (2, 2), 3, \"same\", \"channels_first\"),\n-            ((5, 5), (2, 2), 3, \"same\", \"channels_last\"),\n-        ]\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.uniform(size=(3, 50, 50)).astype(\"float32\") * 255\n+        transform = np.random.uniform(size=(6)).astype(\"float32\")\n+        transform = np.pad(transform, (0, 2))  # makes c0, c1 always 0\n+        out = kimage.affine_transform(\n+            x, transform, interpolation=interpolation, fill_mode=fill_mode\n         )\n-    def test_extract_patches(\n-        self, size, strides, dilation_rate, padding, data_format\n-    ):\n-        if (\n-            data_format == \"channels_first\"\n-            and backend.backend() == \"tensorflow\"\n-        ):\n-            pytest.skip(\"channels_first unsupported on CPU with TF\")\n+        coordinates = _compute_affine_transform_coordinates(\n+            np.transpose(x, [1, 2, 0]), transform\n+        )\n+        ref_out = _fixed_map_coordinates(\n+            np.transpose(x, [1, 2, 0]),\n+            coordinates,\n+            order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+            fill_mode=fill_mode,\n+        )\n+        ref_out = np.transpose(ref_out, [2, 0, 1])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=1e-2)\n \n-        if (\n-            isinstance(strides, tuple)\n-            and backend.backend() == \"tensorflow\"\n-            and dilation_rate > 1\n-        ):\n-            pytest.skip(\"dilation_rate>1 with strides>1 not supported with TF\")\n-        if data_format == \"channels_first\":\n-            image = np.random.uniform(size=(1, 3, 20, 20))\n-        else:\n-            image = np.random.uniform(size=(1, 20, 20, 3))\n+        x = np.random.uniform(size=(2, 3, 50, 50)).astype(\"float32\") * 255\n+        transform = np.random.uniform(size=(2, 6)).astype(\"float32\")\n+        transform = np.pad(transform, [(0, 0), (0, 2)])  # makes c0, c1 always 0\n+        out = kimage.affine_transform(\n+            x,\n+            transform,\n+            interpolation=interpolation,\n+            fill_mode=fill_mode,\n+        )\n+        coordinates = _compute_affine_transform_coordinates(\n+            np.transpose(x, [0, 2, 3, 1]), transform\n+        )\n+        ref_out = np.stack(\n+            [\n+                _fixed_map_coordinates(\n+                    np.transpose(x[i], [1, 2, 0]),\n+                    coordinates[i],\n+                    order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                    fill_mode=fill_mode,\n+                )\n+                for i in range(x.shape[0])\n+            ],\n+            axis=0,\n+        )\n+        ref_out = np.transpose(ref_out, [0, 3, 1, 2])\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=1e-2)\n+\n+        # Test class\n+        out = kimage.AffineTransform(\n+            interpolation=interpolation, fill_mode=fill_mode\n+        )(x, transform)\n+        self.assertAllClose(ref_out, out, atol=1e-2)\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            size=[(3, 3), (5, 5)],\n+            strides=[None, (1, 1), (2, 2)],\n+            dilation_rate=[1, 3],\n+            padding=[\"valid\", \"same\"],\n+        )\n+    )\n+    def test_extract_patches(self, size, strides, dilation_rate, padding):\n         patch_h, patch_w = size[0], size[1]\n         if strides is None:\n             strides_h, strides_w = patch_h, patch_w\n         else:\n             strides_h, strides_w = strides[0], strides[1]\n+        if (\n+            backend.backend() == \"tensorflow\"\n+            and strides_h > 1\n+            or strides_w > 1\n+            and dilation_rate > 1\n+        ):\n+            pytest.skip(\"dilation_rate>1 with strides>1 not supported with TF\")\n \n+        # Test channels_last\n+        image = np.random.uniform(size=(1, 20, 20, 3)).astype(\"float32\")\n         patches_out = kimage.extract_patches(\n-            backend.convert_to_tensor(image, dtype=\"float32\"),\n+            image,\n             size=size,\n             strides=strides,\n             dilation_rate=dilation_rate,\n             padding=padding,\n-            data_format=data_format,\n         )\n-        if data_format == \"channels_first\":\n-            patches_out = backend.numpy.transpose(\n-                patches_out, axes=[0, 2, 3, 1]\n-            )\n-        if data_format == \"channels_first\":\n-            image = np.transpose(image, [0, 2, 3, 1])\n         patches_ref = tf.image.extract_patches(\n             image,\n             sizes=(1, patch_h, patch_w, 1),\n@@ -687,11 +708,44 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding.upper(),\n         )\n         self.assertEqual(tuple(patches_out.shape), tuple(patches_ref.shape))\n-        self.assertAllClose(\n-            patches_ref.numpy(), backend.convert_to_numpy(patches_out), atol=0.3\n-        )\n+        self.assertAllClose(patches_ref, patches_out, atol=0.3)\n \n-    @parameterized.product(\n+        # Test channels_first\n+        if backend.backend() == \"tensorflow\":\n+            # tensorflow doesn't support channels_first in\n+            # `kimage.extract_patches`\n+            return\n+        backend.set_image_data_format(\"channels_first\")\n+        image = np.random.uniform(size=(1, 3, 20, 20)).astype(\"float32\")\n+        patches_out = kimage.extract_patches(\n+            image,\n+            size=size,\n+            strides=strides,\n+            dilation_rate=dilation_rate,\n+            padding=padding,\n+        )\n+        patches_ref = tf.image.extract_patches(\n+            np.transpose(image, [0, 2, 3, 1]),\n+            sizes=(1, patch_h, patch_w, 1),\n+            strides=(1, strides_h, strides_w, 1),\n+            rates=(1, dilation_rate, dilation_rate, 1),\n+            padding=padding.upper(),\n+        )\n+        patches_ref = tf.transpose(patches_ref, [0, 3, 1, 2])\n+        self.assertEqual(tuple(patches_out.shape), tuple(patches_ref.shape))\n+        self.assertAllClose(patches_ref, patches_out, atol=0.3)\n+\n+        # Test class\n+        patches_out = kimage.ExtractPatches(\n+            size=size,\n+            strides=strides,\n+            dilation_rate=dilation_rate,\n+            padding=padding,\n+        )(image)\n+        self.assertAllClose(patches_ref, patches_out, atol=0.3)\n+\n+    @parameterized.named_parameters(\n+        named_product(\n             # (input_shape, coordinates_shape)\n             shape=[((5,), (7,)), ((3, 4, 5), (2, 3, 4))],\n             # TODO: scipy.ndimage.map_coordinates does not support float16\n@@ -700,6 +754,7 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             order=[0, 1],\n             fill_mode=[\"constant\", \"nearest\", \"wrap\", \"mirror\", \"reflect\"],\n         )\n+    )\n     def test_map_coordinates(self, shape, dtype, order, fill_mode):\n         input_shape, coordinates_shape = shape\n         input = np.arange(math.prod(input_shape), dtype=dtype).reshape(\n@@ -715,21 +770,22 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ]\n         output = kimage.map_coordinates(input, coordinates, order, fill_mode)\n         expected = _fixed_map_coordinates(input, coordinates, order, fill_mode)\n+        self.assertAllClose(output, expected)\n \n+        # Test class\n+        output = kimage.MapCoordinates(order, fill_mode)(input, coordinates)\n         self.assertAllClose(output, expected)\n \n     @parameterized.parameters(\n         [\n-            (0, 0, 3, 3, None, None, \"channels_last\"),\n-            (1, 0, 4, 3, None, None, \"channels_last\"),\n-            (0, 1, 3, 4, None, None, \"channels_last\"),\n-            (0, 0, 4, 3, None, None, \"channels_last\"),\n-            (0, 0, 3, 4, None, None, \"channels_last\"),\n-            (0, 0, None, None, 0, 1, \"channels_last\"),\n-            (0, 0, None, None, 1, 0, \"channels_last\"),\n-            (1, 2, None, None, 3, 4, \"channels_last\"),\n-            (0, 0, 3, 3, None, None, \"channels_first\"),\n-            (1, 0, 4, 3, None, None, \"channels_first\"),\n+            (0, 0, 3, 3, None, None),\n+            (1, 0, 4, 3, None, None),\n+            (0, 1, 3, 4, None, None),\n+            (0, 0, 4, 3, None, None),\n+            (0, 0, 3, 4, None, None),\n+            (0, 0, None, None, 0, 1),\n+            (0, 0, None, None, 1, 0),\n+            (1, 2, None, None, 3, 4),\n         ]\n     )\n     def test_pad_images(\n@@ -740,12 +796,15 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         target_width,\n         bottom_padding,\n         right_padding,\n-        data_format,\n     ):\n-        if data_format == \"channels_last\":\n-            image = np.random.uniform(size=(3, 3, 1))\n-        else:\n-            image = np.random.uniform(size=(1, 3, 3))\n+        # Test channels_last\n+        image = np.random.uniform(size=(3, 3, 1)).astype(\"float32\")\n+        _target_height = target_height  # For `tf.image.pad_to_bounding_box`\n+        _target_width = target_width  # For `tf.image.pad_to_bounding_box`\n+        if _target_height is None:\n+            _target_height = image.shape[0] + top_padding + bottom_padding\n+        if _target_width is None:\n+            _target_width = image.shape[1] + left_padding + right_padding\n         padded_image = kimage.pad_images(\n             image,\n             top_padding,\n@@ -754,42 +813,61 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             right_padding,\n             target_height,\n             target_width,\n-            data_format=data_format,\n         )\n-        if target_height is None:\n-            target_height = image.shape[0] + top_padding + bottom_padding\n-        if target_width is None:\n-            target_width = image.shape[1] + left_padding + right_padding\n-\n-        # `tf.image.pad_to_bounding_box` only accepts channels_last format\n-        if data_format == \"channels_first\":\n-            image = np.transpose(image, [1, 2, 0])\n         ref_padded_image = tf.image.pad_to_bounding_box(\n-            image, top_padding, left_padding, target_height, target_width\n+            image, top_padding, left_padding, _target_height, _target_width\n         )\n-        ref_padded_image = ref_padded_image.numpy()\n-        if data_format == \"channels_first\":\n-            ref_padded_image = np.transpose(ref_padded_image, [2, 0, 1])\n-\n         self.assertEqual(\n             tuple(padded_image.shape), tuple(ref_padded_image.shape)\n         )\n-        self.assertAllClose(\n-            ref_padded_image, backend.convert_to_numpy(padded_image)\n+        self.assertAllClose(ref_padded_image, padded_image)\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        image = np.random.uniform(size=(1, 3, 3)).astype(\"float32\")\n+        padded_image = kimage.pad_images(\n+            image,\n+            top_padding,\n+            left_padding,\n+            bottom_padding,\n+            right_padding,\n+            target_height,\n+            target_width,\n         )\n+        ref_padded_image = tf.image.pad_to_bounding_box(\n+            np.transpose(image, [1, 2, 0]),\n+            top_padding,\n+            left_padding,\n+            _target_height,\n+            _target_width,\n+        )\n+        ref_padded_image = tf.transpose(ref_padded_image, [2, 0, 1])\n+        self.assertEqual(\n+            tuple(padded_image.shape), tuple(ref_padded_image.shape)\n+        )\n+        self.assertAllClose(ref_padded_image, padded_image)\n+\n+        # Test class\n+        padded_image = kimage.PadImages(\n+            top_padding,\n+            left_padding,\n+            bottom_padding,\n+            right_padding,\n+            target_height,\n+            target_width,\n+        )(image)\n+        self.assertAllClose(ref_padded_image, padded_image)\n \n     @parameterized.parameters(\n         [\n-            (0, 0, 3, 3, None, None, \"channels_last\"),\n-            (1, 0, 4, 3, None, None, \"channels_last\"),\n-            (0, 1, 3, 4, None, None, \"channels_last\"),\n-            (0, 0, 4, 3, None, None, \"channels_last\"),\n-            (0, 0, 3, 4, None, None, \"channels_last\"),\n-            (0, 0, None, None, 0, 1, \"channels_last\"),\n-            (0, 0, None, None, 1, 0, \"channels_last\"),\n-            (1, 2, None, None, 3, 4, \"channels_last\"),\n-            (0, 0, 3, 3, None, None, \"channels_first\"),\n-            (1, 0, 4, 3, None, None, \"channels_first\"),\n+            (0, 0, 3, 3, None, None),\n+            (1, 0, 4, 3, None, None),\n+            (0, 1, 3, 4, None, None),\n+            (0, 0, 4, 3, None, None),\n+            (0, 0, 3, 4, None, None),\n+            (0, 0, None, None, 0, 1),\n+            (0, 0, None, None, 1, 0),\n+            (1, 2, None, None, 3, 4),\n         ]\n     )\n     def test_crop_images(\n@@ -800,12 +878,15 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         target_width,\n         bottom_cropping,\n         right_cropping,\n-        data_format,\n     ):\n-        if data_format == \"channels_last\":\n-            image = np.random.uniform(size=(10, 10, 1))\n-        else:\n-            image = np.random.uniform(size=(1, 10, 10))\n+        # Test channels_last\n+        image = np.random.uniform(size=(10, 10, 1)).astype(\"float32\")\n+        _target_height = target_height  # For `tf.image.pad_to_bounding_box`\n+        _target_width = target_width  # For `tf.image.pad_to_bounding_box`\n+        if _target_height is None:\n+            _target_height = image.shape[0] - top_cropping - bottom_cropping\n+        if _target_width is None:\n+            _target_width = image.shape[1] - left_cropping - right_cropping\n         cropped_image = kimage.crop_images(\n             image,\n             top_cropping,\n@@ -814,183 +895,50 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             right_cropping,\n             target_height,\n             target_width,\n-            data_format,\n         )\n-        if target_height is None:\n-            target_height = image.shape[0] - top_cropping - bottom_cropping\n-        if target_width is None:\n-            target_width = image.shape[1] - left_cropping - right_cropping\n-\n-        # `tf.image.crop_to_bounding_box` only accepts channels_last format\n-        if data_format == \"channels_first\":\n-            image = np.transpose(image, [1, 2, 0])\n         ref_cropped_image = tf.image.crop_to_bounding_box(\n-            image, top_cropping, left_cropping, target_height, target_width\n+            image, top_cropping, left_cropping, _target_height, _target_width\n         )\n-        ref_cropped_image = ref_cropped_image.numpy()\n-        if data_format == \"channels_first\":\n-            ref_cropped_image = np.transpose(ref_cropped_image, [2, 0, 1])\n-\n         self.assertEqual(\n             tuple(cropped_image.shape), tuple(ref_cropped_image.shape)\n         )\n-        self.assertAllClose(\n-            ref_cropped_image, backend.convert_to_numpy(cropped_image)\n+        self.assertAllClose(ref_cropped_image, cropped_image)\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        image = np.random.uniform(size=(1, 10, 10)).astype(\"float32\")\n+        cropped_image = kimage.crop_images(\n+            image,\n+            top_cropping,\n+            left_cropping,\n+            bottom_cropping,\n+            right_cropping,\n+            target_height,\n+            target_width,\n         )\n-\n-    def test_rgb_to_grayscale_invalid_rank_two_tensor(self):\n-        rgb_to_gray = kimage.RGBToGrayscale()\n-        invalid_image = np.random.uniform(size=(10, 10))\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"Invalid image rank: expected rank 3\",\n-        ):\n-            rgb_to_gray.compute_output_spec(invalid_image)\n-\n-    def test_rgb_to_grayscale_invalid_rank_five_tensor(self):\n-        rgb_to_gray = kimage.RGBToGrayscale()\n-        invalid_image = np.random.uniform(size=(2, 3, 10, 10, 3))\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"Invalid image rank: expected rank 3\",\n-        ):\n-            rgb_to_gray.compute_output_spec(invalid_image)\n-\n-    def test_rgb_to_grayscale_valid_rank_three_tensor(self):\n-        rgb_to_gray = kimage.RGBToGrayscale()\n-        valid_image = np.random.uniform(size=(10, 10, 3))\n-        output_spec = rgb_to_gray.compute_output_spec(valid_image)\n+        ref_cropped_image = tf.image.crop_to_bounding_box(\n+            np.transpose(image, [1, 2, 0]),\n+            top_cropping,\n+            left_cropping,\n+            _target_height,\n+            _target_width,\n+        )\n+        ref_cropped_image = tf.transpose(ref_cropped_image, [2, 0, 1])\n         self.assertEqual(\n-            output_spec.shape,\n-            (10, 10, 1),\n-            \"Output shape should match expected grayscale image shape\",\n+            tuple(cropped_image.shape), tuple(ref_cropped_image.shape)\n         )\n+        self.assertAllClose(ref_cropped_image, cropped_image)\n \n-    def test_rgb_to_grayscale_valid_rank_four_tensor(self):\n-        rgb_to_gray = kimage.RGBToGrayscale()\n-        valid_image = np.random.uniform(size=(5, 10, 10, 3))\n-        output_spec = rgb_to_gray.compute_output_spec(valid_image)\n-        self.assertEqual(\n-            output_spec.shape,\n-            (5, 10, 10, 1),\n-            \"Output shape should match expected grayscale image shape\",\n-        )\n-\n-    def test_affine_transform_compute_output_spec_image_rank_too_low(self):\n-        affine_transform = kimage.AffineTransform()\n-        # Test with an image of rank 2 (invalid)\n-        image_2d = np.random.uniform(size=(10, 10))\n-        transform_valid = np.random.uniform(size=(6,))\n-        with self.assertRaisesRegex(\n-            ValueError, \"Invalid image rank: expected rank 3\"\n-        ):\n-            affine_transform.compute_output_spec(image_2d, transform_valid)\n-\n-    def test_affine_transform_compute_output_spec_image_rank_too_high(self):\n-        affine_transform = kimage.AffineTransform()\n-        # Test with an image of rank 5 (invalid)\n-        image_5d = np.random.uniform(size=(2, 10, 10, 3, 1))\n-        transform_valid = np.random.uniform(size=(6,))\n-        with self.assertRaisesRegex(\n-            ValueError, \"Invalid image rank: expected rank 3\"\n-        ):\n-            affine_transform.compute_output_spec(image_5d, transform_valid)\n-\n-    def test_affine_transform_compute_output_spec_transform_rank_too_high(self):\n-        affine_transform = kimage.AffineTransform()\n-        # Test with a valid image rank 3\n-        image_valid = np.random.uniform(size=(10, 10, 3))\n-        # Test with a transform of rank 3 (invalid)\n-        transform_invalid_rank3 = np.random.uniform(size=(2, 3, 2))\n-        with self.assertRaisesRegex(\n-            ValueError, \"Invalid transform rank: expected rank 1\"\n-        ):\n-            affine_transform.compute_output_spec(\n-                image_valid, transform_invalid_rank3\n-            )\n-\n-    def test_affine_transform_compute_output_spec_transform_rank_too_low(self):\n-        affine_transform = kimage.AffineTransform()\n-        # Test with a valid image rank 3\n-        image_valid = np.random.uniform(size=(10, 10, 3))\n-        # Test with a transform of rank 0 (invalid)\n-        transform_invalid_rank0 = np.random.uniform(size=())\n-        with self.assertRaisesRegex(\n-            ValueError, \"Invalid transform rank: expected rank 1\"\n-        ):\n-            affine_transform.compute_output_spec(\n-                image_valid, transform_invalid_rank0\n-            )\n-\n-    def test_extract_patches_with_invalid_tuple_size(self):\n-        size = (3, 3, 3)  # Invalid size, too many dimensions\n-        image = np.random.uniform(size=(2, 20, 20, 3))\n-        with self.assertRaisesRegex(\n-            TypeError, \"Expected an int or a tuple of length 2\"\n-        ):\n-            kimage.extract_patches(image, size)\n-\n-    def test_extract_patches_with_incorrect_type_size(self):\n-        size = \"5\"  # Invalid size type\n-        image = np.random.uniform(size=(2, 20, 20, 3))\n-        with self.assertRaisesRegex(\n-            TypeError, \"Expected an int or a tuple of length 2\"\n-        ):\n-            kimage.extract_patches(image, size)\n-\n-    def test_extract_patches_with_integer_size(self):\n-        size = 5\n-        # Use float32 for compatibility with TensorFlow convolution operations\n-        image = np.random.uniform(size=(1, 20, 20, 3)).astype(np.float32)\n-        patches = kimage.extract_patches(image, size)\n-        # Expecting 4x4 patches with each patch having 75 values (5x5x3)\n-        expected_shape = (1, 4, 4, 75)\n-        self.assertEqual(patches.shape, expected_shape)\n-\n-    def test_extract_patches_with_tuple_size(self):\n-        size = (5, 5)\n-        image = np.random.uniform(size=(1, 20, 20, 3)).astype(np.float32)\n-        patches = kimage.extract_patches(image, size)\n-        # Expecting 4x4 patches with each patch having 75 values (5x5x3)\n-        expected_shape = (1, 4, 4, 75)\n-        self.assertEqual(patches.shape, expected_shape)\n-\n-    def test_map_coordinates_image_coordinates_rank_mismatch(self):\n-        map_coordinates = kimage.MapCoordinates()\n-        image = np.random.uniform(size=(10, 10, 3))\n-        coordinates = np.random.uniform(size=(2, 10, 10))\n-        with self.assertRaisesRegex(\n-            ValueError, \"must be the same as the rank of `image`\"\n-        ):\n-            map_coordinates.compute_output_spec(image, coordinates)\n-\n-    def test_map_coordinates_image_coordinates_rank_mismatch_order_zero(self):\n-        map_coordinates = kimage.MapCoordinates(order=0)\n-        image = np.random.uniform(size=(10, 10, 3))\n-        coordinates = np.random.uniform(size=(2, 10, 10))\n-        with self.assertRaisesRegex(\n-            ValueError, \"must be the same as the rank of `image`\"\n-        ):\n-            map_coordinates.compute_output_spec(image, coordinates)\n-\n-    def test_map_coordinates_coordinates_rank_too_low(self):\n-        map_coordinates = kimage.MapCoordinates()\n-        image = np.random.uniform(size=(10, 10, 3))\n-        coordinates = np.random.uniform(size=(3,))\n-        with self.assertRaisesRegex(ValueError, \"expected at least rank 2\"):\n-            map_coordinates.compute_output_spec(image, coordinates)\n-\n-    def test_map_coordinates_valid_input(self):\n-        map_coordinates = kimage.MapCoordinates()\n-        image = np.random.uniform(size=(10, 10, 3))\n-        coordinates = np.random.uniform(size=(3, 10, 10))\n-        output_spec = map_coordinates.compute_output_spec(image, coordinates)\n-        expected_shape = (10, 10)\n-        self.assertEqual(\n-            output_spec.shape,\n-            expected_shape,\n-            \"Output shape should be correct for valid inputs\",\n-        )\n+        # Test class\n+        cropped_image = kimage.CropImages(\n+            top_cropping,\n+            left_cropping,\n+            bottom_cropping,\n+            right_cropping,\n+            target_height,\n+            target_width,\n+        )(image)\n+        self.assertAllClose(ref_cropped_image, cropped_image)\n \n \n class ImageOpsBehaviorTests(testing.TestCase):\n@@ -1004,6 +952,159 @@ class ImageOpsBehaviorTests(testing.TestCase):\n         backend.set_image_data_format(self.data_format)\n         return super().tearDown()\n \n+    def test_rgb_to_grayscale_invalid_rank(self):\n+        # Test rank=2\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid images rank: expected rank 3\",\n+        ):\n+            kimage.rgb_to_grayscale(invalid_image)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid images rank: expected rank 3\",\n+        ):\n+            kimage.RGBToGrayscale()(invalid_image)\n+\n+        # Test rank=5\n+        invalid_image = np.random.uniform(size=(2, 3, 10, 10, 3))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid images rank: expected rank 3\",\n+        ):\n+            kimage.rgb_to_grayscale(invalid_image)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid images rank: expected rank 3\",\n+        ):\n+            kimage.RGBToGrayscale()(invalid_image)\n+\n+        # Test rank=2, symbolic tensor\n+        invalid_image = KerasTensor(shape=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid images rank: expected rank 3\",\n+        ):\n+            kimage.rgb_to_grayscale(invalid_image)\n+\n+    def test_resize_invalid_rank(self):\n+        # Test rank=2\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.resize(invalid_image, (5, 5))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.Resize((5, 5))(invalid_image)\n+\n+        # Test rank=2, symbolic tensor\n+        invalid_image = KerasTensor(shape=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.resize(invalid_image, (5, 5))\n+\n+    def test_affine_transform_invalid_images_rank(self):\n+        # Test rank=2\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        transform = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.affine_transform(invalid_image, transform)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.AffineTransform()(invalid_image, transform)\n+\n+        # Test rank=5\n+        invalid_image = np.random.uniform(size=(2, 10, 10, 3, 1))\n+        transform = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.affine_transform(invalid_image, transform)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.AffineTransform()(invalid_image, transform)\n+\n+        # Test rank=2, symbolic tensor\n+        invalid_image = KerasTensor(shape=(10, 10))\n+        transform = KerasTensor(shape=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.affine_transform(invalid_image, transform)\n+\n+    def test_affine_transform_invalid_transform_rank(self):\n+        # Test rank=3\n+        images = np.random.uniform(size=(10, 10, 3))\n+        invalid_transform = np.random.uniform(size=(2, 3, 2))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            kimage.affine_transform(images, invalid_transform)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            kimage.AffineTransform()(images, invalid_transform)\n+\n+        # Test rank=0\n+        invalid_transform = np.random.uniform(size=())\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            kimage.affine_transform(images, invalid_transform)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            kimage.AffineTransform()(images, invalid_transform)\n+\n+        # Test rank=3, symbolic tensor\n+        images = KerasTensor(shape=(10, 10, 3))\n+        invalid_transform = KerasTensor(shape=(2, 3, 2))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            kimage.affine_transform(images, invalid_transform)\n+\n+    def test_extract_patches_invalid_size(self):\n+        size = (3, 3, 3)  # Invalid size, too many dimensions\n+        image = np.random.uniform(size=(2, 20, 20, 3))\n+        with self.assertRaisesRegex(\n+            TypeError, \"Expected an int or a tuple of length 2\"\n+        ):\n+            kimage.extract_patches(image, size)\n+\n+        size = \"5\"  # Invalid size type\n+        with self.assertRaisesRegex(\n+            TypeError, \"Expected an int or a tuple of length 2\"\n+        ):\n+            kimage.extract_patches(image, size)\n+\n+    def test_map_coordinates_invalid_coordinates_rank(self):\n+        # Test mismatched dim of coordinates\n+        image = np.random.uniform(size=(10, 10, 3))\n+        coordinates = np.random.uniform(size=(2, 10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"must be the same as the rank of `inputs`\"\n+        ):\n+            kimage.map_coordinates(image, coordinates, 0)\n+        with self.assertRaisesRegex(\n+            ValueError, \"must be the same as the rank of `inputs`\"\n+        ):\n+            kimage.MapCoordinates(0)(image, coordinates)\n+\n+        # Test rank=1\n+        coordinates = np.random.uniform(size=(3,))\n+        with self.assertRaisesRegex(ValueError, \"expected at least rank 2\"):\n+            kimage.map_coordinates(image, coordinates, 0)\n+        with self.assertRaisesRegex(ValueError, \"expected at least rank 2\"):\n+            kimage.MapCoordinates(0)(image, coordinates)\n+\n     def test_crop_images_unknown_shape(self):\n         # Test unknown height and target_height\n         x = KerasTensor([None, 10, 3])\n\n@@ -41,5 +41,6 @@ gfile = LazyModule(\"tensorflow.io.gfile\", pip_name=\"tensorflow\")\n tensorflow_io = LazyModule(\"tensorflow_io\")\n scipy = LazyModule(\"scipy\")\n jax = LazyModule(\"jax\")\n+torchvision = LazyModule(\"torchvision\")\n optree = LazyModule(\"optree\")\n dmtree = LazyModule(\"tree\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
