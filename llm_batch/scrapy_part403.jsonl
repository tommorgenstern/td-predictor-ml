{"custom_id": "scrapy#3ded1dfe31510f00e14a70811b7c01dae8b5a641", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 472 | Lines Deleted: 528 | Files Changed: 13 | Hunks: 250 | Methods Changed: 117 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 1000 | Churn Cumulative: 9901 | Contributors (this commit): 45 | Commits (past 90d): 38 | Contributors (cumulative): 151 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,7 +31,7 @@ from scrapy.utils.test import get_crawler\n from tests import get_testdata, tests_datadir\n \n \n-class SpiderTest(unittest.TestCase):\n+class TestSpider(unittest.TestCase):\n     spider_class = Spider\n \n     def setUp(self):\n@@ -42,19 +42,19 @@ class SpiderTest(unittest.TestCase):\n \n     def test_base_spider(self):\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(spider.name, \"example.com\")\n-        self.assertEqual(spider.start_urls, [])\n+        assert spider.name == \"example.com\"\n+        assert spider.start_urls == []  # pylint: disable=use-implicit-booleaness-not-comparison\n \n     def test_start_requests(self):\n         spider = self.spider_class(\"example.com\")\n         start_requests = spider.start_requests()\n-        self.assertTrue(inspect.isgenerator(start_requests))\n-        self.assertEqual(list(start_requests), [])\n+        assert inspect.isgenerator(start_requests)\n+        assert not list(start_requests)\n \n     def test_spider_args(self):\n         \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n         spider = self.spider_class(\"example.com\", foo=\"bar\")\n-        self.assertEqual(spider.foo, \"bar\")\n+        assert spider.foo == \"bar\"\n \n     def test_spider_without_name(self):\n         \"\"\"``__init__`` method arguments are assigned to spider attributes\"\"\"\n@@ -67,10 +67,10 @@ class SpiderTest(unittest.TestCase):\n     def test_from_crawler_crawler_and_settings_population(self):\n         crawler = get_crawler()\n         spider = self.spider_class.from_crawler(crawler, \"example.com\")\n-        self.assertTrue(hasattr(spider, \"crawler\"))\n-        self.assertIs(spider.crawler, crawler)\n-        self.assertTrue(hasattr(spider, \"settings\"))\n-        self.assertIs(spider.settings, crawler.settings)\n+        assert hasattr(spider, \"crawler\")\n+        assert spider.crawler is crawler\n+        assert hasattr(spider, \"settings\")\n+        assert spider.settings is crawler.settings\n \n     def test_from_crawler_init_call(self):\n         with mock.patch.object(\n@@ -92,7 +92,7 @@ class SpiderTest(unittest.TestCase):\n         crawler.signals.send_catch_log(\n             signal=signals.spider_closed, spider=spider, reason=None\n         )\n-        self.assertTrue(spider.closed_called)\n+        assert spider.closed_called\n \n     def test_update_settings(self):\n         spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n@@ -101,9 +101,9 @@ class SpiderTest(unittest.TestCase):\n         settings = Settings(project_settings, priority=\"project\")\n \n         self.spider_class.update_settings(settings)\n-        self.assertEqual(settings.get(\"TEST1\"), \"spider\")\n-        self.assertEqual(settings.get(\"TEST2\"), \"spider\")\n-        self.assertEqual(settings.get(\"TEST3\"), \"project\")\n+        assert settings.get(\"TEST1\") == \"spider\"\n+        assert settings.get(\"TEST2\") == \"spider\"\n+        assert settings.get(\"TEST3\") == \"project\"\n \n     @inlineCallbacks\n     def test_settings_in_from_crawler(self):\n@@ -121,11 +121,11 @@ class SpiderTest(unittest.TestCase):\n                 return spider\n \n         crawler = Crawler(TestSpider, project_settings)\n-        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n-        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n+        assert crawler.settings.get(\"TEST1\") == \"spider\"\n+        assert crawler.settings.get(\"TEST2\") == \"spider\"\n+        assert crawler.settings.get(\"TEST3\") == \"project\"\n         yield crawler.crawl()\n-        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider_instance\")\n+        assert crawler.settings.get(\"TEST1\") == \"spider_instance\"\n \n     def test_logger(self):\n         spider = self.spider_class(\"example.com\")\n@@ -134,8 +134,8 @@ class SpiderTest(unittest.TestCase):\n         lc.check((\"example.com\", \"INFO\", \"test log msg\"))\n \n         record = lc.records[0]\n-        self.assertIn(\"spider\", record.__dict__)\n-        self.assertIs(record.spider, spider)\n+        assert \"spider\" in record.__dict__\n+        assert record.spider is spider\n \n     def test_log(self):\n         spider = self.spider_class(\"example.com\")\n@@ -144,11 +144,11 @@ class SpiderTest(unittest.TestCase):\n         mock_logger.log.assert_called_once_with(\"INFO\", \"test log msg\")\n \n \n-class InitSpiderTest(SpiderTest):\n+class TestInitSpider(TestSpider):\n     spider_class = InitSpider\n \n \n-class XMLFeedSpiderTest(SpiderTest):\n+class TestXMLFeedSpider(TestSpider):\n     spider_class = XMLFeedSpider\n \n     def test_register_namespace(self):\n@@ -180,10 +180,8 @@ class XMLFeedSpiderTest(SpiderTest):\n         for iterator in (\"iternodes\", \"xml\"):\n             spider = _XMLSpider(\"example\", iterator=iterator)\n             output = list(spider._parse(response))\n-            self.assertEqual(len(output), 2, iterator)\n-            self.assertEqual(\n-                output,\n-                [\n+            assert len(output) == 2, iterator\n+            assert output == [\n                 {\n                     \"loc\": [\"http://www.example.com/Special-Offers.html\"],\n                     \"updated\": [\"2009-08-16\"],\n@@ -196,12 +194,10 @@ class XMLFeedSpiderTest(SpiderTest):\n                     \"other\": [\"foo\"],\n                     \"custom\": [],\n                 },\n-                ],\n-                iterator,\n-            )\n+            ], iterator\n \n \n-class CSVFeedSpiderTest(SpiderTest):\n+class TestCSVFeedSpider(TestSpider):\n     spider_class = CSVFeedSpider\n \n     def test_parse_rows(self):\n@@ -222,7 +218,7 @@ class CSVFeedSpiderTest(SpiderTest):\n         assert len(rows) == 4\n \n \n-class CrawlSpiderTest(SpiderTest):\n+class TestCrawlSpider(TestSpider):\n     test_body = b\"\"\"<html><head><title>Page title<title>\n     <body>\n     <p><a href=\"item/12.html\">Item 12</a></p>\n@@ -247,16 +243,13 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n             \"http://example.org/nofollow.html\",\n-            ],\n-        )\n+        ]\n \n     def test_process_links(self):\n         response = HtmlResponse(\n@@ -273,16 +266,13 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n             \"http://example.org/nofollow.html\",\n-            ],\n-        )\n+        ]\n \n     def test_process_links_filter(self):\n         response = HtmlResponse(\n@@ -302,15 +292,12 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 2)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 2\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n-            ],\n-        )\n+        ]\n \n     def test_process_links_generator(self):\n         response = HtmlResponse(\n@@ -327,16 +314,13 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n             \"http://example.org/nofollow.html\",\n-            ],\n-        )\n+        ]\n \n     def test_process_request(self):\n         response = HtmlResponse(\n@@ -355,16 +339,13 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.com/somepage/item/12.html\",\n             \"http://example.com/about.html\",\n             \"http://example.com/nofollow.html\",\n-            ],\n-        )\n+        ]\n \n     def test_process_request_with_response(self):\n         response = HtmlResponse(\n@@ -386,20 +367,18 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n             \"http://example.org/nofollow.html\",\n-            ],\n-        )\n-        self.assertEqual(\n-            [r.meta[\"response_class\"] for r in output],\n-            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n-        )\n+        ]\n+        assert [r.meta[\"response_class\"] for r in output] == [\n+            \"HtmlResponse\",\n+            \"HtmlResponse\",\n+            \"HtmlResponse\",\n+        ]\n \n     def test_process_request_instance_method(self):\n         response = HtmlResponse(\n@@ -416,16 +395,13 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             safe_url_string(\"http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML\"),\n             safe_url_string(\"http://EXAMPLE.ORG/ABOUT.HTML\"),\n             safe_url_string(\"http://EXAMPLE.ORG/NOFOLLOW.HTML\"),\n-            ],\n-        )\n+        ]\n \n     def test_process_request_instance_method_with_response(self):\n         response = HtmlResponse(\n@@ -448,32 +424,30 @@ class CrawlSpiderTest(SpiderTest):\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n-        self.assertEqual(len(output), 3)\n-        self.assertTrue(all(isinstance(r, Request) for r in output))\n-        self.assertEqual(\n-            [r.url for r in output],\n-            [\n+        assert len(output) == 3\n+        assert all(isinstance(r, Request) for r in output)\n+        assert [r.url for r in output] == [\n             \"http://example.org/somepage/item/12.html\",\n             \"http://example.org/about.html\",\n             \"http://example.org/nofollow.html\",\n-            ],\n-        )\n-        self.assertEqual(\n-            [r.meta[\"response_class\"] for r in output],\n-            [\"HtmlResponse\", \"HtmlResponse\", \"HtmlResponse\"],\n-        )\n+        ]\n+        assert [r.meta[\"response_class\"] for r in output] == [\n+            \"HtmlResponse\",\n+            \"HtmlResponse\",\n+            \"HtmlResponse\",\n+        ]\n \n     def test_follow_links_attribute_population(self):\n         crawler = get_crawler()\n         spider = self.spider_class.from_crawler(crawler, \"example.com\")\n-        self.assertTrue(hasattr(spider, \"_follow_links\"))\n-        self.assertTrue(spider._follow_links)\n+        assert hasattr(spider, \"_follow_links\")\n+        assert spider._follow_links\n \n         settings_dict = {\"CRAWLSPIDER_FOLLOW_LINKS\": False}\n         crawler = get_crawler(settings_dict=settings_dict)\n         spider = self.spider_class.from_crawler(crawler, \"example.com\")\n-        self.assertTrue(hasattr(spider, \"_follow_links\"))\n-        self.assertFalse(spider._follow_links)\n+        assert hasattr(spider, \"_follow_links\")\n+        assert not spider._follow_links\n \n     def test_start_url(self):\n         spider = self.spider_class(\"example.com\")\n@@ -483,7 +457,7 @@ class CrawlSpiderTest(SpiderTest):\n             list(spider.start_requests())\n \n \n-class SitemapSpiderTest(SpiderTest):\n+class TestSitemapSpider(TestSpider):\n     spider_class = SitemapSpider\n \n     BODY = b\"SITEMAP\"\n@@ -496,7 +470,7 @@ class SitemapSpiderTest(SpiderTest):\n     def assertSitemapBody(self, response, body):\n         crawler = get_crawler()\n         spider = self.spider_class.from_crawler(crawler, \"example.com\")\n-        self.assertEqual(spider._get_sitemap_body(response), body)\n+        assert spider._get_sitemap_body(response) == body\n \n     def test_get_sitemap_body(self):\n         r = XmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n@@ -543,15 +517,12 @@ Sitemap: /sitemap-relative-url.xml\n \n         r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n             \"http://example.com/sitemap.xml\",\n             \"http://example.com/sitemap-product-index.xml\",\n             \"http://example.com/sitemap-uppercase.xml\",\n             \"http://www.example.com/sitemap-relative-url.xml\",\n-            ],\n-        )\n+        ]\n \n     def test_alternate_url_locs(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -570,21 +541,17 @@ Sitemap: /sitemap-relative-url.xml\n     </urlset>\"\"\"\n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\"http://www.example.com/english/\"],\n-        )\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n+            \"http://www.example.com/english/\"\n+        ]\n \n         spider.sitemap_alternate_links = True\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n             \"http://www.example.com/english/\",\n             \"http://www.example.com/deutsch/\",\n             \"http://www.example.com/schweiz-deutsch/\",\n             \"http://www.example.com/italiano/\",\n-            ],\n-        )\n+        ]\n \n     def test_sitemap_filter(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -611,16 +578,15 @@ Sitemap: /sitemap-relative-url.xml\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\"http://www.example.com/english/\", \"http://www.example.com/portuguese/\"],\n-        )\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n+            \"http://www.example.com/english/\",\n+            \"http://www.example.com/portuguese/\",\n+        ]\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\"http://www.example.com/english/\"],\n-        )\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n+            \"http://www.example.com/english/\"\n+        ]\n \n     def test_sitemap_filter_with_alternate_links(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -649,19 +615,15 @@ Sitemap: /sitemap-relative-url.xml\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n             \"http://www.example.com/english/article_1/\",\n             \"http://www.example.com/english/article_2/\",\n-            ],\n-        )\n+        ]\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\"http://www.example.com/deutsch/article_1/\"],\n-        )\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n+            \"http://www.example.com/deutsch/article_1/\"\n+        ]\n \n     def test_sitemapindex_filter(self):\n         sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -689,19 +651,15 @@ Sitemap: /sitemap-relative-url.xml\n \n         r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n         spider = self.spider_class(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n             \"http://www.example.com/sitemap1.xml\",\n             \"http://www.example.com/sitemap2.xml\",\n-            ],\n-        )\n+        ]\n \n         spider = FilteredSitemapSpider(\"example.com\")\n-        self.assertEqual(\n-            [req.url for req in spider._parse_sitemap(r)],\n-            [\"http://www.example.com/sitemap2.xml\"],\n-        )\n+        assert [req.url for req in spider._parse_sitemap(r)] == [\n+            \"http://www.example.com/sitemap2.xml\"\n+        ]\n \n     def test_compression_bomb_setting(self):\n         settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}\n@@ -711,7 +669,7 @@ Sitemap: /sitemap-relative-url.xml\n         body = body_path.read_bytes()\n         request = Request(url=\"https://example.com\")\n         response = Response(url=\"https://example.com\", body=body, request=request)\n-        self.assertIsNone(spider._get_sitemap_body(response))\n+        assert spider._get_sitemap_body(response) is None\n \n     def test_compression_bomb_spider_attr(self):\n         class DownloadMaxSizeSpider(self.spider_class):\n@@ -723,7 +681,7 @@ Sitemap: /sitemap-relative-url.xml\n         body = body_path.read_bytes()\n         request = Request(url=\"https://example.com\")\n         response = Response(url=\"https://example.com\", body=body, request=request)\n-        self.assertIsNone(spider._get_sitemap_body(response))\n+        assert spider._get_sitemap_body(response) is None\n \n     def test_compression_bomb_request_meta(self):\n         crawler = get_crawler()\n@@ -734,7 +692,7 @@ Sitemap: /sitemap-relative-url.xml\n             url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}\n         )\n         response = Response(url=\"https://example.com\", body=body, request=request)\n-        self.assertIsNone(spider._get_sitemap_body(response))\n+        assert spider._get_sitemap_body(response) is None\n \n     def test_download_warnsize_setting(self):\n         settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}\n@@ -814,13 +772,13 @@ Sitemap: /sitemap-relative-url.xml\n         )\n \n \n-class DeprecationTest(unittest.TestCase):\n+class TestDeprecation:\n     def test_crawl_spider(self):\n         assert issubclass(CrawlSpider, Spider)\n         assert isinstance(CrawlSpider(name=\"foo\"), Spider)\n \n \n-class NoParseMethodSpiderTest(unittest.TestCase):\n+class TestNoParseMethodSpider:\n     spider_class = Spider\n \n     def test_undefined_parse_method(self):\n\n@@ -8,7 +8,6 @@ from tempfile import mkdtemp\n from unittest import mock\n \n import pytest\n-from twisted.trial import unittest\n from zope.interface.verify import verifyObject\n \n # ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n@@ -28,8 +27,8 @@ def _copytree(source: Path, target: Path):\n         shutil.copytree(source, target)\n \n \n-class SpiderLoaderTest(unittest.TestCase):\n-    def setUp(self):\n+class TestSpiderLoader:\n+    def setup_method(self):\n         orig_spiders_dir = module_dir / \"test_spiders\"\n         self.tmpdir = Path(tempfile.mkdtemp())\n         self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n@@ -38,7 +37,7 @@ class SpiderLoaderTest(unittest.TestCase):\n         settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         del self.spider_loader\n         del sys.modules[\"test_spiders_xxx\"]\n         sys.path.remove(str(self.tmpdir))\n@@ -47,37 +46,35 @@ class SpiderLoaderTest(unittest.TestCase):\n         verifyObject(ISpiderLoader, self.spider_loader)\n \n     def test_list(self):\n-        self.assertEqual(\n-            set(self.spider_loader.list()), {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n-        )\n+        assert set(self.spider_loader.list()) == {\n+            \"spider1\",\n+            \"spider2\",\n+            \"spider3\",\n+            \"spider4\",\n+        }\n \n     def test_load(self):\n         spider1 = self.spider_loader.load(\"spider1\")\n-        self.assertEqual(spider1.__name__, \"Spider1\")\n+        assert spider1.__name__ == \"Spider1\"\n \n     def test_find_by_request(self):\n-        self.assertEqual(\n-            self.spider_loader.find_by_request(Request(\"http://scrapy1.org/test\")),\n-            [\"spider1\"],\n-        )\n-        self.assertEqual(\n-            self.spider_loader.find_by_request(Request(\"http://scrapy2.org/test\")),\n-            [\"spider2\"],\n-        )\n-        self.assertEqual(\n-            set(self.spider_loader.find_by_request(Request(\"http://scrapy3.org/test\"))),\n-            {\"spider1\", \"spider2\"},\n-        )\n-        self.assertEqual(\n-            self.spider_loader.find_by_request(Request(\"http://scrapy999.org/test\")), []\n-        )\n-        self.assertEqual(\n-            self.spider_loader.find_by_request(Request(\"http://spider3.com\")), []\n-        )\n-        self.assertEqual(\n-            self.spider_loader.find_by_request(Request(\"http://spider3.com/onlythis\")),\n-            [\"spider3\"],\n+        assert self.spider_loader.find_by_request(\n+            Request(\"http://scrapy1.org/test\")\n+        ) == [\"spider1\"]\n+        assert self.spider_loader.find_by_request(\n+            Request(\"http://scrapy2.org/test\")\n+        ) == [\"spider2\"]\n+        assert set(\n+            self.spider_loader.find_by_request(Request(\"http://scrapy3.org/test\"))\n+        ) == {\"spider1\", \"spider2\"}\n+        assert (\n+            self.spider_loader.find_by_request(Request(\"http://scrapy999.org/test\"))\n+            == []\n         )\n+        assert self.spider_loader.find_by_request(Request(\"http://spider3.com\")) == []\n+        assert self.spider_loader.find_by_request(\n+            Request(\"http://spider3.com/onlythis\")\n+        ) == [\"spider3\"]\n \n     def test_load_spider_module(self):\n         module = \"tests.test_spiderloader.test_spiders.spider1\"\n@@ -113,9 +110,9 @@ class SpiderLoaderTest(unittest.TestCase):\n         runner = CrawlerRunner({\"ADDONS\": {SpiderModuleAddon: 1}})\n \n         crawler = runner.create_crawler(\"spider_from_addon\")\n-        self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n-        self.assertEqual(crawler.spidercls.name, \"spider_from_addon\")\n-        self.assertTrue(len(crawler.settings[\"SPIDER_MODULES\"]) == 1)\n+        assert issubclass(crawler.spidercls, scrapy.Spider)\n+        assert crawler.spidercls.name == \"spider_from_addon\"\n+        assert len(crawler.settings[\"SPIDER_MODULES\"]) == 1\n \n     def test_crawler_runner_loading(self):\n         module = \"tests.test_spiderloader.test_spiders.spider1\"\n@@ -129,8 +126,8 @@ class SpiderLoaderTest(unittest.TestCase):\n             runner.create_crawler(\"spider2\")\n \n         crawler = runner.create_crawler(\"spider1\")\n-        self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n-        self.assertEqual(crawler.spidercls.name, \"spider1\")\n+        assert issubclass(crawler.spidercls, scrapy.Spider)\n+        assert crawler.spidercls.name == \"spider1\"\n \n     def test_bad_spider_modules_exception(self):\n         module = \"tests.test_spiderloader.test_spiders.doesnotexist\"\n@@ -150,10 +147,10 @@ class SpiderLoaderTest(unittest.TestCase):\n                 # at least until all six versions we can import (including botocore.vendored.six)\n                 # are updated to 1.16.0+\n                 w.pop(0)\n-            self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n+            assert \"Could not load spiders from module\" in str(w[0].message)\n \n             spiders = spider_loader.list()\n-            self.assertEqual(spiders, [])\n+            assert not spiders\n \n     def test_syntax_error_exception(self):\n         module = \"tests.test_spiderloader.test_spiders.spider1\"\n@@ -179,14 +176,14 @@ class SpiderLoaderTest(unittest.TestCase):\n                 # at least until all six versions we can import (including botocore.vendored.six)\n                 # are updated to 1.16.0+\n                 w.pop(0)\n-            self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n+            assert \"Could not load spiders from module\" in str(w[0].message)\n \n             spiders = spider_loader.list()\n-            self.assertEqual(spiders, [])\n+            assert not spiders\n \n \n-class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n-    def setUp(self):\n+class TestDuplicateSpiderNameLoader:\n+    def setup_method(self):\n         orig_spiders_dir = module_dir / \"test_spiders\"\n         self.tmpdir = Path(mkdtemp())\n         self.spiders_dir = self.tmpdir / \"test_spiders_xxx\"\n@@ -194,7 +191,7 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n         sys.path.append(str(self.tmpdir))\n         self.settings = Settings({\"SPIDER_MODULES\": [\"test_spiders_xxx\"]})\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         del sys.modules[\"test_spiders_xxx\"]\n         sys.path.remove(str(self.tmpdir))\n \n@@ -208,18 +205,18 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n \n-            self.assertEqual(len(w), 1)\n+            assert len(w) == 1\n             msg = str(w[0].message)\n-            self.assertIn(\"several spiders with the same name\", msg)\n-            self.assertIn(\"'spider3'\", msg)\n-            self.assertTrue(msg.count(\"'spider3'\") == 2)\n+            assert \"several spiders with the same name\" in msg\n+            assert \"'spider3'\" in msg\n+            assert msg.count(\"'spider3'\") == 2\n \n-            self.assertNotIn(\"'spider1'\", msg)\n-            self.assertNotIn(\"'spider2'\", msg)\n-            self.assertNotIn(\"'spider4'\", msg)\n+            assert \"'spider1'\" not in msg\n+            assert \"'spider2'\" not in msg\n+            assert \"'spider4'\" not in msg\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n+            assert spiders == {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n \n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n@@ -236,17 +233,17 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n \n-            self.assertEqual(len(w), 1)\n+            assert len(w) == 1\n             msg = str(w[0].message)\n-            self.assertIn(\"several spiders with the same name\", msg)\n-            self.assertIn(\"'spider1'\", msg)\n-            self.assertTrue(msg.count(\"'spider1'\") == 2)\n+            assert \"several spiders with the same name\" in msg\n+            assert \"'spider1'\" in msg\n+            assert msg.count(\"'spider1'\") == 2\n \n-            self.assertIn(\"'spider2'\", msg)\n-            self.assertTrue(msg.count(\"'spider2'\") == 2)\n+            assert \"'spider2'\" in msg\n+            assert msg.count(\"'spider2'\") == 2\n \n-            self.assertNotIn(\"'spider3'\", msg)\n-            self.assertNotIn(\"'spider4'\", msg)\n+            assert \"'spider3'\" not in msg\n+            assert \"'spider4'\" not in msg\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, {\"spider1\", \"spider2\", \"spider3\", \"spider4\"})\n+            assert spiders == {\"spider1\", \"spider2\", \"spider3\", \"spider4\"}\n\n@@ -18,7 +18,7 @@ from scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler\n \n \n-class SpiderMiddlewareTestCase(TestCase):\n+class TestSpiderMiddleware(TestCase):\n     def setUp(self):\n         self.request = Request(\"http://example.com/index.html\")\n         self.response = Response(self.request.url, request=self.request)\n@@ -41,7 +41,7 @@ class SpiderMiddlewareTestCase(TestCase):\n         return results[0]\n \n \n-class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n+class TestProcessSpiderInputInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_input method\"\"\"\n \n     def test_invalid_process_spider_input(self):\n@@ -51,11 +51,11 @@ class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):\n \n         self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())\n         result = self._scrape_response()\n-        self.assertIsInstance(result, Failure)\n-        self.assertIsInstance(result.value, _InvalidOutput)\n+        assert isinstance(result, Failure)\n+        assert isinstance(result.value, _InvalidOutput)\n \n \n-class ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):\n+class TestProcessSpiderOutputInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_output method\"\"\"\n \n     def test_invalid_process_spider_output(self):\n@@ -65,11 +65,11 @@ class ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):\n \n         self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())\n         result = self._scrape_response()\n-        self.assertIsInstance(result, Failure)\n-        self.assertIsInstance(result.value, _InvalidOutput)\n+        assert isinstance(result, Failure)\n+        assert isinstance(result.value, _InvalidOutput)\n \n \n-class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n+class TestProcessSpiderExceptionInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_exception method\"\"\"\n \n     def test_invalid_process_spider_exception(self):\n@@ -84,11 +84,11 @@ class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):\n         self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n         result = self._scrape_response()\n-        self.assertIsInstance(result, Failure)\n-        self.assertIsInstance(result.value, _InvalidOutput)\n+        assert isinstance(result, Failure)\n+        assert isinstance(result.value, _InvalidOutput)\n \n \n-class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n+class TestProcessSpiderExceptionReRaise(TestSpiderMiddleware):\n     \"\"\"Re raise the exception by returning None\"\"\"\n \n     def test_process_spider_exception_return_none(self):\n@@ -103,11 +103,11 @@ class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):\n         self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n         result = self._scrape_response()\n-        self.assertIsInstance(result, Failure)\n-        self.assertIsInstance(result.value, ZeroDivisionError)\n+        assert isinstance(result, Failure)\n+        assert isinstance(result.value, ZeroDivisionError)\n \n \n-class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n+class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n     \"\"\"Helpers for testing sync, async and mixed middlewares.\n \n     Should work for process_spider_output and, when it's supported, process_start_requests.\n@@ -148,14 +148,13 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n             result = yield self._get_middleware_result(\n                 *mw_classes, start_index=start_index\n             )\n-        self.assertIsInstance(result, Iterable)\n+        assert isinstance(result, Iterable)\n         result_list = list(result)\n-        self.assertEqual(len(result_list), self.RESULT_COUNT)\n-        self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n-        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n-        self.assertEqual(\n-            \"doesn't support asynchronous spider output\" in str(log),\n-            ProcessSpiderOutputSimpleMiddleware in mw_classes,\n+        assert len(result_list) == self.RESULT_COUNT\n+        assert isinstance(result_list[0], self.ITEM_TYPE)\n+        assert (\"downgraded to a non-async\" in str(log)) == downgrade\n+        assert (\"doesn't support asynchronous spider output\" in str(log)) == (\n+            ProcessSpiderOutputSimpleMiddleware in mw_classes\n         )\n \n     @defer.inlineCallbacks\n@@ -166,11 +165,11 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n             result = yield self._get_middleware_result(\n                 *mw_classes, start_index=start_index\n             )\n-        self.assertIsInstance(result, AsyncIterator)\n+        assert isinstance(result, AsyncIterator)\n         result_list = yield deferred_from_coro(collect_asyncgen(result))\n-        self.assertEqual(len(result_list), self.RESULT_COUNT)\n-        self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n-        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n+        assert len(result_list) == self.RESULT_COUNT\n+        assert isinstance(result_list[0], self.ITEM_TYPE)\n+        assert (\"downgraded to a non-async\" in str(log)) == downgrade\n \n \n class ProcessSpiderOutputSimpleMiddleware:\n@@ -212,7 +211,7 @@ class ProcessSpiderExceptionAsyncIterableMiddleware:\n         yield {\"foo\": 3}\n \n \n-class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n+class TestProcessSpiderOutputSimple(TestBaseAsyncSpiderMiddleware):\n     \"\"\"process_spider_output tests for simple callbacks\"\"\"\n \n     ITEM_TYPE = dict\n@@ -257,7 +256,7 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n         return self._test_asyncgen_base(self.MW_UNIVERSAL, self.MW_ASYNCGEN)\n \n \n-class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n+class TestProcessSpiderOutputAsyncGen(TestProcessSpiderOutputSimple):\n     \"\"\"process_spider_output tests for async generator callbacks\"\"\"\n \n     async def _scrape_func(self, *args, **kwargs):\n@@ -297,7 +296,7 @@ class ProcessSpiderOutputCoroutineMiddleware:\n         return result\n \n \n-class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n+class TestProcessSpiderOutputInvalidResult(TestBaseAsyncSpiderMiddleware):\n     @defer.inlineCallbacks\n     def test_non_iterable(self):\n         with pytest.raises(\n@@ -324,7 +323,7 @@ class ProcessStartRequestsSimpleMiddleware:\n         yield from start_requests\n \n \n-class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n+class TestProcessStartRequestsSimple(TestBaseAsyncSpiderMiddleware):\n     \"\"\"process_start_requests tests for simple start_requests\"\"\"\n \n     ITEM_TYPE = (Request, dict)\n@@ -373,67 +372,65 @@ class UniversalMiddlewareBothAsync:\n         yield\n \n \n-class UniversalMiddlewareManagerTest(TestCase):\n-    def setUp(self):\n+class TestUniversalMiddlewareManager:\n+    def setup_method(self):\n         self.mwman = SpiderMiddlewareManager()\n \n     def test_simple_mw(self):\n         mw = ProcessSpiderOutputSimpleMiddleware()\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(\n-            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        assert (\n+            self.mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n         )\n \n     def test_async_mw(self):\n         mw = ProcessSpiderOutputAsyncGenMiddleware()\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(\n-            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        assert (\n+            self.mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n         )\n \n     def test_universal_mw(self):\n         mw = ProcessSpiderOutputUniversalMiddleware()\n         self.mwman._add_middleware(mw)\n-        self.assertEqual(\n-            self.mwman.methods[\"process_spider_output\"][0],\n-            (mw.process_spider_output, mw.process_spider_output_async),\n+        assert self.mwman.methods[\"process_spider_output\"][0] == (\n+            mw.process_spider_output,\n+            mw.process_spider_output_async,\n         )\n \n     def test_universal_mw_no_sync(self):\n         with LogCapture() as log:\n             self.mwman._add_middleware(UniversalMiddlewareNoSync())\n-        self.assertIn(\n+        assert (\n             \"UniversalMiddlewareNoSync has process_spider_output_async\"\n-            \" without process_spider_output\",\n-            str(log),\n+            \" without process_spider_output\" in str(log)\n         )\n-        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n+        assert self.mwman.methods[\"process_spider_output\"][0] is None\n \n     def test_universal_mw_both_sync(self):\n         mw = UniversalMiddlewareBothSync()\n         with LogCapture() as log:\n             self.mwman._add_middleware(mw)\n-        self.assertIn(\n+        assert (\n             \"UniversalMiddlewareBothSync.process_spider_output_async \"\n-            \"is not an async generator function\",\n-            str(log),\n+            \"is not an async generator function\" in str(log)\n         )\n-        self.assertEqual(\n-            self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n+        assert (\n+            self.mwman.methods[\"process_spider_output\"][0] == mw.process_spider_output  # pylint: disable=comparison-with-callable\n         )\n \n     def test_universal_mw_both_async(self):\n         with LogCapture() as log:\n             self.mwman._add_middleware(UniversalMiddlewareBothAsync())\n-        self.assertIn(\n+        assert (\n             \"UniversalMiddlewareBothAsync.process_spider_output \"\n-            \"is an async generator function while process_spider_output_async exists\",\n-            str(log),\n+            \"is an async generator function while process_spider_output_async exists\"\n+            in str(log)\n         )\n-        self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n+        assert self.mwman.methods[\"process_spider_output\"][0] is None\n \n \n-class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n+class TestBuiltinMiddlewareSimple(TestBaseAsyncSpiderMiddleware):\n     ITEM_TYPE = dict\n     MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n@@ -474,7 +471,7 @@ class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n         return self._test_simple_base(self.MW_UNIVERSAL)\n \n \n-class BuiltinMiddlewareAsyncGenTest(BuiltinMiddlewareSimpleTest):\n+class TestBuiltinMiddlewareAsyncGen(TestBuiltinMiddlewareSimple):\n     async def _scrape_func(self, *args, **kwargs):\n         for item in super()._scrape_func():\n             yield item\n@@ -503,7 +500,7 @@ class BuiltinMiddlewareAsyncGenTest(BuiltinMiddlewareSimpleTest):\n         return self._test_asyncgen_base(self.MW_UNIVERSAL)\n \n \n-class ProcessSpiderExceptionTest(BaseAsyncSpiderMiddlewareTestCase):\n+class TestProcessSpiderException(TestBaseAsyncSpiderMiddleware):\n     ITEM_TYPE = dict\n     MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n\n@@ -1,5 +1,3 @@\n-from unittest import TestCase\n-\n from scrapy.http import Request, Response\n from scrapy.spidermiddlewares.depth import DepthMiddleware\n from scrapy.spiders import Spider\n@@ -7,8 +5,8 @@ from scrapy.statscollectors import StatsCollector\n from scrapy.utils.test import get_crawler\n \n \n-class TestDepthMiddleware(TestCase):\n-    def setUp(self):\n+class TestDepthMiddleware:\n+    def setup_method(self):\n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider(\"scrapytest.org\")\n \n@@ -24,18 +22,18 @@ class TestDepthMiddleware(TestCase):\n         result = [Request(\"http://scrapytest.org\")]\n \n         out = list(self.mw.process_spider_output(resp, result, self.spider))\n-        self.assertEqual(out, result)\n+        assert out == result\n \n         rdc = self.stats.get_value(\"request_depth_count/1\", spider=self.spider)\n-        self.assertEqual(rdc, 1)\n+        assert rdc == 1\n \n         req.meta[\"depth\"] = 1\n \n         out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n-        self.assertEqual(out2, [])\n+        assert not out2\n \n         rdm = self.stats.get_value(\"request_depth_max\", spider=self.spider)\n-        self.assertEqual(rdm, 1)\n+        assert rdm == 1\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         self.stats.close_spider(self.spider, \"\")\n\n@@ -1,10 +1,9 @@\n import logging\n-from unittest import TestCase\n \n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.trial.unittest import TestCase as TrialTestCase\n+from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n@@ -59,8 +58,8 @@ def _responses(request, status_codes):\n     return responses\n \n \n-class TestHttpErrorMiddleware(TestCase):\n-    def setUp(self):\n+class TestHttpErrorMiddleware:\n+    def setup_method(self):\n         crawler = get_crawler(Spider)\n         self.spider = Spider.from_crawler(crawler, name=\"foo\")\n         self.mw = HttpErrorMiddleware(Settings({}))\n@@ -68,19 +67,20 @@ class TestHttpErrorMiddleware(TestCase):\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n \n     def test_process_spider_input(self):\n-        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n+        assert self.mw.process_spider_input(self.res200, self.spider) is None\n         with pytest.raises(HttpError):\n             self.mw.process_spider_input(self.res404, self.spider)\n \n     def test_process_spider_exception(self):\n-        self.assertEqual(\n-            [],\n+        assert (\n             self.mw.process_spider_exception(\n                 self.res404, HttpError(self.res404), self.spider\n-            ),\n             )\n-        self.assertIsNone(\n+            == []\n+        )\n+        assert (\n             self.mw.process_spider_exception(self.res404, Exception(), self.spider)\n+            is None\n         )\n \n     def test_handle_httpstatus_list(self):\n@@ -88,26 +88,26 @@ class TestHttpErrorMiddleware(TestCase):\n         res.request = Request(\n             \"http://scrapytest.org\", meta={\"handle_httpstatus_list\": [404]}\n         )\n-        self.assertIsNone(self.mw.process_spider_input(res, self.spider))\n+        assert self.mw.process_spider_input(res, self.spider) is None\n \n         self.spider.handle_httpstatus_list = [404]\n-        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n+        assert self.mw.process_spider_input(self.res404, self.spider) is None\n \n \n-class TestHttpErrorMiddlewareSettings(TestCase):\n+class TestHttpErrorMiddlewareSettings:\n     \"\"\"Similar test, but with settings\"\"\"\n \n-    def setUp(self):\n+    def setup_method(self):\n         self.spider = Spider(\"foo\")\n         self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOWED_CODES\": (402,)}))\n         self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n+        assert self.mw.process_spider_input(self.res200, self.spider) is None\n         with pytest.raises(HttpError):\n             self.mw.process_spider_input(self.res404, self.spider)\n-        self.assertIsNone(self.mw.process_spider_input(self.res402, self.spider))\n+        assert self.mw.process_spider_input(self.res402, self.spider) is None\n \n     def test_meta_overrides_settings(self):\n         request = Request(\n@@ -118,27 +118,27 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n+        assert self.mw.process_spider_input(res404, self.spider) is None\n         with pytest.raises(HttpError):\n             self.mw.process_spider_input(res402, self.spider)\n \n     def test_spider_override_settings(self):\n         self.spider.handle_httpstatus_list = [404]\n-        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n+        assert self.mw.process_spider_input(self.res404, self.spider) is None\n         with pytest.raises(HttpError):\n             self.mw.process_spider_input(self.res402, self.spider)\n \n \n-class TestHttpErrorMiddlewareHandleAll(TestCase):\n-    def setUp(self):\n+class TestHttpErrorMiddlewareHandleAll:\n+    def setup_method(self):\n         self.spider = Spider(\"foo\")\n         self.mw = HttpErrorMiddleware(Settings({\"HTTPERROR_ALLOW_ALL\": True}))\n         self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n+        assert self.mw.process_spider_input(self.res200, self.spider) is None\n+        assert self.mw.process_spider_input(self.res404, self.spider) is None\n \n     def test_meta_overrides_settings(self):\n         request = Request(\n@@ -149,7 +149,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n+        assert self.mw.process_spider_input(res404, self.spider) is None\n         with pytest.raises(HttpError):\n             self.mw.process_spider_input(res402, self.spider)\n \n@@ -169,10 +169,10 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n \n         with pytest.raises(HttpError):\n             mw.process_spider_input(res404, self.spider)\n-        self.assertIsNone(mw.process_spider_input(res402, self.spider))\n+        assert mw.process_spider_input(res402, self.spider) is None\n \n \n-class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n+class TestHttpErrorMiddlewareIntegrational(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -187,28 +187,28 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         crawler = get_crawler(_HttpErrorSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert not crawler.spider.skipped, crawler.spider.skipped\n-        self.assertEqual(crawler.spider.parsed, {\"200\"})\n-        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n+        assert crawler.spider.parsed == {\"200\"}\n+        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n \n         get_value = crawler.stats.get_value\n-        self.assertEqual(get_value(\"httperror/response_ignored_count\"), 3)\n-        self.assertEqual(get_value(\"httperror/response_ignored_status_count/404\"), 1)\n-        self.assertEqual(get_value(\"httperror/response_ignored_status_count/402\"), 1)\n-        self.assertEqual(get_value(\"httperror/response_ignored_status_count/500\"), 1)\n+        assert get_value(\"httperror/response_ignored_count\") == 3\n+        assert get_value(\"httperror/response_ignored_status_count/404\") == 1\n+        assert get_value(\"httperror/response_ignored_status_count/402\") == 1\n+        assert get_value(\"httperror/response_ignored_status_count/500\") == 1\n \n     @defer.inlineCallbacks\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})\n-        self.assertEqual(crawler.spider.parsed, {\"200\", \"402\"})\n-        self.assertEqual(crawler.spider.skipped, {\"402\"})\n-        self.assertEqual(crawler.spider.failed, {\"404\", \"500\"})\n+        assert crawler.spider.parsed == {\"200\", \"402\"}\n+        assert crawler.spider.skipped == {\"402\"}\n+        assert crawler.spider.failed == {\"404\", \"500\"}\n \n-        self.assertIn(\"Ignoring response <404\", str(log))\n-        self.assertIn(\"Ignoring response <500\", str(log))\n-        self.assertNotIn(\"Ignoring response <200\", str(log))\n-        self.assertNotIn(\"Ignoring response <402\", str(log))\n+        assert \"Ignoring response <404\" in str(log)\n+        assert \"Ignoring response <500\" in str(log)\n+        assert \"Ignoring response <200\" not in str(log)\n+        assert \"Ignoring response <402\" not in str(log)\n \n     @defer.inlineCallbacks\n     def test_logging_level(self):\n@@ -216,22 +216,22 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.INFO) as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(crawler.spider.parsed, {\"200\"})\n-        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n+        assert crawler.spider.parsed == {\"200\"}\n+        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n \n-        self.assertIn(\"Ignoring response <402\", str(log))\n-        self.assertIn(\"Ignoring response <404\", str(log))\n-        self.assertIn(\"Ignoring response <500\", str(log))\n-        self.assertNotIn(\"Ignoring response <200\", str(log))\n+        assert \"Ignoring response <402\" in str(log)\n+        assert \"Ignoring response <404\" in str(log)\n+        assert \"Ignoring response <500\" in str(log)\n+        assert \"Ignoring response <200\" not in str(log)\n \n         # with level WARNING, we shouldn't capture anything from HttpError\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.WARNING) as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(crawler.spider.parsed, {\"200\"})\n-        self.assertEqual(crawler.spider.failed, {\"404\", \"402\", \"500\"})\n+        assert crawler.spider.parsed == {\"200\"}\n+        assert crawler.spider.failed == {\"404\", \"402\", \"500\"}\n \n-        self.assertNotIn(\"Ignoring response <402\", str(log))\n-        self.assertNotIn(\"Ignoring response <404\", str(log))\n-        self.assertNotIn(\"Ignoring response <500\", str(log))\n-        self.assertNotIn(\"Ignoring response <200\", str(log))\n+        assert \"Ignoring response <402\" not in str(log)\n+        assert \"Ignoring response <404\" not in str(log)\n+        assert \"Ignoring response <500\" not in str(log)\n+        assert \"Ignoring response <200\" not in str(log)\n\n@@ -1,5 +1,4 @@\n import warnings\n-from unittest import TestCase\n from urllib.parse import urlparse\n \n from scrapy.http import Request, Response\n@@ -8,8 +7,8 @@ from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n-class TestOffsiteMiddleware(TestCase):\n-    def setUp(self):\n+class TestOffsiteMiddleware:\n+    def setup_method(self):\n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider(**self._get_spiderargs())\n         self.mw = OffsiteMiddleware.from_crawler(crawler)\n@@ -46,7 +45,7 @@ class TestOffsiteMiddleware(TestCase):\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEqual(out, onsite_reqs)\n+        assert out == onsite_reqs\n \n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n@@ -57,7 +56,7 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n         res = Response(\"http://scrapytest.org\")\n         reqs = [Request(\"http://a.com/b.html\"), Request(\"http://b.com/1\")]\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEqual(out, reqs)\n+        assert out == reqs\n \n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n@@ -77,7 +76,7 @@ class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n         res = Response(\"http://scrapytest.org\")\n         reqs = [Request(\"http://scrapytest.org/1\")]\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEqual(out, reqs)\n+        assert out == reqs\n \n \n class TestOffsiteMiddleware5(TestOffsiteMiddleware4):\n\n@@ -324,9 +324,9 @@ class TestSpiderMiddleware(TestCase):\n         was enqueued from the recovery middleware)\n         \"\"\"\n         log = yield self.crawl_log(RecoverySpider)\n-        self.assertIn(\"Middleware: TabError exception caught\", str(log))\n-        self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n-        self.assertIn(\"'item_scraped_count': 3\", str(log))\n+        assert \"Middleware: TabError exception caught\" in str(log)\n+        assert str(log).count(\"Middleware: TabError exception caught\") == 1\n+        assert \"'item_scraped_count': 3\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_recovery_asyncgen(self):\n@@ -334,9 +334,9 @@ class TestSpiderMiddleware(TestCase):\n         Same as test_recovery but with an async callback.\n         \"\"\"\n         log = yield self.crawl_log(RecoveryAsyncGenSpider)\n-        self.assertIn(\"Middleware: TabError exception caught\", str(log))\n-        self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n-        self.assertIn(\"'item_scraped_count': 3\", str(log))\n+        assert \"Middleware: TabError exception caught\" in str(log)\n+        assert str(log).count(\"Middleware: TabError exception caught\") == 1\n+        assert \"'item_scraped_count': 3\" in str(log)\n \n     @defer.inlineCallbacks\n     def test_process_spider_input_without_errback(self):\n@@ -345,8 +345,8 @@ class TestSpiderMiddleware(TestCase):\n         process_spider_exception chain from the start if the Request has no errback\n         \"\"\"\n         log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithoutErrback)\n-        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n-        self.assertIn(\"Middleware: IndexError exception caught\", str(log1))\n+        assert \"Middleware: will raise IndexError\" in str(log1)\n+        assert \"Middleware: IndexError exception caught\" in str(log1)\n \n     @defer.inlineCallbacks\n     def test_process_spider_input_with_errback(self):\n@@ -355,12 +355,12 @@ class TestSpiderMiddleware(TestCase):\n         process_spider_exception chain if the Request has an errback\n         \"\"\"\n         log1 = yield self.crawl_log(ProcessSpiderInputSpiderWithErrback)\n-        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n-        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n-        self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n-        self.assertIn(\"{'from': 'errback'}\", str(log1))\n-        self.assertNotIn(\"{'from': 'callback'}\", str(log1))\n-        self.assertIn(\"'item_scraped_count': 1\", str(log1))\n+        assert \"Middleware: IndexError exception caught\" not in str(log1)\n+        assert \"Middleware: will raise IndexError\" in str(log1)\n+        assert \"Got a Failure on the Request errback\" in str(log1)\n+        assert \"{'from': 'errback'}\" in str(log1)\n+        assert \"{'from': 'callback'}\" not in str(log1)\n+        assert \"'item_scraped_count': 1\" in str(log1)\n \n     @defer.inlineCallbacks\n     def test_generator_callback(self):\n@@ -370,8 +370,8 @@ class TestSpiderMiddleware(TestCase):\n         exception is raised should be processed normally.\n         \"\"\"\n         log2 = yield self.crawl_log(GeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n+        assert \"Middleware: ImportError exception caught\" in str(log2)\n+        assert \"'item_scraped_count': 2\" in str(log2)\n \n     @defer.inlineCallbacks\n     def test_async_generator_callback(self):\n@@ -379,8 +379,8 @@ class TestSpiderMiddleware(TestCase):\n         Same as test_generator_callback but with an async callback.\n         \"\"\"\n         log2 = yield self.crawl_log(AsyncGeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n+        assert \"Middleware: ImportError exception caught\" in str(log2)\n+        assert \"'item_scraped_count': 2\" in str(log2)\n \n     @defer.inlineCallbacks\n     def test_generator_callback_right_after_callback(self):\n@@ -389,8 +389,8 @@ class TestSpiderMiddleware(TestCase):\n         even if the middleware is placed right after the spider\n         \"\"\"\n         log21 = yield self.crawl_log(GeneratorCallbackSpiderMiddlewareRightAfterSpider)\n-        self.assertIn(\"Middleware: ImportError exception caught\", str(log21))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log21))\n+        assert \"Middleware: ImportError exception caught\" in str(log21)\n+        assert \"'item_scraped_count': 2\" in str(log21)\n \n     @defer.inlineCallbacks\n     def test_not_a_generator_callback(self):\n@@ -399,8 +399,8 @@ class TestSpiderMiddleware(TestCase):\n         be caught by the process_spider_exception chain. No items should be processed.\n         \"\"\"\n         log3 = yield self.crawl_log(NotGeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n-        self.assertNotIn(\"item_scraped_count\", str(log3))\n+        assert \"Middleware: ZeroDivisionError exception caught\" in str(log3)\n+        assert \"item_scraped_count\" not in str(log3)\n \n     @defer.inlineCallbacks\n     def test_not_a_generator_callback_right_after_callback(self):\n@@ -411,8 +411,8 @@ class TestSpiderMiddleware(TestCase):\n         log31 = yield self.crawl_log(\n             NotGeneratorCallbackSpiderMiddlewareRightAfterSpider\n         )\n-        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log31))\n-        self.assertNotIn(\"item_scraped_count\", str(log31))\n+        assert \"Middleware: ZeroDivisionError exception caught\" in str(log31)\n+        assert \"item_scraped_count\" not in str(log31)\n \n     @defer.inlineCallbacks\n     def test_generator_output_chain(self):\n@@ -425,22 +425,22 @@ class TestSpiderMiddleware(TestCase):\n         process_spider_exception chain)\n         \"\"\"\n         log4 = yield self.crawl_log(GeneratorOutputChainSpider)\n-        self.assertIn(\"'item_scraped_count': 2\", str(log4))\n-        self.assertIn(\n-            \"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4),\n+        assert \"'item_scraped_count': 2\" in str(log4)\n+        assert (\n+            \"GeneratorRecoverMiddleware.process_spider_exception: LookupError caught\"\n+            in str(log4)\n         )\n-        self.assertIn(\n-            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4),\n+        assert (\n+            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught\"\n+            in str(log4)\n         )\n-        self.assertNotIn(\n-            \"GeneratorFailMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4),\n+        assert (\n+            \"GeneratorFailMiddleware.process_spider_exception: LookupError caught\"\n+            not in str(log4)\n         )\n-        self.assertNotIn(\n-            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught\",\n-            str(log4),\n+        assert (\n+            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught\"\n+            not in str(log4)\n         )\n         item_from_callback = {\n             \"processed\": [\n@@ -457,9 +457,9 @@ class TestSpiderMiddleware(TestCase):\n                 \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n             ]\n         }\n-        self.assertIn(str(item_from_callback), str(log4))\n-        self.assertIn(str(item_recovered), str(log4))\n-        self.assertNotIn(\"parse-second-item\", str(log4))\n+        assert str(item_from_callback) in str(log4)\n+        assert str(item_recovered) in str(log4)\n+        assert \"parse-second-item\" not in str(log4)\n \n     @defer.inlineCallbacks\n     def test_not_a_generator_output_chain(self):\n@@ -472,22 +472,22 @@ class TestSpiderMiddleware(TestCase):\n         from the spider callback are lost)\n         \"\"\"\n         log5 = yield self.crawl_log(NotGeneratorOutputChainSpider)\n-        self.assertIn(\"'item_scraped_count': 1\", str(log5))\n-        self.assertIn(\n-            \"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5),\n+        assert \"'item_scraped_count': 1\" in str(log5)\n+        assert (\n+            \"GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught\"\n+            in str(log5)\n         )\n-        self.assertIn(\n-            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5),\n+        assert (\n+            \"GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught\"\n+            in str(log5)\n         )\n-        self.assertNotIn(\n-            \"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5),\n+        assert (\n+            \"GeneratorFailMiddleware.process_spider_exception: ReferenceError caught\"\n+            not in str(log5)\n         )\n-        self.assertNotIn(\n-            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught\",\n-            str(log5),\n+        assert (\n+            \"GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught\"\n+            not in str(log5)\n         )\n         item_recovered = {\n             \"processed\": [\n@@ -495,6 +495,6 @@ class TestSpiderMiddleware(TestCase):\n                 \"NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output\",\n             ]\n         }\n-        self.assertIn(str(item_recovered), str(log5))\n-        self.assertNotIn(\"parse-first-item\", str(log5))\n-        self.assertNotIn(\"parse-second-item\", str(log5))\n+        assert str(item_recovered) in str(log5)\n+        assert \"parse-first-item\" not in str(log5)\n+        assert \"parse-second-item\" not in str(log5)\n\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import warnings\n from typing import Any\n-from unittest import TestCase\n from urllib.parse import urlparse\n \n import pytest\n@@ -35,7 +34,7 @@ from scrapy.spidermiddlewares.referer import (\n from scrapy.spiders import Spider\n \n \n-class TestRefererMiddleware(TestCase):\n+class TestRefererMiddleware:\n     req_meta: dict[str, Any] = {}\n     resp_headers: dict[str, str] = {}\n     settings: dict[str, Any] = {}\n@@ -43,7 +42,7 @@ class TestRefererMiddleware(TestCase):\n         (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n     ]\n \n-    def setUp(self):\n+    def setup_method(self):\n         self.spider = Spider(\"foo\")\n         settings = Settings(self.settings)\n         self.mw = RefererMiddleware(settings)\n@@ -59,7 +58,7 @@ class TestRefererMiddleware(TestCase):\n             response = self.get_response(origin)\n             request = self.get_request(target)\n             out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEqual(out[0].headers.get(\"Referer\"), referrer)\n+            assert out[0].headers.get(\"Referer\") == referrer\n \n \n class MixinDefault:\n@@ -773,7 +772,7 @@ class TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n     req_meta = {\"referrer_policy\": POLICY_UNSAFE_URL}\n \n \n-class TestRequestMetaSettingFallback(TestCase):\n+class TestRequestMetaSettingFallback:\n     params = [\n         (\n             # When an unknown policy is referenced in Request.meta\n@@ -844,14 +843,14 @@ class TestRequestMetaSettingFallback(TestCase):\n \n             with warnings.catch_warnings(record=True) as w:\n                 policy = mw.policy(response, request)\n-                self.assertIsInstance(policy, policy_class)\n+                assert isinstance(policy, policy_class)\n \n                 if check_warning:\n-                    self.assertEqual(len(w), 1)\n-                    self.assertEqual(w[0].category, RuntimeWarning, w[0].message)\n+                    assert len(w) == 1\n+                    assert w[0].category is RuntimeWarning, w[0].message\n \n \n-class TestSettingsPolicyByName(TestCase):\n+class TestSettingsPolicyByName:\n     def test_valid_name(self):\n         for s, p in [\n             (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n@@ -866,7 +865,7 @@ class TestSettingsPolicyByName(TestCase):\n         ]:\n             settings = Settings({\"REFERRER_POLICY\": s})\n             mw = RefererMiddleware(settings)\n-            self.assertEqual(mw.default_policy, p)\n+            assert mw.default_policy == p\n \n     def test_valid_name_casevariants(self):\n         for s, p in [\n@@ -882,7 +881,7 @@ class TestSettingsPolicyByName(TestCase):\n         ]:\n             settings = Settings({\"REFERRER_POLICY\": s.upper()})\n             mw = RefererMiddleware(settings)\n-            self.assertEqual(mw.default_policy, p)\n+            assert mw.default_policy == p\n \n     def test_invalid_name(self):\n         settings = Settings({\"REFERRER_POLICY\": \"some-custom-unknown-policy\"})\n@@ -902,7 +901,7 @@ class TestSettingsPolicyByName(TestCase):\n             }\n         )\n         mw1 = RefererMiddleware(settings1)\n-        self.assertEqual(mw1.default_policy, StrictOriginWhenCrossOriginPolicy)\n+        assert mw1.default_policy == StrictOriginWhenCrossOriginPolicy\n \n         # test parsing with space(s) after the comma\n         settings2 = Settings(\n@@ -915,7 +914,7 @@ class TestSettingsPolicyByName(TestCase):\n             }\n         )\n         mw2 = RefererMiddleware(settings2)\n-        self.assertEqual(mw2.default_policy, UnsafeUrlPolicy)\n+        assert mw2.default_policy == UnsafeUrlPolicy\n \n     def test_multiple_policy_tokens_all_invalid(self):\n         settings = Settings(\n@@ -1003,7 +1002,7 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n         ),\n     ]\n \n-    def setUp(self):\n+    def setup_method(self):\n         self.spider = Spider(\"foo\")\n         settings = Settings(self.settings)\n         self.referrermw = RefererMiddleware(settings)\n@@ -1023,7 +1022,7 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n             out = list(\n                 self.referrermw.process_spider_output(response, [request], self.spider)\n             )\n-            self.assertEqual(out[0].headers.get(\"Referer\"), init_referrer)\n+            assert out[0].headers.get(\"Referer\") == init_referrer\n \n             for status, url in redirections:\n                 response = Response(\n@@ -1035,7 +1034,7 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n                 self.referrermw.request_scheduled(request, self.spider)\n \n             assert isinstance(request, Request)\n-            self.assertEqual(request.headers.get(\"Referer\"), final_referrer)\n+            assert request.headers.get(\"Referer\") == final_referrer\n \n \n class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n\n@@ -1,5 +1,3 @@\n-from unittest import TestCase\n-\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n@@ -8,8 +6,8 @@ from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n-class TestUrlLengthMiddleware(TestCase):\n-    def setUp(self):\n+class TestUrlLengthMiddleware:\n+    def setup_method(self):\n         self.maxlength = 25\n         crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n@@ -27,7 +25,7 @@ class TestUrlLengthMiddleware(TestCase):\n         )\n \n     def test_middleware_works(self):\n-        self.assertEqual(self.process_spider_output(), [self.short_url_req])\n+        assert self.process_spider_output() == [self.short_url_req]\n \n     def test_logging(self):\n         with LogCapture() as log:\n@@ -36,6 +34,6 @@ class TestUrlLengthMiddleware(TestCase):\n         ric = self.stats.get_value(\n             \"urllength/request_ignored_count\", spider=self.spider\n         )\n-        self.assertEqual(ric, 1)\n+        assert ric == 1\n \n-        self.assertIn(f\"Ignoring link (url length > {self.maxlength})\", str(log))\n+        assert f\"Ignoring link (url length > {self.maxlength})\" in str(log)\n\n@@ -3,7 +3,6 @@ from datetime import datetime, timezone\n from tempfile import mkdtemp\n \n import pytest\n-from twisted.trial import unittest\n \n from scrapy.exceptions import NotConfigured\n from scrapy.extensions.spiderstate import SpiderState\n@@ -11,7 +10,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n-class SpiderStateTest(unittest.TestCase):\n+class TestSpiderState:\n     def test_store_load(self):\n         jobdir = mkdtemp()\n         try:\n@@ -27,7 +26,7 @@ class SpiderStateTest(unittest.TestCase):\n             spider2 = Spider(name=\"default\")\n             ss2 = SpiderState(jobdir)\n             ss2.spider_opened(spider2)\n-            self.assertEqual(spider.state, {\"one\": 1, \"dt\": dt})\n+            assert spider.state == {\"one\": 1, \"dt\": dt}\n             ss2.spider_closed(spider2)\n         finally:\n             shutil.rmtree(jobdir)\n@@ -38,7 +37,7 @@ class SpiderStateTest(unittest.TestCase):\n         spider = Spider(name=\"default\")\n         ss = SpiderState()\n         ss.spider_opened(spider)\n-        self.assertEqual(spider.state, {})\n+        assert spider.state == {}\n         ss.spider_closed(spider)\n \n     def test_not_configured(self):\n\n@@ -50,9 +50,9 @@ class FifoDiskQueueTestMixin:\n         q.push(\"a\")\n         q.push(123)\n         q.push({\"a\": \"dict\"})\n-        self.assertEqual(q.pop(), \"a\")\n-        self.assertEqual(q.pop(), 123)\n-        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n+        assert q.pop() == \"a\"\n+        assert q.pop() == 123\n+        assert q.pop() == {\"a\": \"dict\"}\n \n     test_nonserializable_object = nonserializable_object_test\n \n@@ -92,7 +92,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         q.push(i)\n         i2 = q.pop()\n         assert isinstance(i2, MyItem)\n-        self.assertEqual(i, i2)\n+        assert i == i2\n \n     def test_serialize_loader(self):\n         q = self.queue()\n@@ -101,7 +101,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         loader2 = q.pop()\n         assert isinstance(loader2, MyLoader)\n         assert loader2.default_item_class is MyItem\n-        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n+        assert loader2.name_out(\"x\") == \"xx\"\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n@@ -110,23 +110,26 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         q.push(r)\n         r2 = q.pop()\n         assert isinstance(r2, Request)\n-        self.assertEqual(r.url, r2.url)\n+        assert r.url == r2.url\n         assert r2.meta[\"request\"] is r2\n \n     def test_non_pickable_object(self):\n         q = self.queue()\n-        try:\n+        with pytest.raises(\n+            ValueError,\n+            match=\"Can't (get|pickle) local object|Can't pickle .*: it's not found as\",\n+        ) as exc_info:\n             q.push(lambda x: x)\n-        except ValueError as exc:\n         if hasattr(sys, \"pypy_version_info\"):\n-                self.assertIsInstance(exc.__context__, pickle.PicklingError)\n+            assert isinstance(exc_info.value.__context__, pickle.PicklingError)\n         else:\n-                self.assertIsInstance(exc.__context__, AttributeError)\n+            assert isinstance(exc_info.value.__context__, AttributeError)\n         sel = Selector(text=\"<html><body><p>some text</p></body></html>\")\n-        try:\n+        with pytest.raises(\n+            ValueError, match=\"can't pickle Selector objects\"\n+        ) as exc_info:\n             q.push(sel)\n-        except ValueError as exc:\n-            self.assertIsInstance(exc.__context__, TypeError)\n+        assert isinstance(exc_info.value.__context__, TypeError)\n \n \n class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n@@ -151,9 +154,9 @@ class LifoDiskQueueTestMixin:\n         q.push(\"a\")\n         q.push(123)\n         q.push({\"a\": \"dict\"})\n-        self.assertEqual(q.pop(), {\"a\": \"dict\"})\n-        self.assertEqual(q.pop(), 123)\n-        self.assertEqual(q.pop(), \"a\")\n+        assert q.pop() == {\"a\": \"dict\"}\n+        assert q.pop() == 123\n+        assert q.pop() == \"a\"\n \n     test_nonserializable_object = nonserializable_object_test\n \n@@ -173,7 +176,7 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n         q.push(i)\n         i2 = q.pop()\n         assert isinstance(i2, MyItem)\n-        self.assertEqual(i, i2)\n+        assert i == i2\n \n     def test_serialize_loader(self):\n         q = self.queue()\n@@ -182,7 +185,7 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n         loader2 = q.pop()\n         assert isinstance(loader2, MyLoader)\n         assert loader2.default_item_class is MyItem\n-        self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n+        assert loader2.name_out(\"x\") == \"xx\"\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n@@ -191,5 +194,5 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n         q.push(r)\n         r2 = q.pop()\n         assert isinstance(r2, Request)\n-        self.assertEqual(r.url, r2.url)\n+        assert r.url == r2.url\n         assert r2.meta[\"request\"] is r2\n\n@@ -22,14 +22,14 @@ from scrapy.squeues import (\n from scrapy.utils.test import get_crawler\n \n \n-class BaseQueueTestCase(unittest.TestCase):\n-    def setUp(self):\n+class TestBaseQueue:\n+    def setup_method(self):\n         self.tmpdir = tempfile.mkdtemp(prefix=\"scrapy-queue-tests-\")\n         self.qpath = self.tempfilename()\n         self.qdir = tempfile.mkdtemp()\n         self.crawler = get_crawler(Spider)\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         shutil.rmtree(self.tmpdir)\n \n     def tempfilename(self):\n@@ -48,36 +48,36 @@ class RequestQueueTestMixin:\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         req = Request(\"http://www.example.com\")\n         q.push(req)\n-        self.assertEqual(len(q), 1)\n-        self.assertEqual(q.peek().url, req.url)\n-        self.assertEqual(q.pop().url, req.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 1\n+        assert q.peek().url == req.url\n+        assert q.pop().url == req.url\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         q.close()\n \n     def test_one_element_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.pop() is None\n         req = Request(\"http://www.example.com\")\n         q.push(req)\n-        self.assertEqual(len(q), 1)\n+        assert len(q) == 1\n         with pytest.raises(\n             NotImplementedError,\n             match=\"The underlying queue class does not implement 'peek'\",\n         ):\n             q.peek()\n-        self.assertEqual(q.pop().url, req.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert q.pop().url == req.url\n+        assert len(q) == 0\n+        assert q.pop() is None\n         q.close()\n \n \n@@ -86,35 +86,35 @@ class FifoQueueMixin(RequestQueueTestMixin):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n         req2 = Request(\"http://www.example.com/2\")\n         req3 = Request(\"http://www.example.com/3\")\n         q.push(req1)\n         q.push(req2)\n         q.push(req3)\n-        self.assertEqual(len(q), 3)\n-        self.assertEqual(q.peek().url, req1.url)\n-        self.assertEqual(q.pop().url, req1.url)\n-        self.assertEqual(len(q), 2)\n-        self.assertEqual(q.peek().url, req2.url)\n-        self.assertEqual(q.pop().url, req2.url)\n-        self.assertEqual(len(q), 1)\n-        self.assertEqual(q.peek().url, req3.url)\n-        self.assertEqual(q.pop().url, req3.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 3\n+        assert q.peek().url == req1.url\n+        assert q.pop().url == req1.url\n+        assert len(q) == 2\n+        assert q.peek().url == req2.url\n+        assert q.pop().url == req2.url\n+        assert len(q) == 1\n+        assert q.peek().url == req3.url\n+        assert q.pop().url == req3.url\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         q.close()\n \n     def test_fifo_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n         req2 = Request(\"http://www.example.com/2\")\n         req3 = Request(\"http://www.example.com/3\")\n@@ -126,14 +126,14 @@ class FifoQueueMixin(RequestQueueTestMixin):\n             match=\"The underlying queue class does not implement 'peek'\",\n         ):\n             q.peek()\n-        self.assertEqual(len(q), 3)\n-        self.assertEqual(q.pop().url, req1.url)\n-        self.assertEqual(len(q), 2)\n-        self.assertEqual(q.pop().url, req2.url)\n-        self.assertEqual(len(q), 1)\n-        self.assertEqual(q.pop().url, req3.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 3\n+        assert q.pop().url == req1.url\n+        assert len(q) == 2\n+        assert q.pop().url == req2.url\n+        assert len(q) == 1\n+        assert q.pop().url == req3.url\n+        assert len(q) == 0\n+        assert q.pop() is None\n         q.close()\n \n \n@@ -142,35 +142,35 @@ class LifoQueueMixin(RequestQueueTestMixin):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n         req2 = Request(\"http://www.example.com/2\")\n         req3 = Request(\"http://www.example.com/3\")\n         q.push(req1)\n         q.push(req2)\n         q.push(req3)\n-        self.assertEqual(len(q), 3)\n-        self.assertEqual(q.peek().url, req3.url)\n-        self.assertEqual(q.pop().url, req3.url)\n-        self.assertEqual(len(q), 2)\n-        self.assertEqual(q.peek().url, req2.url)\n-        self.assertEqual(q.pop().url, req2.url)\n-        self.assertEqual(len(q), 1)\n-        self.assertEqual(q.peek().url, req1.url)\n-        self.assertEqual(q.pop().url, req1.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.peek())\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 3\n+        assert q.peek().url == req3.url\n+        assert q.pop().url == req3.url\n+        assert len(q) == 2\n+        assert q.peek().url == req2.url\n+        assert q.pop().url == req2.url\n+        assert len(q) == 1\n+        assert q.peek().url == req1.url\n+        assert q.pop().url == req1.url\n+        assert len(q) == 0\n+        assert q.peek() is None\n+        assert q.pop() is None\n         q.close()\n \n     def test_lifo_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n         q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 0\n+        assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n         req2 = Request(\"http://www.example.com/2\")\n         req3 = Request(\"http://www.example.com/3\")\n@@ -182,46 +182,46 @@ class LifoQueueMixin(RequestQueueTestMixin):\n             match=\"The underlying queue class does not implement 'peek'\",\n         ):\n             q.peek()\n-        self.assertEqual(len(q), 3)\n-        self.assertEqual(q.pop().url, req3.url)\n-        self.assertEqual(len(q), 2)\n-        self.assertEqual(q.pop().url, req2.url)\n-        self.assertEqual(len(q), 1)\n-        self.assertEqual(q.pop().url, req1.url)\n-        self.assertEqual(len(q), 0)\n-        self.assertIsNone(q.pop())\n+        assert len(q) == 3\n+        assert q.pop().url == req3.url\n+        assert len(q) == 2\n+        assert q.pop().url == req2.url\n+        assert len(q) == 1\n+        assert q.pop().url == req1.url\n+        assert len(q) == 0\n+        assert q.pop() is None\n         q.close()\n \n \n-class PickleFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+class TestPickleFifoDiskQueueRequest(FifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return PickleFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/fifo\")\n \n \n-class PickleLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+class TestPickleLifoDiskQueueRequest(LifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return PickleLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/lifo\")\n \n \n-class MarshalFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+class TestMarshalFifoDiskQueueRequest(FifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return MarshalFifoDiskQueue.from_crawler(\n             crawler=self.crawler, key=\"marshal/fifo\"\n         )\n \n \n-class MarshalLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+class TestMarshalLifoDiskQueueRequest(LifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return MarshalLifoDiskQueue.from_crawler(\n             crawler=self.crawler, key=\"marshal/lifo\"\n         )\n \n \n-class FifoMemoryQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+class TestFifoMemoryQueueRequest(FifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return FifoMemoryQueue.from_crawler(crawler=self.crawler)\n \n \n-class LifoMemoryQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+class TestLifoMemoryQueueRequest(LifoQueueMixin, TestBaseQueue):\n     def queue(self):\n         return LifoMemoryQueue.from_crawler(crawler=self.crawler)\n\n@@ -1,4 +1,3 @@\n-import unittest\n from datetime import datetime\n from unittest import mock\n \n@@ -8,8 +7,8 @@ from scrapy.statscollectors import DummyStatsCollector, StatsCollector\n from scrapy.utils.test import get_crawler\n \n \n-class CoreStatsExtensionTest(unittest.TestCase):\n-    def setUp(self):\n+class TestCoreStatsExtension:\n+    def setup_method(self):\n         self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider(\"foo\")\n \n@@ -24,9 +23,7 @@ class CoreStatsExtensionTest(unittest.TestCase):\n         ext.response_received(self.spider)\n         ext.item_dropped({}, self.spider, ZeroDivisionError())\n         ext.spider_closed(self.spider, \"finished\")\n-        self.assertEqual(\n-            ext.stats._stats,\n-            {\n+        assert ext.stats._stats == {\n             \"start_time\": fixed_datetime,\n             \"finish_time\": fixed_datetime,\n             \"item_scraped_count\": 1,\n@@ -35,8 +32,7 @@ class CoreStatsExtensionTest(unittest.TestCase):\n             \"item_dropped_reasons_count/ZeroDivisionError\": 1,\n             \"finish_reason\": \"finished\",\n             \"elapsed_time_seconds\": 0.0,\n-            },\n-        )\n+        }\n \n     def test_core_stats_dummy_stats_collector(self):\n         self.crawler.stats = DummyStatsCollector(self.crawler)\n@@ -46,51 +42,51 @@ class CoreStatsExtensionTest(unittest.TestCase):\n         ext.response_received(self.spider)\n         ext.item_dropped({}, self.spider, ZeroDivisionError())\n         ext.spider_closed(self.spider, \"finished\")\n-        self.assertEqual(ext.stats._stats, {})\n+        assert ext.stats._stats == {}\n \n \n-class StatsCollectorTest(unittest.TestCase):\n-    def setUp(self):\n+class TestStatsCollector:\n+    def setup_method(self):\n         self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider(\"foo\")\n \n     def test_collector(self):\n         stats = StatsCollector(self.crawler)\n-        self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_value(\"anything\"), None)\n-        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n+        assert stats.get_stats() == {}\n+        assert stats.get_value(\"anything\") is None\n+        assert stats.get_value(\"anything\", \"default\") == \"default\"\n         stats.set_value(\"test\", \"value\")\n-        self.assertEqual(stats.get_stats(), {\"test\": \"value\"})\n+        assert stats.get_stats() == {\"test\": \"value\"}\n         stats.set_value(\"test2\", 23)\n-        self.assertEqual(stats.get_stats(), {\"test\": \"value\", \"test2\": 23})\n-        self.assertEqual(stats.get_value(\"test2\"), 23)\n+        assert stats.get_stats() == {\"test\": \"value\", \"test2\": 23}\n+        assert stats.get_value(\"test2\") == 23\n         stats.inc_value(\"test2\")\n-        self.assertEqual(stats.get_value(\"test2\"), 24)\n+        assert stats.get_value(\"test2\") == 24\n         stats.inc_value(\"test2\", 6)\n-        self.assertEqual(stats.get_value(\"test2\"), 30)\n+        assert stats.get_value(\"test2\") == 30\n         stats.max_value(\"test2\", 6)\n-        self.assertEqual(stats.get_value(\"test2\"), 30)\n+        assert stats.get_value(\"test2\") == 30\n         stats.max_value(\"test2\", 40)\n-        self.assertEqual(stats.get_value(\"test2\"), 40)\n+        assert stats.get_value(\"test2\") == 40\n         stats.max_value(\"test3\", 1)\n-        self.assertEqual(stats.get_value(\"test3\"), 1)\n+        assert stats.get_value(\"test3\") == 1\n         stats.min_value(\"test2\", 60)\n-        self.assertEqual(stats.get_value(\"test2\"), 40)\n+        assert stats.get_value(\"test2\") == 40\n         stats.min_value(\"test2\", 35)\n-        self.assertEqual(stats.get_value(\"test2\"), 35)\n+        assert stats.get_value(\"test2\") == 35\n         stats.min_value(\"test4\", 7)\n-        self.assertEqual(stats.get_value(\"test4\"), 7)\n+        assert stats.get_value(\"test4\") == 7\n \n     def test_dummy_collector(self):\n         stats = DummyStatsCollector(self.crawler)\n-        self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_value(\"anything\"), None)\n-        self.assertEqual(stats.get_value(\"anything\", \"default\"), \"default\")\n+        assert stats.get_stats() == {}\n+        assert stats.get_value(\"anything\") is None\n+        assert stats.get_value(\"anything\", \"default\") == \"default\"\n         stats.set_value(\"test\", \"value\")\n         stats.inc_value(\"v1\")\n         stats.max_value(\"v2\", 100)\n         stats.min_value(\"v3\", 100)\n         stats.open_spider(\"a\")\n         stats.set_value(\"test\", \"value\", spider=self.spider)\n-        self.assertEqual(stats.get_stats(), {})\n-        self.assertEqual(stats.get_stats(\"a\"), {})\n+        assert stats.get_stats() == {}\n+        assert stats.get_stats(\"a\") == {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
