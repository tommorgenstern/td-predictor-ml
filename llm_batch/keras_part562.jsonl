{"custom_id": "keras#dcbc2930e63873f29f6f0fa6fa55af56453848df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 50 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,8 @@\n import collections\n import re\n \n-from keras_core.backend.global_state import get_global_attribute\n+from keras_core.api_export import keras_core_export\n+from keras_core.backend import global_state\n \n \n def auto_name(prefix):\n@@ -10,7 +11,7 @@ def auto_name(prefix):\n \n \n def uniquify(name):\n-    object_name_uids = get_global_attribute(\n+    object_name_uids = global_state.get_global_attribute(\n         \"object_name_uids\",\n         default=collections.defaultdict(int),\n         set_to_default=True,\n@@ -30,9 +31,36 @@ def to_snake_case(name):\n     return name\n \n \n+@keras_core_export(\"keras_core.backend.get_uid\")\n+def get_uid(prefix=\"\"):\n+    \"\"\"Associates a string prefix with an integer counter.\n+\n+    Args:\n+        prefix: String prefix to index.\n+\n+    Returns:\n+        Unique integer ID.\n+\n+    Example:\n+\n+    >>> get_uid('dense')\n+    1\n+    >>> get_uid('dense')\n+    2\n+    \"\"\"\n+    object_name_uids = global_state.get_global_attribute(\n+        \"object_name_uids\",\n+        default=collections.defaultdict(int),\n+        set_to_default=True,\n+    )\n+    object_name_uids[prefix] += 1\n+    return object_name_uids[prefix]\n+\n+\n def reset_uids():\n-    global OBJECT_NAME_UIDS\n-    OBJECT_NAME_UIDS = collections.defaultdict(int)\n+    global_state.set_global_attribute(\n+        \"object_name_uids\", collections.defaultdict(int)\n+    )\n \n \n def get_object_name(obj):\n\n@@ -0,0 +1,14 @@\n+from keras_core.testing import test_case\n+from keras_core.utils import naming\n+\n+\n+class NamingUtilsTest(test_case.TestCase):\n+    def test_auto_name(self):\n+        self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name\")\n+        self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name_1\")\n+        self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name_2\")\n+\n+    def test_get_uid(self):\n+        self.assertEqual(naming.get_uid(\"very_unique_name\"), 1)\n+        self.assertEqual(naming.get_uid(\"very_unique_name\"), 2)\n+        self.assertEqual(naming.get_uid(\"very_unique_name\"), 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#018d9fe63346937fb46ae548e28dd4f1d2ecf9f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 106 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,92 @@\n+import threading\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+\n+GLOBAL_STATE_TRACKER = threading.local()\n+GLOBAL_SETTINGS_TRACKER = threading.local()\n+\n+\n+def set_global_attribute(name, value):\n+    setattr(GLOBAL_STATE_TRACKER, name, value)\n+\n+\n+def get_global_attribute(name, default=None, set_to_default=False):\n+    attr = getattr(GLOBAL_STATE_TRACKER, name, None)\n+    if attr is None and default is not None:\n+        attr = default\n+        if set_to_default:\n+            set_global_attribute(name, attr)\n+    return attr\n+\n+\n+def set_global_setting(name, value):\n+    setattr(GLOBAL_SETTINGS_TRACKER, name, value)\n+\n+\n+def get_global_setting(name, default=None, set_to_default=False):\n+    attr = getattr(GLOBAL_SETTINGS_TRACKER, name, None)\n+    if attr is None and default is not None:\n+        attr = default\n+        if set_to_default:\n+            set_global_setting(name, attr)\n+    return attr\n+\n+\n+@keras_core_export(\"keras_core.backend.clear_session\")\n+def clear_session():\n+    \"\"\"Resets all state generated by Keras.\n+\n+    Keras manages a global state, which it uses to implement the Functional\n+    model-building API and to uniquify autogenerated layer names.\n+\n+    If you are creating many models in a loop, this global state will consume\n+    an increasing amount of memory over time, and you may want to clear it.\n+    Calling `clear_session()` releases the global state: this helps avoid\n+    clutter from old models and layers, especially when memory is limited.\n+\n+    Example 1: calling `clear_session()` when creating models in a loop\n+\n+    ```python\n+    for _ in range(100):\n+      # Without `clear_session()`, each iteration of this loop will\n+      # slightly increase the size of the global state managed by Keras\n+      model = keras_core.Sequential([\n+          keras_core.layers.Dense(10) for _ in range(10)])\n+\n+    for _ in range(100):\n+      # With `clear_session()` called at the beginning,\n+      # Keras starts with a blank state at each iteration\n+      # and memory consumption is constant over time.\n+      keras_core.backend.clear_session()\n+      model = keras_core.Sequential([\n+          keras_core.layers.Dense(10) for _ in range(10)])\n+    ```\n+\n+    Example 2: resetting the layer name generation counter\n+\n+    >>> layers = [keras_core.layers.Dense(10) for _ in range(10)]\n+    >>> new_layer = keras_core.layers.Dense(10)\n+    >>> print(new_layer.name)\n+    dense_10\n+    >>> keras_core.backend.clear_session()\n+    >>> new_layer = keras_core.layers.Dense(10)\n+    >>> print(new_layer.name)\n+    dense\n+    \"\"\"\n+    global GLOBAL_STATE_TRACKER\n+    global GLOBAL_SETTINGS_TRACKER\n+\n+    GLOBAL_STATE_TRACKER = threading.local()\n+    GLOBAL_SETTINGS_TRACKER = threading.local()\n+\n+    if backend.backend() == \"tensorflow\":\n+        import tensorflow as tf\n+\n+        tf.compat.v1.reset_default_graph()\n+        if tf.executing_eagerly():\n+            # Clear pending nodes in eager executors, kernel caches and\n+            # step_containers.\n+            from tensorflow.python.eager import context\n+\n+            context.context().clear_kernel_cache()\n\n@@ -0,0 +1,14 @@\n+from keras_core.backend import global_state\n+from keras_core.testing import test_case\n+from keras_core.utils.naming import auto_name\n+\n+\n+class GlobalStateTest(test_case.TestCase):\n+    def test_clear_session(self):\n+        name0 = auto_name(\"somename\")\n+        self.assertEqual(name0, \"somename\")\n+        name1 = auto_name(\"somename\")\n+        self.assertEqual(name1, \"somename_1\")\n+        global_state.clear_session()\n+        name0 = auto_name(\"somename\")\n+        self.assertEqual(name0, \"somename\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5033248616c6a48f84f286c082f9dc5fe57b4be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 423 | Lines Deleted: 5 | Files Changed: 12 | Hunks: 15 | Methods Changed: 32 | Complexity Δ (Sum/Max): 35/15 | Churn Δ: 428 | Churn Cumulative: 7218 | Contributors (this commit): 6 | Commits (past 90d): 72 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,9 +7,9 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     )\n \n \n-def top_k(x, k, sorted=False):\n-    if sorted:\n+def top_k(x, k, sorted=True):\n+    if not sorted:\n         return ValueError(\n-            \"Jax backend does not support `sorted=True` for `ops.top_k`\"\n+            \"Jax backend does not support `sorted=False` for `ops.top_k`\"\n         )\n     return jax.lax.top_k(x, k)\n\n@@ -339,6 +339,10 @@ def moveaxis(x, source, destination):\n     return jnp.moveaxis(x, source=source, destination=destination)\n \n \n+def nan_to_num(x):\n+    return jnp.nan_to_num(x)\n+\n+\n def ndim(x):\n     return jnp.ndim(x)\n \n\n@@ -8,5 +8,5 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n         return tf.math.unsorted_segment_sum(data, segment_ids, num_segments)\n \n \n-def top_k(x, k, sorted=False):\n+def top_k(x, k, sorted=True):\n     return tf.math.top_k(x, k, sorted=sorted)\n\n@@ -340,6 +340,19 @@ def moveaxis(x, source, destination):\n     return tfnp.moveaxis(x, source=source, destination=destination)\n \n \n+def nan_to_num(x):\n+    # Replace NaN with 0\n+    x = tf.where(tf.math.is_nan(x), 0, x)\n+\n+    # Replace positive infinitiy with dtype.max\n+    x = tf.where(tf.math.is_inf(x) & (x > 0), x.dtype.max, x)\n+\n+    # Replace negative infinity with dtype.min\n+    x = tf.where(tf.math.is_inf(x) & (x < 0), x.dtype.min, x)\n+\n+    return x\n+\n+\n def ndim(x):\n     return tfnp.ndim(x)\n \n\n@@ -1,6 +1,7 @@\n from keras_core.api_export import keras_core_export\n from keras_core.metrics.confusion_metrics import FalseNegatives\n from keras_core.metrics.confusion_metrics import FalsePositives\n+from keras_core.metrics.confusion_metrics import Precision\n from keras_core.metrics.confusion_metrics import TrueNegatives\n from keras_core.metrics.confusion_metrics import TruePositives\n from keras_core.metrics.hinge_metrics import CategoricalHinge\n\n@@ -1,8 +1,10 @@\n from keras_core import backend\n from keras_core import initializers\n+from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.metrics import metrics_utils\n from keras_core.metrics.metric import Metric\n+from keras_core.utils.python_utils import to_list\n \n \n class _ConfusionMatrixConditionCount(Metric):\n@@ -244,3 +246,160 @@ class TruePositives(_ConfusionMatrixConditionCount):\n             name=name,\n             dtype=dtype,\n         )\n+\n+\n+@keras_core_export(\"keras_core.metrics.Precision\")\n+class Precision(Metric):\n+    \"\"\"Computes the precision of the predictions with respect to the labels.\n+\n+    The metric creates two local variables, `true_positives` and\n+    `false_positives` that are used to compute the precision. This value is\n+    ultimately returned as `precision`, an idempotent operation that simply\n+    divides `true_positives` by the sum of `true_positives` and\n+    `false_positives`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `top_k` is set, we'll calculate precision as how often on average a class\n+    among the top-k classes with the highest predicted values of a batch entry\n+    is correct and can be found in the label for that entry.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold and/or in\n+    the top-k highest predictions, and computing the fraction of them for which\n+    `class_id` is indeed a correct label.\n+\n+    Args:\n+        thresholds: (Optional) A float value, or a Python list/tuple of float\n+            threshold values in [0, 1]. A threshold is compared with prediction\n+            values to determine the truth value of predictions (i.e., above the\n+            threshold is `true`, below is `false`). If used with a loss function\n+            that sets `from_logits=True` (i.e. no sigmoid applied to\n+            predictions), `thresholds` should be set to 0. One metric value is\n+            generated for each threshold value. If neither thresholds nor top_k\n+            are set, the default is to calculate precision with\n+            `thresholds=0.5`.\n+        top_k: (Optional) Unset by default. An int value specifying the top-k\n+            predictions to consider when calculating precision.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.Precision()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n+    >>> m.result()\n+    0.6666667\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result()\n+    1.0\n+\n+    >>> # With top_k=2, it will calculate precision over y_true[:2]\n+    >>> # and y_pred[:2]\n+    >>> m = keras_core.metrics.Precision(top_k=2)\n+    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n+    >>> m.result()\n+    0.0\n+\n+    >>> # With top_k=4, it will calculate precision over y_true[:4]\n+    >>> # and y_pred[:4]\n+    >>> m = keras_core.metrics.Precision(top_k=4)\n+    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n+    >>> m.result()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[keras_core.metrics.Precision()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras_core.metrics.Precision(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.init_thresholds = thresholds\n+        self.top_k = top_k\n+        self.class_id = class_id\n+\n+        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=default_threshold\n+        )\n+        self._thresholds_distributed_evenly = (\n+            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n+        )\n+        self.true_positives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"true_positives\",\n+        )\n+        self.false_positives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"false_positives\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates true positive and false positive statistics.\n+\n+        Args:\n+            y_true: The ground truth values, with the same dimensions as\n+                `y_pred`. Will be cast to `bool`.\n+            y_pred: The predicted values. Each element must be in the range\n+                `[0, 1]`.\n+            sample_weight: Optional weighting of each example. Defaults to 1.\n+                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            top_k=self.top_k,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def result(self):\n+        result = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_positives + backend.epsilon(),\n+        )\n+        return result[0] if len(self.thresholds) == 1 else result\n+\n+    def reset_state(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        self.true_positives.assign(ops.zeros((num_thresholds,)))\n+        self.false_positives.assign(ops.zeros((num_thresholds,)))\n+\n+    def get_config(self):\n+        config = {\n+            \"thresholds\": self.init_thresholds,\n+            \"top_k\": self.top_k,\n+            \"class_id\": self.class_id,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -368,3 +368,169 @@ class TruePositiveTest(testing.TestCase):\n             r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n         ):\n             metrics.TruePositives(thresholds=[None])\n+\n+\n+class PrecisionTest(testing.TestCase):\n+    def test_config(self):\n+        p_obj = metrics.Precision(\n+            name=\"my_precision\", thresholds=[0.4, 0.9], top_k=15, class_id=12\n+        )\n+        self.assertEqual(p_obj.name, \"my_precision\")\n+        self.assertLen(p_obj.variables, 2)\n+        self.assertEqual(\n+            [v.name for v in p_obj.variables],\n+            [\"true_positives\", \"false_positives\"],\n+        )\n+        self.assertEqual(p_obj.thresholds, [0.4, 0.9])\n+        self.assertEqual(p_obj.top_k, 15)\n+        self.assertEqual(p_obj.class_id, 12)\n+\n+        # Check save and restore config\n+        p_obj2 = metrics.Precision.from_config(p_obj.get_config())\n+        self.assertEqual(p_obj2.name, \"my_precision\")\n+        self.assertLen(p_obj2.variables, 2)\n+        self.assertEqual(p_obj2.thresholds, [0.4, 0.9])\n+        self.assertEqual(p_obj2.top_k, 15)\n+        self.assertEqual(p_obj2.class_id, 12)\n+\n+    def test_unweighted(self):\n+        p_obj = metrics.Precision()\n+        y_pred = np.array([1, 0, 1, 0])\n+        y_true = np.array([0, 1, 1, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(0.5, result)\n+\n+    def test_unweighted_all_incorrect(self):\n+        p_obj = metrics.Precision(thresholds=[0.5])\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = np.array(inputs)\n+        y_true = np.array(1 - inputs)\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(0, result)\n+\n+    def test_weighted(self):\n+        p_obj = metrics.Precision()\n+        y_pred = np.array([[1, 0, 1, 0], [1, 0, 1, 0]])\n+        y_true = np.array([[0, 1, 1, 0], [1, 0, 0, 1]])\n+        result = p_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=np.array([[1, 2, 3, 4], [4, 3, 2, 1]]),\n+        )\n+        weighted_tp = 3.0 + 4.0\n+        weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)\n+        expected_precision = weighted_tp / weighted_positives\n+        self.assertAlmostEqual(expected_precision, result)\n+\n+    def test_div_by_zero(self):\n+        p_obj = metrics.Precision()\n+        y_pred = np.array([0, 0, 0, 0])\n+        y_true = np.array([0, 0, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertEqual(0, result)\n+\n+    def test_unweighted_with_threshold(self):\n+        p_obj = metrics.Precision(thresholds=[0.5, 0.7])\n+        y_pred = np.array([1, 0, 0.6, 0])\n+        y_true = np.array([0, 1, 1, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual([0.5, 0.0], result, 0)\n+\n+    def test_weighted_with_threshold(self):\n+        p_obj = metrics.Precision(thresholds=[0.5, 1.0])\n+        y_true = np.array([[0, 1], [1, 0]])\n+        y_pred = np.array([[1, 0], [0.6, 0]], dtype=\"float32\")\n+        weights = np.array([[4, 0], [3, 1]], dtype=\"float32\")\n+        result = p_obj(y_true, y_pred, sample_weight=weights)\n+        weighted_tp = 0 + 3.0\n+        weighted_positives = (0 + 3.0) + (4.0 + 0.0)\n+        expected_precision = weighted_tp / weighted_positives\n+        self.assertAlmostEqual([expected_precision, 0], result, 1e-3)\n+\n+    def test_multiple_updates(self):\n+        p_obj = metrics.Precision(thresholds=[0.5, 1.0])\n+        y_true = np.array([[0, 1], [1, 0]])\n+        y_pred = np.array([[1, 0], [0.6, 0]], dtype=\"float32\")\n+        weights = np.array([[4, 0], [3, 1]], dtype=\"float32\")\n+        for _ in range(2):\n+            p_obj.update_state(y_true, y_pred, sample_weight=weights)\n+\n+        weighted_tp = (0 + 3.0) + (0 + 3.0)\n+        weighted_positives = ((0 + 3.0) + (4.0 + 0.0)) + (\n+            (0 + 3.0) + (4.0 + 0.0)\n+        )\n+        expected_precision = weighted_tp / weighted_positives\n+        self.assertAlmostEqual([expected_precision, 0], p_obj.result(), 1e-3)\n+\n+    def test_unweighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred = np.array([0.2, 0.1, 0.5, 0, 0.2])\n+        y_true = np.array([0, 1, 1, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1.0 / 3, result)\n+\n+    def test_weighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred1 = np.array([[0.2, 0.1, 0.4, 0, 0.2]])\n+        y_true1 = np.array([[0, 1, 1, 0, 1]])\n+        p_obj(y_true1, y_pred1, sample_weight=np.array([[1, 4, 2, 3, 5]]))\n+\n+        y_pred2 = np.array([0.2, 0.6, 0.4, 0.2, 0.2])\n+        y_true2 = np.array([1, 0, 1, 1, 1])\n+        result = p_obj(y_true2, y_pred2, sample_weight=np.array(3))\n+\n+        tp = (2 + 5) + (3 + 3)\n+        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n+        expected_precision = tp / predicted_positives\n+        self.assertAlmostEqual(expected_precision, result)\n+\n+    def test_unweighted_class_id(self):\n+        p_obj = metrics.Precision(class_id=2)\n+\n+        y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n+        y_true = np.array([0, 1, 1, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(0, p_obj.false_positives)\n+\n+        y_pred = np.array([0.2, 0.1, 0, 0, 0.2])\n+        y_true = np.array([0, 1, 1, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(0, p_obj.false_positives)\n+\n+        y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n+        y_true = np.array([0, 1, 0, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(0.5, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(1, p_obj.false_positives)\n+\n+    def test_unweighted_top_k_and_class_id(self):\n+        p_obj = metrics.Precision(class_id=2, top_k=2)\n+\n+        y_pred = np.array([0.2, 0.6, 0.3, 0, 0.2])\n+        y_true = np.array([0, 1, 1, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(0, p_obj.false_positives)\n+\n+        y_pred = np.array([1, 1, 0.9, 1, 1])\n+        y_true = np.array([0, 1, 1, 0, 0])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(0, p_obj.false_positives)\n+\n+    def test_unweighted_top_k_and_threshold(self):\n+        p_obj = metrics.Precision(thresholds=0.7, top_k=2)\n+\n+        y_pred = np.array([0.2, 0.8, 0.6, 0, 0.2])\n+        y_true = np.array([0, 1, 1, 0, 1])\n+        result = p_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1, result)\n+        self.assertAlmostEqual(1, p_obj.true_positives)\n+        self.assertAlmostEqual(0, p_obj.false_positives)\n\n@@ -533,7 +533,7 @@ def _filter_top_k(x, k):\n     Returns:\n       tensor with same shape and dtype as x.\n     \"\"\"\n-    _, top_k_idx = ops.top_k(x, k, sorted=False)\n+    _, top_k_idx = ops.top_k(x, k)\n     top_k_mask = ops.sum(\n         ops.one_hot(top_k_idx, ops.shape(x)[-1], axis=-1), axis=-2\n     )\n\n@@ -9,5 +9,6 @@ from keras_core.backend import is_tensor\n from keras_core.backend import name_scope\n from keras_core.backend import random\n from keras_core.backend import shape\n+from keras_core.operations.math import *  # noqa: F403\n from keras_core.operations.nn import *  # noqa: F403\n from keras_core.operations.numpy import *  # noqa: F403\n\n@@ -0,0 +1,30 @@\n+\"\"\"\n+segment_sum\n+top_k\n+\"\"\"\n+\n+from keras_core import backend\n+from keras_core.backend import any_symbolic_tensors\n+from keras_core.operations.operation import Operation\n+\n+\n+class SegmentSum(Operation):\n+    def call(self, x, segment_ids, num_segments=None, sorted=False):\n+        return backend.math.segment_sum(x, segment_ids, num_segments, sorted)\n+\n+\n+def segment_sum(x, segment_ids, num_segments=None, sorted=False):\n+    if any_symbolic_tensors((x,)):\n+        return SegmentSum().symbolic_call(x, segment_ids, num_segments, sorted)\n+    return backend.math.segment_sum(x, segment_ids, num_segments, sorted)\n+\n+\n+class TopK(Operation):\n+    def call(self, x, k, sorted=True):\n+        return backend.math.top_k(x, k, sorted)\n+\n+\n+def top_k(x, k, sorted=True):\n+    if any_symbolic_tensors((x,)):\n+        return TopK().symbolic_call(x, k, sorted)\n+    return backend.math.top_k(x, k, sorted)\n\n@@ -0,0 +1,35 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.operations import math as kmath\n+\n+\n+@pytest.mark.skipif(\n+    not backend.DYNAMIC_SHAPES_OK,\n+    reason=\"Backend does not support dynamic shapes\",\n+)\n+class MathOpsDynamicShapeTest(testing.TestCase):\n+    def test_topk(self):\n+        x = KerasTensor([None, 2, 3])\n+        values, indices = kmath.top_k(x, k=1)\n+        self.assertEqual(values.shape, (None, 2, 1))\n+        self.assertEqual(indices.shape, (None, 2, 1))\n+\n+\n+class MathOpsStaticShapeTest(testing.TestCase):\n+    def test_topk(self):\n+        x = KerasTensor([1, 2, 3])\n+        values, indices = kmath.top_k(x, k=1)\n+        self.assertEqual(values.shape, (1, 2, 1))\n+        self.assertEqual(indices.shape, (1, 2, 1))\n+\n+\n+class MathOpsCorrectnessTest(testing.TestCase):\n+    def test_topk(self):\n+        x = np.array([0, 4, 2, 1, 3, -1], dtype=np.float32)\n+        values, indices = kmath.top_k(x, k=2)\n+        self.assertAllClose(values, [4, 3])\n+        self.assertAllClose(indices, [1, 4])\n\n@@ -2035,6 +2035,15 @@ def moveaxis(x, source, destination):\n     return backend.numpy.moveaxis(x, source=source, destination=destination)\n \n \n+class NanToNum(Operation):\n+    def call(self, x):\n+        return backend.numpy.nan_to_num(x)\n+\n+\n+def nan_to_num(x):\n+    return backend.numpy.nan_to_num(x)\n+\n+\n class Ndim(Operation):\n     def call(self, x):\n         return backend.numpy.ndim(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a702c33ea0aaa552eff34c3adb2a62140c48d079", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 425 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -95,7 +95,7 @@ class ReLU(ops.Operation):\n             x = backend.nn.relu(x)\n \n         if clip_max:\n-            x = backend.clip(x, 0.0, max_value)\n+            x = backend.numpy.clip(x, 0.0, max_value)\n \n         if negative_slope != 0.0:\n             x -= negative_slope * negative_part\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#27a2d06cf24a5470bdeb6d9ea609aaa8431a5f3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 901 | Lines Deleted: 0 | Files Changed: 11 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): 59/24 | Churn Δ: 901 | Churn Cumulative: 1006 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.6615853658536586\n\nDIFF:\n@@ -0,0 +1 @@\n+\"\"\"Small NumPy datasets for debugging/testing.\"\"\"\n\n@@ -0,0 +1,11 @@\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.datasets.boston_housing.load_data\")\n+def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n+    raise NotImplementedError(\n+        \"The Boston Housing dataset is no longer distributed with Keras. \"\n+        \"We recommend that you use instead the \"\n+        \"California Housing dataset, available via \"\n+        \"`keras_core.datasets.california_housing.load_data()`.\"\n+    )\n\n@@ -0,0 +1,86 @@\n+\"\"\"Boston housing price regression dataset.\"\"\"\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+\n+\n+@keras_core_export(\"keras_core.datasets.boston_housing.load_data\")\n+def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n+    \"\"\"Loads the California Housing dataset.\n+\n+    This dataset was obtained from the [StatLib repository](\n+    https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html).\n+\n+    It's a continuous regression dataset with 20,640 samples with\n+    8 features each.\n+\n+    The target variable is a scalar: the median house value\n+    for California districts, in dollars.\n+\n+    The 8 input features are the following:\n+\n+    - MedInc: median income in block group\n+    - HouseAge: median house age in block group\n+    - AveRooms: average number of rooms per household\n+    - AveBedrms: average number of bedrooms per household\n+    - Population: block group population\n+    - AveOccup: average number of household members\n+    - Latitude: block group latitude\n+    - Longitude: block group longitude\n+\n+    This dataset was derived from the 1990 U.S. census, using one row per census\n+    block group. A block group is the smallest geographical unit for which the U.S.\n+    Census Bureau publishes sample data (a block group typically has a population\n+    of 600 to 3,000 people).\n+\n+    A household is a group of people residing within a home. Since the average\n+    number of rooms and bedrooms in this dataset are provided per household, these\n+    columns may take surprisingly large values for block groups with few households\n+    and many empty houses, such as vacation resorts.\n+\n+    Args:\n+        path: path where to cache the dataset locally\n+            (relative to `~/.keras/datasets`).\n+        test_split: fraction of the data to reserve as test set.\n+        seed: Random seed for shuffling the data\n+            before computing the test split.\n+\n+    Returns:\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`, `x_test`**: numpy arrays with shape `(num_samples, 8)`\n+      containing either the training samples (for `x_train`),\n+      or test samples (for `y_train`).\n+\n+    **`y_train`, `y_test`**: numpy arrays of shape `(num_samples,)` containing the\n+      target scalars. The targets are float scalars typically between 25,000 and\n+      500,000 that represent the home prices in dollars.\n+    \"\"\"\n+    assert 0 <= test_split < 1\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        path,\n+        origin=origin_folder + \"california_housing.npz\",\n+        file_hash=(  # noqa: E501\n+            \"1a2e3a52e0398de6463aebe6f4a8da34fb21fbb6b934cf88c3425e766f2a1a6f\"\n+        ),\n+    )\n+    with np.load(path, allow_pickle=True) as f:\n+        x = f[\"x\"]\n+        y = f[\"y\"]\n+\n+    rng = np.random.RandomState(seed)\n+    indices = np.arange(len(x))\n+    rng.shuffle(indices)\n+    x = x[indices]\n+    y = y[indices]\n+\n+    x_train = np.array(x[: int(len(x) * (1 - test_split))])\n+    y_train = np.array(y[: int(len(x) * (1 - test_split))])\n+    x_test = np.array(x[int(len(x) * (1 - test_split)) :])\n+    y_test = np.array(y[int(len(x) * (1 - test_split)) :])\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -0,0 +1,28 @@\n+\"\"\"Utilities common to CIFAR10 and CIFAR100 datasets.\"\"\"\n+\n+import _pickle as cPickle\n+\n+\n+def load_batch(fpath, label_key=\"labels\"):\n+    \"\"\"Internal utility for parsing CIFAR data.\n+\n+    Args:\n+        fpath: path the file to parse.\n+        label_key: key for label data in the retrieve\n+            dictionary.\n+\n+    Returns:\n+        A tuple `(data, labels)`.\n+    \"\"\"\n+    with open(fpath, \"rb\") as f:\n+        d = cPickle.load(f, encoding=\"bytes\")\n+        # decode utf8\n+        d_decoded = {}\n+        for k, v in d.items():\n+            d_decoded[k.decode(\"utf8\")] = v\n+        d = d_decoded\n+    data = d[\"data\"]\n+    labels = d[label_key]\n+\n+    data = data.reshape(data.shape[0], 3, 32, 32)\n+    return data, labels\n\n@@ -0,0 +1,99 @@\n+\"\"\"CIFAR10 small images classification dataset.\"\"\"\n+\n+import os\n+\n+import numpy as np\n+from keras import backend\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.datasets.cifar import load_batch\n+from keras_core.utils.file_utils import get_file\n+\n+\n+@keras_core_export(\"keras_core.datasets.cifar10.load_data\")\n+def load_data():\n+    \"\"\"Loads the CIFAR10 dataset.\n+\n+    This is a dataset of 50,000 32x32 color training images and 10,000 test\n+    images, labeled over 10 categories. See more info at the\n+    [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n+\n+    The classes are:\n+\n+    | Label | Description |\n+    |:-----:|-------------|\n+    |   0   | airplane    |\n+    |   1   | automobile  |\n+    |   2   | bird        |\n+    |   3   | cat         |\n+    |   4   | deer        |\n+    |   5   | dog         |\n+    |   6   | frog        |\n+    |   7   | horse       |\n+    |   8   | ship        |\n+    |   9   | truck       |\n+\n+    Returns:\n+        Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(50000, 32, 32, 3)`, containing the training data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_train`**: `uint8` NumPy array of labels (integers in range 0-9)\n+      with shape `(50000, 1)` for the training data.\n+\n+    **`x_test`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(10000, 32, 32, 3)`, containing the test data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_test`**: `uint8` NumPy array of labels (integers in range 0-9)\n+      with shape `(10000, 1)` for the test data.\n+\n+    Example:\n+\n+    ```python\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+    assert x_train.shape == (50000, 32, 32, 3)\n+    assert x_test.shape == (10000, 32, 32, 3)\n+    assert y_train.shape == (50000, 1)\n+    assert y_test.shape == (10000, 1)\n+    ```\n+    \"\"\"\n+    dirname = \"cifar-10-batches-py\"\n+    origin = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n+    path = get_file(\n+        fname=dirname,\n+        origin=origin,\n+        untar=True,\n+        file_hash=(  # noqa: E501\n+            \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n+        ),\n+    )\n+\n+    num_train_samples = 50000\n+\n+    x_train = np.empty((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n+    y_train = np.empty((num_train_samples,), dtype=\"uint8\")\n+\n+    for i in range(1, 6):\n+        fpath = os.path.join(path, \"data_batch_\" + str(i))\n+        (\n+            x_train[(i - 1) * 10000 : i * 10000, :, :, :],\n+            y_train[(i - 1) * 10000 : i * 10000],\n+        ) = load_batch(fpath)\n+\n+    fpath = os.path.join(path, \"test_batch\")\n+    x_test, y_test = load_batch(fpath)\n+\n+    y_train = np.reshape(y_train, (len(y_train), 1))\n+    y_test = np.reshape(y_test, (len(y_test), 1))\n+\n+    if backend.image_data_format() == \"channels_last\":\n+        x_train = x_train.transpose(0, 2, 3, 1)\n+        x_test = x_test.transpose(0, 2, 3, 1)\n+\n+    x_test = x_test.astype(x_train.dtype)\n+    y_test = y_test.astype(y_train.dtype)\n+\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -0,0 +1,85 @@\n+\"\"\"CIFAR100 small images classification dataset.\"\"\"\n+\n+import os\n+\n+import numpy as np\n+from keras import backend\n+from keras.datasets.cifar import load_batch\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+\n+\n+@keras_core_export(\"keras_core.datasets.cifar100.load_data\")\n+def load_data(label_mode=\"fine\"):\n+    \"\"\"Loads the CIFAR100 dataset.\n+\n+    This is a dataset of 50,000 32x32 color training images and\n+    10,000 test images, labeled over 100 fine-grained classes that are\n+    grouped into 20 coarse-grained classes. See more info at the\n+    [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n+\n+    Args:\n+        label_mode: one of `\"fine\"`, `\"coarse\"`.\n+            If it is `\"fine\"`, the category labels\n+            are the fine-grained labels, and if it is `\"coarse\"`,\n+            the output labels are the coarse-grained superclasses.\n+\n+    Returns:\n+        Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(50000, 32, 32, 3)`, containing the training data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_train`**: `uint8` NumPy array of labels (integers in range 0-99)\n+      with shape `(50000, 1)` for the training data.\n+\n+    **`x_test`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(10000, 32, 32, 3)`, containing the test data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_test`**: `uint8` NumPy array of labels (integers in range 0-99)\n+      with shape `(10000, 1)` for the test data.\n+\n+    Example:\n+\n+    ```python\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+    assert x_train.shape == (50000, 32, 32, 3)\n+    assert x_test.shape == (10000, 32, 32, 3)\n+    assert y_train.shape == (50000, 1)\n+    assert y_test.shape == (10000, 1)\n+    ```\n+    \"\"\"\n+    if label_mode not in [\"fine\", \"coarse\"]:\n+        raise ValueError(\n+            '`label_mode` must be one of `\"fine\"`, `\"coarse\"`. '\n+            f\"Received: label_mode={label_mode}.\"\n+        )\n+\n+    dirname = \"cifar-100-python\"\n+    origin = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n+    path = get_file(\n+        fname=dirname,\n+        origin=origin,\n+        untar=True,\n+        file_hash=(  # noqa: E501\n+            \"85cd44d02ba6437773c5bbd22e183051d648de2e7d6b014e1ef29b855ba677a7\"\n+        ),\n+    )\n+\n+    fpath = os.path.join(path, \"train\")\n+    x_train, y_train = load_batch(fpath, label_key=label_mode + \"_labels\")\n+\n+    fpath = os.path.join(path, \"test\")\n+    x_test, y_test = load_batch(fpath, label_key=label_mode + \"_labels\")\n+\n+    y_train = np.reshape(y_train, (len(y_train), 1))\n+    y_test = np.reshape(y_test, (len(y_test), 1))\n+\n+    if backend.image_data_format() == \"channels_last\":\n+        x_train = x_train.transpose(0, 2, 3, 1)\n+        x_test = x_test.transpose(0, 2, 3, 1)\n+\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -0,0 +1,96 @@\n+\"\"\"Fashion-MNIST dataset.\"\"\"\n+\n+import gzip\n+import os\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+\n+\n+@keras_core_export(\"keras_core.datasets.mnist.load_data\")\n+def load_data():\n+    \"\"\"Loads the Fashion-MNIST dataset.\n+\n+    This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n+    along with a test set of 10,000 images. This dataset can be used as\n+    a drop-in replacement for MNIST.\n+\n+    The classes are:\n+\n+    | Label | Description |\n+    |:-----:|-------------|\n+    |   0   | T-shirt/top |\n+    |   1   | Trouser     |\n+    |   2   | Pullover    |\n+    |   3   | Dress       |\n+    |   4   | Coat        |\n+    |   5   | Sandal      |\n+    |   6   | Shirt       |\n+    |   7   | Sneaker     |\n+    |   8   | Bag         |\n+    |   9   | Ankle boot  |\n+\n+    Returns:\n+\n+    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(60000, 28, 28)`, containing the training data.\n+\n+    **`y_train`**: `uint8` NumPy array of labels (integers in range 0-9)\n+      with shape `(60000,)` for the training data.\n+\n+    **`x_test`**: `uint8` NumPy array of grayscale image data with shapes\n+      (10000, 28, 28), containing the test data.\n+\n+    **`y_test`**: `uint8` NumPy array of labels (integers in range 0-9)\n+      with shape `(10000,)` for the test data.\n+\n+    Example:\n+\n+    ```python\n+    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n+    assert x_train.shape == (60000, 28, 28)\n+    assert x_test.shape == (10000, 28, 28)\n+    assert y_train.shape == (60000,)\n+    assert y_test.shape == (10000,)\n+    ```\n+\n+    License:\n+\n+    The copyright for Fashion-MNIST is held by Zalando SE.\n+    Fashion-MNIST is licensed under the [MIT license](\n+        https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n+    \"\"\"\n+    dirname = os.path.join(\"datasets\", \"fashion-mnist\")\n+    base = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    files = [\n+        \"train-labels-idx1-ubyte.gz\",\n+        \"train-images-idx3-ubyte.gz\",\n+        \"t10k-labels-idx1-ubyte.gz\",\n+        \"t10k-images-idx3-ubyte.gz\",\n+    ]\n+\n+    paths = []\n+    for fname in files:\n+        paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))\n+\n+    with gzip.open(paths[0], \"rb\") as lbpath:\n+        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n+\n+    with gzip.open(paths[1], \"rb\") as imgpath:\n+        x_train = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n+            len(y_train), 28, 28\n+        )\n+\n+    with gzip.open(paths[2], \"rb\") as lbpath:\n+        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n+\n+    with gzip.open(paths[3], \"rb\") as imgpath:\n+        x_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n+            len(y_test), 28, 28\n+        )\n+\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -0,0 +1,185 @@\n+\"\"\"IMDB sentiment classification dataset.\"\"\"\n+\n+import json\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+from keras_core.utils.python_utils import remove_long_seq\n+\n+\n+@keras_core_export(\"keras_core.datasets.imdb.load_data\")\n+def load_data(\n+    path=\"imdb.npz\",\n+    num_words=None,\n+    skip_top=0,\n+    maxlen=None,\n+    seed=113,\n+    start_char=1,\n+    oov_char=2,\n+    index_from=3,\n+    **kwargs,\n+):\n+    \"\"\"Loads the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n+\n+    This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment\n+    (positive/negative). Reviews have been preprocessed, and each review is\n+    encoded as a list of word indexes (integers).\n+    For convenience, words are indexed by overall frequency in the dataset,\n+    so that for instance the integer \"3\" encodes the 3rd most frequent word in\n+    the data. This allows for quick filtering operations such as:\n+    \"only consider the top 10,000 most\n+    common words, but eliminate the top 20 most common words\".\n+\n+    As a convention, \"0\" does not stand for a specific word, but instead is used\n+    to encode the pad token.\n+\n+    Args:\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+        num_words: integer or None. Words are\n+            ranked by how often they occur (in the training set) and only\n+            the `num_words` most frequent words are kept. Any less frequent word\n+            will appear as `oov_char` value in the sequence data. If None,\n+            all words are kept. Defaults to `None`.\n+        skip_top: skip the top N most frequently occurring words\n+            (which may not be informative). These words will appear as\n+            `oov_char` value in the dataset. When 0, no words are\n+            skipped. Defaults to `0`.\n+        maxlen: int or None. Maximum sequence length.\n+            Any longer sequence will be truncated. None, means no truncation.\n+            Defaults to `None`.\n+        seed: int. Seed for reproducible data shuffling.\n+        start_char: int. The start of a sequence will be marked with this\n+            character. 0 is usually the padding character. Defaults to `1`.\n+        oov_char: int. The out-of-vocabulary character.\n+            Words that were cut out because of the `num_words` or\n+            `skip_top` limits will be replaced with this character.\n+        index_from: int. Index actual words with this index and higher.\n+\n+    Returns:\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`, `x_test`**: lists of sequences, which are lists of indexes\n+      (integers). If the num_words argument was specific, the maximum\n+      possible index value is `num_words - 1`. If the `maxlen` argument was\n+      specified, the largest possible sequence length is `maxlen`.\n+\n+    **`y_train`, `y_test`**: lists of integer labels (1 or 0).\n+\n+    **Note**: The 'out of vocabulary' character is only used for\n+    words that were present in the training set but are not included\n+    because they're not making the `num_words` cut here.\n+    Words that were not seen in the training set but are in the test set\n+    have simply been skipped.\n+    \"\"\"\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        fname=path,\n+        origin=origin_folder + \"imdb.npz\",\n+        file_hash=(  # noqa: E501\n+            \"69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\"\n+        ),\n+    )\n+    with np.load(path, allow_pickle=True) as f:\n+        x_train, labels_train = f[\"x_train\"], f[\"y_train\"]\n+        x_test, labels_test = f[\"x_test\"], f[\"y_test\"]\n+\n+    rng = np.random.RandomState(seed)\n+    indices = np.arange(len(x_train))\n+    rng.shuffle(indices)\n+    x_train = x_train[indices]\n+    labels_train = labels_train[indices]\n+\n+    indices = np.arange(len(x_test))\n+    rng.shuffle(indices)\n+    x_test = x_test[indices]\n+    labels_test = labels_test[indices]\n+\n+    if start_char is not None:\n+        x_train = [[start_char] + [w + index_from for w in x] for x in x_train]\n+        x_test = [[start_char] + [w + index_from for w in x] for x in x_test]\n+    elif index_from:\n+        x_train = [[w + index_from for w in x] for x in x_train]\n+        x_test = [[w + index_from for w in x] for x in x_test]\n+\n+    if maxlen:\n+        x_train, labels_train = remove_long_seq(maxlen, x_train, labels_train)\n+        x_test, labels_test = remove_long_seq(maxlen, x_test, labels_test)\n+        if not x_train or not x_test:\n+            raise ValueError(\n+                \"After filtering for sequences shorter than maxlen=\"\n+                f\"{str(maxlen)}, no sequence was kept. Increase maxlen.\"\n+            )\n+\n+    xs = x_train + x_test\n+    labels = np.concatenate([labels_train, labels_test])\n+\n+    if not num_words:\n+        num_words = max(max(x) for x in xs)\n+\n+    # by convention, use 2 as OOV word\n+    # reserve 'index_from' (=3 by default) characters:\n+    # 0 (padding), 1 (start), 2 (OOV)\n+    if oov_char is not None:\n+        xs = [\n+            [w if (skip_top <= w < num_words) else oov_char for w in x]\n+            for x in xs\n+        ]\n+    else:\n+        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n+\n+    idx = len(x_train)\n+    x_train, y_train = np.array(xs[:idx], dtype=\"object\"), labels[:idx]\n+    x_test, y_test = np.array(xs[idx:], dtype=\"object\"), labels[idx:]\n+    return (x_train, y_train), (x_test, y_test)\n+\n+\n+@keras_core_export(\"keras_core.datasets.imdb.get_word_index\")\n+def get_word_index(path=\"imdb_word_index.json\"):\n+    \"\"\"Retrieves a dict mapping words to their index in the IMDB dataset.\n+\n+    Args:\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+\n+    Returns:\n+        The word index dictionary. Keys are word strings, values are their\n+        index.\n+\n+    Example:\n+\n+    ```python\n+    # Use the default parameters to keras.datasets.imdb.load_data\n+    start_char = 1\n+    oov_char = 2\n+    index_from = 3\n+    # Retrieve the training sequences.\n+    (x_train, _), _ = keras.datasets.imdb.load_data(\n+        start_char=start_char, oov_char=oov_char, index_from=index_from\n+    )\n+    # Retrieve the word index file mapping words to indices\n+    word_index = keras.datasets.imdb.get_word_index()\n+    # Reverse the word index to obtain a dict mapping indices to words\n+    # And add `index_from` to indices to sync with `x_train`\n+    inverted_word_index = dict(\n+        (i + index_from, word) for (word, i) in word_index.items()\n+    )\n+    # Update `inverted_word_index` to include `start_char` and `oov_char`\n+    inverted_word_index[start_char] = \"[START]\"\n+    inverted_word_index[oov_char] = \"[OOV]\"\n+    # Decode the first sequence in the dataset\n+    decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n+    ```\n+    \"\"\"\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        fname=path,\n+        origin=origin_folder + \"imdb_word_index.json\",\n+        file_hash=\"bfafd718b763782e994055a2d397834f\",\n+    )\n+    with open(path) as f:\n+        return json.load(f)\n\n@@ -0,0 +1,71 @@\n+\"\"\"MNIST handwritten digits dataset.\"\"\"\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+\n+\n+@keras_core_export(\"keras_core.datasets.mnist.load_data\")\n+def load_data(path=\"mnist.npz\"):\n+    \"\"\"Loads the MNIST dataset.\n+\n+    This is a dataset of 60,000 28x28 grayscale images of the 10 digits,\n+    along with a test set of 10,000 images.\n+    More info can be found at the\n+    [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n+\n+    Args:\n+        path: path where to cache the dataset locally\n+            (relative to `~/.keras/datasets`).\n+\n+    Returns:\n+        Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(60000, 28, 28)`, containing the training data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_train`**: `uint8` NumPy array of digit labels (integers in range 0-9)\n+      with shape `(60000,)` for the training data.\n+\n+    **`x_test`**: `uint8` NumPy array of grayscale image data with shapes\n+      `(10000, 28, 28)`, containing the test data. Pixel values range\n+      from 0 to 255.\n+\n+    **`y_test`**: `uint8` NumPy array of digit labels (integers in range 0-9)\n+      with shape `(10000,)` for the test data.\n+\n+    Example:\n+\n+    ```python\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+    assert x_train.shape == (60000, 28, 28)\n+    assert x_test.shape == (10000, 28, 28)\n+    assert y_train.shape == (60000,)\n+    assert y_test.shape == (10000,)\n+    ```\n+\n+    License:\n+\n+    Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n+    which is a derivative work from original NIST datasets.\n+    MNIST dataset is made available under the terms of the\n+    [Creative Commons Attribution-Share Alike 3.0 license.](\n+        https://creativecommons.org/licenses/by-sa/3.0/)\n+    \"\"\"\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        fname=path,\n+        origin=origin_folder + \"mnist.npz\",\n+        file_hash=(  # noqa: E501\n+            \"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"\n+        ),\n+    )\n+    with np.load(path, allow_pickle=True) as f:\n+        x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n+        x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n+\n+        return (x_train, y_train), (x_test, y_test)\n\n@@ -0,0 +1,220 @@\n+\"\"\"Reuters topic classification dataset.\"\"\"\n+\n+import json\n+\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.file_utils import get_file\n+from keras_core.utils.python_utils import remove_long_seq\n+\n+\n+@keras_core_export(\"keras_core.datasets.reuters.load_data\")\n+def load_data(\n+    path=\"reuters.npz\",\n+    num_words=None,\n+    skip_top=0,\n+    maxlen=None,\n+    test_split=0.2,\n+    seed=113,\n+    start_char=1,\n+    oov_char=2,\n+    index_from=3,\n+):\n+    \"\"\"Loads the Reuters newswire classification dataset.\n+\n+    This is a dataset of 11,228 newswires from Reuters, labeled over 46 topics.\n+\n+    This was originally generated by parsing and preprocessing the classic\n+    Reuters-21578 dataset, but the preprocessing code is no longer packaged\n+    with Keras. See this\n+    [GitHub discussion](https://github.com/keras-team/keras/issues/12072)\n+    for more info.\n+\n+    Each newswire is encoded as a list of word indexes (integers).\n+    For convenience, words are indexed by overall frequency in the dataset,\n+    so that for instance the integer \"3\" encodes the 3rd most frequent word in\n+    the data. This allows for quick filtering operations such as:\n+    \"only consider the top 10,000 most\n+    common words, but eliminate the top 20 most common words\".\n+\n+    As a convention, \"0\" does not stand for a specific word, but instead is used\n+    to encode any unknown word.\n+\n+    Args:\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+        num_words: integer or None. Words are\n+            ranked by how often they occur (in the training set) and only\n+            the `num_words` most frequent words are kept. Any less frequent word\n+            will appear as `oov_char` value in the sequence data. If None,\n+            all words are kept. Defaults to `None`.\n+        skip_top: skip the top N most frequently occurring words\n+            (which may not be informative). These words will appear as\n+            `oov_char` value in the dataset. 0 means no words are\n+            skipped. Defaults to `0`.\n+        maxlen: int or None. Maximum sequence length.\n+            Any longer sequence will be truncated. None means no truncation.\n+            Defaults to `None`.\n+        test_split: Float between `0.` and `1.`. Fraction of the dataset to be\n+            used as test data. `0.2` means that 20% of the dataset is used as\n+            test data. Defaults to `0.2`.\n+        seed: int. Seed for reproducible data shuffling.\n+        start_char: int. The start of a sequence will be marked with this\n+            character. 0 is usually the padding character. Defaults to `1`.\n+        oov_char: int. The out-of-vocabulary character.\n+            Words that were cut out because of the `num_words` or\n+            `skip_top` limits will be replaced with this character.\n+        index_from: int. Index actual words with this index and higher.\n+\n+    Returns:\n+        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **`x_train`, `x_test`**: lists of sequences, which are lists of indexes\n+      (integers). If the num_words argument was specific, the maximum\n+      possible index value is `num_words - 1`. If the `maxlen` argument was\n+      specified, the largest possible sequence length is `maxlen`.\n+\n+    **`y_train`, `y_test`**: lists of integer labels (1 or 0).\n+\n+    **Note**: The 'out of vocabulary' character is only used for\n+    words that were present in the training set but are not included\n+    because they're not making the `num_words` cut here.\n+    Words that were not seen in the training set but are in the test set\n+    have simply been skipped.\n+    \"\"\"\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        fname=path,\n+        origin=origin_folder + \"reuters.npz\",\n+        file_hash=(  # noqa: E501\n+            \"d6586e694ee56d7a4e65172e12b3e987c03096cb01eab99753921ef915959916\"\n+        ),\n+    )\n+    with np.load(path, allow_pickle=True) as f:\n+        xs, labels = f[\"x\"], f[\"y\"]\n+\n+    rng = np.random.RandomState(seed)\n+    indices = np.arange(len(xs))\n+    rng.shuffle(indices)\n+    xs = xs[indices]\n+    labels = labels[indices]\n+\n+    if start_char is not None:\n+        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n+    elif index_from:\n+        xs = [[w + index_from for w in x] for x in xs]\n+\n+    if maxlen:\n+        xs, labels = remove_long_seq(maxlen, xs, labels)\n+\n+    if not num_words:\n+        num_words = max(max(x) for x in xs)\n+\n+    # by convention, use 2 as OOV word\n+    # reserve 'index_from' (=3 by default) characters:\n+    # 0 (padding), 1 (start), 2 (OOV)\n+    if oov_char is not None:\n+        xs = [\n+            [w if skip_top <= w < num_words else oov_char for w in x]\n+            for x in xs\n+        ]\n+    else:\n+        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n+\n+    idx = int(len(xs) * (1 - test_split))\n+    x_train, y_train = np.array(xs[:idx], dtype=\"object\"), np.array(\n+        labels[:idx]\n+    )\n+    x_test, y_test = np.array(xs[idx:], dtype=\"object\"), np.array(labels[idx:])\n+\n+    return (x_train, y_train), (x_test, y_test)\n+\n+\n+@keras_core_export(\"keras_core.datasets.reuters.get_word_index\")\n+def get_word_index(path=\"reuters_word_index.json\"):\n+    \"\"\"Retrieves a dict mapping words to their index in the Reuters dataset.\n+\n+    Actual word indices starts from 3, with 3 indices reserved for:\n+    0 (padding), 1 (start), 2 (oov).\n+\n+    E.g. word index of 'the' is 1, but the in the actual training data, the\n+    index of 'the' will be 1 + 3 = 4. Vice versa, to translate word indices in\n+    training data back to words using this mapping, indices need to substract 3.\n+\n+    Args:\n+        path: where to cache the data (relative to `~/.keras/dataset`).\n+\n+    Returns:\n+        The word index dictionary. Keys are word strings, values are their\n+        index.\n+    \"\"\"\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n+    )\n+    path = get_file(\n+        path,\n+        origin=origin_folder + \"reuters_word_index.json\",\n+        file_hash=\"4d44cc38712099c9e383dc6e5f11a921\",\n+    )\n+    with open(path) as f:\n+        return json.load(f)\n+\n+\n+@keras_core_export(\"keras_core.datasets.reuters.get_label_names\")\n+def get_label_names():\n+    \"\"\"Returns labels as a list of strings with indices matching training data.\n+\n+    Reference:\n+\n+    - [Reuters Dataset](https://martin-thoma.com/nlp-reuters/)\n+    \"\"\"\n+    return (\n+        \"cocoa\",\n+        \"grain\",\n+        \"veg-oil\",\n+        \"earn\",\n+        \"acq\",\n+        \"wheat\",\n+        \"copper\",\n+        \"housing\",\n+        \"money-supply\",\n+        \"coffee\",\n+        \"sugar\",\n+        \"trade\",\n+        \"reserves\",\n+        \"ship\",\n+        \"cotton\",\n+        \"carcass\",\n+        \"crude\",\n+        \"nat-gas\",\n+        \"cpi\",\n+        \"money-fx\",\n+        \"interest\",\n+        \"gnp\",\n+        \"meal-feed\",\n+        \"alum\",\n+        \"oilseed\",\n+        \"gold\",\n+        \"tin\",\n+        \"strategic-metal\",\n+        \"livestock\",\n+        \"retail\",\n+        \"ipi\",\n+        \"iron-steel\",\n+        \"rubber\",\n+        \"heat\",\n+        \"jobs\",\n+        \"lei\",\n+        \"bop\",\n+        \"zinc\",\n+        \"orange\",\n+        \"pet-chem\",\n+        \"dlr\",\n+        \"gas\",\n+        \"silver\",\n+        \"wpi\",\n+        \"hog\",\n+        \"lead\",\n+    )\n\n@@ -103,3 +103,22 @@ def to_list(x):\n     if isinstance(x, list):\n         return x\n     return [x]\n+\n+\n+def remove_long_seq(maxlen, seq, label):\n+    \"\"\"Removes sequences that exceed the maximum length.\n+\n+    Args:\n+        maxlen: Int, maximum length of the output sequences.\n+        seq: List of lists, where each sublist is a sequence.\n+        label: List where each element is an integer.\n+\n+    Returns:\n+        new_seq, new_label: shortened lists for `seq` and `label`.\n+    \"\"\"\n+    new_seq, new_label = [], []\n+    for x, y in zip(seq, label):\n+        if len(x) < maxlen:\n+            new_seq.append(x)\n+            new_label.append(y)\n+    return new_seq, new_label\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
