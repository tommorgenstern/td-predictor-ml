{"custom_id": "keras#d6318c4245288036d3f381ca1dd38747097ee227", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1073 | Lines Deleted: 62 | Files Changed: 6 | Hunks: 34 | Methods Changed: 37 | Complexity Δ (Sum/Max): 106/36 | Churn Δ: 1135 | Churn Cumulative: 14295 | Contributors (this commit): 10 | Commits (past 90d): 82 | Contributors (cumulative): 32 | DMM Complexity: 0.895618556701031\n\nDIFF:\n@@ -4,6 +4,8 @@ import numpy as np\n from jax import lax\n from jax import nn as jnn\n \n+from keras_core.backend.config import epsilon\n+\n \n def relu(x):\n     return jnn.relu(x)\n@@ -356,3 +358,76 @@ def conv_transpose(\n \n def one_hot(x, num_classes, axis=-1):\n     return jnn.one_hot(x, num_classes, axis=axis)\n+\n+\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    target = jnp.array(target)\n+    output = jnp.array(output)\n+\n+    if target.shape != output.shape:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+    if len(target.shape) < 1:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must be at least rank 1. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+    if from_logits:\n+        log_prob = jax.nn.log_softmax(output, axis=axis)\n+    else:\n+        output = output / jnp.sum(output, axis, keepdims=True)\n+        output = jnp.clip(output, epsilon(), 1.0 - epsilon())\n+        log_prob = jnp.log(output)\n+    return -jnp.sum(target * log_prob, axis=axis)\n+\n+\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    target = jnp.array(target, dtype=\"int64\")\n+    output = jnp.array(output)\n+    if len(target.shape) == len(output.shape) and target.shape[-1] == 1:\n+        target = jnp.squeeze(target, axis=-1)\n+\n+    if len(output.shape) < 1:\n+        raise ValueError(\n+            \"Argument `output` must be at least rank 1. \"\n+            \"Received: \"\n+            f\"output.shape={output.shape}\"\n+        )\n+    if target.shape != output.shape[:-1]:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape \"\n+            \"up until the last dimension: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+    if from_logits:\n+        log_prob = jax.nn.log_softmax(output, axis=axis)\n+    else:\n+        output = output / jnp.sum(output, axis, keepdims=True)\n+        output = jnp.clip(output, epsilon(), 1.0 - epsilon())\n+        log_prob = jnp.log(output)\n+    target = jnn.one_hot(target, output.shape[axis], axis=axis)\n+    return -jnp.sum(target * log_prob, axis=axis)\n+\n+\n+def binary_crossentropy(target, output, from_logits=False):\n+    target = jnp.array(target)\n+    output = jnp.array(output)\n+\n+    if target.shape != output.shape:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+\n+    if from_logits:\n+        output = jnn.sigmoid(output)\n+\n+    output = jnp.clip(output, epsilon(), 1.0 - epsilon())\n+    bce = target * jnp.log(output)\n+    bce += (1.0 - target) * jnp.log(1.0 - output)\n+    return -bce\n\n@@ -1,8 +1,11 @@\n+import warnings\n+\n import tensorflow as tf\n \n from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n+from keras_core.backend.config import epsilon\n \n \n def relu(x):\n@@ -302,3 +305,203 @@ def conv_transpose(\n \n def one_hot(x, num_classes, axis=-1):\n     return tf.one_hot(x, num_classes, axis=axis)\n+\n+\n+def _get_logits(output, from_logits, op_type, fn_name):\n+    \"\"\"Retrieves logits tensor from maybe-softmax or maybe-sigmoid tensor.\"\"\"\n+    output_ = output\n+    from_logits_ = from_logits\n+\n+    has_keras_logits = hasattr(output, \"_keras_logits\")\n+    if has_keras_logits:\n+        output_ = output._keras_logits\n+        from_logits_ = True\n+\n+    from_expected_op_type = (\n+        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n+        and output.op.type == op_type\n+    ) and not has_keras_logits\n+\n+    if from_expected_op_type:\n+        # When softmax activation function is used for output operation, we\n+        # use logits from the softmax function directly to compute loss in order\n+        # to prevent collapsing zero when training.\n+        assert len(output.op.inputs) == 1\n+        output_ = output.op.inputs[0]\n+        from_logits_ = True\n+\n+    if from_logits and (has_keras_logits or from_expected_op_type):\n+        warnings.warn(\n+            f'\"`{fn_name}` received `from_logits=True`, but '\n+            f\"the `output` argument was produced by a {op_type} \"\n+            \"activation and thus does not represent logits. \"\n+            \"Was this intended?\",\n+            stacklevel=2,\n+        )\n+    return output_, from_logits_\n+\n+\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n+\n+    Args:\n+        target: A tensor of the same shape as `output`.\n+        output: A tensor resulting from a softmax\n+            (unless `from_logits` is `True`, in which\n+            case `output` is expected to be the logits).\n+        from_logits: Boolean, whether `output` is the\n+            result of a softmax, or is a tensor of logits.\n+        axis: Int specifying the channels axis. `axis=-1` corresponds to data\n+            format `channels_last`, and `axis=1` corresponds to data format\n+            `channels_first`.\n+\n+    Returns:\n+        Output tensor.\n+\n+    Example:\n+\n+    >>> a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3])\n+    >>> print(a)\n+    tf.Tensor(\n+      [[1. 0. 0.]\n+       [0. 1. 0.]\n+       [0. 0. 1.]], shape=(3, 3), dtype=float32)\n+    >>> b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94],\n+    ...                 shape=[3, 3])\n+    >>> print(b)\n+    tf.Tensor(\n+      [[0.9  0.05 0.05]\n+       [0.05 0.89 0.06]\n+       [0.05 0.01 0.94]], shape=(3, 3), dtype=float32)\n+    >>> loss = categorical_crossentropy(a, b)\n+    >>> print(np.around(loss, 5))\n+    [0.10536 0.11653 0.06188]\n+    >>> loss = categorical_crossentropy(a, a)\n+    >>> print(np.around(loss, 5))\n+    [0. 0. 0.]\n+    \"\"\"\n+    target = tf.convert_to_tensor(target)\n+    output = tf.convert_to_tensor(output)\n+\n+    if target.shape != output.shape:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+    if len(target.shape) < 1:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must be at least rank 1. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"categorical_crossentropy\"\n+    )\n+    if from_logits:\n+        return tf.nn.softmax_cross_entropy_with_logits(\n+            labels=target, logits=output, axis=axis\n+        )\n+\n+    # Adjust the predictions so that the probability of\n+    # each class for every sample adds up to 1\n+    # This is needed to ensure that the cross entropy is\n+    # computed correctly.\n+    output = output / tf.reduce_sum(output, axis, keepdims=True)\n+\n+    # Compute cross entropy from probabilities.\n+    output = tf.clip_by_value(output, epsilon(), 1.0 - epsilon())\n+    return -tf.reduce_sum(target * tf.math.log(output), axis)\n+\n+\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    \"\"\"Categorical crossentropy with integer targets.\n+\n+    Args:\n+        target: An integer tensor.\n+        output: A tensor resulting from a softmax\n+            (unless `from_logits` is True, in which\n+            case `output` is expected to be the logits).\n+        from_logits: Boolean, whether `output` is the\n+            result of a softmax, or is a tensor of logits.\n+        axis: Int specifying the channels axis. `axis=-1` corresponds to data\n+            format `channels_last`, and `axis=1` corresponds to data format\n+            `channels_first`.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n+    if axis != -1 and axis != len(output.shape) - 1:\n+        raise ValueError(\n+            f\"Only axis=-1 is currently supported. Received: axis={axis}\"\n+        )\n+\n+    target = tf.convert_to_tensor(target)\n+    target = tf.cast(target, dtype=\"int64\")\n+    output = tf.convert_to_tensor(output)\n+    if len(target.shape) == len(output.shape) and target.shape[-1] == 1:\n+        target = tf.squeeze(target, axis=-1)\n+\n+    if len(output.shape) < 1:\n+        raise ValueError(\n+            \"Argument `output` must be at least rank 1. \"\n+            \"Received: \"\n+            f\"output.shape={output.shape}\"\n+        )\n+    if target.shape != output.shape[:-1]:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape \"\n+            \"up until the last dimension: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"sparse_categorical_crossentropy\"\n+    )\n+    if not from_logits:\n+        output = tf.clip_by_value(output, epsilon(), 1 - epsilon())\n+        output = tf.math.log(output)\n+\n+    result = tf.nn.sparse_softmax_cross_entropy_with_logits(\n+        labels=target, logits=output\n+    )\n+    return result\n+\n+\n+def binary_crossentropy(target, output, from_logits=False):\n+    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n+\n+    Args:\n+        target: A tensor with the same shape as `output`.\n+        output: A tensor.\n+        from_logits: Whether `output` is expected to be a logits tensor.\n+            By default, we consider that `output`\n+            encodes a probability distribution.\n+\n+    Returns:\n+        A tensor.\n+    \"\"\"\n+    target = tf.convert_to_tensor(target)\n+    output = tf.convert_to_tensor(output)\n+\n+    if target.shape != output.shape:\n+        raise ValueError(\n+            \"Arguments `target` and `output` must have the same shape. \"\n+            \"Received: \"\n+            f\"target.shape={target.shape}, output.shape={output.shape}\"\n+        )\n+\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Sigmoid\", \"binary_crossentropy\"\n+    )\n+    if from_logits:\n+        return tf.nn.sigmoid_cross_entropy_with_logits(\n+            labels=target, logits=output\n+        )\n+\n+    # Compute cross entropy from probabilities.\n+    output = tf.clip_by_value(output, epsilon(), 1.0 - epsilon())\n+    bce = target * tf.math.log(output)\n+    bce += (1 - target) * tf.math.log(1 - output)\n+    return -bce\n\n@@ -413,6 +413,17 @@ class BinaryCrossentropy(LossFunctionWrapper):\n             axis=axis,\n         )\n         self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+        self.axis = axis\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+            \"axis\": self.axis,\n+        }\n \n \n @keras_core_export(\"keras_core.losses.BinaryFocalCrossentropy\")\n@@ -558,7 +569,6 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         reduction=\"sum_over_batch_size\",\n         name=\"binary_focal_crossentropy\",\n     ):\n-        \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n         super().__init__(\n             binary_focal_crossentropy,\n             apply_class_balancing=apply_class_balancing,\n@@ -571,18 +581,23 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n             axis=axis,\n         )\n         self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+        self.axis = axis\n         self.apply_class_balancing = apply_class_balancing\n         self.alpha = alpha\n         self.gamma = gamma\n \n     def get_config(self):\n-        config = {\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+            \"axis\": self.axis,\n             \"apply_class_balancing\": self.apply_class_balancing,\n             \"alpha\": self.alpha,\n             \"gamma\": self.gamma,\n         }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n \n \n @keras_core_export(\"keras_core.losses.CategoricalCrossentropy\")\n@@ -664,6 +679,18 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             label_smoothing=label_smoothing,\n             axis=axis,\n         )\n+        self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+        self.axis = axis\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+            \"axis\": self.axis,\n+        }\n \n \n @keras_core_export(\"keras_core.losses.CategoricalFocalCrossentropy\")\n@@ -789,16 +816,21 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             axis=axis,\n         )\n         self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+        self.axis = axis\n         self.alpha = alpha\n         self.gamma = gamma\n \n     def get_config(self):\n-        config = {\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+            \"axis\": self.axis,\n             \"alpha\": self.alpha,\n             \"gamma\": self.gamma,\n         }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n \n \n @keras_core_export(\"keras_core.losses.SparseCategoricalCrossentropy\")\n@@ -820,11 +852,6 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     Args:\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n-        ignore_class: Optional integer. The ID of a class to be ignored during\n-            loss computation. This is useful, for example, in segmentation\n-            problems featuring a \"void\" class (commonly -1 or 255) in\n-            segmentation maps.\n-            By default (`ignore_class=None`), all classes are considered.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n             Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n@@ -866,7 +893,6 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     def __init__(\n         self,\n         from_logits=False,\n-        ignore_class=None,\n         reduction=\"sum_over_batch_size\",\n         name=\"sparse_categorical_crossentropy\",\n     ):\n@@ -875,8 +901,15 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             name=name,\n             reduction=reduction,\n             from_logits=from_logits,\n-            ignore_class=ignore_class,\n         )\n+        self.from_logits = from_logits\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+            \"from_logits\": self.from_logits,\n+        }\n \n \n def convert_binary_labels_to_hinge(y_true):\n@@ -1419,14 +1452,27 @@ def categorical_focal_crossentropy(\n             label_smoothing / num_classes\n         )\n \n-    return ops.categorical_focal_crossentropy(\n-        target=y_true,\n-        output=y_pred,\n-        alpha=alpha,\n-        gamma=gamma,\n-        from_logits=from_logits,\n-        axis=axis,\n-    )\n+    if from_logits:\n+        y_pred = ops.softmax(y_pred, axis=axis)\n+\n+    # Adjust the predictions so that the probability of\n+    # each class for every sample adds up to 1\n+    # This is needed to ensure that the cross entropy is\n+    # computed correctly.\n+    output = y_pred / ops.sum(y_pred, axis=axis, keepdims=True)\n+    output = ops.clip(output, backend.epsilon(), 1.0 - backend.epsilon())\n+\n+    # Calculate cross entropy\n+    cce = -y_true * ops.log(output)\n+\n+    # Calculate factors\n+    modulating_factor = ops.power(1.0 - output, gamma)\n+    weighting_factor = ops.multiply(modulating_factor, alpha)\n+\n+    # Apply weighting factor\n+    focal_cce = ops.multiply(weighting_factor, cce)\n+    focal_cce = ops.sum(focal_cce, axis=axis)\n+    return focal_cce\n \n \n @keras_core_export(\n@@ -1435,9 +1481,7 @@ def categorical_focal_crossentropy(\n         \"keras_core.losses.sparse_categorical_crossentropy\",\n     ]\n )\n-def sparse_categorical_crossentropy(\n-    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n-):\n+def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     \"\"\"Computes the sparse categorical crossentropy loss.\n \n     Args:\n@@ -1447,11 +1491,6 @@ def sparse_categorical_crossentropy(\n             default, we assume that `y_pred` encodes a probability distribution.\n         axis: Defaults to -1. The dimension along which the entropy is\n             computed.\n-        ignore_class: Optional integer. The ID of a class to be ignored during\n-            loss computation. This is useful, for example, in segmentation\n-            problems featuring a \"void\" class (commonly -1 or 255)\n-            in segmentation maps. By default (`ignore_class=None`),\n-            all classes are considered.\n \n     Returns:\n         Sparse categorical crossentropy loss value.\n@@ -1464,27 +1503,11 @@ def sparse_categorical_crossentropy(\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([0.0513, 2.303], dtype=float32)\n-\n-    >>> y_true = [[[ 0,  2],\n-    ...            [-1, -1]],\n-    ...           [[ 0,  2],\n-    ...            [-1, -1]]]\n-    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n-    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n-    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n-    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n-    >>> loss = keras_core.losses.sparse_categorical_crossentropy(\n-    ...   y_true, y_pred, ignore_class=-1)\n-    array([[[2.3841855e-07, 2.3841855e-07],\n-            [0.0000000e+00, 0.0000000e+00]],\n-           [[2.3841855e-07, 6.9314730e-01],\n-            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n     \"\"\"\n     return ops.sparse_categorical_crossentropy(\n         y_true,\n         y_pred,\n         from_logits=from_logits,\n-        ignore_class=ignore_class,\n         axis=axis,\n     )\n \n@@ -1606,14 +1629,23 @@ def binary_focal_crossentropy(\n     if label_smoothing:\n         y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n \n-    return ops.mean(\n-        ops.binary_focal_crossentropy(\n+    if from_logits:\n+        y_pred = ops.sigmoid(y_pred)\n+\n+    bce = ops.binary_crossentropy(\n         target=y_true,\n         output=y_pred,\n-            apply_class_balancing=apply_class_balancing,\n-            alpha=alpha,\n-            gamma=gamma,\n-            from_logits=from_logits,\n-        ),\n-        axis=axis,\n+        from_logits=False,\n     )\n+\n+    # Calculate focal factor\n+    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n+    focal_factor = ops.power(1.0 - p_t, gamma)\n+\n+    focal_bce = focal_factor * bce\n+\n+    if apply_class_balancing:\n+        weight = y_true * alpha + (1 - y_true) * (1 - alpha)\n+        focal_bce = weight * focal_bce\n+\n+    return ops.mean(focal_bce, axis=axis)\n\n@@ -6,8 +6,7 @@ from keras_core.losses import losses\n \n class MeanSquaredErrorTest(testing.TestCase):\n     def test_config(self):\n-        # TODO\n-        pass\n+        self.run_class_serialization_test(losses.MeanSquaredError(name=\"mymse\"))\n \n     def test_all_correct_unweighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -69,8 +68,9 @@ class MeanSquaredErrorTest(testing.TestCase):\n \n class MeanAbsoluteErrorTest(testing.TestCase):\n     def test_config(self):\n-        # TODO\n-        pass\n+        self.run_class_serialization_test(\n+            losses.MeanAbsoluteError(name=\"mymae\")\n+        )\n \n     def test_all_correct_unweighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n@@ -132,8 +132,9 @@ class MeanAbsoluteErrorTest(testing.TestCase):\n \n class MeanAbsolutePercentageErrorTest(testing.TestCase):\n     def test_config(self):\n-        # TODO\n-        pass\n+        self.run_class_serialization_test(\n+            losses.MeanAbsolutePercentageError(name=\"mymape\")\n+        )\n \n     def test_all_correct_unweighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n@@ -184,8 +185,9 @@ class MeanAbsolutePercentageErrorTest(testing.TestCase):\n \n class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n     def test_config(self):\n-        # TODO\n-        pass\n+        self.run_class_serialization_test(\n+            losses.MeanSquaredLogarithmicError(name=\"mysloge\")\n+        )\n \n     def test_unweighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n@@ -629,3 +631,490 @@ class PoissonTest(testing.TestCase):\n         poisson_obj = losses.Poisson()\n         loss = poisson_obj(self.y_true, self.y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n+\n+\n+class BinaryCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.BinaryCrossentropy(name=\"bce\", axis=-1)\n+        )\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"float32\")\n+        bce_obj = losses.BinaryCrossentropy()\n+        loss = bce_obj(y_true, y_true)\n+        self.assertAlmostEqual(loss, 0.0)\n+\n+        # Test with logits.\n+        logits = np.array(\n+            [\n+                [10.0, -10.0, -10.0],\n+                [-10.0, 10.0, -10.0],\n+                [-10.0, -10.0, 10.0],\n+            ]\n+        )\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0)\n+\n+    def test_unweighted(self):\n+        y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.array([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        bce_obj = losses.BinaryCrossentropy()\n+        loss = bce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 3.98559)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        logits = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 3.3333)\n+\n+    def test_scalar_weighted(self):\n+        bce_obj = losses.BinaryCrossentropy()\n+        y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.array([1, 1, 1, 0], dtype=\"float32\").reshape([2, 2])\n+        loss = bce_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 9.1668)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        logits = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 7.666)\n+\n+    def test_sample_weighted(self):\n+        bce_obj = losses.BinaryCrossentropy()\n+        y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.array([1, 1, 1, 0], dtype=\"float32\").reshape([2, 2])\n+        sample_weight = np.array([1.2, 3.4]).reshape((2, 1))\n+        loss = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 4.7827)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        logits = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        weights = np.array([4, 3])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits, sample_weight=weights)\n+        self.assertAlmostEqual(loss, 10.0)\n+\n+    def test_no_reduction(self):\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        logits = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True, reduction=None)\n+        loss = bce_obj(y_true, logits)\n+        self.assertAllClose(loss, [0.0, 6.666], atol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        logits = np.array([[10.0, -10.0, -10.0]])\n+        y_true = np.array([[1, 0, 1]])\n+        label_smoothing = 0.1\n+        bce_obj = losses.BinaryCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        loss = bce_obj(y_true, logits)\n+        expected_value = (10.0 + 5.0 * label_smoothing) / 3.0\n+        self.assertAlmostEqual(loss, expected_value)\n+\n+    def test_shape_mismatch(self):\n+        y_true = np.array([[0], [1], [2]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]]\n+        )\n+        cce_obj = losses.BinaryCrossentropy()\n+        with self.assertRaisesRegex(ValueError, \"must have the same shape\"):\n+            cce_obj(y_true, y_pred)\n+\n+\n+class CategoricalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.CategoricalCrossentropy(name=\"cce\", axis=-1)\n+        )\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"int64\")\n+        y_pred = np.array(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=\"float32\",\n+        )\n+        cce_obj = losses.CategoricalCrossentropy()\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.0)\n+\n+        # Test with logits.\n+        logits = np.array(\n+            [[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]\n+        )\n+        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0)\n+\n+    def test_unweighted(self):\n+        cce_obj = losses.CategoricalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.3239)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0573)\n+\n+    def test_scalar_weighted(self):\n+        cce_obj = losses.CategoricalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.7449)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.1317)\n+\n+    def test_sample_weighted(self):\n+        cce_obj = losses.CategoricalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        sample_weight = np.array([[1.2], [3.4], [5.6]]).reshape((3, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 1.0696)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.31829)\n+\n+    def test_no_reduction(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalCrossentropy(\n+            from_logits=True, reduction=None\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose((0.001822, 0.000459, 0.169846), loss, 3)\n+\n+    def test_label_smoothing(self):\n+        logits = np.array([[100.0, -100.0, -100.0]])\n+        y_true = np.array([[1, 0, 0]])\n+        label_smoothing = 0.1\n+        cce_obj = losses.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        loss = cce_obj(y_true, logits)\n+        expected_value = 400.0 * label_smoothing / 3.0\n+        self.assertAlmostEqual(loss, expected_value)\n+\n+    def test_label_smoothing_ndarray(self):\n+        logits = np.asarray([[100.0, -100.0, -100.0]])\n+        y_true = np.asarray([[1, 0, 0]])\n+        label_smoothing = 0.1\n+        cce_obj = losses.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        loss = cce_obj(y_true, logits)\n+        expected_value = 400.0 * label_smoothing / 3.0\n+        self.assertAlmostEqual(loss, expected_value)\n+\n+    def test_shape_mismatch(self):\n+        y_true = np.array([[0], [1], [2]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]]\n+        )\n+\n+        cce_obj = losses.CategoricalCrossentropy()\n+        with self.assertRaisesRegex(ValueError, \"must have the same shape\"):\n+            cce_obj(y_true, y_pred)\n+\n+\n+class SparseCategoricalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.SparseCategoricalCrossentropy(name=\"scce\")\n+        )\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = np.array([[0], [1], [2]], dtype=\"int64\")\n+        y_pred = np.array(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=\"float32\",\n+        )\n+        cce_obj = losses.SparseCategoricalCrossentropy()\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+        # Test with logits.\n+        logits = np.array(\n+            [[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]\n+        )\n+        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+    def test_unweighted(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy()\n+        y_true = np.array([0, 1, 2])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.3239, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0573, 3)\n+\n+    def test_scalar_weighted(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy()\n+        y_true = np.array([[0], [1], [2]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.7449, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.1317, 3)\n+\n+    def test_sample_weighted(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy()\n+        y_true = np.array([[0], [1], [2]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        sample_weight = np.array([[1.2], [3.4], [5.6]]).reshape((3, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 1.0696, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.31829, 3)\n+\n+    def test_no_reduction(self):\n+        y_true = np.array([[0], [1], [2]])\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.SparseCategoricalCrossentropy(\n+            from_logits=True, reduction=None\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose((0.001822, 0.000459, 0.169846), loss, 3)\n+\n+\n+class BinaryFocalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.BinaryFocalCrossentropy(name=\"bfce\")\n+        )\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = np.array(\n+            [\n+                [1, 0, 0],\n+                [0, 1, 0],\n+                [0, 0, 1],\n+            ],\n+            dtype=\"float32\",\n+        )\n+        obj = losses.BinaryFocalCrossentropy(gamma=1.5)\n+        loss = obj(y_true, y_true)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+        # Test with logits.\n+        logits = np.array(\n+            [\n+                [100.0, -100.0, -100.0],\n+                [-100.0, 100.0, -100.0],\n+                [-100.0, -100.0, 100.0],\n+            ]\n+        )\n+        obj = losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True)\n+        loss = obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+    def test_unweighted(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape(\n+            [2, 2]\n+        )\n+        obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+        loss = obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.268, 3)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 1, 0], [0, 1, 0]], dtype=\"float32\")\n+        logits = np.array([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+        obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+        loss = obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.799, 3)\n+\n+    def test_scalar_weighted(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape(\n+            [2, 2]\n+        )\n+        obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+        loss = obj(y_true, y_pred, sample_weight=1.23)\n+        self.assertAlmostEqual(loss, 0.3296, 3)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 1, 0], [0, 1, 0]], dtype=\"float32\")\n+        logits = np.array([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+        obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+        loss = obj(y_true, logits, sample_weight=3.21)\n+        self.assertAlmostEqual(loss, 2.565, 3)\n+\n+    def test_sample_weighted(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape(\n+            [2, 2]\n+        )\n+        sample_weight = np.array([1.2, 3.4]).reshape((2, 1))\n+        obj = losses.BinaryFocalCrossentropy(gamma=2.0)\n+        loss = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.34415, 3)\n+\n+        # Test with logits.\n+        y_true = np.array([[1, 1, 0], [0, 1, 0]], dtype=\"float32\")\n+        logits = np.array([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+        obj = losses.BinaryFocalCrossentropy(gamma=3.0, from_logits=True)\n+        loss = obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.95977, 3)\n+\n+    def test_no_reduction(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape(\n+            [2, 2]\n+        )\n+        obj = losses.BinaryFocalCrossentropy(\n+            gamma=2.0,\n+            reduction=None,\n+        )\n+        loss = obj(y_true, y_pred)\n+        self.assertAllClose(loss, (0.5155, 0.0205), 3)\n+\n+\n+class CategoricalFocalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.CategoricalFocalCrossentropy(name=\"cfce\")\n+        )\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"int64\")\n+        y_pred = np.array(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=\"float32\",\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0)\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+        # Test with logits.\n+        logits = np.array(\n+            [[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+    def test_unweighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(loss, 0.02059, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(loss, 0.000345, 3)\n+\n+    def test_scalar_weighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.047368, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=2.3)\n+        self.assertAlmostEqual(loss, 0.000794, 4)\n+\n+    def test_sample_weighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=\"float32\",\n+        )\n+        sample_weight = np.array([[1.2], [3.4], [5.6]]).reshape((3, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.06987, 3)\n+\n+        # Test with logits.\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 0.001933, 3)\n+\n+    def test_no_reduction(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        logits = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, reduction=None\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose(\n+            (1.5096224e-09, 2.4136547e-11, 1.0360638e-03),\n+            loss,\n+            3,\n+        )\n+\n+    def test_label_smoothing(self):\n+        logits = np.array([[4.9, -0.5, 2.05]])\n+        y_true = np.array([[1, 0, 0]])\n+        label_smoothing = 0.1\n+\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        loss = cce_obj(y_true, logits)\n+\n+        expected_value = 0.06685\n+        self.assertAlmostEqual(loss, expected_value, 3)\n\n@@ -1013,3 +1013,110 @@ def one_hot(x, num_classes, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return OneHot(num_classes, axis=axis).symbolic_call(x)\n     return backend.nn.one_hot(x, num_classes, axis=axis)\n+\n+\n+class BinaryCrossentropy(Operation):\n+    def __init__(self, from_logits=False):\n+        super().__init__()\n+        self.from_logits = from_logits\n+\n+    def call(self, target, output):\n+        return backend.nn.binary_crossentropy(\n+            target, output, from_logits=self.from_logits\n+        )\n+\n+    def compute_output_spec(self, target, output):\n+        if target.shape != output.shape:\n+            raise ValueError(\n+                \"Arguments `target` and `output` must have the same shape. \"\n+                \"Received: \"\n+                f\"target.shape={target.shape}, output.shape={output.shape}\"\n+            )\n+        return KerasTensor(output.shape, dtype=output.dtype)\n+\n+\n+def binary_crossentropy(target, output, from_logits=False):\n+    if any_symbolic_tensors((target, output)):\n+        return BinaryCrossentropy(from_logits=from_logits).symbolic_call(\n+            target, output\n+        )\n+    return backend.nn.binary_crossentropy(\n+        target, output, from_logits=from_logits\n+    )\n+\n+\n+class CategoricalCrossentropy(Operation):\n+    def __init__(self, from_logits=False, axis=-1):\n+        super().__init__()\n+        self.from_logits = from_logits\n+        self.axis = axis\n+\n+    def call(self, target, output):\n+        return backend.nn.categorical_crossentropy(\n+            target, output, from_logits=self.from_logits, axis=self.axis\n+        )\n+\n+    def compute_output_spec(self, target, output):\n+        if target.shape != output.shape:\n+            raise ValueError(\n+                \"Arguments `target` and `output` must have the same shape. \"\n+                \"Received: \"\n+                f\"target.shape={target.shape}, output.shape={output.shape}\"\n+            )\n+        if len(target.shape) < 1:\n+            raise ValueError(\n+                \"Arguments `target` and `output` must be at least rank 1. \"\n+                \"Received: \"\n+                f\"target.shape={target.shape}, output.shape={output.shape}\"\n+            )\n+        return KerasTensor(output.shape[:-1], dtype=output.dtype)\n+\n+\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    if any_symbolic_tensors((target, output)):\n+        return CategoricalCrossentropy(\n+            from_logits=from_logits, axis=axis\n+        ).symbolic_call(target, output)\n+    return backend.nn.categorical_crossentropy(\n+        target, output, from_logits=from_logits, axis=axis\n+    )\n+\n+\n+class SparseCategoricalCrossentropy(Operation):\n+    def __init__(self, from_logits=False, axis=-1):\n+        super().__init__()\n+        self.from_logits = from_logits\n+        self.axis = axis\n+\n+    def call(self, target, output):\n+        return backend.nn.sparse_categorical_crossentropy(\n+            target, output, from_logits=self.from_logits, axis=self.axis\n+        )\n+\n+    def compute_output_spec(self, target, output):\n+        if len(output.shape) < 1:\n+            raise ValueError(\n+                \"Argument `output` must be at least rank 1. \"\n+                \"Received: \"\n+                f\"output.shape={output.shape}\"\n+            )\n+        target_shape = target.shape\n+        if len(target_shape) == len(output.shape) and target_shape[-1] == 1:\n+            target_shape = target_shape[:-1]\n+        if target_shape != output.shape[:-1]:\n+            raise ValueError(\n+                \"Arguments `target` and `output` must have the same shape \"\n+                \"up until the last dimension: \"\n+                f\"target.shape={target.shape}, output.shape={output.shape}\"\n+            )\n+        return KerasTensor(output.shape[:-1], dtype=output.dtype)\n+\n+\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    if any_symbolic_tensors((target, output)):\n+        return SparseCategoricalCrossentropy(\n+            from_logits=from_logits, axis=axis\n+        ).symbolic_call(target, output)\n+    return backend.nn.sparse_categorical_crossentropy(\n+        target, output, from_logits=from_logits, axis=axis\n+    )\n\n@@ -553,6 +553,23 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knn.one_hot(x, 5, 1).shape, (2, 5, 3, 1))\n         self.assertEqual(knn.one_hot(x, 5, 2).shape, (2, 3, 5, 1))\n \n+    def test_binary_crossentropy(self):\n+        x1 = KerasTensor([2, 3, 1])\n+        x2 = KerasTensor([2, 3, 1])\n+        self.assertEqual(knn.binary_crossentropy(x1, x2).shape, (2, 3, 1))\n+\n+    def test_categorical_crossentropy(self):\n+        x1 = KerasTensor([2, 3, 4])\n+        x2 = KerasTensor([2, 3, 4])\n+        self.assertEqual(knn.categorical_crossentropy(x1, x2).shape, (2, 3))\n+\n+    def test_sparse_categorical_crossentropy(self):\n+        x1 = KerasTensor([2, 3], dtype=\"int32\")\n+        x2 = KerasTensor([2, 3, 4])\n+        self.assertEqual(\n+            knn.sparse_categorical_crossentropy(x1, x2).shape, (2, 3)\n+        )\n+\n \n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n@@ -955,3 +972,91 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.one_hot(indices_2d, 4, axis=1),\n             tf.one_hot(indices_2d, 4, axis=1),\n         )\n+\n+    def test_binary_crossentropy(self):\n+        # Test with from_logits=False\n+        target = np.array([[0.1], [0.9], [0.2], [1.0]])\n+        output = np.array([[0.1], [0.2], [0.3], [0.4]])\n+        result = knn.binary_crossentropy(target, output, from_logits=False)\n+        self.assertAllClose(\n+            result,\n+            np.array([[0.32508277], [1.47080801], [0.52613434], [0.91629048]]),\n+        )\n+\n+        # Test with from_logits=True\n+        target = np.array([[0.1], [0.9], [0.2], [1.0]])\n+        output = np.array([[0.1], [0.2], [0.3], [0.4]])\n+        result = knn.binary_crossentropy(target, output, from_logits=True)\n+        self.assertAllClose(\n+            result,\n+            np.array([[0.73439666], [0.61813887], [0.79435524], [0.51301525]]),\n+        )\n+\n+        # Test with output clipping\n+        target = np.array([[0.1], [0.9], [0.2], [1.0]])\n+        output = np.array([[0.99], [-0.2], [0.9], [-0.4]])\n+        result = knn.binary_crossentropy(target, output, from_logits=True)\n+        self.assertAllClose(\n+            result,\n+            np.array([[1.206961], [0.778139], [1.061154], [0.913015]]),\n+        )\n+\n+    def test_categorical_crossentropy(self):\n+        target = np.array(\n+            [\n+                [0.33008796, 0.0391289, 0.9503603],\n+                [0.80376694, 0.92363342, 0.19147756],\n+            ]\n+        )\n+        output = np.array(\n+            [\n+                [0.23446431, 0.35822914, 0.06683268],\n+                [0.3413979, 0.05420256, 0.81619654],\n+            ]\n+        )\n+\n+        # Test from_logits=False\n+        result = knn.categorical_crossentropy(\n+            target, output, from_logits=False, axis=-1\n+        )\n+        self.assertAllClose(result, np.array([2.54095299, 3.96374412]))\n+\n+        # Test axis\n+        result = knn.categorical_crossentropy(\n+            target, output, from_logits=False, axis=0\n+        )\n+        self.assertAllClose(\n+            result, np.array([0.71683073, 1.87988172, 2.46810762])\n+        )\n+\n+        # Test from_logits=True\n+        result = knn.categorical_crossentropy(\n+            target, output, from_logits=True, axis=-1\n+        )\n+        self.assertAllClose(result, np.array([1.59419954, 2.49880593]))\n+\n+        # Test with output clipping\n+        output = np.array(\n+            [\n+                [1.23446431, -0.35822914, 1.06683268],\n+                [0.3413979, -0.05420256, 0.81619654],\n+            ]\n+        )\n+        result = knn.categorical_crossentropy(\n+            target, output, from_logits=True, axis=-1\n+        )\n+        self.assertAllClose(result, np.array([1.16825923, 2.55436813]))\n+\n+    def test_sparse_categorical_crossentropy(self):\n+        target = np.array([0, 1, 2])\n+        output = np.array(\n+            [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06], [0.05, 0.01, 0.94]]\n+        )\n+        result = knn.sparse_categorical_crossentropy(target, output)\n+        self.assertAllClose(result, [0.105361, 0.116534, 0.061875])\n+\n+        output = np.array([[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]])\n+        result = knn.sparse_categorical_crossentropy(\n+            target, output, from_logits=True\n+        )\n+        self.assertAllClose(result, [0.001822, 0.000459, 0.169846])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
