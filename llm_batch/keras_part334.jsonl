{"custom_id": "keras#58c20dbe9cd05274df66eac37d9be8dc5a5ba121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 2902 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -289,14 +289,13 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     path = os.path.join(self.get_temp_dir(), 'no_optimizer')\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n \n-    with self.cached_session():\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(1))\n     model.compile('adam', loss='mse')\n     model.train_on_batch(x, y)\n     model.save(path, save_format='tf', include_optimizer=False)\n-\n     variables = get_variables(path)\n+\n     for v in variables:\n       self.assertNotIn('optimizer', v)\n \n@@ -1034,7 +1033,6 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     if not tf.executing_eagerly() and not fit:\n       self.skipTest('b/181767784')\n \n-    with self.cached_session():\n     input_ = keras.Input((4,))\n     model = keras.Model(\n         input_,\n\n@@ -1276,13 +1276,9 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n       del y_true, y_pred\n       return 0\n \n-    with self.cached_session():\n-      custom_metric = CustomMetric()\n     model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(loss='mse', optimizer='SGD',\n-                    metrics=[custom_metric, zero_metric])\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self.evaluate([v.initializer for v in custom_metric.variables])\n+                  metrics=[CustomMetric(), zero_metric])\n     model.fit(x, y)\n     saved_model_dir = self._save_model_dir()\n     tf.saved_model.save(model, saved_model_dir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98a762224578cf5e15be39fddf6917cf8efea6e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 531 | Lines Deleted: 154 | Files Changed: 29 | Hunks: 133 | Methods Changed: 73 | Complexity Δ (Sum/Max): 40/11 | Churn Δ: 685 | Churn Cumulative: 63459 | Contributors (this commit): 232 | Commits (past 90d): 161 | Contributors (cumulative): 323 | DMM Complexity: 0.7519685039370079\n\nDIFF:\n@@ -150,7 +150,7 @@ def MobileNetV2(input_shape=None,\n           of filters in each layer.\n       - If `alpha` > 1.0, proportionally increases the number\n           of filters in each layer.\n-      - If `alpha` = 1, default number of filters from the paper\n+      - If `alpha` = 1.0, default number of filters from the paper\n           are used at each layer.\n     include_top: Boolean, whether to include the fully-connected\n       layer at the top of the network. Defaults to `True`.\n@@ -405,13 +405,13 @@ def MobileNetV2(input_shape=None,\n   if weights == 'imagenet':\n     if include_top:\n       model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n-                    str(alpha) + '_' + str(rows) + '.h5')\n+                    str(float(alpha)) + '_' + str(rows) + '.h5')\n       weight_path = BASE_WEIGHT_PATH + model_name\n       weights_path = data_utils.get_file(\n           model_name, weight_path, cache_subdir='models')\n     else:\n       model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n-                    str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n+                    str(float(alpha)) + '_' + str(rows) + '_no_top' + '.h5')\n       weight_path = BASE_WEIGHT_PATH + model_name\n       weights_path = data_utils.get_file(\n           model_name, weight_path, cache_subdir='models')\n\n@@ -77,11 +77,15 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n-  For ModelNetV3, input preprocessing is included as part of the model\n-  (as a `Rescaling` layer), and thus\n+  For ModelNetV3, by default input preprocessing is included as a part of the\n+  model (as a `Rescaling` layer), and thus\n   `tf.keras.applications.mobilenet_v3.preprocess_input` is actually a\n-  pass-through function. ModelNetV3 models expect their inputs to be float\n-  tensors of pixels with values in the [0-255] range.\n+  pass-through function. In this use case, ModelNetV3 models expect their inputs\n+  to be float tensors of pixels with values in the [0-255] range.\n+  At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n+  layer) can be disabled by setting `include_preprocessing` argument to False.\n+  With preprocessing disabled ModelNetV3 models expect their inputs to be float\n+  tensors of pixels with values in the [-1, 1] range.\n \n   Args:\n     input_shape: Optional shape tuple, to be specified if you would\n@@ -138,10 +142,13 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n       When loading pretrained weights, `classifier_activation` can only\n       be `None` or `\"softmax\"`.\n+    include_preprocessing: Boolean, whether to include the preprocessing\n+      layer (`Rescaling`) at the bottom of the network. Defaults to `True`.\n \n   Call arguments:\n     inputs: A floating point `numpy.array` or a `tf.Tensor`, 4D with 3 color\n-      channels, with values in the range [0, 255].\n+      channels, with values in the range [0, 255] if `include_preprocessing`\n+      is True and in the range [-1, 1] otherwise.\n \n   Returns:\n     A `keras.Model` instance.\n@@ -160,7 +167,8 @@ def MobileNetV3(stack_fn,\n                 classes=1000,\n                 pooling=None,\n                 dropout_rate=0.2,\n-                classifier_activation='softmax'):\n+                classifier_activation='softmax',\n+                include_preprocessing=True):\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n                      '`None` (random initialization), `imagenet` '\n@@ -262,6 +270,7 @@ def MobileNetV3(stack_fn,\n     se_ratio = 0.25\n \n   x = img_input\n+  if include_preprocessing:\n     x = layers.Rescaling(scale=1. / 127.5, offset=-1.)(x)\n   x = layers.Conv2D(\n       16,\n@@ -357,7 +366,8 @@ def MobileNetV3Small(input_shape=None,\n                      classes=1000,\n                      pooling=None,\n                      dropout_rate=0.2,\n-                     classifier_activation='softmax'):\n+                     classifier_activation='softmax',\n+                     include_preprocessing=True):\n \n   def stack_fn(x, kernel, activation, se_ratio):\n \n@@ -380,7 +390,7 @@ def MobileNetV3Small(input_shape=None,\n \n   return MobileNetV3(stack_fn, 1024, input_shape, alpha, 'small', minimalistic,\n                      include_top, weights, input_tensor, classes, pooling,\n-                     dropout_rate, classifier_activation)\n+                     dropout_rate, classifier_activation, include_preprocessing)\n \n \n @keras_export('keras.applications.MobileNetV3Large')\n@@ -393,7 +403,8 @@ def MobileNetV3Large(input_shape=None,\n                      classes=1000,\n                      pooling=None,\n                      dropout_rate=0.2,\n-                     classifier_activation='softmax'):\n+                     classifier_activation='softmax',\n+                     include_preprocessing=True):\n \n   def stack_fn(x, kernel, activation, se_ratio):\n \n@@ -422,7 +433,7 @@ def MobileNetV3Large(input_shape=None,\n \n   return MobileNetV3(stack_fn, 1280, input_shape, alpha, 'large', minimalistic,\n                      include_top, weights, input_tensor, classes, pooling,\n-                     dropout_rate, classifier_activation)\n+                     dropout_rate, classifier_activation, include_preprocessing)\n \n \n MobileNetV3Small.__doc__ = BASE_DOCSTRING.format(name='MobileNetV3Small')\n\n@@ -2245,7 +2245,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     \"\"\"Writes Keras model train_function graph to TensorBoard.\"\"\"\n     with self._train_writer.as_default():\n       with tf.summary.record_if(True):\n-        train_fn = self.model.train_function\n+        train_fn = self.model.train_tf_function\n         # If the train_function is a `tf.function`, we can write out a graph\n         if hasattr(train_fn, 'function_spec'):\n           tf.summary.graph(train_fn._concrete_stateful_fn.graph)  # pylint: disable=protected-access\n\n@@ -37,10 +37,9 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n   def testModelFitWithDatasetInstance(self, strategy):\n     with self.assertRaisesRegex(\n         NotImplementedError,\n-        \"Only `tf.keras.utils.experimental.DatasetCreator` input is supported \"\n-        \"with `ParameterServerStrategy` at this time. Please see \"\n-        \"`tf.keras.utils.experimental.DatasetCreator` class docstring for \"\n-        \"more information.\"):\n+        \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n+        \"numpy arrays and pandas dataframes are supported types at this \"\n+        \"time.\"):\n       self._model_fit(\n           strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n \n@@ -106,13 +105,11 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n   def testModelEvaluateWithDatasetInstance(self, strategy):\n     with self.assertRaisesRegex(\n         NotImplementedError,\n-        \"Only `tf.keras.utils.experimental.DatasetCreator` input is supported \"\n-        \"with `ParameterServerStrategy` at this time. Please see \"\n-        \"`tf.keras.utils.experimental.DatasetCreator` class docstring for more \"\n-        \"information.\"):\n+        \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n+        \"numpy arrays and pandas dataframes are supported types at this \"\n+        \"time.\"):\n       self._model_evaluate(\n-          strategy,\n-          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n+          strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n \n   def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy):\n \n\n@@ -16,6 +16,8 @@\n \"\"\"Tests for `DatasetCreator` with `Model.fit` across usages and strategies.\"\"\"\n \n import tensorflow.compat.v2 as tf\n+\n+import numpy as np\n from tensorflow.python.framework import test_util\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n from keras.distribute import strategy_combinations\n@@ -42,6 +44,30 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n     model = self._model_fit(strategy)\n     self.assertEqual(model.optimizer.iterations, 100)\n \n+  def testModelFitWithNumpyData(self, strategy):\n+    x = np.random.rand(100, 10)\n+    y = np.random.rand(100, 1)\n+    model = self._model_fit(\n+        strategy,\n+        x=x,\n+        y=y,\n+        batch_size=1,\n+        validation_data=(x, y),\n+    )\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n+  def testModelFitWithTensorData(self, strategy):\n+    x = tf.random.uniform((100, 10))\n+    y = tf.random.uniform((100,))\n+    model = self._model_fit(\n+        strategy,\n+        x=x,\n+        y=y,\n+        batch_size=1,\n+        validation_data=(x, y),\n+    )\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n   def testModelFitWithLookupLayer(self, strategy):\n     model = self._model_fit(strategy, use_lookup_layer=True)\n     self.assertEqual(model.optimizer.iterations, 100)\n@@ -66,6 +92,28 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n     self._model_evaluate(strategy)\n     self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)\n \n+  def testModelEvaluateWithNumpyData(self, strategy):\n+    x = np.random.rand(100, 10)\n+    y = np.random.rand(100, 1)\n+    self._model_evaluate(\n+        strategy,\n+        x=x,\n+        y=y,\n+        batch_size=1,\n+    )\n+    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)\n+\n+  def testModelEvaluateWithTensorData(self, strategy):\n+    x = tf.random.uniform((100, 10))\n+    y = tf.random.uniform((100,))\n+    self._model_evaluate(\n+        strategy,\n+        x=x,\n+        y=y,\n+        batch_size=1,\n+    )\n+    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)\n+\n   def testModelEvaluateWithNormalizationLayer(self, strategy):\n     self._model_evaluate(strategy, with_normalization_layer=True)\n     self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)\n@@ -92,6 +140,21 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n     self.assertFalse(\n         all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n \n+  def testModelPredictWithNumpyData(self, strategy):\n+    x = np.array([1., 2., 3., 1., 5., 1.])\n+    _, predictions = self._model_predict(strategy, test_data=x)\n+\n+    self.assertTrue(all(predictions[0] == predictions[i] for i in [0, 3, 5]))\n+    self.assertFalse(\n+        all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n+\n+  def testModelPredictWithTensorData(self, strategy):\n+    x = tf.constant([1., 2., 3., 1., 5., 1.])\n+    _, predictions = self._model_predict(strategy, test_data=x)\n+    self.assertTrue(all(predictions[0] == predictions[i] for i in [0, 3, 5]))\n+    self.assertFalse(\n+        all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n+\n   def testModelPredictWithNormalizationLayer(self, strategy):\n     _, predictions = self._model_predict(\n         strategy, with_normalization_layer=True, steps=3)\n@@ -161,6 +224,10 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n     self.assertFalse(\n         all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n \n+  def testModelTrainTFFunction(self, strategy):\n+    model = self._model_fit(strategy)\n+    self.assertIsInstance(model.train_tf_function, tf.__internal__.function.Function)\n+\n \n if __name__ == \"__main__\":\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -116,6 +116,9 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                  steps_per_execution=1,\n                  validation_data=None,\n                  x=None,\n+                 y=None,\n+                 shuffle=True,\n+                 batch_size=None,\n                  steps_per_epoch=10,\n                  run_eagerly=False,\n                  with_normalization_layer=False,\n@@ -131,14 +134,18 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                                                    use_lookup_layer)\n     callbacks += default_callbacks\n \n-    x = x or dataset_creator.DatasetCreator(\n+    if x is None:\n+      x = dataset_creator.DatasetCreator(self._get_dataset_fn(use_lookup_layer))\n+\n+    if validation_data is None:\n+      validation_data = dataset_creator.DatasetCreator(\n           self._get_dataset_fn(use_lookup_layer))\n-    validation_data = (\n-        validation_data or\n-        dataset_creator.DatasetCreator(self._get_dataset_fn(use_lookup_layer)))\n \n     model.fit(\n         x,\n+        y,\n+        shuffle=shuffle,\n+        batch_size=batch_size,\n         epochs=10,\n         steps_per_epoch=steps_per_epoch,\n         callbacks=callbacks,\n@@ -149,7 +156,9 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n   def _model_evaluate(self,\n                       strategy,\n                       steps_per_execution=1,\n-                      validation_data=None,\n+                      x=None,\n+                      y=None,\n+                      batch_size=None,\n                       steps=10,\n                       run_eagerly=False,\n                       with_normalization_layer=False,\n@@ -172,9 +181,11 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n       return tf.data.Dataset.from_tensor_slices(\n           (x, y)).shuffle(10).repeat().batch(8)\n \n-    validation_data = (\n-        validation_data or dataset_creator.DatasetCreator(dataset_fn))\n-    model.evaluate(x=validation_data, steps=steps, callbacks=callbacks)\n+    if x is None:\n+      x = dataset_creator.DatasetCreator(dataset_fn)\n+\n+    model.evaluate(\n+        x=x, y=y, steps=steps, callbacks=callbacks, batch_size=batch_size)\n     return model\n \n   def _model_predict(\n@@ -199,7 +210,9 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n     def create_test_data():\n       x = tf.constant([1., 2., 3., 1., 5., 1.])\n       return tf.data.Dataset.from_tensor_slices(x).repeat().batch(2)\n-    test_data = test_data or create_test_data()\n+\n+    if test_data is None:\n+      test_data = create_test_data()\n \n     predictions = model.predict(x=test_data, steps=steps, callbacks=callbacks)\n     predictions = np.around(predictions, 4)\n\n@@ -232,7 +232,6 @@ def all_strategy_minus_default_and_tpu_combinations():\n           tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       mode=['graph', 'eager'])\n \n@@ -1287,9 +1286,7 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-              tf.__internal__.distribute.combinations\n-              .mirrored_strategy_with_two_gpus_no_merge_call,\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n           ],\n           mode=['graph', 'eager']))\n   def test_learning_phase_value(self, distribution):\n@@ -2025,9 +2022,7 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-              tf.__internal__.distribute.combinations\n-              .mirrored_strategy_with_two_gpus_no_merge_call,\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n           ],\n           mode=['graph', 'eager'],\n           reduction=[\n@@ -2183,9 +2178,7 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n               tf.__internal__.distribute.combinations.one_device_strategy,\n               tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-              tf.__internal__.distribute.combinations\n-              .mirrored_strategy_with_two_gpus_no_merge_call,\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n           ],\n           mode=['eager']))\n   def test_distribution_strategy_with_add_metric_object(\n\n@@ -18,6 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n from keras import metrics\n+from keras.engine import base_layer\n \n \n def _labeled_dataset_fn():\n@@ -69,8 +70,7 @@ def all_combinations():\n           tf.__internal__.distribute.combinations.default_strategy,\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n+          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n       ],\n       mode=[\"graph\"])\n \n@@ -118,6 +118,60 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n     self._test_metric(distribution, _dataset_fn, metrics.Mean, _expected_fn)\n \n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(\n+          distribution=[\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_one_cpu,\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+              tf.__internal__.distribute.combinations.tpu_strategy_packed_var\n+          ],\n+          mode=[\"eager\"],\n+          jit_compile=[False]) +\n+      tf.__internal__.test.combinations.combine(\n+          distribution=[tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus],\n+          mode=[\"eager\"],\n+          jit_compile=[True]))\n+  def testAddMetric(self, distribution, jit_compile):\n+    if not tf.__internal__.tf2.enabled():\n+      self.skipTest(\"Skip test since tf2 is not enabled. Pass \"\n+                    \" --test_env=TF2_BEHAVIOR=1 to enable tf2 behavior.\")\n+\n+    class MetricLayer(base_layer.Layer):\n+\n+      def __init__(self):\n+        super(MetricLayer, self).__init__(name=\"metric_layer\")\n+        self.sum = metrics.Sum(name=\"sum\")\n+        self.sum_var = tf.Variable(1.0)\n+\n+      def call(self, inputs):\n+        self.add_metric(self.sum(inputs))\n+        self.add_metric(\n+            tf.reduce_mean(inputs), name=\"mean\", aggregation=\"mean\")\n+        self.sum_var.assign(self.sum.result())\n+        return inputs\n+\n+    with distribution.scope():\n+      layer = MetricLayer()\n+\n+    def func():\n+      return layer(tf.ones(()))\n+\n+    if jit_compile:\n+      func = tf.function(jit_compile=True)(func)\n+\n+    @tf.function\n+    def run():\n+      return distribution.run(func)\n+\n+    run()\n+\n+    self.assertEqual(layer.metrics[0].result().numpy(),\n+                     1.0 * distribution.num_replicas_in_sync)\n+    self.assertEqual(layer.metrics[1].result().numpy(), 1.0)\n+    self.assertEqual(layer.sum_var.read_value().numpy(),\n+                     1.0 * distribution.num_replicas_in_sync)\n+\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -16,6 +16,7 @@\n \"\"\"Tests for ClusterCoordinator and Keras models.\"\"\"\n \n import tensorflow.compat.v2 as tf\n+import keras\n from keras.distribute import multi_worker_testing_utils\n from keras.engine import base_layer\n \n@@ -105,7 +106,33 @@ class ShardedVariableTest(tf.test.TestCase):\n     checkpoint_deps = set(dep.ref for dep in layer._checkpoint_dependencies)\n     self.assertEqual(checkpoint_deps, set([layer.w, layer.b]))\n \n+  def test_saved_model(self):\n \n-if __name__ == \"__main__\":\n+    def create_model():\n+      inputs = keras.layers.Input(shape=(4,))\n+      outputs = keras.layers.Dense(2)(inputs)\n+      model = keras.Model(inputs, outputs)\n+      model.compile(optimizer='adam', loss='mean_squared_error')\n+      return model\n+\n+    with self.strategy.scope():\n+      model = create_model()\n+\n+    inputs = tf.random.normal(shape=(8, 4))\n+    expect = model(inputs)\n+    saved_dir = self.get_temp_dir()\n+    model.save(saved_dir)\n+\n+    loaded_model = keras.models.load_model(saved_dir)\n+    got = loaded_model(inputs)\n+    self.assertAllClose(got, expect)\n+    self.assertGreater(len(model.variables), len(loaded_model.variables))\n+\n+    with self.assertRaises(ValueError):\n+      with self.strategy.scope():\n+        keras.models.load_model(saved_dir)\n+\n+\n+if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -20,15 +20,13 @@ import tensorflow.compat.v2 as tf\n multidevice_strategies = [\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-    tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.tpu_strategy,\n ]\n \n multiworker_strategies = [\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n-    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n-    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu_no_merge_call\n+    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu\n ]\n \n strategies_minus_default_minus_tpu = [\n@@ -36,7 +34,6 @@ strategies_minus_default_minus_tpu = [\n     tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-    tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.central_storage_strategy_with_gpu_and_cpu\n ]\n \n@@ -46,15 +43,13 @@ strategies_minus_tpu = [\n     tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-    tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.central_storage_strategy_with_gpu_and_cpu\n ]\n \n multi_worker_mirrored_strategies = [\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n-    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n-    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu_no_merge_call\n+    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu\n ]\n \n tpu_strategies = [\n\n@@ -1135,7 +1135,6 @@ class DataHandler(object):\n       self._steps_per_execution_value = steps_per_execution.numpy().item()\n \n     adapter_cls = select_data_adapter(x, y)\n-    self._verify_data_adapter_compatibility(adapter_cls)\n     self._adapter = adapter_cls(\n         x,\n         y,\n@@ -1159,9 +1158,6 @@ class DataHandler(object):\n     self._configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n                                                class_weight, distribute)\n \n-  def _verify_data_adapter_compatibility(self, adapter_cls):\n-    pass\n-\n   def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n                                             class_weight, distribute):\n     \"\"\"Configure the `_dataset` and `_inferred_steps` attributes.\"\"\"\n@@ -1318,15 +1314,30 @@ class DataHandler(object):\n class _ClusterCoordinatorDataHandler(DataHandler):\n   \"\"\"A `DataHandler` that is compatible with `ClusterCoordinator`.\"\"\"\n \n-  def _verify_data_adapter_compatibility(self, adapter_cls):\n-    if adapter_cls != DatasetCreatorAdapter:\n-      # TODO(b/186414920): Update the error message once `DatasetCreator` is no\n-      # longer experimental.\n+  def __init__(self, x, y=None, **kwargs):\n+    if not isinstance(x, dataset_creator.DatasetCreator):\n+      x = self._convert_to_dataset_creator(x, y, **kwargs)\n+\n+    super().__init__(x=x, **kwargs)\n+\n+  def _convert_to_dataset_creator(self, x, y, **kwargs):\n+    \"\"\"Converts non-tf.data.Dataset to `DatasetCreator` instances.\"\"\"\n+\n+    def _dataset_fn(input_context):\n+      del input_context\n+      data_adapter_cls = select_data_adapter(x, y)\n+      return data_adapter_cls(x=x, y=y, **kwargs).get_dataset()\n+\n+    # This check is needed because types like `tf.data.Dataset` don't work with\n+    # PSS yet. So only apply this logic to the types we can support.\n+    if (isinstance(x, _get_tensor_types()) and\n+        isinstance(y, _get_tensor_types())):\n+      return dataset_creator.DatasetCreator(_dataset_fn)\n+    else:\n       raise NotImplementedError(\n-          \"Only `tf.keras.utils.experimental.DatasetCreator` input is \"\n-          \"supported with `ParameterServerStrategy` at this time. Please see \"\n-          \"`tf.keras.utils.experimental.DatasetCreator` class docstring for \"\n-          \"more information.\")\n+          \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n+          \"numpy arrays and pandas dataframes are supported types at this \"\n+          \"time.\")\n \n   def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n                                             class_weight, distribute):\n\n@@ -129,6 +129,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   model = tf.keras.Model(inputs=inputs, outputs=outputs)\n   ```\n \n+  Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n+  inputs are not supported (e.g. lists of list or dicts of dict).\n+\n   2 - By subclassing the `Model` class: in that case, you should define your\n   layers in `__init__` and you should implement the model's forward pass\n   in `call`.\n@@ -435,7 +438,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     i.e. `model(inputs)`, which relies on the underlying `call` method.\n \n     Args:\n-        inputs: A tensor or list of tensors.\n+        inputs: Input tensor, or dict/list/tuple of input tensors.\n         training: Boolean or boolean scalar tensor, indicating whether to run\n           the `Network` in training mode or inference mode.\n         mask: A mask or list of masks. A mask can be\n@@ -588,6 +591,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self.train_function = None\n     self.test_function = None\n     self.predict_function = None\n+    # Used to cache the `tf.function`'ed `train_function` to be logged in\n+    # TensorBoard, since the original `train_function` is not necessarily\n+    # a `tf.function` (e.g., with ParameterServerStrategy, the `train_function`\n+    # is a scheduling of the actual training function to a remote worker).\n+    self.train_tf_function = None\n \n     # Used to cache `trainable` attr of `Layer`s for `fit`.\n     self._compiled_trainable_state = self._get_trainable_state()\n@@ -840,6 +848,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     if not self.run_eagerly:\n       train_function = tf.function(\n           train_function, experimental_relax_shapes=True)\n+      self.train_tf_function = train_function\n \n     self.train_function = train_function\n \n@@ -2707,14 +2716,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     train_function = self.train_function\n     test_function = self.test_function\n     predict_function = self.predict_function\n+    train_tf_function = self.train_tf_function\n     self.train_function = None\n     self.test_function = None\n     self.predict_function = None\n+    self.train_tf_function = None\n     functions = super(\n         Model, self)._list_functions_for_serialization(serialization_cache)\n     self.train_function = train_function\n     self.test_function = test_function\n     self.predict_function = predict_function\n+    self.train_tf_function = train_tf_function\n     return functions\n \n   def _should_eval(self, epoch, validation_freq):\n\n@@ -42,21 +42,23 @@ def _build_attention_equation(rank, attn_axes):\n   \"\"\"Builds einsum equations for the attention computation.\n \n   Query, key, value inputs after projection are expected to have the shape as:\n-  (bs, <non-attention dims>, <attention dims>, num_heads, channels).\n-  bs and <non-attention dims> are treated as <batch dims>.\n+  `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n+  `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n+\n   The attention operations can be generalized:\n   (1) Query-key dot product:\n-  (<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n+  `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n   <key attention dims>, num_heads, channels) -> (<batch dims>,\n-  num_heads, <query attention dims>, <key attention dims>)\n+  num_heads, <query attention dims>, <key attention dims>)`\n   (2) Combination:\n-  (<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n+  `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n   (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n-  <query attention dims>, num_heads, channels)\n+  <query attention dims>, num_heads, channels)`\n \n   Args:\n-    rank: the rank of query, key, value tensors.\n-    attn_axes: a list/tuple of axes, [-1, rank), that will do attention.\n+    rank: Rank of query, key, value tensors.\n+    attn_axes: List/tuple of axes, `[-1, rank)`,\n+      that attention will be applied to.\n \n   Returns:\n     Einsum equations.\n@@ -121,16 +123,17 @@ def _get_output_shape(output_rank, known_last_dims):\n class MultiHeadAttention(Layer):\n   \"\"\"MultiHeadAttention layer.\n \n-  This is an implementation of multi-headed attention based on \"Attention\n-  is all you Need\". If `query`, `key,` `value` are the same, then\n+  This is an implementation of multi-headed attention as described in the paper\n+  \"Attention is all you Need\" (Vaswani et al., 2017).\n+  If `query`, `key,` `value` are the same, then\n   this is self-attention. Each timestep in `query` attends to the\n   corresponding sequence in `key`, and returns a fixed-width vector.\n \n   This layer first projects `query`, `key` and `value`. These are\n   (effectively) a list of tensors of length `num_attention_heads`, where the\n-  corresponding shapes are [batch_size, <query dimensions>, key_dim],\n-  [batch_size, <key/value dimensions>, key_dim],\n-  [batch_size, <key/value dimensions>, value_dim].\n+  corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n+  `(batch_size, <key/value dimensions>, key_dim)`,\n+  `(batch_size, <key/value dimensions>, value_dim)`.\n \n   Then, the query and key tensors are dot-producted and scaled. These are\n   softmaxed to obtain attention probabilities. The value tensors are then\n@@ -182,11 +185,11 @@ class MultiHeadAttention(Layer):\n     bias_constraint: Constraint for dense layer kernels.\n \n   Call arguments:\n-    query: Query `Tensor` of shape `[B, T, dim]`.\n-    value: Value `Tensor` of shape `[B, S, dim]`.\n-    key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n+    query: Query `Tensor` of shape `(B, T, dim)`.\n+    value: Value `Tensor` of shape `(B, S, dim)`.\n+    key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n       `value` for both `key` and `value`, which is the most common case.\n-    attention_mask: a boolean mask of shape `[B, T, S]`, that prevents\n+    attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n       attention to certain positions. The boolean mask specifies which query\n       elements can attend to which key elements, 1 indicates attention and 0\n       indicates no attention. Broadcasting can happen for the missing batch\n@@ -200,7 +203,7 @@ class MultiHeadAttention(Layer):\n       or False (inference) if there is no parent layer.\n \n   Returns:\n-    attention_output: The result of the computation, of shape [B, T, E],\n+    attention_output: The result of the computation, of shape `(B, T, E)`,\n       where `T` is for target sequence shapes and `E` is the query input last\n       dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n       are project to the shape specified by `output_shape`.\n@@ -247,20 +250,13 @@ class MultiHeadAttention(Layer):\n \n   def get_config(self):\n     config = {\n-        \"num_heads\":\n-            self._num_heads,\n-        \"key_dim\":\n-            self._key_dim,\n-        \"value_dim\":\n-            self._value_dim,\n-        \"dropout\":\n-            self._dropout,\n-        \"use_bias\":\n-            self._use_bias,\n-        \"output_shape\":\n-            self._output_shape,\n-        \"attention_axes\":\n-            self._attention_axes,\n+        \"num_heads\": self._num_heads,\n+        \"key_dim\": self._key_dim,\n+        \"value_dim\": self._value_dim,\n+        \"dropout\": self._dropout,\n+        \"use_bias\": self._use_bias,\n+        \"output_shape\": self._output_shape,\n+        \"attention_axes\": self._attention_axes,\n         \"kernel_initializer\":\n             initializers.serialize(self._kernel_initializer),\n         \"bias_initializer\":\n@@ -292,7 +288,7 @@ class MultiHeadAttention(Layer):\n     layer = cls(**config)\n     if None in [query_shape, key_shape, value_shape]:\n       logging.warning(\n-          \"One of the input shape is missing. They should be \"\n+          \"One of dimensions of the input shape is missing. It should have been\"\n           \" memorized when the layer was serialized. \"\n           \"%s is created without weights.\",\n           str(cls))\n@@ -306,9 +302,9 @@ class MultiHeadAttention(Layer):\n     Once the method is called, self._built_from_signature will be set to True.\n \n     Args:\n-      query: query tensor or TensorShape.\n-      value: value tensor or TensorShape.\n-      key: key tensor or TensorShape.\n+      query: Query tensor or TensorShape.\n+      value: Value tensor or TensorShape.\n+      key: Key tensor or TensorShape.\n     \"\"\"\n     self._built_from_signature = True\n     if hasattr(query, \"shape\"):\n@@ -380,7 +376,7 @@ class MultiHeadAttention(Layer):\n     Args:\n       free_dims: Number of free dimensions for einsum equation building.\n       common_kwargs: Common keyword arguments for einsum layer.\n-      name: the name for the projection layer.\n+      name: Name for the projection layer.\n \n     Returns:\n       Projection layer.\n@@ -447,10 +443,10 @@ class MultiHeadAttention(Layer):\n     attention implementation.\n \n     Args:\n-      query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n-      key: Projected key `Tensor` of shape `[B, T, N, key_dim]`.\n-      value: Projected value `Tensor` of shape `[B, T, N, value_dim]`.\n-      attention_mask: a boolean mask of shape `[B, T, S]`, that prevents\n+      query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n+      key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n+      value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n+      attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n         attention to certain positions.\n       training: Python boolean indicating whether the layer should behave in\n         training mode (adding dropout) or in inference mode (doing nothing).\n\n@@ -169,14 +169,17 @@ class CategoryEncoding(base_layer.Layer):\n             tf.cast(out_depth, max_value.dtype), max_value),\n         tf.greater_equal(\n             min_value, tf.cast(0, min_value.dtype)))\n-    tf.Assert(condition, [\n+    assertion = tf.Assert(condition, [\n         \"Input values must be in the range 0 <= values < num_tokens\"\n         \" with num_tokens={}\".format(out_depth)\n     ])\n+    with tf.control_dependencies([assertion]):\n       if self.sparse:\n-      return sparse_bincount(inputs, out_depth, multi_hot_output, count_weights)\n+        return sparse_bincount(inputs, out_depth, multi_hot_output,\n+                               count_weights)\n       else:\n-      return dense_bincount(inputs, out_depth, multi_hot_output, count_weights)\n+        return dense_bincount(inputs, out_depth, multi_hot_output,\n+                              count_weights)\n \n \n def sparse_bincount(inputs, out_depth, multi_hot_output, count_weights=None):\n\n@@ -246,7 +246,8 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     _ = model.predict(input_array, steps=1)\n \n   def test_dense_oov_input(self):\n-    input_array = tf.constant([[0, 1, 2], [2, 3, 1]])\n+    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])\n+    invalid_array = tf.constant([[0, 1, 2], [2, 3, 1]])\n     num_tokens = 3\n     expected_output_shape = [None, num_tokens]\n     encoder_layer = category_encoding.CategoryEncoding(num_tokens)\n@@ -254,13 +255,16 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     int_data = encoder_layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=int_data)\n+    # Call predict once on valid input to compile a graph and test control flow.\n+    _ = model.predict(valid_array, steps=1)\n     with self.assertRaisesRegex(\n         tf.errors.InvalidArgumentError,\n         \".*must be in the range 0 <= values < num_tokens.*\"):\n-      _ = model.predict(input_array, steps=1)\n+      _ = model.predict(invalid_array, steps=1)\n \n   def test_dense_negative(self):\n-    input_array = tf.constant([[1, 2, 0], [2, 2, -1]])\n+    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])\n+    invalid_array = tf.constant([[1, 2, 0], [2, 2, -1]])\n     num_tokens = 3\n     expected_output_shape = [None, num_tokens]\n     encoder_layer = category_encoding.CategoryEncoding(num_tokens)\n@@ -268,10 +272,12 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     int_data = encoder_layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=int_data)\n+    # Call predict once on valid input to compile a graph and test control flow.\n+    _ = model.predict(valid_array, steps=1)\n     with self.assertRaisesRegex(\n         tf.errors.InvalidArgumentError,\n         \".*must be in the range 0 <= values < num_tokens.*\"):\n-      _ = model.predict(input_array, steps=1)\n+      _ = model.predict(invalid_array, steps=1)\n \n   def test_legacy_max_tokens_arg(self):\n     input_array = np.array([[1, 2, 3, 1]])\n\n@@ -22,6 +22,7 @@ from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.engine.input_spec import InputSpec\n+from keras.preprocessing import image as image_preprocessing\n from keras.utils import control_flow_util\n from tensorflow.python.ops import stateless_random_ops\n from tensorflow.python.util.tf_export import keras_export\n@@ -66,24 +67,38 @@ class Resizing(base_layer.Layer):\n     interpolation: String, the interpolation method. Defaults to `bilinear`.\n       Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n       `gaussian`, `mitchellcubic`\n+    crop_to_aspect_ratio: If True, resize the images without aspect\n+      ratio distortion. When the original aspect ratio differs from the target\n+      aspect ratio, the output image will be cropped so as to return the largest\n+      possible window in the image (of size `(height, width)`) that matches\n+      the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n+      aspect ratio may not be preserved.\n   \"\"\"\n \n   def __init__(self,\n                height,\n                width,\n                interpolation='bilinear',\n+               crop_to_aspect_ratio=False,\n                **kwargs):\n     self.target_height = height\n     self.target_width = width\n     self.interpolation = interpolation\n+    self.crop_to_aspect_ratio = crop_to_aspect_ratio\n     self._interpolation_method = get_interpolation(interpolation)\n     self.input_spec = InputSpec(ndim=4)\n     super(Resizing, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Resizing').set(True)\n \n   def call(self, inputs):\n+    if self.crop_to_aspect_ratio:\n+      outputs = image_preprocessing.smart_resize(\n+          inputs,\n+          size=[self.target_height, self.target_width],\n+          interpolation=self._interpolation_method)\n+    else:\n       outputs = tf.image.resize(\n-        images=inputs,\n+          inputs,\n           size=[self.target_height, self.target_width],\n           method=self._interpolation_method)\n     return outputs\n@@ -98,6 +113,7 @@ class Resizing(base_layer.Layer):\n         'height': self.target_height,\n         'width': self.target_width,\n         'interpolation': self.interpolation,\n+        'crop_to_aspect_ratio': self.crop_to_aspect_ratio,\n     }\n     base_config = super(Resizing, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -60,6 +60,9 @@ class ResizingTest(keras_parameterized.TestCase):\n       'interpolation': 'area'\n   }, 2, 2), ('down_sample_area_3_by_2', {\n       'interpolation': 'area'\n+  }, 3, 2), ('down_sample_crop_to_aspect_ratio_3_by_2', {\n+      'interpolation': 'bilinear',\n+      'crop_to_aspect_ratio': True,\n   }, 3, 2))\n   def test_down_sampling(self, kwargs, expected_height, expected_width):\n     with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n@@ -77,7 +80,10 @@ class ResizingTest(keras_parameterized.TestCase):\n       'interpolation': 'area'\n   }, 10, 12), ('up_sample_area_12_by_12', {\n       'interpolation': 'area'\n-  }, 12, 12))\n+  }, 12, 12), ('up_sample_crop_to_aspect_ratio_12_by_14', {\n+      'interpolation': 'bilinear',\n+      'crop_to_aspect_ratio': True,\n+  }, 12, 14))\n   def test_up_sampling(self, kwargs, expected_height, expected_width):\n     with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n       self._run_test(kwargs, expected_height, expected_width)\n@@ -133,6 +139,20 @@ class ResizingTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.Resizing.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_crop_to_aspect_ratio(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype('float32')\n+      layer = image_preprocessing.Resizing(4, 2, crop_to_aspect_ratio=True)\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [1, 2],\n+          [5, 6],\n+          [9, 10],\n+          [13, 14]\n+      ]).astype('float32')\n+      expected_output = np.reshape(expected_output, (1, 4, 2, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n \n def get_numpy_center_crop(images, expected_height, expected_width):\n   orig_height = images.shape[1]\n\n@@ -28,6 +28,7 @@ from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import table_utils\n from keras.saving.saved_model import layer_serialization\n from keras.utils import layer_utils\n+from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n \n INT = \"int\"\n@@ -617,12 +618,21 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     lookup_checks = []\n \n     if self.num_oov_indices == 0 and not self.invert:\n-      oov_indices = tf.where(tf.equal(lookup_result, -1))\n-      oov_inputs = tf.compat.v1.gather_nd(inputs, oov_indices)\n+      if tf_utils.is_sparse(inputs):\n+        lookup_values = lookup_result.values\n+        input_values = inputs.values\n+      elif tf_utils.is_ragged(inputs):\n+        lookup_values = lookup_result.flat_values\n+        input_values = inputs.flat_values\n+      else:\n+        lookup_values = lookup_result\n+        input_values = inputs\n+      oov_indices = tf.where(tf.equal(lookup_values, -1))\n+      oov_inputs = tf.compat.v1.gather_nd(input_values, oov_indices)\n       msg = tf.strings.format(\n           \"When `num_oov_indices=0` all inputs should be in vocabulary, \"\n-          \"found OOV values {} at indices {}, consider setting \"\n-          \"`num_oov_indices=1`.\", (oov_inputs, oov_indices))\n+          \"found OOV values {}, consider setting `num_oov_indices=1`.\",\n+          (oov_inputs,))\n       assertion = tf.Assert(\n           tf.equal(tf.compat.v1.size(oov_indices), 0), [msg])\n       lookup_checks.append(assertion)\n\n@@ -761,6 +761,59 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n                                 \"found OOV values.*michigan\"):\n       _ = model.predict(invalid_input)\n \n+  def test_int_output_no_oov_ragged(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    valid_input = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"\"]])\n+    invalid_input = np.array([[\"earth\", \"wind\", \"and\", \"michigan\"],\n+                              [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    valid_input = tf.RaggedTensor.from_tensor(valid_input)\n+    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=None,\n+        num_oov_indices=0,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string)\n+    layer.set_vocabulary(vocab_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_data = model.predict(valid_input)\n+    self.assertAllEqual(expected_output, output_data)\n+    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n+                                \"found OOV values.*michigan\"):\n+      _ = model.predict(invalid_input)\n+\n+  def test_int_output_no_oov_sparse(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    valid_input = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"\"]])\n+    invalid_input = np.array([[\"earth\", \"wind\", \"and\", \"michigan\"],\n+                              [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    valid_input = tf.sparse.from_dense(valid_input)\n+    invalid_input = tf.sparse.from_dense(invalid_input)\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=None,\n+        num_oov_indices=0,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string)\n+    layer.set_vocabulary(vocab_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_data = model.predict(valid_input)\n+    self.assertAllEqual(expected_output,\n+                        tf.sparse.to_dense(output_data))\n+    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n+                                \"found OOV values.*michigan\"):\n+      _ = model.predict(invalid_input)\n+\n   def test_int_output_explicit_vocab(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n\n@@ -1953,7 +1953,7 @@ class AUC(Metric):\n   of binary classifiers. Unlike the accuracy, and like cross-entropy\n   losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n \n-  This classes approximates AUCs using a Riemann sum: During the metric\n+  This class approximates AUCs using a Riemann sum. During the metric\n   accumulation phrase, predictions are accumulated within predefined buckets\n   by value. The AUC is then computed by interpolating per-bucket averages. These\n   buckets define the evaluated operational points.\n\n@@ -89,8 +89,9 @@ def smart_resize(x, size, interpolation='bilinear'):\n   we resize the `(340, 340)` crop to `(200, 200)`.\n \n   Args:\n-    x: Input image (as a tensor or NumPy array). Must be in format\n-      `(height, width, channels)`.\n+    x: Input image or batch of images (as a tensor or NumPy array).\n+      Must be in format `(height, width, channels)` or\n+      `(batch_size, height, width, channels)`.\n     size: Tuple of `(height, width)` integer. Target size.\n     interpolation: String, interpolation to use for resizing.\n       Defaults to `'bilinear'`. Supports `bilinear`, `nearest`, `bicubic`,\n@@ -106,11 +107,16 @@ def smart_resize(x, size, interpolation='bilinear'):\n                      'but got: %s' % (size,))\n   img = tf.convert_to_tensor(x)\n   if img.shape.rank is not None:\n-    if img.shape.rank != 3:\n+    if img.shape.rank < 3 or img.shape.rank > 4:\n       raise ValueError(\n-          'Expected an image array with shape `(height, width, channels)`, but '\n+          'Expected an image array with shape `(height, width, channels)`, '\n+          'or `(batch_size, height, width, channels)` but '\n           'got input with incorrect rank, of shape %s' % (img.shape,))\n   shape = tf.compat.v1.shape(img)\n+  if img.shape.rank == 4:\n+    height, width = shape[1], shape[2]\n+    static_num_channels = img.shape[-1]\n+  else:\n     height, width = shape[0], shape[1]\n   target_height, target_width = size\n \n@@ -128,6 +134,10 @@ def smart_resize(x, size, interpolation='bilinear'):\n   crop_box_wstart = tf.cast(\n       tf.cast(width - crop_width, 'float32') / 2, 'int32')\n \n+  if img.shape.rank == 4:\n+    crop_box_start = tf.stack([0, crop_box_hstart, crop_box_wstart, 0])\n+    crop_box_size = tf.stack([-1, crop_height, crop_width, -1])\n+  else:\n     crop_box_start = tf.stack([crop_box_hstart, crop_box_wstart, 0])\n     crop_box_size = tf.stack([crop_height, crop_width, -1])\n \n@@ -136,12 +146,16 @@ def smart_resize(x, size, interpolation='bilinear'):\n       images=img,\n       size=size,\n       method=interpolation)\n+  if img.shape.rank == 4:\n+    # Apparent bug in resize_images_v2 may cause shape to be lost\n+    img.set_shape((None, None, None, static_num_channels))\n   if isinstance(x, np.ndarray):\n     return img.numpy()\n   return img\n \n \n-@keras_export('keras.preprocessing.image.array_to_img')\n+@keras_export('keras.utils.array_to_img',\n+              'keras.preprocessing.image.array_to_img')\n def array_to_img(x, data_format=None, scale=True, dtype=None):\n   \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n \n@@ -184,7 +198,8 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n   return image.array_to_img(x, data_format=data_format, scale=scale, **kwargs)\n \n \n-@keras_export('keras.preprocessing.image.img_to_array')\n+@keras_export('keras.utils.img_to_array',\n+              'keras.preprocessing.image.img_to_array')\n def img_to_array(img, data_format=None, dtype=None):\n   \"\"\"Converts a PIL Image instance to a Numpy array.\n \n@@ -225,7 +240,8 @@ def img_to_array(img, data_format=None, dtype=None):\n   return image.img_to_array(img, data_format=data_format, **kwargs)\n \n \n-@keras_export('keras.preprocessing.image.save_img')\n+@keras_export('keras.utils.save_img',\n+              'keras.preprocessing.image.save_img')\n def save_img(path,\n              x,\n              data_format=None,\n@@ -255,6 +271,8 @@ def save_img(path,\n                  scale=scale, **kwargs)\n \n \n+@keras_export('keras.utils.load_img',\n+              'keras.preprocessing.image.load_img')\n def load_img(path, grayscale=False, color_mode='rgb', target_size=None,\n              interpolation='nearest'):\n   \"\"\"Loads an image into PIL format.\n@@ -1129,4 +1147,3 @@ keras_export(\n keras_export('keras.preprocessing.image.random_brightness', allow_multiple_exports=True)(random_brightness)\n keras_export(\n     'keras.preprocessing.image.apply_affine_transform', allow_multiple_exports=True)(apply_affine_transform)\n-keras_export('keras.preprocessing.image.load_img', allow_multiple_exports=True)(load_img)\n\n@@ -27,7 +27,9 @@ from tensorflow.python.util.tf_export import keras_export\n ALLOWLIST_FORMATS = ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n \n \n-@keras_export('keras.preprocessing.image_dataset_from_directory', v1=[])\n+@keras_export('keras.utils.image_dataset_from_directory',\n+              'keras.preprocessing.image_dataset_from_directory',\n+              v1=[])\n def image_dataset_from_directory(directory,\n                                  labels='inferred',\n                                  label_mode='int',\n@@ -41,7 +43,8 @@ def image_dataset_from_directory(directory,\n                                  subset=None,\n                                  interpolation='bilinear',\n                                  follow_links=False,\n-                                 smart_resize=False):\n+                                 crop_to_aspect_ratio=False,\n+                                 **kwargs):\n   \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n \n   If your directory structure is:\n@@ -110,11 +113,13 @@ def image_dataset_from_directory(directory,\n       `area`, `lanczos3`, `lanczos5`, `gaussian`, `mitchellcubic`.\n     follow_links: Whether to visits subdirectories pointed to by symlinks.\n         Defaults to False.\n-    smart_resize: If True, the resizing function used will be\n-      `tf.keras.preprocessing.image.smart_resize`, which preserves the aspect\n-      ratio of the original image by using a mixture of resizing and cropping.\n-      If False (default), the resizing function is `tf.image.resize`, which\n-      does not preserve aspect ratio.\n+    crop_to_aspect_ratio: If True, resize the images without aspect\n+      ratio distortion. When the original aspect ratio differs from the target\n+      aspect ratio, the output image will be cropped so as to return the largest\n+      possible window in the image (of size `image_size`) that matches\n+      the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n+      aspect ratio may not be preserved.\n+    **kwargs: Legacy keyword arguments.\n \n   Returns:\n     A `tf.data.Dataset` object.\n@@ -142,6 +147,10 @@ def image_dataset_from_directory(directory,\n     - if `color_mode` is `rgba`,\n       there are 4 channel in the image tensors.\n   \"\"\"\n+  if 'smart_resize' in kwargs:\n+    crop_to_aspect_ratio = kwargs.pop('smart_resize')\n+  if kwargs:\n+    raise TypeError(f'Unknown keywords argument(s): {tuple(kwargs.keys())}')\n   if labels not in ('inferred', None):\n     if not isinstance(labels, (list, tuple)):\n       raise ValueError(\n@@ -205,7 +214,7 @@ def image_dataset_from_directory(directory,\n       label_mode=label_mode,\n       num_classes=len(class_names),\n       interpolation=interpolation,\n-      smart_resize=smart_resize)\n+      crop_to_aspect_ratio=crop_to_aspect_ratio)\n   if shuffle:\n     # Shuffle locally at each iteration\n     dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n@@ -224,11 +233,11 @@ def paths_and_labels_to_dataset(image_paths,\n                                 label_mode,\n                                 num_classes,\n                                 interpolation,\n-                                smart_resize=False):\n+                                crop_to_aspect_ratio=False):\n   \"\"\"Constructs a dataset of images and labels.\"\"\"\n   # TODO(fchollet): consider making num_parallel_calls settable\n   path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n-  args = (image_size, num_channels, interpolation, smart_resize)\n+  args = (image_size, num_channels, interpolation, crop_to_aspect_ratio)\n   img_ds = path_ds.map(\n       lambda x: load_image(x, *args))\n   if label_mode:\n@@ -238,12 +247,12 @@ def paths_and_labels_to_dataset(image_paths,\n \n \n def load_image(path, image_size, num_channels, interpolation,\n-               smart_resize=False):\n+               crop_to_aspect_ratio=False):\n   \"\"\"Load an image from a path and resize it.\"\"\"\n   img = tf.io.read_file(path)\n   img = tf.image.decode_image(\n       img, channels=num_channels, expand_animations=False)\n-  if smart_resize:\n+  if crop_to_aspect_ratio:\n     img = keras_image_ops.smart_resize(img, image_size,\n                                        interpolation=interpolation)\n   else:\n\n@@ -278,13 +278,13 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'No images found.'):\n       _ = image_dataset.image_dataset_from_directory(directory)\n \n-  def test_image_dataset_from_directory_smart_resize(self):\n+  def test_image_dataset_from_directory_crop_to_aspect_ratio(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n \n     directory = self._prepare_directory(num_classes=2, count=5)\n     dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=5, image_size=(18, 18), smart_resize=True)\n+        directory, batch_size=5, image_size=(18, 18), crop_to_aspect_ratio=True)\n     batch = next(iter(dataset))\n     self.assertLen(batch, 2)\n     self.assertEqual(batch[0].shape, (5, 18, 18, 3))\n\n@@ -24,7 +24,6 @@ from absl.testing import parameterized\n import numpy as np\n from keras import keras_parameterized\n from keras import layers\n-from keras import testing_utils\n from keras.engine import sequential\n from keras.preprocessing import image as preprocessing_image\n \n@@ -54,7 +53,6 @@ def _generate_test_images():\n \n class TestImage(keras_parameterized.TestCase):\n \n-  @testing_utils.run_v2_only\n   def test_smart_resize(self):\n     test_input = np.random.random((20, 40, 3))\n     output = preprocessing_image.smart_resize(test_input, size=(50, 50))\n@@ -72,7 +70,6 @@ class TestImage(keras_parameterized.TestCase):\n       ('size2', (10, 10)),\n       ('size3', (100, 50)),\n       ('size4', (5, 15)))\n-  @testing_utils.run_v2_only\n   def test_smart_resize_tf_dataset(self, size):\n     test_input_np = np.random.random((2, 20, 40, 3))\n     test_ds = tf.data.Dataset.from_tensor_slices(test_input_np)\n@@ -83,12 +80,21 @@ class TestImage(keras_parameterized.TestCase):\n       self.assertIsInstance(sample, np.ndarray)\n       self.assertListEqual(list(sample.shape), [size[0], size[1], 3])\n \n+  def test_smart_resize_batch(self):\n+    img = np.random.random((2, 20, 40, 3))\n+    out = preprocessing_image.smart_resize(img, size=(20, 20))\n+    self.assertListEqual(list(out.shape), [2, 20, 20, 3])\n+    self.assertAllClose(out, img[:, :, 10:-10, :])\n+\n   def test_smart_resize_errors(self):\n     with self.assertRaisesRegex(ValueError, 'a tuple of 2 integers'):\n       preprocessing_image.smart_resize(\n           np.random.random((20, 20, 2)), size=(10, 5, 3))\n     with self.assertRaisesRegex(ValueError, 'incorrect rank'):\n-      preprocessing_image.smart_resize(np.random.random((20, 40)), size=(10, 5))\n+      preprocessing_image.smart_resize(np.random.random((2, 4)), size=(10, 5))\n+    with self.assertRaisesRegex(ValueError, 'incorrect rank'):\n+      preprocessing_image.smart_resize(\n+          np.random.random((2, 4, 4, 5, 3)), size=(10, 5))\n \n   def test_image_data_generator(self):\n     if PIL is None:\n@@ -437,4 +443,5 @@ class TestImage(keras_parameterized.TestCase):\n \n \n if __name__ == '__main__':\n+  tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -21,7 +21,9 @@ from keras.preprocessing import dataset_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.preprocessing.text_dataset_from_directory', v1=[])\n+@keras_export('keras.utils.text_dataset_from_directory',\n+              'keras.preprocessing.text_dataset_from_directory',\n+              v1=[])\n def text_dataset_from_directory(directory,\n                                 labels='inferred',\n                                 label_mode='int',\n\n@@ -21,7 +21,9 @@ import numpy as np\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.preprocessing.timeseries_dataset_from_array', v1=[])\n+@keras_export('keras.utils.timeseries_dataset_from_array',\n+              'keras.preprocessing.timeseries_dataset_from_array',\n+              v1=[])\n def timeseries_dataset_from_array(\n     data,\n     targets,\n@@ -77,6 +79,7 @@ def timeseries_dataset_from_array(\n     only `batch_of_sequences`.\n \n   Example 1:\n+\n   Consider indices `[0, 1, ... 99]`.\n   With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,\n   `shuffle=False`, the dataset will yield batches of sequences\n@@ -94,7 +97,8 @@ def timeseries_dataset_from_array(\n   can be generated to include them (the next sequence would have started\n   at index 81, and thus its last step would have gone over 99).\n \n-  Example 2: temporal regression. \n+  Example 2: Temporal regression.\n+\n   Consider an array `data` of scalar values, of shape `(steps,)`.\n   To generate a dataset that uses the past 10\n   timesteps to predict the next timestep, you would use:\n@@ -111,7 +115,8 @@ def timeseries_dataset_from_array(\n     break\n   ```\n \n-  Example 3: temporal regression for many-to-many architectures.\n+  Example 3: Temporal regression for many-to-many architectures.\n+\n   Consider two arrays of scalar values `X` and `Y`,\n   both of shape `(100,)`. The resulting dataset should consist samples with\n   20 timestamps each. The samples should not overlap.\n\n@@ -101,7 +101,34 @@ def result_wrapper(result_fn):\n     \"\"\"Decorated function with merge_call.\"\"\"\n     has_strategy = tf.distribute.has_strategy()\n     replica_context = tf.distribute.get_replica_context()\n-    if not has_strategy or replica_context is None:\n+\n+    # The purpose of using `merge_call` to call `result()` is to trigger cross\n+    # replica aggregation of metric state variables (SyncOnReadVariable). After\n+    # we introduced `variable_sync_on_read_context`, in principle there is no\n+    # need to use `merge_call` here. However the branch still exists because:\n+    #\n+    # 1. Keras V1 training code sometimes assumes `result_t` is the same tensor\n+    #    across replicas (achieved by `merge_call`). With\n+    #    `variable_sync_on_read_context` each replica gets their own tensors\n+    #    residing on replica's device, thus breaking the assumption.\n+    # 2. Keras c/fit creates a tf.function (a.k.a, train_function) that returns\n+    #    the metric values of the first replica. With\n+    #    `variable_sync_on_read_context` since each replica gets their own\n+    #    tensors, the metric result tensors on the non-first replicas are not in\n+    #    the return value of train_function, making TF graph optimizer prune the\n+    #    branch that computes and aggregates those metric results. As a result,\n+    #    if NCCL is used to do the aggregation, the program will hang because\n+    #    NCCL ops are only launched on the non-pruned first replica.\n+    #\n+    # We condition on strategy.extended._use_merge_call() since we know if it is\n+    # false, the program uses `jit_compile` to compile replica fn, meaning it is\n+    # not V1 training (hence #1 is okay), and no pruning will happen as\n+    # compiled functions are not inlined (hence #2 is okay).\n+\n+    if (not has_strategy or replica_context is None or\n+        not tf.distribute.get_strategy(\n+        ).extended._use_merge_call()):\n+      with tf.__internal__.distribute.variable_sync_on_read_context():\n         raw_result = result_fn(*args)\n         # Results need to be wrapped in a `tf.identity` op to ensure\n         # correct execution order.\n@@ -109,8 +136,10 @@ def result_wrapper(result_fn):\n                       (tf.Tensor, tf.Variable, float, int)):\n           result_t = tf.identity(raw_result)\n         elif isinstance(raw_result, dict):\n-        result_t = {key: tf.identity(value)\n-                    for key, value in raw_result.items()}\n+          result_t = {\n+              key: tf.identity(value)\n+              for key, value in raw_result.items()\n+          }\n         else:\n           try:\n             result_t = tf.identity(raw_result)\n\n@@ -377,6 +377,13 @@ def is_ragged(tensor):\n       (tf.RaggedTensor, tf.compat.v1.ragged.RaggedTensorValue))\n \n \n+def is_sparse(tensor):\n+  \"\"\"Returns true if `tensor` is a sparse tensor or sparse tensor value.\"\"\"\n+  return isinstance(\n+      tensor,\n+      (tf.SparseTensor, tf.compat.v1.SparseTensorValue))\n+\n+\n def is_tensor_or_variable(x):\n   return tf.is_tensor(x) or isinstance(x, tf.Variable)\n \n\n@@ -193,6 +193,23 @@ class TestIsRagged(tf.test.TestCase):\n     self.assertFalse(tf_utils.is_ragged(tensor))\n \n \n+class TestIsSparse(tf.test.TestCase):\n+\n+  def test_is_sparse_return_true_for_sparse_tensor(self):\n+    tensor = tf.SparseTensor(\n+        indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n+    self.assertTrue(tf_utils.is_sparse(tensor))\n+\n+  def test_is_sparse_return_true_for_sparse_tensor_value(self):\n+    tensor = tf.compat.v1.SparseTensorValue(\n+        indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n+    self.assertTrue(tf_utils.is_sparse(tensor))\n+\n+  def test_is_sparse_return_false_for_list(self):\n+    tensor = [1., 2., 3.]\n+    self.assertFalse(tf_utils.is_sparse(tensor))\n+\n+\n class TestIsExtensionType(tf.test.TestCase):\n \n   def test_is_extension_type_return_true_for_ragged_tensor(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
