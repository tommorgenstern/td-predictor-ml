{"custom_id": "keras#f2dcc8084fe8c0860707ed6a489774f2880582ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 237 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 16 | Complexity Δ (Sum/Max): 33/27 | Churn Δ: 237 | Churn Cumulative: 11925 | Contributors (this commit): 24 | Commits (past 90d): 16 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -236,6 +236,10 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     return distributed_training_utils.call_replica_local_fn(\n         replica_local_fn, *args, **kwargs)\n \n+  def __str__(self):\n+    args = ','.join(f'{k}={v}' for k, v in self.get_config().items())\n+    return f'{self.__class__.__name__}({args})'\n+\n   @property\n   def dtype(self):\n     return self._dtype\n@@ -279,6 +283,42 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     \"\"\"\n     raise NotImplementedError('Must be implemented in subclasses.')\n \n+  def merge_state(self, metrics):\n+    \"\"\"Merges the state from one or more metrics.\n+\n+    This method can be used by distributed systems to merge the state computed\n+    by different metric instances. Typically the state will be stored in the\n+    form of the metric's weights. For example, a tf.keras.metrics.Mean metric\n+    contains a list of two weight values: a total and a count. If there were two\n+    instances of a tf.keras.metrics.Accuracy that each independently aggregated\n+    partial state for an overall accuracy calculation, these two metric's states\n+    could be combined as follows:\n+\n+    >>> m1 = tf.keras.metrics.Accuracy()\n+    >>> _ = m1.update_state([[1], [2]], [[0], [2]])\n+\n+    >>> m2 = tf.keras.metrics.Accuracy()\n+    >>> _ = m2.update_state([[3], [4]], [[3], [4]])\n+\n+    >>> m2.merge_state([m1])\n+    >>> m2.result().numpy()\n+    0.75\n+\n+    Args:\n+      metrics: an iterable of metrics. The metrics must have compatible state.\n+\n+    Raises:\n+      ValueError: If the provided iterable does not contain metrics matching the\n+        metric's required specifications.\n+    \"\"\"\n+    assign_add_ops = []\n+    for metric in metrics:\n+      if len(self.weights) != len(metric.weights):\n+        raise ValueError(f'Metric {metric} is not compatible with {self}')\n+      for weight, weight_to_add in zip(self.weights, metric.weights):\n+        assign_add_ops.append(weight.assign_add(weight_to_add))\n+    return assign_add_ops\n+\n   @abc.abstractmethod\n   def result(self):\n     \"\"\"Computes and returns the metric value tensor.\n\n@@ -2364,5 +2364,202 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       backend.set_floatx('float32')\n \n \n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class MergeStateTest(keras_parameterized.TestCase):\n+\n+  def test_merge_state_incompatible_metrics(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                'Metric .* is not compatible with .*'):\n+      obj1 = metrics.FalsePositives()\n+      self.evaluate(tf.compat.v1.variables_initializer(obj1.variables))\n+      obj2 = metrics.Accuracy()\n+      self.evaluate(tf.compat.v1.variables_initializer(obj2.variables))\n+      self.evaluate(obj1.merge_state([obj2]))\n+\n+  def test_merge_state_accuracy(self):\n+    a_objs = []\n+    for y_true, y_pred in zip([[[1], [2]], [[3], [4]]],\n+                              [[[0], [2]], [[3], [4]]]):\n+      a_obj = metrics.Accuracy()\n+      a_objs.append(a_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+      self.evaluate(a_obj.update_state(y_true, y_pred))\n+    self.evaluate(a_objs[0].merge_state(a_objs[1:]))\n+    self.assertEqual(self.evaluate(a_objs[0].total), 3.)\n+    self.assertEqual(self.evaluate(a_objs[0].count), 4.)\n+    self.assertEqual(self.evaluate(a_objs[0].result()), 0.75)\n+\n+  def test_merge_state_false_positives(self):\n+    fp_objs = []\n+    for _ in range(4):\n+      fp_obj = metrics.FalsePositives()\n+      fp_objs.append(fp_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))\n+      y_true = np.zeros((25, 1))\n+      y_pred = np.ones((25, 1))\n+      self.evaluate(fp_obj.update_state(y_true, y_pred))\n+    self.evaluate(fp_objs[0].merge_state(fp_objs[1:]))\n+    self.assertEqual(self.evaluate(fp_objs[0].accumulator), 100.)\n+\n+  def test_merge_state_false_negatives(self):\n+    fn_objs = []\n+    for _ in range(4):\n+      fn_obj = metrics.FalseNegatives()\n+      fn_objs.append(fn_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))\n+      y_true = np.ones((25, 1))\n+      y_pred = np.zeros((25, 1))\n+      self.evaluate(fn_obj.update_state(y_true, y_pred))\n+    self.evaluate(fn_objs[0].merge_state(fn_objs[1:]))\n+    self.assertEqual(self.evaluate(fn_objs[0].accumulator), 100.)\n+\n+  def test_merge_state_true_negatives(self):\n+    tn_objs = []\n+    for _ in range(4):\n+      tn_obj = metrics.TrueNegatives()\n+      tn_objs.append(tn_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))\n+      y_true = np.zeros((25, 1))\n+      y_pred = np.zeros((25, 1))\n+      self.evaluate(tn_obj.update_state(y_true, y_pred))\n+    self.evaluate(tn_objs[0].merge_state(tn_objs[1:]))\n+    self.assertEqual(self.evaluate(tn_objs[0].accumulator), 100.)\n+\n+  def test_merge_state_true_positives(self):\n+    tp_objs = []\n+    for _ in range(4):\n+      tp_obj = metrics.TruePositives()\n+      tp_objs.append(tp_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))\n+      y_true = np.ones((25, 1))\n+      y_pred = np.ones((25, 1))\n+      self.evaluate(tp_obj.update_state(y_true, y_pred))\n+    self.evaluate(tp_objs[0].merge_state(tp_objs[1:]))\n+    self.assertEqual(self.evaluate(tp_objs[0].accumulator), 100.)\n+\n+  def test_merge_state_precision(self):\n+    p_objs = []\n+    for _ in range(5):\n+      p_obj = metrics.Precision()\n+      p_objs.append(p_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))\n+      y_true = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n+      y_pred = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n+      self.evaluate(p_obj.update_state(y_true, y_pred))\n+    self.evaluate(p_objs[0].merge_state(p_objs[1:]))\n+    self.assertEqual(self.evaluate(p_objs[0].true_positives), 50.)\n+    self.assertEqual(self.evaluate(p_objs[0].false_positives), 50.)\n+\n+  def test_merge_state_recall(self):\n+    r_objs = []\n+    for _ in range(5):\n+      r_obj = metrics.Recall()\n+      r_objs.append(r_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))\n+      y_true = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n+      y_pred = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n+      self.evaluate(r_obj.update_state(y_true, y_pred))\n+    self.evaluate(r_objs[0].merge_state(r_objs[1:]))\n+    self.assertEqual(self.evaluate(r_objs[0].true_positives), 50.)\n+    self.assertEqual(self.evaluate(r_objs[0].false_negatives), 50.)\n+\n+  def test_merge_state_sensitivity_at_specificity(self):\n+    sas_objs = []\n+    for _ in range(5):\n+      sas_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n+      sas_objs.append(sas_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n+      y_true = np.concatenate((np.ones((5, 1)), np.zeros((5, 1)), np.ones(\n+          (5, 1)), np.zeros((5, 1))))\n+      y_pred = np.concatenate((np.ones((5, 1)), np.zeros(\n+          (5, 1)), np.zeros((5, 1)), np.ones((5, 1))))\n+      self.evaluate(sas_obj.update_state(y_true, y_pred))\n+    self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n+    self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.)\n+\n+  def test_merge_state_specificity_at_sensitivity(self):\n+    sas_objs = []\n+    for _ in range(5):\n+      sas_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n+      sas_objs.append(sas_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n+      y_true = np.concatenate((np.ones((5, 1)), np.zeros((5, 1)), np.ones(\n+          (5, 1)), np.zeros((5, 1))))\n+      y_pred = np.concatenate((np.ones((5, 1)), np.zeros(\n+          (5, 1)), np.zeros((5, 1)), np.ones((5, 1))))\n+      self.evaluate(sas_obj.update_state(y_true, y_pred))\n+    self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n+    self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.)\n+    self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.)\n+\n+  def test_merge_state_precision_at_recall(self):\n+    par_objs = []\n+    for _ in range(5):\n+      par_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n+      par_objs.append(par_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(par_obj.variables))\n+      y_true = np.concatenate((np.ones((5, 1)), np.zeros((5, 1)), np.ones(\n+          (5, 1)), np.zeros((5, 1))))\n+      y_pred = np.concatenate((np.ones((5, 1)), np.zeros(\n+          (5, 1)), np.zeros((5, 1)), np.ones((5, 1))))\n+      self.evaluate(par_obj.update_state(y_true, y_pred))\n+    self.evaluate(par_objs[0].merge_state(par_objs[1:]))\n+    self.assertEqual(self.evaluate(par_objs[0].true_positives), 25.)\n+    self.assertEqual(self.evaluate(par_objs[0].false_positives), 25.)\n+    self.assertEqual(self.evaluate(par_objs[0].false_negatives), 25.)\n+    self.assertEqual(self.evaluate(par_objs[0].true_negatives), 25.)\n+\n+  def test_merge_state_recall_at_precision(self):\n+    rap_objs = []\n+    for _ in range(5):\n+      rap_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n+      rap_objs.append(rap_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(rap_obj.variables))\n+      y_true = np.concatenate((np.ones((5, 1)), np.zeros((5, 1)), np.ones(\n+          (5, 1)), np.zeros((5, 1))))\n+      y_pred = np.concatenate((np.ones((5, 1)), np.zeros(\n+          (5, 1)), np.zeros((5, 1)), np.ones((5, 1))))\n+      self.evaluate(rap_obj.update_state(y_true, y_pred))\n+    self.evaluate(rap_objs[0].merge_state(rap_objs[1:]))\n+    self.assertEqual(self.evaluate(rap_objs[0].true_positives), 25.)\n+    self.assertEqual(self.evaluate(rap_objs[0].false_positives), 25.)\n+    self.assertEqual(self.evaluate(rap_objs[0].false_negatives), 25.)\n+    self.assertEqual(self.evaluate(rap_objs[0].true_negatives), 25.)\n+\n+  def test_merge_state_auc(self):\n+    auc_objs = []\n+    for _ in range(5):\n+      auc_obj = metrics.AUC(num_thresholds=3)\n+      auc_objs.append(auc_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n+      y_true = np.concatenate((np.ones((5, 1)), np.zeros((5, 1)), np.ones(\n+          (5, 1)), np.zeros((5, 1))))\n+      y_pred = np.concatenate((np.ones((5, 1)), np.zeros(\n+          (5, 1)), np.zeros((5, 1)), np.ones((5, 1))))\n+      self.evaluate(auc_obj.update_state(y_true, y_pred))\n+    self.evaluate(auc_objs[0].merge_state(auc_objs[1:]))\n+    self.assertEqual(self.evaluate(auc_objs[0].true_positives[1]), 25.)\n+    self.assertEqual(self.evaluate(auc_objs[0].false_positives[1]), 25.)\n+    self.assertEqual(self.evaluate(auc_objs[0].false_negatives[1]), 25.)\n+    self.assertEqual(self.evaluate(auc_objs[0].true_negatives[1]), 25.)\n+\n+  def test_merge_state_mean_iou(self):\n+    m_objs = []\n+    for y_true, y_pred in zip([[0], [1], [1], [1]],\n+                              [[0.5], [1.0], [1.0], [1.0]]):\n+      m_obj = metrics.MeanIoU(num_classes=2)\n+      m_objs.append(m_obj)\n+      self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+      self.evaluate(m_obj.update_state(y_true, y_pred))\n+    self.evaluate(m_objs[0].merge_state(m_objs[1:]))\n+    self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[0], [1, 0], 1e-1)\n+    self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[1], [0, 3], 1e-1)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c5fe22a71992da7d6dff801c76cbe10dab7a5e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 783 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -159,13 +159,13 @@ def piecewise_constant(x, boundaries, values, name=None):\n         boundaries[i] = b\n       else:\n         raise ValueError(\n-            \"Boundaries (%s) must have the same dtype as x (%s).\" %\n-            (b.dtype.base_dtype, x_recomp.dtype.base_dtype))\n+            f\"`boundaries` ({b.dtype.base_dtype}) must have the same dtype as \"\n+            f\"x ({x_recomp.dtype.base_dtype}).\")\n   for v in values[1:]:\n     if v.dtype.base_dtype != values[0].dtype.base_dtype:\n       raise ValueError(\n-          \"Values must have elements all with the same dtype (%s vs %s).\" %\n-          (values[0].dtype.base_dtype, v.dtype.base_dtype))\n+          f\"`values` must have elements all with the same dtype \"\n+          f\"({values[0].dtype.base_dtype} vs {v.dtype.base_dtype}).\")\n   decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n       boundaries, values, name=name)\n   if not tf.executing_eagerly():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb4eab91b1d4922ff4fea6bc6eb57a4418d9b6a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 930 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -136,7 +136,8 @@ class RMSprop(optimizer_v2.OptimizerV2):\n     if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:\n       self._momentum = True\n     if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n-      raise ValueError(\"`momentum` must be between [0, 1].\")\n+      raise ValueError(f\"`momentum` must be between [0, 1]. Received: \"\n+                       f\"momentum={momentum} (of type {type(momentum)}).\")\n     self._set_hyper(\"momentum\", momentum)\n \n     self.epsilon = epsilon or backend_config.epsilon()\n\n@@ -534,6 +534,14 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(\n         self.evaluate(opt.variables()[0]), self.evaluate(opt.iterations))\n \n+  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  def testMomentumProperValue(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                r\"`momentum` must be between \\[0, 1\\]. \"\n+                                r\"Received: momentum=2.5 \\(of type <class \"\n+                                r\"\\'float\\'>\\).\"):\n+      rmsprop.RMSprop(1., momentum=2.5, centered=False)\n+\n \n @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n class SlotColocationTest(tf.test.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09e9571156ab0e50316ecabd8c4204f888290022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 258 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,7 +66,8 @@ def load_data(label_mode='fine'):\n   ```\n   \"\"\"\n   if label_mode not in ['fine', 'coarse']:\n-    raise ValueError('`label_mode` must be one of `\"fine\"`, `\"coarse\"`.')\n+    raise ValueError('`label_mode` must be one of `\"fine\"`, `\"coarse\"`. '\n+                     f'Received: label_mode={label_mode}.')\n \n   dirname = 'cifar-100-python'\n   origin = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce7d8ca13e5580eff0673167a85d759a5452c69e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 730 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -97,7 +97,7 @@ def load_data(path='imdb.npz',\n                     'has been renamed `num_words`.')\n     num_words = kwargs.pop('nb_words')\n   if kwargs:\n-    raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n+    raise TypeError(f'Unrecognized keyword arguments: {str(kwargs)}.')\n \n   origin_folder = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n   path = get_file(\n@@ -131,9 +131,8 @@ def load_data(path='imdb.npz',\n     x_train, labels_train = _remove_long_seq(maxlen, x_train, labels_train)\n     x_test, labels_test = _remove_long_seq(maxlen, x_test, labels_test)\n     if not x_train or not x_test:\n-      raise ValueError('After filtering for sequences shorter than maxlen=' +\n-                       str(maxlen) + ', no sequence was kept. '\n-                       'Increase maxlen.')\n+      raise ValueError('After filtering for sequences shorter than maxlen='\n+                       f'{str(maxlen)}, no sequence was kept. Increase maxlen.')\n \n   xs = x_train + x_test\n   labels = np.concatenate([labels_train, labels_test])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b1b53150f31fa7fdb0e4a723425602c540a7eb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 353 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -111,23 +111,25 @@ class Ftrl(optimizer_v2.OptimizerV2):\n \n     if initial_accumulator_value < 0.0:\n       raise ValueError(\n-          'initial_accumulator_value %f needs to be positive or zero' %\n-          initial_accumulator_value)\n+          '`initial_accumulator_value` needs to be positive or zero. Received: '\n+          f'initial_accumulator_value={initial_accumulator_value}.')\n     if learning_rate_power > 0.0:\n-      raise ValueError('learning_rate_power %f needs to be negative or zero' %\n-                       learning_rate_power)\n+      raise ValueError(\n+          '`learning_rate_power` needs to be negative or zero. Received: '\n+          f'learning_rate_power={learning_rate_power}.')\n     if l1_regularization_strength < 0.0:\n       raise ValueError(\n-          'l1_regularization_strength %f needs to be positive or zero' %\n-          l1_regularization_strength)\n+          '`l1_regularization_strength` needs to be positive or zero. '\n+          f'Received: l1_regularization_strength={l1_regularization_strength}.')\n     if l2_regularization_strength < 0.0:\n       raise ValueError(\n-          'l2_regularization_strength %f needs to be positive or zero' %\n-          l2_regularization_strength)\n+          '`l2_regularization_strength` needs to be positive or zero. '\n+          f'Received: l2_regularization_strength={l2_regularization_strength}.')\n     if l2_shrinkage_regularization_strength < 0.0:\n       raise ValueError(\n-          'l2_shrinkage_regularization_strength %f needs to be positive'\n-          ' or zero' % l2_shrinkage_regularization_strength)\n+          '`l2_shrinkage_regularization_strength` needs to be positive or '\n+          'zero. Received: l2_shrinkage_regularization_strength'\n+          f'={l2_shrinkage_regularization_strength}.')\n \n     self._set_hyper('learning_rate', learning_rate)\n     self._set_hyper('decay', self._initial_decay)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b001bd2b5cffe411feb9a770bd10db4643f725b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1445 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -28,14 +28,14 @@ from tensorflow.python.util.tf_export import keras_export\n def _check_penalty_number(x):\n   \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n   if not isinstance(x, (float, int)):\n-    raise ValueError(('Value: {} is not a valid regularization penalty number, '\n-                      'expected an int or float value').format(x))\n+    raise ValueError(\n+        f'Value: {x} is not a valid regularization penalty number, '\n+        'expected an int or float value')\n \n   if math.isinf(x) or math.isnan(x):\n     raise ValueError(\n-        ('Value: {} is not a valid regularization penalty number, '\n-         'a positive/negative infinity or NaN is not a property value'\n-        ).format(x))\n+        f'Value: {x} is not a valid regularization penalty number, '\n+        'an infinity nubmer or NaN are not valid value')\n \n \n def _none_to_default(inputs, default):\n@@ -204,7 +204,7 @@ class Regularizer:\n     Returns:\n         Python dictionary.\n     \"\"\"\n-    raise NotImplementedError(str(self) + ' does not implement get_config()')\n+    raise NotImplementedError(f'{self} does not implement get_config()')\n \n \n @keras_export('keras.regularizers.L1L2')\n@@ -272,7 +272,7 @@ class L1(Regularizer):\n   def __init__(self, l1=0.01, **kwargs):  # pylint: disable=redefined-outer-name\n     l1 = kwargs.pop('l', l1)  # Backwards compatibility\n     if kwargs:\n-      raise TypeError('Argument(s) not recognized: %s' % (kwargs,))\n+      raise TypeError(f'Argument(s) not recognized: {kwargs}')\n \n     l1 = 0.01 if l1 is None else l1\n     _check_penalty_number(l1)\n@@ -306,7 +306,7 @@ class L2(Regularizer):\n   def __init__(self, l2=0.01, **kwargs):  # pylint: disable=redefined-outer-name\n     l2 = kwargs.pop('l', l2)  # Backwards compatibility\n     if kwargs:\n-      raise TypeError('Argument(s) not recognized: %s' % (kwargs,))\n+      raise TypeError(f'Argument(s) not recognized: {kwargs}')\n \n     l2 = 0.01 if l2 is None else l2\n     _check_penalty_number(l2)\n@@ -376,4 +376,4 @@ def get(identifier):\n     return identifier\n   else:\n     raise ValueError(\n-        'Could not interpret regularizer identifier: {}'.format(identifier))\n+        f'Could not interpret regularizer identifier: {identifier}')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c04e5dfd9a23657953ec1e62e7e4c0042c8461c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 20 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 1691 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 0.5882352941176471\n\nDIFF:\n@@ -347,7 +347,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     for k in kwargs:\n       if k not in allowed_kwargs:\n         raise TypeError(\"Unexpected keyword argument \"\n-                        \"passed to optimizer: \" + str(k))\n+                        f\"passed to optimizer: {str(k)}. Allowed kwargs are \"\n+                        f\"{allowed_kwargs}.\")\n       # checks that all keyword arguments are non-negative.\n       if kwargs[k] is not None and kwargs[k] < 0:\n         raise ValueError(\"Expected {} >= 0, received: {}\".format(k, kwargs[k]))\n@@ -374,7 +375,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n     decay = kwargs.pop(\"decay\", 0.0)\n     if decay < 0.:\n-      raise ValueError(\"decay cannot be less than 0: {}\".format(decay))\n+      raise ValueError(\"decay cannot be less than 0. \"\n+                       \"Received: decay={}.\".format(decay))\n     self._initial_decay = decay\n \n     self._hypers_created = False\n@@ -395,8 +397,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     self.clipnorm = kwargs.pop(\"clipnorm\", None)\n     self.global_clipnorm = kwargs.pop(\"global_clipnorm\", None)\n     if self.clipnorm is not None and self.global_clipnorm is not None:\n-      raise ValueError(\"Cannot accept both `clipnorm` and `global_clipnorm`, \"\n-                       \"passed `clipnorm` {}, `global_clipnorm` {}\".format(\n+      raise ValueError(\"Cannot accept both `clipnorm` and `global_clipnorm`. \"\n+                       \"Received: `clipnorm`={}, `global_clipnorm`={}.\".format(\n                            self.clipnorm, self.global_clipnorm))\n     self.clipvalue = kwargs.pop(\"clipvalue\", None)\n \n@@ -415,7 +417,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if val is not None and self.gradient_transformers:\n       raise ValueError(\"`clipnorm` cannot be set when `gradient_transformers` \"\n                        \"is set. Instead, use the `gradient_transformers` to \"\n-                       \"specify clipping and other transformations.\")\n+                       \"specify clipping and other transformations. Received: \"\n+                       f\"val={val}, \"\n+                       f\"gradient_transformers={self.gradient_transformers}.\")\n     self._clipnorm = val\n     self._clipnorm_fn = optimizer_utils.make_gradient_clipnorm_fn(\n         self._clipnorm)\n@@ -423,9 +427,12 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   @global_clipnorm.setter\n   def global_clipnorm(self, val):\n     if val is not None and self.gradient_transformers:\n-      raise ValueError(\"`clipnorm` cannot be set when `gradient_transformers` \"\n+      raise ValueError(\"`global_clipnorm` cannot be set when \"\n+                       \"`gradient_transformers` \"\n                        \"is set. Instead, use the `gradient_transformers` to \"\n-                       \"specify clipping and other transformations.\")\n+                       \"specify clipping and other transformations. Received: \"\n+                       f\"val={val}, \"\n+                       f\"gradient_transformers={self.gradient_transformers}.\")\n     self._global_clipnorm = val\n     self._global_clipnorm_fn = optimizer_utils.make_global_gradient_clipnorm_fn(\n         self._global_clipnorm)\n@@ -440,7 +447,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if val is not None and self.gradient_transformers:\n       raise ValueError(\"`clipvalue` cannot be set when `gradient_transformers` \"\n                        \"is set. Instead, use the `gradient_transformers` to \"\n-                       \"specify clipping and other transformations.\")\n+                       \"specify clipping and other transformations. Received: \"\n+                       f\"val={val}, \"\n+                       f\"gradient_transformers={self.gradient_transformers}.\")\n     self._clipvalue = val\n     self._clipvalue_fn = optimizer_utils.make_gradient_clipvalue_fn(\n         self._clipvalue)\n@@ -553,7 +562,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     \"\"\"\n     # TODO(joshl): Test that we handle weight decay in a reasonable way.\n     if not callable(loss) and tape is None:\n-      raise ValueError(\"`tape` is required when a `Tensor` loss is passed.\")\n+      raise ValueError(\"`tape` is required when a `Tensor` loss is passed. \"\n+                       f\"Received: loss={loss}, tape={tape}.\")\n     tape = tape if tape is not None else tf.GradientTape()\n \n     if callable(loss):\n@@ -636,7 +646,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         raise RuntimeError(\n             \"`apply_gradients() cannot be called in cross-replica context. \"\n             \"Use `tf.distribute.Strategy.run` to enter replica \"\n-            \"context.\")\n+            \"context. For more information, please see the docstring of \"\n+            \"`tf.distribute.get_replica_context`.\")\n \n       strategy = tf.distribute.get_strategy()\n       if (not experimental_aggregate_gradients and strategy and\n@@ -647,7 +658,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                       tf.compat.v1.distribute.experimental.CentralStorageStrategy))):\n         raise NotImplementedError(\n             \"`experimental_aggregate_gradients=False is not supported for \"\n-            \"ParameterServerStrategy and CentralStorageStrategy\")\n+            \"ParameterServerStrategy and CentralStorageStrategy. Used: \"\n+            f\"strategy={strategy}.\")\n \n       apply_state = self._prepare(var_list)\n       if experimental_aggregate_gradients:\n@@ -672,13 +684,15 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     def apply_grad_to_update_var(var, grad):\n       \"\"\"Apply gradient to variable.\"\"\"\n       if isinstance(var, tf.Tensor):\n-        raise NotImplementedError(\"Trying to update a Tensor \", var)\n+        raise NotImplementedError(\n+            f\"Updating a `Tensor` is not implemented. Received: var={var}.\")\n \n       apply_kwargs = {}\n       if isinstance(grad, tf.IndexedSlices):\n         if var.constraint is not None:\n           raise RuntimeError(\n-              \"Cannot use a constraint function on a sparse variable.\")\n+              \"Cannot use a constraint function on a sparse variable. \"\n+              f\"Received: grad={grad}, var.constraint={var.constraint}.\")\n         if \"apply_state\" in self._sparse_apply_args:\n           apply_kwargs[\"apply_state\"] = apply_state\n         return self._resource_apply_sparse_duplicate_indices(\n@@ -894,7 +908,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n               \"tf.distribute.Strategy ({}), which is different from the scope \"\n               \"used for the original variable ({}). Make sure the slot \"\n               \"variables are created under the same strategy scope. This may \"\n-              \"happen if you're restoring from a checkpoint outside the scope\"\n+              \"happen if you're restoring from a checkpoint outside the scope.\"\n               .format(strategy, var))\n \n         with strategy.extended.colocate_vars_with(var):\n@@ -985,7 +999,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   def iterations(self, variable):\n     if self._iterations is not None:\n       raise RuntimeError(\"Cannot set `iterations` to a new Variable after \"\n-                         \"the Optimizer weights have been created\")\n+                         \"the Optimizer weights have been created. Here it is \"\n+                         f\"attempting to set `iterations` to {variable}.\")\n     self._iterations = variable\n     self._weights.append(self._iterations)\n \n@@ -1130,19 +1145,19 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     params = self.weights\n     if len(params) != len(weights):\n       raise ValueError(\n-          \"You called `set_weights(weights)` on optimizer \" + self._name +\n-          \" with a  weight list of length \" + str(len(weights)) +\n-          \", but the optimizer was expecting \" + str(len(params)) +\n-          \" weights. Provided weights: \" + str(weights)[:50] + \"...\")\n+          \"You called `set_weights(weights)` on optimizer {self._name} \"\n+          f\"with a  weight list of length {str(len(weights))}, \"\n+          f\"but the optimizer was expecting {str(len(params))} \"\n+          f\"weights. Provided weights: {str(weights)[:50]}...\")\n     if not params:\n       return\n     weight_value_tuples = []\n     param_values = backend.batch_get_value(params)\n     for pv, p, w in zip(param_values, params, weights):\n       if pv.shape != w.shape:\n-        raise ValueError(\"Optimizer weight shape \" + str(pv.shape) +\n+        raise ValueError(f\"Optimizer weight shape {str(pv.shape)} \"\n                          \"not compatible with \"\n-                         \"provided weight shape \" + str(w.shape))\n+                         f\"provided weight shape {str(w.shape)}.\")\n       weight_value_tuples.append((p, w))\n     backend.batch_set_value(weight_value_tuples)\n \n@@ -1209,8 +1224,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     for t in tensors:\n       dtype = t.dtype.base_dtype\n       if dtype not in valid_dtypes:\n-        raise ValueError(\"Invalid type %r for %s, expected: %s.\" %\n-                         (dtype, t.name, [v for v in valid_dtypes]))\n+        raise ValueError(\"Invalid type {} for {}, expected: {}.\".format(\n+            dtype, t.name, [v for v in valid_dtypes]))\n \n   def _valid_dtypes(self):\n     \"\"\"Valid types for loss, variables and gradients.\n@@ -1238,7 +1253,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     Returns:\n       An `Operation` which updates the value of the variable.\n     \"\"\"\n-    raise NotImplementedError(\"Must be implemented in subclasses.\")\n+    raise NotImplementedError(\"`_resource_apply_dense` must be implemented in \"\n+                              \"subclasses.\")\n \n   def _resource_apply_sparse_duplicate_indices(self, grad, handle, indices,\n                                                **kwargs):\n@@ -1287,7 +1303,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     Returns:\n       An `Operation` which updates the value of the variable.\n     \"\"\"\n-    raise NotImplementedError(\"Must be implemented in subclasses.\")\n+    raise NotImplementedError(\"`_resource_apply_sparse` Must be implemented in \"\n+                              \"subclasses.\")\n \n   def _resource_scatter_add(self, x, i, v):\n     with tf.control_dependencies([\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f30baf31dc84c0f414fe73bfae4f65d248db357e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 931 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -85,10 +85,15 @@ def get_strategy():\n     `tf.distribute.Strategy` instance.\n   \"\"\"\n   cls = FLAGS.keras_distribute_strategy_class\n+  accepted_strats = {\n+      'tpu', 'multi_worker_mirrored', 'mirrored',\n+      'parameter_server', 'one_device'}\n   if cls == 'tpu':\n     tpu_addr = FLAGS.keras_distribute_strategy_tpu_addr\n     if not tpu_addr:\n-      raise ValueError('Must set flag: keras_distribute_strategy_tpu_addr')\n+      raise ValueError(\n+          'When using a TPU strategy, you must set the flag '\n+          '`keras_distribute_strategy_tpu_addr` (TPU address).')\n     cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n         tpu=tpu_addr)\n     tf.config.experimental_connect_to_cluster(cluster_resolver)\n@@ -105,5 +110,8 @@ def get_strategy():\n   elif cls == 'one_device':\n     strategy = tf.distribute.OneDeviceStrategy('/gpu:0')\n   else:\n-    raise ValueError('Unknown strategy: {}'.format(cls))\n+    raise ValueError(\n+        'Unknown distribution strategy flag. Received: '\n+        f'keras_distribute_strategy_class={cls}. '\n+        f'It should be one of {accepted_strats}')\n   return strategy\n\n@@ -205,20 +205,21 @@ class SidecarEvaluator:\n         # TensorSliceReader constructor.\n         # TODO(rchao): Remove this except block once b/150954027 is resolved.\n         logging.info(\n-            'SidecarEvaluator has an error loading '\n-            'checkpoint: %s. Retrying. Error: %s: %s', latest_checkpoint,\n-            e.__class__.__name__, e)\n+            'SidecarEvaluator encountered an error when loading the checkpoint '\n+            f'at {latest_checkpoint}. Retrying. '\n+            f'Error: {e.__class__.__name__}: {e}')\n         continue\n \n       if self._iterations.numpy() == _ITERATIONS_UNINITIALIZED:\n         raise RuntimeError(\n-            '`iterations` cannot be loaded from the '\n-            'checkpoint file. Please ensure `iterations` is '\n-            'tracked in the `checkpoint` saved by the coordinator.')\n+            'Variable `iterations` cannot be loaded from the '\n+            f'checkpoint file at {self.checkpoint_dir}. '\n+            'Please ensure `iterations` is '\n+            'included in the checkpoint saved during training.')\n \n       logging.info(\n           'Evaluation starts: Model weights loaded from latest '\n-          'checkpoint file: %s.', latest_checkpoint)\n+          f'checkpoint file {latest_checkpoint}')\n \n       self.model.evaluate(\n           self.data, steps=self.steps, callbacks=self.callbacks, verbose=2)\n\n@@ -152,8 +152,9 @@ class SerializedAttributes:\n     elif isinstance(obj, base_layer.Layer):\n       return LayerAttributes()\n     else:\n-      raise TypeError('Internal error during serialization: Expected Keras '\n-                      'Layer object, got {} of type {}'.format(obj, type(obj)))\n+      raise TypeError('Internal error during serialization. Expected Keras '\n+                      f'Layer object. Received: {obj} '\n+                      f'(of type {type(obj)})')\n \n   def __init__(self):\n     self._object_dict = {}\n@@ -200,8 +201,9 @@ class SerializedAttributes:\n                             tf.types.experimental.ConcreteFunction,\n                             save_impl.LayerCall))):\n           raise ValueError(\n-              'Function dictionary contained a non-function object: {} (for key'\n-              ' {})'.format(function_dict[key], key))\n+              'The tf.function dictionary contained a non-function object: '\n+              f'{function_dict[key]} (for key {key}). Only tf.function '\n+              'instances or ConcreteFunction instances should be passed.')\n         fn = function_dict[key]\n         self._function_dict[key] = fn\n \n@@ -209,8 +211,8 @@ class SerializedAttributes:\n         tf_fn = fn.wrapped_call if isinstance(fn, save_impl.LayerCall) else fn\n         setattr(self._keras_trackable, key, tf_fn)\n       else:\n-        raise ValueError('Function {} missing from serialized function dict.'\n-                         .format(key))\n+        raise ValueError(\n+            f'Function {key} missing from serialized tf.function dictionary.')\n     return self.functions\n \n   def set_and_validate_objects(self, object_dict):\n@@ -219,13 +221,14 @@ class SerializedAttributes:\n       if key in object_dict:\n         if not isinstance(object_dict[key], tf.__internal__.tracking.Trackable):\n           raise ValueError(\n-              'Object dictionary contained a non-trackable object: {} (for key'\n-              ' {})'.format(object_dict[key], key))\n+              'The object dictionary contained a non-trackable object: '\n+              f'{object_dict[key]} (for key {key}). Only trackable objects are '\n+              f'allowed, such as Keras layers/models or tf.Module instances.')\n         self._object_dict[key] = object_dict[key]\n         setattr(self._keras_trackable, key, object_dict[key])\n       else:\n         raise ValueError(\n-            'Object {} missing from serialized object dict.'.format(key))\n+            f'Object {key} missing from serialized object dictionary.')\n     return self.checkpointable_objects\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf14e59462b6e0a7a57cef28cde0ef19f5939672", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 3944 | Contributors (this commit): 39 | Commits (past 90d): 6 | Contributors (cumulative): 39 | DMM Complexity: 0.0\n\nDIFF:\n@@ -376,12 +376,12 @@ def register_keras_serializable(package='Custom', name=None):\n \n     if registered_name in _GLOBAL_CUSTOM_OBJECTS:\n       raise ValueError(\n-          '%s has already been registered to %s' %\n-          (registered_name, _GLOBAL_CUSTOM_OBJECTS[registered_name]))\n+          f'{registered_name} has already been registered to '\n+          f'{_GLOBAL_CUSTOM_OBJECTS[registered_name]}')\n \n     if arg in _GLOBAL_CUSTOM_NAMES:\n-      raise ValueError('%s has already been registered to %s' %\n-                       (arg, _GLOBAL_CUSTOM_NAMES[arg]))\n+      raise ValueError(\n+          f'{arg} has already been registered to {_GLOBAL_CUSTOM_NAMES[arg]}')\n     _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n     _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n \n@@ -527,7 +527,8 @@ def serialize_keras_object(instance):\n         name, serialization_config, instance)\n   if hasattr(instance, '__name__'):\n     return get_registered_name(instance)\n-  raise ValueError('Cannot serialize', instance)\n+  raise ValueError(f'Cannot serialize {instance} since it doesn\\'t implement '\n+                   '`get_config()`, and also doesn\\t have `__name__`')\n \n \n def get_custom_objects_by_name(item, custom_objects=None):\n@@ -548,17 +549,18 @@ def class_and_config_for_serialized_keras_object(\n   if (not isinstance(config, dict)\n       or 'class_name' not in config\n       or 'config' not in config):\n-    raise ValueError('Improper config format: ' + str(config))\n+    raise ValueError(\n+        f'Improper config format for {config}. '\n+        'Expecting python dict contains `class_name` and `config` as keys')\n \n   class_name = config['class_name']\n   cls = get_registered_object(class_name, custom_objects, module_objects)\n   if cls is None:\n     raise ValueError(\n-        'Unknown {}: {}. Please ensure this object is '\n-        'passed to the `custom_objects` argument. See '\n+        f'Unknown {printable_module_name}: {class_name}. Please ensure this '\n+        'object is passed to the `custom_objects` argument. See '\n         'https://www.tensorflow.org/guide/keras/save_and_serialize'\n-        '#registering_the_custom_object for details.'\n-        .format(printable_module_name, class_name))\n+        '#registering_the_custom_object for details.')\n \n   cls_config = config['config']\n   # Check if `cls_config` is a list. If it is a list, return the class and the\n@@ -702,11 +704,10 @@ def deserialize_keras_object(identifier,\n       obj = module_objects.get(object_name)\n       if obj is None:\n         raise ValueError(\n-            'Unknown {}: {}. Please ensure this object is '\n-            'passed to the `custom_objects` argument. See '\n+            f'Unknown {printable_module_name}: {object_name}. Please ensure '\n+            'this object is passed to the `custom_objects` argument. See '\n             'https://www.tensorflow.org/guide/keras/save_and_serialize'\n-            '#registering_the_custom_object for details.'\n-            .format(printable_module_name, object_name))\n+            '#registering_the_custom_object for details.')\n \n     # Classes passed by name are instantiated with no args, functions are\n     # returned as-is.\n@@ -717,8 +718,8 @@ def deserialize_keras_object(identifier,\n     # If a function has already been deserialized, return as is.\n     return identifier\n   else:\n-    raise ValueError('Could not interpret serialized %s: %s' %\n-                     (printable_module_name, identifier))\n+    raise ValueError(\n+        f'Could not interpret serialized {printable_module_name}: {identifier}')\n \n \n def func_dump(func):\n@@ -1093,7 +1094,7 @@ def slice_arrays(arrays, start=None, stop=None):\n     return [None]\n   if isinstance(start, list) and stop is not None:\n     raise ValueError('The stop argument has to be None if the value of start '\n-                     'is a list.')\n+                     f'is a list. Received start={start}, stop={stop}')\n   elif isinstance(arrays, list):\n     if hasattr(start, '__len__'):\n       # hdf5 datasets only support list objects as indices\n@@ -1153,9 +1154,9 @@ def is_all_none(structure):\n def check_for_unexpected_keys(name, input_dict, expected_values):\n   unknown = set(input_dict.keys()).difference(expected_values)\n   if unknown:\n-    raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n-                     'following keys: {}'.format(name, list(unknown),\n-                                                 expected_values))\n+    raise ValueError(\n+        f'Unknown entries in {name} dictionary: {list(unknown)}. Only expected '\n+        f'following keys: {expected_values}')\n \n \n def validate_kwargs(kwargs,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e19c5069ae4c10ef403f8d22a64a460f78a80728", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 752 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -103,7 +103,7 @@ def load_data(path='reuters.npz',\n                     'has been renamed `num_words`.')\n     num_words = kwargs.pop('nb_words')\n   if kwargs:\n-    raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n+    raise TypeError(f'Unrecognized keyword arguments: {str(kwargs)}')\n \n   origin_folder = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n   path = get_file(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8299d01cfb529c69521785179a992f0980beab92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 111 | Churn Cumulative: 387 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -60,6 +60,113 @@ class RandomNormal(tf.compat.v1.random_normal_initializer):\n                   'keras.initializers.random_uniform',\n                   'keras.initializers.uniform'])\n class RandomUniform(tf.compat.v1.random_uniform_initializer):\n+  \"\"\"Initializer that generates tensors with a uniform distribution.\n+\n+  Args:\n+    minval: A python scalar or a scalar tensor. Lower bound of the range of\n+      random values to generate.\n+    maxval: A python scalar or a scalar tensor. Upper bound of the range of\n+      random values to generate.  Defaults to 1 for float types.\n+    seed: A Python integer. Used to create random seeds. See\n+      `tf.compat.v1.set_random_seed` for behavior.\n+    dtype: Default data type, used if no `dtype` argument is provided when\n+      calling the initializer.\n+\n+  @compatibility(TF2)\n+  Although it is a legacy `compat.v1` api,\n+  `tf.compat.v1.keras.initializers.RandomUniform` is compatible with eager\n+  execution and `tf.function`.\n+\n+  To switch to native TF2, switch to using\n+  `tf.keras.initializers.RandomUniform` (not from `compat.v1`) and\n+  if you need to change the default dtype use\n+  `tf.keras.backend.set_floatx(float_dtype)`\n+  or pass the dtype when calling the initializer, rather than passing it\n+  when constructing the initializer.\n+\n+  Random seed behavior:\n+\n+  Also be aware that if you pass a seed to the TF2 initializer\n+  API it will reuse that same seed for every single initialization\n+  (unlike the TF1 intializer)\n+\n+  #### Structural Mapping to Native TF2\n+\n+  Before:\n+\n+  ```python\n+\n+  initializer = tf.compat.v1.keras.initializers.RandomUniform(\n+    minval=minval,\n+    maxval=maxval,\n+    seed=seed,\n+    dtype=dtype)\n+\n+  weight_one = tf.Variable(initializer(shape_one))\n+  weight_two = tf.Variable(initializer(shape_two))\n+  ```\n+\n+  After:\n+\n+  ```python\n+  initializer = tf.keras.initializers.RandomUniform(\n+    minval=minval,\n+    maxval=maxval,\n+    # seed=seed,  # Setting a seed in the native TF2 API\n+                  # causes it to produce the same initializations\n+                  # across multiple calls of the same initializer.\n+    )\n+\n+  weight_one = tf.Variable(initializer(shape_one, dtype=dtype))\n+  weight_two = tf.Variable(initializer(shape_two, dtype=dtype))\n+  ```\n+\n+  #### How to Map Arguments\n+\n+  | TF1 Arg Name      | TF2 Arg Name    | Note                       |\n+  | :---------------- | :-------------- | :------------------------- |\n+  | `minval`            | `minval`          | No change to defaults |\n+  | `maxval`          | `maxval`        | No change to defaults |\n+  | `seed`            | `seed`          | Different random number generation |\n+  :                    :        : semantics (to change in a :\n+  :                    :        : future version). If set, the TF2 version :\n+  :                    :        : will use stateless random number :\n+  :                    :        : generation which will produce the exact :\n+  :                    :        : same initialization even across multiple :\n+  :                    :        : calls of the initializer instance. the :\n+  :                    :        : `compat.v1` version will generate new :\n+  :                    :        : initializations each time. Do not set :\n+  :                    :        : a seed if you need different          :\n+  :                    :        : initializations each time. Instead    :\n+  :                    :        : either set a global tf seed with\n+  :                    :        : `tf.random.set_seed` if you need :\n+  :                    :        : determinism, or initialize each weight :\n+  :                    :        : with a separate initializer instance  :\n+  :                    :        : and a different seed.                 :\n+  | `dtype`           | `dtype`  | The TF2 native api only takes it  |\n+  :                   :      : as a `__call__` arg, not a constructor arg. :\n+  | `partition_info`  | -    |  (`__call__` arg in TF1) Not supported      |\n+\n+  #### Example of fixed-seed behavior differences\n+\n+  `compat.v1` Fixed seed behavior:\n+\n+  >>> initializer = tf.compat.v1.keras.initializers.RandomUniform(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n+\n+  After:\n+\n+  >>> initializer = tf.keras.initializers.RandomUniform(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+\n+  @end_compatibility\n+  \"\"\"\n \n   def __init__(self, minval=-0.05, maxval=0.05, seed=None,\n                dtype=tf.float32):\n@@ -100,7 +207,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n   when constructing the initializer.\n \n   Random seed behavior:\n-  Also be aware that as of TF 2.6, if you pass a seed to the TF2 initializer\n+  Also be aware that if you pass a seed to the TF2 initializer\n   API it will reuse that same seed for every single initialization\n   (unlike the TF1 intializer)\n \n@@ -122,8 +229,6 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n   After:\n \n   ```python\n-  tf.random.set_seed(...) # set determinism globally if needed\n-\n   initializer = tf.keras.initializers.TruncatedNormal(\n     mean=mean,\n     # seed=seed,  # Setting a seed in the native TF2 API\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
