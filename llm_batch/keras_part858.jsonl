{"custom_id": "keras#57c35892ed50bb17726fb070a13c6f797fb89c42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 397 | Lines Deleted: 18 | Files Changed: 10 | Hunks: 17 | Methods Changed: 16 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 415 | Churn Cumulative: 2218 | Contributors (this commit): 17 | Commits (past 90d): 32 | Contributors (cumulative): 56 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from keras.src.losses.losses import BinaryFocalCrossentropy\n from keras.src.losses.losses import CategoricalCrossentropy\n from keras.src.losses.losses import CategoricalFocalCrossentropy\n from keras.src.losses.losses import CategoricalHinge\n+from keras.src.losses.losses import Circle\n from keras.src.losses.losses import CosineSimilarity\n from keras.src.losses.losses import Dice\n from keras.src.losses.losses import Hinge\n@@ -34,6 +35,7 @@ from keras.src.losses.losses import binary_focal_crossentropy\n from keras.src.losses.losses import categorical_crossentropy\n from keras.src.losses.losses import categorical_focal_crossentropy\n from keras.src.losses.losses import categorical_hinge\n+from keras.src.losses.losses import circle\n from keras.src.losses.losses import cosine_similarity\n from keras.src.losses.losses import ctc\n from keras.src.losses.losses import dice\n\n@@ -14,6 +14,7 @@ from keras.src.losses.losses import BinaryFocalCrossentropy\n from keras.src.losses.losses import CategoricalCrossentropy\n from keras.src.losses.losses import CategoricalFocalCrossentropy\n from keras.src.losses.losses import CategoricalHinge\n+from keras.src.losses.losses import Circle\n from keras.src.losses.losses import CosineSimilarity\n from keras.src.losses.losses import Dice\n from keras.src.losses.losses import Hinge\n@@ -33,6 +34,7 @@ from keras.src.losses.losses import binary_focal_crossentropy\n from keras.src.losses.losses import categorical_crossentropy\n from keras.src.losses.losses import categorical_focal_crossentropy\n from keras.src.losses.losses import categorical_hinge\n+from keras.src.losses.losses import circle\n from keras.src.losses.losses import cosine_similarity\n from keras.src.losses.losses import ctc\n from keras.src.losses.losses import dice\n\n@@ -52,11 +52,7 @@ def in_top_k(targets, predictions, k):\n \n \n def logsumexp(x, axis=None, keepdims=False):\n-    max_x = jnp.max(x, axis=axis, keepdims=True)\n-    result = (\n-        jnp.log(jnp.sum(jnp.exp(x - max_x), axis=axis, keepdims=True)) + max_x\n-    )\n-    return jnp.squeeze(result) if not keepdims else result\n+    return jax.scipy.special.logsumexp(x, axis=axis, keepdims=keepdims)\n \n \n def qr(x, mode=\"reduced\"):\n\n@@ -76,9 +76,7 @@ def in_top_k(targets, predictions, k):\n \n \n def logsumexp(x, axis=None, keepdims=False):\n-    max_x = np.max(x, axis=axis, keepdims=True)\n-    result = np.log(np.sum(np.exp(x - max_x), axis=axis, keepdims=True)) + max_x\n-    return np.squeeze(result) if not keepdims else result\n+    return scipy.special.logsumexp(x, axis=axis, keepdims=keepdims)\n \n \n def qr(x, mode=\"reduced\"):\n\n@@ -81,16 +81,8 @@ def in_top_k(targets, predictions, k):\n \n def logsumexp(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    if axis is None:\n-        max_x = torch.max(x)\n-        return torch.log(torch.sum(torch.exp(x - max_x))) + max_x\n-\n-    max_x = torch.amax(x, dim=axis, keepdim=True)\n-    result = (\n-        torch.log(torch.sum(torch.exp(x - max_x), dim=axis, keepdim=True))\n-        + max_x\n-    )\n-    return torch.squeeze(result, dim=axis) if not keepdims else result\n+    axis = tuple(range(x.dim())) if axis is None else axis\n+    return torch.logsumexp(x, dim=axis, keepdim=keepdims)\n \n \n def qr(x, mode=\"reduced\"):\n\n@@ -8,6 +8,7 @@ from keras.src.losses.losses import BinaryFocalCrossentropy\n from keras.src.losses.losses import CategoricalCrossentropy\n from keras.src.losses.losses import CategoricalFocalCrossentropy\n from keras.src.losses.losses import CategoricalHinge\n+from keras.src.losses.losses import Circle\n from keras.src.losses.losses import CosineSimilarity\n from keras.src.losses.losses import Dice\n from keras.src.losses.losses import Hinge\n@@ -28,6 +29,7 @@ from keras.src.losses.losses import binary_focal_crossentropy\n from keras.src.losses.losses import categorical_crossentropy\n from keras.src.losses.losses import categorical_focal_crossentropy\n from keras.src.losses.losses import categorical_hinge\n+from keras.src.losses.losses import circle\n from keras.src.losses.losses import cosine_similarity\n from keras.src.losses.losses import ctc\n from keras.src.losses.losses import dice\n@@ -72,6 +74,8 @@ ALL_OBJECTS = {\n     # Image segmentation\n     Dice,\n     Tversky,\n+    # Similarity\n+    Circle,\n     # Sequence\n     CTC,\n     # Probabilistic\n@@ -97,6 +101,8 @@ ALL_OBJECTS = {\n     # Image segmentation\n     dice,\n     tversky,\n+    # Similarity\n+    circle,\n     # Sequence\n     ctc,\n }\n\n@@ -7,6 +7,7 @@ from keras.src.api_export import keras_export\n from keras.src.losses.loss import Loss\n from keras.src.losses.loss import squeeze_or_expand_to_same_rank\n from keras.src.saving import serialization_lib\n+from keras.src.utils.numerical_utils import build_pos_neg_masks\n from keras.src.utils.numerical_utils import normalize\n \n \n@@ -1403,6 +1404,97 @@ class Tversky(LossFunctionWrapper):\n         return config\n \n \n+@keras_export(\"keras.losses.Circle\")\n+class Circle(LossFunctionWrapper):\n+    \"\"\"Computes Circle Loss between integer labels and L2-normalized embeddings.\n+\n+    This is a metric learning loss designed to minimize within-class distance\n+    and maximize between-class distance in a flexible manner by dynamically\n+    adjusting the penalty strength based on optimization status of each\n+    similarity score.\n+\n+    To use Circle Loss effectively, the model should output embeddings without\n+    an activation function (such as a `Dense` layer with `activation=None`)\n+    followed by UnitNormalization layer to ensure unit-norm embeddings.\n+\n+    Args:\n+        gamma: Scaling factor that determines the largest scale of each\n+            similarity score. Defaults to `80`.\n+        margin: The relaxation factor, below this distance, negatives are\n+        up weighted and positives are down weighted. Similarly, above this\n+        distance negatives are down weighted and positive are up weighted.\n+            Defaults to `0.4`.\n+        remove_diagonal: Boolean, whether to remove self-similarities from the\n+            positive mask. Defaults to `True`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n+\n+    Examples:\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model = models.Sequential([\n+        keras.layers.Input(shape=(224, 224, 3)),\n+        keras.layers.Conv2D(16, (3, 3), activation='relu'),\n+        keras.layers.Flatten(),\n+        keras.layers.Dense(64, activation=None),  # No activation\n+        keras.layers.UnitNormalization()  # L2 normalization\n+    ])\n+\n+    model.compile(optimizer=\"adam\", loss=losses.Circle()\n+    ```\n+\n+    Reference:\n+    - [Yifan Sun et al., 2020](https://arxiv.org/abs/2002.10857)\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        gamma=80.0,\n+        margin=0.4,\n+        remove_diagonal=True,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"circle\",\n+        dtype=None,\n+    ):\n+        super().__init__(\n+            circle,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n+            gamma=gamma,\n+            margin=margin,\n+            remove_diagonal=remove_diagonal,\n+        )\n+        self.gamma = gamma\n+        self.margin = margin\n+        self.remove_diagonal = remove_diagonal\n+\n+    def get_config(self):\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n+                \"gamma\": self.gamma,\n+                \"margin\": self.margin,\n+                \"remove_diagonal\": self.remove_diagonal,\n+            }\n+        )\n+        return config\n+\n+\n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n     are_zeros = ops.equal(y_true, 0)\n@@ -2406,3 +2498,91 @@ def tversky(y_true, y_pred, alpha=0.5, beta=0.5):\n     )\n \n     return 1 - tversky\n+\n+\n+@keras_export(\"keras.losses.circle\")\n+def circle(\n+    y_true,\n+    y_pred,\n+    ref_labels=None,\n+    ref_embeddings=None,\n+    remove_diagonal=True,\n+    gamma=80,\n+    margin=0.4,\n+):\n+    \"\"\"Computes the Circle loss.\n+\n+    It is designed to minimize within-class distances and maximize between-class\n+    distances in L2 normalized embedding space.\n+\n+    Args:\n+        y_true: Tensor with ground truth labels in integer format.\n+        y_pred: Tensor with predicted L2 normalized embeddings.\n+        ref_labels: Optional integer tensor with labels for reference\n+            embeddings. If `None`, defaults to `y_true`.\n+        ref_embeddings: Optional tensor with L2 normalized reference embeddings.\n+            If `None`, defaults to `y_pred`.\n+        remove_diagonal: Boolean, whether to remove self-similarities from\n+            positive mask. Defaults to `True`.\n+        gamma: Float, scaling factor for the loss. Defaults to `80`.\n+        margin: Float, relaxation factor for the loss. Defaults to `0.4`.\n+\n+    Returns:\n+        Circle loss value.\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, \"int32\")\n+    ref_embeddings = (\n+        y_pred\n+        if ref_embeddings is None\n+        else ops.convert_to_tensor(ref_embeddings)\n+    )\n+    ref_labels = y_true if ref_labels is None else ops.cast(ref_labels, \"int32\")\n+\n+    optim_pos = margin\n+    optim_neg = 1 + margin\n+    delta_pos = margin\n+    delta_neg = 1 - margin\n+\n+    pairwise_cosine_distances = 1 - ops.matmul(\n+        y_pred, ops.transpose(ref_embeddings)\n+    )\n+\n+    pairwise_cosine_distances = ops.maximum(pairwise_cosine_distances, 0.0)\n+    positive_mask, negative_mask = build_pos_neg_masks(\n+        y_true,\n+        ref_labels,\n+        remove_diagonal=remove_diagonal,\n+    )\n+    positive_mask = ops.cast(\n+        positive_mask, dtype=pairwise_cosine_distances.dtype\n+    )\n+    negative_mask = ops.cast(\n+        negative_mask, dtype=pairwise_cosine_distances.dtype\n+    )\n+\n+    pos_weights = optim_pos + pairwise_cosine_distances\n+    pos_weights = pos_weights * positive_mask\n+    pos_weights = ops.maximum(pos_weights, 0.0)\n+    neg_weights = optim_neg - pairwise_cosine_distances\n+    neg_weights = neg_weights * negative_mask\n+    neg_weights = ops.maximum(neg_weights, 0.0)\n+\n+    pos_dists = delta_pos - pairwise_cosine_distances\n+    neg_dists = delta_neg - pairwise_cosine_distances\n+\n+    pos_wdists = -1 * gamma * pos_weights * pos_dists\n+    neg_wdists = gamma * neg_weights * neg_dists\n+\n+    p_loss = ops.logsumexp(\n+        ops.where(positive_mask, pos_wdists, float(\"-inf\")),\n+        axis=1,\n+    )\n+    n_loss = ops.logsumexp(\n+        ops.where(negative_mask, neg_wdists, float(\"-inf\")),\n+        axis=1,\n+    )\n+\n+    circle_loss = ops.softplus(p_loss + n_loss)\n+    backend.set_keras_mask(circle_loss, circle_loss > 0)\n+    return circle_loss\n\n@@ -1645,3 +1645,99 @@ class TverskyTest(testing.TestCase):\n         y_pred = np.array(([[4, 1], [6, 1]]))\n         output = losses.Tversky(dtype=\"bfloat16\")(y_true, y_pred)\n         self.assertDType(output, \"bfloat16\")\n+\n+\n+class CircleTest(testing.TestCase):\n+    def setup(self):\n+        self.y_true = np.array([1, 1, 2, 2, 3])\n+        self.y_pred = np.array(\n+            [\n+                [0.70014004, -0.42008403, 0.14002801, 0.56011203],\n+                [0.17609018, 0.70436073, -0.52827054, 0.44022545],\n+                [-0.34050261, 0.25537696, -0.68100522, 0.59587957],\n+                [0.32163376, -0.75047877, 0.53605627, -0.21442251],\n+                [0.51261459, -0.34174306, 0.17087153, 0.76892189],\n+            ]\n+        )\n+        self.ref_labels = np.array([1, 1, 2, 2, 3, 4])\n+        self.ref_embeddings = np.array(\n+            [\n+                [0.40824829, -0.54433105, 0.27216553, 0.68041382],\n+                [0.76376261, 0.10910895, -0.54554473, 0.32732684],\n+                [-0.74420841, 0.24806947, 0.49613894, -0.3721042],\n+                [0.52981294, -0.13245324, 0.79471941, -0.26490647],\n+                [0.54554473, -0.32732684, 0.10910895, 0.76376261],\n+                [-0.27216553, 0.68041382, 0.40824829, -0.54433105],\n+            ]\n+        )\n+\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            losses.Circle(name=\"mycircle\", gamma=80.0, margin=0.4)\n+        )\n+\n+    def test_correctness(self):\n+        self.setup()\n+        circle_loss = losses.Circle(gamma=80.0, margin=0.4)\n+        loss = circle_loss(self.y_true, self.y_pred)\n+        self.assertAlmostEqual(loss, 188.3883)\n+\n+        circle_loss = losses.Circle(gamma=256, margin=0.25)\n+        loss = circle_loss(self.y_true, self.y_pred)\n+        self.assertAlmostEqual(loss, 652.7617)\n+\n+        loss = losses.circle(\n+            self.y_true,\n+            self.y_pred,\n+            ref_labels=self.ref_labels,\n+            ref_embeddings=self.ref_embeddings,\n+            gamma=80.0,\n+            margin=0.4,\n+            remove_diagonal=False,\n+        )\n+\n+        self.assertAllClose(\n+            loss, (61.5844, 94.3465, 276.9344, 90.9873, 48.8963)\n+        )\n+\n+    def test_correctness_weighted(self):\n+        self.setup()\n+        sample_weight = np.array([2.0, 2.0, 1.0, 1.0, 0.5])\n+        circle_loss = losses.Circle(gamma=80.0, margin=0.4)\n+        loss = circle_loss(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        self.assertAlmostEqual(loss, 244.91918)\n+\n+    def test_no_reduction(self):\n+        self.setup()\n+        circle_loss = losses.Circle(gamma=80.0, margin=0.4, reduction=None)\n+        loss = circle_loss(self.ref_labels, self.ref_embeddings)\n+\n+        self.assertAllClose(\n+            loss, [82.9116, 36.7942, 92.4590, 52.6798, 0.0, 0.0]\n+        )\n+\n+    def test_sum_reduction(self):\n+        self.setup()\n+        circle_loss = losses.Circle(gamma=80.0, margin=0.4, reduction=\"sum\")\n+        loss = circle_loss(self.ref_labels, self.ref_embeddings)\n+\n+        self.assertAlmostEqual(loss, 264.845)\n+\n+    def test_mean_with_sample_weight_reduction(self):\n+        self.setup()\n+        sample_weight = np.array([2.0, 2.0, 1.0, 1.0, 0.5])\n+        circle_loss = losses.Circle(\n+            gamma=80.0, margin=0.4, reduction=\"mean_with_sample_weight\"\n+        )\n+        loss = circle_loss(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        self.assertAlmostEqual(loss, 163.27948)\n+\n+    def test_dtype_arg(self):\n+        self.setup()\n+        circle_loss = losses.Circle(dtype=\"bfloat16\")\n+        loss = circle_loss(self.y_true, self.y_pred)\n+        self.assertDType(loss, \"bfloat16\")\n\n@@ -193,3 +193,33 @@ def encode_categorical_inputs(\n             axis=reduction_axis,\n         )\n         return outputs\n+\n+\n+def build_pos_neg_masks(\n+    query_labels,\n+    key_labels,\n+    remove_diagonal=True,\n+):\n+    from keras.src import ops\n+\n+    if ops.ndim(query_labels) == 1:\n+        query_labels = ops.reshape(query_labels, (-1, 1))\n+\n+    if ops.ndim(key_labels) == 1:\n+        key_labels = ops.reshape(key_labels, (-1, 1))\n+\n+    positive_mask = ops.equal(query_labels, ops.transpose(key_labels))\n+    negative_mask = ops.logical_not(positive_mask)\n+\n+    if remove_diagonal:\n+        positive_mask = ops.logical_and(\n+            positive_mask,\n+            ~ops.eye(\n+                ops.size(query_labels),\n+                ops.size(key_labels),\n+                k=0,\n+                dtype=\"bool\",\n+            ),\n+        )\n+\n+    return positive_mask, negative_mask\n\n@@ -72,3 +72,80 @@ class TestNumericalUtils(testing.TestCase):\n         out = numerical_utils.normalize(xb, axis=-1, order=order)\n         self.assertTrue(backend.is_tensor(out))\n         self.assertAllClose(backend.convert_to_numpy(out), expected)\n+\n+    def test_build_pos_neg_masks(self):\n+        query_labels = np.array([0, 1, 2, 2, 0])\n+        key_labels = np.array([0, 1, 2, 0, 2])\n+        expected_shape = (len(query_labels), len(key_labels))\n+\n+        positive_mask, negative_mask = numerical_utils.build_pos_neg_masks(\n+            query_labels, key_labels, remove_diagonal=False\n+        )\n+\n+        positive_mask = backend.convert_to_numpy(positive_mask)\n+        negative_mask = backend.convert_to_numpy(negative_mask)\n+        self.assertEqual(positive_mask.shape, expected_shape)\n+        self.assertEqual(negative_mask.shape, expected_shape)\n+        self.assertTrue(\n+            np.all(np.logical_not(np.logical_and(positive_mask, negative_mask)))\n+        )\n+\n+        expected_positive_mask_keep_diag = np.array(\n+            [\n+                [1, 0, 0, 1, 0],\n+                [0, 1, 0, 0, 0],\n+                [0, 0, 1, 0, 1],\n+                [0, 0, 1, 0, 1],\n+                [1, 0, 0, 1, 0],\n+            ],\n+            dtype=\"bool\",\n+        )\n+        self.assertTrue(\n+            np.all(positive_mask == expected_positive_mask_keep_diag)\n+        )\n+        self.assertTrue(\n+            np.all(\n+                negative_mask\n+                == np.logical_not(expected_positive_mask_keep_diag)\n+            )\n+        )\n+\n+        positive_mask, negative_mask = numerical_utils.build_pos_neg_masks(\n+            query_labels, key_labels, remove_diagonal=True\n+        )\n+        positive_mask = backend.convert_to_numpy(positive_mask)\n+        negative_mask = backend.convert_to_numpy(negative_mask)\n+        self.assertEqual(positive_mask.shape, expected_shape)\n+        self.assertEqual(negative_mask.shape, expected_shape)\n+        self.assertTrue(\n+            np.all(np.logical_not(np.logical_and(positive_mask, negative_mask)))\n+        )\n+\n+        expected_positive_mask_with_remove_diag = np.array(\n+            [\n+                [0, 0, 0, 1, 0],\n+                [0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 1],\n+                [0, 0, 1, 0, 1],\n+                [1, 0, 0, 1, 0],\n+            ],\n+            dtype=\"bool\",\n+        )\n+        self.assertTrue(\n+            np.all(positive_mask == expected_positive_mask_with_remove_diag)\n+        )\n+\n+        query_labels = np.array([1, 2, 3])\n+        key_labels = np.array([1, 2, 3, 1])\n+\n+        positive_mask, negative_mask = numerical_utils.build_pos_neg_masks(\n+            query_labels, key_labels, remove_diagonal=True\n+        )\n+        positive_mask = backend.convert_to_numpy(positive_mask)\n+        negative_mask = backend.convert_to_numpy(negative_mask)\n+        expected_shape_diff_sizes = (len(query_labels), len(key_labels))\n+        self.assertEqual(positive_mask.shape, expected_shape_diff_sizes)\n+        self.assertEqual(negative_mask.shape, expected_shape_diff_sizes)\n+        self.assertTrue(\n+            np.all(np.logical_not(np.logical_and(positive_mask, negative_mask)))\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0bd3311aa15fb1c0016ab002f562c4cf9ef18f5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1166 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1442,6 +1442,7 @@ class Circle(LossFunctionWrapper):\n             provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n+\n     Usage with the `compile()` API:\n \n     ```python\n@@ -1453,7 +1454,7 @@ class Circle(LossFunctionWrapper):\n         keras.layers.UnitNormalization()  # L2 normalization\n     ])\n \n-    model.compile(optimizer=\"adam\", loss=losses.Circle()\n+    model.compile(optimizer=\"adam\", loss=keras.losses.Circle())\n     ```\n \n     Reference:\n\n@@ -33,11 +33,12 @@ class TorchModuleWrapper(Layer):\n     PyTorch modules.\n \n     ```python\n+    import torch\n     import torch.nn as nn\n     import torch.nn.functional as F\n \n     import keras\n-    from keras.src.layers import TorchModuleWrapper\n+    from keras.layers import TorchModuleWrapper\n \n     class Classifier(keras.Model):\n         def __init__(self, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d96e54d706cbbc70dd2901c85b1acdede35e7ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 115 | Churn Cumulative: 1908 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n+        # Specifies how many steps of the step_per_execution loop to unroll.\n+        # Increasing this value can reduce kernel launch overhead,\n+        # but will increase memory usage and compilation time.\n+        self.unrolled_steps_per_execution = 1\n+\n         # Model must be created under scope of DistStrat it will be trained\n         # with.\n         if tf.distribute.has_strategy():\n@@ -133,11 +138,31 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                     next_optional_inputs.has_value(),\n                 )\n \n-            def body(execution_step, optional_outputs, next_optional_inputs):\n+            def inner_body(\n+                execution_step, optional_outputs, next_optional_inputs\n+            ):\n+                def has_next():\n                     next_optional_outputs = tf.experimental.Optional.from_value(\n                         one_step_on_data(next_optional_inputs.get_value())\n                     )\n-                empty_outputs._element_spec = next_optional_outputs.element_spec\n+                    empty_outputs._element_spec = (\n+                        next_optional_outputs.element_spec\n+                    )\n+                    return next_optional_outputs\n+\n+                def no_has_next():\n+                    optional_outputs._element_spec = empty_outputs._element_spec\n+                    return optional_outputs\n+\n+                next_optional_outputs = tf.cond(\n+                    tf.logical_and(\n+                        tf.less(execution_step, self.steps_per_execution),\n+                        next_optional_inputs.has_value(),\n+                    ),\n+                    has_next,\n+                    no_has_next,\n+                )\n+\n                 return (\n                     execution_step + 1,\n                     next_optional_outputs,\n@@ -150,6 +175,23 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                     ),\n                 )\n \n+            def body(execution_step, optional_outputs, next_optional_inputs):\n+                for _ in range(\n+                    min(\n+                        self.unrolled_steps_per_execution,\n+                        self.steps_per_execution,\n+                    )\n+                ):\n+                    execution_step, optional_outputs, next_optional_inputs = (\n+                        inner_body(\n+                            execution_step,\n+                            optional_outputs,\n+                            next_optional_inputs,\n+                        )\n+                    )\n+\n+                return (execution_step, optional_outputs, next_optional_inputs)\n+\n             execution_step = tf.constant(0)\n             next_optional_inputs = iterator.get_next_as_optional()\n \n\n@@ -968,6 +968,75 @@ class TestTrainer(testing.TestCase):\n         )\n         self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n \n+    @parameterized.named_parameters(\n+        named_product(steps_per_execution=[3, 8, 32])\n+    )\n+    @pytest.mark.requires_trainable_backend\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"`unrolled_steps_per_execution` is only \"\n+        \"available with the tensorflow backend.\",\n+    )\n+    def test_steps_per_execution_unrolled_steps_steps_count(\n+        self, steps_per_execution\n+    ):\n+        data_size = 100\n+        batch_size = 16\n+        epochs = 2\n+        unrolled_steps_per_execution = 8\n+\n+        batches_indices = list(\n+            range(0, data_size, steps_per_execution * batch_size)\n+        )\n+\n+        x = np.ones((data_size, 4))\n+        y = np.ones((data_size, 1))\n+\n+        model = ExampleModel(units=1)\n+        model.compile(\n+            loss=\"mse\",\n+            optimizer=\"sgd\",\n+            steps_per_execution=steps_per_execution,\n+            jit_compile=True,\n+        )\n+        step_count = StepCount(batches_indices, batch_size)\n+        model.unrolled_steps_per_execution = unrolled_steps_per_execution\n+        history = model.fit(\n+            x=x,\n+            y=y,\n+            batch_size=batch_size,\n+            epochs=epochs,\n+            callbacks=[step_count],\n+            verbose=0,\n+        )\n+\n+        self.assertEqual(step_count.begin_count, len(batches_indices))\n+        self.assertEqual(step_count.end_count, step_count.begin_count)\n+        self.assertEqual(step_count.epoch_begin_count, epochs)\n+        self.assertEqual(\n+            step_count.epoch_end_count, step_count.epoch_begin_count\n+        )\n+\n+        model_2 = ExampleModel(units=1)\n+        model_2.compile(\n+            loss=\"mse\",\n+            optimizer=\"sgd\",\n+            steps_per_execution=steps_per_execution,\n+            jit_compile=True,\n+        )\n+        model_2.unrolled_steps_per_execution = 1\n+        history_2 = model_2.fit(\n+            x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=0\n+        )\n+\n+        self.assertAllClose(history.history[\"loss\"], history_2.history[\"loss\"])\n+        self.assertAllClose(model.get_weights(), model_2.get_weights())\n+        self.assertAllClose(\n+            model.predict(x, batch_size=batch_size),\n+            model_2.predict(x, batch_size=batch_size),\n+        )\n+        self.assertAllClose(model.evaluate(x, y), model_2.evaluate(x, y))\n+\n     @parameterized.named_parameters(\n         named_product(\n             steps_per_execution=[1, 50], mode=[\"eager\", \"non_jit\", \"jit\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#647554aad8518de07a30b00316f688f313b767f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 65 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -474,18 +474,18 @@ class MultiHeadAttention(Layer):\n         #   N = `num_attention_heads`\n         #   H = `size_per_head`\n         # `query` = [B, T, N ,H]\n-        query = self._query_dense(query)\n+        query = self._query_dense.call(query)\n \n         # `key` = [B, S, N, H]\n-        key = self._key_dense(key)\n+        key = self._key_dense.call(key)\n \n         # `value` = [B, S, N, H]\n-        value = self._value_dense(value)\n+        value = self._value_dense.call(value)\n \n         attention_output, attention_scores = self._compute_attention(\n             query, key, value, attention_mask, training\n         )\n-        attention_output = self._output_dense(attention_output)\n+        attention_output = self._output_dense.call(attention_output)\n \n         if return_attention_scores:\n             return attention_output, attention_scores\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37cd53b5b2a3aec528211c91ad2a39525857cc9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 78 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -194,6 +194,7 @@ class TensorBoard(Callback):\n \n         self._init_profile_batch(profile_batch)\n         self._global_train_batch = 0\n+        self._global_test_batch = 0\n         self._previous_epoch_iterations = 0\n         self._train_accumulated_time = 0\n         self._batch_start_time = 0\n@@ -212,11 +213,7 @@ class TensorBoard(Callback):\n         self._log_write_dir = self.log_dir\n \n         self._train_dir = os.path.join(self._log_write_dir, \"train\")\n-        self._train_step = 0\n-\n         self._val_dir = os.path.join(self._log_write_dir, \"validation\")\n-        self._val_step = 0\n-\n         self._writers = {}  # Resets writers.\n \n         self._should_write_train_graph = False\n@@ -409,7 +406,7 @@ class TensorBoard(Callback):\n     def on_train_begin(self, logs=None):\n         self._global_train_batch = 0\n         self._previous_epoch_iterations = 0\n-        self._push_writer(self._train_writer, self._train_step)\n+        self._push_writer(self._train_writer, self._global_train_batch)\n \n     def on_train_end(self, logs=None):\n         self._pop_writer()\n@@ -420,7 +417,7 @@ class TensorBoard(Callback):\n         self._close_writers()\n \n     def on_test_begin(self, logs=None):\n-        self._push_writer(self._val_writer, self._val_step)\n+        self._push_writer(self._val_writer, self._global_test_batch)\n \n     def on_test_end(self, logs=None):\n         if self.model.optimizer and hasattr(self.model.optimizer, \"iterations\"):\n@@ -433,11 +430,6 @@ class TensorBoard(Callback):\n                     )\n         self._pop_writer()\n \n-    def _implements_train_batch_hooks(self):\n-        # Only call batch hooks when tracing or write_steps_per_second are\n-        # enabled\n-        return self._should_trace or self.write_steps_per_second\n-\n     def on_train_batch_begin(self, batch, logs=None):\n         self._global_train_batch += 1\n         if self.write_steps_per_second:\n@@ -461,14 +453,14 @@ class TensorBoard(Callback):\n             self.summary.scalar(\n                 \"batch_steps_per_second\",\n                 1.0 / batch_run_time,\n-                step=self._train_step,\n+                step=self._global_train_batch,\n             )\n \n         # `logs` isn't necessarily always a dict\n         if isinstance(logs, dict):\n             for name, value in logs.items():\n                 self.summary.scalar(\n-                    \"batch_\" + name, value, step=self._train_step\n+                    \"batch_\" + name, value, step=self._global_train_batch\n                 )\n \n         if not self._should_trace:\n@@ -481,6 +473,9 @@ class TensorBoard(Callback):\n             if self._global_train_batch >= self._stop_batch:\n                 self._stop_trace()\n \n+    def on_test_batch_begin(self, batch, logs=None):\n+        self._global_test_batch += 1\n+\n     def on_epoch_begin(self, epoch, logs=None):\n         # Keeps track of epoch for profiling.\n         if self.write_steps_per_second:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f4828ccce090aed4727673afaf2791cce837b47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 1719 | Contributors (this commit): 19 | Commits (past 90d): 20 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2449,6 +2449,9 @@ class NNOpsDtypeTest(testing.TestCase):\n         import jax.nn as jnn\n         import jax.numpy as jnp\n \n+        if dtype == \"bfloat16\":\n+            self.skipTest(\"Weirdness with numpy\")\n+\n         x = knp.ones((2), dtype=dtype)\n         x_jax = jnp.ones((2), dtype=dtype)\n         expected_dtype = standardize_dtype(jnn.glu(x_jax).dtype)\n\n@@ -3423,9 +3423,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.Ceil()(x), np.ceil(x))\n \n     def test_clip(self):\n-        x = np.array([[1.2, 2.1, -2.5], [2.4, -11.9, -5.5]])\n-        self.assertAllClose(knp.clip(x, -2, 2), np.clip(x, -2, 2))\n-        self.assertAllClose(knp.clip(x, -2, 2), np.clip(x, -2, 2))\n+        x = np.array([[1.2, 2.1, 0.5], [2.4, 11.9, 0.5]])\n+        self.assertAllClose(knp.clip(x, 1, 2), np.clip(x, 1, 2))\n+        self.assertAllClose(knp.clip(x, 1, 2), np.clip(x, 1, 2))\n \n         self.assertAllClose(knp.Clip(0, 1)(x), np.clip(x, 0, 1))\n         self.assertAllClose(knp.Clip(0, 1)(x), np.clip(x, 0, 1))\n@@ -6066,15 +6066,15 @@ class NumpyDtypeTest(testing.TestCase):\n \n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n-        expected_dtype = standardize_dtype(jnp.clip(x_jax, -2, 2).dtype)\n+        expected_dtype = standardize_dtype(jnp.clip(x_jax, 1, 2).dtype)\n         if dtype == \"bool\":\n             expected_dtype = \"int32\"\n \n         self.assertEqual(\n-            standardize_dtype(knp.clip(x, -2, 2).dtype), expected_dtype\n+            standardize_dtype(knp.clip(x, 1, 2).dtype), expected_dtype\n         )\n         self.assertEqual(\n-            standardize_dtype(knp.Clip(-2, 2).symbolic_call(x).dtype),\n+            standardize_dtype(knp.Clip(1, 2).symbolic_call(x).dtype),\n             expected_dtype,\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccb07dfdf04b76e2491d4d3eb662042efe18c2b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 18 | Files Changed: 5 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 83 | Churn Cumulative: 883 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,9 @@\n import inspect\n \n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.initializers.constant_initializers import Constant\n from keras.src.initializers.constant_initializers import Identity\n@@ -82,7 +86,7 @@ def get(identifier):\n     (case-sensitively).\n \n     >>> identifier = 'Ones'\n-    >>> keras.initializers.deserialize(identifier)\n+    >>> keras.initializers.get(identifier)\n     <...keras.initializers.initializers.Ones...>\n \n     You can also specify `config` of the initializer to this function by passing\n@@ -90,15 +94,34 @@ def get(identifier):\n     the `class_name` must map to a `Initializer` class.\n \n     >>> cfg = {'class_name': 'Ones', 'config': {}}\n-    >>> keras.initializers.deserialize(cfg)\n+    >>> keras.initializers.get(cfg)\n     <...keras.initializers.initializers.Ones...>\n \n     In the case that the `identifier` is a class, this method will return a new\n     instance of the class by its constructor.\n \n+    You may also pass a callable function with a signature that includes `shape`\n+    and `dtype=None` as an identifier.\n+\n+    >>> fn = lambda shape, dtype=None: ops.ones(shape, dtype)\n+    >>> keras.initializers.get(fn)\n+    <function <lambda> at ...>\n+\n+    Alternatively, you can pass a backend tensor or numpy array as the\n+    `identifier` to define the initializer values directly. Note that when\n+    calling the initializer, the specified `shape` argument must be the same as\n+    the shape of the tensor.\n+\n+    >>> tensor = ops.ones(shape=(5, 5))\n+    >>> keras.initializers.get(tensor)\n+    <function get.<locals>.initialize_fn at ...>\n+\n     Args:\n-        identifier: String or dict that contains the initializer name or\n-            configurations.\n+        identifier: A string, dict, callable function, or tensor specifying\n+            the initializer. If a string, it should be the name of an\n+            initializer. If a dict, it should contain the configuration of an\n+            initializer. Callable functions or predefined tensors are also\n+            accepted.\n \n     Returns:\n         Initializer instance base on the input identifier.\n@@ -110,6 +133,22 @@ def get(identifier):\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n         obj = deserialize(config)\n+    elif ops.is_tensor(identifier) or isinstance(\n+        identifier, (np.generic, np.ndarray)\n+    ):\n+\n+        def initialize_fn(shape, dtype=None):\n+            dtype = backend.standardize_dtype(dtype)\n+            if backend.standardize_shape(shape) != backend.standardize_shape(\n+                identifier.shape\n+            ):\n+                raise ValueError(\n+                    f\"Expected `shape` to be {identifier.shape} for direct \"\n+                    f\"tensor as initializer. Received shape={shape}\"\n+                )\n+            return ops.cast(identifier, dtype)\n+\n+        obj = initialize_fn\n     else:\n         obj = identifier\n \n\n@@ -162,6 +162,25 @@ class InitializersTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             initializers.get(\"typo\")\n \n+    def test_get_method_with_tensor(self):\n+        shape = (5, 5)\n+\n+        # Test backend tensor\n+        tensor = random.uniform(shape=shape)\n+        initializer = initializers.get(tensor)\n+        values = initializer(shape=shape)\n+        self.assertAllClose(values, tensor)\n+\n+        # Test numpy array\n+        tensor = np.random.uniform(size=shape).astype(\"float32\")\n+        initializer = initializers.get(tensor)\n+        values = initializer(shape=shape)\n+        self.assertAllClose(values, tensor)\n+\n+        # Test bad `shape` argument\n+        with self.assertRaisesRegex(ValueError, r\"Expected `shape` to be\"):\n+            initializer(shape=(10, 10))\n+\n     def test_variance_scaling_invalid_scale(self):\n         seed = 1234\n \n\n@@ -524,11 +524,7 @@ class Dense(Layer):\n             del self._kernel\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n-            self._int8_build(\n-                kernel_shape,\n-                lambda shape, dtype: kernel_value,\n-                lambda shape, dtype: kernel_scale,\n-            )\n+            self._int8_build(kernel_shape, kernel_value, kernel_scale)\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n\n@@ -662,11 +662,7 @@ class EinsumDense(Layer):\n             del self._kernel\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n-            self._int8_build(\n-                kernel_shape,\n-                lambda shape, dtype: kernel_value,\n-                lambda shape, dtype: kernel_scale,\n-            )\n+            self._int8_build(kernel_shape, kernel_value, kernel_scale)\n         elif mode == \"float8\":\n             self._float8_build()\n         else:\n\n@@ -358,10 +358,7 @@ class Embedding(Layer):\n             del self._embeddings\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n-            self._int8_build(\n-                lambda shape, dtype: embeddings_value,\n-                lambda shape, dtype: embeddings_scale,\n-            )\n+            self._int8_build(embeddings_value, embeddings_scale)\n         else:\n             raise self._quantization_mode_error(mode)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30a6b870b698464c7ea7a8f6453730c1fcd86811", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 430 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -464,7 +464,7 @@ def affine_transform(\n     # transform the indices\n     coordinates = jnp.einsum(\"Bhwij, Bjk -> Bhwik\", indices, transform)\n     coordinates = jnp.moveaxis(coordinates, source=-1, destination=1)\n-    coordinates += jnp.reshape(a=offset, newshape=(*offset.shape, 1, 1, 1))\n+    coordinates += jnp.reshape(a=offset, shape=(*offset.shape, 1, 1, 1))\n \n     # apply affine transformation\n     _map_coordinates = functools.partial(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5bf4ac7a6678463998afdb075b688c7f9d5f55ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 290 | Lines Deleted: 16 | Files Changed: 7 | Hunks: 29 | Methods Changed: 14 | Complexity Δ (Sum/Max): 33/16 | Churn Δ: 306 | Churn Cumulative: 1488 | Contributors (this commit): 14 | Commits (past 90d): 27 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,6 +13,9 @@ from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n from keras.src.dtype_policies.dtype_policy import dtype_policy\n from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n+from keras.src.layers.attention.attention import disable_flash_attention\n+from keras.src.layers.attention.attention import enable_flash_attention\n+from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.saving.serialization_lib import enable_unsafe_deserialization\n from keras.src.utils.backend_utils import set_backend\n from keras.src.utils.io_utils import disable_interactive_logging\n\n@@ -13,6 +13,9 @@ from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n from keras.src.dtype_policies.dtype_policy import dtype_policy\n from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n+from keras.src.layers.attention.attention import disable_flash_attention\n+from keras.src.layers.attention.attention import enable_flash_attention\n+from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.saving.serialization_lib import enable_unsafe_deserialization\n from keras.src.utils.backend_utils import set_backend\n from keras.src.utils.io_utils import disable_interactive_logging\n\n@@ -6,6 +6,9 @@ import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n from jax import lax\n from jax import nn as jnn\n+from jax.experimental.pallas.ops.tpu import (\n+    flash_attention as flash_attention_tpu,\n+)\n \n from keras.src import backend\n from keras.src.backend.common.backend_utils import (\n@@ -1019,7 +1022,18 @@ def dot_product_attention(\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n-\n+    is_tpu = jax.devices()[0].platform == \"tpu\"\n+    if is_tpu and flash_attention:\n+        # Use TPU-optimized flash attention from Pallas\n+        return flash_attention_tpu(\n+            query,\n+            key,\n+            value,\n+            ab=bias,\n+            segment_ids=mask,\n+            causal=is_causal,\n+            sm_scale=scale,\n+        )\n     # `dot_product_attention` is only available in jax>=0.4.31\n     if hasattr(jax.nn, \"dot_product_attention\"):\n         implementation = \"cudnn\" if flash_attention else \"xla\"\n@@ -1040,7 +1054,6 @@ def dot_product_attention(\n             \"current JAX version. Please update it \"\n             \"using `pip install -U jax jaxlib`.\"\n         )\n-\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.33/jax/_src/nn/functions.py#L886\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n\n@@ -954,7 +954,14 @@ def dot_product_attention(\n                 scale=scale,\n             )\n     else:\n+        if mask is not None:\n+            mask = mask.contiguous()\n         attention_output = torch.nn.functional.scaled_dot_product_attention(\n-            query, key, value, attn_mask=mask, is_causal=is_causal, scale=scale\n+            query.contiguous(),\n+            key.contiguous(),\n+            value.contiguous(),\n+            attn_mask=mask,\n+            is_causal=is_causal,\n+            scale=scale,\n         )\n     return torch.transpose(attention_output, axis1, axis0)\n\n@@ -1,6 +1,7 @@\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.backend.common import global_state\n from keras.src.layers.layer import Layer\n \n \n@@ -282,3 +283,49 @@ class Attention(Layer):\n             \"dropout\": self.dropout,\n         }\n         return {**base_config, **config}\n+\n+\n+@keras_export(\"keras.config.enable_flash_attention\")\n+def enable_flash_attention():\n+    \"\"\"Enable flash attention.\n+\n+    Flash attention offers performance optimization for attention layers,\n+    making it especially useful for large language models (LLMs) that\n+    benefit from faster and more memory-efficient attention computations.\n+\n+    Once enabled, supported layers like `MultiHeadAttention` will\n+    use flash attention for faster computations.\n+    \"\"\"\n+    global_state.set_global_attribute(\"flash_attention\", True)\n+\n+\n+@keras_export(\"keras.config.disable_flash_attention\")\n+def disable_flash_attention():\n+    \"\"\"Disable flash attention.\n+\n+    Flash attention offers performance optimization for attention layers,\n+    making it especially useful for large language models (LLMs) that\n+    benefit from faster and more memory-efficient attention computations.\n+\n+    Once disabled, supported layers like `MultiHeadAttention` will not\n+    use flash attention for faster computations.\n+    \"\"\"\n+    global_state.set_global_attribute(\"flash_attention\", False)\n+\n+\n+@keras_export(\"keras.config.is_flash_attention_enabled\")\n+def is_flash_attention_enabled():\n+    \"\"\"Checks whether flash attention is globally enabled in Keras.\n+\n+    Flash attention is a performance-optimized method for computing attention\n+    in large models, such as transformers, allowing for faster and more\n+    memory-efficient operations. This function checks the global Keras\n+    configuration to determine if flash attention is enabled for compatible\n+    layers (e.g., `MultiHeadAttention`).\n+\n+    Returns:\n+        bool or None: Returns `True` if flash attention is enabled,\n+                      `False` if it is disabled, and `None` if the global\n+                      setting has not been defined.\n+    \"\"\"\n+    return global_state.get_global_attribute(\"flash_attention\", default=None)\n\n@@ -11,6 +11,7 @@ from keras.src import ops\n from keras.src import regularizers\n from keras.src.api_export import keras_export\n from keras.src.layers.activations.softmax import Softmax\n+from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.layers.core.einsum_dense import EinsumDense\n from keras.src.layers.layer import Layer\n from keras.src.layers.regularization.dropout import Dropout\n@@ -52,6 +53,9 @@ class MultiHeadAttention(Layer):\n             feature dim (the query input's last dimension).\n         attention_axes: axes over which the attention is applied. `None` means\n             attention over all axes, but batch, heads, and features.\n+        flash_attention: If unspecified, defaults to the global flash attention\n+            configuration setting (which can be set via\n+            `keras.config.enable_flash_attention().\n         kernel_initializer: Initializer for dense layer kernels.\n         bias_initializer: Initializer for dense layer biases.\n         kernel_regularizer: Regularizer for dense layer kernels.\n@@ -104,6 +108,7 @@ class MultiHeadAttention(Layer):\n         use_bias=True,\n         output_shape=None,\n         attention_axes=None,\n+        flash_attention=None,\n         kernel_initializer=\"glorot_uniform\",\n         bias_initializer=\"zeros\",\n         kernel_regularizer=None,\n@@ -131,6 +136,8 @@ class MultiHeadAttention(Layer):\n         self._activity_regularizer = regularizers.get(activity_regularizer)\n         self._kernel_constraint = constraints.get(kernel_constraint)\n         self._bias_constraint = constraints.get(bias_constraint)\n+        self._flash_attention = flash_attention or is_flash_attention_enabled()\n+\n         if isinstance(attention_axes, int):\n             attention_axes = (attention_axes,)\n         elif attention_axes and not isinstance(attention_axes, (list, tuple)):\n@@ -392,7 +399,13 @@ class MultiHeadAttention(Layer):\n         return self._softmax(attention_scores, mask=attention_mask)\n \n     def _compute_attention(\n-        self, query, key, value, attention_mask=None, training=None\n+        self,\n+        query,\n+        key,\n+        value,\n+        return_attention_scores,\n+        attention_mask=None,\n+        training=None,\n     ):\n         \"\"\"Applies Dot-product attention with query, key, value tensors.\n \n@@ -415,9 +428,57 @@ class MultiHeadAttention(Layer):\n           attention_output: Multi-headed outputs of attention computation.\n           attention_scores: Multi-headed attention weights.\n         \"\"\"\n-        # Note: Applying scalar multiply at the smaller end of einsum improves\n-        # XLA performance, but may introduce slight numeric differences in\n-        # the Transformer attention head.\n+\n+        # Check for flash attention constraints\n+        if self._flash_attention and return_attention_scores:\n+            raise ValueError(\n+                \"Returning attention scores is not supported when flash \"\n+                \"attention is enabled. Please disable flash attention to access\"\n+                \" attention scores.\"\n+            )\n+        if self._flash_attention and self._dropout > 0.0:\n+            raise ValueError(\n+                \"Dropout is not supported when flash \"\n+                \"attention is enabled. Please set dropout to 0.0 to use \"\n+                \"flash attention.\"\n+            )\n+\n+        # Determine whether to use dot-product attention\n+        use_dot_product_attention = not (\n+            self._dropout > 0.0\n+            or return_attention_scores\n+            or (len(query.shape) != 4)\n+        )\n+\n+        if use_dot_product_attention:\n+            if attention_mask is not None:\n+                # Ensure attention_mask has the correct shape for broadcasting\n+                # Expected shape: [batch_size, num_heads, query_seq_len,\n+                # key_seq_len]. This is because masked_softmax is not supported\n+                # in JAX.\n+                while len(attention_mask.shape) < 4:\n+                    attention_mask = ops.expand_dims(\n+                        attention_mask, axis=1\n+                    )  # Add dimension for num_heads\n+                if attention_mask.shape[1] != self._num_heads:\n+                    attention_mask = ops.tile(\n+                        attention_mask, [1, self._num_heads, 1, 1]\n+                    )\n+            # Directly compute the attention output using dot-product attention\n+            attention_output = ops.dot_product_attention(\n+                query=query,\n+                key=key,\n+                value=value,\n+                bias=None,\n+                mask=attention_mask,\n+                scale=self._inverse_sqrt_key_dim,\n+                is_causal=False,\n+                flash_attention=self._flash_attention,\n+            )\n+            return attention_output, None\n+\n+        # Default behavior without flash attention, with explicit attention\n+        # scores\n         query = ops.multiply(\n             query, ops.cast(self._inverse_sqrt_key_dim, query.dtype)\n         )\n@@ -426,13 +487,13 @@ class MultiHeadAttention(Layer):\n         # attention scores.\n         attention_scores = ops.einsum(self._dot_product_equation, key, query)\n \n+        # Apply the mask using the custom masked softmax\n         attention_scores = self._masked_softmax(\n             attention_scores, attention_mask\n         )\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        if self.dropout:\n+        # Apply dropout to the attention scores if needed\n+        if self._dropout > 0.0:\n             final_attn_scores = self._dropout_layer(\n                 attention_scores, training=training\n             )\n@@ -460,7 +521,6 @@ class MultiHeadAttention(Layer):\n     ):\n         if key is None:\n             key = value\n-\n         attention_mask = self._compute_attention_mask(\n             query,\n             value,\n@@ -470,9 +530,9 @@ class MultiHeadAttention(Layer):\n             attention_mask=attention_mask,\n             use_causal_mask=use_causal_mask,\n         )\n-\n         #   N = `num_attention_heads`\n         #   H = `size_per_head`\n+\n         # `query` = [B, T, N ,H]\n         query = self._query_dense.call(query)\n \n@@ -481,9 +541,13 @@ class MultiHeadAttention(Layer):\n \n         # `value` = [B, S, N, H]\n         value = self._value_dense.call(value)\n-\n         attention_output, attention_scores = self._compute_attention(\n-            query, key, value, attention_mask, training\n+            query,\n+            key,\n+            value,\n+            return_attention_scores,\n+            attention_mask,\n+            training,\n         )\n         attention_output = self._output_dense.call(attention_output)\n \n\n@@ -12,6 +12,8 @@ from keras.src import layers\n from keras.src import models\n from keras.src import saving\n from keras.src import testing\n+from keras.src.layers.attention.attention import disable_flash_attention\n+from keras.src.layers.attention.attention import enable_flash_attention\n \n \n class MultiHeadAttentionTest(testing.TestCase):\n@@ -51,6 +53,79 @@ class MultiHeadAttentionTest(testing.TestCase):\n             run_training_check=False,\n         )\n \n+    def test_basics_with_flash_attention(self):\n+        if backend.backend() in [\n+            \"torch\",\n+            \"tensorflow\",\n+            \"numpy\",\n+        ]:\n+            self.skipTest(\n+                \"Not supported in TF and NumPy and supported for \"\n+                \"PyTorch with specific requirements.\"\n+            )\n+\n+        if backend.backend() == \"jax\":\n+            try:\n+                enable_flash_attention()\n+                self.run_layer_test(\n+                    layers.MultiHeadAttention,\n+                    init_kwargs={\n+                        \"num_heads\": 2,\n+                        \"key_dim\": 2,\n+                    },\n+                    input_shape={\n+                        \"query_shape\": (2, 8, 16),\n+                        \"value_shape\": (2, 4, 16),\n+                    },\n+                    expected_output_shape=(2, 8, 16),\n+                    expected_num_trainable_weights=8,\n+                    expected_num_non_trainable_weights=0,\n+                    expected_num_seed_generators=0,\n+                    expected_num_losses=0,\n+                    supports_masking=True,\n+                    run_training_check=False,\n+                )\n+\n+                self.run_layer_test(\n+                    layers.MultiHeadAttention,\n+                    init_kwargs={\n+                        \"num_heads\": 2,\n+                        \"key_dim\": 2,\n+                        \"value_dim\": 4,\n+                        \"use_bias\": False,\n+                        \"dropout\": 0.5,\n+                    },\n+                    input_shape={\n+                        \"query_shape\": (2, 8, 16),\n+                        \"value_shape\": (2, 4, 16),\n+                    },\n+                    expected_output_shape=(2, 8, 16),\n+                    expected_num_trainable_weights=4,\n+                    expected_num_non_trainable_weights=0,\n+                    expected_num_seed_generators=1,\n+                    expected_num_losses=0,\n+                    supports_masking=True,\n+                    run_training_check=False,\n+                )\n+                disable_flash_attention()\n+            except ValueError as e:\n+                if e.args[0].startswith(\n+                    \"Flash attention is not supported in your \"\n+                    \"current JAX version\"\n+                ):\n+                    self.skipTest(\n+                        \"JAX version does not have \"\n+                        \"`dot_product_attention` function.\"\n+                    )\n+            except RuntimeError as e:\n+                if e.args[0] == \"cuDNN is not detected.\":\n+                    self.skipTest(\"No CuDNN to run flash attention for JAX.\")\n+                elif e.args[0] == \"Require at least Ampere arch to run\":\n+                    self.skipTest(\n+                        \"Requires at least Ampere arch to run flash attention \"\n+                        \"for JAX.\"\n+                    )\n+\n     @parameterized.named_parameters(\n         (\"4d_inputs_1freebatch_mask2\", (3, 4), (3, 2), (4, 2), (2,)),\n         (\"4d_inputs_1freebatch_mask3\", (3, 4), (3, 2), (3, 4, 2), (2,)),\n@@ -189,12 +264,23 @@ class MultiHeadAttentionTest(testing.TestCase):\n     )\n     def test_query_mask_propagation(self):\n         \"\"\"Test automatic propagation of the query's mask.\"\"\"\n+        try:\n             layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n             self.assertTrue(layer.supports_masking)\n-        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+            query = np.array(\n+                [[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]]\n+            )\n             masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n             value = np.random.normal(size=(3, 3, 8))\n             output = layer(query=masked_query, value=value)\n+        except RuntimeError as e:\n+            if e.args[0].startswith(\n+                \"(*bias): last dimension must be contiguous\"\n+            ):\n+                self.skipTest(\n+                    \"PyTorch errors out on GPU: issue to track bug is here \"\n+                    \"https://github.com/keras-team/keras/issues/20459\"\n+                )\n         self.assertAllClose(masked_query._keras_mask, output._keras_mask)\n \n     @parameterized.named_parameters((\"causal\", True), (\"not_causal\", 0))\n@@ -252,7 +338,6 @@ class MultiHeadAttentionTest(testing.TestCase):\n         bias = np.zeros((2, 2))\n         output_bias = np.zeros((2,))\n         layer.set_weights([kernel, bias] * 3 + [kernel, output_bias])\n-\n         # Call layer and assert output.\n         output, scores = layer(\n             query=query,\n@@ -413,3 +498,55 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertDType(layer._query_dense._kernel, \"int8\")\n         self.assertDType(layer._key_dense._kernel, \"int8\")\n         self.assertDType(layer._value_dense._kernel, \"int8\")\n+\n+    def test_flash_attention_with_attention_scores_error(self):\n+        # Enable flash attention globally\n+        if backend.backend() == \"numpy\" or \"tensorflow\":\n+            pytest.skip(\n+                reason=\"Flash attention is not supported on Tensorflow\"\n+                \"and numpy.\"\n+            )\n+        # Setup layer with required parameters\n+        layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+\n+        # Define sample input\n+        query = np.random.random((2, 4, 8))\n+        value = np.random.random((2, 4, 8))\n+\n+        # Check if ValueError is raised when return_attention_scores=True\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Returning attention scores is not supported when flash \"\n+            \"attention is enabled. Please disable flash attention to access\"\n+            \" attention scores.\",\n+        ):\n+            layer(query=query, value=value, return_attention_scores=True)\n+\n+    def test_flash_attention_numerical_correctness(self):\n+        if backend.backend() == \"numpy\" or backend.backend() == \"tensorflow\":\n+            pytest.skip(\n+                reason=\"Flash attention is not supported on Tensorflow \"\n+                \"and numpy.\"\n+            )\n+        # Create sample input data\n+        # Define sample input\n+        query = np.random.random((2, 4, 8))\n+        value = np.random.random((2, 4, 8))\n+\n+        # Initialize MultiHeadAttention layer\n+        mha_layer = layers.MultiHeadAttention(\n+            num_heads=2,\n+            key_dim=2,\n+        )\n+\n+        # Run with flash attention enabled\n+        enable_flash_attention()\n+        output_with_flash = mha_layer(query=query, value=value, training=False)\n+\n+        disable_flash_attention()\n+        # Run with flash attention disabled\n+        output_without_flash = mha_layer(\n+            query=query, value=value, training=False\n+        )\n+\n+        self.assertAllClose(output_with_flash, output_without_flash)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
