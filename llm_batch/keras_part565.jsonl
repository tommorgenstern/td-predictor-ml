{"custom_id": "keras#302b8da880d5589a4e206d61b97f1c6b848d751b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 502 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -185,13 +185,13 @@ class SeparableConv2D(SeparableConv):\n             strides = (1,) + self.strides + (1,)\n         else:\n             strides = (1, 1) + self.strides\n-        outputs = tf.compat.v1.nn.separable_conv2d(\n+        outputs = tf.nn.separable_conv2d(\n             inputs,\n             self.depthwise_kernel,\n             self.pointwise_kernel,\n             strides=strides,\n             padding=self.padding.upper(),\n-            rate=self.dilation_rate,\n+            dilations=self.dilation_rate,\n             data_format=conv_utils.convert_data_format(\n                 self.data_format, ndim=4\n             ),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3eeae02494bbbcd034405a6eb0651333547e7cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 6967 | Contributors (this commit): 5 | Commits (past 90d): 32 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,8 @@ class DropoutTest(testing.TestCase):\n         self.assertAllClose(np.max(outputs), 2.0)\n \n     @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\", reason=\"Requires dynamic shapes\"\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n     )\n     def test_dropout_partial_noise_shape_dynamic(self):\n         inputs = np.ones((20, 5, 10))\n\n@@ -2,15 +2,15 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n+from keras_core import backend\n from keras_core import testing\n-from keras_core.backend import backend\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.operations import nn as knn\n \n \n @pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+    not backend.DYNAMIC_SHAPES_OK,\n+    reason=\"Backend does not support dynamic shapes\",\n )\n class NNOpsDynamicShapeTest(testing.TestCase):\n     def test_relu(self):\n\n@@ -2,8 +2,8 @@ import numpy as np\n import pytest\n from tensorflow.python.ops.numpy_ops import np_config\n \n+from keras_core import backend\n from keras_core import testing\n-from keras_core.backend import backend\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.operations import numpy as knp\n \n@@ -12,8 +12,8 @@ np_config.enable_numpy_behavior()\n \n \n @pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+    not backend.DYNAMIC_SHAPES_OK,\n+    reason=\"Backend does not support dynamic shapes\",\n )\n class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n     def test_add(self):\n@@ -624,8 +624,8 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n \n \n @pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+    not backend.DYNAMIC_SHAPES_OK,\n+    reason=\"Backend does not support dynamic shapes\",\n )\n class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n     def test_mean(self):\n@@ -2635,7 +2635,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.meshgrid(x, y, z, indexing=\"ij\"),\n         )\n \n-        if backend() == \"tensorflow\":\n+        if backend.backend() == \"tensorflow\":\n             # Arguments to `jax.numpy.meshgrid` must be 1D now.\n             x = np.ones([1, 2, 3])\n             y = np.ones([4, 5, 6, 6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1989342ad29cacce201f0323bcce669578d570b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 403 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 4 | Methods Changed: 14 | Complexity Δ (Sum/Max): 59/52 | Churn Δ: 403 | Churn Cumulative: 425 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 0.35177865612648224\n\nDIFF:\n@@ -4,6 +4,8 @@ from keras_core.layers.core.embedding import Embedding\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n from keras_core.layers.layer import Layer\n+from keras_core.layers.merging.add import Add\n+from keras_core.layers.merging.add import add\n from keras_core.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n\n\n@@ -0,0 +1,68 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Add\")\n+class Add(Merge):\n+    \"\"\"Performs elementwise addition operation.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Add()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `added = keras_core.layers.add([x1, x2])`\n+    >>> added = keras_core.layers.Add()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(added)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output += inputs[i]\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.layers.add\")\n+def add(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Add` layer.\n+\n+    Args:\n+        inputs: A list of input tensors with the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the sum of the inputs. It has the same shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.add([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> added = keras_core.layers.add([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(added)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Add(**kwargs)(inputs)\n\n@@ -0,0 +1,232 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.layers.layer import Layer\n+\n+\n+class Merge(Layer):\n+    \"\"\"Generic merge layer for elementwise merge functions.\n+\n+    Used to implement `Sum`, `Average`, etc.\n+\n+    Args:\n+        **kwargs: standard layer keyword arguments.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+\n+    def _merge_function(self, inputs):\n+        raise NotImplementedError\n+\n+    def _compute_elemwise_op_output_shape(self, shape1, shape2):\n+        \"\"\"Computes the shape of the resultant of an elementwise operation.\n+\n+        Args:\n+            shape1: Tuple or None. Shape of the first tensor\n+            shape2: Tuple or None. Shape of the second tensor\n+\n+        Returns:\n+            Expected output shape when an element-wise operation is\n+            carried out on 2 tensors with shapes shape1 and shape2.\n+            tuple or None.\n+\n+        Raises:\n+            ValueError: If shape1 and shape2 are not compatible for\n+                element-wise operations.\n+        \"\"\"\n+\n+        if None in [shape1, shape2]:\n+            return None\n+        elif len(shape1) < len(shape2):\n+            return self._compute_elemwise_op_output_shape(shape2, shape1)\n+        elif not shape2:\n+            return shape1\n+        output_shape = list(shape1[: -len(shape2)])\n+        for i, j in zip(shape1[-len(shape2) :], shape2):\n+            if i is None or j is None:\n+                output_shape.append(None)\n+            elif i == 1:\n+                output_shape.append(j)\n+            elif j == 1:\n+                output_shape.append(i)\n+            else:\n+                if i != j:\n+                    raise ValueError(\n+                        \"Inputs have incompatible shapes. \"\n+                        f\"Received shapes {shape1} and {shape2}\"\n+                    )\n+                output_shape.append(i)\n+        return tuple(output_shape)\n+\n+    def build(self, input_shape):\n+        # Used purely for shape validation.\n+        if not isinstance(input_shape[0], tuple):\n+            raise ValueError(\n+                \"A merge layer should be called on a list of inputs. \"\n+                f\"Received: input_shape={input_shape} (not a list of shapes)\"\n+            )\n+        if len(input_shape) < 1:\n+            raise ValueError(\n+                \"A merge layer should be called \"\n+                \"on a list of at least 1 input. \"\n+                f\"Received {len(input_shape)} inputs. \"\n+                f\"Full input_shape received: {input_shape}\"\n+            )\n+\n+        batch_sizes = {s[0] for s in input_shape if s} - {None}\n+        if len(batch_sizes) > 1:\n+            raise ValueError(\n+                \"Cannot merge tensors with different batch sizes. \"\n+                f\"Received tensors with shapes {input_shape}\"\n+            )\n+\n+        if input_shape[0] is None:\n+            output_shape = None\n+        else:\n+            output_shape = input_shape[0][1:]\n+\n+        for i in range(1, len(input_shape)):\n+            if input_shape[i] is None:\n+                shape = None\n+            else:\n+                shape = input_shape[i][1:]\n+            output_shape = self._compute_elemwise_op_output_shape(\n+                output_shape, shape\n+            )\n+\n+        # If the inputs have different ranks, we have to reshape them\n+        # to make them broadcastable.\n+        if None not in input_shape and len(set(map(len, input_shape))) == 1:\n+            self._reshape_required = False\n+        else:\n+            self._reshape_required = True\n+        self.built = True\n+\n+    def call(self, inputs):\n+        if not isinstance(inputs, (list, tuple)):\n+            raise ValueError(\n+                \"A merge layer should be called on a list of inputs. \"\n+                f\"Received: inputs={inputs} (not a list of tensors)\"\n+            )\n+        if self._reshape_required:\n+            reshaped_inputs = []\n+            input_ndims = list(map(ops.ndim, inputs))\n+            if None not in input_ndims:\n+                # If ranks of all inputs are available,\n+                # we simply expand each of them at axis=1\n+                # until all of them have the same rank.\n+                max_ndim = max(input_ndims)\n+                for x in inputs:\n+                    x_ndim = ops.ndim(x)\n+                    for _ in range(max_ndim - x_ndim):\n+                        x = ops.expand_dims(x, axis=1)\n+                    reshaped_inputs.append(x)\n+                return self._merge_function(reshaped_inputs)\n+            else:\n+                # Transpose all inputs so that batch size is the last dimension.\n+                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... ,\n+                # batch_size)\n+                transposed = False\n+                for x in inputs:\n+                    x_ndim = ops.ndim(x)\n+\n+                    if x_ndim is None:\n+                        x_shape = ops.shape(x)\n+                        batch_size = x_shape[0]\n+\n+                        new_shape = backend.concatenate(\n+                            [x_shape[1:], ops.expand_dims(batch_size, axis=-1)]\n+                        )\n+                        x_transposed = ops.reshape(\n+                            x,\n+                            ops.stack(\n+                                [batch_size, ops.prod(x_shape[1:])],\n+                                axis=0,\n+                            ),\n+                        )\n+                        x_transposed = ops.transpose(x_transposed, perm=(1, 0))\n+                        x_transposed = ops.reshape(x_transposed, new_shape)\n+\n+                        reshaped_inputs.append(x_transposed)\n+                        transposed = True\n+\n+                    elif x_ndim > 1:\n+                        dims = list(range(1, x_ndim)) + [0]\n+                        reshaped_inputs.append(ops.transpose(x, perm=dims))\n+                        print(dims)\n+                        transposed = True\n+                    else:\n+                        # We don't transpose inputs if they are 1D vectors or\n+                        # scalars.\n+                        reshaped_inputs.append(x)\n+\n+                y = self._merge_function(reshaped_inputs)\n+                y_ndim = ops.ndim(y)\n+\n+                if transposed:\n+                    # If inputs have been transposed, we have to transpose the\n+                    # output too.\n+                    if y_ndim is None:\n+                        y_shape = ops.shape(y)\n+                        y_ndim = ops.shape(y_shape)[0]\n+                        batch_size = y_shape[y_ndim - 1]\n+                        new_shape = ops.concatenate(\n+                            [\n+                                ops.expand_dims(batch_size, axis=-1),\n+                                y_shape[: y_ndim - 1],\n+                            ]\n+                        )\n+                        y = ops.reshape(y, (-1, batch_size))\n+                        y = ops.transpose(y, perm=(1, 0))\n+                        y = ops.reshape(y, new_shape)\n+                    elif y_ndim > 1:\n+                        dims = [y_ndim - 1] + list(range(y_ndim - 1))\n+                        y = ops.transpose(y, perm=dims)\n+                return y\n+        else:\n+            return self._merge_function(inputs)\n+\n+    def compute_output_shape(self, input_shape):\n+        if input_shape[0] is None:\n+            output_shape = None\n+        else:\n+            output_shape = input_shape[0][1:]\n+\n+        for i in range(1, len(input_shape)):\n+            if input_shape[i] is None:\n+                shape = None\n+            else:\n+                shape = input_shape[i][1:]\n+            output_shape = self._compute_elemwise_op_output_shape(\n+                output_shape, shape\n+            )\n+        batch_sizes = {s[0] for s in input_shape if s is not None} - {None}\n+        if len(batch_sizes) == 1:\n+            output_shape = (list(batch_sizes)[0],) + output_shape\n+        else:\n+            output_shape = (None,) + output_shape\n+        return output_shape\n+\n+    def compute_mask(self, inputs, mask=None):\n+        if mask is None:\n+            return None\n+        if not isinstance(mask, (tuple, list)):\n+            raise ValueError(f\"`mask` should be a list. Received: mask={mask}\")\n+        if not isinstance(inputs, (tuple, list)):\n+            raise ValueError(\n+                f\"`inputs` should be a list. Received: inputs={inputs}\"\n+            )\n+        if len(mask) != len(inputs):\n+            raise ValueError(\n+                \"The lists `inputs` and `mask` should have the same length. \"\n+                f\"Received: inputs={inputs} of length {len(inputs)}, and \"\n+                f\"mask={mask} of length {len(mask)}\"\n+            )\n+        if all(m is None for m in mask):\n+            return None\n+        masks = [ops.expand_dims(m, axis=0) for m in mask if m is not None]\n+        return ops.all(ops.concatenate(masks, axis=0), axis=0, keepdims=False)\n+\n+    def get_config(self):\n+        return super().get_config()\n\n@@ -0,0 +1,101 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import models\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class MergingLayersTest(testing.TestCase):\n+    def test_add_basic(self):\n+        self.run_layer_test(\n+            layers.Add,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+    )\n+    def test_add_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.convert_to_tensor(x1 + x2)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        add_layer = layers.Add()\n+        out = add_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            add_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                add_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_add_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.convert_to_tensor(x1 + x2)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        add_layer = layers.Add()\n+        out = add_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            add_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                add_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_add_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        add_layer = layers.Add()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            add_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            add_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            add_layer.compute_mask([input_1, input_2], [None])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb9e71f2f1aa9a1c71e273497d668673036f98a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1010 | Lines Deleted: 102 | Files Changed: 1 | Hunks: 33 | Methods Changed: 20 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 1112 | Churn Cumulative: 4633 | Contributors (this commit): 7 | Commits (past 90d): 25 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n@@ -43,10 +45,10 @@ class MeanSquaredError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -69,10 +71,10 @@ class MeanAbsoluteError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -95,10 +97,10 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -125,10 +127,10 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -164,10 +166,10 @@ class CosineSimilarity(LossFunctionWrapper):\n     Args:\n         axis: The axis along which the cosine similarity is computed\n             (the features axis). Defaults to -1.\n-        reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`. Defaults to\n-            `\"sum_over_batch_size\"`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -195,10 +197,10 @@ class Hinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n@@ -222,10 +224,10 @@ class SquaredHinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"squared_hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n@@ -248,11 +250,10 @@ class CategoricalHinge(LossFunctionWrapper):\n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to\n-            `\"categorical_hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -275,10 +276,10 @@ class KLDivergence(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to 'kl_divergence'.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\"):\n@@ -299,10 +300,10 @@ class Poisson(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"poisson\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"poisson\"):\n@@ -312,6 +313,572 @@ class Poisson(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n+@keras_core_export(\"keras_core.losses.BinaryCrossentropy\")\n+class BinaryCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n+\n+    Use this cross-entropy loss for binary (0 or 1) classification applications.\n+    The loss function requires the following inputs:\n+\n+    - `y_true` (true label): This is either 0 or 1.\n+    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n+      floating-point value which either represents a\n+      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n+      when `from_logits=True`) or a probability (i.e, value in [0., 1.] when\n+      `from_logits=False`).\n+\n+    Args:\n+        from_logits: Whether to interpret `y_pred` as a tensor of\n+            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n+            assume that `y_pred` contains probabilities (i.e., values in [0,\n+            1]).\n+        label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.\n+            When > 0, we compute the loss between the predicted labels\n+            and a smoothed version of the true labels, where the smoothing\n+            squeezes the labels towards 0.5. Larger values of\n+            `label_smoothing` correspond to heavier smoothing.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis).  Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    **Recommended Usage:** (set `from_logits=True`)\n+\n+    With `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n+        ...\n+    )\n+    ```\n+\n+    As a standalone function:\n+\n+    >>> # Example 1: (batch_size = 1, number of samples = 4)\n+    >>> y_true = [0, 1, 0, 0]\n+    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce(y_true, y_pred)\n+    0.865\n+\n+    >>> # Example 2: (batch_size = 2, number of samples = 4)\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce(y_true, y_pred)\n+    0.865\n+    >>> # Using 'sample_weight' attribute\n+    >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.243\n+    >>> # Using 'sum' reduction` type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> bce(y_true, y_pred)\n+    1.730\n+    >>> # Using 'none' reduction type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    ...     reduction=None)\n+    >>> bce(y_true, y_pred)\n+    array([0.235, 1.496], dtype=float32)\n+\n+    **Default Usage:** (set `from_logits=False`)\n+\n+    >>> # Make the following updates to the above \"Recommended Usage\" section\n+    >>> # 1. Set `from_logits=False`\n+    >>> keras_core.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n+    >>> # 2. Update `y_pred` to use probabilities instead of logits\n+    >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"binary_crossentropy\",\n+    ):\n+        super().__init__(\n+            binary_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+\n+\n+@keras_core_export(\"keras_core.losses.BinaryFocalCrossentropy\")\n+class BinaryFocalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes focal cross-entropy loss between true labels and predictions.\n+\n+    Binary cross-entropy loss is often used for binary (0 or 1) classification\n+    tasks. The loss function requires the following inputs:\n+\n+    - `y_true` (true label): This is either 0 or 1.\n+    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n+      floating-point value which either represents a\n+      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n+      when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when\n+      `from_logits=False`).\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a \"focal factor\" to down-weight easy examples and focus more\n+    on hard examples. By default, the focal tensor is computed as follows:\n+\n+    `focal_factor = (1 - output) ** gamma` for class 1\n+    `focal_factor = output ** gamma` for class 0\n+    where `gamma` is a focusing parameter. When `gamma=0`, this function is\n+    equivalent to the binary crossentropy loss.\n+\n+    Args:\n+        apply_class_balancing: A bool, whether to apply weight balancing on the\n+            binary classes 0 and 1.\n+        alpha: A weight balancing factor for class 1, default is `0.25` as\n+            mentioned in reference [Lin et al., 2018](\n+            https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is\n+            `1.0 - alpha`.\n+        gamma: A focusing parameter used to compute the focal factor, default is\n+            `2.0` as mentioned in the reference\n+            [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+        from_logits: Whether to interpret `y_pred` as a tensor of\n+            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n+            assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).\n+        label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs.\n+            When > `0`, we compute the loss between the predicted labels\n+            and a smoothed version of the true labels, where the smoothing\n+            squeezes the labels towards `0.5`.\n+            Larger values of `label_smoothing` correspond to heavier smoothing.\n+        axis: The axis along which to compute crossentropy (the features axis).\n+            Defaults to `-1`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    With the `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        loss=keras_core.losses.BinaryFocalCrossentropy(\n+            gamma=2.0, from_logits=True),\n+        ...\n+    )\n+    ```\n+\n+    As a standalone function:\n+\n+    >>> # Example 1: (batch_size = 1, number of samples = 4)\n+    >>> y_true = [0, 1, 0, 0]\n+    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...    gamma=2, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.691\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=2, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.51\n+\n+    >>> # Example 2: (batch_size = 2, number of samples = 4)\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.647\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.482\n+\n+    >>> # Using 'sample_weight' attribute with focal effect\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.133\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.097\n+\n+    >>> # Using 'sum' reduction` type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=4, from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> loss(y_true, y_pred)\n+    1.222\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=4, from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> loss(y_true, y_pred)\n+    0.914\n+\n+    >>> # Using 'none' reduction type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=5, from_logits=True,\n+    ...     reduction=None)\n+    >>> loss(y_true, y_pred)\n+    array([0.0017 1.1561], dtype=float32)\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=5, from_logits=True,\n+    ...     reduction=None)\n+    >>> loss(y_true, y_pred)\n+    array([0.0004 0.8670], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        apply_class_balancing=False,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"binary_focal_crossentropy\",\n+    ):\n+        \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n+        super().__init__(\n+            binary_focal_crossentropy,\n+            apply_class_balancing=apply_class_balancing,\n+            alpha=alpha,\n+            gamma=gamma,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.apply_class_balancing = apply_class_balancing\n+        self.alpha = alpha\n+        self.gamma = gamma\n+\n+    def get_config(self):\n+        config = {\n+            \"apply_class_balancing\": self.apply_class_balancing,\n+            \"alpha\": self.alpha,\n+            \"gamma\": self.gamma,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.losses.CategoricalCrossentropy\")\n+class CategoricalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the crossentropy loss between the labels and predictions.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes. We expect labels to be provided in a `one_hot` representation. If\n+    you want to provide labels as integers, please use\n+    `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n+    point values per feature.\n+\n+    In the snippet below, there is `# classes` floating pointing values per\n+    example. The shape of both `y_pred` and `y_true` are\n+    `[batch_size, num_classes]`.\n+\n+    Args:\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed. For example, if\n+            `0.1`, use `0.1 / num_classes` for non-target labels and\n+            `0.9 + 0.1 / num_classes` for target labels.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy()\n+    >>> cce(y_true, y_pred)\n+    1.177\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.814\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> cce(y_true, y_pred)\n+    2.354\n+\n+    >>> # Using 'none' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    ...     reduction=None)\n+    >>> cce(y_true, y_pred)\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.CategoricalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"categorical_crossentropy\",\n+    ):\n+        super().__init__(\n+            categorical_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.losses.CategoricalFocalCrossentropy\")\n+class CategoricalFocalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the alpha balanced focal crossentropy loss.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes and if you want to handle class imbalance without using\n+    `class_weights`. We expect labels to be provided in a `one_hot`\n+    representation.\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a focal factor to down-weight easy examples and focus more on\n+    hard examples. The general formula for the focal loss (FL)\n+    is as follows:\n+\n+    `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`\n+\n+    where `p_t` is defined as follows:\n+    `p_t = output if y_true == 1, else 1 - output`\n+\n+    `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.\n+    `gamma` reduces the importance given to simple examples in a smooth manner.\n+\n+    The authors use alpha-balanced variant of focal loss (FL) in the paper:\n+    `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`\n+\n+    where `alpha` is the weight factor for the classes. If `alpha` = 1, the\n+    loss won't be able to handle class imbalance properly as all\n+    classes will have the same weight. This can be a constant or a list of\n+    constants. If alpha is a list, it must have the same length as the number\n+    of classes.\n+\n+    The formula above can be generalized to:\n+    `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`\n+\n+    where minus comes from `CrossEntropy(y_true, y_pred)` (CE).\n+\n+    Extending this to multi-class case is straightforward:\n+    `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`\n+\n+    In the snippet below, there is `# classes` floating pointing values per\n+    example. The shape of both `y_pred` and `y_true` are\n+    `(batch_size, num_classes)`.\n+\n+    Args:\n+        alpha: A weight balancing factor for all classes, default is `0.25` as\n+            mentioned in the reference. It can be a list of floats or a scalar.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference. It helps to gradually reduce the importance given to\n+            simple (easy) examples in a smooth manner.\n+        from_logits: Whether `output` is expected to be a logits tensor. By\n+            default, we consider that `output` encodes a probability\n+            distribution.\n+        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed. For example, if\n+            `0.1`, use `0.1 / num_classes` for non-target labels and\n+            `0.9 + 0.1 / num_classes` for target labels.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy()\n+    >>> cce(y_true, y_pred)\n+    0.23315276\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.1632\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> cce(y_true, y_pred)\n+    0.46631\n+\n+    >>> # Using 'none' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    ...     reduction=None)\n+    >>> cce(y_true, y_pred)\n+    array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=keras_core.losses.CategoricalFocalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"categorical_focal_crossentropy\",\n+    ):\n+        \"\"\"Initializes `CategoricalFocalCrossentropy` instance.\"\"\"\n+        super().__init__(\n+            categorical_focal_crossentropy,\n+            alpha=alpha,\n+            gamma=gamma,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.alpha = alpha\n+        self.gamma = gamma\n+\n+    def get_config(self):\n+        config = {\n+            \"alpha\": self.alpha,\n+            \"gamma\": self.gamma,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.losses.SparseCategoricalCrossentropy\")\n+class SparseCategoricalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the crossentropy loss between the labels and predictions.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes.  We expect labels to be provided as integers. If you want to\n+    provide labels using `one-hot` representation, please use\n+    `CategoricalCrossentropy` loss.  There should be `# classes` floating point\n+    values per feature for `y_pred` and a single floating point value per\n+    feature for `y_true`.\n+\n+    In the snippet below, there is a single floating point value per example for\n+    `y_true` and `# classes` floating pointing values per example for `y_pred`.\n+    The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n+    `[batch_size, num_classes]`.\n+\n+    Args:\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255) in\n+            segmentation maps.\n+            By default (`ignore_class=None`), all classes are considered.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    >>> y_true = [1, 2]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy()\n+    >>> scce(y_true, y_pred)\n+    1.177\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.814\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> scce(y_true, y_pred)\n+    2.354\n+\n+    >>> # Using 'none' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    ...     reduction=None)\n+    >>> scce(y_true, y_pred)\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.SparseCategoricalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        ignore_class=None,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"sparse_categorical_crossentropy\",\n+    ):\n+        super().__init__(\n+            sparse_categorical_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            ignore_class=ignore_class,\n+        )\n+\n+\n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n     are_zeros = ops.equal(y_true, 0)\n@@ -347,12 +914,6 @@ def hinge(y_true, y_pred):\n     loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided they will be converted\n@@ -361,6 +922,12 @@ def hinge(y_true, y_pred):\n \n     Returns:\n         Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, dtype=y_pred.dtype)\n@@ -384,12 +951,6 @@ def squared_hinge(y_true, y_pred):\n     loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided we will convert them\n@@ -398,6 +959,12 @@ def squared_hinge(y_true, y_pred):\n \n     Returns:\n         Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -424,13 +991,6 @@ def categorical_hinge(y_true, y_pred):\n \n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 3, size=(2,))\n-    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be\n             either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with\n@@ -439,6 +999,13 @@ def categorical_hinge(y_true, y_pred):\n \n     Returns:\n         Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 3, size=(2,))\n+    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -495,18 +1062,18 @@ def mean_absolute_error(y_true, y_pred):\n     loss = mean(abs(y_true - y_pred), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n         Mean absolute error values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -523,18 +1090,16 @@ def mean_absolute_error(y_true, y_pred):\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n-    `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n+    Formula:\n+\n+    ```python\n+    loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)\n+    ```\n \n     Division by zero is prevented by dividing by `maximum(y_true, epsilon)`\n     where `epsilon = keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.random(size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n@@ -542,6 +1107,12 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     Returns:\n         Mean absolute percentage error values with shape = `[batch_size, d0, ..\n         dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.random(size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -570,19 +1141,19 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     values and 0 values will be replaced with `keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n-        Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n+        Mean squared logarithmic error values with shape = `[batch_size, d0, ..\n         dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -610,12 +1181,6 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     similarity will be 0 regardless of the proximity between predictions\n     and targets.\n \n-    Standalone usage:\n-    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n-    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n-    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n-    [-0., -0.99999994, 0.99999994]\n-\n     Args:\n         y_true: Tensor of true targets.\n         y_pred: Tensor of predicted targets.\n@@ -623,6 +1188,13 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n \n     Returns:\n         Cosine similarity tensor.\n+\n+    Example:\n+\n+    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n+    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n+    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    [-0., -0.99999994, 0.99999994]\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -647,7 +1219,14 @@ def kl_divergence(y_true, y_pred):\n     loss = y_true * log(y_true / y_pred)\n     ```\n \n-    Standalone usage:\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+\n+    Returns:\n+        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -657,13 +1236,6 @@ def kl_divergence(y_true, y_pred):\n     >>> y_pred = ops.clip(y_pred, 1e-7, 1)\n     >>> assert np.array_equal(\n     ...     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n-\n-    Args:\n-        y_true: Tensor of true targets.\n-        y_pred: Tensor of predicted targets.\n-\n-    Returns:\n-        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, y_pred.dtype)\n@@ -687,7 +1259,14 @@ def poisson(y_true, y_pred):\n     loss = y_pred - y_true * log(y_pred)\n     ```\n \n-    Standalone usage:\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+\n+    Returns:\n+        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -697,15 +1276,344 @@ def poisson(y_true, y_pred):\n     >>> assert np.allclose(\n     ...     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n     ...     atol=1e-5)\n-\n-    Args:\n-        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.categorical_crossentropy\",\n+        \"keras_core.losses.categorical_crossentropy\",\n+    ]\n+)\n+def categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n+):\n+    \"\"\"Computes the categorical crossentropy loss.\n+\n+    Args:\n+        y_true: Tensor of one-hot true targets.\n+        y_pred: Tensor of predicted targets.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+\n+    Returns:\n+        Categorical crossentropy loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> loss = keras_core.losses.categorical_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss.numpy()\n+    array([0.0513, 2.303], dtype=float32)\n+    \"\"\"\n+    if isinstance(axis, bool):\n+        raise ValueError(\n+            \"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n+        )\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"In loss categorical_crossentropy, expected \"\n+            \"y_pred.shape to be (batch_size, num_classes) \"\n+            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n+            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n+            SyntaxWarning,\n+            stacklevel=2,\n+        )\n+\n+    if label_smoothing:\n+        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n+        y_true = y_true * (1.0 - label_smoothing) + (\n+            label_smoothing / num_classes\n+        )\n+\n+    return ops.categorical_crossentropy(\n+        y_true, y_pred, from_logits=from_logits, axis=axis\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.categorical_focal_crossentropy\",\n+        \"keras_core.losses.categorical_focal_crossentropy\",\n+    ]\n+)\n+def categorical_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.0,\n+    axis=-1,\n+):\n+    \"\"\"Computes the categorical focal crossentropy loss.\n+\n+    Args:\n+        y_true: Tensor of one-hot true targets.\n+        y_pred: Tensor of predicted targets.\n+        alpha: A weight balancing factor for all classes, default is `0.25` as\n+            mentioned in the reference. It can be a list of floats or a scalar.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference. It helps to gradually reduce the importance given to\n+            simple examples in a smooth manner. When `gamma` = 0, there is\n+            no focal effect on the categorical crossentropy.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability\n+            distribution.\n+        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+\n+    Returns:\n+        Categorical focal crossentropy loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n+    >>> loss = keras_core.losses.categorical_focal_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n+    \"\"\"\n+    if isinstance(axis, bool):\n+        raise ValueError(\n+            \"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n+        )\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"In loss categorical_focal_crossentropy, expected \"\n+            \"y_pred.shape to be (batch_size, num_classes) \"\n+            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n+            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n+            SyntaxWarning,\n+            stacklevel=2,\n+        )\n+\n+    if label_smoothing:\n+        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n+        y_true = y_true * (1.0 - label_smoothing) + (\n+            label_smoothing / num_classes\n+        )\n+\n+    return ops.categorical_focal_crossentropy(\n+        target=y_true,\n+        output=y_pred,\n+        alpha=alpha,\n+        gamma=gamma,\n+        from_logits=from_logits,\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.sparse_categorical_crossentropy\",\n+        \"keras_core.losses.sparse_categorical_crossentropy\",\n+    ]\n+)\n+def sparse_categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n+):\n+    \"\"\"Computes the sparse categorical crossentropy loss.\n+\n+    Args:\n+        y_true: Ground truth values.\n+        y_pred: The predicted values.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+        ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255)\n+            in segmentation maps. By default (`ignore_class=None`),\n+            all classes are considered.\n+\n+    Returns:\n+        Sparse categorical crossentropy loss value.\n+\n+    Examples:\n+\n+    >>> y_true = [1, 2]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> loss = keras_core.losses.sparse_categorical_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    >>> y_true = [[[ 0,  2],\n+    ...            [-1, -1]],\n+    ...           [[ 0,  2],\n+    ...            [-1, -1]]]\n+    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n+    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n+    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n+    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n+    >>> loss = keras_core.losses.sparse_categorical_crossentropy(\n+    ...   y_true, y_pred, ignore_class=-1)\n+    array([[[2.3841855e-07, 2.3841855e-07],\n+            [0.0000000e+00, 0.0000000e+00]],\n+           [[2.3841855e-07, 6.9314730e-01],\n+            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n+    \"\"\"\n+    return ops.sparse_categorical_crossentropy(\n+        y_true,\n+        y_pred,\n+        from_logits=from_logits,\n+        ignore_class=ignore_class,\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.binary_crossentropy\",\n+        \"keras_core.losses.binary_crossentropy\",\n+    ]\n+)\n+def binary_crossentropy(\n+    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n+):\n+    \"\"\"Computes the binary crossentropy loss.\n+\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n+            squeezing them towards 0.5, that is,\n+            using `1. - 0.5 * label_smoothing` for the target class\n+            and `0.5 * label_smoothing` for the non-target class.\n+        axis: The axis along which the mean is computed. Defaults to -1.\n+\n+    Returns:\n+        Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n+    >>> loss = keras_core.losses.binary_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.916 , 0.714], dtype=float32)\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if label_smoothing:\n+        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+    return ops.mean(\n+        ops.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.binary_focal_crossentropy\",\n+        \"keras_core.losses.binary_focal_crossentropy\",\n+    ]\n+)\n+def binary_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    apply_class_balancing=False,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.0,\n+    axis=-1,\n+):\n+    \"\"\"Computes the binary focal crossentropy loss.\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a focal factor to down-weight easy examples and focus more on\n+    hard examples. By default, the focal tensor is computed as follows:\n+\n+    `focal_factor = (1 - output)**gamma` for class 1\n+    `focal_factor = output**gamma` for class 0\n+    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+    effect on the binary crossentropy loss.\n+\n+    If `apply_class_balancing == True`, this function also takes into account a\n+    weight balancing factor for the binary classes 0 and 1 as follows:\n+\n+    `weight = alpha` for class 1 (`target == 1`)\n+    `weight = 1 - alpha` for class 0\n+    where `alpha` is a float in the range of `[0, 1]`.\n+\n+    Args:\n+        y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n+        y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n+        apply_class_balancing: A bool, whether to apply weight balancing on the\n+            binary classes 0 and 1.\n+        alpha: A weight balancing factor for class 1, default is `0.25` as\n+            mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n+            squeezing them towards 0.5, that is,\n+            using `1. - 0.5 * label_smoothing` for the target class\n+            and `0.5 * label_smoothing` for the non-target class.\n+        axis: The axis along which the mean is computed. Defaults to `-1`.\n+\n+    Returns:\n+        Binary focal crossentropy loss value\n+        with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n+    >>> loss = keras_core.losses.binary_focal_crossentropy(\n+    ...        y_true, y_pred, gamma=2)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.330, 0.206], dtype=float32)\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if label_smoothing:\n+        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+    return ops.mean(\n+        ops.binary_focal_crossentropy(\n+            target=y_true,\n+            output=y_pred,\n+            apply_class_balancing=apply_class_balancing,\n+            alpha=alpha,\n+            gamma=gamma,\n+            from_logits=from_logits,\n+        ),\n+        axis=axis,\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
