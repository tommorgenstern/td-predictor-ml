{"custom_id": "scrapy#aeb95200a11b6de305be8798d9ecdf35613f25c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 37 | Churn Cumulative: 353 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: 0.4\n\nDIFF:\n@@ -19,31 +19,33 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n         parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", help=\"try to match and parse the url with the defined rules (if any)\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", help=\"use the provided callback for parsing the url\")\n+        parser.add_option(\"-c\", \"--callbacks\", dest=\"callbacks\", action=\"store\", help=\"use the provided callback(s) for parsing the url (separated with commas)\")\n+\n+    def process_options(self, args, opts):\n+        self.callbacks = opts.callbacks.split(',') if opts.callbacks else []\n \n     def pipeline_process(self, item, spider, opts):\n         return item\n \n-    def run_method(self, spider, response, method, args, opts):\n+    def run_callback(self, spider, response, callback, args, opts):\n         spider = spiders.fromurl(response.url)\n         if not spider:\n             log.msg('Cannot find spider for url: %s' % response.url, level=log.ERROR)\n             return (), ()\n \n-        items = []\n-        links = []\n-        if method:\n-            method_fcn = method if callable(method) else getattr(spider, method, None)\n-            if not method_fcn:\n-                log.msg('Cannot find method %s in %s spider' % (method, spider.domain_name))\n+        if callback:\n+            callback_fcn = callback if callable(callback) else getattr(spider, callback, None)\n+            if not callback_fcn:\n+                log.msg('Cannot find callback %s in %s spider' % (callback, spider.domain_name))\n                 return (), ()\n \n-            result = method_fcn(response)\n+            result = callback_fcn(response)\n             links = [i for i in result if isinstance(i, Request)]\n             items = [self.pipeline_process(i, spider, opts) for i in result if isinstance(i, ScrapedItem)]\n-\n             return items, links\n \n+        return (), ()\n+\n     def print_results(self, items, links, opts):\n         display.nocolour = opts.nocolour\n         if not opts.noitems:\n@@ -70,18 +72,23 @@ class Command(ScrapyCommand):\n                 log.msg('Cannot find spider for \"%s\"' % response.url)\n                 continue\n \n-            if opts.callback:\n-                items, links = self.run_method(spider, response, opts.callback, args, opts)\n+            if self.callbacks:\n+                items, links = [], []\n+                for callback in self.callbacks:\n+                    r_items, r_links = self.run_callback(spider, response, callback, args, opts)\n+                    items.extend(r_items)\n+                    links.extend(r_links)\n+\n             elif opts.rules:\n                 for rule in getattr(spider, 'rules', ()):\n                     if rule.link_extractor.matches(response.url):\n-                        items, links = self.run_method(spider, response, rule.callback, args, opts)\n+                        items, links = self.run_callback(spider, response, rule.callback, args, opts)\n                         break\n                 else:\n-                    log.msg('No rules found for spider \"%s\", please specify a parsing method' % spider.domain_name)\n+                    log.msg('No rules found for spider \"%s\", please specify a parsing callback' % spider.domain_name)\n                     continue\n             else:\n-                items, links = self.run_method(spider, response, 'parse', args, opts)\n+                items, links = self.run_callback(spider, response, 'parse', args, opts)\n \n             ret_items.extend(items)\n             ret_links.extend(links)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f373fc0f7cc8e9926136549078376ad9b76f1a61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 373 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,20 +73,24 @@ class Command(ScrapyCommand):\n                 continue\n \n             if self.callbacks:\n-                items, links = [], []\n                 for callback in self.callbacks:\n-                    r_items, r_links = self.run_callback(spider, response, callback, args, opts)\n-                    items.extend(r_items)\n-                    links.extend(r_links)\n+                    items, links = self.run_callback(spider, response, callback, args, opts)\n+                    ret_items.extend(items)\n+                    ret_links.extend(links)\n+                continue\n \n             elif opts.rules:\n-                for rule in getattr(spider, 'rules', ()):\n-                    if rule.link_extractor.matches(response.url):\n+                rules = getattr(spider, 'rules')\n+                if rules:\n+                    items, links = [], []\n+                    for rule in rules:\n+                        if rule.callback and rule.link_extractor.matches(response.url):\n                             items, links = self.run_callback(spider, response, rule.callback, args, opts)\n                             break\n                 else:\n-                    log.msg('No rules found for spider \"%s\", please specify a parsing callback' % spider.domain_name)\n-                    continue\n+                    log.msg('No rules found for spider \"%s\", calling default method \"parse\"' % spider.domain_name)\n+                    items, links = self.run_callback(spider, response, 'parse', args, opts)\n+\n             else:\n                 items, links = self.run_callback(spider, response, 'parse', args, opts)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fe8afdade5caf4884bbbe5edbee6f1b771ca337", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 78 | Churn Cumulative: 228 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,21 +1,51 @@\n+import MySQLdb\n+from twisted.trial import unittest\n from datetime import datetime, timedelta\n from time import sleep\n-\n-from twisted.trial import unittest\n+from scrapy.store.db import DomainDataHistory\n+from scrapy.utils.db import mysql_connect, parse_uri, URIValidationError\n from scrapy.conf import settings\n \n-\n-class ProductComparisonTestCase(unittest.TestCase):\n-    \"\"\" Test product comparison functions \"\"\"\n+class ConnectionTestCase(unittest.TestCase):\n+    \"\"\" Test connection wrapper \"\"\"\n+    test_db = settings.get('TEST_SCRAPING_DB')\n+\n+    def setUp(self):\n+        if not self.test_db:\n+            raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n+\n+        try:\n+            mysql_connect(self.test_db)\n+        except ImportError:\n+            raise unittest.SkipTest(\"MySQLdb module not available\")\n+        except MySQLdb.OperationalError:\n+            raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n+\n+    def test_parse_uri(self):\n+        self.assertRaises(URIValidationError, parse_uri, [self.test_db])\n+        self.assertRaises(URIValidationError, parse_uri, self.test_db.replace('mysql:', 'gopher:'))\n+\n+        d = parse_uri(self.test_db)\n+\n+        self.assertTrue(isinstance(d, dict))\n+        self.assertTrue(all(key in d for key in ('user', 'host', 'db')))\n+\n+    def test_mysql_connect(self):\n+        if not self.test_db:\n+            raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n+\n+        self.assertTrue(isinstance(mysql_connect(self.test_db), MySQLdb.connection))\n+        self.assertTrue(isinstance(mysql_connect(parse_uri(self.test_db)), MySQLdb.connection))\n+\n+class ProductComparisonTestCase(unittest.TestCase):\n+    \"\"\" Test product comparison functions \"\"\"\n+    test_db = settings.get('TEST_SCRAPING_DB')\n \n     def setUp(self):\n-        self.test_db = settings.get('TEST_SCRAPING_DB')\n         if not self.test_db:\n             raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n \n         try:\n-            import MySQLdb\n-            from scrapy.utils.db import mysql_connect\n             mysql_connect(self.test_db)\n         except ImportError:\n             raise unittest.SkipTest(\"MySQLdb module not available\")\n@@ -23,9 +53,6 @@ class ProductComparisonTestCase(unittest.TestCase):\n             raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n \n     def test_domaindatahistory(self):\n-        from scrapy.store.db import DomainDataHistory\n-        from time import sleep\n-\n         ddh = DomainDataHistory(self.test_db, 'domain_data_history')\n         c = ddh.mysql_conn.cursor()\n         c.execute(\"DELETE FROM domain_data_history\")\n\n@@ -6,20 +6,35 @@ from scrapy.conf import settings\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n \n-def mysql_connect(db_uri, **kwargs):\n-    \"\"\"\n-    Connects to a MySQL DB given a mysql URI\n-    \"\"\"\n-    import MySQLdb\n+class URIValidationError(Exception):\n+    pass\n+\n+def parse_uri(db_uri):\n+    \"\"\"\n+    Parse mysql URI and return settings dict\n+    \"\"\"\n+    if not (isinstance(db_uri, basestring) and db_uri.startswith('mysql://')):\n+        raise URIValidationError(\"Incorrect MySQL URI: %s\" % db_uri)\n \n-    if not db_uri or not db_uri.startswith('mysql://'):\n-        raise Exception(\"Incorrect MySQL URI: %s\" % db_uri)\n     m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$\", db_uri)\n     if m:\n         d = m.groupdict()\n         if d['passwd'] is None:\n             del(d['passwd'])\n+        return d\n \n+def mysql_connect(db_uri_or_dict, **kwargs):\n+    \"\"\"\n+    Connects to a MySQL DB given a mysql URI\n+    \"\"\"\n+    import MySQLdb\n+\n+    if isinstance(db_uri_or_dict, dict):\n+        d = db_uri_or_dict\n+    else:\n+        d = parse_uri(db_uri_or_dict)\n+\n+    if d:\n         d['charset'] = 'utf8'\n         d.update(settings.get(\"MYSQL_CONNECTION_SETTINGS\", {}))\n         d.update(kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b52f7726f101625253ca2898cf6f1fc23db8a068", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 397 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,18 +46,18 @@ class Command(ScrapyCommand):\n \n         return (), ()\n \n-    def print_results(self, items, links, opts):\n+    def print_results(self, items, links, cb_name, opts):\n         display.nocolour = opts.nocolour\n         if not opts.noitems:\n             for item in items:\n                 for key in item.__dict__.keys():\n                     if key.startswith('_'):\n                         item.__dict__.pop(key, None)\n-            print \"# Scraped Items\", \"-\"*60\n+            print \"# Scraped Items - callback: %s\" % cb_name, \"-\"*60\n             display.pprint(list(items))\n \n         if not opts.nolinks:\n-            print \"# Links\", \"-\"*68\n+            print \"# Links - callback: %s\" % cb_name, \"-\"*68\n             display.pprint(list(links))\n \n     def run(self, args, opts):\n@@ -65,7 +65,6 @@ class Command(ScrapyCommand):\n             print \"An URL is required\"\n             return\n \n-        ret_items, ret_links = [], []\n         for response in fetch(args):\n             spider = spiders.fromurl(response.url)\n             if not spider:\n@@ -75,27 +74,22 @@ class Command(ScrapyCommand):\n             if self.callbacks:\n                 for callback in self.callbacks:\n                     items, links = self.run_callback(spider, response, callback, args, opts)\n-                    ret_items.extend(items)\n-                    ret_links.extend(links)\n-                continue\n+                    self.print_results(items, links, callback, opts)\n \n             elif opts.rules:\n-                rules = getattr(spider, 'rules')\n+                rules = getattr(spider, 'rules', None)\n                 if rules:\n                     items, links = [], []\n                     for rule in rules:\n                         if rule.callback and rule.link_extractor.matches(response.url):\n                             items, links = self.run_callback(spider, response, rule.callback, args, opts)\n+                            self.print_results(items, links, rule.callback, opts)\n                             break\n                 else:\n-                    log.msg('No rules found for spider \"%s\", calling default method \"parse\"' % spider.domain_name)\n-                    items, links = self.run_callback(spider, response, 'parse', args, opts)\n+                    log.msg('No rules found for spider \"%s\", please specify a callback for parsing' % spider.domain_name)\n+                    continue\n \n             else:\n                 items, links = self.run_callback(spider, response, 'parse', args, opts)\n-\n-            ret_items.extend(items)\n-            ret_links.extend(links)\n-\n-        self.print_results(ret_items, ret_links, opts)\n+                self.print_results(items, links, 'parse', opts)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fade1730ceebf9703ab28168a19a6bf88e826198", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 407 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -154,6 +154,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n                 'X-Amz-Acl': 'public-read',\n                 'X-Amz-Meta-Width': str(width),\n                 'X-Amz-Meta-Height': str(height),\n+                'Cache-Control': 'max-age=172800',\n                 }\n \n         buf.seek(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29024d8a4c58d70a8671acd6270c4dc2726b15ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 409 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -170,6 +170,6 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         \"\"\"\n         if self.s3_custom_spider:\n             return scrapyengine.schedule(request, self.s3_custom_spider)\n-        return self.download(request, info.spider)\n+        return self.download(request, info)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#528d2391241efa1dc16678dc1c2cb913a24de749", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 169 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -94,7 +94,9 @@ class ExtractImages(object):\n         return ret\n \n     def __call__(self, locations):\n-        if isinstance(locations, basestring):\n+        if not locations:\n+            return []\n+        elif isinstance(locations, basestring):\n             locations = [locations]\n \n         rel_urls = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c0a4222ec1bf9e99813019dc2124f90a44f1b56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 215 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,9 +32,12 @@ class AdaptorsTestCase(unittest.TestCase):\n         sample_xsel = XmlXPathSelector(sample_response)\n         sample_adaptor = adaptors.ExtractImages(response=sample_response)\n \n-        self.assertEqual(sample_adaptor(sample_xsel.x('//@href')),\n+        self.assertEqual(sample_adaptor(None), [])\n+        self.assertEqual(sample_adaptor([]), [])\n+        self.assertEqual(sample_adaptor('http://foobar.com/my_image.jpg'), ['http://foobar.com/my_image.jpg'])\n+        self.assertEqual(sample_adaptor([sample_xsel.x('//@href'), 'my_image.gif']),\n                          [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html'])\n+                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html', u'http://foobar.com/my_image.gif'])\n         self.assertEqual(sample_adaptor(sample_xsel.x('//a')),\n                          [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n                           u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/imgs/lala4.jpg'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#991fabb78340e10f8eb7236c8f3c0ecb25d97522", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 94 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n+from scrapy.utils.decompressor import Decompressor\n from scrapy.xpath.selector import XmlXPathSelector\n from scrapy.core.exceptions import UsageError, NotConfigured\n \n@@ -40,6 +41,9 @@ class XMLFeedSpider(BaseSpider):\n         if not hasattr(self, 'parse_item'):\n             raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n \n+        decompressor = Decompressor()\n+        response = decompressor.extract(response)\n+\n         if self.iternodes:\n             nodes = xmliter(response, self.itertag)\n         else:\n@@ -82,6 +86,9 @@ class CSVFeedSpider(BaseSpider):\n         if not hasattr(self, 'parse_row'):\n             raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n \n+        decompressor = Decompressor()\n+        response = decompressor.extract(response)\n+\n         response = self.adapt_response(response)\n         return self.parse_rows(response)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ec69bf1332ef6e2d02e073f105800c0a79b1684", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n A pipeline to persist objects using shove. \n \n-New is a \"new generation\" shelve. For more information see: \n+Shove is a \"new generation\" shelve. For more information see: \n http://pypi.python.org/pypi/shove\n \"\"\"\n \n@@ -10,6 +10,7 @@ from string import Template\n from shove import Shove\n from pydispatch import dispatcher\n \n+from scrapy import log\n from scrapy.core import signals\n from scrapy.conf import settings\n \n@@ -26,7 +27,18 @@ class ShoveItemPipeline(object):\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n \n     def process_item(self, domain, response, item):\n-        self.stores[domain][str(item.guid)] = item\n+        guid = str(item.guid)\n+\n+        if guid in self.stores[domain]:\n+            if self.stores[domain][guid] == item:\n+                status = 'old'\n+            else:\n+                status = 'upd'\n+        else:\n+            status = 'new'\n+\n+        self.stores[domain][guid] = item\n+        self.log(domain, item, status)\n         return item\n \n     def domain_open(self, domain):\n@@ -35,3 +47,6 @@ class ShoveItemPipeline(object):\n \n     def domain_closed(self, domain):\n         self.stores[domain].sync()\n+\n+    def log(self, domain, item, status):\n+        log.msg(\"Shove (%s): Item guid=%s\" % (status, item.guid), level=log.DEBUG, domain=domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7efa85ccc72739b464ac9a9e920243ae439d5573", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 58 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,7 @@ from pydispatch import dispatcher\n from scrapy import log\n from scrapy.core import signals\n from scrapy.conf import settings\n+from scrapy.core.exceptions import NotConfigured\n \n class ShoveItemPipeline(object):\n \n@@ -37,6 +38,7 @@ class ShoveItemPipeline(object):\n         else:\n             status = 'new'\n \n+        if not status == 'old':\n             self.stores[domain][guid] = item\n         self.log(domain, item, status)\n         return item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd4c106a611f5972f3ec79469a66fbbe0090fd0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 51 | Churn Cumulative: 341 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,5 +26,5 @@ PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n EXTENSIONS = []\n \n # contrib.middleware.retry.RetryMiddleware default settings\n-RETRY_TIMES = 3\n+RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n\n@@ -38,26 +38,19 @@ class RetryMiddleware(object):\n \n     def __init__(self):\n         self.failed_count = {}\n-        self.max_retries = settings.getint('RETRY_TIMES')\n+        self.retry_times = settings.getint('RETRY_TIMES')\n+        self.retry_http_codes = map(int, settings.getlist('RETRY_HTTP_CODES'))\n \n     def process_exception(self, request, exception, spider):\n-        retry = False\n-\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n-            retry = True\n-        elif isinstance(exception, HttpException):\n-            if exception.status in settings.getlist('RETRY_HTTP_CODES'):\n-                retry = True\n-\n-        if retry:\n+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) or (isinstance(exception, HttpException) and (int(exception.status) in self.retry_http_codes)):\n             fp = request.fingerprint()\n-            count = self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n+            self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n \n-            if self.failed_count[fp] < self.max_retries:\n-                log.msg(\"Retrying %s (failed %d times): %s\" % (request, count, exception), level=log.DEBUG, domain=spider.domain_name)\n+            if self.failed_count[fp] <= self.retry_times:\n+                log.msg(\"Retrying %s (failed %d times): %s\" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)\n                 retryreq = request.copy()\n                 retryreq.dont_filter = True\n                 return retryreq\n             else:\n-                log.msg(\"Discarding %s (failed %d times): %s\" % (request, count, exception), domain=spider.domain_name, level=log.DEBUG)\n+                log.msg(\"Discarding %s (failed %d times): %s\" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)\n \n\n@@ -70,6 +70,7 @@ class DownloaderMiddlewareManager(object):\n \n         process_response MUST return a Response object. It could alter the given\n         response, or it could create a brand-new Response.\n+        To drop the response entirely an IgnoreRequest exception must be raised.\n \n \n     process_exception(self, request, exception, spider)\n\n@@ -0,0 +1,27 @@\n+import unittest\n+\n+from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n+from scrapy.core.exceptions import HttpException\n+from scrapy.spider import spiders\n+from scrapy.http import Request, Response\n+\n+class RetryTest(unittest.TestCase):\n+    def setUp(self):\n+        spiders.spider_modules = ['scrapy.tests.test_spiders']\n+        spiders.reload()\n+        self.spider = spiders.fromdomain('scrapytest.org')\n+\n+    def test_process_exception(self):\n+        exception_404 = (Request('http://www.scrapytest.org/404'), HttpException('404', None, Response('scrapytest.org', 'http://www.scrapytest.org/404', body='')), self.spider)\n+        exception_503 = (Request('http://www.scrapytest.org/503'), HttpException('503', None, Response('scrapytest.org', 'http://www.scrapytest.org/503', body='')), self.spider)\n+\n+        mw = RetryMiddleware()\n+        mw.retry_times = 1\n+\n+        self.assertTrue(mw.process_exception(*exception_404) is None)\n+\n+        self.assertTrue(isinstance(mw.process_exception(*exception_503), Request))\n+        self.assertTrue(mw.process_exception(*exception_503) is None)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d16160bf7626dfc94f4000d20e2d5f74c362eb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 97 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,6 +29,7 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n \n from scrapy import log\n from scrapy.core.exceptions import HttpException\n+from scrapy.stats import stats\n from scrapy.conf import settings\n \n class RetryMiddleware(object):\n@@ -45,6 +46,16 @@ class RetryMiddleware(object):\n         if isinstance(exception, self.EXCEPTIONS_TO_RETRY) or (isinstance(exception, HttpException) and (int(exception.status) in self.retry_http_codes)):\n             fp = request.fingerprint()\n             self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n+            if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+                exception_name = exception.__name__\n+                if exception_name == 'TimeoutError':\n+                    if 'defer' in str(exception):\n+                        exception_name = 'UserTimeoutError'\n+                    else:\n+                        exception_name = 'ServerTimeoutError'\n+                stats.incpath('%s/conn_error_count/%s' % (spider.domain_name, exception_name))\n+            else:\n+                stats.incpath('%s/http_error_count/%d' % (spider.domain_name, int(exception.status)))\n \n             if self.failed_count[fp] <= self.retry_times:\n                 log.msg(\"Retrying %s (failed %d times): %s\" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1510efcac9e48741edba589d18d1f4abe10971b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/7 | Churn Δ: 77 | Churn Cumulative: 243 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,6 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n \n from scrapy import log\n from scrapy.core.exceptions import HttpException\n-from scrapy.stats import stats\n from scrapy.conf import settings\n \n class RetryMiddleware(object):\n@@ -46,16 +45,6 @@ class RetryMiddleware(object):\n         if isinstance(exception, self.EXCEPTIONS_TO_RETRY) or (isinstance(exception, HttpException) and (int(exception.status) in self.retry_http_codes)):\n             fp = request.fingerprint()\n             self.failed_count[fp] = self.failed_count.get(fp, 0) + 1\n-            if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n-                exception_name = exception.__name__\n-                if exception_name == 'TimeoutError':\n-                    if 'defer' in str(exception):\n-                        exception_name = 'UserTimeoutError'\n-                    else:\n-                        exception_name = 'ServerTimeoutError'\n-                stats.incpath('%s/conn_error_count/%s' % (spider.domain_name, exception_name))\n-            else:\n-                stats.incpath('%s/http_error_count/%d' % (spider.domain_name, int(exception.status)))\n \n             if self.failed_count[fp] <= self.retry_times:\n                 log.msg(\"Retrying %s (failed %d times): %s\" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)\n\n@@ -0,0 +1,49 @@\n+from scrapy.core.exceptions import HttpException, NotConfigured\n+from scrapy.stats import stats\n+from scrapy.conf import settings\n+\n+class DownloaderStats(object):\n+    \"\"\"DownloaderStats store stats of all requests, responses and\n+    exceptions that pass through it.\n+\n+    They are stored in the following keys:\n+\n+    SPIDER/downloader/request_method_count\n+    SPIDER/downloader/response_status_count\n+    SPIDER/downloader/exception_count\n+\n+    To use this middleware you must enable the DOWNLOADER_STATS setting.\n+    \"\"\"\n+\n+    def __init__(self):\n+        if not settings.getbool('DOWNLOADER_STATS'):\n+            raise NotConfigured\n+            \n+    def process_request(self, request, spider):\n+        stats.incpath('_global/downloader/request_count')\n+        stats.incpath('%s/downloader/request_count' % spider.domain_name)\n+        stats.incpath('%s/downloader/request_method_count/%s' % (spider.domain_name, request.method))\n+        reqlen = len(request)\n+        stats.incpath('%s/downloader/request_bytes' % spider.domain_name, reqlen)\n+        stats.incpath('_global/downloader/request_bytes', reqlen)\n+\n+    def process_response(self, request, response, spider):\n+        self._inc_response_count(response, spider.domain_name)\n+        return response\n+        \n+    def process_exception(self, request, exception, spider):\n+        ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n+        stats.incpath('_globl/downloader/exception_count')\n+        stats.incpath('%s/downloader/exception_count' % spider.domain_name)\n+        stats.incpath('%s/downloader/exception_type_count/%s' % (spider.domain_name, ex_class))\n+        if isinstance(exception, HttpException):\n+            self._inc_response_count(exception.response, spider.domain_name)\n+        raise exception\n+\n+    def _inc_response_count(self, response, domain):\n+        stats.incpath('_global/downloader/response_count')\n+        stats.incpath('%s/downloader/response_count' % domain)\n+        stats.incpath('%s/downloader/response_status_count/%s' % (domain, response.status))\n+        reslen = len(response)\n+        stats.incpath('%s/downloader/response_bytes' % domain, reslen)\n+        stats.incpath('_global/downloader/response_bytes', reslen)\n\n@@ -26,8 +26,6 @@ class CoreStats(object):\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n-        dispatcher.connect(self.request_uploaded, signal=signals.request_uploaded)\n \n     def stats_domain_open(self, domain, spider):\n         stats.setpath('%s/start_time' % domain, datetime.datetime.now())\n@@ -52,18 +50,3 @@ class CoreStats(object):\n         stats.incpath('%s/item_dropped_count' % spider.domain_name)\n         stats.incpath('%s/item_dropped_reasons_count/%s' % (spider.domain_name, reason))\n         stats.incpath('_global/item_dropped_count')\n-\n-    def response_downloaded(self, response, spider):\n-        stats.incpath('%s/response_count' % spider.domain_name)\n-        stats.incpath('%s/response_status_count/%s' % (spider.domain_name, response.status))\n-        stats.incpath('_global/response_downloaded_count')\n-\n-        reslen = len(response)\n-        stats.incpath('%s/transfer/downloaded_bytes' % spider.domain_name, reslen)\n-        stats.incpath('_global/transfer/downloaded_bytes', reslen)\n-\n-    def request_uploaded(self, request, spider):\n-        reqlen = len(request)\n-        stats.incpath('%s/transfer/uploaded_bytes' % spider.domain_name, reqlen)\n-        stats.incpath('_global/transfer/uploaded_bytes', reqlen)\n-        \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4599893f3548d56769bd43c6a6678a8b0b435b6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 50 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,7 +38,6 @@ class DownloaderStats(object):\n         stats.incpath('%s/downloader/exception_type_count/%s' % (spider.domain_name, ex_class))\n         if isinstance(exception, HttpException):\n             self._inc_response_count(exception.response, spider.domain_name)\n-        raise exception\n \n     def _inc_response_count(self, response, domain):\n         stats.incpath('_global/downloader/response_count')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b0e1f090e00f8fae3be8dcfb2508409c7e55146", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 23/17 | Churn Δ: 74 | Churn Cumulative: 168 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,24 +1,26 @@\n-\"\"\"\n-Utility for autodetecting and decompressing responses\n-\"\"\"\n-\n import zipfile\n import tarfile\n import gzip\n import bz2\n-from scrapy.http import ResponseBody\n try:\n     from cStringIO import StringIO\n except:\n     from StringIO import StringIO\n \n-class Decompressor(object):\n-    class ArchiveIsEmpty(Exception):\n-        pass\n+from scrapy import log\n+from scrapy.http import Response, ResponseBody\n+\n+class DecompressionMiddleware(object):\n+    \"\"\" This middleware tries to recognise and extract the possibly compressed\n+    responses that may arrive. \"\"\"\n \n     def __init__(self):\n-        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,\n-                              'gz': self.is_gzip, 'bz2': self.is_bzip2}        \n+        self.decompressors = {\n+            'tar': self.is_tar,\n+            'zip': self.is_zip,\n+            'gz': self.is_gzip,\n+            'bz2': self.is_bzip2\n+        }\n \n     def is_tar(self, response):\n         try:\n@@ -56,7 +58,11 @@ class Decompressor(object):\n             return False\n         return response.replace(body=ResponseBody(decompressed_body))\n \n-    def extract_winfo(self, response):\n+    def extract(self, response):\n+        \"\"\" This method tries to decompress the given response, if possible,\n+        and returns a tuple containing the resulting response, and the name\n+        of the used decompressor \"\"\"\n+\n         self.body = response.body.to_string()\n         self.archive = StringIO()\n         self.archive.write(self.body)\n@@ -65,8 +71,12 @@ class Decompressor(object):\n             self.archive.seek(0)\n             new_response = self.decompressors[decompressor](response)\n             if new_response:\n-                return new_response, decompressor\n-        return response, ''\n+                return (new_response, decompressor)\n+        return (response, None)\n \n-    def extract(self, response):\n-        return self.extract_winfo(response)[0]\n+    def process_response(self, request, response, spider):\n+        if isinstance(response, Response):\n+            response, format = self.extract(response)\n+            if format:\n+                log.msg('Decompressed response with format: %s' % format, log.DEBUG, domain=spider.domain_name)\n+        return response\n\n@@ -3,7 +3,6 @@ from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.utils.decompressor import Decompressor\n from scrapy.xpath.selector import XmlXPathSelector\n from scrapy.core.exceptions import UsageError, NotConfigured\n \n@@ -41,9 +40,6 @@ class XMLFeedSpider(BaseSpider):\n         if not hasattr(self, 'parse_item'):\n             raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n \n-        decompressor = Decompressor()\n-        response = decompressor.extract(response)\n-\n         if self.iternodes:\n             nodes = xmliter(response, self.itertag)\n         else:\n@@ -85,10 +81,6 @@ class CSVFeedSpider(BaseSpider):\n     def parse(self, response):\n         if not hasattr(self, 'parse_row'):\n             raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n-\n-        decompressor = Decompressor()\n-        response = decompressor.extract(response)\n-\n         response = self.adapt_response(response)\n         return self.parse_rows(response)\n \n\n@@ -1,12 +1,12 @@\n import os\n from unittest import TestCase, main\n from scrapy.http import Response, ResponseBody\n-from scrapy.utils.decompressor import Decompressor\n+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n \n-class ScrapyDecompressTest(TestCase):\n+class ScrapyDecompressionTest(TestCase):\n     uncompressed_body = ''\n     test_responses = {}\n-    decompressor = Decompressor()\n+    middleware = DecompressionMiddleware()\n     \n     def setUp(self):\n         self.datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n@@ -22,24 +22,20 @@ class ScrapyDecompressTest(TestCase):\n             self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n     \n     def test_tar(self):\n-        ret = self.decompressor.extract(self.test_responses['tar'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        response, format = self.middleware.extract(self.test_responses['tar'])\n+        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n         \n     def test_zip(self):\n-        ret = self.decompressor.extract(self.test_responses['zip'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        response, format = self.middleware.extract(self.test_responses['zip'])\n+        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n     \n     def test_gz(self):\n-        ret = self.decompressor.extract(self.test_responses['xml.gz'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        response, format = self.middleware.extract(self.test_responses['xml.gz'])\n+        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n \n     def test_bz2(self):\n-        ret = self.decompressor.extract(self.test_responses['xml.bz2'])\n-        if ret:\n-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        response, format = self.middleware.extract(self.test_responses['xml.bz2'])\n+        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n         \n if __name__ == '__main__':\n     main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d0cca538eb8922e66b3b6472b8d76998b67308b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,7 @@\n #!/usr/bin/env python\n \n+import sys, os\n+sys.path.append(os.path.dirname(os.getcwd()))\n+\n from scrapy.command.cmdline import execute\n execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cf569f88abfcabd2b3552b3f1bc6cd57a0fbd17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,4 @@\n #!/usr/bin/env python\n \n-import sys, os\n-sys.path.append(os.path.dirname(os.getcwd()))\n-\n from scrapy.command.cmdline import execute\n execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9931518aa0886ba84f4e095e4689fb9bcbdf278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 72 | Churn Cumulative: 216 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,72 @@\n+\"\"\"\n+Utility for autodetecting and decompressing responses\n+\"\"\"\n+\n+import zipfile\n+import tarfile\n+import gzip\n+import bz2\n+from scrapy.http import ResponseBody\n+try:\n+    from cStringIO import StringIO\n+except:\n+    from StringIO import StringIO\n+\n+class Decompressor(object):\n+    class ArchiveIsEmpty(Exception):\n+        pass\n+    \n+    def __init__(self):\n+        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,\n+                              'gz': self.is_gzip, 'bz2': self.is_bzip2}        \n+    \n+    def is_tar(self, response):\n+        try:\n+            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)\n+        except tarfile.ReadError:\n+            return False\n+        if tar_file.members:\n+            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))\n+        else:\n+            raise self.ArchiveIsEmpty\n+    \n+    def is_zip(self, response):\n+        try:\n+            zip_file = zipfile.ZipFile(self.archive)\n+        except zipfile.BadZipfile:\n+            return False\n+        namelist = zip_file.namelist()\n+        if namelist:\n+            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))\n+        else:\n+            raise self.ArchiveIsEmpty\n+            \n+    def is_gzip(self, response):\n+        try:\n+            gzip_file = gzip.GzipFile(fileobj=self.archive)\n+            decompressed_body = gzip_file.read()\n+        except IOError:\n+            return False\n+        return response.replace(body=decompressed_body)\n+            \n+    def is_bzip2(self, response):\n+        try:\n+            decompressed_body = bz2.decompress(self.body)\n+        except IOError:\n+            return False\n+        return response.replace(body=ResponseBody(decompressed_body))\n+    \n+    def extract_winfo(self, response):\n+        self.body = response.body.to_string()\n+        self.archive = StringIO()\n+        self.archive.write(self.body)\n+        \n+        for decompressor in self.decompressors.keys():\n+            self.archive.seek(0)\n+            new_response = self.decompressors[decompressor](response)\n+            if new_response:\n+                return new_response, decompressor\n+        return response, ''\n+        \n+    def extract(self, response):\n+        return self.extract_winfo(response)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26c749b396f89b5c3cee3fb46b092ef6b5d79a67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 197 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,6 @@\n+def setup(app):\n+    app.add_description_unit(\n+        directivename = \"setting\",\n+        rolename      = \"setting\",\n+        indextemplate = \"pair: %s; setting\",\n+    )\n\n@@ -16,14 +16,14 @@ import sys, os\n # If your extensions are in another directory, add it here. If the directory\n # is relative to the documentation root, use os.path.abspath to make it\n # absolute, like shown here.\n-#sys.path.append(os.path.abspath('.'))\n+sys.path.append(os.path.join(os.path.dirname(__file__), \"_ext\"))\n \n # General configuration\n # ---------------------\n \n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n-extensions = []\n+extensions = ['scrapydocs']\n \n # Add any paths that contain templates here, relative to this directory.\n templates_path = ['../templates/docs']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
