{"custom_id": "keras#1193427fd39355431bc0b1f36bfbd9e3c2360385", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7406 | Contributors (this commit): 27 | Commits (past 90d): 44 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -2682,7 +2682,7 @@ def _collect_input_shape(input_tensors):\n \n \n def save_weights_to_hdf5_group(f, layers):\n-    from . import __version__ as keras_version\n+    from .. import __version__ as keras_version\n \n     f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in layers]\n     f.attrs['backend'] = K.backend().encode('utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f623b2ab57d2dc18a7f12d8d7cd0158207cdb939", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 29 | Churn Cumulative: 2469 | Contributors (this commit): 40 | Commits (past 90d): 22 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -612,56 +612,27 @@ class TensorBoard(Callback):\n             for layer in self.model.layers:\n \n                 for weight in layer.weights:\n-                    if hasattr(tf, 'histogram_summary'):\n-                        tf.histogram_summary(weight.name, weight)\n-                    else:\n                     tf.summary.histogram(weight.name, weight)\n-\n                     if self.write_images:\n                         w_img = tf.squeeze(weight)\n-\n                         shape = w_img.get_shape()\n                         if len(shape) > 1 and shape[0] > shape[1]:\n                             w_img = tf.transpose(w_img)\n-\n                         if len(shape) == 1:\n                             w_img = tf.expand_dims(w_img, 0)\n-\n                         w_img = tf.expand_dims(tf.expand_dims(w_img, 0), -1)\n-\n-                        if hasattr(tf, 'image_summary'):\n-                            tf.image_summary(weight.name, w_img)\n-                        else:\n                         tf.summary.image(weight.name, w_img)\n \n                 if hasattr(layer, 'output'):\n-                    if hasattr(tf, 'histogram_summary'):\n-                        tf.histogram_summary('{}_out'.format(layer.name),\n-                                             layer.output)\n-                    else:\n                     tf.summary.histogram('{}_out'.format(layer.name),\n                                          layer.output)\n-\n-        if hasattr(tf, 'merge_all_summaries'):\n-            self.merged = tf.merge_all_summaries()\n-        else:\n         self.merged = tf.summary.merge_all()\n \n         if self.write_graph:\n-            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n             self.writer = tf.summary.FileWriter(self.log_dir,\n                                                 self.sess.graph)\n-            elif parse_version(tf.__version__) >= parse_version('0.8.0'):\n-                self.writer = tf.train.SummaryWriter(self.log_dir,\n-                                                     self.sess.graph)\n         else:\n-                self.writer = tf.train.SummaryWriter(self.log_dir,\n-                                                     self.sess.graph_def)\n-        else:\n-            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n             self.writer = tf.summary.FileWriter(self.log_dir)\n-            else:\n-                self.writer = tf.train.SummaryWriter(self.log_dir)\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a164addd8d5d1642d613100226a94398d929804d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 1888 | Contributors (this commit): 19 | Commits (past 90d): 16 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -117,10 +117,10 @@ class SGD(Optimizer):\n     def __init__(self, lr=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n-        self.iterations = K.variable(0.)\n-        self.lr = K.variable(lr)\n-        self.momentum = K.variable(momentum)\n-        self.decay = K.variable(decay)\n+        self.iterations = K.variable(0., name='iterations')\n+        self.lr = K.variable(lr, name='lr')\n+        self.momentum = K.variable(momentum, name='momentum')\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n         self.nesterov = nesterov\n \n@@ -186,12 +186,12 @@ class RMSprop(Optimizer):\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n-        self.lr = K.variable(lr)\n-        self.rho = K.variable(rho)\n+        self.lr = K.variable(lr, name='lr')\n+        self.rho = K.variable(rho, name='rho')\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay)\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n-        self.iterations = K.variable(0.)\n+        self.iterations = K.variable(0., name='iterations')\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n@@ -244,11 +244,11 @@ class Adagrad(Optimizer):\n \n     def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n-        self.lr = K.variable(lr)\n+        self.lr = K.variable(lr, name='lr')\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay)\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n-        self.iterations = K.variable(0.)\n+        self.iterations = K.variable(0., name='iterations')\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n@@ -301,12 +301,12 @@ class Adadelta(Optimizer):\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n-        self.lr = K.variable(lr)\n+        self.lr = K.variable(lr, name='lr')\n         self.rho = rho\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay)\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n-        self.iterations = K.variable(0.)\n+        self.iterations = K.variable(0., name='iterations')\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n@@ -369,12 +369,12 @@ class Adam(Optimizer):\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adam, self).__init__(**kwargs)\n-        self.iterations = K.variable(0)\n-        self.lr = K.variable(lr)\n-        self.beta_1 = K.variable(beta_1)\n-        self.beta_2 = K.variable(beta_2)\n+        self.iterations = K.variable(0, name='iterations')\n+        self.lr = K.variable(lr, name='lr')\n+        self.beta_1 = K.variable(beta_1, name='beta_1')\n+        self.beta_2 = K.variable(beta_2, name='beta_2')\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay)\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n \n     def get_updates(self, params, constraints, loss):\n@@ -439,12 +439,12 @@ class Adamax(Optimizer):\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n-        self.iterations = K.variable(0.)\n-        self.lr = K.variable(lr)\n-        self.beta_1 = K.variable(beta_1)\n-        self.beta_2 = K.variable(beta_2)\n+        self.iterations = K.variable(0., name='iterations')\n+        self.lr = K.variable(lr, name='lr')\n+        self.beta_1 = K.variable(beta_1, name='beta_1')\n+        self.beta_2 = K.variable(beta_2, name='beta_2')\n         self.epsilon = epsilon\n-        self.decay = K.variable(decay)\n+        self.decay = K.variable(decay, name='decay')\n         self.initial_decay = decay\n \n     def get_updates(self, params, constraints, loss):\n@@ -515,11 +515,11 @@ class Nadam(Optimizer):\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n-        self.iterations = K.variable(0.)\n-        self.m_schedule = K.variable(1.)\n-        self.lr = K.variable(lr)\n-        self.beta_1 = K.variable(beta_1)\n-        self.beta_2 = K.variable(beta_2)\n+        self.iterations = K.variable(0., name='iterations')\n+        self.m_schedule = K.variable(1., name='m_schedule')\n+        self.lr = K.variable(lr, name='lr')\n+        self.beta_1 = K.variable(beta_1, name='beta_1')\n+        self.beta_2 = K.variable(beta_2, name='beta_2')\n         self.epsilon = epsilon\n         self.schedule_decay = schedule_decay\n \n@@ -580,7 +580,7 @@ class TFOptimizer(Optimizer):\n \n     def __init__(self, optimizer):\n         self.optimizer = optimizer\n-        self.iterations = K.variable(0.)\n+        self.iterations = K.variable(0., name='iterations')\n         self.updates = []\n \n     def get_updates(self, params, constraints, loss):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cad4e4e8cc1680c075b8a49c92517d5f353f2c9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3282 | Contributors (this commit): 40 | Commits (past 90d): 32 | Contributors (cumulative): 49 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,6 @@ import warnings\n from collections import deque\n from collections import OrderedDict\n from collections import Iterable\n-from pkg_resources import parse_version\n from .utils.generic_utils import Progbar\n from . import backend as K\n \n\n@@ -314,7 +314,8 @@ def test_TensorBoard():\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n \n-    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n+    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n+                                write_images=True)\n     cbks = [tsb]\n \n     # fit with validation data\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#173a40ddc18b35ab803b017cf73c544af319c5d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 186 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ setup(name='Keras',\n       url='https://github.com/fchollet/keras',\n       download_url='https://github.com/fchollet/keras/tarball/2.0.0',\n       license='MIT',\n-      install_requires=['theano', 'pyyaml', 'six'],\n+      install_requires=['tensorflow', 'pyyaml', 'six'],\n       extras_require={\n           'h5py': ['h5py'],\n           'visualize': ['pydot-ng'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4101b5fdb2262b666ba7eb901f1776ef6579902a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 28 | Churn Cumulative: 7358 | Contributors (this commit): 61 | Commits (past 90d): 61 | Contributors (cumulative): 61 | DMM Complexity: 0.6956521739130435\n\nDIFF:\n@@ -291,11 +291,7 @@ def variable(value, dtype=None, name=None):\n def _initialize_variables():\n     \"\"\"Utility to initialize uninitialized variables on the fly.\n     \"\"\"\n-    if hasattr(tf, 'global_variables'):\n     variables = tf.global_variables()\n-    else:\n-        variables = tf.all_variables()\n-\n     uninitialized_variables = []\n     for v in variables:\n         if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n@@ -303,10 +299,7 @@ def _initialize_variables():\n             v._keras_initialized = True\n     if uninitialized_variables:\n         sess = get_session()\n-        if hasattr(tf, 'variables_initializer'):\n         sess.run(tf.variables_initializer(uninitialized_variables))\n-        else:\n-            sess.run(tf.initialize_variables(uninitialized_variables))\n \n \n def constant(value, dtype=None, shape=None, name=None):\n@@ -1645,10 +1638,7 @@ def repeat_elements(x, rep, axis):\n                          'Typically you need to pass a fully-defined '\n                          '`input_shape` argument to your first layer.')\n     # slices along the repeat axis\n-    try:\n     splits = tf.split(value=x, num_or_size_splits=x_shape[axis], axis=axis)\n-    except TypeError:\n-        splits = tf.split(value=x, num_split=x_shape[axis], split_dim=axis)\n     # repeat each slice the given number of reps\n     x_rep = [s for s in splits for _ in range(rep)]\n     return concatenate(x_rep, axis)\n@@ -1917,12 +1907,7 @@ def reverse(x, axes):\n     \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n-    try:\n-        return tf.reverse_v2(x, axes)\n-    except AttributeError:\n-        # Older TF versions.\n-        dims = [True if i in axes else False for i in range(len(x.get_shape()._dims))]\n-        return tf.reverse(x, dims)\n+    return tf.reverse(x, axes)\n \n \n # VALUE MANIPULATION\n@@ -2557,11 +2542,8 @@ def categorical_crossentropy(output, target, from_logits=False):\n         return - tf.reduce_sum(target * tf.log(output),\n                                reduction_indices=len(output.get_shape()) - 1)\n     else:\n-        try:\n         return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                        logits=output)\n-        except TypeError:\n-            return tf.nn.softmax_cross_entropy_with_logits(output, target)\n \n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n@@ -2588,13 +2570,9 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n     output_shape = output.get_shape()\n     targets = cast(flatten(target), 'int64')\n     logits = tf.reshape(output, [-1, int(output_shape[-1])])\n-    try:\n     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n         labels=targets,\n         logits=logits)\n-    except TypeError:\n-        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n-            logits, targets)\n     if len(output_shape) == 3:\n         # if our output includes timesteps we need to reshape\n         return tf.reshape(res, tf.shape(output)[:-1])\n@@ -2622,11 +2600,9 @@ def binary_crossentropy(output, target, from_logits=False):\n         epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n         output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n         output = tf.log(output / (1 - output))\n-    try:\n+\n     return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                    logits=output)\n-    except TypeError:\n-        return tf.nn.sigmoid_cross_entropy_with_logits(output, target)\n \n \n def sigmoid(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b666ef18a126c847eb6d25da08434c92cf988b7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 949 | Contributors (this commit): 17 | Commits (past 90d): 19 | Contributors (cumulative): 17 | DMM Complexity: 0.75\n\nDIFF:\n@@ -1,7 +1,6 @@\n from __future__ import print_function\n \n from .conv_utils import convert_kernel\n-from ..models import Model, Sequential\n from .. import backend as K\n import numpy as np\n \n@@ -15,10 +14,6 @@ def print_summary(model, line_length=None, positions=None):\n         positions: relative or absolute positions of log elements in each line.\n             If not provided, defaults to `[.33, .55, .67, 1.]`.\n     \"\"\"\n-\n-    if isinstance(model, Sequential):\n-        sequential_like = True\n-    else:\n     sequential_like = True\n     for v in model.nodes_by_depth.values():\n         if len(v) > 1:\n@@ -140,7 +135,7 @@ def count_total_params(layers, layer_set=None):\n         if layer in layer_set:\n             continue\n         layer_set.add(layer)\n-        if isinstance(layer, (Model, Sequential)):\n+        if hasattr(layer, 'layers'):\n             t, nt = count_total_params(layer.layers, layer_set)\n             trainable_count += t\n             non_trainable_count += nt\n@@ -161,18 +156,17 @@ def convert_all_kernels_in_model(model):\n     # Note: SeparableConvolution not included\n     # since only supported by TF.\n     conv_classes = {\n-        'Convolution1D',\n-        'Convolution2D',\n-        'Convolution3D',\n-        'AtrousConvolution2D',\n-        'Deconvolution2D',\n+        'Conv1D',\n+        'Conv2D',\n+        'Conv3D',\n+        'Conv2DTranspose',\n     }\n     to_assign = []\n     for layer in model.layers:\n         if layer.__class__.__name__ in conv_classes:\n-            original_w = K.get_value(layer.W)\n-            converted_w = convert_kernel(original_w)\n-            to_assign.append((layer.W, converted_w))\n+            original_kernel = K.get_value(layer.kernel)\n+            converted_kernel = convert_kernel(original_kernel)\n+            to_assign.append((layer.kernel, converted_kernel))\n     K.batch_set_value(to_assign)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#579cc22cda42c84119a311f20c5fbe09b3e93803", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 8362 | Contributors (this commit): 41 | Commits (past 90d): 65 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from six.moves import zip\n from .. import backend as K\n from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n+from ..utils.layer_utils import print_summary as print_layer_summary\n \n try:\n     import h5py\n@@ -2544,8 +2545,7 @@ class Container(Layer):\n         return yaml.dump(self._updated_config(), **kwargs)\n \n     def summary(self, line_length=None, positions=None):\n-        from ..utils.layer_utils import print_summary\n-        print_summary(self,\n+        print_layer_summary(self,\n                             line_length=line_length,\n                             positions=positions)\n \n\n@@ -14,6 +14,9 @@ def print_summary(model, line_length=None, positions=None):\n         positions: relative or absolute positions of log elements in each line.\n             If not provided, defaults to `[.33, .55, .67, 1.]`.\n     \"\"\"\n+    if model.__class__.__name__ == 'Sequential':\n+        sequential_like = True\n+    else:\n         sequential_like = True\n         for v in model.nodes_by_depth.values():\n             if len(v) > 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ba07aa8cec97ed41cc252bd99d516457c99bbff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 9033 | Contributors (this commit): 54 | Commits (past 90d): 28 | Contributors (cumulative): 54 | DMM Complexity: None\n\nDIFF:\n@@ -1002,7 +1002,7 @@ class Sequential(Model):\n         on images on CPU in parallel to training your model on GPU.\n \n         # Arguments\n-            generator: a generator.\n+            generator: A generator.\n                 The output of the generator must be either\n                 - a tuple (inputs, targets)\n                 - a tuple (inputs, targets, sample_weights).\n@@ -1015,28 +1015,29 @@ class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of unique samples if your dataset\n                 divided by the batch size.\n-            epochs: integer, total number of iterations on the data.\n-            verbose: verbosity mode, 0, 1, or 2.\n-            callbacks: list of callbacks to be called during training.\n-            validation_data: this can be either\n-                - a generator for the validation data\n-                - a tuple (inputs, targets)\n-                - a tuple (inputs, targets, sample_weights).\n-            num_val_samples: only relevant if `validation_data` is a generator.\n-                number of samples to use from validation generator\n+            epochs: Integer, total number of iterations on the data.\n+            verbose: Verbosity mode, 0, 1, or 2.\n+            callbacks: List of callbacks to be called during training.\n+            validation_data: This can be either\n+                - A generator for the validation data\n+                - A tuple (inputs, targets)\n+                - A tuple (inputs, targets, sample_weights).\n+            validation_steps: Only relevant if `validation_data`\n+                is a generator.\n+                Number of samples to use from validation generator\n                 at the end of every epoch.\n-            class_weight: dictionary mapping class indices to a weight\n+            class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n-            max_q_size: maximum size for the generator queue\n-            workers: maximum number of processes to spin up\n-            pickle_safe: if True, use process based threading.\n+            max_q_size: Maximum size for the generator queue\n+            workers: Maximum number of processes to spin up\n+            pickle_safe: Ff True, use process based threading.\n                 Note that because\n                 this implementation relies on multiprocessing,\n                 you should not pass\n                 non picklable arguments to the generator\n                 as they can't be passed\n                 easily to children processes.\n-            initial_epoch: epoch at which to start training\n+            initial_epoch: Epoch at which to start training\n                 (useful for resuming a previous training run)\n \n         # Returns\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8282437a7f75f9c3af7a1b2ad57297be41863ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 188 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import numpy as np\n import json\n \n from ..utils.data_utils import get_file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f5be01da96175a3cb90a2c2a20b01b3d14f80e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4713 | Contributors (this commit): 36 | Commits (past 90d): 44 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,6 @@ import warnings\n \n from ..models import Model\n from .. import layers\n-from ..layers import Flatten\n from ..layers import Dense\n from ..layers import Input\n from ..layers import BatchNormalization\n\n@@ -2050,9 +2050,8 @@ class Model(Container):\n                 if isinstance(generator_output, tuple):\n                     if len(generator_output) == 2:\n                         x, y = generator_output\n-                        sample_weight = None\n                     elif len(generator_output) == 3:\n-                        x, y, sample_weight = generator_output\n+                        x, y, _ = generator_output\n                     else:\n                         raise ValueError('output of generator should be '\n                                          'a tuple `(x, y, sample_weight)` '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4992707d5705eb6ad1eefd81697fe2bf268bc33e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4147 | Contributors (this commit): 34 | Commits (past 90d): 33 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -2048,16 +2048,20 @@ class Model(Container):\n                         time.sleep(wait_time)\n \n                 if isinstance(generator_output, tuple):\n+                    # Compatibility with the generators\n+                    # used for training.\n                     if len(generator_output) == 2:\n-                        x, y = generator_output\n+                        x, _ = generator_output\n                     elif len(generator_output) == 3:\n-                        x, y, _ = generator_output\n+                        x, _, _ = generator_output\n                     else:\n                         raise ValueError('output of generator should be '\n                                          'a tuple `(x, y, sample_weight)` '\n                                          'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n                 else:\n+                    # Assumes a generator that only\n+                    # yields inputs (not targets and sample weights).\n                     x = generator_output\n \n                 outs = self.predict_on_batch(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4487510c90b0398d7107e6c931c96c8904d83f9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 98 | Churn Cumulative: 22858 | Contributors (this commit): 97 | Commits (past 90d): 164 | Contributors (cumulative): 161 | DMM Complexity: 0.5483870967741935\n\nDIFF:\n@@ -14,6 +14,7 @@ from .common import floatx\n from .common import _EPSILON\n from .common import image_data_format\n py_all = all\n+py_sum = sum\n \n # INTERNAL UTILS\n \n@@ -2785,14 +2786,19 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         x: Tensor or variable.\n         kernel: kernel tensor.\n         strides: stride integer.\n-        padding: string, `\"same\"` or `\"valid\"`.\n+        padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n         data_format: string, one of \"channels_last\", \"channels_first\".\n         dilation_rate: integer dilate rate.\n \n     # Returns\n         A tensor, result of 1D convolution.\n     \"\"\"\n-    # pre-process dtype\n+    kernel_shape = kernel.get_shape().as_list()\n+    if padding == 'causal':\n+        # causal (dilated) convolution:\n+        left_pad = dilation_rate * (kernel_shape[0] - 1)\n+        x = temporal_padding(x, (left_pad, 0))\n+        padding = 'valid'\n     padding = _preprocess_padding(padding)\n     if data_format == 'channels_last':\n         tf_data_format = 'NWC'\n\n@@ -17,6 +17,7 @@ import inspect\n import numpy as np\n from .common import _FLOATX, floatx, _EPSILON, image_data_format\n py_all = all\n+py_sum = sum\n \n \n # INTERNAL UTILS\n@@ -801,14 +802,18 @@ def temporal_padding(x, padding=(1, 1)):\n     Apologies for the inane API, but Theano makes this\n     really hard.\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     assert len(padding) == 2\n     input_shape = x.shape\n     output_shape = (input_shape[0],\n                     input_shape[1] + padding[0] + padding[1],\n                     input_shape[2])\n     output = T.zeros(output_shape)\n-    return T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[1], :], x)\n+    result = T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[0], :], x)\n+    if hasattr(x, '_keras_shape'):\n+        result._keras_shape = (x._keras_shape[0],\n+                               x._keras_shape[1] + py_sum(padding),\n+                               x._keras_shape[2])\n+    return result\n \n \n def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n@@ -1516,7 +1521,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     # Arguments\n         kernel: kernel tensor.\n         strides: stride integer.\n-        padding: string, \"same\" or \"valid\".\n+        padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n         data_format: string, one of \"channels_last\", \"channels_first\"\n         dilation_rate: integer.\n     \"\"\"\n@@ -1524,6 +1529,18 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ', data_format)\n+\n+    if hasattr(kernel, '_keras_shape'):\n+        kernel_shape = kernel._keras_shape\n+    else:\n+        kernel_shape = None\n+    if padding == 'causal':\n+        # causal (dilated) convolution:\n+        if not kernel_shape:\n+            raise AttributeError('Causal padding requires kernel._keras_shape set.')\n+        left_pad = dilation_rate * (kernel_shape[0] - 1)\n+        x = temporal_padding(x, (left_pad, 0))\n+        padding = 'valid'\n     if hasattr(x, '_keras_shape'):\n         shape = x._keras_shape\n     else:\n\n@@ -251,7 +251,11 @@ class Conv1D(_Conv):\n             specifying the stride length of the convolution.\n             Specifying any stride value != 1 is incompatible with specifying\n             any `dilation_rate` value != 1.\n-        padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+        padding: One of `\"valid\"`, `\"causal\"` or `\"same\"` (case-insensitive).\n+            `\"causal\"` results in causal (dilated) convolutions, e.g. output[t]\n+            depends solely on input[:t-1]. Useful when modeling temporal data\n+            where the model should not violate the temporal order.\n+            See [WaveNet: A Generative Model for Raw Audio, section 2.1](https://arxiv.org/abs/1609.03499).\n         dilation_rate: an integer or tuple/list of a single integer, specifying\n             the dilation rate to use for dilated convolution.\n             Currently, specifying any `dilation_rate` value != 1 is\n\n@@ -55,11 +55,11 @@ def normalize_data_format(value):\n \n def normalize_padding(value):\n     padding = value.lower()\n-    allowed = {'valid', 'same'}\n+    allowed = {'valid', 'same', 'causal'}\n     if K.backend() == 'theano':\n         allowed.add('full')\n     if padding not in allowed:\n-        raise ValueError('The `padding` argument must be one of \"valid\", \"same\". '\n+        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" (or \"causal\" for Conv1D). '\n                          'Received: ' + str(padding))\n     return padding\n \n@@ -102,12 +102,14 @@ def conv_output_length(input_length, filter_size,\n     \"\"\"\n     if input_length is None:\n         return None\n-    assert padding in {'same', 'valid', 'full'}\n+    assert padding in {'same', 'valid', 'full', 'causal'}\n     dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n     if padding == 'same':\n         output_length = input_length\n     elif padding == 'valid':\n         output_length = input_length - dilated_filter_size + 1\n+    elif padding == 'causal':\n+        output_length = input_length\n     elif padding == 'full':\n         output_length = input_length + dilated_filter_size - 1\n     return (output_length + stride - 1) // stride\n\n@@ -55,9 +55,11 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n                 input_data_shape[i] = np.random.randint(1, 4)\n         input_data = (10 * np.random.random(input_data_shape))\n         input_data = input_data.astype(input_dtype)\n-    elif input_shape is None:\n+    else:\n+        if input_shape is None:\n             input_shape = input_data.shape\n-\n+        if input_dtype is None:\n+            input_dtype = input_data.dtype\n     if expected_output_dtype is None:\n         expected_output_dtype = input_dtype\n \n\n@@ -16,6 +16,51 @@ else:\n     _convolution_paddings = ['valid', 'same']\n \n \n+@keras_test\n+def test_causal_dilated_conv():\n+    # Causal:\n+    layer_test(convolutional.Conv1D,\n+               input_data=np.reshape(np.arange(4, dtype='float32'), (1, 4, 1)),\n+               kwargs={\n+                   'filters': 1,\n+                   'kernel_size': 2,\n+                   'dilation_rate': 1,\n+                   'padding': 'causal',\n+                   'kernel_initializer': 'ones',\n+                   'use_bias': False,\n+               },\n+               expected_output=[[[0], [1], [3], [5]]]\n+               )\n+\n+    # Non-causal:\n+    layer_test(convolutional.Conv1D,\n+               input_data=np.reshape(np.arange(4, dtype='float32'), (1, 4, 1)),\n+               kwargs={\n+                   'filters': 1,\n+                   'kernel_size': 2,\n+                   'dilation_rate': 1,\n+                   'padding': 'valid',\n+                   'kernel_initializer': 'ones',\n+                   'use_bias': False,\n+               },\n+               expected_output=[[[1], [3], [5]]]\n+               )\n+\n+    # Causal dilated with larger kernel size:\n+    layer_test(convolutional.Conv1D,\n+               input_data=np.reshape(np.arange(10, dtype='float32'), (1, 10, 1)),\n+               kwargs={\n+                   'filters': 1,\n+                   'kernel_size': 3,\n+                   'dilation_rate': 2,\n+                   'padding': 'causal',\n+                   'kernel_initializer': 'ones',\n+                   'use_bias': False,\n+               },\n+               expected_output=np.float32([[[0], [1], [2], [4], [6], [9], [12], [15], [18], [21]]])\n+               )\n+\n+\n @keras_test\n def test_conv_1d():\n     batch_size = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a85378979521640eaa7767acfd3c9af639f83711", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9581 | Contributors (this commit): 34 | Commits (past 90d): 36 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -1777,7 +1777,7 @@ class Cropping3D(Layer):\n                               self.cropping[2][0]:,\n                               :]\n             elif self.cropping[0][1] == self.cropping[2][1] == 0:\n-                return x[:,\n+                return inputs[:,\n                               self.cropping[0][0]:,\n                               self.cropping[1][0]:-self.cropping[1][1],\n                               self.cropping[2][0]:,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b2f06de4949dc7f2fe9a5ac42531127fb04e48a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 5386 | Contributors (this commit): 57 | Commits (past 90d): 56 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1727,9 +1727,9 @@ def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n     if padding == 'same':\n         w_pad = pool_size[0] - 2 if pool_size[0] > 2 and pool_size[0] % 2 == 1 else pool_size[0] - 1\n         h_pad = pool_size[1] - 2 if pool_size[1] > 2 and pool_size[1] % 2 == 1 else pool_size[1] - 1\n-        padding = (w_pad, h_pad)\n+        pad = (w_pad, h_pad)\n     elif padding == 'valid':\n-        padding = (0, 0)\n+        pad = (0, 0)\n     else:\n         raise ValueError('Invalid border mode:', padding)\n \n@@ -1742,20 +1742,18 @@ def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n     if pool_mode == 'max':\n         pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,\n                                 ignore_border=True,\n-                                pad=padding,\n+                                pad=pad,\n                                 mode='max')\n     elif pool_mode == 'avg':\n         pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,\n                                 ignore_border=True,\n-                                pad=padding,\n+                                pad=pad,\n                                 mode='average_exc_pad')\n     else:\n         raise ValueError('Invalid pooling mode:', pool_mode)\n-\n     if padding == 'same':\n         expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n         expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n-\n         pool_out = pool_out[:, :,\n                             : expected_width,\n                             : expected_height]\n\n@@ -113,10 +113,11 @@ def test_maxpooling_1d():\n \n @keras_test\n def test_averagepooling_1d():\n+  for padding in ['valid', 'same']:\n     for stride in [1, 2]:\n         layer_test(convolutional.AveragePooling1D,\n                    kwargs={'strides': stride,\n-                           'padding': 'valid'},\n+                           'padding': padding},\n                    input_shape=(3, 5, 4))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad21188a13b870ccd3d7dcc1afb95f8b9aa20200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 7 | Churn Cumulative: 9040 | Contributors (this commit): 55 | Commits (past 90d): 29 | Contributors (cumulative): 55 | DMM Complexity: 0.0\n\nDIFF:\n@@ -138,6 +138,13 @@ def save_model(model, filepath, overwrite=True):\n                 weight_values = K.batch_get_value(symbolic_weights)\n                 weight_names = []\n                 for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n+                    # Default values of symbolic_weights is /variable for theano\n+                    if K.backend() == 'theano':\n+                        if hasattr(w, 'name') and w.name != \"/variable\":\n+                            name = str(w.name)\n+                        else:\n+                            name = 'param_' + str(i)\n+                    else:\n                         if hasattr(w, 'name') and w.name:\n                             name = str(w.name)\n                         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b5984b8ce8799601ddf8f3fae0344b626c4ba48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ def _test_optimizer(optimizer, target=0.75):\n     model.compile(loss='categorical_crossentropy',\n                   optimizer=optimizer,\n                   metrics=['accuracy'])\n-    history = model.fit(x_train, y_train, epochs=1, batch_size=16, verbose=0)\n+    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n     assert history.history['acc'][-1] >= target\n     config = optimizers.serialize(optimizer)\n     optim = optimizers.deserialize(config)\n@@ -60,8 +60,8 @@ def test_adagrad():\n \n \n def test_adadelta():\n-    _test_optimizer(optimizers.Adadelta())\n-    _test_optimizer(optimizers.Adadelta(decay=1e-3))\n+    _test_optimizer(optimizers.Adadelta(), target=0.6)\n+    _test_optimizer(optimizers.Adadelta(decay=1e-3), target=0.6)\n \n \n def test_adam():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d8cf2fd6e68847b4ac6d1b6eec9d2c1eccabf75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 4160 | Contributors (this commit): 35 | Commits (past 90d): 34 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -541,8 +541,17 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n                              'sample_weight array is 1D.')\n \n     if sample_weight is not None:\n-        assert len(sample_weight.shape) <= len(y.shape)\n-        assert y.shape[:sample_weight.ndim] == sample_weight.shape\n+        if len(sample_weight.shape) > len(y.shape):\n+            raise ValueError('Found a sample_weight with shape' +\n+                             str(sample_weight.shape) + '.'\n+                             'Expected sample_weight with rank '\n+                             'less than or equal to ' + str(len(y.shape)))\n+\n+        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) + ' for an input with shape ' +\n+                             str(y.shape) + '. '\n+                             'sample_weight cannot be broadcast.')\n         return sample_weight\n     elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38a6dae44a507d816d5cdb0fb2f3329f21ad9f2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 619 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -57,8 +57,8 @@ class _Merge(Layer):\n         return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n \n \n-class Sum(_Merge):\n-    \"\"\"Layer that sums a list of inputs.\n+class Add(_Merge):\n+    \"\"\"Layer that adds a list of inputs.\n \n     It takes as input a list of tensors,\n     all of the same shape, and returns\n@@ -313,8 +313,8 @@ class Dot(_Merge):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-def sum(inputs, **kwargs):\n-    \"\"\"Functional interface to the `Sum` layer.\n+def add(inputs, **kwargs):\n+    \"\"\"Functional interface to the `Add` layer.\n \n     # Arguments\n         inputs: A list of input tensors (at least 2).\n\n@@ -9,11 +9,11 @@ from keras.layers import merge\n \n \n @keras_test\n-def test_merge_sum():\n+def test_merge_add():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n     i3 = layers.Input(shape=(4, 5))\n-    o = layers.sum([i1, i2, i3])\n+    o = layers.add([i1, i2, i3])\n     assert o._keras_shape == (None, 4, 5)\n     model = models.Model([i1, i2, i3], o)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eec61d9d4921fb7a62926e217fbfe43cd7062ac4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 102 | Files Changed: 8 | Hunks: 63 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 206 | Churn Cumulative: 1855 | Contributors (this commit): 20 | Commits (past 90d): 26 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from keras.preprocessing import sequence\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation\n from keras.layers import Embedding\n-from keras.layers import Convolution1D, GlobalMaxPooling1D\n+from keras.layers import Conv1D, GlobalMaxPooling1D\n from keras.datasets import imdb\n \n \n@@ -24,7 +24,7 @@ maxlen = 400\n batch_size = 32\n embedding_dims = 50\n filters = 250\n-filter_length = 3\n+kernel_size = 3\n hidden_dims = 250\n epochs = 2\n \n@@ -46,16 +46,16 @@ model = Sequential()\n # our vocab indices into embedding_dims dimensions\n model.add(Embedding(max_features,\n                     embedding_dims,\n-                    input_length=maxlen,\n-                    dropout=0.2))\n+                    input_length=maxlen))\n+model.add(Dropout(0.2))\n \n # we add a Convolution1D, which will learn filters\n # word group filters of size filter_length:\n-model.add(Convolution1D(filters=filters,\n-                        filter_length=filter_length,\n-                        border_mode='valid',\n+model.add(Conv1D(filters,\n+                 kernel_size,\n+                 padding='valid',\n                  activation='relu',\n-                        subsample_length=1))\n+                 strides=1))\n # we use max pooling:\n model.add(GlobalMaxPooling1D())\n \n\n@@ -12,7 +12,7 @@ from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation\n from keras.layers import Embedding\n from keras.layers import LSTM\n-from keras.layers import Convolution1D, MaxPooling1D\n+from keras.layers import Conv1D, MaxPooling1D\n from keras.datasets import imdb\n \n \n@@ -22,9 +22,9 @@ maxlen = 100\n embedding_size = 128\n \n # Convolution\n-filter_length = 5\n+kernel_size = 5\n filters = 64\n-pool_length = 4\n+pool_size = 4\n \n # LSTM\n lstm_output_size = 70\n@@ -55,12 +55,12 @@ print('Build model...')\n model = Sequential()\n model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n model.add(Dropout(0.25))\n-model.add(Convolution1D(filters=filters,\n-                        filter_length=filter_length,\n-                        border_mode='valid',\n+model.add(Conv1D(filters,\n+                 kernel_size,\n+                 padding='valid',\n                  activation='relu',\n-                        subsample_length=1))\n-model.add(MaxPooling1D(pool_length=pool_length))\n+                 strides=1))\n+model.add(MaxPooling1D(pool_size=pool_size))\n model.add(LSTM(lstm_output_size))\n model.add(Dense(1))\n model.add(Activation('sigmoid'))\n\n@@ -1,13 +1,13 @@\n '''Compare LSTM implementations on the IMDB sentiment classification task.\n \n-consume_less='cpu' preprocesses input to the LSTM which typically results in\n+implementation=0 preprocesses input to the LSTM which typically results in\n faster computations at the expense of increased peak memory usage as the\n preprocessed input must be kept in memory.\n \n-consume_less='mem' does away with the preprocessing, meaning that it might take\n+implementation=1 does away with the preprocessing, meaning that it might take\n a little longer, but should require less peak memory.\n \n-consume_less='gpu' concatenates the input, output and forget gate's weights\n+implementation=2 concatenates the input, output and forget gate's weights\n into one, large matrix, resulting in faster computation time as the GPU can\n utilize more cores, at the expense of reduced regularization because the same\n dropout is shared across the gates.\n@@ -22,7 +22,7 @@ import matplotlib.pyplot as plt\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Embedding, Dense, LSTM\n+from keras.layers import Embedding, Dense, LSTM, Dropout\n from keras.datasets import imdb\n \n max_features = 20000\n@@ -30,7 +30,7 @@ max_length = 80\n embedding_dim = 256\n batch_size = 128\n epochs = 10\n-modes = ['cpu', 'mem', 'gpu']\n+modes = [0, 1, 2]\n \n print('Loading data...')\n (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n@@ -40,11 +40,12 @@ X_test = sequence.pad_sequences(X_test, max_length)\n # Compile and train different models while meauring performance.\n results = []\n for mode in modes:\n-    print('Testing mode: consume_less=\"{}\"'.format(mode))\n+    print('Testing mode: implementation={}'.format(mode))\n \n     model = Sequential()\n-    model.add(Embedding(max_features, embedding_dim, input_length=max_length, dropout=0.2))\n-    model.add(LSTM(embedding_dim, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n+    model.add(Embedding(max_features, embedding_dim, input_length=max_length))\n+    model.add(Dropout(0.2))\n+    model.add(LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2, implementation=mode))\n     model.add(Dense(1, activation='sigmoid'))\n     model.compile(loss='binary_crossentropy',\n                   optimizer='adam',\n\n@@ -57,17 +57,17 @@ def build_generator(latent_size):\n \n     # upsample to (..., 14, 14)\n     cnn.add(UpSampling2D(size=(2, 2)))\n-    cnn.add(Convolution2D(256, 5, 5, border_mode='same',\n-                          activation='relu', init='glorot_normal'))\n+    cnn.add(Convolution2D(256, 5, padding='same',\n+                          activation='relu', kernel_initializer='glorot_normal'))\n \n     # upsample to (..., 28, 28)\n     cnn.add(UpSampling2D(size=(2, 2)))\n-    cnn.add(Convolution2D(128, 5, 5, border_mode='same',\n-                          activation='relu', init='glorot_normal'))\n+    cnn.add(Convolution2D(128, 5, padding='same',\n+                          activation='relu', kernel_initializer='glorot_normal'))\n \n     # take a channel axis reduction\n-    cnn.add(Convolution2D(1, 2, 2, border_mode='same',\n-                          activation='tanh', init='glorot_normal'))\n+    cnn.add(Convolution2D(1, 2, padding='same',\n+                          activation='tanh', kernel_initializer='glorot_normal'))\n \n     # this is the z space commonly refered to in GAN papers\n     latent = Input(shape=(latent_size, ))\n@@ -77,14 +77,14 @@ def build_generator(latent_size):\n \n     # 10 classes in MNIST\n     cls = Flatten()(Embedding(10, latent_size,\n-                              init='glorot_normal')(image_class))\n+                              embeddings_initializer='glorot_normal')(image_class))\n \n     # hadamard product between z-space and a class conditional embedding\n     h = merge([latent, cls], mode='mul')\n \n     fake_image = cnn(h)\n \n-    return Model(input=[latent, image_class], output=fake_image)\n+    return Model([latent, image_class], fake_image)\n \n \n def build_discriminator():\n@@ -92,20 +92,20 @@ def build_discriminator():\n     # the reference paper\n     cnn = Sequential()\n \n-    cnn.add(Convolution2D(32, 3, 3, border_mode='same', subsample=(2, 2),\n+    cnn.add(Convolution2D(32, 3, padding='same', strides=2,\n                           input_shape=(1, 28, 28)))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n-    cnn.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n+    cnn.add(Convolution2D(64, 3, padding='same', strides=2))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n-    cnn.add(Convolution2D(128, 3, 3, border_mode='same', subsample=(2, 2)))\n+    cnn.add(Convolution2D(128, 3, padding='same', strides=2))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n-    cnn.add(Convolution2D(256, 3, 3, border_mode='same', subsample=(1, 1)))\n+    cnn.add(Convolution2D(256, 3, padding='same', strides=1))\n     cnn.add(LeakyReLU())\n     cnn.add(Dropout(0.3))\n \n@@ -122,12 +122,12 @@ def build_discriminator():\n     fake = Dense(1, activation='sigmoid', name='generation')(features)\n     aux = Dense(10, activation='softmax', name='auxiliary')(features)\n \n-    return Model(input=image, output=[fake, aux])\n+    return Model(image, [fake, aux])\n \n if __name__ == '__main__':\n \n     # batch and latent size taken from the paper\n-    epochss = 50\n+    epochs = 50\n     batch_size = 100\n     latent_size = 100\n \n@@ -156,7 +156,7 @@ if __name__ == '__main__':\n     # we only want to be able to train generation for the combined model\n     discriminator.trainable = False\n     fake, aux = discriminator(fake)\n-    combined = Model(input=[latent, image_class], output=[fake, aux])\n+    combined = Model([latent, image_class], [fake, aux])\n \n     combined.compile(\n         optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n@@ -177,8 +177,8 @@ if __name__ == '__main__':\n     train_history = defaultdict(list)\n     test_history = defaultdict(list)\n \n-    for epoch in range(epochss):\n-        print('Epoch {} of {}'.format(epoch + 1, epochss))\n+    for epoch in range(epochs):\n+        print('Epoch {} of {}'.format(epoch + 1, epochs))\n \n         num_batches = int(X_train.shape[0] / batch_size)\n         progress_bar = Progbar(target=num_batches)\n\n@@ -35,53 +35,53 @@ from keras.utils import np_utils\n # Training parameters.\n batch_size = 32\n num_classes = 10\n-epochss = 5\n+epochs = 5\n \n # Embedding dimensions.\n row_hidden = 128\n col_hidden = 128\n \n # The data, shuffled and split between train and test sets.\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n # Reshapes data to 4D for Hierarchical RNN.\n-X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n-X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-print('X_train shape:', X_train.shape)\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n+x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # Converts class vectors to binary class matrices.\n Y_train = np_utils.to_categorical(y_train, num_classes)\n Y_test = np_utils.to_categorical(y_test, num_classes)\n \n-row, col, pixel = X_train.shape[1:]\n+row, col, pixel = x_train.shape[1:]\n \n # 4D input.\n x = Input(shape=(row, col, pixel))\n \n # Encodes a row of pixels using TimeDistributed Wrapper.\n-encoded_rows = TimeDistributed(LSTM(output_dim=row_hidden))(x)\n+encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n \n # Encodes columns of encoded rows.\n encoded_columns = LSTM(col_hidden)(encoded_rows)\n \n # Final predictions and model.\n prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n-model = Model(input=x, output=prediction)\n+model = Model(x, prediction)\n model.compile(loss='categorical_crossentropy',\n               optimizer='rmsprop',\n               metrics=['accuracy'])\n \n # Training.\n-model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochss,\n-          verbose=1, validation_data=(X_test, Y_test))\n+model.fit(x_train, Y_train, batch_size=batch_size, epochs=epochs,\n+          verbose=1, validation_data=(x_test, Y_test))\n \n # Evaluation.\n-scores = model.evaluate(X_test, Y_test, verbose=0)\n+scores = model.evaluate(x_test, Y_test, verbose=0)\n print('Test loss:', scores[0])\n print('Test accuracy:', scores[1])\n\n@@ -25,7 +25,7 @@ from keras.utils import np_utils\n \n batch_size = 32\n num_classes = 10\n-epochss = 200\n+epochs = 200\n hidden_units = 100\n \n learning_rate = 1e-6\n@@ -50,9 +50,9 @@ Y_test = np_utils.to_categorical(y_test, num_classes)\n \n print('Evaluate IRNN...')\n model = Sequential()\n-model.add(SimpleRNN(output_dim=hidden_units,\n-                    init=initializers.RandomNormal(stddev=0.001),\n-                    inner_init=initializers.Identity(gain=1.0),\n+model.add(SimpleRNN(hidden_units,\n+                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n+                    recurrent_initializer=initializers.Identity(gain=1.0),\n                     activation='relu',\n                     input_shape=X_train.shape[1:]))\n model.add(Dense(num_classes))\n@@ -62,7 +62,7 @@ model.compile(loss='categorical_crossentropy',\n               optimizer=rmsprop,\n               metrics=['accuracy'])\n \n-model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochss,\n+model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n           verbose=1, validation_data=(X_test, Y_test))\n \n scores = model.evaluate(X_test, Y_test, verbose=0)\n\n@@ -79,22 +79,22 @@ def compute_accuracy(predictions, labels):\n \n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train = X_train.reshape(60000, 784)\n-X_test = X_test.reshape(10000, 784)\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+x_train = x_train.reshape(60000, 784)\n+x_test = x_test.reshape(10000, 784)\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n input_dim = 784\n epochs = 20\n \n # create training+test positive and negative pairs\n digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n-tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n+tr_pairs, tr_y = create_pairs(x_train, digit_indices)\n \n digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n-te_pairs, te_y = create_pairs(X_test, digit_indices)\n+te_pairs, te_y = create_pairs(x_test, digit_indices)\n \n # network definition\n base_network = create_base_network(input_dim)\n@@ -110,7 +110,7 @@ processed_b = base_network(input_b)\n \n distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n \n-model = Model(input=[input_a, input_b], output=distance)\n+model = Model([input_a, input_b], distance)\n \n # train\n rms = RMSprop()\n\n@@ -20,7 +20,7 @@ np.random.seed(1337)  # for reproducibility\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Convolution2D, MaxPooling2D\n+from keras.layers import Conv2D, MaxPooling2D\n from keras.utils import np_utils\n from keras import backend as K\n \n@@ -33,7 +33,7 @@ epochs = 5\n # input image dimensions\n img_rows, img_cols = 28, 28\n # number of convolutional filters to use\n-filterss = 32\n+filters = 32\n # size of pooling area for max pooling\n pool_size = 2\n # convolution kernel size\n@@ -46,61 +46,62 @@ else:\n \n \n def train_model(model, train, test, num_classes):\n-    X_train = train[0].reshape((train[0].shape[0],) + input_shape)\n-    X_test = test[0].reshape((test[0].shape[0],) + input_shape)\n-    X_train = X_train.astype('float32')\n-    X_test = X_test.astype('float32')\n-    X_train /= 255\n-    X_test /= 255\n-    print('X_train shape:', X_train.shape)\n-    print(X_train.shape[0], 'train samples')\n-    print(X_test.shape[0], 'test samples')\n+    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n+    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n+    x_train = x_train.astype('float32')\n+    x_test = x_test.astype('float32')\n+    x_train /= 255\n+    x_test /= 255\n+    print('x_train shape:', x_train.shape)\n+    print(x_train.shape[0], 'train samples')\n+    print(x_test.shape[0], 'test samples')\n \n     # convert class vectors to binary class matrices\n-    Y_train = np_utils.to_categorical(train[1], num_classes)\n-    Y_test = np_utils.to_categorical(test[1], num_classes)\n+    y_train = np_utils.to_categorical(train[1], num_classes)\n+    y_test = np_utils.to_categorical(test[1], num_classes)\n \n     model.compile(loss='categorical_crossentropy',\n                   optimizer='adadelta',\n                   metrics=['accuracy'])\n \n     t = now()\n-    model.fit(X_train, Y_train,\n+    model.fit(x_train, y_train,\n               batch_size=batch_size, epochs=epochs,\n               verbose=1,\n-              validation_data=(X_test, Y_test))\n+              validation_data=(x_test, y_test))\n     print('Training time: %s' % (now() - t))\n-    score = model.evaluate(X_test, Y_test, verbose=0)\n+    score = model.evaluate(x_test, y_test, verbose=0)\n     print('Test score:', score[0])\n     print('Test accuracy:', score[1])\n \n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n # create two datasets one with digits below 5 and one with 5 and above\n-X_train_lt5 = X_train[y_train < 5]\n+x_train_lt5 = x_train[y_train < 5]\n y_train_lt5 = y_train[y_train < 5]\n-X_test_lt5 = X_test[y_test < 5]\n+x_test_lt5 = x_test[y_test < 5]\n y_test_lt5 = y_test[y_test < 5]\n \n-X_train_gte5 = X_train[y_train >= 5]\n+x_train_gte5 = x_train[y_train >= 5]\n y_train_gte5 = y_train[y_train >= 5] - 5  # make classes start at 0 for\n-X_test_gte5 = X_test[y_test >= 5]         # np_utils.to_categorical\n+x_test_gte5 = x_test[y_test >= 5]         # np_utils.to_categorical\n y_test_gte5 = y_test[y_test >= 5] - 5\n \n # define two groups of layers: feature (convolutions) and classification (dense)\n feature_layers = [\n-    Convolution2D(filterss, kernel_size, kernel_size,\n-                  border_mode='valid',\n+    Conv2D(filters, kernel_size,\n+           padding='valid',\n            input_shape=input_shape),\n     Activation('relu'),\n-    Convolution2D(filterss, kernel_size, kernel_size),\n+    Conv2D(filters, kernel_size),\n     Activation('relu'),\n-    MaxPooling2D(pool_size=(pool_size, pool_size)),\n+    MaxPooling2D(pool_size=pool_size),\n     Dropout(0.25),\n     Flatten(),\n ]\n+\n classification_layers = [\n     Dense(128),\n     Activation('relu'),\n@@ -114,8 +115,8 @@ model = Sequential(feature_layers + classification_layers)\n \n # train model for 5-digit classification [0..4]\n train_model(model,\n-            (X_train_lt5, y_train_lt5),\n-            (X_test_lt5, y_test_lt5), num_classes)\n+            (x_train_lt5, y_train_lt5),\n+            (x_test_lt5, y_test_lt5), num_classes)\n \n # freeze feature layers and rebuild model\n for l in feature_layers:\n@@ -123,5 +124,5 @@ for l in feature_layers:\n \n # transfer: train dense layers for new classification task [5..9]\n train_model(model,\n-            (X_train_gte5, y_train_gte5),\n-            (X_test_gte5, y_test_gte5), num_classes)\n+            (x_train_gte5, y_train_gte5),\n+            (x_test_gte5, y_test_gte5), num_classes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
