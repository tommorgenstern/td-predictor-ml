{"custom_id": "keras#cf199f341bd2c179c1737dde75ec55d07422f69e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 92 | Files Changed: 9 | Hunks: 40 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 196 | Churn Cumulative: 3542 | Contributors (this commit): 9 | Commits (past 90d): 90 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,6 @@ _DTENSOR_API_ENABLED = True\n if _DTENSOR_API_ENABLED:\n     from tensorflow.compat.v2.experimental import dtensor as dtensor_api\n else:\n-    # Leave it with a placeholder, so that the import line from other python file\n-    # will not break.\n+    # Leave it with a placeholder, so that the import line from other python\n+    # file will not break.\n     dtensor_api = None\n\n@@ -111,8 +111,8 @@ class InitializersTest(test_util.DTensorBaseTest):\n             new_value = initializer(shape=shape, layout=layout)\n             self.assertAllClose(value, new_value)\n         finally:\n-            # Unset the keras global generator so that it doesn't affect other tests\n-            # that need to verify the existence of global generator.\n+            # Unset the keras global generator so that it doesn't affect other\n+            # tests that need to verify the existence of global generator.\n             backend._SEED_GENERATOR.generator = None\n \n     @parameterized.named_parameters(\n\n@@ -51,10 +51,10 @@ def get_current_layout_map():\n class LayoutMap(collections.abc.MutableMapping):\n     \"\"\"A dict-like object that maps string to `Layout` instances.\n \n-    `LayoutMap` uses a string as key and a `Layout` as value. There is a behavior\n-    difference between a normal Python dict and this class. The string key will be\n-    treated as a regex when retrieving the value. See the docstring of\n-    `get` for more details.\n+    `LayoutMap` uses a string as key and a `Layout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n \n     See below for a usage example. You can define the naming schema\n     of the `Layout`, and then retrieve the corresponding `Layout` instance.\n@@ -91,9 +91,9 @@ class LayoutMap(collections.abc.MutableMapping):\n         \"\"\"Retrieve the corresponding layout by the string key.\n \n         When there isn't an exact match, all the existing keys in the layout map\n-        will be treated as a regex and map against the input key again. The first\n-        match will be returned, based on the key insertion order. Return None if\n-        there isn't any match found.\n+        will be treated as a regex and map against the input key again. The\n+        first match will be returned, based on the key insertion order. Return\n+        None if there isn't any match found.\n \n         Args:\n           key: the string key as the query for the layout.\n@@ -159,10 +159,10 @@ def layout_map_scope(layout_map):\n     to map the variable against the layout.\n \n     For subclassed models, the full object/attribute name is used as the key.\n-    For Functional/Sequential models, since the layers within the model do not get\n-    assigned to a meaningful attribute, we use `layer.name` as the key\n-    for the layer, followed by the attribute name. Keras ensures\n-    name uniqueness among the layers in all Functional/Sequential models.\n+    For Functional/Sequential models, since the layers within the model do not\n+    get assigned to a meaningful attribute, we use `layer.name` as the key for\n+    the layer, followed by the attribute name. Keras ensures name uniqueness\n+    among the layers in all Functional/Sequential models.\n \n     See the following examples that show the variable object names\n     for different Keras model types:\n@@ -230,8 +230,8 @@ def layout_map_scope(layout_map):\n     ```\n \n     Args:\n-      layout_map: a LayoutMap which contains the variable_object_path (string) ->\n-        Layout. When a layout is not found for the variable, a default all\n+      layout_map: a LayoutMap which contains the variable_object_path (string)\n+        -> Layout. When a layout is not found for the variable, a default all\n         replicated layout will be created for the variable.\n \n     Yields:\n@@ -299,11 +299,12 @@ def _map_functional_model_variable(model, layout_map):\n         # name based on the class name.\n         layer_name = layer.name\n         for path, variable in layer._flatten(\n-            predicate=_is_lazy_init_variable,  # pylint: disable=protected-access\n+            predicate=_is_lazy_init_variable,\n             with_path=True,\n         ):\n             # Note that path is a tuple that contains string and ints, eg:\n-            # ('d1', '_trainable_weights', 0) maps to model.d1._trainable_weights[0]\n+            # ('d1', '_trainable_weights', 0) maps to\n+            # model.d1._trainable_weights[0]\n             if [a for a in _KERAS_ATTRIBUTES_TO_SKIP if a in path]:\n                 continue\n             # Convert all the ints to string and join with .\n@@ -319,10 +320,11 @@ def _map_functional_model_variable(model, layout_map):\n             layer, lazy_init_variable_to_tf_variable_map\n         )\n \n-        # After we replaced all the variables, we want to make sure all the cached\n-        # attributes are having the new variable, rather than old LazyInitVariable.\n+        # After we replaced all the variables, we want to make sure all the\n+        # cached attributes are having the new variable, rather than old\n+        # LazyInitVariable.\n         for path, variable in layer._flatten(\n-            predicate=_is_lazy_init_variable,  # pylint: disable=protected-access\n+            predicate=_is_lazy_init_variable,\n             with_path=True,\n         ):\n             tf_variable = lazy_init_variable_to_tf_variable_map[id(variable)]\n@@ -335,14 +337,15 @@ def _map_functional_model_variable(model, layout_map):\n def _init_state_variable_for_rng(model, layout_map):\n     \"\"\"Init the state variable in tf.ranodm.Generator.\n \n-    Since the BaseRandomLayer in keras explicitly untrack the tf.random.Generator,\n-    the variable in it will stay as LazyInitVariable, which cause runtime error if\n-    we don't replace them with proper DVariable. Since user usually are not\n-    aware the existence of those variable, we will just give them replicated\n-    layout since they are tiny.\n+    Since the BaseRandomLayer in keras explicitly untrack the\n+    tf.random.Generator, the variable in it will stay as LazyInitVariable, which\n+    cause runtime error if we don't replace them with proper DVariable. Since\n+    user usually are not aware the existence of those variable, we will just\n+    give them replicated layout since they are tiny.\n \n     Args:\n-      model: the model whose layers will be checked to find the BaseRandomLayers.\n+      model: the model whose layers will be checked to find the\n+        BaseRandomLayers.\n       layout_map: used to get the default mesh information to create DVariable.\n     \"\"\"\n     # pylint: disable=protected-access\n@@ -352,10 +355,10 @@ def _init_state_variable_for_rng(model, layout_map):\n         keras_generator = l._random_generator\n         if keras_generator._built and keras_generator._generator is None:\n             raise ValueError(\n-                \"Keras is expected to use tf.random.Generator when using DTensor API.\"\n-                \"Please call \"\n-                \"`tf.keras.backend.experimental.enable_tf_random_generator` at the \"\n-                \"beginning of your program.\"\n+                \"Keras is expected to use tf.random.Generator when using \"\n+                \"DTensor API. Please call \"\n+                \"`tf.keras.backend.experimental.enable_tf_random_generator` at \"\n+                \"the beginning of your program.\"\n             )\n         if hasattr(keras_generator, \"_generator\") and _is_lazy_init_variable(\n             keras_generator._generator._state_var\n@@ -365,8 +368,9 @@ def _init_state_variable_for_rng(model, layout_map):\n                 layout_map, \"\", keras_generator._generator._state_var\n             )\n         else:\n-            # When the keras_generator is not built yet. Call the init function with\n-            # DTensor device to init all the variable with default replicated layout.\n+            # When the keras_generator is not built yet. Call the init function\n+            # with DTensor device to init all the variable with default\n+            # replicated layout.\n             with dtensor.run_on(layout_map.get_default_mesh()):\n                 keras_generator._maybe_init()\n \n@@ -376,17 +380,17 @@ def _config_dvariable_regularization(\n ):\n     \"\"\"Update the weights regularizer for newly created `DVariable`.\n \n-    The weight regularization usually happens when `layer.add_weight()` is called,\n-    at which point the library will first create a `LazyInitVariable`, and then\n-    replace it with a `DVariable`. We will defer the creation of those losses,\n-    until the DVariable is created.\n+    The weight regularization usually happens when `layer.add_weight()` is\n+    called, at which point the library will first create a `LazyInitVariable`,\n+    and then replace it with a `DVariable`. We will defer the creation of those\n+    losses, until the DVariable is created.\n \n     See `layer._captured_weight_regularizer` for more details.\n \n     Args:\n       layer: the layer instance for DVariable regularization config.\n-      lazy_init_variable_to_tf_variable_map: the dict between LazyInitVariable ID\n-        and newly created DVariable.\n+      lazy_init_variable_to_tf_variable_map: the dict between LazyInitVariable\n+        ID and newly created DVariable.\n     \"\"\"\n     # pylint: disable=protected-access\n     for (name, variable, regualarizer) in layer._captured_weight_regularizer:\n@@ -411,8 +415,8 @@ def _create_dvariable(layout_map, object_path, variable):\n     find any variables.\n \n     Args:\n-      layout_map: a LayoutMap which contains the variable_object_path (string) ->\n-        Layout.\n+      layout_map: a LayoutMap which contains the variable_object_path (string)\n+        -> Layout.\n       object_path: string, the object attribute path for the variable.\n       variable: LazyInitVariable which will be replaced by the newly created\n         tf.Variable.\n@@ -432,8 +436,8 @@ def _create_dvariable(layout_map, object_path, variable):\n         with lazy_variable.disable_init_variable_creator():\n             init_val = utils.call_with_layout(init_val, layout)\n     else:\n-        # The init value is probably already created as a tensor, we will just copy\n-        # it to mesh and give it a proper layout.\n+        # The init value is probably already created as a tensor, we will just\n+        # copy it to mesh and give it a proper layout.\n         init_val = dtensor.copy_to_mesh(init_val, layout)\n     # Use the original variable name for new DVariable creation. TF was adding\n     # \":0\" suffix to it.\n@@ -460,8 +464,8 @@ def _set_object_by_path(object_to_set, path, value):\n         if i == len(path) - 1:\n             # We found the actual attribute to set\n             if isinstance(attr_name, int):\n-                # This means we are trying to set an element in the array, make sure the\n-                # instance is array like object.\n+                # This means we are trying to set an element in the array, make\n+                # sure the instance is array like object.\n                 object_to_set[attr_name] = value\n             else:\n                 setattr(object_to_set, attr_name, value)\n\n@@ -81,8 +81,8 @@ class LayoutMapTest(test_util.DTensorBaseTest):\n         self.assertEqual(layout_map[\"dense/kernel\"], self.sharded_2d)\n         self.assertEqual(layout_map[\"dense/bias\"], self.sharded_1d)\n \n-        # Map against the wildcard bias rule for dense, and based on the order of\n-        # insertion, it will not use .*bias.\n+        # Map against the wildcard bias rule for dense, and based on the order\n+        # of insertion, it will not use .*bias.\n         self.assertEqual(layout_map[\"dense_2/kernel\"], self.layout_2d)\n         self.assertEqual(layout_map[\"dense_2/bias\"], self.layout_1d)\n \n@@ -181,8 +181,8 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         with layout_map_lib.layout_map_scope(layout_map):\n             model = SubclassModel(name=\"model\")\n \n-        # Init the model with eager tensor, make sure the model weights have correct\n-        # layout, as well as produce correct result.\n+        # Init the model with eager tensor, make sure the model weights have\n+        # correct layout, as well as produce correct result.\n         inputs = tf.zeros((10, 10))\n         inputs = dtensor.copy_to_mesh(inputs, layout=self.layout_2d)\n         result = model(inputs)\n@@ -206,9 +206,9 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n     def test_init_functional_model_variable_with_layout(self):\n         # Note that the functional model is using layers name + attribute name\n-        # the layer name are unique among the functional model, and when the layer\n-        # doesn't have a name, keras will give it a unique name based on the layer\n-        # class.\n+        # the layer name are unique among the functional model, and when the\n+        # layer doesn't have a name, keras will give it a unique name based on\n+        # the layer class.\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n         layout_map[\"d1.kernel\"] = self.layout_2d\n         layout_map[\"d1.bias\"] = self.layout_1d\n@@ -251,9 +251,9 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n     def test_init_sequential_model_variable_with_layout(self):\n         # Note that the sequential model is using layers name + attribute name\n-        # the layer name are unique among the functional model, and when the layer\n-        # doesn't have a name, keras will give it a unique name based on the layer\n-        # class.\n+        # the layer name are unique among the functional model, and when the\n+        # layer doesn't have a name, keras will give it a unique name based on\n+        # the layer class.\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n         layout_map[\"d1.kernel\"] = self.layout_2d\n         layout_map[\"d1.bias\"] = self.layout_1d\n\n@@ -55,8 +55,8 @@ def _infer_shape_dtype_and_create_handle(initial_value, shape, dtype, name):\n                         initial_value, trackable.CheckpointInitialValue\n                     ):\n                         raise NotImplementedError(\n-                            \"CheckpointInitialValue is not supported to be the initial \"\n-                            \"value of a lazy variable.\"\n+                            \"CheckpointInitialValue is not supported to be the \"\n+                            \"initial value of a lazy variable.\"\n                         )\n                     initial_value = ops.convert_to_tensor(\n                         initial_value, name=\"initial_value\", dtype=dtype\n@@ -69,7 +69,8 @@ def _infer_shape_dtype_and_create_handle(initial_value, shape, dtype, name):\n \n             assert dtype\n             assert shape\n-            handle = resource_variable_ops._variable_handle_from_shape_and_dtype(  # pylint: disable=protected-access\n+            handle = (\n+                resource_variable_ops._variable_handle_from_shape_and_dtype(\n                     shape=shape,\n                     dtype=dtype,\n                     shared_name=None,  # Never shared\n@@ -77,7 +78,9 @@ def _infer_shape_dtype_and_create_handle(initial_value, shape, dtype, name):\n                     graph_mode=False,\n                     initial_value=None,\n                 )\n-            # initial_value=initial_value if not callable(initial_value) else None)\n+            )\n+            # initial_value=initial_value if not callable(initial_value) else\n+            # None)\n     return initial_value, shape, dtype, handle, handle_name, unique_id\n \n \n@@ -188,9 +191,10 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n \n                 if not initial_value.shape.is_compatible_with(self._shape):\n                     raise ValueError(\n-                        f\"In this `tf.Variable` creation, the initial value's shape \"\n-                        f\"({initial_value.shape}) is not compatible with \"\n-                        f\"the explicitly supplied `shape` argument ({self._shape}).\"\n+                        f\"In this `tf.Variable` creation, the initial value's \"\n+                        f\"shape ({initial_value.shape}) is not compatible with \"\n+                        f\"the explicitly supplied `shape` \"\n+                        f\"argument ({self._shape}).\"\n                     )\n                 assert self._dtype is initial_value.dtype.base_dtype\n             gen_resource_variable_ops.assign_variable_op(\n\n@@ -64,8 +64,8 @@ class MnistTest(test_util.DTensorBaseTest):\n         self.assertEqual(train_losses, sorted(train_losses, reverse=True))\n \n     def DISABLED_test_mnist_training_tpu(self):\n-        # TODO(scottzhu): Enable TPU test once the dtensor_test rule is migrated out\n-        # of learning/brain\n+        # TODO(scottzhu): Enable TPU test once the dtensor_test rule is migrated\n+        # out of learning/brain\n         tpu_util.dtensor_initialize_tpu_system()\n         total_tpu_device_count = dtensor.num_global_devices(\"TPU\")\n         mesh_shape = [total_tpu_device_count]\n\n@@ -49,11 +49,11 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n             state variables created by this optimizer.\n           mesh: dtensor.Mesh. The optional Mesh which will be used to create\n             the states. Note that usually the state variable will use the layout\n-            from the corresponding model variables. This mesh only used for global\n-            variables like globle steps, learning rate, etc.\n+            from the corresponding model variables. This mesh only used for\n+            global variables like globle steps, learning rate, etc.\n         \"\"\"\n-        # TODO(scottzhu): Skip the gradients_clip_option and ema_option for now, and\n-        # will cover them in future if really needed.\n+        # TODO(scottzhu): Skip the gradients_clip_option and ema_option for now,\n+        # and will cover them in future if really needed.\n         # TODO(scottzhu): We might want to make mesh to be required in future.\n         self._mesh = mesh\n         super().__init__(name=name)\n@@ -65,7 +65,8 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n                 init_val, dtensor.Layout.replicated(self._mesh, rank=0)\n             )\n         with tf.init_scope():\n-            # Lift the variable creation to init scope to avoid environment issue.\n+            # Lift the variable creation to init scope to avoid environment\n+            # issue.\n             self._iterations = dtensor.DVariable(init_val, name=\"iteration\")\n \n     ################## Override methods from keras.Optimizer ################\n@@ -79,20 +80,20 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n         corresponding momemtum variable is created of the same shape and dtype.\n \n         Args:\n-          model_variable: The corresponding model variable to the optimizer variable\n-            to be created.\n-          variable_name: The name prefix of the optimizer variable to be created.\n-            The create variables name will follow the pattern\n+          model_variable: The corresponding model variable to the optimizer\n+            variable to be created.\n+          variable_name: The name prefix of the optimizer variable to be\n+            created.  The create variables name will follow the pattern\n             `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n-          initial_value: The initial value of the optimizer variable, if None, the\n-            value will be default to 0.\n+          initial_value: The initial value of the optimizer variable, if None,\n+            the value will be default to 0.\n \n         Returns:\n           An optimizer variable.\n         \"\"\"\n         if initial_value is None:\n-            # Use tf.zeros_like which will propagate the layout information from the\n-            # model weights if any.\n+            # Use tf.zeros_like which will propagate the layout information from\n+            # the model weights if any.\n             initial_value = tf.zeros_like(model_variable)\n         elif isinstance(initial_value, tf.Tensor):\n             initial_value = dtensor.copy_to_mesh(\n@@ -162,8 +163,8 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n             learning_rate, learning_rate_schedule.LearningRateSchedule\n         ):\n             # Create a variable to hold the current learning rate.\n-            # Note that the init value `learning_rate(self.iterations)` should have\n-            # the correct layout information from self.iterations.\n+            # Note that the init value `learning_rate(self.iterations)` should\n+            # have the correct layout information from self.iterations.\n             self._current_learning_rate = dtensor.DVariable(\n                 learning_rate(self.iterations),\n                 name=\"learning_rate\",\n\n@@ -44,11 +44,13 @@ class DTensorBaseTest(tf.test.TestCase, parameterized.TestCase):\n         \"\"\"Configs corresponding mesh given test context.\n \n         If runs on a CPU mesh, set virtual device on CPU.\n-        If runs on a GPU mesh, sets virtual device on GPU with proper memory limits.\n+        If runs on a GPU mesh, sets virtual device on GPU with proper memory\n+        limits.\n         if runs on a TPU mesh, initializes TPU system.\n \n         Args:\n-          device_type_mesh_map: A dictionary containing device_type -> mesh mapping.\n+          device_type_mesh_map: A dictionary containing device_type -> mesh\n+            mapping.\n \n         Returns:\n           A properly configured mesh for use in test.\n\n@@ -65,11 +65,12 @@ def allow_initializer_layout(init_method):\n \n     By adding this annotation, it will:\n \n-    1. Filter out the kwargs based on some keywords, eg if the 'kernel_initialzer'\n-       appears in method signature, then it will try to pop the 'kernel_layout' if\n-       it presents. Same for \"bias\" and \"recurrent_kernel\", etc. This will make\n-       sure the layout related param is not passed to `BaseLayer.__init__`, which\n-       will raise error about unexpect keyword args.\n+    1. Filter out the kwargs based on some keywords, eg if the\n+      'kernel_initialzer' appears in method signature, then it will try to pop\n+      the 'kernel_layout' if it presents. Same for \"bias\" and\n+      \"recurrent_kernel\", etc. This will make sure the layout related param is\n+      not passed to `BaseLayer.__init__`, which will raise error about unexpect\n+      keyword args.\n     2. Set the self.kernel/bias_layout attribute after the `__init__` method is\n        called. Keras framework will use those fields to create weights down the\n        stream.\n@@ -111,8 +112,8 @@ def inject_mesh(init_method):\n     DTensor mesh to create the weights, but doesn't want to change the current\n     public API interface.\n \n-    This is for temporary usage and eventually the mesh/layout information will be\n-    public arguments in the `__init__` method\n+    This is for temporary usage and eventually the mesh/layout information will\n+    be public arguments in the `__init__` method.\n \n     Sample usage:\n     ```python\n@@ -135,9 +136,9 @@ def inject_mesh(init_method):\n \n     def _wrap_function(instance, *args, **kwargs):\n         mesh = kwargs.pop(\"mesh\", None)\n-        # Note that the injection of _mesh need to happen before the invocation of\n-        # __init__, since the class might need the mesh to create weights in the\n-        # __init__.\n+        # Note that the injection of _mesh need to happen before the invocation\n+        # of __init__, since the class might need the mesh to create weights in\n+        # the __init__.\n         if mesh is not None:\n             instance._mesh = mesh  # pylint: disable=protected-access\n         init_method(instance, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba2b102dc6aadc84abe6906feef8737f9c989389", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 49 | Files Changed: 1 | Hunks: 15 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 102 | Churn Cumulative: 563 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -42,9 +42,9 @@ def model_to_estimator(\n ):\n     \"\"\"Constructs an `Estimator` instance from given keras model.\n \n-    If you use infrastructure or other tooling that relies on Estimators, you can\n-    still build a Keras model and use model_to_estimator to convert the Keras\n-    model to an Estimator for use with downstream systems.\n+    If you use infrastructure or other tooling that relies on Estimators, you\n+    can still build a Keras model and use model_to_estimator to convert the\n+    Keras model to an Estimator for use with downstream systems.\n \n     For usage example, please see:\n     [Creating estimators from Keras Models](\n@@ -99,13 +99,14 @@ def model_to_estimator(\n         exclusive with `keras_model_path`. Estimator's `model_fn` uses the\n         structure of the model to clone the model. Defaults to `None`.\n       keras_model_path: Path to a compiled Keras model saved on disk, in HDF5\n-        format, which can be generated with the `save()` method of a Keras model.\n-        This argument is mutually exclusive with `keras_model`.\n+        format, which can be generated with the `save()` method of a Keras\n+        model.  This argument is mutually exclusive with `keras_model`.\n         Defaults to `None`.\n       custom_objects: Dictionary for cloning customized objects. This is\n         used with classes that is not part of this pip package. For example, if\n-        user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,\n-        then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.\n+        user maintains a `relu6` class that inherits from\n+        `tf.keras.layers.Layer`, then pass `custom_objects={'relu6': relu6}`.\n+        Defaults to `None`.\n       model_dir: Directory to save `Estimator` model parameters, graph, summary\n         files for TensorBoard, etc. If unset a directory will be created with\n         `tempfile.mkdtemp`\n@@ -114,19 +115,19 @@ def model_to_estimator(\n         `model_dir`. Defaults to `None`. If both `config.model_dir` and the\n         `model_dir` argument (above) are specified the `model_dir` **argument**\n         takes precedence.\n-      checkpoint_format: Sets the format of the checkpoint saved by the estimator\n-        when training. May be `saver` or `checkpoint`, depending on whether to\n-        save checkpoints from `tf.train.Saver` or `tf.train.Checkpoint`. This\n-        argument currently defaults to `saver`. When 2.0 is released, the default\n-        will be `checkpoint`. Estimators use name-based `tf.train.Saver`\n-        checkpoints, while Keras models use object-based checkpoints from\n-        `tf.train.Checkpoint`. Currently, saving object-based checkpoints from\n-        `model_to_estimator` is only supported by Functional and Sequential\n-        models. Defaults to 'saver'.\n+      checkpoint_format: Sets the format of the checkpoint saved by the\n+        estimator when training. May be `saver` or `checkpoint`, depending on\n+        whether to save checkpoints from `tf.train.Saver` or\n+        `tf.train.Checkpoint`. This argument currently defaults to `saver`. When\n+        2.0 is released, the default will be `checkpoint`. Estimators use\n+        name-based `tf.train.Saver` checkpoints, while Keras models use\n+        object-based checkpoints from `tf.train.Checkpoint`. Currently, saving\n+        object-based checkpoints from `model_to_estimator` is only supported by\n+        Functional and Sequential models. Defaults to 'saver'.\n       metric_names_map: Optional dictionary mapping Keras model output metric\n         names to custom names. This can be used to override the default Keras\n-        model output metrics names in a multi IO model use case and provide custom\n-        names for the `eval_metric_ops` in Estimator.\n+        model output metrics names in a multi IO model use case and provide\n+        custom names for the `eval_metric_ops` in Estimator.\n         The Keras model metric names can be obtained using `model.metrics_names`\n         excluding any loss metrics such as total loss and output losses.\n         For example, if your Keras model has two outputs `out_1` and `out_2`,\n@@ -143,9 +144,10 @@ def model_to_estimator(\n         A dict `{name: output}` where:\n           * name: An arbitrary name for this output.\n           * output: an `ExportOutput` class such as `ClassificationOutput`,\n-            `RegressionOutput`, or `PredictOutput`. Single-headed models only need\n-            to specify one entry in this dictionary. Multi-headed models should\n-            specify one entry for each head, one of which must be named using\n+            `RegressionOutput`, or `PredictOutput`. Single-headed models only\n+            need to specify one entry in this dictionary. Multi-headed models\n+            should specify one entry for each head, one of which must be named\n+            using\n             `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`\n             If no entry is provided, a default `PredictOutput` mapping to\n             `predictions` will be created.\n@@ -167,8 +169,8 @@ def model_to_estimator(\n         )\n     except ImportError:\n         raise NotImplementedError(\n-            \"tf.keras.estimator.model_to_estimator function not available in your \"\n-            \"installation.\"\n+            \"tf.keras.estimator.model_to_estimator function not available in \"\n+            \"your installation.\"\n         )\n     _model_to_estimator_usage_gauge.get_cell(\"v1\").set(True)\n     return (\n@@ -199,9 +201,9 @@ def model_to_estimator_v2(\n ):\n     \"\"\"Constructs an `Estimator` instance from given keras model.\n \n-    If you use infrastructure or other tooling that relies on Estimators, you can\n-    still build a Keras model and use model_to_estimator to convert the Keras\n-    model to an Estimator for use with downstream systems.\n+    If you use infrastructure or other tooling that relies on Estimators, you\n+    can still build a Keras model and use model_to_estimator to convert the\n+    Keras model to an Estimator for use with downstream systems.\n \n     For usage example, please see:\n     [Creating estimators from Keras Models](\n@@ -251,10 +253,10 @@ def model_to_estimator_v2(\n     estimator.train(input_fn, steps=1)\n     ```\n \n-    Note: We do not support creating weighted metrics in Keras and converting them\n-    to weighted metrics in the Estimator API using `model_to_estimator`.\n-    You will have to create these metrics directly on the estimator spec using the\n-    `add_metrics` function.\n+    Note: We do not support creating weighted metrics in Keras and converting\n+    them to weighted metrics in the Estimator API using `model_to_estimator`.\n+    You will have to create these metrics directly on the estimator spec using\n+    the `add_metrics` function.\n \n     To customize the estimator `eval_metric_ops` names, you can pass in the\n     `metric_names_map` dictionary mapping the keras model output metric names\n@@ -296,13 +298,14 @@ def model_to_estimator_v2(\n         exclusive with `keras_model_path`. Estimator's `model_fn` uses the\n         structure of the model to clone the model. Defaults to `None`.\n       keras_model_path: Path to a compiled Keras model saved on disk, in HDF5\n-        format, which can be generated with the `save()` method of a Keras model.\n-        This argument is mutually exclusive with `keras_model`.\n+        format, which can be generated with the `save()` method of a Keras\n+        model.  This argument is mutually exclusive with `keras_model`.\n         Defaults to `None`.\n       custom_objects: Dictionary for cloning customized objects. This is\n         used with classes that is not part of this pip package. For example, if\n-        user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,\n-        then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.\n+        user maintains a `relu6` class that inherits from\n+        `tf.keras.layers.Layer`, then pass `custom_objects={'relu6': relu6}`.\n+        Defaults to `None`.\n       model_dir: Directory to save `Estimator` model parameters, graph, summary\n         files for TensorBoard, etc. If unset a directory will be created with\n         `tempfile.mkdtemp`\n@@ -311,18 +314,18 @@ def model_to_estimator_v2(\n         `model_dir`. Defaults to `None`. If both `config.model_dir` and the\n         `model_dir` argument (above) are specified the `model_dir` **argument**\n         takes precedence.\n-      checkpoint_format: Sets the format of the checkpoint saved by the estimator\n-        when training. May be `saver` or `checkpoint`, depending on whether to\n-        save checkpoints from `tf.compat.v1.train.Saver` or `tf.train.Checkpoint`.\n-        The default is `checkpoint`. Estimators use name-based `tf.train.Saver`\n-        checkpoints, while Keras models use object-based checkpoints from\n-        `tf.train.Checkpoint`. Currently, saving object-based checkpoints from\n-        `model_to_estimator` is only supported by Functional and Sequential\n-        models. Defaults to 'checkpoint'.\n+      checkpoint_format: Sets the format of the checkpoint saved by the\n+        estimator when training. May be `saver` or `checkpoint`, depending on\n+        whether to save checkpoints from `tf.compat.v1.train.Saver` or\n+        `tf.train.Checkpoint`.  The default is `checkpoint`. Estimators use\n+        name-based `tf.train.Saver` checkpoints, while Keras models use\n+        object-based checkpoints from `tf.train.Checkpoint`. Currently, saving\n+        object-based checkpoints from `model_to_estimator` is only supported by\n+        Functional and Sequential models. Defaults to 'checkpoint'.\n       metric_names_map: Optional dictionary mapping Keras model output metric\n         names to custom names. This can be used to override the default Keras\n-        model output metrics names in a multi IO model use case and provide custom\n-        names for the `eval_metric_ops` in Estimator.\n+        model output metrics names in a multi IO model use case and provide\n+        custom names for the `eval_metric_ops` in Estimator.\n         The Keras model metric names can be obtained using `model.metrics_names`\n         excluding any loss metrics such as total loss and output losses.\n         For example, if your Keras model has two outputs `out_1` and `out_2`,\n@@ -339,9 +342,10 @@ def model_to_estimator_v2(\n         A dict `{name: output}` where:\n           * name: An arbitrary name for this output.\n           * output: an `ExportOutput` class such as `ClassificationOutput`,\n-            `RegressionOutput`, or `PredictOutput`. Single-headed models only need\n-            to specify one entry in this dictionary. Multi-headed models should\n-            specify one entry for each head, one of which must be named using\n+            `RegressionOutput`, or `PredictOutput`. Single-headed models only\n+            need to specify one entry in this dictionary. Multi-headed models\n+            should specify one entry for each head, one of which must be named\n+            using\n             `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`\n             If no entry is provided, a default `PredictOutput` mapping to\n             `predictions` will be created.\n@@ -363,8 +367,8 @@ def model_to_estimator_v2(\n         )\n     except ImportError:\n         raise NotImplementedError(\n-            \"tf.keras.estimator.model_to_estimator function not available in your \"\n-            \"installation.\"\n+            \"tf.keras.estimator.model_to_estimator function not available in \"\n+            \"your installation.\"\n         )\n     _model_to_estimator_usage_gauge.get_cell(\"v2\").set(True)\n     return (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6fafb567af4e4d9f42974d0b6c55b18bc03e17eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 86 | Files Changed: 7 | Hunks: 37 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 193 | Churn Cumulative: 6541 | Contributors (this commit): 9 | Commits (past 90d): 27 | Contributors (cumulative): 47 | DMM Complexity: 0.2\n\nDIFF:\n@@ -60,7 +60,7 @@ class _BaseFeaturesLayer(Layer):\n     ):\n         super().__init__(name=name, trainable=trainable, **kwargs)\n         self._feature_columns = _normalize_feature_columns(feature_columns)\n-        self._state_manager = tf.__internal__.feature_column.StateManager(  # pylint: disable=protected-access\n+        self._state_manager = tf.__internal__.feature_column.StateManager(\n             self, self.trainable\n         )\n         self._partitioner = partitioner\n@@ -182,8 +182,8 @@ def _verify_static_batch_size_equality(tensors, columns):\n                 expected_batch_size = batch_size\n             elif not expected_batch_size.is_compatible_with(batch_size):\n                 raise ValueError(\n-                    \"Batch size (first dimension) of each feature must be same. \"\n-                    \"Batch size of columns ({}, {}): ({}, {})\".format(\n+                    \"Batch size (first dimension) of each feature must be \"\n+                    \"same. Batch size of columns ({}, {}): ({}, {})\".format(\n                         columns[bath_size_column_index].name,\n                         columns[i].name,\n                         expected_batch_size,\n@@ -195,9 +195,9 @@ def _verify_static_batch_size_equality(tensors, columns):\n def _normalize_feature_columns(feature_columns):\n     \"\"\"Normalizes the `feature_columns` input.\n \n-    This method converts the `feature_columns` to list type as best as it can. In\n-    addition, verifies the type and other parts of feature_columns, required by\n-    downstream library.\n+    This method converts the `feature_columns` to list type as best as it can.\n+    In addition, verifies the type and other parts of feature_columns, required\n+    by downstream library.\n \n     Args:\n       feature_columns: The raw feature columns, usually passed by users.\n\n@@ -32,18 +32,19 @@ from keras.saving.saved_model import json_utils\n class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n     \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\n \n-    Generally a single example in training data is described with FeatureColumns.\n-    At the first layer of the model, this column-oriented data should be converted\n-    to a single `Tensor`.\n+    Generally a single example in training data is described with\n+    FeatureColumns.  At the first layer of the model, this column-oriented data\n+    should be converted to a single `Tensor`.\n \n     This layer can be called multiple times with different features.\n \n-    This is the V1 version of this layer that uses variable_scope's or partitioner\n-    to create variables which works well with PartitionedVariables. Variable\n-    scopes are deprecated in V2, so the V2 version uses name_scopes instead. But\n-    currently that lacks support for partitioned variables. Use this if you need\n-    partitioned variables. Use the partitioner argument if you have a Keras model\n-    and uses `tf.compat.v1.keras.estimator.model_to_estimator` for training.\n+    This is the V1 version of this layer that uses variable_scope's or\n+    partitioner to create variables which works well with PartitionedVariables.\n+    Variable scopes are deprecated in V2, so the V2 version uses name_scopes\n+    instead. But currently that lacks support for partitioned variables. Use\n+    this if you need partitioned variables. Use the partitioner argument if you\n+    have a Keras model and uses\n+    `tf.compat.v1.keras.estimator.model_to_estimator` for training.\n \n     Example:\n \n@@ -79,11 +80,11 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n \n         Args:\n           feature_columns: An iterable containing the FeatureColumns to use as\n-            inputs to your model. All items should be instances of classes derived\n-            from `DenseColumn` such as `numeric_column`, `embedding_column`,\n-            `bucketized_column`, `indicator_column`. If you have categorical\n-            features, you can wrap them with an `embedding_column` or\n-            `indicator_column`.\n+            inputs to your model. All items should be instances of classes\n+            derived from `DenseColumn` such as `numeric_column`,\n+            `embedding_column`, `bucketized_column`, `indicator_column`. If you\n+            have categorical features, you can wrap them with an\n+            `embedding_column` or `indicator_column`.\n           trainable:  Boolean, whether the layer's variables will be updated via\n             gradient descent during training.\n           name: Name to give to the DenseFeatures.\n@@ -111,7 +112,8 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n         \"\"\"String stored in metadata field in the SavedModel proto.\n \n         Returns:\n-          A serialized JSON storing information necessary for recreating this layer.\n+          A serialized JSON storing information necessary for recreating this\n+          layer.\n         \"\"\"\n         metadata = json.loads(super()._tracking_metadata)\n         metadata[\"_is_feature_layer\"] = True\n@@ -130,22 +132,24 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n         ...    dimension=8)\n         >>> t2 = tf.feature_column.numeric_column('t2')\n         >>> feature_layer = tf.compat.v1.keras.layers.DenseFeatures([t1, t2])\n-        >>> features = {\"t1\": tf.constant([\"a\", \"b\"]), \"t2\": tf.constant([1, 2])}\n+        >>> features = {\"t1\": tf.constant([\"a\", \"b\"]),\n+        ...             \"t2\": tf.constant([1, 2])}\n         >>> dense_tensor = feature_layer(features, training=True)\n \n         Args:\n           features: A mapping from key to tensors. `FeatureColumn`s look up via\n-            these keys. For example `numeric_column('price')` will look at 'price'\n-            key in this dict. Values can be a `SparseTensor` or a `Tensor` depends\n-            on corresponding `FeatureColumn`.\n+            these keys. For example `numeric_column('price')` will look at\n+            'price' key in this dict. Values can be a `SparseTensor` or a\n+            `Tensor` depends on corresponding `FeatureColumn`.\n           cols_to_output_tensors: If not `None`, this will be filled with a dict\n             mapping feature columns to output tensors created.\n-          training: Python boolean or None, indicating whether to the layer is being\n-            run in training mode. This argument is passed to the call method of any\n-            `FeatureColumn` that takes a `training` argument. For example, if a\n-            `FeatureColumn` performed dropout, the column could expose a `training`\n-            argument to control whether the dropout should be applied. If `None`,\n-            defaults to `tf.keras.backend.learning_phase()`.\n+          training: Python boolean or None, indicating whether to the layer is\n+            being run in training mode. This argument is passed to the call\n+            method of any `FeatureColumn` that takes a `training` argument. For\n+            example, if a `FeatureColumn` performed dropout, the column could\n+            expose a `training` argument to control whether the dropout should\n+            be applied. If `None`, defaults to\n+            `tf.keras.backend.learning_phase()`.\n \n \n         Returns:\n\n@@ -86,8 +86,8 @@ class DenseFeaturesTest(test_combinations.TestCase):\n         # Check that only one variable was created.\n         self.assertEqual(1, len(variables))\n \n-        # Check that invoking dense_features on the same features does not create\n-        # additional variables\n+        # Check that invoking dense_features on the same features does not\n+        # create additional variables\n         _ = dense_features(features)\n         self.assertEqual(1, len(variables))\n         self.assertIs(variables[0], dense_features.variables[0])\n@@ -137,8 +137,8 @@ class DenseFeaturesTest(test_combinations.TestCase):\n         # Check that only one variable was created.\n         self.assertEqual(2, len(variables))\n \n-        # Check that invoking dense_features on the same features does not create\n-        # additional variables\n+        # Check that invoking dense_features on the same features does not\n+        # create additional variables\n         _ = dense_features(features)\n         self.assertEqual(2, len(variables))\n         self.assertIs(variables[0], dense_features.variables[0])\n@@ -894,7 +894,8 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n         expected_lookups = (\n             # example 0, ids [2], embedding = [7, 11]\n             (7.0, 11.0),\n-            # example 1, ids [0, 1], embedding = mean([1, 2] + [3, 5]) = [2, 3.5]\n+            # example 1, ids [0, 1], embedding = mean([1, 2] + [3, 5]) = [2,\n+            # 3.5]\n             (2.0, 3.5),\n             # example 2, ids [], embedding = [0, 0]\n             (0.0, 0.0),\n@@ -928,8 +929,10 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n         if partition_variables:\n             self.assertCountEqual(\n                 (\n-                    \"vars/dense_features/aaa_embedding/embedding_weights/part_0:0\",\n-                    \"vars/dense_features/aaa_embedding/embedding_weights/part_1:0\",\n+                    \"vars/dense_features/aaa_embedding/embedding_weights/\"\n+                    \"part_0:0\",\n+                    \"vars/dense_features/aaa_embedding/embedding_weights/\"\n+                    \"part_1:0\",\n                 ),\n                 tuple([v.name for v in global_vars]),\n             )\n@@ -946,8 +949,10 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n         if partition_variables:\n             self.assertCountEqual(\n                 (\n-                    \"vars/dense_features/aaa_embedding/embedding_weights/part_0:0\",\n-                    \"vars/dense_features/aaa_embedding/embedding_weights/part_1:0\",\n+                    \"vars/dense_features/aaa_embedding/embedding_weights/\"\n+                    \"part_0:0\",\n+                    \"vars/dense_features/aaa_embedding/embedding_weights/\"\n+                    \"part_1:0\",\n                 ),\n                 tuple([v.name for v in trainable_vars]),\n             )\n@@ -1012,7 +1017,8 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n         expected_lookups = (\n             # example 0, ids [2], embedding = [7, 11]\n             (7.0, 11.0),\n-            # example 1, ids [0, 1], embedding = mean([1, 2] + [3, 5]) = [2, 3.5]\n+            # example 1, ids [0, 1], embedding = mean([1, 2] + [3, 5]) = [2,\n+            # 3.5]\n             (2.0, 3.5),\n             # example 2, ids [], embedding = [0, 0]\n             (0.0, 0.0),\n@@ -1328,8 +1334,8 @@ class SequenceFeatureColumnsTest(tf.test.TestCase):\n         input_layer = df.DenseFeatures([embedding_column_a])\n         with self.assertRaisesRegex(\n             ValueError,\n-            r\"In embedding_column: aaa_embedding\\. categorical_column must not be \"\n-            r\"of type SequenceCategoricalColumn\\.\",\n+            r\"In embedding_column: aaa_embedding\\. categorical_column must not \"\n+            r\"be of type SequenceCategoricalColumn\\.\",\n         ):\n             _ = input_layer({\"aaa\": sparse_input})\n \n@@ -1356,8 +1362,8 @@ class SequenceFeatureColumnsTest(tf.test.TestCase):\n         input_layer = df.DenseFeatures([indicator_column_a])\n         with self.assertRaisesRegex(\n             ValueError,\n-            r\"In indicator_column: aaa_indicator\\. categorical_column must not be \"\n-            r\"of type SequenceCategoricalColumn\\.\",\n+            r\"In indicator_column: aaa_indicator\\. categorical_column must not \"\n+            r\"be of type SequenceCategoricalColumn\\.\",\n         ):\n             _ = input_layer({\"aaa\": sparse_input})\n \n\n@@ -30,9 +30,9 @@ from keras.utils import tf_contextlib\n class DenseFeatures(dense_features.DenseFeatures):\n     \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\n \n-    Generally a single example in training data is described with FeatureColumns.\n-    At the first layer of the model, this column oriented data should be converted\n-    to a single `Tensor`.\n+    Generally a single example in training data is described with\n+    FeatureColumns.  At the first layer of the model, this column oriented data\n+    should be converted to a single `Tensor`.\n \n     This layer can be called multiple times with different features.\n \n@@ -45,7 +45,8 @@ class DenseFeatures(dense_features.DenseFeatures):\n     ```python\n     price = tf.feature_column.numeric_column('price')\n     keywords_embedded = tf.feature_column.embedding_column(\n-        tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),\n+        tf.feature_column.categorical_column_with_hash_bucket(\"keywords\",\n+                                                              10000),\n         dimensions=16)\n     columns = [price, keywords_embedded, ...]\n     feature_layer = tf.keras.layers.DenseFeatures(columns)\n@@ -54,7 +55,8 @@ class DenseFeatures(dense_features.DenseFeatures):\n         ..., features=tf.feature_column.make_parse_example_spec(columns))\n     dense_tensor = feature_layer(features)\n     for units in [128, 64, 32]:\n-      dense_tensor = tf.keras.layers.Dense(units, activation='relu')(dense_tensor)\n+      dense_tensor = tf.keras.layers.Dense(units, activation='relu')(\n+        dense_tensor)\n     prediction = tf.keras.layers.Dense(1)(dense_tensor)\n     ```\n     \"\"\"\n@@ -64,11 +66,11 @@ class DenseFeatures(dense_features.DenseFeatures):\n \n         Args:\n           feature_columns: An iterable containing the FeatureColumns to use as\n-            inputs to your model. All items should be instances of classes derived\n-            from `DenseColumn` such as `numeric_column`, `embedding_column`,\n-            `bucketized_column`, `indicator_column`. If you have categorical\n-            features, you can wrap them with an `embedding_column` or\n-            `indicator_column`.\n+            inputs to your model. All items should be instances of classes\n+            derived from `DenseColumn` such as `numeric_column`,\n+            `embedding_column`, `bucketized_column`, `indicator_column`. If you\n+            have categorical features, you can wrap them with an\n+            `embedding_column` or `indicator_column`.\n           trainable:  Boolean, whether the layer's variables will be updated via\n             gradient descent during training.\n           name: Name to give to the DenseFeatures.\n@@ -114,8 +116,8 @@ class _StateManagerImplV2(\n         if name in self._cols_to_vars_map[feature_column]:\n             raise ValueError(\"Variable already exists.\")\n \n-        # We explicitly track these variables since `name` is not guaranteed to be\n-        # unique and disable manual tracking that the add_weight call does.\n+        # We explicitly track these variables since `name` is not guaranteed to\n+        # be unique and disable manual tracking that the add_weight call does.\n         with no_manual_dependency_tracking_scope(self._layer):\n             var = self._layer.add_weight(\n                 name=name,\n@@ -138,9 +140,9 @@ def no_manual_dependency_tracking_scope(obj):\n     \"\"\"A context that disables manual dependency tracking for the given `obj`.\n \n     Sometimes library methods might track objects on their own and we might want\n-    to disable that and do the tracking on our own. One can then use this context\n-    manager to disable the tracking the library method does and do your own\n-    tracking.\n+    to disable that and do the tracking on our own. One can then use this\n+    context manager to disable the tracking the library method does and do your\n+    own tracking.\n \n     For example:\n \n@@ -148,7 +150,8 @@ def no_manual_dependency_tracking_scope(obj):\n       def build():\n         with no_manual_dependency_tracking_scope(self):\n           var = self.add_weight(\"name1\")  # Creates a var and doesn't track it\n-        self._track_trackable(\"name2\", var)  # We track variable with name `name2`\n+        # We track variable with name `name2`\n+        self._track_trackable(\"name2\", var)\n \n     Args:\n       obj: A trackable object.\n\n@@ -82,8 +82,8 @@ class DenseFeaturesTest(test_combinations.TestCase):\n         # Check that only one variable was created.\n         self.assertEqual(1, len(variables))\n \n-        # Check that invoking dense_features on the same features does not create\n-        # additional variables\n+        # Check that invoking dense_features on the same features does not\n+        # create additional variables\n         _ = dense_features(features)\n         self.assertEqual(1, len(variables))\n         self.assertIs(variables[0], dense_features.variables[0])\n\n@@ -83,8 +83,10 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n         \"\"\" \"Constructs a SequenceFeatures layer.\n \n         Args:\n-          feature_columns: An iterable of dense sequence columns. Valid columns are\n-            - `embedding_column` that wraps a `sequence_categorical_column_with_*`\n+          feature_columns: An iterable of dense sequence columns. Valid columns\n+            are\n+            - `embedding_column` that wraps a\n+              `sequence_categorical_column_with_*`\n             - `sequence_numeric_column`.\n           trainable: Boolean, whether the layer's variables will be updated via\n             gradient descent during training.\n@@ -115,22 +117,23 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n \n         Args:\n           features: A dict mapping keys to tensors.\n-          training: Python boolean or None, indicating whether to the layer is being\n-            run in training mode. This argument is passed to the call method of any\n-            `FeatureColumn` that takes a `training` argument. For example, if a\n-            `FeatureColumn` performed dropout, the column could expose a `training`\n-            argument to control whether the dropout should be applied. If `None`,\n-            defaults to `tf.keras.backend.learning_phase()`.\n+          training: Python boolean or None, indicating whether to the layer is\n+            being run in training mode. This argument is passed to the call\n+            method of any `FeatureColumn` that takes a `training` argument. For\n+            example, if a `FeatureColumn` performed dropout, the column could\n+            expose a `training` argument to control whether the dropout should\n+            be applied. If `None`, defaults to\n+            `tf.keras.backend.learning_phase()`.\n \n \n         Returns:\n           An `(input_layer, sequence_length)` tuple where:\n           - input_layer: A float `Tensor` of shape `[batch_size, T, D]`.\n-              `T` is the maximum sequence length for this batch, which could differ\n-              from batch to batch. `D` is the sum of `num_elements` for all\n-              `feature_columns`.\n-          - sequence_length: An int `Tensor` of shape `[batch_size]`. The sequence\n-              length for each example.\n+              `T` is the maximum sequence length for this batch, which could\n+              differ from batch to batch. `D` is the sum of `num_elements` for\n+              all `feature_columns`.\n+          - sequence_length: An int `Tensor` of shape `[batch_size]`. The\n+            sequence length for each example.\n \n         Raises:\n           ValueError: If features are not a dictionary.\n@@ -172,7 +175,7 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n                 sequence_lengths.append(sequence_length)\n \n         # Check and process sequence lengths.\n-        kfc._verify_static_batch_size_equality(  # pylint: disable=protected-access\n+        kfc._verify_static_batch_size_equality(\n             sequence_lengths, self._feature_columns\n         )\n         sequence_length = _assert_all_equal_and_return(sequence_lengths)\n\n@@ -209,8 +209,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n         sequence_input_layer = ksfc.SequenceFeatures([embedding_column_a])\n         with self.assertRaisesRegex(\n             ValueError,\n-            r\"In embedding_column: aaa_embedding\\. categorical_column must be of \"\n-            r\"type SequenceCategoricalColumn to use SequenceFeatures\\.\",\n+            r\"In embedding_column: aaa_embedding\\. categorical_column must be \"\n+            r\"of type SequenceCategoricalColumn to use SequenceFeatures\\.\",\n         ):\n             _, _ = sequence_input_layer({\"aaa\": sparse_input})\n \n@@ -303,7 +303,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n                 )\n \n     def test_shared_embedding_column_with_non_sequence_categorical(self):\n-        \"\"\"Tests that error is raised for non-sequence shared embedding column.\"\"\"\n+        \"\"\"Tests that error is raised for non-sequence shared embedding\n+        column.\"\"\"\n         with tf.Graph().as_default():\n             vocabulary_size = 3\n             sparse_input_a = tf.compat.v1.SparseTensorValue(\n@@ -342,7 +343,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n                 ValueError,\n                 r\"In embedding_column: aaa_shared_embedding\\. \"\n                 r\"categorical_column must \"\n-                r\"be of type SequenceCategoricalColumn to use SequenceFeatures\\.\",\n+                r\"be of type SequenceCategoricalColumn to use \"\n+                r\"SequenceFeatures\\.\",\n             ):\n                 _, _ = sequence_input_layer(\n                     {\"aaa\": sparse_input_a, \"bbb\": sparse_input_b}\n@@ -476,8 +478,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n         sequence_input_layer = ksfc.SequenceFeatures([indicator_column_a])\n         with self.assertRaisesRegex(\n             ValueError,\n-            r\"In indicator_column: aaa_indicator\\. categorical_column must be of \"\n-            r\"type SequenceCategoricalColumn to use SequenceFeatures\\.\",\n+            r\"In indicator_column: aaa_indicator\\. categorical_column must be \"\n+            r\"of type SequenceCategoricalColumn to use SequenceFeatures\\.\",\n         ):\n             _, _ = sequence_input_layer({\"aaa\": sparse_input})\n \n@@ -570,7 +572,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n                 \"dense_shape\": (2, 8),\n             },\n             \"expected_input_layer\": [\n-                # The output of numeric_column._get_dense_tensor should be flattened.\n+                # The output of numeric_column._get_dense_tensor should be\n+                # flattened.\n                 [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0]],\n                 [[10.0, 11.0, 12.0, 13.0], [0.0, 0.0, 0.0, 0.0]],\n             ],\n@@ -612,7 +615,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n                 \"dense_shape\": (2, 2, 4),\n             },\n             \"expected_input_layer\": [\n-                # The output of numeric_column._get_dense_tensor should be flattened.\n+                # The output of numeric_column._get_dense_tensor should be\n+                # flattened.\n                 [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0]],\n                 [[10.0, 11.0, 12.0, 13.0], [0.0, 0.0, 0.0, 0.0]],\n             ],\n@@ -670,7 +674,8 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n         {\n             \"testcase_name\": \"2D\",\n             \"sparse_input_args\": {\n-                # example 0, values [[[0., 1.],  [2., 3.]], [[4., 5.],  [6., 7.]]]\n+                # example 0, values [[[0., 1.],  [2., 3.]], [[4., 5.],  [6.,\n+                # 7.]]]\n                 # example 1, [[[10., 11.],  [12., 13.]]]\n                 \"indices\": (\n                     (0, 0),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
