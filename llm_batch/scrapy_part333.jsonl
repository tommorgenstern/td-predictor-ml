{"custom_id": "scrapy#b0f5503cb8d0590aab4d9c91b0ee660d98d1e4a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 774 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class Command(BaseRunSpiderCommand):\n         parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n         parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=\"maximum depth for parsing requests [default: %default]\")\n+                            help=f\"maximum depth for parsing requests [default: {self.max_level}]\")\n         parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56c9098d6af2a57cc11927959d47253999cddd46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 776 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class Command(BaseRunSpiderCommand):\n         parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n         parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=f\"maximum depth for parsing requests [default: {self.max_level}]\")\n+                            help=\"maximum depth for parsing requests [default: 1]\")\n         parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#915c288205e2b9a0bdbbe18cc67cd23ba5bb4de3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 778 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class Command(BaseRunSpiderCommand):\n         parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n         parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=\"maximum depth for parsing requests [default: 1]\")\n+                            help=\"maximum depth for parsing requests [default: %(default)s]\")\n         parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7de9ed5bd42052f6491c939513adde6f6243ce2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 506 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -15,10 +15,12 @@ from scrapy.http import Request\n from scrapy.pipelines.files import FileException, FilesPipeline\n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n+from scrapy.utils.decorators import deprecated\n from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n \n \n+@deprecated()\n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f659bd63c04656f1c0601d6338f9050d5049fe8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 515 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,25 +5,28 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n+import warnings\n from contextlib import suppress\n from io import BytesIO\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import DropItem, NotConfigured\n+from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.pipelines.files import FileException, FilesPipeline\n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n-from scrapy.utils.decorators import deprecated\n from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n \n \n-@deprecated()\n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n \n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\"The NoimagesDrop class is deprecated\", category=ScrapyDeprecationWarning, stacklevel=2)\n+        super().__init__(*args, **kwargs)\n+\n \n class ImageException(FileException):\n     \"\"\"General image error exception\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc16af35af8b20e04b42ca84431820b2b092f379", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 1037 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,14 +4,16 @@ import random\n from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import skipIf\n+from warnings import catch_warnings\n \n import attr\n from itemadapter import ItemAdapter\n from twisted.trial import unittest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n-from scrapy.pipelines.images import ImagesPipeline\n+from scrapy.pipelines.images import ImagesPipeline, NoimagesDrop\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n@@ -413,6 +415,22 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n                              expected_value)\n \n \n+class NoimagesDropTestCase(unittest.TestCase):\n+\n+    def test_deprecation_warning(self):\n+        arg = str()\n+        with catch_warnings(record=True) as warnings:\n+            NoimagesDrop(arg)\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+        with catch_warnings(record=True) as warnings:\n+            class SubclassedNoimagesDrop(NoimagesDrop):\n+                pass\n+            SubclassedNoimagesDrop(arg)\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n+\n def _create_image(format, *a, **kw):\n     buf = io.BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2afcbfe2bf090827540d072866bef0d1ab3a3e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 37 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy import signals\n from scrapy.mail import MailSender\n from scrapy.exceptions import NotConfigured\n \n+\n class StatsMailer:\n \n     def __init__(self, stats, recipients, mail):\n\n@@ -8,6 +8,7 @@ class AsyncioReactorSpider1(scrapy.Spider):\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n     }\n \n+\n class AsyncioReactorSpider2(scrapy.Spider):\n     name = 'asyncio_reactor2'\n     custom_settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83c1939281197242511931c6e9f356f2498eb623", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1121 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,6 +146,7 @@ class Command(BaseRunSpiderCommand):\n \n         def _start_requests(spider):\n             yield self.prepare_request(spider, Request(url), opts)\n+        if self.spidercls:\n             self.spidercls.start_requests = _start_requests\n \n     def start_parsing(self, url, opts):\n\n@@ -1,6 +1,7 @@\n import os\n import argparse\n from os.path import join, abspath, isfile, exists\n+\n from twisted.internet import defer\n from scrapy.commands import parse\n from scrapy.settings import Settings\n@@ -222,6 +223,10 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n \n+        status, out, stderr = yield self.execute([self.url('/invalid_url')])\n+        self.assertEqual(status, 0)\n+        self.assertIn(\"\"\"\"\"\", _textmode(stderr))\n+\n     @defer.inlineCallbacks\n     def test_output_flag(self):\n         \"\"\"Checks if a file was created successfully having\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2006060688976ca469d794cdd3b753a00bfb83c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 847 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,7 +103,9 @@ class Downloader:\n     def _get_slot(self, request, spider):\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n-            conc = self.per_slot_settings.get(key, {}).get('concurrency', self.ip_concurrency if self.ip_concurrency else self.domain_concurrency)\n+            conc = self.per_slot_settings.get(key, {}).get(\n+                'concurrency', self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            )\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n             delay = self.per_slot_settings.get(key, {}).get('delay', delay)\n             randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ffc52a491e6e6c46196b3aa92767856f64a8ebc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 75 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,75 @@\n+import time\n+\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.http import Request\n+\n+from tests.mockserver import MockServer\n+from tests.spiders import MetaSpider\n+\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n+\n+class DownloaderSlotsSettingsTestSpider(MetaSpider):\n+\n+    name = 'downloader_slots'\n+\n+    custom_settings = {\n+        \"DOWNLOAD_DELAY\": 1,\n+        \"RANDOMIZE_DOWNLOAD_DELAY\": False,\n+        \"DOWNLOAD_SLOTS\": {\n+            'quotes.toscrape.com': {\n+                'concurrency': 1,\n+                'delay': 1.5,\n+                'randomize_delay': False\n+            },\n+            'books.toscrape.com': {\n+                'delay': 2,\n+                'randomize_delay': False\n+            }\n+        }\n+    }\n+\n+    def start_requests(self):\n+        self.times = {None: []}\n+\n+        slots = list(self.custom_settings.get('DOWNLOAD_SLOTS', {}).keys()) + [None]\n+\n+        for slot in slots:\n+            url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n+            self.times[slot] = []\n+            yield Request(url, callback=self.parse, meta={'download_slot': slot})\n+\n+    def parse(self, response):\n+        slot = response.meta.get('download_slot', None)\n+        self.times[slot].append(time.time())\n+        url = self.mockserver.url(f\"/?downloader_slot={slot}&req=2\")\n+        yield Request(url, callback=self.not_parse, meta={'download_slot': slot})\n+\n+    def not_parse(self, response):\n+        slot = response.meta.get('download_slot', None)\n+        self.times[slot].append(time.time())\n+\n+\n+class CrawlTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self.runner = CrawlerRunner()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_delay(self):\n+        crawler = CrawlerRunner().create_crawler(DownloaderSlotsSettingsTestSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        slots = crawler.engine.downloader.slots\n+        times = crawler.spider.times\n+        tolerance = 0.3\n+\n+        delays_real = {k: v[1] - v[0] for k, v in times.items()}\n+        error_delta = {k: 1 - delays_real[k] / v.delay for k, v in slots.items()}\n+\n+        self.assertTrue(max(list(error_delta.values())) < tolerance)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c031b8a8dd719e6011ee29889bc8181cdbc9a9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 29 | Churn Cumulative: 1050 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,9 +80,8 @@ class CaselessDict(dict):\n \n \n class CaseInsensitiveDict(collections.UserDict):\n-    \"\"\"A dict-like structure that accepts strings or bytes as keys and allows case-insensitive lookups.\n-\n-    It also allows overriding key and value normalization by defining custom `normkey` and `normvalue` methods.\n+    \"\"\"A dict-like structure that accepts strings or bytes\n+    as keys and allows case-insensitive lookups.\n     \"\"\"\n \n     def __init__(self, *args, **kwargs) -> None:\n@@ -90,32 +89,32 @@ class CaseInsensitiveDict(collections.UserDict):\n         super().__init__(*args, **kwargs)\n \n     def __getitem__(self, key: AnyStr) -> Any:\n-        normalized_key = self.normkey(key)\n+        normalized_key = self._normkey(key)\n         return super().__getitem__(self._keys[normalized_key.lower()])\n \n     def __setitem__(self, key: AnyStr, value: Any) -> None:\n-        normalized_key = self.normkey(key)\n+        normalized_key = self._normkey(key)\n         if normalized_key.lower() in self._keys:\n             del self[self._keys[normalized_key.lower()]]\n-        super().__setitem__(normalized_key, self.normvalue(value))\n+        super().__setitem__(normalized_key, self._normvalue(value))\n         self._keys[normalized_key.lower()] = normalized_key\n \n     def __delitem__(self, key: AnyStr) -> None:\n-        normalized_key = self.normkey(key)\n+        normalized_key = self._normkey(key)\n         stored_key = self._keys.pop(normalized_key.lower())\n         super().__delitem__(stored_key)\n \n     def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n-        normalized_key = self.normkey(key)\n+        normalized_key = self._normkey(key)\n         return normalized_key.lower() in self._keys\n \n     def __repr__(self) -> str:\n         return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n \n-    def normkey(self, key: AnyStr) -> AnyStr:\n+    def _normkey(self, key: AnyStr) -> AnyStr:\n         return key\n \n-    def normvalue(self, value: Any) -> Any:\n+    def _normvalue(self, value: Any) -> Any:\n         return value\n \n \n\n@@ -1,8 +1,8 @@\n import copy\n-from typing import Iterator\n import unittest\n import warnings\n from collections.abc import Mapping, MutableMapping\n+from typing import Iterator\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n@@ -144,19 +144,23 @@ class CaseInsensitiveDictMixin:\n \n     def test_normkey(self):\n         class MyDict(self.dict_class):\n-            def normkey(self, key):\n+            def _normkey(self, key):\n                 return key.title()\n \n+            normkey = _normkey  # deprecated CaselessDict class\n+\n         d = MyDict()\n         d['key-one'] = 2\n         self.assertEqual(list(d.keys()), ['Key-One'])\n \n     def test_normvalue(self):\n         class MyDict(self.dict_class):\n-            def normvalue(self, value):\n+            def _normvalue(self, value):\n                 if value is not None:\n                     return value + 1\n \n+            normvalue = _normvalue  # deprecated CaselessDict class\n+\n         d = MyDict({'key': 1})\n         self.assertEqual(d['key'], 2)\n         self.assertEqual(d.get('key'), 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#078622cfb0ee364acba5d91a20244f9c1ee87d30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 33 | Churn Cumulative: 1505 | Contributors (this commit): 29 | Commits (past 90d): 2 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -222,8 +222,8 @@ class GCSFilesStore:\n                 return {'checksum': checksum, 'last_modified': last_modified}\n             else:\n                 return {}\n-\n-        return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)\n+        blob_path = self._get_blob_path(path)\n+        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess)\n \n     def _get_content_type(self, headers):\n         if headers and 'Content-Type' in headers:\n@@ -231,8 +231,12 @@ class GCSFilesStore:\n         else:\n             return 'application/octet-stream'\n \n+    def _get_blob_path(self, path):\n+        return self.prefix + path\n+\n     def persist_file(self, path, buf, info, meta=None, headers=None):\n-        blob = self.bucket.blob(self.prefix + path)\n+        blob_path = self._get_blob_path(path)\n+        blob = self.bucket.blob(blob_path)\n         blob.cache_control = self.CACHE_CONTROL\n         blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n         return threads.deferToThread(\n\n@@ -525,6 +525,29 @@ class TestGCSFilesStore(unittest.TestCase):\n         self.assertEqual(blob.content_type, 'application/octet-stream')\n         self.assertIn(expected_policy, acl)\n \n+    @defer.inlineCallbacks\n+    def test_blob_path_consistency(self):\n+        \"\"\"Test to make sure that paths used to store files is the same as the one used to get\n+        already uploaded files.\n+        \"\"\"\n+        assert_gcs_environ()\n+        try:\n+            import google.cloud.storage # noqa\n+        except ModuleNotFoundError:\n+            raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n+        else:\n+            with mock.patch('google.cloud.storage') as _:\n+                with mock.patch('scrapy.pipelines.files.time') as _:\n+                    uri = 'gs://my_bucket/my_prefix/'\n+                    store = GCSFilesStore(uri)\n+                    store.bucket = mock.Mock()\n+                    path = 'full/my_data.txt'\n+                    yield store.persist_file(path, mock.Mock(), info=None, meta=None, headers=None)\n+                    yield store.stat_file(path, info=None)\n+                    expected_blob_path = store.prefix + path\n+                    store.bucket.blob.assert_called_with(expected_blob_path)\n+                    store.bucket.get_blob.assert_called_with(expected_blob_path)\n+\n \n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5c15d87ff5770220bca31792c89e58804b923bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 345 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -223,9 +223,10 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_not_exists_with_not_matched_url(self):\n         status, out, stderr = yield self.execute([self.url('/invalid_url')])\n         self.assertEqual(status, 0)\n-        self.assertIn(\"\"\"\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_output_flag(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86331900125dc311223cbd1ebb0e10d09e7c592d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 519 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -141,7 +141,7 @@ class ImagesPipeline(FilesPipeline):\n         yield path, image, buf\n \n         for thumb_id, size in self.thumbs.items():\n-            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n+            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info, item=item)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n \n@@ -179,6 +179,6 @@ class ImagesPipeline(FilesPipeline):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'full/{image_guid}.jpg'\n \n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n+    def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c586d78f0e1c5b66358ed644bb6e528ad4b062b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 44 | Churn Cumulative: 1574 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,6 +93,22 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n+    def test_thumbnail_name_from_item(self):\n+        \"\"\"\n+        Custom thumbnail name based on item data, overriding default implementation\n+        \"\"\"\n+\n+        class CustomImagesPipeline(ImagesPipeline):\n+            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+                return f\"thumb/{thumb_id}/{item.get('path')}\"\n+\n+        thumb_path = CustomImagesPipeline.from_settings(Settings(\n+            {'IMAGES_STORE': self.tempdir}\n+        )).thumb_path\n+        item = dict(path='path-to-store-file')\n+        request = Request(\"http://example.com\")\n+        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n+\n     def test_convert_image(self):\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\n\n@@ -1,4 +1,5 @@\n from typing import Optional\n+import io\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n@@ -355,9 +356,12 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n     def get_media_requests(self, item, info):\n         item_url = item['image_urls'][0]\n+        output_img = io.BytesIO()\n+        img = Image.new('RGB', (60, 30), color='red')\n+        img.save(output_img, format='JPEG')\n         return Request(\n             item_url,\n-            meta={'response': Response(item_url, status=200, body=b'data')}\n+            meta={'response': Response(item_url, status=200, body=output_img.getvalue())}\n         )\n \n     def inc_stats(self, *args, **kwargs):\n@@ -379,9 +383,13 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n         self._mockcalled.append('file_path')\n         return super(MockedMediaPipelineDeprecatedMethods, self).file_path(request, response, info)\n \n+    def thumb_path(self, request, thumb_id, response=None, info=None):\n+        self._mockcalled.append('thumb_path')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).thumb_path(request, thumb_id, response, info)\n+\n     def get_images(self, response, request, info):\n         self._mockcalled.append('get_images')\n-        return []\n+        return super(MockedMediaPipelineDeprecatedMethods, self).get_images(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n         self._mockcalled.append('image_downloaded')\n@@ -392,7 +400,11 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n     def setUp(self):\n-        self.pipe = MockedMediaPipelineDeprecatedMethods(store_uri='store-uri', download_func=_mocked_download_func)\n+        self.pipe = MockedMediaPipelineDeprecatedMethods(\n+            store_uri='store-uri',\n+            download_func=_mocked_download_func,\n+            settings=Settings({\"IMAGES_THUMBS\": {'small': (50, 50)}})\n+        )\n         self.pipe.open_spider(None)\n         self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n \n@@ -444,6 +456,16 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n         )\n         self._assert_method_called_with_warnings('file_path', message, warnings)\n \n+    @inlineCallbacks\n+    def test_thumb_path_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, '\n+            'please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('thumb_path', message, warnings)\n+\n     @inlineCallbacks\n     def test_get_images_called(self):\n         yield self.pipe.process_item(self.item, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#896f16f2def7c276350421352dddf6e7b0145519", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 842 | Contributors (this commit): 26 | Commits (past 90d): 5 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -179,6 +179,6 @@ class ImagesPipeline(FilesPipeline):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'full/{image_guid}.jpg'\n \n-    def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+    def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\n\n@@ -121,7 +121,7 @@ class MediaPipeline:\n     def _make_compatible(self):\n         \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n         methods = [\n-            \"file_path\", \"media_to_download\", \"media_downloaded\",\n+            \"file_path\", \"thumb_path\", \"media_to_download\", \"media_downloaded\",\n             \"file_downloaded\", \"image_downloaded\", \"get_images\"\n         ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c65066ad9e293630da2c594af06ad483abe800d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1065 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -94,8 +94,11 @@ class CaseInsensitiveDict(collections.UserDict):\n \n     def __setitem__(self, key: AnyStr, value: Any) -> None:\n         normalized_key = self._normkey(key)\n-        if normalized_key.lower() in self._keys:\n-            del self[self._keys[normalized_key.lower()]]\n+        try:\n+            lower_key = self._keys[normalized_key.lower()]\n+            del self[lower_key]\n+        except KeyError:\n+            pass\n         super().__setitem__(normalized_key, self._normvalue(value))\n         self._keys[normalized_key.lower()] = normalized_key\n \n\n@@ -187,9 +187,15 @@ class CaseInsensitiveDictMixin:\n     def test_copy(self):\n         h1 = self.dict_class({'header1': 'value'})\n         h2 = copy.copy(h1)\n+        assert isinstance(h2, self.dict_class)\n         self.assertEqual(h1, h2)\n         self.assertEqual(h1.get('header1'), h2.get('header1'))\n-        assert isinstance(h2, self.dict_class)\n+        self.assertEqual(h1.get('header1'), h2.get('HEADER1'))\n+        h3 = h1.copy()\n+        assert isinstance(h3, self.dict_class)\n+        self.assertEqual(h1, h3)\n+        self.assertEqual(h1.get('header1'), h3.get('header1'))\n+        self.assertEqual(h1.get('header1'), h3.get('HEADER1'))\n \n \n class CaseInsensitiveDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5627af15bcf413c04539aeb47dd07cf8b3e4092", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1021 | Lines Deleted: 142 | Files Changed: 13 | Hunks: 80 | Methods Changed: 90 | Complexity Δ (Sum/Max): 97/70 | Churn Δ: 1163 | Churn Cumulative: 7754 | Contributors (this commit): 93 | Commits (past 90d): 16 | Contributors (cumulative): 231 | DMM Complexity: 0.7550274223034735\n\nDIFF:\n@@ -294,7 +294,9 @@ intersphinx_mapping = {\n     'tox': ('https://tox.readthedocs.io/en/latest', None),\n     'twisted': ('https://twistedmatrix.com/documents/current', None),\n     'twistedapi': ('https://twistedmatrix.com/documents/current/api', None),\n+    'w3lib': ('https://w3lib.readthedocs.io/en/latest', None),\n }\n+intersphinx_disabled_reftypes = []\n \n \n # Options for sphinx-hoverxref options\n\n@@ -51,6 +51,7 @@ class Crawler:\n         self.spidercls.update_settings(self.settings)\n \n         self.signals = SignalManager(self)\n+\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n \n         handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n@@ -71,6 +72,12 @@ class Crawler:\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n \n+        self.request_fingerprinter = create_instance(\n+            load_object(self.settings['REQUEST_FINGERPRINTER_CLASS']),\n+            settings=self.settings,\n+            crawler=self,\n+        )\n+\n         reactor_class = self.settings.get(\"TWISTED_REACTOR\")\n         if init_reactor:\n             # this needs to be done after the spider settings are merged,\n\n@@ -1,14 +1,16 @@\n import logging\n import os\n from typing import Optional, Set, Type, TypeVar\n+from warnings import warn\n \n from twisted.internet.defer import Deferred\n \n from scrapy.http.request import Request\n from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n-from scrapy.utils.request import referer_str, request_fingerprint\n+from scrapy.utils.request import referer_str, RequestFingerprinter\n \n \n BaseDupeFilterTV = TypeVar(\"BaseDupeFilterTV\", bound=\"BaseDupeFilter\")\n@@ -39,8 +41,15 @@ RFPDupeFilterTV = TypeVar(\"RFPDupeFilterTV\", bound=\"RFPDupeFilter\")\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n-    def __init__(self, path: Optional[str] = None, debug: bool = False) -> None:\n+    def __init__(\n+        self,\n+        path: Optional[str] = None,\n+        debug: bool = False,\n+        *,\n+        fingerprinter=None,\n+    ) -> None:\n         self.file = None\n+        self.fingerprinter = fingerprinter or RequestFingerprinter()\n         self.fingerprints: Set[str] = set()\n         self.logdupes = True\n         self.debug = debug\n@@ -51,9 +60,39 @@ class RFPDupeFilter(BaseDupeFilter):\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n     @classmethod\n-    def from_settings(cls: Type[RFPDupeFilterTV], settings: BaseSettings) -> RFPDupeFilterTV:\n+    def from_settings(cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None) -> RFPDupeFilterTV:\n         debug = settings.getbool('DUPEFILTER_DEBUG')\n-        return cls(job_dir(settings), debug)\n+        try:\n+            return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        except TypeError:\n+            warn(\n+                \"RFPDupeFilter subclasses must either modify their '__init__' \"\n+                \"method to support a 'fingerprinter' parameter or reimplement \"\n+                \"the 'from_settings' class method.\",\n+                ScrapyDeprecationWarning,\n+            )\n+            result = cls(job_dir(settings), debug)\n+            result.fingerprinter = fingerprinter\n+            return result\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        try:\n+            return cls.from_settings(\n+                crawler.settings,\n+                fingerprinter=crawler.request_fingerprinter,\n+            )\n+        except TypeError:\n+            warn(\n+                \"RFPDupeFilter subclasses must either modify their overridden \"\n+                \"'__init__' method and 'from_settings' class method to \"\n+                \"support a 'fingerprinter' parameter, or reimplement the \"\n+                \"'from_crawler' class method.\",\n+                ScrapyDeprecationWarning,\n+            )\n+            result = cls.from_settings(crawler.settings)\n+            result.fingerprinter = crawler.request_fingerprinter\n+            return result\n \n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n@@ -65,7 +104,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         return False\n \n     def request_fingerprint(self, request: Request) -> str:\n-        return request_fingerprint(request)\n+        return self.fingerprinter.fingerprint(request).hex()\n \n     def close(self, reason: str) -> None:\n         if self.file:\n\n@@ -14,7 +14,6 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.utils.request import request_fingerprint\n \n \n logger = logging.getLogger(__name__)\n@@ -228,6 +227,8 @@ class DbmCacheStorage:\n \n         logger.debug(\"Using DBM cache storage in %(cachepath)s\", {'cachepath': dbpath}, extra={'spider': spider})\n \n+        self._fingerprinter = spider.crawler.request_fingerprinter\n+\n     def close_spider(self, spider):\n         self.db.close()\n \n@@ -244,7 +245,7 @@ class DbmCacheStorage:\n         return response\n \n     def store_response(self, spider, request, response):\n-        key = self._request_key(request)\n+        key = self._fingerprinter.fingerprint(request).hex()\n         data = {\n             'status': response.status,\n             'url': response.url,\n@@ -255,7 +256,7 @@ class DbmCacheStorage:\n         self.db[f'{key}_time'] = str(time())\n \n     def _read_data(self, spider, request):\n-        key = self._request_key(request)\n+        key = self._fingerprinter.fingerprint(request).hex()\n         db = self.db\n         tkey = f'{key}_time'\n         if tkey not in db:\n@@ -267,9 +268,6 @@ class DbmCacheStorage:\n \n         return pickle.loads(db[f'{key}_data'])\n \n-    def _request_key(self, request):\n-        return request_fingerprint(request)\n-\n \n class FilesystemCacheStorage:\n \n@@ -283,6 +281,8 @@ class FilesystemCacheStorage:\n         logger.debug(\"Using filesystem cache storage in %(cachedir)s\", {'cachedir': self.cachedir},\n                      extra={'spider': spider})\n \n+        self._fingerprinter = spider.crawler.request_fingerprinter\n+\n     def close_spider(self, spider):\n         pass\n \n@@ -329,7 +329,7 @@ class FilesystemCacheStorage:\n             f.write(request.body)\n \n     def _get_request_path(self, spider, request):\n-        key = request_fingerprint(request)\n+        key = self._fingerprinter.fingerprint(request).hex()\n         return os.path.join(self.cachedir, spider.name, key[0:2], key)\n \n     def _read_meta(self, spider, request):\n\n@@ -11,7 +11,6 @@ from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.log import failure_to_exc_info\n \n@@ -77,6 +76,7 @@ class MediaPipeline:\n         except AttributeError:\n             pipe = cls()\n         pipe.crawler = crawler\n+        pipe._fingerprinter = crawler.request_fingerprinter\n         return pipe\n \n     def open_spider(self, spider):\n@@ -90,7 +90,7 @@ class MediaPipeline:\n         return dfd.addCallback(self.item_completed, item, info)\n \n     def _process_request(self, request, info, item):\n-        fp = request_fingerprint(request)\n+        fp = self._fingerprinter.fingerprint(request)\n         cb = request.callback or (lambda _: _)\n         eb = request.errback\n         request.callback = None\n\n@@ -246,6 +246,9 @@ REDIRECT_PRIORITY_ADJUST = +2\n REFERER_ENABLED = True\n REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'\n \n+REQUEST_FINGERPRINTER_CLASS = 'scrapy.utils.request.RequestFingerprinter'\n+REQUEST_FINGERPRINTER_IMPLEMENTATION = 'PREVIOUS_VERSION'\n+\n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n\n@@ -4,7 +4,9 @@ scrapy.http.Request objects\n \"\"\"\n \n import hashlib\n-from typing import Dict, Iterable, Optional, Tuple, Union\n+import json\n+import warnings\n+from typing import Dict, Iterable, List, Optional, Tuple, Union\n from urllib.parse import urlunparse\n from weakref import WeakKeyDictionary\n \n@@ -12,13 +14,22 @@ from w3lib.http import basic_auth_header\n from w3lib.url import canonicalize_url\n \n from scrapy import Request, Spider\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n-_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n-_fingerprint_cache = WeakKeyDictionary()\n+_deprecated_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n+_deprecated_fingerprint_cache = WeakKeyDictionary()\n+\n+\n+def _serialize_headers(headers, request):\n+    for header in headers:\n+        if header in request.headers:\n+            yield header\n+            for value in request.headers.getlist(header):\n+                yield value\n \n \n def request_fingerprint(\n@@ -26,6 +37,123 @@ def request_fingerprint(\n     include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n     keep_fragments: bool = False,\n ) -> str:\n+    \"\"\"\n+    Return the request fingerprint as an hexadecimal string.\n+\n+    The request fingerprint is a hash that uniquely identifies the resource the\n+    request points to. For example, take the following two urls:\n+\n+    http://www.example.com/query?id=111&cat=222\n+    http://www.example.com/query?cat=222&id=111\n+\n+    Even though those are two different URLs both point to the same resource\n+    and are equivalent (i.e. they should return the same response).\n+\n+    Another example are cookies used to store session ids. Suppose the\n+    following page is only accessible to authenticated users:\n+\n+    http://www.example.com/members/offers.html\n+\n+    Lots of sites use a cookie to store the session id, which adds a random\n+    component to the HTTP Request and thus should be ignored when calculating\n+    the fingerprint.\n+\n+    For this reason, request headers are ignored by default when calculating\n+    the fingerprint. If you want to include specific headers use the\n+    include_headers argument, which is a list of Request headers to include.\n+\n+    Also, servers usually ignore fragments in urls when handling requests,\n+    so they are also ignored by default when calculating the fingerprint.\n+    If you want to include them, set the keep_fragments argument to True\n+    (for instance when handling requests with a headless browser).\n+    \"\"\"\n+    if include_headers or keep_fragments:\n+        message = (\n+            'Call to deprecated function '\n+            'scrapy.utils.request.request_fingerprint().\\n'\n+            '\\n'\n+            'If you are using this function in a Scrapy component because you '\n+            'need a non-default fingerprinting algorithm, and you are OK '\n+            'with that non-default fingerprinting algorithm being used by '\n+            'all Scrapy components and not just the one calling this '\n+            'function, use crawler.request_fingerprinter.fingerprint() '\n+            'instead in your Scrapy component (you can get the crawler '\n+            'object from the \\'from_crawler\\' class method), and use the '\n+            '\\'REQUEST_FINGERPRINTER_CLASS\\' setting to configure your '\n+            'non-default fingerprinting algorithm.\\n'\n+            '\\n'\n+            'Otherwise, consider using the '\n+            'scrapy.utils.request.fingerprint() function instead.\\n'\n+            '\\n'\n+            'If you switch to \\'fingerprint()\\', or assign the '\n+            '\\'REQUEST_FINGERPRINTER_CLASS\\' setting a class that uses '\n+            '\\'fingerprint()\\', the generated fingerprints will not only be '\n+            'bytes instead of a string, but they will also be different from '\n+            'those generated by \\'request_fingerprint()\\'. Before you switch, '\n+            'make sure that you understand the consequences of this (e.g. '\n+            'cache invalidation) and are OK with them; otherwise, consider '\n+            'implementing your own function which returns the same '\n+            'fingerprints as the deprecated \\'request_fingerprint()\\' function.'\n+        )\n+    else:\n+        message = (\n+            'Call to deprecated function '\n+            'scrapy.utils.request.request_fingerprint().\\n'\n+            '\\n'\n+            'If you are using this function in a Scrapy component, and you '\n+            'are OK with users of your component changing the fingerprinting '\n+            'algorithm through settings, use '\n+            'crawler.request_fingerprinter.fingerprint() instead in your '\n+            'Scrapy component (you can get the crawler object from the '\n+            '\\'from_crawler\\' class method).\\n'\n+            '\\n'\n+            'Otherwise, consider using the '\n+            'scrapy.utils.request.fingerprint() function instead.\\n'\n+            '\\n'\n+            'Either way, the resulting fingerprints will be returned as '\n+            'bytes, not as a string, and they will also be different from '\n+            'those generated by \\'request_fingerprint()\\'. Before you switch, '\n+            'make sure that you understand the consequences of this (e.g. '\n+            'cache invalidation) and are OK with them; otherwise, consider '\n+            'implementing your own function which returns the same '\n+            'fingerprints as the deprecated \\'request_fingerprint()\\' function.'\n+        )\n+    warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n+    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n+    if include_headers:\n+        processed_include_headers = tuple(\n+            to_bytes(h.lower()) for h in sorted(include_headers)\n+        )\n+    cache = _deprecated_fingerprint_cache.setdefault(request, {})\n+    cache_key = (processed_include_headers, keep_fragments)\n+    if cache_key not in cache:\n+        fp = hashlib.sha1()\n+        fp.update(to_bytes(request.method))\n+        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n+        fp.update(request.body or b'')\n+        if processed_include_headers:\n+            for part in _serialize_headers(processed_include_headers, request):\n+                fp.update(part)\n+        cache[cache_key] = fp.hexdigest()\n+    return cache[cache_key]\n+\n+\n+def _request_fingerprint_as_bytes(*args, **kwargs):\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\")\n+        return bytes.fromhex(request_fingerprint(*args, **kwargs))\n+\n+\n+_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\"\n+_fingerprint_cache = WeakKeyDictionary()\n+\n+\n+def fingerprint(\n+    request: Request,\n+    *,\n+    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n+    keep_fragments: bool = False,\n+) -> bytes:\n     \"\"\"\n     Return the request fingerprint.\n \n@@ -43,7 +171,7 @@ def request_fingerprint(\n \n     http://www.example.com/members/offers.html\n \n-    Lot of sites use a cookie to store the session id, which adds a random\n+    Lots of sites use a cookie to store the session id, which adds a random\n     component to the HTTP Request and thus should be ignored when calculating\n     the fingerprint.\n \n@@ -55,29 +183,96 @@ def request_fingerprint(\n     so they are also ignored by default when calculating the fingerprint.\n     If you want to include them, set the keep_fragments argument to True\n     (for instance when handling requests with a headless browser).\n-\n     \"\"\"\n-    headers: Optional[Tuple[bytes, ...]] = None\n+    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n     if include_headers:\n-        headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n+        processed_include_headers = tuple(\n+            to_bytes(h.lower()) for h in sorted(include_headers)\n+        )\n     cache = _fingerprint_cache.setdefault(request, {})\n-    cache_key = (headers, keep_fragments)\n+    cache_key = (processed_include_headers, keep_fragments)\n     if cache_key not in cache:\n-        fp = hashlib.sha1()\n-        fp.update(to_bytes(request.method))\n-        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n-        fp.update(request.body or b'')\n-        if headers:\n-            for hdr in headers:\n-                if hdr in request.headers:\n-                    fp.update(hdr)\n-                    for v in request.headers.getlist(hdr):\n-                        fp.update(v)\n-        cache[cache_key] = fp.hexdigest()\n+        # To decode bytes reliably (JSON does not support bytes), regardless of\n+        # character encoding, we use bytes.hex()\n+        headers: Dict[str, List[str]] = {}\n+        if processed_include_headers:\n+            for header in processed_include_headers:\n+                if header in request.headers:\n+                    headers[header.hex()] = [\n+                        header_value.hex()\n+                        for header_value in request.headers.getlist(header)\n+                    ]\n+        fingerprint_data = {\n+            'method': to_unicode(request.method),\n+            'url': canonicalize_url(request.url, keep_fragments=keep_fragments),\n+            'body': (request.body or b'').hex(),\n+            'headers': headers,\n+        }\n+        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n+        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()\n     return cache[cache_key]\n \n \n-def request_authenticate(request: Request, username: str, password: str) -> None:\n+class RequestFingerprinter:\n+    \"\"\"Default fingerprinter.\n+\n+    It takes into account a canonical version\n+    (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url\n+    <scrapy.http.Request.url>` and the values of :attr:`request.method\n+    <scrapy.http.Request.method>` and :attr:`request.body\n+    <scrapy.http.Request.body>`. It then generates an `SHA1\n+    <https://en.wikipedia.org/wiki/SHA-1>`_ hash.\n+\n+    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.\n+    \"\"\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n+    def __init__(self, crawler=None):\n+        if crawler:\n+            implementation = crawler.settings.get(\n+                'REQUEST_FINGERPRINTER_IMPLEMENTATION'\n+            )\n+        else:\n+            implementation = 'PREVIOUS_VERSION'\n+        if implementation == 'PREVIOUS_VERSION':\n+            message = (\n+                '\\'PREVIOUS_VERSION\\' is a deprecated value for the '\n+                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting.\\n'\n+                '\\n'\n+                'It is also the default value. In other words, it is normal '\n+                'to get this warning if you have not defined a value for the '\n+                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting. This is so '\n+                'for backward compatibility reasons, but it will change in a '\n+                'future version of Scrapy.\\n'\n+                '\\n'\n+                'See the documentation of the '\n+                '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting for '\n+                'information on how to handle this deprecation.'\n+            )\n+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n+            self._fingerprint = _request_fingerprint_as_bytes\n+        elif implementation == 'VERSION':\n+            self._fingerprint = fingerprint\n+        else:\n+            raise ValueError(\n+                f'Got an invalid value on setting '\n+                f'\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\': '\n+                f'{implementation!r}. Valid values are \\'PREVIOUS_VERSION\\' (deprecated) '\n+                f'and \\'VERSION\\'.'\n+            )\n+\n+    def fingerprint(self, request):\n+        return self._fingerprint(request)\n+\n+\n+def request_authenticate(\n+    request: Request,\n+    username: str,\n+    password: str,\n+) -> None:\n     \"\"\"Authenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n\n@@ -54,7 +54,7 @@ def get_ftp_content_and_delete(\n     return \"\".join(ftp_data)\n \n \n-def get_crawler(spidercls=None, settings_dict=None):\n+def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n     priority.\n@@ -62,7 +62,12 @@ def get_crawler(spidercls=None, settings_dict=None):\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.spiders import Spider\n \n-    runner = CrawlerRunner(settings_dict)\n+    # Set by default settings that prevent deprecation warnings.\n+    settings = {}\n+    if prevent_warnings:\n+        settings['REQUEST_FINGERPRINTER_IMPLEMENTATION'] = 'VERSION'\n+    settings.update(settings_dict or {})\n+    runner = CrawlerRunner(settings)\n     return runner.create_crawler(spidercls or Spider)\n \n \n\n@@ -104,7 +104,8 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             custom_settings = {\n                 'LOG_LEVEL': 'INFO',\n                 'LOG_FILE': log_file,\n-                # disable telnet if not available to avoid an extra warning\n+                # settings to avoid extra warnings\n+                'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n                 'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n\n@@ -15,6 +15,16 @@ from scrapy.utils.test import get_crawler\n from tests.spiders import SimpleSpider\n \n \n+def _get_dupefilter(*, crawler=None, settings=None, open=True):\n+    if crawler is None:\n+        crawler = get_crawler(settings_dict=settings)\n+    scheduler = Scheduler.from_crawler(crawler)\n+    dupefilter = scheduler.df\n+    if open:\n+        dupefilter.open()\n+    return dupefilter\n+\n+\n class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n \n     @classmethod\n@@ -64,9 +74,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         self.assertEqual(scheduler.df.method, 'n/a')\n \n     def test_filter(self):\n-        dupefilter = RFPDupeFilter()\n-        dupefilter.open()\n-\n+        dupefilter = _get_dupefilter()\n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n@@ -85,7 +93,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n         path = tempfile.mkdtemp()\n         try:\n-            df = RFPDupeFilter(path)\n+            df = _get_dupefilter(settings={'JOBDIR': path}, open=False)\n             try:\n                 df.open()\n                 assert not df.request_seen(r1)\n@@ -93,7 +101,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n             finally:\n                 df.close('finished')\n \n-            df2 = RFPDupeFilter(path)\n+            df2 = _get_dupefilter(settings={'JOBDIR': path}, open=False)\n+            assert df != df2\n             try:\n                 df2.open()\n                 assert df2.request_seen(r1)\n@@ -109,26 +118,24 @@ class RFPDupeFilterTest(unittest.TestCase):\n         output of request_seen.\n \n         \"\"\"\n+        dupefilter = _get_dupefilter()\n         r1 = Request('http://scrapytest.org/index.html')\n         r2 = Request('http://scrapytest.org/INDEX.html')\n \n-        dupefilter = RFPDupeFilter()\n-        dupefilter.open()\n-\n         assert not dupefilter.request_seen(r1)\n         assert not dupefilter.request_seen(r2)\n \n         dupefilter.close('finished')\n \n-        class CaseInsensitiveRFPDupeFilter(RFPDupeFilter):\n+        class RequestFingerprinter:\n \n-            def request_fingerprint(self, request):\n+            def fingerprint(self, request):\n                 fp = hashlib.sha1()\n                 fp.update(to_bytes(request.url.lower()))\n-                return fp.hexdigest()\n+                return fp.digest()\n \n-        case_insensitive_dupefilter = CaseInsensitiveRFPDupeFilter()\n-        case_insensitive_dupefilter.open()\n+        settings = {'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter}\n+        case_insensitive_dupefilter = _get_dupefilter(settings=settings)\n \n         assert not case_insensitive_dupefilter.request_seen(r1)\n         assert case_insensitive_dupefilter.request_seen(r2)\n@@ -142,8 +149,10 @@ class RFPDupeFilterTest(unittest.TestCase):\n         r1 = Request('http://scrapytest.org/1')\n \n         path = tempfile.mkdtemp()\n+        crawler = get_crawler(settings_dict={'JOBDIR': path})\n         try:\n-            df = RFPDupeFilter(path)\n+            scheduler = Scheduler.from_crawler(crawler)\n+            df = scheduler.df\n             df.open()\n             df.request_seen(r1)\n             df.close('finished')\n@@ -164,11 +173,8 @@ class RFPDupeFilterTest(unittest.TestCase):\n             settings = {'DUPEFILTER_DEBUG': False,\n                         'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n-            scheduler = Scheduler.from_crawler(crawler)\n             spider = SimpleSpider.from_crawler(crawler)\n-\n-            dupefilter = scheduler.df\n-            dupefilter.open()\n+            dupefilter = _get_dupefilter(crawler=crawler)\n \n             r1 = Request('http://scrapytest.org/index.html')\n             r2 = Request('http://scrapytest.org/index.html')\n@@ -193,11 +199,41 @@ class RFPDupeFilterTest(unittest.TestCase):\n             settings = {'DUPEFILTER_DEBUG': True,\n                         'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n-            scheduler = Scheduler.from_crawler(crawler)\n             spider = SimpleSpider.from_crawler(crawler)\n-\n-            dupefilter = scheduler.df\n-            dupefilter.open()\n+            dupefilter = _get_dupefilter(crawler=crawler)\n+\n+            r1 = Request('http://scrapytest.org/index.html')\n+            r2 = Request('http://scrapytest.org/index.html',\n+                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})\n+\n+            dupefilter.log(r1, spider)\n+            dupefilter.log(r2, spider)\n+\n+            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            log.check_present(\n+                (\n+                    'scrapy.dupefilters',\n+                    'DEBUG',\n+                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'\n+                )\n+            )\n+            log.check_present(\n+                (\n+                    'scrapy.dupefilters',\n+                    'DEBUG',\n+                    'Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n+                    ' (referer: http://scrapytest.org/INDEX.html)'\n+                )\n+            )\n+\n+            dupefilter.close('finished')\n+\n+    def test_log_debug_default_dupefilter(self):\n+        with LogCapture() as log:\n+            settings = {'DUPEFILTER_DEBUG': True}\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            spider = SimpleSpider.from_crawler(crawler)\n+            dupefilter = _get_dupefilter(crawler=crawler)\n \n             r1 = Request('http://scrapytest.org/index.html')\n             r2 = Request('http://scrapytest.org/index.html',\n\n@@ -25,6 +25,7 @@ from scrapy.pipelines.files import (\n from scrapy.settings import Settings\n from scrapy.utils.test import (\n     assert_gcs_environ,\n+    get_crawler,\n     get_ftp_content_and_delete,\n     get_gcs_content_and_delete,\n     skip_if_no_boto,\n@@ -47,7 +48,9 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir}))\n+        settings_dict = {'FILES_STORE': self.tempdir}\n+        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n+        self.pipeline = FilesPipeline.from_crawler(crawler)\n         self.pipeline.download_func = _mocked_download_func\n         self.pipeline.open_spider(None)\n \n\n@@ -7,17 +7,17 @@ from twisted.python.failure import Failure\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n+from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.request import request_fingerprint\n+from scrapy.pipelines.files import FileException\n from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n-from scrapy.pipelines.files import FileException\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n-from scrapy import signals\n+from scrapy.utils.test import get_crawler\n \n \n try:\n@@ -39,11 +39,14 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n     settings = None\n \n     def setUp(self):\n-        self.spider = Spider('media.com')\n-        self.pipe = self.pipeline_class(download_func=_mocked_download_func,\n-                                        settings=Settings(self.settings))\n+        spider_cls = Spider\n+        self.spider = spider_cls('media.com')\n+        crawler = get_crawler(spider_cls, self.settings)\n+        self.pipe = self.pipeline_class.from_crawler(crawler)\n+        self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(self.spider)\n         self.info = self.pipe.spiderinfo\n+        self.fingerprint = crawler.request_fingerprinter.fingerprint\n \n     def tearDown(self):\n         for name, signal in vars(signals).items():\n@@ -156,7 +159,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         self.assertEqual(failure.value.__context__, def_gen_return_exc)\n \n         # Let's calculate the request fingerprint and fake some runtime data...\n-        fp = request_fingerprint(request)\n+        fp = self.fingerprint(request)\n         info = self.pipe.spiderinfo\n         info.downloading.add(fp)\n         info.waiting[fp] = []\n@@ -273,7 +276,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=req)  # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n-        assert request_fingerprint(req) in self.info.downloaded\n+        self.assertIn(self.fingerprint(req), self.info.downloaded)\n \n         # returns iterable of Requests\n         req1 = Request('http://url1')\n@@ -281,8 +284,8 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=iter([req1, req2]))\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n-        assert request_fingerprint(req1) in self.info.downloaded\n-        assert request_fingerprint(req2) in self.info.downloaded\n+        assert self.fingerprint(req1) in self.info.downloaded\n+        assert self.fingerprint(req2) in self.info.downloaded\n \n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n@@ -298,7 +301,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=req2)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(request_fingerprint(req1), request_fingerprint(req2))\n+        self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n         self.assertEqual(new_item['results'], [(True, rsp1)])\n \n     @inlineCallbacks\n@@ -314,7 +317,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_wait_if_request_is_downloading(self):\n         def _check_downloading(response):\n-            fp = request_fingerprint(req1)\n+            fp = self.fingerprint(req1)\n             self.assertTrue(fp in self.info.downloading)\n             self.assertTrue(fp in self.info.waiting)\n             self.assertTrue(fp not in self.info.downloaded)\n@@ -351,7 +354,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n     def __init__(self, *args, **kwargs):\n-        super(MockedMediaPipelineDeprecatedMethods, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self._mockcalled = []\n \n     def get_media_requests(self, item, info):\n@@ -369,19 +372,19 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n     def media_to_download(self, request, info):\n         self._mockcalled.append('media_to_download')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).media_to_download(request, info)\n+        return super().media_to_download(request, info)\n \n     def media_downloaded(self, response, request, info):\n         self._mockcalled.append('media_downloaded')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).media_downloaded(response, request, info)\n+        return super().media_downloaded(response, request, info)\n \n     def file_downloaded(self, response, request, info):\n         self._mockcalled.append('file_downloaded')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).file_downloaded(response, request, info)\n+        return super().file_downloaded(response, request, info)\n \n     def file_path(self, request, response=None, info=None):\n         self._mockcalled.append('file_path')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).file_path(request, response, info)\n+        return super().file_path(request, response, info)\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n         self._mockcalled.append('thumb_path')\n@@ -393,18 +396,20 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n     def image_downloaded(self, response, request, info):\n         self._mockcalled.append('image_downloaded')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).image_downloaded(response, request, info)\n+        return super().image_downloaded(response, request, info)\n \n \n class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n     def setUp(self):\n-        self.pipe = MockedMediaPipelineDeprecatedMethods(\n-            store_uri='store-uri',\n-            download_func=_mocked_download_func,\n-            settings=Settings({\"IMAGES_THUMBS\": {'small': (50, 50)}})\n-        )\n+        settings_dict = {\n+            'IMAGES_STORE': 'store-uri',\n+            'IMAGES_THUMBS': {'small': (50, 50)},\n+        }\n+        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n+        self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)\n+        self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(None)\n         self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n \n\n@@ -1,73 +1,29 @@\n import unittest\n+import warnings\n+from hashlib import sha1\n+from typing import Dict, Mapping, Optional, Tuple, Union\n+from weakref import WeakKeyDictionary\n+\n+import pytest\n+from w3lib.url import canonicalize_url\n+\n from scrapy.http import Request\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n+from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n+    _deprecated_fingerprint_cache,\n     _fingerprint_cache,\n+    _request_fingerprint_as_bytes,\n+    fingerprint,\n     request_authenticate,\n     request_fingerprint,\n     request_httprepr,\n )\n+from scrapy.utils.test import get_crawler\n \n \n class UtilsRequestTest(unittest.TestCase):\n \n-    def test_request_fingerprint(self):\n-        r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n-        r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r1))\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n-\n-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78132,199')\n-        r2 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n-        self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2))\n-\n-        # make sure caching is working\n-        self.assertEqual(request_fingerprint(r1), _fingerprint_cache[r1][(None, False)])\n-\n-        r1 = Request(\"http://www.example.com/members/offers.html\")\n-        r2 = Request(\"http://www.example.com/members/offers.html\")\n-        r2.headers['SESSIONID'] = b\"somehash\"\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n-\n-        r1 = Request(\"http://www.example.com/\")\n-        r2 = Request(\"http://www.example.com/\")\n-        r2.headers['Accept-Language'] = b'en'\n-        r3 = Request(\"http://www.example.com/\")\n-        r3.headers['Accept-Language'] = b'en'\n-        r3.headers['SESSIONID'] = b\"somehash\"\n-\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2), request_fingerprint(r3))\n-\n-        self.assertEqual(request_fingerprint(r1),\n-                         request_fingerprint(r1, include_headers=['Accept-Language']))\n-\n-        self.assertNotEqual(\n-            request_fingerprint(r1),\n-            request_fingerprint(r2, include_headers=['Accept-Language']))\n-\n-        self.assertEqual(request_fingerprint(r3, include_headers=['accept-language', 'sessionid']),\n-                         request_fingerprint(r3, include_headers=['SESSIONID', 'Accept-Language']))\n-\n-        r1 = Request(\"http://www.example.com/test.html\")\n-        r2 = Request(\"http://www.example.com/test.html#fragment\")\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n-        self.assertEqual(request_fingerprint(r1), request_fingerprint(r1, keep_fragments=True))\n-        self.assertNotEqual(request_fingerprint(r2), request_fingerprint(r2, keep_fragments=True))\n-        self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2, keep_fragments=True))\n-\n-        r1 = Request(\"http://www.example.com\")\n-        r2 = Request(\"http://www.example.com\", method='POST')\n-        r3 = Request(\"http://www.example.com\", method='POST', body=b'request body')\n-\n-        self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2))\n-        self.assertNotEqual(request_fingerprint(r2), request_fingerprint(r3))\n-\n-        # cached fingerprint must be cleared on request copy\n-        r1 = Request(\"http://www.example.com\")\n-        fp1 = request_fingerprint(r1)\n-        r2 = r1.replace(url=\"http://www.example.com/other\")\n-        fp2 = request_fingerprint(r2)\n-        self.assertNotEqual(fp1, fp2)\n-\n     def test_request_authenticate(self):\n         r = Request(\"http://www.example.com\")\n         request_authenticate(r, 'someuser', 'somepass')\n@@ -93,5 +49,632 @@ class UtilsRequestTest(unittest.TestCase):\n         request_httprepr(Request(\"ftp://localhost/tmp/foo.txt\"))\n \n \n+class FingerprintTest(unittest.TestCase):\n+    maxDiff = None\n+\n+    function = staticmethod(fingerprint)\n+    cache: Union[\n+        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\",\n+        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\",\n+    ] = _fingerprint_cache\n+    default_cache_key = (None, False)\n+    known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n+        (\n+            Request(\"http://example.org\"),\n+            b'xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\"),\n+            b'\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a\"),\n+            b'G\\xad\\xb8Ck\\x19\\x1c\\xed\\x838,\\x01\\xc4\\xde;\\xee\\xa5\\x94a\\x0c',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b\"),\n+            b'\\x024MYb\\x8a\\xc2\\x1e\\xbc>\\xd6\\xac*\\xda\\x9cF\\xc1r\\x7f\\x17',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b&a\"),\n+            b't+\\xe8*\\xfb\\x84\\xe3v\\x1a}\\x88p\\xc0\\xccB\\xd7\\x9d\\xfez\\x96',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b&a=c\"),\n+            b'\\xda\\x1ec\\xd0\\x9c\\x08s`\\xb4\\x9b\\xe2\\xb6R\\xf8k\\xef\\xeaQG\\xef',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", method='POST'),\n+            b'\\x9d\\xcdA\\x0fT\\x02:\\xca\\xa0}\\x90\\xda\\x05B\\xded\\x8aN7\\x1d',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", body=b'a'),\n+            b'\\xc34z>\\xd8\\x99\\x8b\\xda7\\x05r\\x99I\\xa8\\xa0x;\\xa41_',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", method='POST', body=b'a'),\n+            b'5`\\xe2y4\\xd0\\x9d\\xee\\xe0\\xbatw\\x87Q\\xe8O\\xd78\\xfc\\xe7',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            b'\\xc04\\x85P,\\xaa\\x91\\x06\\xf8t\\xb4\\xbd*\\xd9\\xe9\\x8a:m\\xc3l',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            b']\\xc7\\x1f\\xf2\\xafG2\\xbc\\xa4\\xfa\\x99\\n33\\xda\\x18\\x94\\x81U.',\n+            {'include_headers': ['A']},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            b'<\\x1a\\xeb\\x85y\\xdeW\\xfb\\xdcq\\x88\\xee\\xaf\\x17\\xdd\\x0c\\xbfH\\x18\\x1f',\n+            {'keep_fragments': True},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            b'\\xc1\\xef~\\x94\\x9bS\\xc1\\x83\\t\\xdcz8\\x9f\\xdc{\\x11\\x16I.\\x11',\n+            {'include_headers': ['A'], 'keep_fragments': True},\n+        ),\n+        (\n+            Request(\"https://example.org/ab\"),\n+            b'N\\xe5l\\xb8\\x12@iw\\xe2\\xf3\\x1bp\\xea\\xffp!u\\xe2\\x8a\\xc6',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org/a\", body=b'b'),\n+            b'_NOv\\xbco$6\\xfcW\\x9f\\xb24g\\x9f\\xbb\\xdd\\xa82\\xc5',\n+            {},\n+        ),\n+    )\n+\n+    def test_query_string_key_order(self):\n+        r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n+        r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n+        self.assertEqual(self.function(r1), self.function(r1))\n+        self.assertEqual(self.function(r1), self.function(r2))\n+\n+    def test_query_string_key_without_value(self):\n+        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78132,199')\n+        r2 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n+        self.assertNotEqual(self.function(r1), self.function(r2))\n+\n+    def test_caching(self):\n+        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n+        self.assertEqual(\n+            self.function(r1),\n+            self.cache[r1][self.default_cache_key]\n+        )\n+\n+    def test_header(self):\n+        r1 = Request(\"http://www.example.com/members/offers.html\")\n+        r2 = Request(\"http://www.example.com/members/offers.html\")\n+        r2.headers['SESSIONID'] = b\"somehash\"\n+        self.assertEqual(self.function(r1), self.function(r2))\n+\n+    def test_headers(self):\n+        r1 = Request(\"http://www.example.com/\")\n+        r2 = Request(\"http://www.example.com/\")\n+        r2.headers['Accept-Language'] = b'en'\n+        r3 = Request(\"http://www.example.com/\")\n+        r3.headers['Accept-Language'] = b'en'\n+        r3.headers['SESSIONID'] = b\"somehash\"\n+\n+        self.assertEqual(self.function(r1), self.function(r2), self.function(r3))\n+\n+        self.assertEqual(self.function(r1),\n+                         self.function(r1, include_headers=['Accept-Language']))\n+\n+        self.assertNotEqual(\n+            self.function(r1),\n+            self.function(r2, include_headers=['Accept-Language']))\n+\n+        self.assertEqual(self.function(r3, include_headers=['accept-language', 'sessionid']),\n+                         self.function(r3, include_headers=['SESSIONID', 'Accept-Language']))\n+\n+    def test_fragment(self):\n+        r1 = Request(\"http://www.example.com/test.html\")\n+        r2 = Request(\"http://www.example.com/test.html#fragment\")\n+        self.assertEqual(self.function(r1), self.function(r2))\n+        self.assertEqual(self.function(r1), self.function(r1, keep_fragments=True))\n+        self.assertNotEqual(self.function(r2), self.function(r2, keep_fragments=True))\n+        self.assertNotEqual(self.function(r1), self.function(r2, keep_fragments=True))\n+\n+    def test_method_and_body(self):\n+        r1 = Request(\"http://www.example.com\")\n+        r2 = Request(\"http://www.example.com\", method='POST')\n+        r3 = Request(\"http://www.example.com\", method='POST', body=b'request body')\n+\n+        self.assertNotEqual(self.function(r1), self.function(r2))\n+        self.assertNotEqual(self.function(r2), self.function(r3))\n+\n+    def test_request_replace(self):\n+        # cached fingerprint must be cleared on request copy\n+        r1 = Request(\"http://www.example.com\")\n+        fp1 = self.function(r1)\n+        r2 = r1.replace(url=\"http://www.example.com/other\")\n+        fp2 = self.function(r2)\n+        self.assertNotEqual(fp1, fp2)\n+\n+    def test_part_separation(self):\n+        # An old implementation used to serialize request data in a way that\n+        # would put the body right after the URL.\n+        r1 = Request(\"http://www.example.com/foo\")\n+        fp1 = self.function(r1)\n+        r2 = Request(\"http://www.example.com/f\", body=b'oo')\n+        fp2 = self.function(r2)\n+        self.assertNotEqual(fp1, fp2)\n+\n+    def test_hashes(self):\n+        \"\"\"Test hardcoded hashes, to make sure future changes to not introduce\n+        backward incompatibilities.\"\"\"\n+        actual = [\n+            self.function(request, **kwargs)\n+            for request, _, kwargs in self.known_hashes\n+        ]\n+        expected = [\n+            _fingerprint\n+            for _, _fingerprint, _ in self.known_hashes\n+        ]\n+        self.assertEqual(actual, expected)\n+\n+\n+class RequestFingerprintTest(FingerprintTest):\n+    function = staticmethod(request_fingerprint)\n+    cache = _deprecated_fingerprint_cache\n+    known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n+        (\n+            Request(\"http://example.org\"),\n+            'b2e5245ef826fd9576c93bd6e392fce3133fab62',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\"),\n+            'bd10a0a89ea32cdee77917320f1309b0da87e892',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a\"),\n+            '2fb7d48ae02f04b749f40caa969c0bc3c43204ce',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b\"),\n+            '42e5fe149b147476e3f67ad0670c57b4cc57856a',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b&a\"),\n+            'd23a9787cb56c6375c2cae4453c5a8c634526942',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org?a=b&a=c\"),\n+            '9a18a7a8552a9182b7f1e05d33876409e421e5c5',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", method='POST'),\n+            'ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", body=b'a'),\n+            '4bb136e54e715a4ea7a9dd1101831765d33f2d60',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org\", method='POST', body=b'a'),\n+            '6c6595374a304b293be762f7b7be3f54e9947c65',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            'bd10a0a89ea32cdee77917320f1309b0da87e892',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            '515b633cb3ca502a33a9d8c890e889ec1e425e65',\n+            {'include_headers': ['A']},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            '505c96e7da675920dfef58725e8c957dfdb38f47',\n+            {'keep_fragments': True},\n+        ),\n+        (\n+            Request(\"https://example.org#a\", headers={'A': b'B'}),\n+            'd6f673cdcb661b7970c2b9a00ee63e87d1e2e5da',\n+            {'include_headers': ['A'], 'keep_fragments': True},\n+        ),\n+        (\n+            Request(\"https://example.org/ab\"),\n+            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',\n+            {},\n+        ),\n+        (\n+            Request(\"https://example.org/a\", body=b'b'),\n+            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',\n+            {},\n+        ),\n+    )\n+\n+    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)\n+    def test_part_separation(self):\n+        super().test_part_separation()\n+\n+    def test_deprecation_default_parameters(self):\n+        with pytest.warns(ScrapyDeprecationWarning) as warnings:\n+            self.function(Request(\"http://www.example.com\"))\n+        messages = [str(warning.message) for warning in warnings]\n+        self.assertTrue(\n+            any(\n+                'Call to deprecated function' in message\n+                for message in messages\n+            )\n+        )\n+        self.assertFalse(any('non-default' in message for message in messages))\n+\n+    def test_deprecation_non_default_parameters(self):\n+        with pytest.warns(ScrapyDeprecationWarning) as warnings:\n+            self.function(Request(\"http://www.example.com\"), keep_fragments=True)\n+        messages = [str(warning.message) for warning in warnings]\n+        self.assertTrue(\n+            any(\n+                'Call to deprecated function' in message\n+                for message in messages\n+            )\n+        )\n+        self.assertTrue(any('non-default' in message for message in messages))\n+\n+\n+class RequestFingerprintAsBytesTest(FingerprintTest):\n+    function = staticmethod(_request_fingerprint_as_bytes)\n+    cache = _deprecated_fingerprint_cache\n+    known_hashes = RequestFingerprintTest.known_hashes\n+\n+    def test_caching(self):\n+        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')\n+        self.assertEqual(\n+            self.function(r1),\n+            bytes.fromhex(self.cache[r1][self.default_cache_key])\n+        )\n+\n+    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)\n+    def test_part_separation(self):\n+        super().test_part_separation()\n+\n+    def test_hashes(self):\n+        actual = [\n+            self.function(request, **kwargs)\n+            for request, _, kwargs in self.known_hashes\n+        ]\n+        expected = [\n+            bytes.fromhex(_fingerprint)\n+            for _, _fingerprint, _ in self.known_hashes\n+        ]\n+        self.assertEqual(actual, expected)\n+\n+\n+_fingerprint_cache_2_6: Mapping[Request, Tuple[None, bool]] = WeakKeyDictionary()\n+\n+\n+def request_fingerprint_2_6(request, include_headers=None, keep_fragments=False):\n+    if include_headers:\n+        include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n+    cache = _fingerprint_cache_2_6.setdefault(request, {})\n+    cache_key = (include_headers, keep_fragments)\n+    if cache_key not in cache:\n+        fp = sha1()\n+        fp.update(to_bytes(request.method))\n+        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n+        fp.update(request.body or b'')\n+        if include_headers:\n+            for hdr in include_headers:\n+                if hdr in request.headers:\n+                    fp.update(hdr)\n+                    for v in request.headers.getlist(hdr):\n+                        fp.update(v)\n+        cache[cache_key] = fp.hexdigest()\n+    return cache[cache_key]\n+\n+\n+REQUEST_OBJECTS_TO_TEST = (\n+    Request(\"http://www.example.com/\"),\n+    Request(\"http://www.example.com/query?id=111&cat=222\"),\n+    Request(\"http://www.example.com/query?cat=222&id=111\"),\n+    Request('http://www.example.com/hnnoticiaj1.aspx?78132,199'),\n+    Request('http://www.example.com/hnnoticiaj1.aspx?78160,199'),\n+    Request(\"http://www.example.com/members/offers.html\"),\n+    Request(\n+        \"http://www.example.com/members/offers.html\",\n+        headers={'SESSIONID': b\"somehash\"},\n+    ),\n+    Request(\n+        \"http://www.example.com/\",\n+        headers={'Accept-Language': b\"en\"},\n+    ),\n+    Request(\n+        \"http://www.example.com/\",\n+        headers={\n+            'Accept-Language': b\"en\",\n+            'SESSIONID': b\"somehash\",\n+        },\n+    ),\n+    Request(\"http://www.example.com/test.html\"),\n+    Request(\"http://www.example.com/test.html#fragment\"),\n+    Request(\"http://www.example.com\", method='POST'),\n+    Request(\"http://www.example.com\", method='POST', body=b'request body'),\n+)\n+\n+\n+class BackwardCompatibilityTestCase(unittest.TestCase):\n+\n+    def test_function_backward_compatibility(self):\n+        include_headers_to_test = (\n+            None,\n+            ['Accept-Language'],\n+            ['accept-language', 'sessionid'],\n+            ['SESSIONID', 'Accept-Language'],\n+        )\n+        for request_object in REQUEST_OBJECTS_TO_TEST:\n+            for include_headers in include_headers_to_test:\n+                for keep_fragments in (False, True):\n+                    with warnings.catch_warnings():\n+                        warnings.simplefilter(\"ignore\")\n+                        fp = request_fingerprint(\n+                            request_object,\n+                            include_headers=include_headers,\n+                            keep_fragments=keep_fragments,\n+                        )\n+                    old_fp = request_fingerprint_2_6(\n+                        request_object,\n+                        include_headers=include_headers,\n+                        keep_fragments=keep_fragments,\n+                    )\n+                    self.assertEqual(fp, old_fp)\n+\n+    def test_component_backward_compatibility(self):\n+        for request_object in REQUEST_OBJECTS_TO_TEST:\n+            with warnings.catch_warnings():\n+                warnings.simplefilter(\"ignore\")\n+                crawler = get_crawler(prevent_warnings=False)\n+                fp = crawler.request_fingerprinter.fingerprint(request_object)\n+            old_fp = request_fingerprint_2_6(request_object)\n+            self.assertEqual(fp.hex(), old_fp)\n+\n+    def test_custom_component_backward_compatibility(self):\n+        \"\"\"Tests that the backward-compatible request fingerprinting class featured\n+        in the documentation is indeed backward compatible and does not cause a\n+        warning to be logged.\"\"\"\n+\n+        class RequestFingerprinter:\n+\n+            cache = WeakKeyDictionary()\n+\n+            def fingerprint(self, request):\n+                if request not in self.cache:\n+                    fp = sha1()\n+                    fp.update(to_bytes(request.method))\n+                    fp.update(to_bytes(canonicalize_url(request.url)))\n+                    fp.update(request.body or b'')\n+                    self.cache[request] = fp.digest()\n+                return self.cache[request]\n+\n+        for request_object in REQUEST_OBJECTS_TO_TEST:\n+            with warnings.catch_warnings() as logged_warnings:\n+                settings = {\n+                    'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+                }\n+                crawler = get_crawler(settings_dict=settings)\n+                fp = crawler.request_fingerprinter.fingerprint(request_object)\n+            old_fp = request_fingerprint_2_6(request_object)\n+            self.assertEqual(fp.hex(), old_fp)\n+            self.assertFalse(logged_warnings)\n+\n+\n+class RequestFingerprinterTestCase(unittest.TestCase):\n+\n+    def test_default_implementation(self):\n+        with warnings.catch_warnings(record=True) as logged_warnings:\n+            crawler = get_crawler(prevent_warnings=False)\n+        request = Request('https://example.com')\n+        self.assertEqual(\n+            crawler.request_fingerprinter.fingerprint(request),\n+            _request_fingerprint_as_bytes(request),\n+        )\n+        self.assertTrue(logged_warnings)\n+\n+    def test_deprecated_implementation(self):\n+        settings = {\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'PREVIOUS_VERSION',\n+        }\n+        with warnings.catch_warnings(record=True) as logged_warnings:\n+            crawler = get_crawler(settings_dict=settings)\n+        request = Request('https://example.com')\n+        self.assertEqual(\n+            crawler.request_fingerprinter.fingerprint(request),\n+            _request_fingerprint_as_bytes(request),\n+        )\n+        self.assertTrue(logged_warnings)\n+\n+    def test_recommended_implementation(self):\n+        settings = {\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+        }\n+        with warnings.catch_warnings(record=True) as logged_warnings:\n+            crawler = get_crawler(settings_dict=settings)\n+        request = Request('https://example.com')\n+        self.assertEqual(\n+            crawler.request_fingerprinter.fingerprint(request),\n+            fingerprint(request),\n+        )\n+        self.assertFalse(logged_warnings)\n+\n+    def test_unknown_implementation(self):\n+        settings = {\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.5',\n+        }\n+        with self.assertRaises(ValueError):\n+            get_crawler(settings_dict=settings)\n+\n+\n+class CustomRequestFingerprinterTestCase(unittest.TestCase):\n+\n+    def test_include_headers(self):\n+\n+        class RequestFingerprinter:\n+\n+            def fingerprint(self, request):\n+                return fingerprint(request, include_headers=['X-ID'])\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        r1 = Request(\"http://www.example.com\", headers={'X-ID': '1'})\n+        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n+        r2 = Request(\"http://www.example.com\", headers={'X-ID': '2'})\n+        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n+        self.assertNotEqual(fp1, fp2)\n+\n+    def test_dont_canonicalize(self):\n+\n+        class RequestFingerprinter:\n+            cache = WeakKeyDictionary()\n+\n+            def fingerprint(self, request):\n+                if request not in self.cache:\n+                    fp = sha1()\n+                    fp.update(to_bytes(request.url))\n+                    self.cache[request] = fp.digest()\n+                return self.cache[request]\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        r1 = Request(\"http://www.example.com?a=1&a=2\")\n+        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n+        r2 = Request(\"http://www.example.com?a=2&a=1\")\n+        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n+        self.assertNotEqual(fp1, fp2)\n+\n+    def test_meta(self):\n+\n+        class RequestFingerprinter:\n+\n+            def fingerprint(self, request):\n+                if 'fingerprint' in request.meta:\n+                    return request.meta['fingerprint']\n+                return fingerprint(request)\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        r1 = Request(\"http://www.example.com\")\n+        fp1 = crawler.request_fingerprinter.fingerprint(r1)\n+        r2 = Request(\"http://www.example.com\", meta={'fingerprint': 'a'})\n+        fp2 = crawler.request_fingerprinter.fingerprint(r2)\n+        r3 = Request(\"http://www.example.com\", meta={'fingerprint': 'a'})\n+        fp3 = crawler.request_fingerprinter.fingerprint(r3)\n+        r4 = Request(\"http://www.example.com\", meta={'fingerprint': 'b'})\n+        fp4 = crawler.request_fingerprinter.fingerprint(r4)\n+        self.assertNotEqual(fp1, fp2)\n+        self.assertNotEqual(fp1, fp4)\n+        self.assertNotEqual(fp2, fp4)\n+        self.assertEqual(fp2, fp3)\n+\n+    def test_from_crawler(self):\n+\n+        class RequestFingerprinter:\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler)\n+\n+            def __init__(self, crawler):\n+                self._fingerprint = crawler.settings['FINGERPRINT']\n+\n+            def fingerprint(self, request):\n+                return self._fingerprint\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            'FINGERPRINT': b'fingerprint',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        request = Request(\"http://www.example.com\")\n+        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n+        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+\n+    def test_from_settings(self):\n+\n+        class RequestFingerprinter:\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                return cls(settings)\n+\n+            def __init__(self, settings):\n+                self._fingerprint = settings['FINGERPRINT']\n+\n+            def fingerprint(self, request):\n+                return self._fingerprint\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            'FINGERPRINT': b'fingerprint',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        request = Request(\"http://www.example.com\")\n+        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n+        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+\n+    def test_from_crawler_and_settings(self):\n+\n+        class RequestFingerprinter:\n+\n+            # This method is ignored due to the presence of from_crawler\n+            @classmethod\n+            def from_settings(cls, settings):\n+                return cls(settings)\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler)\n+\n+            def __init__(self, crawler):\n+                self._fingerprint = crawler.settings['FINGERPRINT']\n+\n+            def fingerprint(self, request):\n+                return self._fingerprint\n+\n+        settings = {\n+            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,\n+            'FINGERPRINT': b'fingerprint',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+\n+        request = Request(\"http://www.example.com\")\n+        fingerprint = crawler.request_fingerprinter.fingerprint(request)\n+        self.assertEqual(fingerprint, settings['FINGERPRINT'])\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
