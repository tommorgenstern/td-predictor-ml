{"custom_id": "keras#bba7b1a0d6cbee94b04f70514228fca9c1d165ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 21 | Files Changed: 4 | Hunks: 28 | Methods Changed: 12 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 111 | Churn Cumulative: 866 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 13 | DMM Complexity: 0.8611111111111112\n\nDIFF:\n@@ -396,21 +396,24 @@ class RandomBehaviorTest(testing.TestCase, parameterized.TestCase):\n         import tensorflow as tf\n \n         from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+        from keras.src.random.seed_generator import SeedGenerator\n \n         class BetaLayer(TFDataLayer):\n-            def __init__(self, **kwargs):\n+            def __init__(self, seed=None, **kwargs):\n                 super().__init__(**kwargs)\n-                self.generator = keras.random.SeedGenerator()\n+                self.seed = seed\n+                self.generator = SeedGenerator(seed)\n \n             def compute_output_shape(self, input_shape):\n                 return input_shape\n \n             def call(self, inputs):\n+                seed_generator = self._get_seed_generator(self.backend._backend)\n                 noise = self.backend.random.beta(\n                     self.backend.shape(inputs),\n                     alpha=0.5,\n                     beta=0.5,\n-                    seed=self._get_seed_generator(),\n+                    seed=seed_generator,\n                 )\n                 inputs = inputs + noise\n                 return inputs\n\n@@ -203,8 +203,10 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n                     # First, open the .h5 file, then write it to `zf` at the end\n                     # of the function call.\n                     working_dir = pathlib.Path(fileobj.name).parent\n-                    weights_file_path = working_dir / _VARS_FNAME_H5\n-                    weights_store = H5IOStore(weights_file_path, mode=\"w\")\n+                    weights_file_path = tempfile.NamedTemporaryFile(\n+                        dir=working_dir\n+                    )\n+                    weights_store = H5IOStore(weights_file_path.name, mode=\"w\")\n                     write_zf = True\n                 else:\n                     # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n@@ -242,9 +244,9 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n             if asset_store:\n                 asset_store.close()\n             if write_zf and weights_file_path:\n-                zf.write(weights_file_path, weights_file_path.name)\n+                zf.write(weights_file_path.name, _VARS_FNAME_H5)\n             if weights_file_path:\n-                weights_file_path.unlink()\n+                weights_file_path.close()\n \n \n def _upload_model_to_hf(model, hf_path, weights_format):\n@@ -425,7 +427,7 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n         )\n \n         all_filenames = zf.namelist()\n-        weights_file_path = None\n+        extract_dir = None\n         weights_store = None\n         asset_store = None\n         try:\n@@ -433,14 +435,17 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n                 if isinstance(fileobj, io.BufferedReader):\n                     # First, extract the model.weights.h5 file, then load it\n                     # using h5py.\n-                    working_dir = pathlib.Path(fileobj.name).parent\n                     try:\n-                        zf.extract(_VARS_FNAME_H5, working_dir)\n-                        weights_file_path = working_dir / _VARS_FNAME_H5\n-                        weights_store = H5IOStore(weights_file_path, mode=\"r\")\n+                        extract_dir = tempfile.TemporaryDirectory(\n+                            dir=pathlib.Path(fileobj.name).parent\n+                        )\n+                        zf.extract(_VARS_FNAME_H5, extract_dir.name)\n+                        weights_store = H5IOStore(\n+                            pathlib.Path(extract_dir.name, _VARS_FNAME_H5),\n+                            mode=\"r\",\n+                        )\n                     except OSError:\n                         # Fall back when it is a read-only system\n-                        weights_file_path = None\n                         weights_store = H5IOStore(_VARS_FNAME_H5, zf, mode=\"r\")\n                 else:\n                     # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n@@ -472,8 +477,8 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n                 weights_store.close()\n             if asset_store:\n                 asset_store.close()\n-            if weights_file_path:\n-                weights_file_path.unlink()\n+            if extract_dir:\n+                extract_dir.cleanup()\n \n         if failed_saveables:\n             _raise_loading_failure(error_msgs)\n\n@@ -12,6 +12,7 @@ import numpy as np\n import pytest\n \n import keras\n+from keras.src import backend\n from keras.src import ops\n from keras.src import testing\n from keras.src.saving import saving_lib\n@@ -239,7 +240,11 @@ def _get_subclassed_functional_model(compile=True):\n     return functional_model\n \n \n-@pytest.mark.requires_trainable_backend\n+# We need a global function for `Pool.apply_async`\n+def _load_model_fn(filepath):\n+    saving_lib.load_model(filepath)\n+\n+\n class SavingTest(testing.TestCase):\n     def _test_inference_after_instantiation(self, model):\n         x_ref = np.random.random((2, 4))\n@@ -309,26 +314,32 @@ class SavingTest(testing.TestCase):\n         for ref_m, m in zip(ref_metrics, new_metrics):\n             self.assertAllClose(ref_m, m)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_subclassed(self):\n         model = _get_subclassed_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_basic_sequential(self):\n         model = _get_basic_sequential_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_custom_sequential(self):\n         model = _get_custom_sequential_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_basic_functional(self):\n         model = _get_basic_functional_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_custom_functional(self):\n         model = _get_custom_functional_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_preserved_subclassed_functional(self):\n         model = _get_subclassed_functional_model(compile=True)\n         self._test_compile_preserved(model)\n@@ -342,6 +353,7 @@ class SavingTest(testing.TestCase):\n         self.assertFalse(subclassed_model.built)\n         self.assertFalse(loaded_model.built)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_saved_module_paths_and_class_names(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = _get_subclassed_model()\n@@ -368,6 +380,7 @@ class SavingTest(testing.TestCase):\n             \"my_mean_squared_error\",\n         )\n \n+    @pytest.mark.requires_trainable_backend\n     def test_saving_custom_assets_and_variables(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         model = ModelWithCustomSaving()\n@@ -501,6 +514,7 @@ class SavingTest(testing.TestCase):\n         saving_lib.load_weights_only(model, temp_filepath)\n         self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compile_arg(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n         model = _get_basic_functional_model()\n@@ -589,6 +603,7 @@ class SavingTest(testing.TestCase):\n             np.array(new_model.layers[2].kernel), new_layer_kernel_value\n         )\n \n+    @pytest.mark.requires_trainable_backend\n     def test_save_to_fileobj(self) -> None:\n         model = keras.Sequential(\n             [keras.layers.Dense(1, input_shape=(1,)), keras.layers.Dense(1)]\n@@ -634,7 +649,10 @@ class SavingTest(testing.TestCase):\n         with zipfile.ZipFile(filepath) as zf:\n             all_filenames = zf.namelist()\n             self.assertNotIn(\"model.weights.h5\", all_filenames)\n-        self.assertFalse(Path(filepath).with_name(\"model.weights.h5\").exists())\n+\n+        # Ensure we don't have any temporary files left.\n+        self.assertLen(os.listdir(Path(filepath).parent), 1)\n+        self.assertIn(\"model.keras\", os.listdir(Path(filepath).parent))\n \n     def test_load_model_exception_raised(self):\n         # Assume we have an error in `load_own_variables`.\n@@ -656,9 +674,48 @@ class SavingTest(testing.TestCase):\n                 filepath, custom_objects={\"RaiseErrorLayer\": RaiseErrorLayer}\n             )\n \n-        # Ensure we don't leave a bad \"model.weights.h5\" on the filesystem.\n-        self.assertTrue(Path(filepath).exists())\n-        self.assertFalse(Path(filepath).with_name(\"model.weights.h5\").exists())\n+        # Ensure we don't have any temporary files left.\n+        self.assertLen(os.listdir(Path(filepath).parent), 1)\n+        self.assertIn(\"model.keras\", os.listdir(Path(filepath).parent))\n+\n+    def test_load_model_read_only_system(self):\n+        model = keras.Sequential([keras.Input([1]), keras.layers.Dense(2)])\n+        filepath = f\"{self.get_temp_dir()}/model.keras\"\n+        saving_lib.save_model(model, filepath)\n+\n+        # Load the model correctly, regardless of whether an OSError occurs.\n+        original_mode = os.stat(Path(filepath).parent).st_mode\n+        os.chmod(Path(filepath).parent, mode=0o555)\n+        model = saving_lib.load_model(filepath)\n+        os.chmod(Path(filepath).parent, mode=original_mode)\n+\n+        # Ensure we don't have any temporary files left.\n+        self.assertLen(os.listdir(Path(filepath).parent), 1)\n+        self.assertIn(\"model.keras\", os.listdir(Path(filepath).parent))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"jax\",\n+        reason=\"JAX backend doesn't support Python's multiprocessing\",\n+    )\n+    @pytest.mark.skipif(\n+        testing.tensorflow_uses_gpu() or testing.torch_uses_gpu(),\n+        reason=\"This test doesn't support GPU\",\n+    )\n+    def test_load_model_concurrently(self):\n+        import multiprocessing as mp\n+\n+        model = keras.Sequential([keras.Input([1]), keras.layers.Dense(2)])\n+        filepath = f\"{self.get_temp_dir()}/model.keras\"\n+        saving_lib.save_model(model, filepath)\n+\n+        # Load the model concurrently.\n+        results = []\n+        with mp.Pool(4) as pool:\n+            for i in range(4):\n+                results.append(pool.apply_async(_load_model_fn, (filepath,)))\n+            pool.close()\n+            pool.join()\n+        [r.get() for r in results]  # No error occurs here\n \n \n @pytest.mark.requires_trainable_backend\n\n@@ -586,7 +586,11 @@ def jax_uses_gpu():\n \n \n def torch_uses_gpu():\n-    return backend.backend() == \"torch\" and uses_gpu()\n+    if backend.backend() != \"torch\":\n+        return False\n+    from keras.src.backend.torch.core import get_device\n+\n+    return get_device() == \"cuda\"\n \n \n def uses_gpu():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f26f5507f999264ca1c8faa0b8dfa1c8f769aa88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 8 | Methods Changed: 13 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 71 | Churn Cumulative: 835 | Contributors (this commit): 5 | Commits (past 90d): 28 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,12 +37,45 @@ class KerasTensor:\n     ):\n         from keras.src import backend\n \n-        self.shape = backend.standardize_shape(shape)\n-        self.dtype = backend.standardize_dtype(dtype)\n-        self.sparse = sparse\n+        self._shape = backend.standardize_shape(shape)\n+        self._dtype = backend.standardize_dtype(dtype)\n+        self._sparse = bool(sparse)\n         self.name = name or auto_name(self.__class__.__name__)\n         self.record_history = record_history\n \n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @shape.setter\n+    def shape(self, value):\n+        raise AttributeError(\n+            f\"The shape of {self.__class__.__name__} is immutable. One should \"\n+            \"create a new instance of KerasTensor for this.\"\n+        )\n+\n+    @property\n+    def dtype(self):\n+        return self._dtype\n+\n+    @dtype.setter\n+    def dtype(self, value):\n+        raise AttributeError(\n+            f\"The dtype of {self.__class__.__name__} is immutable. One should \"\n+            \"create a new instance of KerasTensor for this.\"\n+        )\n+\n+    @property\n+    def sparse(self):\n+        return self._sparse\n+\n+    @sparse.setter\n+    def sparse(self, value):\n+        raise AttributeError(\n+            f\"The sparse of {self.__class__.__name__} is immutable. One should \"\n+            \"create a new instance of KerasTensor for this.\"\n+        )\n+\n     @property\n     def ndim(self):\n         return len(self.shape)\n\n@@ -17,6 +17,20 @@ class KerasTensorTest(testing.TestCase):\n         self.assertEqual(x.shape, (3,))\n         self.assertEqual(x.sparse, True)\n \n+        # Raise error if trying to set attributes\n+        with self.assertRaisesRegex(\n+            AttributeError, \"The shape of KerasTensor is immutable.\"\n+        ):\n+            x.shape = [3, 2]\n+        with self.assertRaisesRegex(\n+            AttributeError, \"The dtype of KerasTensor is immutable.\"\n+        ):\n+            x.dtype = \"int32\"\n+        with self.assertRaisesRegex(\n+            AttributeError, \"The sparse of KerasTensor is immutable.\"\n+        ):\n+            x.sparse = False\n+\n     def test_numpy_methods(self):\n         x = keras_tensor.KerasTensor(shape=(3, 2), dtype=\"float32\")\n \n\n@@ -1,4 +1,5 @@\n from keras.src import backend\n+from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.backend.common import global_state\n \n@@ -161,7 +162,7 @@ class DTypePolicy:\n             return x\n         elif backend.is_keras_tensor(x):\n             if self._should_cast(x, autocast, dtype):\n-                x.dtype = dtype\n+                x = ops.cast(x, dtype=dtype)\n             return x\n         elif hasattr(x, \"__array__\"):\n             try:\n\n@@ -39,8 +39,9 @@ class Map(Operation):\n         y = backend.compute_output_spec(f, x)\n \n         def append_batch_axis(x):\n-            x.shape = (n,) + x.shape\n-            return x\n+            return KerasTensor(\n+                shape=(n,) + x.shape, dtype=x.dtype, sparse=x.sparse\n+            )\n \n         y = tree.map_structure(append_batch_axis, y)\n         return y\n@@ -114,9 +115,9 @@ class Scan(Operation):\n             )\n             x = xs[0]\n \n-        carry_spec, y_spec = backend.compute_output_spec(f, init, x)\n-        y_spec.shape = (n,) + y_spec.shape\n-        return carry_spec, y_spec\n+        carry, y = backend.compute_output_spec(f, init, x)\n+        y = KerasTensor(shape=(n,) + y.shape, dtype=y.dtype, sparse=y.sparse)\n+        return carry, y\n \n \n @keras_export(\"keras.ops.scan\")\n\n@@ -127,8 +127,8 @@ class Function(Operation):\n         # Ensure that dtype and sparse settings are the same as self._inputs,\n         # because we only care about the shape in this function.\n         for x, x_ref in zip(tree.flatten(input_shape_struct), self._inputs):\n-            x.dtype = x_ref.dtype\n-            x.sparse = x_ref.sparse\n+            x._dtype = x_ref.dtype\n+            x._sparse = x_ref.sparse\n         output_spec = self.compute_output_spec(input_shape_struct)\n         return tree.map_structure(lambda x: x.shape, output_spec)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98198c17d6c15b59c5985155f4854112d7e42116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 70 | Churn Cumulative: 1197 | Contributors (this commit): 8 | Commits (past 90d): 38 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2126,34 +2126,32 @@ def tri(N, M=None, k=0, dtype=None):\n def tril(x, k=0):\n     x = convert_to_tensor(x)\n \n-    if k >= 0:\n-        return tf.linalg.band_part(x, -1, k)\n-\n+    def _negative_k_branch():\n         shape = tf.shape(x)\n         rows, cols = shape[-2], shape[-1]\n-\n         i, j = tf.meshgrid(tf.range(rows), tf.range(cols), indexing=\"ij\")\n-\n         mask = i >= j - k\n-\n         return tf.where(tf.broadcast_to(mask, shape), x, tf.zeros_like(x))\n \n+    return tf.cond(\n+        k >= 0, lambda: tf.linalg.band_part(x, -1, k), _negative_k_branch\n+    )\n+\n \n def triu(x, k=0):\n     x = convert_to_tensor(x)\n \n-    if k <= 0:\n-        return tf.linalg.band_part(x, -k, -1)\n-\n+    def _positive_k_branch():\n         shape = tf.shape(x)\n         rows, cols = shape[-2], shape[-1]\n-\n         i, j = tf.meshgrid(tf.range(rows), tf.range(cols), indexing=\"ij\")\n-\n         mask = i <= j - k\n-\n         return tf.where(tf.broadcast_to(mask, shape), x, tf.zeros_like(x))\n \n+    return tf.cond(\n+        k <= 0, lambda: tf.linalg.band_part(x, -k, -1), _positive_k_branch\n+    )\n+\n \n def vdot(x1, x2):\n     x1 = convert_to_tensor(x1)\n\n@@ -4168,13 +4168,15 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         y1 = keras.layers.Lambda(\n             lambda x: keras.ops.tril(\n                 keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1]))\n-            )\n+            ),\n+            output_shape=(None, None, 3),\n         )(x)\n         y2 = keras.layers.Lambda(\n             lambda x: keras.ops.tril(\n                 keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1])),\n                 k=-1,\n-            )\n+            ),\n+            output_shape=(None, None, 3),\n         )(x)\n         model = keras.Model(x, [y1, y2])\n \n@@ -4183,6 +4185,24 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             result, [np.tril(np.ones((2, 2))), np.tril(np.ones((2, 2)), k=-1)]\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"Only test tensorflow backend\",\n+    )\n+    def test_tril_with_jit_in_tf(self):\n+        import tensorflow as tf\n+\n+        x = knp.reshape(knp.arange(24), [1, 2, 3, 4])\n+        k = knp.array(0)\n+        x_np = np.reshape(np.arange(24), [1, 2, 3, 4])\n+        k_np = np.array(0)\n+\n+        @tf.function(jit_compile=True)\n+        def fn(x, k):\n+            return knp.tril(x, k=k)\n+\n+        self.assertAllClose(fn(x, k), np.tril(x_np, k_np))\n+\n     def test_triu(self):\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         self.assertAllClose(knp.triu(x), np.triu(x))\n@@ -4200,13 +4220,15 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         y1 = keras.layers.Lambda(\n             lambda x: keras.ops.triu(\n                 keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1]))\n-            )\n+            ),\n+            output_shape=(None, None, 3),\n         )(x)\n         y2 = keras.layers.Lambda(\n             lambda x: keras.ops.triu(\n                 keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1])),\n                 k=-1,\n-            )\n+            ),\n+            output_shape=(None, None, 3),\n         )(x)\n         model = keras.Model(x, [y1, y2])\n \n@@ -4215,6 +4237,24 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             result, [np.triu(np.ones((2, 2))), np.triu(np.ones((2, 2)), k=-1)]\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"Only test tensorflow backend\",\n+    )\n+    def test_triu_with_jit_in_tf(self):\n+        import tensorflow as tf\n+\n+        x = knp.reshape(knp.arange(24), [1, 2, 3, 4])\n+        k = knp.array(0)\n+        x_np = np.reshape(np.arange(24), [1, 2, 3, 4])\n+        k_np = np.array(0)\n+\n+        @tf.function(jit_compile=True)\n+        def fn(x, k):\n+            return knp.triu(x, k=k)\n+\n+        self.assertAllClose(fn(x, k), np.triu(x_np, k_np))\n+\n     def test_vstack(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         y = np.array([[4, 5, 6], [6, 5, 4]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#272af9c26b7eac8a8bbccef3a1470a69e997c601", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 18 | Files Changed: 11 | Hunks: 25 | Methods Changed: 20 | Complexity Δ (Sum/Max): 4/1 | Churn Δ: 67 | Churn Cumulative: 665 | Contributors (this commit): 6 | Commits (past 90d): 45 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from keras.src.backend.jax.core import convert_to_numpy\n from keras.src.backend.jax.core import convert_to_tensor\n from keras.src.backend.jax.core import device_scope\n from keras.src.backend.jax.core import is_tensor\n+from keras.src.backend.jax.core import random_seed_dtype\n from keras.src.backend.jax.core import scatter\n from keras.src.backend.jax.core import shape\n from keras.src.backend.jax.core import stop_gradient\n\n@@ -346,6 +346,11 @@ def unstack(x, num=None, axis=0):\n     ]\n \n \n+def random_seed_dtype():\n+    # jax random seed uses uint32.\n+    return \"uint32\"\n+\n+\n def custom_gradient(fun):\n     return jax.custom_gradient(fun=fun)\n \n\n@@ -13,6 +13,7 @@ from keras.src.backend.numpy.core import cond\n from keras.src.backend.numpy.core import convert_to_numpy\n from keras.src.backend.numpy.core import convert_to_tensor\n from keras.src.backend.numpy.core import is_tensor\n+from keras.src.backend.numpy.core import random_seed_dtype\n from keras.src.backend.numpy.core import shape\n from keras.src.backend.numpy.core import vectorized_map\n from keras.src.backend.numpy.rnn import cudnn_ok\n\n@@ -298,6 +298,10 @@ def unstack(x, num=None, axis=0):\n     return [x[i] for i in range(x.shape[0])]\n \n \n+def random_seed_dtype():\n+    return \"uint32\"\n+\n+\n class custom_gradient:\n     \"\"\"Decorator for custom gradients.\n \n\n@@ -17,6 +17,7 @@ from keras.src.backend.tensorflow.core import convert_to_tensor\n from keras.src.backend.tensorflow.core import device_scope\n from keras.src.backend.tensorflow.core import is_tensor\n from keras.src.backend.tensorflow.core import name_scope\n+from keras.src.backend.tensorflow.core import random_seed_dtype\n from keras.src.backend.tensorflow.core import scatter\n from keras.src.backend.tensorflow.core import shape\n from keras.src.backend.tensorflow.core import stop_gradient\n\n@@ -417,6 +417,11 @@ def unstack(x, num=None, axis=0):\n     return tf.unstack(x, num=num, axis=axis)\n \n \n+def random_seed_dtype():\n+    # tensorflow random operation only works on int32/int64, not uint32.\n+    return \"int32\"\n+\n+\n def custom_gradient(fun):\n     return tf.custom_gradient(f=fun)\n \n\n@@ -7,14 +7,9 @@ from keras.src.random.seed_generator import draw_seed\n from keras.src.random.seed_generator import make_default_seed\n \n \n-def tf_draw_seed(seed):\n-    # TF ops only accept int32/64 seeds but our base seed is uint32.\n-    return tf.cast(draw_seed(seed), dtype=\"int32\")\n-\n-\n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     return tf.random.stateless_normal(\n         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n@@ -22,7 +17,7 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     return tf.random.stateless_uniform(\n         shape=shape,\n         minval=tf.cast(minval, dtype),\n@@ -33,7 +28,7 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n \n \n def categorical(logits, num_samples, dtype=\"int64\", seed=None):\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     output = tf.random.stateless_categorical(logits, num_samples, seed=seed)\n     return tf.cast(output, dtype)\n \n@@ -42,7 +37,7 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n     intemediate_dtype = dtype\n     if standardize_dtype(dtype) not in [\"int32\", \"int64\"]:\n         intemediate_dtype = \"int64\"\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     output = tf.random.stateless_uniform(\n         shape=shape,\n         minval=minval,\n@@ -55,7 +50,7 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     return tf.random.stateless_truncated_normal(\n         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n@@ -75,7 +70,7 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n     return tf.nn.experimental.stateless_dropout(\n         inputs,\n@@ -88,7 +83,7 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n def shuffle(x, axis=0, seed=None):\n     from keras.src.backend.tensorflow.numpy import swapaxes\n \n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     if axis == 0:\n         return tf.random.experimental.stateless_shuffle(x, seed=seed)\n     x = swapaxes(x, axis1=0, axis2=axis)\n@@ -99,7 +94,7 @@ def shuffle(x, axis=0, seed=None):\n \n def gamma(shape, alpha, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     # TODO: `tf.random.stateless_gamma` doesn't support bfloat16\n     intemediate_dtype = dtype\n     if standardize_dtype(dtype) == \"bfloat16\":\n@@ -117,7 +112,7 @@ def gamma(shape, alpha, dtype=None, seed=None):\n \n def binomial(shape, counts, probabilities, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = tf_draw_seed(seed)\n+    seed = draw_seed(seed)\n     # TODO: `tf.random.stateless_binomial` doesn't support bfloat16\n     intemediate_dtype = dtype\n     if standardize_dtype(dtype) == \"bfloat16\":\n@@ -146,7 +141,7 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n     # gamma random variables to prevent any unintended\n     # dependencies and correlations between the generated values\n     # due to the usage of same seed.\n-    seed_1 = tf_draw_seed(seed)\n+    seed_1 = draw_seed(seed)\n     # The choice of 12 is totally arbitrary, as we're\n     # incrementing the first drawn seed by a CONSTANT to\n     # ensure deterministic results.\n\n@@ -30,6 +30,7 @@ from keras.src.backend.torch.core import convert_to_numpy\n from keras.src.backend.torch.core import convert_to_tensor\n from keras.src.backend.torch.core import device_scope\n from keras.src.backend.torch.core import is_tensor\n+from keras.src.backend.torch.core import random_seed_dtype\n from keras.src.backend.torch.core import scatter\n from keras.src.backend.torch.core import shape\n from keras.src.backend.torch.core import stop_gradient\n\n@@ -501,6 +501,11 @@ def unstack(x, num=None, axis=0):\n     return x.unbind(axis)\n \n \n+def random_seed_dtype():\n+    # uint32 doesn't exist in torch, use int32 instead.\n+    return \"int32\"\n+\n+\n class custom_gradient:\n     \"\"\"Decorator for custom gradients.\n \n\n@@ -75,7 +75,7 @@ class SeedGenerator:\n             self.state = self.backend.Variable(\n                 seed_initializer,\n                 shape=(2,),\n-                dtype=\"uint32\",\n+                dtype=self.backend.random_seed_dtype(),\n                 trainable=False,\n                 name=\"seed_generator_state\",\n             )\n@@ -86,7 +86,7 @@ class SeedGenerator:\n         new_seed_value = seed_state.value * 1\n         if ordered:\n             increment = self.backend.convert_to_tensor(\n-                np.array([0, 1]), dtype=\"uint32\"\n+                np.array([0, 1]), dtype=seed_state.dtype\n             )\n             self.state.assign(seed_state + increment)\n         else:\n@@ -133,11 +133,12 @@ def make_default_seed():\n \n def draw_seed(seed):\n     from keras.src.backend import convert_to_tensor\n+    from keras.src.backend import random_seed_dtype\n \n     if isinstance(seed, SeedGenerator):\n         return seed.next()\n     elif isinstance(seed, int):\n-        return convert_to_tensor([seed, 0], dtype=\"uint32\")\n+        return convert_to_tensor([seed, 0], dtype=random_seed_dtype())\n     elif seed is None:\n         return global_seed_generator().next(ordered=False)\n     raise ValueError(\n\n@@ -37,6 +37,15 @@ class SeedGeneratorTest(testing.TestCase):\n         seed2 = seed_generator.make_default_seed()\n         self.assertNotEqual(seed1, seed2)\n \n+    def test_seed_generator_dtype(self):\n+        gen = seed_generator.SeedGenerator(seed=42)\n+        self.assertEqual(gen.state.dtype, backend.random_seed_dtype())\n+        seed = gen.next()\n+        self.assertEqual(gen.state.dtype, backend.random_seed_dtype())\n+        self.assertEqual(\n+            backend.standardize_dtype(seed.dtype), backend.random_seed_dtype()\n+        )\n+\n     def test_draw_seed_from_seed_generator(self):\n         gen = seed_generator.SeedGenerator(seed=42)\n         seed1 = seed_generator.draw_seed(gen)\n@@ -45,6 +54,9 @@ class SeedGeneratorTest(testing.TestCase):\n     def test_draw_seed_from_integer(self):\n         seed2 = seed_generator.draw_seed(12345)\n         self.assertTrue(backend.is_tensor(seed2))\n+        self.assertEqual(\n+            backend.standardize_dtype(seed2.dtype), backend.random_seed_dtype()\n+        )\n \n     def test_draw_seed_from_none(self):\n         seed3 = seed_generator.draw_seed(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#309f2c9c8959222e59d537b447c087a65c8b8998", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 188 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -419,7 +419,7 @@ def unstack(x, num=None, axis=0):\n \n def random_seed_dtype():\n     # tensorflow random operation only works on int32/int64, not uint32.\n-    return \"int32\"\n+    return \"int64\"\n \n \n def custom_gradient(fun):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c69b3b35ae7094b97fbdfa2c7cd2364a443311a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 116 | Lines Deleted: 28 | Files Changed: 5 | Hunks: 32 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 144 | Churn Cumulative: 548 | Contributors (this commit): 6 | Commits (past 90d): 21 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras.src import backend\n+from keras.src import dtype_policies\n from keras.src import ops\n from keras.src import tree\n from keras.src.api_export import keras_export\n@@ -10,6 +11,17 @@ from keras.src.utils.naming import auto_name\n class Loss(KerasSaveable):\n     \"\"\"Loss base class.\n \n+    Args:\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n+\n     To be implemented by subclasses:\n \n     * `call()`: Contains the logic for loss calculation using `y_true`,\n@@ -27,7 +39,12 @@ class Loss(KerasSaveable):\n     def __init__(self, name=None, reduction=\"sum_over_batch_size\", dtype=None):\n         self.name = name or auto_name(self.__class__.__name__)\n         self.reduction = standardize_reduction(reduction)\n-        self.dtype = dtype or backend.floatx()\n+        self._dtype_policy = dtype_policies.get(dtype)\n+        self._dtype = self._dtype_policy.compute_dtype\n+\n+    @property\n+    def dtype(self):\n+        return self._dtype\n \n     def __call__(self, y_true, y_pred, sample_weight=None):\n         in_mask = getattr(y_pred, \"_keras_mask\", None)\n\n@@ -4,6 +4,7 @@ import numpy as np\n import pytest\n \n from keras.src import backend\n+from keras.src import dtype_policies\n from keras.src import losses as losses_module\n from keras.src import ops\n from keras.src import testing\n@@ -251,4 +252,13 @@ class LossTest(testing.TestCase):\n         # JAX will map float64 to float32.\n         loss_fn = ExampleLoss(dtype=\"float16\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float16\")\n+        self.assertDType(loss, \"float16\")\n+\n+        # Test DTypePolicy for `dtype` argument\n+        loss_fn = ExampleLoss(dtype=dtype_policies.DTypePolicy(\"mixed_float16\"))\n+        loss = loss_fn(y_true, y_pred)\n+        self.assertDType(loss, \"float16\")\n+\n+        # `dtype` setter should raise AttributeError\n+        with self.assertRaises(AttributeError):\n+            loss.dtype = \"bfloat16\"\n\n@@ -57,7 +57,8 @@ class MeanSquaredError(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -92,7 +93,8 @@ class MeanAbsoluteError(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -127,7 +129,8 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -165,7 +168,8 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -212,7 +216,8 @@ class CosineSimilarity(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -261,7 +266,8 @@ class Huber(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -299,7 +305,8 @@ class LogCosh(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -332,7 +339,8 @@ class Hinge(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -365,7 +373,8 @@ class SquaredHinge(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -399,7 +408,8 @@ class CategoricalHinge(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -438,7 +448,8 @@ class KLDivergence(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -470,7 +481,8 @@ class Poisson(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -514,7 +526,8 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n \n@@ -650,7 +663,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n \n@@ -807,7 +821,8 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n \n@@ -944,7 +959,8 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n \n@@ -1048,7 +1064,8 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Examples:\n \n@@ -2020,7 +2037,8 @@ class CTC(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n     \"\"\"\n \n     def __init__(\n@@ -2095,7 +2113,8 @@ class Dice(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Returns:\n         Dice loss value.\n@@ -2206,7 +2225,8 @@ class Tversky(LossFunctionWrapper):\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n             `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`).\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Returns:\n         Tversky loss value.\n\n@@ -1,4 +1,5 @@\n from keras.src import backend\n+from keras.src import dtype_policies\n from keras.src import initializers\n from keras.src import ops\n from keras.src.api_export import keras_export\n@@ -12,8 +13,12 @@ class Metric(KerasSaveable):\n     \"\"\"Encapsulates metric logic and state.\n \n     Args:\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n+        name: Optional name for the metric instance.\n+        dtype: The dtype of the metric's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n \n     Example:\n \n@@ -86,7 +91,8 @@ class Metric(KerasSaveable):\n \n     def __init__(self, dtype=None, name=None):\n         self.name = name or auto_name(self.__class__.__name__)\n-        self._dtype = dtype or backend.floatx()\n+        self._dtype_policy = dtype_policies.get(dtype)\n+        self._dtype = self._dtype_policy.compute_dtype\n         self._metrics = []\n         self._variables = []\n         self._tracker = Tracker(\n\n@@ -3,6 +3,7 @@ import pickle\n import numpy as np\n \n from keras.src import backend\n+from keras.src import dtype_policies\n from keras.src import initializers\n from keras.src import metrics as metrics_module\n from keras.src import ops\n@@ -24,15 +25,18 @@ class ExampleMetric(Metric):\n         )\n \n     def update_state(self, y_true, y_pred):\n-        y_true = ops.convert_to_tensor(y_true)\n-        y_pred = ops.convert_to_tensor(y_pred)\n+        y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n+        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n         sum = ops.sum((y_true - y_pred) ** 2)\n         self.sum.assign(self.sum + sum)\n         batch_size = ops.shape(y_true)[0]\n         self.total.assign(self.total + batch_size)\n \n     def result(self):\n-        return self.sum / (ops.cast(self.total, dtype=\"float32\") + 1e-7)\n+        _sum = ops.cast(self.sum, dtype=self.dtype)\n+        _total = ops.cast(self.total, dtype=self.dtype)\n+        _epsilon = ops.cast(backend.epsilon(), dtype=self.dtype)\n+        return _sum / (_total + _epsilon)\n \n     def reset_state(self):\n         self.sum.assign(0.0)\n@@ -193,3 +197,34 @@ class MetricTest(testing.TestCase):\n \n         with self.assertRaises(ValueError):\n             metrics_module.get(\"typo\")\n+\n+    def test_dtype_arg(self):\n+        metric = ExampleMetric(name=\"mse\", dtype=\"float16\")\n+        self.assertEqual(metric.name, \"mse\")\n+        self.assertEqual(len(metric.variables), 2)\n+\n+        num_samples = 10\n+        y_true = np.random.random((num_samples, 3))\n+        y_pred = np.random.random((num_samples, 3))\n+        metric.update_state(y_true, y_pred)\n+        result = metric.result()\n+        self.assertAllClose(\n+            result, np.sum((y_true - y_pred) ** 2) / num_samples, atol=1e-3\n+        )\n+        self.assertDType(result, \"float16\")\n+\n+        # Test DTypePolicy for `dtype` argument\n+        metric = ExampleMetric(\n+            dtype=dtype_policies.DTypePolicy(\"mixed_float16\")\n+        )\n+        metric.update_state(y_true, y_pred)\n+        metric.update_state(y_true, y_pred)\n+        result = metric.result()\n+        self.assertAllClose(\n+            result, np.sum((y_true - y_pred) ** 2) / num_samples, atol=1e-3\n+        )\n+        self.assertDType(result, \"float16\")\n+\n+        # `dtype` setter should raise AttributeError\n+        with self.assertRaises(AttributeError):\n+            metric.dtype = \"bfloat16\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3964044dcf56b5f3c37899729932fbf441112261", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 393 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -240,25 +240,26 @@ class AssociativeScan(Operation):\n def associative_scan(f, elems, reverse=False, axis=0):\n     \"\"\"Performs a scan with an associative binary operation, in parallel.\n \n-    This operation has a similar use-case to scan. The key difference is that\n-    associative_scan is a parallel implementation with\n+    This operation his similar to `scan`, with the key difference that\n+    `associative_scan` is a parallel implementation with\n     potentially significant performance benefits, especially when jit compiled.\n+    The catch is that it can only be used when `f` is a binary associative\n+    operation (i.e. it must verify `f(a, f(b, c)) == f(f(a, b), c)`).\n \n-    For an introduction to associative scans, refer to the original paper\n-    Blelloch, Guy E. 1990. \"Prefix Sums and Their Applications\"\n-    (https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf).\n+    For an introduction to associative scans, refer to this paper:\n+    Blelloch, Guy E. 1990.\n+    [Prefix Sums and Their Applications](\n+        https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf).\n \n     Args:\n         f: A Python callable implementing an associative binary operation with\n             signature `r = f(a, b)`. Function `f` must be associative, i.e.,\n             it must satisfy the equation\n             `f(a, f(b, c)) == f(f(a, b), c)`.\n-\n             The inputs and result are (possibly nested Python tree structures\n             of) array(s) matching `elems`. Each array has a dimension in place\n             of the `axis` dimension. `f` should be applied elementwise over\n             the `axis` dimension.\n-\n             The result `r` has the same shape (and structure) as the\n             two inputs `a` and `b`.\n         elems: A (possibly nested Python tree structure of) array(s), each with\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
