{"custom_id": "scrapy#b70443f2d06b1b0ad8c474fc5e8a424e363bdd81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 24 | Files Changed: 18 | Hunks: 20 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 11205 | Contributors (this commit): 96 | Commits (past 90d): 58 | Contributors (cumulative): 266 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# pylint: disable=import-error\n from operator import itemgetter\n \n from docutils import nodes\n\n@@ -8,6 +8,8 @@\n #\n # All configuration values have a default; values that are commented out\n # serve to show the default.\n+\n+# pylint: disable=import-error\n import os\n import sys\n from pathlib import Path\n\n@@ -199,7 +199,7 @@ def _create_testcase(method: Callable, desc: str) -> TestCase:\n     spider = method.__self__.name  # type: ignore[attr-defined]\n \n     class ContractTestCase(TestCase):\n-        def __str__(_self) -> str:\n+        def __str__(_self) -> str:  # pylint: disable=no-self-argument\n             return f\"[{spider}] {method.__name__} ({desc})\"\n \n     name = f\"{spider}_{method.__name__}\"\n\n@@ -110,6 +110,8 @@ class ItemFilter:\n class IFeedStorage(Interface):\n     \"\"\"Interface that all Feed Storages must implement\"\"\"\n \n+    # pylint: disable=no-self-argument\n+\n     def __init__(uri, *, feed_options=None):  # pylint: disable=super-init-not-called\n         \"\"\"Initialize the storage with the parameters given in the URI and the\n         feed-specific options (see :setting:`FEEDS`)\"\"\"\n\n@@ -84,7 +84,9 @@ class TelnetConsole(protocol.ServerFactory):\n             \"\"\"An implementation of IPortal\"\"\"\n \n             @defers\n-            def login(self_, credentials, mind, *interfaces):\n+            def login(\n+                self_, credentials, mind, *interfaces\n+            ):  # pylint: disable=no-self-argument\n                 if not (\n                     credentials.username == self.username.encode(\"utf8\")\n                     and credentials.checkPassword(self.password.encode(\"utf8\"))\n\n@@ -1,3 +1,5 @@\n+# pylint:disable=no-method-argument,no-self-argument\n+\n from zope.interface import Interface\n \n \n\n@@ -71,17 +71,16 @@ class Shell:\n         else:\n             self.populate_vars()\n         if self.code:\n+            # pylint: disable-next=eval-used\n             print(eval(self.code, globals(), self.vars))  # noqa: S307\n         else:\n-            \"\"\"\n-            Detect interactive shell setting in scrapy.cfg\n-            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n-            [settings]\n-            # shell can be one of ipython, bpython or python;\n-            # to be used as the interactive python console, if available.\n-            # (default is ipython, fallbacks in the order listed above)\n-            shell = python\n-            \"\"\"\n+            # Detect interactive shell setting in scrapy.cfg\n+            # e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n+            # [settings]\n+            # # shell can be one of ipython, bpython or python;\n+            # # to be used as the interactive python console, if available.\n+            # # (default is ipython, fallbacks in the order listed above)\n+            # shell = python\n             cfg = get_config()\n             section, option = \"settings\", \"shell\"\n             env = os.environ.get(\"SCRAPY_PYTHON_SHELL\")\n\n@@ -59,7 +59,7 @@ def _embed_ptpython_shell(\n     namespace: dict[str, Any] = {}, banner: str = \"\"\n ) -> EmbedFuncT:\n     \"\"\"Start a ptpython shell\"\"\"\n-    import ptpython.repl\n+    import ptpython.repl  # pylint: disable=import-error\n \n     @wraps(_embed_ptpython_shell)\n     def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n\n@@ -57,6 +57,7 @@ def create_deprecated_class(\n \n     # https://github.com/python/mypy/issues/4177\n     class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n+        # pylint: disable=no-self-argument\n         deprecated_class: type | None = None\n         warned_on_subclass: bool = False\n \n\n@@ -30,6 +30,7 @@ def _tty_supports_color() -> bool:\n \n \n def _colorize(text: str, colorize: bool = True) -> str:\n+    # pylint: disable=no-name-in-module\n     if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n         return text\n     try:\n\n@@ -32,7 +32,7 @@ def get_engine_status(engine: ExecutionEngine) -> list[tuple[str, Any]]:\n     checks: list[tuple[str, Any]] = []\n     for test in tests:\n         try:\n-            checks += [(test, eval(test))]  # noqa: S307\n+            checks += [(test, eval(test))]  # noqa: S307  # pylint: disable=eval-used\n         except Exception as e:\n             checks += [(test, f\"{type(e).__name__} (exception)\")]\n \n\n@@ -517,8 +517,8 @@ class ContractsManagerTest(unittest.TestCase):\n                 super().__init__(*args, **kwargs)\n                 self.visited = 0\n \n-            def start_requests(s):\n-                return self.conman.from_spider(s, self.results)\n+            def start_requests(self_):  # pylint: disable=no-self-argument\n+                return self.conman.from_spider(self_, self.results)\n \n             def parse_first(self, response):\n                 self.visited += 1\n\n@@ -216,7 +216,9 @@ class PprintItemExporterTest(BaseItemExporterTest):\n         return PprintItemExporter(self.output, **kwargs)\n \n     def _check_output(self):\n-        self._assert_expected_item(eval(self.output.getvalue()))\n+        self._assert_expected_item(\n+            eval(self.output.getvalue())  # pylint: disable=eval-used\n+        )\n \n \n class PprintItemExporterDataclassTest(PprintItemExporterTest):\n\n@@ -54,7 +54,7 @@ class ItemTest(unittest.TestCase):\n \n         self.assertEqual(itemrepr, \"{'name': 'John Doe', 'number': 123}\")\n \n-        i2 = eval(itemrepr)\n+        i2 = eval(itemrepr)  # pylint: disable=eval-used\n         self.assertEqual(i2[\"name\"], \"John Doe\")\n         self.assertEqual(i2[\"number\"], 123)\n \n\n@@ -49,7 +49,7 @@ class LinkTest(unittest.TestCase):\n         l1 = Link(\n             \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n         )\n-        l2 = eval(repr(l1))\n+        l2 = eval(repr(l1))  # pylint: disable=eval-used\n         self._assert_same_links(l1, l2)\n \n     def test_bytes_url(self):\n\n@@ -3,7 +3,7 @@ import warnings\n \n def test_deprecated_twisted_version():\n     with warnings.catch_warnings(record=True) as warns:\n-        from scrapy import twisted_version\n+        from scrapy import twisted_version  # pylint: disable=no-name-in-module\n \n         assert twisted_version is not None\n         assert isinstance(twisted_version, tuple)\n\n@@ -264,7 +264,7 @@ class JMESPathTestCase(unittest.TestCase):\n         )\n \n     @pytest.mark.skipif(PARSEL_18_PLUS, reason=\"parsel >= 1.8 supports jmespath\")\n-    def test_jmespath_not_available(my_json_page) -> None:\n+    def test_jmespath_not_available(self) -> None:\n         body = \"\"\"\n         {\n             \"website\": {\"name\": \"Example\"}\n\n@@ -1,3 +1,7 @@\n+\"\"\"\n+Queues that handle requests\n+\"\"\"\n+\n import shutil\n import tempfile\n import unittest\n@@ -16,10 +20,6 @@ from scrapy.squeues import (\n )\n from scrapy.utils.test import get_crawler\n \n-\"\"\"\n-Queues that handle requests\n-\"\"\"\n-\n \n class BaseQueueTestCase(unittest.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc706d4fc307f0b51d8122f10dee6ad8e2653629", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 955 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -277,14 +277,12 @@ class DownloaderAwareSchedulerTestMixin:\n         downloader = self.mock_crawler.engine.downloader\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n-            # pylint: disable=protected-access\n             slot = downloader.get_slot_key(request)\n             dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n         for request in requests:\n-            # pylint: disable=protected-access\n             slot = downloader.get_slot_key(request)\n             downloader.decrement(slot)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b10d46d280fbc84f7a1c50e116a1ed828aa286c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 221 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,12 +83,15 @@ class LogStats:\n         self, spider: Spider\n     ) -> tuple[None, None] | tuple[float, float]:\n         start_time = self.stats.get_value(\"start_time\")\n-        finished_time = self.stats.get_value(\"finished_time\")\n+        finish_time = self.stats.get_value(\"finish_time\")\n \n-        if not start_time or not finished_time:\n+        if not start_time or not finish_time:\n             return None, None\n \n-        mins_elapsed = (finished_time - start_time).seconds / 60\n+        mins_elapsed = (finish_time - start_time).seconds / 60\n+\n+        if mins_elapsed == 0:\n+            return None, None\n \n         items = self.stats.get_value(\"item_scraped_count\", 0)\n         pages = self.stats.get_value(\"response_received_count\", 0)\n\n@@ -47,7 +47,7 @@ class TestLogStats(unittest.TestCase):\n \n         # Simulate when spider closes after running for 30 mins\n         self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n-        self.stats.set_value(\"finished_time\", datetime.fromtimestamp(1655101972))\n+        self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655101972))\n         logstats.spider_closed(self.spider, \"test reason\")\n         self.assertEqual(self.stats.get_value(\"responses_per_minute\"), 172.9)\n         self.assertEqual(self.stats.get_value(\"items_per_minute\"), 116.4)\n@@ -60,3 +60,12 @@ class TestLogStats(unittest.TestCase):\n         logstats.spider_closed(self.spider, \"test reason\")\n         self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n         self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n+\n+    def test_stats_calculation_no_elapsed_time(self):\n+        \"\"\"The stat values should be None since the elapsed time is 0.\"\"\"\n+        logstats = LogStats.from_crawler(self.crawler)\n+        self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n+        self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655100172))\n+        logstats.spider_closed(self.spider, \"test reason\")\n+        self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n+        self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ae5b9267145e5b01d282f766fa90d129eb40390", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 88 | Files Changed: 22 | Hunks: 22 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 88 | Churn Cumulative: 19728 | Contributors (this commit): 88 | Commits (past 90d): 52 | Contributors (cumulative): 299 | DMM Complexity: None\n\nDIFF:\n@@ -566,7 +566,3 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n                 assert \"cached\" in res2.flags\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1314,7 +1314,3 @@ def test_meta_refresh_schemes(url, location, target):\n     else:\n         assert isinstance(redirect, Request)\n         assert redirect.url == target\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -643,7 +643,3 @@ class GetRetryRequestTest(unittest.TestCase):\n             f\"{stats_key}/reason_count/{expected_reason}\",\n         ):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -699,7 +699,3 @@ class CustomExporterItemTest(unittest.TestCase):\n \n class CustomExporterDataclassTest(CustomExporterItemTest):\n     item_class = TestDataClass\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1717,7 +1717,3 @@ class JsonRequestTest(RequestTest):\n     def tearDown(self):\n         warnings.resetwarnings()\n         super().tearDown()\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -296,7 +296,3 @@ class ItemMetaClassCellRegression(unittest.TestCase):\n                 # TypeError: __class__ set to <class '__main__.MyItem'>\n                 # defining 'MyItem' as <class '__main__.MyItem'>\n                 super().__init__(*args, **kwargs)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -586,7 +586,3 @@ class FunctionProcessorTestCase(unittest.TestCase):\n         lo.add_value(\"foo\", \"  bar  \")\n         lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n         self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -715,7 +715,3 @@ class FunctionProcessorTestCase(unittest.TestCase):\n         lo.add_value(\"foo\", \"  bar  \")\n         lo.add_value(\"foo\", [\"  asdf  \", \"  qwerty  \"])\n         self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -234,7 +234,3 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n         self.assertNotIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n         self.assertNotIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n         self.assertNotIn(\"Dropped: Ignoring item\", str(lc))\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -157,7 +157,3 @@ class MailSenderTest(unittest.TestCase):\n \n         context = factory.buildProtocol(\"test@scrapy.org\").context\n         self.assertIsInstance(context, ClientTLSOptions)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -126,7 +126,3 @@ class ResponseTypesTest(unittest.TestCase):\n         self.assertEqual(\n             responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0], \"x-scrapy/test\"\n         )\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -497,7 +497,3 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(\n             str(error.exception), \"Trying to modify an immutable Settings object\"\n         )\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -177,7 +177,3 @@ class FeedExportConfigTestCase(unittest.TestCase):\n                 \"item_export_kwargs\": {},\n             },\n         )\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -38,7 +38,3 @@ class UtilsConsoleTestCase(unittest.TestCase):\n         # default shell should be 'ipython'\n         shell = get_shell_embed_func()\n         self.assertEqual(shell.__name__, \"_embed_ipython_shell\")\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -372,7 +372,3 @@ class LocalWeakReferencedCacheTest(unittest.TestCase):\n         for i, r in enumerate(refs):\n             self.assertIn(r, cache)\n             self.assertEqual(cache[r], i)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -20,7 +20,3 @@ class HttpobjUtilsTest(unittest.TestCase):\n         assert req1a is req1b\n         assert req1a is not req2\n         assert req1a is not req2\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -216,7 +216,3 @@ class UtilsMiscTestCase(unittest.TestCase):\n         assert rel_has_nofollow(\"nofollowfoo\") is False\n         assert rel_has_nofollow(\"foonofollow\") is False\n         assert rel_has_nofollow(\"ugc,  ,  nofollow\") is True\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -481,7 +481,3 @@ class RequestToCurlTest(unittest.TestCase):\n             \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n         )\n         self._test_request(request_object, expected_curl_command)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -295,7 +295,3 @@ Disallow: /forum/active/\n         )\n \n         self.assertEqual(list(s), [{\"loc\": \"http://127.0.0.1:8000/\"}])\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -30,7 +30,3 @@ class UtilsSpidersTestCase(unittest.TestCase):\n \n         it = iter_spider_classes(tests.test_utils_spider)\n         self.assertEqual(set(it), {MySpider1, MySpider2})\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -33,7 +33,3 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n \n         render_path.unlink()\n         assert not render_path.exists()  # Failure of test itself\n-\n-\n-if \"__main__\" == __name__:\n-    unittest.main()\n\n@@ -630,7 +630,3 @@ def test_deprecated_imports_from_w3lib(obj_name):\n         getattr(import_module(\"scrapy.utils.url\"), obj_name)\n \n         assert message in warns[0].message.args\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#176ae348c57a536d661ca1b469035e7e1ccbca6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 2842 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -891,7 +891,12 @@ class TestSettingsPolicyByName(TestCase):\n         # test parsing without space(s) after the comma\n         settings1 = Settings(\n             {\n-                \"REFERRER_POLICY\": f\"some-custom-unknown-policy,{POLICY_SAME_ORIGIN},{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},another-custom-unknown-policy\"\n+                \"REFERRER_POLICY\": (\n+                    f\"some-custom-unknown-policy,\"\n+                    f\"{POLICY_SAME_ORIGIN},\"\n+                    f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},\"\n+                    f\"another-custom-unknown-policy\"\n+                )\n             }\n         )\n         mw1 = RefererMiddleware(settings1)\n@@ -900,7 +905,11 @@ class TestSettingsPolicyByName(TestCase):\n         # test parsing with space(s) after the comma\n         settings2 = Settings(\n             {\n-                \"REFERRER_POLICY\": f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},    another-custom-unknown-policy,    {POLICY_UNSAFE_URL}\"\n+                \"REFERRER_POLICY\": (\n+                    f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},\"\n+                    f\"    another-custom-unknown-policy,\"\n+                    f\"    {POLICY_UNSAFE_URL}\"\n+                )\n             }\n         )\n         mw2 = RefererMiddleware(settings2)\n@@ -909,7 +918,11 @@ class TestSettingsPolicyByName(TestCase):\n     def test_multiple_policy_tokens_all_invalid(self):\n         settings = Settings(\n             {\n-                \"REFERRER_POLICY\": \"some-custom-unknown-policy,another-custom-unknown-policy,yet-another-custom-unknown-policy\"\n+                \"REFERRER_POLICY\": (\n+                    \"some-custom-unknown-policy,\"\n+                    \"another-custom-unknown-policy,\"\n+                    \"yet-another-custom-unknown-policy\"\n+                )\n             }\n         )\n         with self.assertRaises(RuntimeError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4d31277bc67169460dc2d8bca80946df8b355b8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 2 | Files Changed: 7 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 3920 | Contributors (this commit): 44 | Commits (past 90d): 16 | Contributors (cumulative): 87 | DMM Complexity: None\n\nDIFF:\n@@ -2,3 +2,8 @@ from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import (\n     HTTP11DownloadHandler as HTTPDownloadHandler,\n )\n+\n+__all__ = [\n+    \"HTTP10DownloadHandler\",\n+    \"HTTPDownloadHandler\",\n+]\n\n@@ -15,3 +15,16 @@ from scrapy.http.response.html import HtmlResponse\n from scrapy.http.response.json import JsonResponse\n from scrapy.http.response.text import TextResponse\n from scrapy.http.response.xml import XmlResponse\n+\n+__all__ = [\n+    \"FormRequest\",\n+    \"Headers\",\n+    \"HtmlResponse\",\n+    \"JsonRequest\",\n+    \"JsonResponse\",\n+    \"Request\",\n+    \"Response\",\n+    \"TextResponse\",\n+    \"XmlResponse\",\n+    \"XmlRpcRequest\",\n+]\n\n@@ -126,3 +126,8 @@ def _is_valid_url(url: str) -> bool:\n \n # Top-level imports\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n+\n+__all__ = [\n+    \"IGNORED_EXTENSIONS\",\n+    \"LinkExtractor\",\n+]\n\n@@ -4,3 +4,8 @@ Selectors\n \n # top-level imports\n from scrapy.selector.unified import Selector, SelectorList\n+\n+__all__ = [\n+    \"Selector\",\n+    \"SelectorList\",\n+]\n\n@@ -117,3 +117,12 @@ class Spider(object_ref):\n from scrapy.spiders.crawl import CrawlSpider, Rule\n from scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\n from scrapy.spiders.sitemap import SitemapSpider\n+\n+__all__ = [\n+    \"CSVFeedSpider\",\n+    \"CrawlSpider\",\n+    \"Rule\",\n+    \"SitemapSpider\",\n+    \"Spider\",\n+    \"XMLFeedSpider\",\n+]\n\n@@ -1,7 +1,8 @@\n import unittest\n+from abc import ABCMeta\n from unittest import mock\n \n-from scrapy.item import ABCMeta, Field, Item, ItemMeta\n+from scrapy.item import Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\n\n@@ -6,7 +6,7 @@ import pytest\n from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import (\n+from scrapy.utils.url import (  # type: ignore[attr-defined]\n     _is_filesystem_path,\n     _public_w3lib_objects,\n     add_http_if_no_scheme,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2234c5b96d4069d6881aacb7007ba3d305dfb0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 4367 | Contributors (this commit): 53 | Commits (past 90d): 27 | Contributors (cumulative): 78 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,7 +44,7 @@ if TYPE_CHECKING:\n     from scrapy.logformatter import LogFormatter\n     from scrapy.spiderloader import SpiderLoader\n     from scrapy.statscollectors import StatsCollector\n-    from scrapy.utils.request import RequestFingerprinter\n+    from scrapy.utils.request import RequestFingerprinterProtocol\n \n \n logger = logging.getLogger(__name__)\n@@ -80,7 +80,7 @@ class Crawler:\n         self.extensions: ExtensionManager | None = None\n         self.stats: StatsCollector | None = None\n         self.logformatter: LogFormatter | None = None\n-        self.request_fingerprinter: RequestFingerprinter | None = None\n+        self.request_fingerprinter: RequestFingerprinterProtocol | None = None\n         self.spider: Spider | None = None\n         self.engine: ExecutionEngine | None = None\n \n\n@@ -29,7 +29,7 @@ if TYPE_CHECKING:\n     from scrapy.http.request import Request\n     from scrapy.settings import BaseSettings\n     from scrapy.spiders import Spider\n-    from scrapy.utils.request import RequestFingerprinter\n+    from scrapy.utils.request import RequestFingerprinterProtocol\n \n \n logger = logging.getLogger(__name__)\n@@ -265,7 +265,9 @@ class DbmCacheStorage:\n         )\n \n         assert spider.crawler.request_fingerprinter\n-        self._fingerprinter: RequestFingerprinter = spider.crawler.request_fingerprinter\n+        self._fingerprinter: RequestFingerprinterProtocol = (\n+            spider.crawler.request_fingerprinter\n+        )\n \n     def close_spider(self, spider: Spider) -> None:\n         self.db.close()\n\n@@ -30,7 +30,7 @@ if TYPE_CHECKING:\n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n     from scrapy.http import Response\n-    from scrapy.utils.request import RequestFingerprinter\n+    from scrapy.utils.request import RequestFingerprinterProtocol\n \n \n class FileInfo(TypedDict):\n@@ -47,7 +47,7 @@ logger = logging.getLogger(__name__)\n \n class MediaPipeline(ABC):\n     crawler: Crawler\n-    _fingerprinter: RequestFingerprinter\n+    _fingerprinter: RequestFingerprinterProtocol\n     _modern_init = False\n \n     LOG_FAILED_RESULTS: bool = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59fcb9b93c4971602b4a4afd4afdf08db7a7f2b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 27 | Files Changed: 7 | Hunks: 24 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 6206 | Contributors (this commit): 64 | Commits (past 90d): 36 | Contributors (cumulative): 143 | DMM Complexity: None\n\nDIFF:\n@@ -284,6 +284,7 @@ intersphinx_mapping = {\n     \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n     \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n     \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n+    \"parsel\": (\"https://parsel.readthedocs.io/en/latest/\", None),\n     \"pytest\": (\"https://docs.pytest.org/en/latest\", None),\n     \"python\": (\"https://docs.python.org/3\", None),\n     \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),\n\n@@ -115,7 +115,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n     @abstractmethod\n     def next_request(self) -> Request | None:\n         \"\"\"\n-        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n+        Return the next :class:`~scrapy.Request` to be processed, or ``None``\n         to indicate that there are no requests to be considered ready at the moment.\n \n         Returning ``None`` implies that no request from the scheduler will be sent\n@@ -263,7 +263,7 @@ class Scheduler(BaseScheduler):\n \n     def next_request(self) -> Request | None:\n         \"\"\"\n-        Return a :class:`~scrapy.http.Request` object from the memory queue,\n+        Return a :class:`~scrapy.Request` object from the memory queue,\n         falling back to the disk queue if the memory queue is empty.\n         Return ``None`` if there are no more enqueued requests.\n \n\n@@ -59,7 +59,7 @@ RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n def NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n     \"\"\"When assigned to the ``callback`` parameter of\n-    :class:`~scrapy.http.Request`, it indicates that the request is not meant\n+    :class:`~scrapy.Request`, it indicates that the request is not meant\n     to have a spider callback at all.\n \n     For example:\n@@ -83,7 +83,7 @@ def NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n \n class Request(object_ref):\n     \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n-    executed by the Downloader, thus generating a :class:`Response`.\n+    executed by the Downloader, thus generating a :class:`~scrapy.http.Response`.\n     \"\"\"\n \n     attributes: tuple[str, ...] = (\n@@ -103,9 +103,9 @@ class Request(object_ref):\n     )\n     \"\"\"A tuple of :class:`str` objects containing the name of all public\n     attributes of the class that are also keyword parameters of the\n-    ``__init__`` method.\n+    ``__init__()`` method.\n \n-    Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n+    Currently used by :meth:`.Request.replace`, :meth:`.Request.to_dict` and\n     :func:`~scrapy.utils.request.request_from_dict`.\n     \"\"\"\n \n@@ -233,7 +233,7 @@ class Request(object_ref):\n         finding unknown options call this method by passing\n         ``ignore_unknown_options=False``.\n \n-        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n+        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.Request`\n                      subclasses, such as :class:`~scrapy.http.JsonRequest`, or\n                      :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                      :ref:`downloader middlewares <topics-downloader-middleware>`\n@@ -244,7 +244,7 @@ class Request(object_ref):\n                      :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                      or\n                      :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n-                     may modify the :class:`~scrapy.http.Request` object.\n+                     may modify the :class:`~scrapy.Request` object.\n \n         To translate a cURL command into a Scrapy request,\n         you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n\n@@ -51,7 +51,7 @@ class Response(object_ref):\n     )\n     \"\"\"A tuple of :class:`str` objects containing the name of all public\n     attributes of the class that are also keyword parameters of the\n-    ``__init__`` method.\n+    ``__init__()`` method.\n \n     Currently used by :meth:`Response.replace`.\n     \"\"\"\n@@ -199,8 +199,8 @@ class Response(object_ref):\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n-        It accepts the same arguments as ``Request.__init__`` method,\n-        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n+        It accepts the same arguments as ``Request.__init__()`` method,\n+        but ``url`` can be a relative URL or a :class:`~scrapy.link.Link` object,\n         not only an absolute URL.\n \n         :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n@@ -254,7 +254,7 @@ class Response(object_ref):\n         .. versionadded:: 2.0\n \n         Return an iterable of :class:`~.Request` instances to follow all links\n-        in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n+        in ``urls``. It accepts the same arguments as ``Request.__init__()`` method,\n         but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n         not only absolute URLs.\n \n\n@@ -185,15 +185,15 @@ class TextResponse(Response):\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n-        It accepts the same arguments as ``Request.__init__`` method,\n+        It accepts the same arguments as ``Request.__init__()`` method,\n         but ``url`` can be not only an absolute URL, but also\n \n         * a relative URL\n         * a :class:`~scrapy.link.Link` object, e.g. the result of\n           :ref:`topics-link-extractors`\n-        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n+        * a :class:`~scrapy.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n           ``response.css('a.my_link')[0]``\n-        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n+        * an attribute :class:`~scrapy.Selector` (not SelectorList), e.g.\n           ``response.css('a::attr(href)')[0]`` or\n           ``response.xpath('//img/@src')[0]``\n \n@@ -241,20 +241,20 @@ class TextResponse(Response):\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n         links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n-        ``__init__`` method, except that each ``urls`` element does not need to be\n+        ``__init__()`` method, except that each ``urls`` element does not need to be\n         an absolute URL, it can be any of the following:\n \n         * a relative URL\n         * a :class:`~scrapy.link.Link` object, e.g. the result of\n           :ref:`topics-link-extractors`\n-        * a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n+        * a :class:`~scrapy.Selector` object for a ``<link>`` or ``<a>`` element, e.g.\n           ``response.css('a.my_link')[0]``\n-        * an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g.\n+        * an attribute :class:`~scrapy.Selector` (not SelectorList), e.g.\n           ``response.css('a::attr(href)')[0]`` or\n           ``response.xpath('//img/@src')[0]``\n \n         In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n-        within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n+        within the ``follow_all()`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n \n         Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n         using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n\n@@ -32,7 +32,7 @@ class ItemLoader(itemloaders.ItemLoader):\n     :param selector: The selector to extract data from, when using the\n         :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n         :meth:`replace_css` method.\n-    :type selector: :class:`~scrapy.selector.Selector` object\n+    :type selector: :class:`~scrapy.Selector` object\n \n     :param response: The response used to construct the selector using the\n         :attr:`default_selector_class`, unless the selector argument is given,\n@@ -79,7 +79,7 @@ class ItemLoader(itemloaders.ItemLoader):\n \n     .. attribute:: selector\n \n-        The :class:`~scrapy.selector.Selector` object to extract data from.\n+        The :class:`~scrapy.Selector` object to extract data from.\n         It's either the selector given in the ``__init__`` method or one created from\n         the response given in the ``__init__`` method using the\n         :attr:`default_selector_class`. This attribute is meant to be\n\n@@ -1,6 +1,6 @@\n \"\"\"\n This module provides some useful functions for working with\n-scrapy.http.Request objects\n+scrapy.Request objects\n \"\"\"\n \n from __future__ import annotations\n@@ -109,12 +109,10 @@ class RequestFingerprinter:\n \n     It takes into account a canonical version\n     (:func:`w3lib.url.canonicalize_url`) of :attr:`request.url\n-    <scrapy.http.Request.url>` and the values of :attr:`request.method\n-    <scrapy.http.Request.method>` and :attr:`request.body\n-    <scrapy.http.Request.body>`. It then generates an `SHA1\n+    <scrapy.Request.url>` and the values of :attr:`request.method\n+    <scrapy.Request.method>` and :attr:`request.body\n+    <scrapy.Request.body>`. It then generates an `SHA1\n     <https://en.wikipedia.org/wiki/SHA-1>`_ hash.\n-\n-    .. seealso:: :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION`.\n     \"\"\"\n \n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dfbecd3924a0d7a9e555e9cc3618cdb06b5415d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 152 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -15,13 +15,6 @@ os.environ[\"http_proxy\"] = \"\"\n os.environ[\"https_proxy\"] = \"\"\n os.environ[\"ftp_proxy\"] = \"\"\n \n-# Absolutize paths to coverage config and output file because tests that\n-# spawn subprocesses also changes current working directory.\n-_sourceroot = Path(__file__).resolve().parent.parent\n-if \"COV_CORE_CONFIG\" in os.environ:\n-    os.environ[\"COVERAGE_FILE\"] = str(_sourceroot / \".coverage\")\n-    os.environ[\"COV_CORE_CONFIG\"] = str(_sourceroot / os.environ[\"COV_CORE_CONFIG\"])\n-\n tests_datadir = str(Path(__file__).parent.resolve() / \"sample_data\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fc91bb46262118c9ff7aa2b4719d880f727699f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 44 | Churn Cumulative: 803 | Contributors (this commit): 20 | Commits (past 90d): 8 | Contributors (cumulative): 37 | DMM Complexity: 0.12\n\nDIFF:\n@@ -40,7 +40,11 @@ class OffsiteMiddleware:\n         self.process_request(request, spider)\n \n     def process_request(self, request: Request, spider: Spider) -> None:\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n\n@@ -61,7 +61,11 @@ class OffsiteMiddleware:\n     def _filter(self, request: Any, spider: Spider) -> bool:\n         if not isinstance(request, Request):\n             return True\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return True\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n\n@@ -64,6 +64,37 @@ def test_process_request_dont_filter(value, filtered):\n         assert mw.process_request(request, spider) is None\n \n \n+@pytest.mark.parametrize(\n+    (\"allow_offsite\", \"dont_filter\", \"filtered\"),\n+    (\n+        (True, UNSET, False),\n+        (True, None, False),\n+        (True, False, False),\n+        (True, True, False),\n+        (False, UNSET, True),\n+        (False, None, True),\n+        (False, False, True),\n+        (False, True, False),\n+    ),\n+)\n+def test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    kwargs = {\"meta\": {}}\n+    if allow_offsite is not UNSET:\n+        kwargs[\"meta\"][\"allow_offsite\"] = allow_offsite\n+    if dont_filter is not UNSET:\n+        kwargs[\"dont_filter\"] = dont_filter\n+    request = Request(\"https://b.example\", **kwargs)\n+    if filtered:\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+    else:\n+        assert mw.process_request(request, spider) is None\n+\n+\n @pytest.mark.parametrize(\n     \"value\",\n     (\n\n@@ -29,6 +29,7 @@ class TestOffsiteMiddleware(TestCase):\n             Request(\"http://scrapy.org/1\"),\n             Request(\"http://sub.scrapy.org/1\"),\n             Request(\"http://offsite.tld/letmepass\", dont_filter=True),\n+            Request(\"http://offsite-2.tld/allow\", meta={\"allow_offsite\": True}),\n             Request(\"http://scrapy.test.org/\"),\n             Request(\"http://scrapy.test.org:8000/\"),\n         ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#402500b164efc01257679247d3dd1628a5f90f5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 44 | Churn Cumulative: 3598 | Contributors (this commit): 43 | Commits (past 90d): 10 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,6 +89,12 @@ def _get_commands_dict(\n     return cmds\n \n \n+def _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n+    return set(_get_commands_dict(settings, inproject=True)) - set(\n+        _get_commands_dict(settings, inproject=False)\n+    )\n+\n+\n def _pop_command_name(argv: list[str]) -> str | None:\n     for i, arg in enumerate(argv[1:]):\n         if not arg.startswith(\"-\"):\n@@ -121,11 +127,25 @@ def _print_commands(settings: BaseSettings, inproject: bool) -> None:\n     print('Use \"scrapy <command> -h\" to see more info about a command')\n \n \n+def _print_unknown_command_msg(\n+    settings: BaseSettings, cmdname: str, inproject: bool\n+) -> None:\n+    proj_only_cmds = _get_project_only_cmds(settings)\n+    if cmdname in proj_only_cmds and not inproject:\n+        cmd_list = \", \".join(sorted(proj_only_cmds))\n+        print(\n+            f\"The {cmdname} command is not available from this location.\\n\"\n+            f\"These commands are only available from within a project: {cmd_list}.\\n\"\n+        )\n+    else:\n+        print(f\"Unknown command: {cmdname}\\n\")\n+\n+\n def _print_unknown_command(\n     settings: BaseSettings, cmdname: str, inproject: bool\n ) -> None:\n     _print_header(settings, inproject)\n-    print(f\"Unknown command: {cmdname}\\n\")\n+    _print_unknown_command_msg(settings, cmdname, inproject)\n     print('Use \"scrapy\" to see available commands')\n \n \n\n@@ -9,6 +9,7 @@ import re\n import subprocess\n import sys\n from contextlib import contextmanager\n+from io import StringIO\n from itertools import chain\n from pathlib import Path\n from shutil import copytree, rmtree\n@@ -16,12 +17,13 @@ from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n from typing import TYPE_CHECKING\n-from unittest import skipIf\n+from unittest import mock, skipIf\n \n from pytest import mark\n from twisted.trial import unittest\n \n import scrapy\n+from scrapy.cmdline import _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n@@ -652,6 +654,24 @@ class MiscCommandsTest(CommandTest):\n     def test_list(self):\n         self.assertEqual(0, self.call(\"list\"))\n \n+    def test_command_not_found(self):\n+        na_msg = \"\"\"\n+The list command is not available from this location.\n+These commands are only available from within a project: check, crawl, edit, list, parse.\n+\"\"\"\n+        not_found_msg = \"\"\"\n+Unknown command: abc\n+\"\"\"\n+        params = [\n+            (\"list\", 0, na_msg),\n+            (\"abc\", 0, not_found_msg),\n+            (\"abc\", 1, not_found_msg),\n+        ]\n+        for cmdname, inproject, message in params:\n+            with mock.patch(\"sys.stdout\", new=StringIO()) as out:\n+                _print_unknown_command_msg(Settings(), cmdname, inproject)\n+                self.assertEqual(out.getvalue().strip(), message.strip())\n+\n \n class RunSpiderCommandTest(CommandTest):\n     spider_filename = \"myspider.py\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98ba61256deceba7b04b938a97005258f4ef5c66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 39 | Churn Cumulative: 1477 | Contributors (this commit): 30 | Commits (past 90d): 12 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -258,6 +258,10 @@ coverage_ignore_pyobjects = [\n     # Base classes of downloader middlewares are implementation details that\n     # are not meant for users.\n     r\"^scrapy\\.downloadermiddlewares\\.\\w*?\\.Base\\w*?Middleware\",\n+    # The interface methods of duplicate request filtering classes are already\n+    # covered in the interface documentation part of the DUPEFILTER_CLASS\n+    # setting documentation.\n+    r\"^scrapy\\.dupefilters\\.[A-Z]\\w*?\\.(from_settings|request_seen|open|close|log)$\",\n     # Private exception used by the command-line interface implementation.\n     r\"^scrapy\\.exceptions\\.UsageError\",\n     # Methods of BaseItemExporter subclasses are only documented in\n\n@@ -4,6 +4,7 @@ import logging\n import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n+from warnings import warn\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n@@ -26,6 +27,9 @@ if TYPE_CHECKING:\n \n \n class BaseDupeFilter:\n+    \"\"\"Dummy duplicate request filtering class (:setting:`DUPEFILTER_CLASS`)\n+    that does not filter out any request.\"\"\"\n+\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n         warnings.warn(\n@@ -50,10 +54,19 @@ class BaseDupeFilter:\n \n     def log(self, request: Request, spider: Spider) -> None:\n         \"\"\"Log that a request has been filtered\"\"\"\n+        warn(\n+            \"Calling BaseDupeFilter.log() is deprecated.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n \n \n class RFPDupeFilter(BaseDupeFilter):\n-    \"\"\"Request Fingerprint duplicates filter\"\"\"\n+    \"\"\"Duplicate request filtering class (:setting:`DUPEFILTER_CLASS`) that\n+    filters out requests with the canonical\n+    (:func:`w3lib.url.canonicalize_url`) :attr:`~scrapy.http.Request.url`,\n+    :attr:`~scrapy.http.Request.method` and :attr:`~scrapy.http.Request.body`.\n+    \"\"\"\n \n     def __init__(\n         self,\n@@ -117,6 +130,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         return False\n \n     def request_fingerprint(self, request: Request) -> str:\n+        \"\"\"Returns a string that uniquely identifies the specified request.\"\"\"\n         return self.fingerprinter.fingerprint(request).hex()\n \n     def close(self, reason: str) -> None:\n\n@@ -4,11 +4,13 @@ import sys\n import tempfile\n import unittest\n from pathlib import Path\n+from warnings import catch_warnings\n \n from testfixtures import LogCapture\n \n from scrapy.core.scheduler import Scheduler\n-from scrapy.dupefilters import RFPDupeFilter\n+from scrapy.dupefilters import BaseDupeFilter, RFPDupeFilter\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n@@ -252,3 +254,18 @@ class RFPDupeFilterTest(unittest.TestCase):\n             )\n \n             dupefilter.close(\"finished\")\n+\n+\n+class BaseDupeFilterTestCase(unittest.TestCase):\n+    def test_log_deprecation(self):\n+        dupefilter = _get_dupefilter(\n+            settings={\"DUPEFILTER_CLASS\": BaseDupeFilter},\n+        )\n+        with catch_warnings(record=True) as warning_list:\n+            dupefilter.log(None, None)\n+        self.assertEqual(len(warning_list), 1)\n+        self.assertEqual(\n+            str(warning_list[0].message),\n+            \"Calling BaseDupeFilter.log() is deprecated.\",\n+        )\n+        self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c1e83895c15dc491c6c133982cde22d778dcae6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 35 | Churn Cumulative: 3633 | Contributors (this commit): 44 | Commits (past 90d): 10 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -96,10 +96,9 @@ def _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n \n \n def _pop_command_name(argv: list[str]) -> str | None:\n-    for i, arg in enumerate(argv[1:]):\n-        if not arg.startswith(\"-\"):\n-            del argv[i]\n-            return arg\n+    for i in range(1, len(argv)):\n+        if not argv[i].startswith(\"-\"):\n+            return argv.pop(i)\n     return None\n \n \n\n@@ -23,7 +23,7 @@ from pytest import mark\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.cmdline import _print_unknown_command_msg\n+from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n@@ -1163,3 +1163,29 @@ class HelpMessageTest(CommandTest):\n         for command in self.commands:\n             _, out, _ = self.proc(command, \"-h\")\n             self.assertIn(\"Usage\", out)\n+\n+\n+class PopCommandNameTest(unittest.TestCase):\n+    def test_valid_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"my_spider\"])\n+\n+    def test_no_command(self):\n+        argv = [\"scrapy\"]\n+        command = _pop_command_name(argv)\n+        self.assertIsNone(command)\n+        self.assertEqual(argv, [\"scrapy\"])\n+\n+    def test_option_before_command(self):\n+        argv = [\"scrapy\", \"-h\", \"crawl\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+\n+    def test_option_after_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"-h\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
