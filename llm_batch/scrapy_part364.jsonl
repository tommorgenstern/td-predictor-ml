{"custom_id": "scrapy#24f21e96b99bac1ba991fe02c8823782833ca298", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 51 | Files Changed: 2 | Hunks: 23 | Methods Changed: 27 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 119 | Churn Cumulative: 1732 | Contributors (this commit): 26 | Commits (past 90d): 8 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -185,7 +185,7 @@ class Response(object_ref):\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[Union[dict, List[dict]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n-        encoding: str = \"utf-8\",\n+        encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable] = None,\n@@ -205,6 +205,8 @@ class Response(object_ref):\n         .. versionadded:: 2.0\n            The *flags* parameter.\n         \"\"\"\n+        if encoding is None:\n+            raise ValueError(\"encoding can't be None\")\n         if isinstance(url, Link):\n             url = url.url\n         elif url is None:\n@@ -236,7 +238,7 @@ class Response(object_ref):\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[Union[dict, List[dict]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n-        encoding: str = \"utf-8\",\n+        encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable] = None,\n\n@@ -8,7 +8,21 @@ from __future__ import annotations\n \n import json\n from contextlib import suppress\n-from typing import TYPE_CHECKING, Any, Generator, Optional, Tuple, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AnyStr,\n+    Callable,\n+    Dict,\n+    Generator,\n+    Iterable,\n+    List,\n+    Mapping,\n+    Optional,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urljoin\n \n import parsel\n@@ -23,6 +37,7 @@ from w3lib.html import strip_html5_whitespace\n \n from scrapy.http import Request\n from scrapy.http.response import Response\n+from scrapy.link import Link\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n@@ -39,20 +54,14 @@ class TextResponse(Response):\n     attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n \n     def __init__(self, *args: Any, **kwargs: Any):\n-        self._encoding = kwargs.pop(\"encoding\", None)\n+        self._encoding: Optional[str] = kwargs.pop(\"encoding\", None)\n         self._cached_benc: Optional[str] = None\n         self._cached_ubody: Optional[str] = None\n         self._cached_selector: Optional[Selector] = None\n         super().__init__(*args, **kwargs)\n \n-    def _set_url(self, url):\n-        if isinstance(url, str):\n-            self._url = to_unicode(url, self.encoding)\n-        else:\n-            super()._set_url(url)\n-\n-    def _set_body(self, body):\n-        self._body = b\"\"  # used by encoding detection\n+    def _set_body(self, body: Union[str, bytes, None]) -> None:\n+        self._body: bytes = b\"\"  # used by encoding detection\n         if isinstance(body, str):\n             if self._encoding is None:\n                 raise TypeError(\n@@ -64,10 +73,10 @@ class TextResponse(Response):\n             super()._set_body(body)\n \n     @property\n-    def encoding(self):\n+    def encoding(self) -> str:\n         return self._declared_encoding() or self._body_inferred_encoding()\n \n-    def _declared_encoding(self):\n+    def _declared_encoding(self) -> Optional[str]:\n         return (\n             self._encoding\n             or self._bom_encoding()\n@@ -75,7 +84,7 @@ class TextResponse(Response):\n             or self._body_declared_encoding()\n         )\n \n-    def json(self):\n+    def json(self) -> Any:\n         \"\"\"\n         .. versionadded:: 2.2\n \n@@ -96,7 +105,7 @@ class TextResponse(Response):\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n-    def urljoin(self, url):\n+    def urljoin(self, url: str) -> str:\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n         return urljoin(get_base_url(self), url)\n@@ -106,7 +115,7 @@ class TextResponse(Response):\n         content_type = cast(bytes, self.headers.get(b\"Content-Type\", b\"\"))\n         return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n-    def _body_inferred_encoding(self):\n+    def _body_inferred_encoding(self) -> str:\n         if self._cached_benc is None:\n             content_type = to_unicode(\n                 cast(bytes, self.headers.get(b\"Content-Type\", b\"\")), encoding=\"latin-1\"\n@@ -121,20 +130,21 @@ class TextResponse(Response):\n             self._cached_ubody = ubody\n         return self._cached_benc\n \n-    def _auto_detect_fun(self, text):\n+    def _auto_detect_fun(self, text: bytes) -> Optional[str]:\n         for enc in (self._DEFAULT_ENCODING, \"utf-8\", \"cp1252\"):\n             try:\n                 text.decode(enc)\n             except UnicodeError:\n                 continue\n             return resolve_encoding(enc)\n+        return None\n \n     @memoizemethod_noargs\n-    def _body_declared_encoding(self):\n+    def _body_declared_encoding(self) -> Optional[str]:\n         return html_body_declared_encoding(self.body)\n \n     @memoizemethod_noargs\n-    def _bom_encoding(self):\n+    def _bom_encoding(self) -> Optional[str]:\n         return read_bom(self.body)[0]\n \n     @property\n@@ -146,6 +156,8 @@ class TextResponse(Response):\n         return self._cached_selector\n \n     def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n+        from scrapy.selector import SelectorList\n+\n         if not hasattr(self.selector, \"jmespath\"):  # type: ignore[attr-defined]\n             raise AttributeError(\n                 \"Please install parsel >= 1.8.1 to get jmespath support\"\n@@ -154,26 +166,30 @@ class TextResponse(Response):\n         return cast(SelectorList, self.selector.jmespath(query, **kwargs))  # type: ignore[attr-defined]\n \n     def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n+        from scrapy.selector import SelectorList\n+\n         return cast(SelectorList, self.selector.xpath(query, **kwargs))\n \n     def css(self, query: str) -> SelectorList:\n+        from scrapy.selector import SelectorList\n+\n         return cast(SelectorList, self.selector.css(query))\n \n     def follow(\n         self,\n-        url,\n-        callback=None,\n-        method=\"GET\",\n-        headers=None,\n-        body=None,\n-        cookies=None,\n-        meta=None,\n-        encoding=None,\n-        priority=0,\n-        dont_filter=False,\n-        errback=None,\n-        cb_kwargs=None,\n-        flags=None,\n+        url: Union[str, Link, parsel.Selector],\n+        callback: Optional[Callable] = None,\n+        method: str = \"GET\",\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        body: Optional[Union[bytes, str]] = None,\n+        cookies: Optional[Union[dict, List[dict]]] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n+        encoding: Optional[str] = None,\n+        priority: int = 0,\n+        dont_filter: bool = False,\n+        errback: Optional[Callable] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        flags: Optional[List[str]] = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -214,21 +230,21 @@ class TextResponse(Response):\n \n     def follow_all(\n         self,\n-        urls=None,\n-        callback=None,\n-        method=\"GET\",\n-        headers=None,\n-        body=None,\n-        cookies=None,\n-        meta=None,\n-        encoding=None,\n-        priority=0,\n-        dont_filter=False,\n-        errback=None,\n-        cb_kwargs=None,\n-        flags=None,\n-        css=None,\n-        xpath=None,\n+        urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n+        callback: Optional[Callable] = None,\n+        method: str = \"GET\",\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        body: Optional[Union[bytes, str]] = None,\n+        cookies: Optional[Union[dict, List[dict]]] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n+        encoding: Optional[str] = None,\n+        priority: int = 0,\n+        dont_filter: bool = False,\n+        errback: Optional[Callable] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        flags: Optional[List[str]] = None,\n+        css: Optional[str] = None,\n+        xpath: Optional[str] = None,\n     ) -> Generator[Request, None, None]:\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n@@ -270,7 +286,7 @@ class TextResponse(Response):\n                 with suppress(_InvalidSelector):\n                     urls.append(_url_from_selector(sel))\n         return super().follow_all(\n-            urls=urls,\n+            urls=cast(Iterable[Union[str, Link]], urls),\n             callback=callback,\n             method=method,\n             headers=headers,\n@@ -292,8 +308,7 @@ class _InvalidSelector(ValueError):\n     \"\"\"\n \n \n-def _url_from_selector(sel):\n-    # type: (parsel.Selector) -> str\n+def _url_from_selector(sel: parsel.Selector) -> str:\n     if isinstance(sel.root, str):\n         # e.g. ::attr(href) result\n         return strip_html5_whitespace(sel.root)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e54dc598999e913526fc6aa5cad26ee0e7ca6140", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 178 | Lines Deleted: 76 | Files Changed: 5 | Hunks: 80 | Methods Changed: 78 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 254 | Churn Cumulative: 1737 | Contributors (this commit): 20 | Commits (past 90d): 5 | Contributors (cumulative): 46 | DMM Complexity: 0.8\n\nDIFF:\n@@ -4,46 +4,67 @@ Depth Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-import logging\n+from __future__ import annotations\n \n-from scrapy.http import Request\n+import logging\n+from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler\n+from scrapy.http import Request, Response\n+from scrapy.statscollectors import StatsCollector\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n \n class DepthMiddleware:\n-    def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):\n+    def __init__(\n+        self,\n+        maxdepth: int,\n+        stats: StatsCollector,\n+        verbose_stats: bool = False,\n+        prio: int = 1,\n+    ):\n         self.maxdepth = maxdepth\n         self.stats = stats\n         self.verbose_stats = verbose_stats\n         self.prio = prio\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         settings = crawler.settings\n         maxdepth = settings.getint(\"DEPTH_LIMIT\")\n         verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n         prio = settings.getint(\"DEPTH_PRIORITY\")\n+        assert crawler.stats\n         return cls(maxdepth, crawler.stats, verbose, prio)\n \n-    def process_spider_output(self, response, result, spider):\n+    def process_spider_output(\n+        self, response: Response, result: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n         self._init_depth(response, spider)\n-        return (r for r in result or () if self._filter(r, response, spider))\n+        return (r for r in result if self._filter(r, response, spider))\n \n-    async def process_spider_output_async(self, response, result, spider):\n+    async def process_spider_output_async(\n+        self, response: Response, result: AsyncIterable[Any], spider: Spider\n+    ) -> AsyncIterable[Any]:\n         self._init_depth(response, spider)\n-        async for r in result or ():\n+        async for r in result:\n             if self._filter(r, response, spider):\n                 yield r\n \n-    def _init_depth(self, response, spider):\n+    def _init_depth(self, response: Response, spider: Spider) -> None:\n         # base case (depth=0)\n         if \"depth\" not in response.meta:\n             response.meta[\"depth\"] = 0\n             if self.verbose_stats:\n                 self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n \n-    def _filter(self, request, response, spider):\n+    def _filter(self, request: Any, response: Response, spider: Spider) -> bool:\n         if not isinstance(request, Request):\n             return True\n         depth = response.meta[\"depth\"] + 1\n\n@@ -3,9 +3,20 @@ HttpError Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-import logging\n+from __future__ import annotations\n \n+import logging\n+from typing import TYPE_CHECKING, Any, Iterable, List, Optional\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n+from scrapy.http import Response\n+from scrapy.settings import BaseSettings\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n@@ -13,21 +24,23 @@ logger = logging.getLogger(__name__)\n class HttpError(IgnoreRequest):\n     \"\"\"A non-200 response was filtered\"\"\"\n \n-    def __init__(self, response, *args, **kwargs):\n+    def __init__(self, response: Response, *args: Any, **kwargs: Any):\n         self.response = response\n         super().__init__(*args, **kwargs)\n \n \n class HttpErrorMiddleware:\n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def __init__(self, settings):\n-        self.handle_httpstatus_all = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n-        self.handle_httpstatus_list = settings.getlist(\"HTTPERROR_ALLOWED_CODES\")\n+    def __init__(self, settings: BaseSettings):\n+        self.handle_httpstatus_all: bool = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n+        self.handle_httpstatus_list: List[int] = settings.getlist(\n+            \"HTTPERROR_ALLOWED_CODES\"\n+        )\n \n-    def process_spider_input(self, response, spider):\n+    def process_spider_input(self, response: Response, spider: Spider) -> None:\n         if 200 <= response.status < 300:  # common case\n             return\n         meta = response.meta\n@@ -45,8 +58,11 @@ class HttpErrorMiddleware:\n             return\n         raise HttpError(response, \"Ignoring non-200 response\")\n \n-    def process_spider_exception(self, response, exception, spider):\n+    def process_spider_exception(\n+        self, response: Response, exception: Exception, spider: Spider\n+    ) -> Optional[Iterable[Any]]:\n         if isinstance(exception, HttpError):\n+            assert spider.crawler.stats\n             spider.crawler.stats.inc_value(\"httperror/response_ignored_count\")\n             spider.crawler.stats.inc_value(\n                 f\"httperror/response_ignored_status_count/{response.status}\"\n@@ -57,3 +73,4 @@ class HttpErrorMiddleware:\n                 extra={\"spider\": spider},\n             )\n             return []\n+        return None\n\n@@ -3,36 +3,51 @@ Offsite Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+from __future__ import annotations\n+\n import logging\n import re\n import warnings\n+from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n \n-from scrapy import signals\n-from scrapy.http import Request\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n+from scrapy.http import Request, Response\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.httpobj import urlparse_cached\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n class OffsiteMiddleware:\n-    def __init__(self, stats):\n-        self.stats = stats\n+    def __init__(self, stats: StatsCollector):\n+        self.stats: StatsCollector = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n-    def process_spider_output(self, response, result, spider):\n-        return (r for r in result or () if self._filter(r, spider))\n+    def process_spider_output(\n+        self, response: Response, result: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n+        return (r for r in result if self._filter(r, spider))\n \n-    async def process_spider_output_async(self, response, result, spider):\n-        async for r in result or ():\n+    async def process_spider_output_async(\n+        self, response: Response, result: AsyncIterable[Any], spider: Spider\n+    ) -> AsyncIterable[Any]:\n+        async for r in result:\n             if self._filter(r, spider):\n                 yield r\n \n-    def _filter(self, request, spider) -> bool:\n+    def _filter(self, request: Any, spider: Spider) -> bool:\n         if not isinstance(request, Request):\n             return True\n         if request.dont_filter or self.should_follow(request, spider):\n@@ -49,13 +64,13 @@ class OffsiteMiddleware:\n         self.stats.inc_value(\"offsite/filtered\", spider=spider)\n         return False\n \n-    def should_follow(self, request, spider):\n+    def should_follow(self, request: Request, spider: Spider) -> bool:\n         regex = self.host_regex\n         # hostname can be None for wrong urls (like javascript links)\n         host = urlparse_cached(request).hostname or \"\"\n         return bool(regex.search(host))\n \n-    def get_host_regex(self, spider):\n+    def get_host_regex(self, spider: Spider) -> re.Pattern[str]:\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n         allowed_domains = getattr(spider, \"allowed_domains\", None)\n         if not allowed_domains:\n@@ -83,9 +98,9 @@ class OffsiteMiddleware:\n         regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n         return re.compile(regex)\n \n-    def spider_opened(self, spider):\n-        self.host_regex = self.get_host_regex(spider)\n-        self.domains_seen = set()\n+    def spider_opened(self, spider: Spider) -> None:\n+        self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n+        self.domains_seen: Set[str] = set()\n \n \n class URLWarning(Warning):\n\n@@ -2,20 +2,39 @@\n RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n+from __future__ import annotations\n+\n import warnings\n-from typing import Tuple\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AsyncIterable,\n+    Dict,\n+    Iterable,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n \n-from scrapy import signals\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n+from scrapy.settings import BaseSettings\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n-LOCAL_SCHEMES = (\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+LOCAL_SCHEMES: Tuple[str, ...] = (\n     \"about\",\n     \"blob\",\n     \"data\",\n@@ -37,18 +56,20 @@ class ReferrerPolicy:\n     NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n     name: str\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         raise NotImplementedError()\n \n-    def stripped_referrer(self, url):\n+    def stripped_referrer(self, url: str) -> Optional[str]:\n         if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n             return self.strip_url(url)\n+        return None\n \n-    def origin_referrer(self, url):\n+    def origin_referrer(self, url: str) -> Optional[str]:\n         if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n             return self.origin(url)\n+        return None\n \n-    def strip_url(self, url, origin_only=False):\n+    def strip_url(self, url: str, origin_only: bool = False) -> Optional[str]:\n         \"\"\"\n         https://www.w3.org/TR/referrer-policy/#strip-url\n \n@@ -72,18 +93,18 @@ class ReferrerPolicy:\n             origin_only=origin_only,\n         )\n \n-    def origin(self, url):\n+    def origin(self, url: str) -> Optional[str]:\n         \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n         return self.strip_url(url, origin_only=True)\n \n-    def potentially_trustworthy(self, url):\n+    def potentially_trustworthy(self, url: str) -> bool:\n         # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n         parsed_url = urlparse(url)\n         if parsed_url.scheme in (\"data\",):\n             return False\n         return self.tls_protected(url)\n \n-    def tls_protected(self, url):\n+    def tls_protected(self, url: str) -> bool:\n         return urlparse(url).scheme in (\"https\", \"ftps\")\n \n \n@@ -98,7 +119,7 @@ class NoReferrerPolicy(ReferrerPolicy):\n \n     name: str = POLICY_NO_REFERRER\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         return None\n \n \n@@ -119,9 +140,10 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n \n     name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         if not self.tls_protected(response_url) or self.tls_protected(request_url):\n             return self.stripped_referrer(response_url)\n+        return None\n \n \n class SameOriginPolicy(ReferrerPolicy):\n@@ -137,9 +159,10 @@ class SameOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_SAME_ORIGIN\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         if self.origin(response_url) == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n+        return None\n \n \n class OriginPolicy(ReferrerPolicy):\n@@ -154,7 +177,7 @@ class OriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_ORIGIN\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         return self.origin_referrer(response_url)\n \n \n@@ -174,13 +197,14 @@ class StrictOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_STRICT_ORIGIN\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         if (\n             self.tls_protected(response_url)\n             and self.potentially_trustworthy(request_url)\n             or not self.tls_protected(response_url)\n         ):\n             return self.origin_referrer(response_url)\n+        return None\n \n \n class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n@@ -197,7 +221,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n@@ -224,7 +248,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n \n     name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n@@ -234,6 +258,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n             or not self.tls_protected(response_url)\n         ):\n             return self.origin_referrer(response_url)\n+        return None\n \n \n class UnsafeUrlPolicy(ReferrerPolicy):\n@@ -252,7 +277,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n \n     name: str = POLICY_UNSAFE_URL\n \n-    def referrer(self, response_url, request_url):\n+    def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n         return self.stripped_referrer(response_url)\n \n \n@@ -267,7 +292,7 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     name: str = POLICY_SCRAPY_DEFAULT\n \n \n-_policy_classes = {\n+_policy_classes: Dict[str, Type[ReferrerPolicy]] = {\n     p.name: p\n     for p in (\n         NoReferrerPolicy,\n@@ -286,14 +311,16 @@ _policy_classes = {\n _policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n \n \n-def _load_policy_class(policy, warning_only=False):\n+def _load_policy_class(\n+    policy: str, warning_only: bool = False\n+) -> Optional[Type[ReferrerPolicy]]:\n     \"\"\"\n     Expect a string for the path to the policy class,\n     otherwise try to interpret the string as a standard value\n     from https://www.w3.org/TR/referrer-policy/#referrer-policies\n     \"\"\"\n     try:\n-        return load_object(policy)\n+        return cast(Type[ReferrerPolicy], load_object(policy))\n     except ValueError:\n         try:\n             return _policy_classes[policy.lower()]\n@@ -307,13 +334,15 @@ def _load_policy_class(policy, warning_only=False):\n \n \n class RefererMiddleware:\n-    def __init__(self, settings=None):\n-        self.default_policy = DefaultReferrerPolicy\n+    def __init__(self, settings: Optional[BaseSettings] = None):\n+        self.default_policy: Type[ReferrerPolicy] = DefaultReferrerPolicy\n         if settings is not None:\n-            self.default_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n+            settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n+            assert settings_policy\n+            self.default_policy = settings_policy\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"REFERER_ENABLED\"):\n             raise NotConfigured\n         mw = cls(crawler.settings)\n@@ -323,7 +352,9 @@ class RefererMiddleware:\n \n         return mw\n \n-    def policy(self, resp_or_url, request):\n+    def policy(\n+        self, resp_or_url: Union[Response, str], request: Request\n+    ) -> ReferrerPolicy:\n         \"\"\"\n         Determine Referrer-Policy to use from a parent Response (or URL),\n         and a Request to be sent.\n@@ -348,21 +379,25 @@ class RefererMiddleware:\n         cls = _load_policy_class(policy_name, warning_only=True)\n         return cls() if cls else self.default_policy()\n \n-    def process_spider_output(self, response, result, spider):\n-        return (self._set_referer(r, response) for r in result or ())\n+    def process_spider_output(\n+        self, response: Response, result: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n+        return (self._set_referer(r, response) for r in result)\n \n-    async def process_spider_output_async(self, response, result, spider):\n-        async for r in result or ():\n+    async def process_spider_output_async(\n+        self, response: Response, result: AsyncIterable[Any], spider: Spider\n+    ) -> AsyncIterable[Any]:\n+        async for r in result:\n             yield self._set_referer(r, response)\n \n-    def _set_referer(self, r, response):\n+    def _set_referer(self, r: Any, response: Response) -> Any:\n         if isinstance(r, Request):\n             referrer = self.policy(response, r).referrer(response.url, r.url)\n             if referrer is not None:\n                 r.headers.setdefault(\"Referer\", referrer)\n         return r\n \n-    def request_scheduled(self, request, spider):\n+    def request_scheduled(self, request: Request, spider: Spider) -> None:\n         # check redirected request to patch \"Referer\" header if necessary\n         redirected_urls = request.meta.get(\"redirect_urls\", [])\n         if redirected_urls:\n@@ -378,7 +413,7 @@ class RefererMiddleware:\n                 policy_referrer = self.policy(parent_url, request).referrer(\n                     parent_url, request.url\n                 )\n-                if policy_referrer != request_referrer:\n+                if policy_referrer != request_referrer.decode(\"latin1\"):\n                     if policy_referrer is None:\n                         request.headers.pop(\"Referer\")\n                     else:\n\n@@ -4,40 +4,54 @@ Url Length Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-import logging\n+from __future__ import annotations\n \n+import logging\n+from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n+\n+from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n+from scrapy.settings import BaseSettings\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n \n class UrlLengthMiddleware:\n-    def __init__(self, maxlength):\n-        self.maxlength = maxlength\n+    def __init__(self, maxlength: int):\n+        self.maxlength: int = maxlength\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\n         return cls(maxlength)\n \n-    def process_spider_output(self, response, result, spider):\n-        return (r for r in result or () if self._filter(r, spider))\n+    def process_spider_output(\n+        self, response: Response, result: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n+        return (r for r in result if self._filter(r, spider))\n \n-    async def process_spider_output_async(self, response, result, spider):\n-        async for r in result or ():\n+    async def process_spider_output_async(\n+        self, response: Response, result: AsyncIterable[Any], spider: Spider\n+    ) -> AsyncIterable[Any]:\n+        async for r in result:\n             if self._filter(r, spider):\n                 yield r\n \n-    def _filter(self, request, spider):\n+    def _filter(self, request: Any, spider: Spider) -> bool:\n         if isinstance(request, Request) and len(request.url) > self.maxlength:\n             logger.info(\n                 \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                 {\"maxlength\": self.maxlength, \"url\": request.url},\n                 extra={\"spider\": spider},\n             )\n+            assert spider.crawler.stats\n             spider.crawler.stats.inc_value(\n                 \"urllength/request_ignored_count\", spider=spider\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dda6feb935b77bf4c7e02b5b00ec6fc0fa6f97ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1667 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -235,20 +235,20 @@ class Base:\n             url2 = \"http://evenmorestuff.com/uglystuff/index\"\n \n             lx = self.extractor_cls(allow=(r\"stuff1\",))\n-            self.assertEqual(lx.matches(url1), True)\n-            self.assertEqual(lx.matches(url2), False)\n+            self.assertTrue(lx.matches(url1))\n+            self.assertFalse(lx.matches(url2))\n \n             lx = self.extractor_cls(deny=(r\"uglystuff\",))\n-            self.assertEqual(lx.matches(url1), True)\n-            self.assertEqual(lx.matches(url2), False)\n+            self.assertTrue(lx.matches(url1))\n+            self.assertFalse(lx.matches(url2))\n \n             lx = self.extractor_cls(allow_domains=(\"evenmorestuff.com\",))\n-            self.assertEqual(lx.matches(url1), False)\n-            self.assertEqual(lx.matches(url2), True)\n+            self.assertFalse(lx.matches(url1))\n+            self.assertTrue(lx.matches(url2))\n \n             lx = self.extractor_cls(deny_domains=(\"lotsofstuff.com\",))\n-            self.assertEqual(lx.matches(url1), False)\n-            self.assertEqual(lx.matches(url2), True)\n+            self.assertFalse(lx.matches(url1))\n+            self.assertTrue(lx.matches(url2))\n \n             lx = self.extractor_cls(\n                 allow=[\"blah1\"],\n@@ -256,10 +256,10 @@ class Base:\n                 allow_domains=[\"blah1.com\"],\n                 deny_domains=[\"blah2.com\"],\n             )\n-            self.assertEqual(lx.matches(\"http://blah1.com/blah1\"), True)\n-            self.assertEqual(lx.matches(\"http://blah1.com/blah2\"), False)\n-            self.assertEqual(lx.matches(\"http://blah2.com/blah1\"), False)\n-            self.assertEqual(lx.matches(\"http://blah2.com/blah2\"), False)\n+            self.assertTrue(lx.matches(\"http://blah1.com/blah1\"))\n+            self.assertFalse(lx.matches(\"http://blah1.com/blah2\"))\n+            self.assertFalse(lx.matches(\"http://blah2.com/blah1\"))\n+            self.assertFalse(lx.matches(\"http://blah2.com/blah2\"))\n \n         def test_restrict_xpaths(self):\n             lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ac3ef73e6208aced93fa538184f1db7f14d125c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 27 | Files Changed: 4 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 38 | Churn Cumulative: 2079 | Contributors (this commit): 34 | Commits (past 90d): 14 | Contributors (cumulative): 66 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +0,0 @@\n-from typing import Any, Callable, NoReturn\n-\n-\n-def obsolete_setter(setter: Callable, attrname: str) -> Callable[[Any, Any], NoReturn]:\n-    def newsetter(self: Any, value: Any) -> NoReturn:\n-        c = self.__class__.__name__\n-        msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n-        raise AttributeError(msg)\n-\n-    return newsetter\n\n@@ -25,7 +25,6 @@ from typing import (\n from w3lib.url import safe_url_string\n \n import scrapy\n-from scrapy.http.common import obsolete_setter\n from scrapy.http.headers import Headers\n from scrapy.utils.curl import curl_to_request_kwargs\n from scrapy.utils.python import to_bytes\n@@ -142,7 +141,8 @@ class Request(object_ref):\n             self._meta = {}\n         return self._meta\n \n-    def _get_url(self) -> str:\n+    @property\n+    def url(self) -> str:\n         return self._url\n \n     def _set_url(self, url: str) -> None:\n@@ -159,16 +159,13 @@ class Request(object_ref):\n         ):\n             raise ValueError(f\"Missing scheme in request url: {self._url}\")\n \n-    url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n-\n-    def _get_body(self) -> bytes:\n+    @property\n+    def body(self) -> bytes:\n         return self._body\n \n     def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n         self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n \n-    body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n-\n     @property\n     def encoding(self) -> str:\n         return self._encoding\n\n@@ -27,7 +27,6 @@ from urllib.parse import urljoin\n from twisted.internet.ssl import Certificate\n \n from scrapy.exceptions import NotSupported\n-from scrapy.http.common import obsolete_setter\n from scrapy.http.headers import Headers\n from scrapy.http.request import Request\n from scrapy.link import Link\n@@ -102,7 +101,8 @@ class Response(object_ref):\n                 \"is not tied to any request\"\n             )\n \n-    def _get_url(self) -> str:\n+    @property\n+    def url(self) -> str:\n         return self._url\n \n     def _set_url(self, url: str) -> None:\n@@ -113,9 +113,8 @@ class Response(object_ref):\n                 f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n             )\n \n-    url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n-\n-    def _get_body(self) -> bytes:\n+    @property\n+    def body(self) -> bytes:\n         return self._body\n \n     def _set_body(self, body: Optional[bytes]) -> None:\n@@ -130,8 +129,6 @@ class Response(object_ref):\n         else:\n             self._body = body\n \n-    body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n-\n     def __repr__(self) -> str:\n         return f\"<{self.status} {self.url}>\"\n \n@@ -149,7 +146,7 @@ class Response(object_ref):\n     def urljoin(self, url: str) -> str:\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n-        return urljoin(cast(str, self.url), url)\n+        return urljoin(self.url, url)\n \n     @property\n     def text(self) -> str:\n\n@@ -217,10 +217,10 @@ def _body_or_str(\n         )\n     if isinstance(obj, Response):\n         if not unicode:\n-            return cast(bytes, obj.body)\n+            return obj.body\n         if isinstance(obj, TextResponse):\n             return obj.text\n-        return cast(bytes, obj.body).decode(\"utf-8\")\n+        return obj.body.decode(\"utf-8\")\n     if isinstance(obj, str):\n         return obj if unicode else obj.encode(\"utf-8\")\n     return obj.decode(\"utf-8\") if unicode else obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eafe828484d95f7e73a475c1c5d23492616ebc07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 406 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,9 @@\n import os\n import warnings\n from importlib import import_module\n+from os import PathLike\n from pathlib import Path\n+from typing import Union\n \n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n@@ -44,7 +46,7 @@ def project_data_dir(project: str = \"default\") -> str:\n     return str(d)\n \n \n-def data_path(path: str, createdir: bool = False) -> str:\n+def data_path(path: Union[str, PathLike], createdir: bool = False) -> str:\n     \"\"\"\n     Return the given path joined with the .scrapy data directory.\n     If given an absolute path, return it unmodified.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c27c22a987b7bb113d6275bf9646b74afffe552", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 373 | Lines Deleted: 136 | Files Changed: 15 | Hunks: 180 | Methods Changed: 152 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 509 | Churn Cumulative: 3029 | Contributors (this commit): 50 | Commits (past 90d): 22 | Contributors (cumulative): 165 | DMM Complexity: 0.5950920245398773\n\nDIFF:\n@@ -1,10 +1,20 @@\n+from __future__ import annotations\n+\n import logging\n import re\n+from typing import TYPE_CHECKING, Union\n \n from w3lib import html\n \n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import HtmlResponse\n+from scrapy.http import HtmlResponse, Response\n+from scrapy.settings import BaseSettings\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n@@ -15,7 +25,7 @@ class AjaxCrawlMiddleware:\n     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n     \"\"\"\n \n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n             raise NotConfigured\n \n@@ -23,13 +33,15 @@ class AjaxCrawlMiddleware:\n         # middleware parses first 4k. 4k turns out to be insufficient\n         # for this middleware, and parsing 100k could be slow.\n         # We use something in between (32K) by default.\n-        self.lookup_bytes = settings.getint(\"AJAXCRAWL_MAXSIZE\", 32768)\n+        self.lookup_bytes: int = settings.getint(\"AJAXCRAWL_MAXSIZE\", 32768)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if not isinstance(response, HtmlResponse) or response.status != 200:\n             return response\n \n@@ -54,7 +66,7 @@ class AjaxCrawlMiddleware:\n         ajax_crawl_request.meta[\"ajax_crawlable\"] = True\n         return ajax_crawl_request\n \n-    def _has_ajax_crawlable_variant(self, response):\n+    def _has_ajax_crawlable_variant(self, response: Response) -> bool:\n         \"\"\"\n         Return True if a page without hash fragment could be \"AJAX crawlable\"\n         according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n@@ -64,12 +76,12 @@ class AjaxCrawlMiddleware:\n \n \n # XXX: move it to w3lib?\n-_ajax_crawlable_re = re.compile(\n+_ajax_crawlable_re: re.Pattern[str] = re.compile(\n     r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n )\n \n \n-def _has_ajaxcrawlable_meta(text):\n+def _has_ajaxcrawlable_meta(text: str) -> bool:\n     \"\"\"\n     >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n     True\n\n@@ -1,21 +1,41 @@\n+from __future__ import annotations\n+\n import logging\n from collections import defaultdict\n+from http.cookiejar import Cookie\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    DefaultDict,\n+    Dict,\n+    Iterable,\n+    Optional,\n+    Sequence,\n+    Union,\n+)\n \n from tldextract import TLDExtract\n \n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n _split_domain = TLDExtract(include_psl_private_domains=True)\n \n \n-def _is_public_domain(domain):\n+def _is_public_domain(domain: str) -> bool:\n     parts = _split_domain(domain)\n     return not parts.domain\n \n@@ -23,23 +43,27 @@ def _is_public_domain(domain):\n class CookiesMiddleware:\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n-    def __init__(self, debug=False):\n-        self.jars = defaultdict(CookieJar)\n-        self.debug = debug\n+    def __init__(self, debug: bool = False):\n+        self.jars: DefaultDict[Any, CookieJar] = defaultdict(CookieJar)\n+        self.debug: bool = debug\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"COOKIES_ENABLED\"):\n             raise NotConfigured\n         return cls(crawler.settings.getbool(\"COOKIES_DEBUG\"))\n \n-    def _process_cookies(self, cookies, *, jar, request):\n+    def _process_cookies(\n+        self, cookies: Iterable[Cookie], *, jar: CookieJar, request: Request\n+    ) -> None:\n         for cookie in cookies:\n             cookie_domain = cookie.domain\n             if cookie_domain.startswith(\".\"):\n                 cookie_domain = cookie_domain[1:]\n \n-            request_domain = urlparse_cached(request).hostname.lower()\n+            hostname = urlparse_cached(request).hostname\n+            assert hostname is not None\n+            request_domain = hostname.lower()\n \n             if cookie_domain and _is_public_domain(cookie_domain):\n                 if cookie_domain != request_domain:\n@@ -48,9 +72,11 @@ class CookiesMiddleware:\n \n             jar.set_cookie_if_ok(cookie, request)\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         if request.meta.get(\"dont_merge_cookies\", False):\n-            return\n+            return None\n \n         cookiejarkey = request.meta.get(\"cookiejar\")\n         jar = self.jars[cookiejarkey]\n@@ -61,8 +87,11 @@ class CookiesMiddleware:\n         request.headers.pop(\"Cookie\", None)\n         jar.add_cookie_header(request)\n         self._debug_cookie(request, spider)\n+        return None\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if request.meta.get(\"dont_merge_cookies\", False):\n             return response\n \n@@ -76,7 +105,7 @@ class CookiesMiddleware:\n \n         return response\n \n-    def _debug_cookie(self, request, spider):\n+    def _debug_cookie(self, request: Request, spider: Spider) -> None:\n         if self.debug:\n             cl = [\n                 to_unicode(c, errors=\"replace\")\n@@ -87,7 +116,7 @@ class CookiesMiddleware:\n                 msg = f\"Sending cookies to: {request}\\n{cookies}\"\n                 logger.debug(msg, extra={\"spider\": spider})\n \n-    def _debug_set_cookie(self, response, spider):\n+    def _debug_set_cookie(self, response: Response, spider: Spider) -> None:\n         if self.debug:\n             cl = [\n                 to_unicode(c, errors=\"replace\")\n@@ -98,7 +127,7 @@ class CookiesMiddleware:\n                 msg = f\"Received cookies from: {response}\\n{cookies}\"\n                 logger.debug(msg, extra={\"spider\": spider})\n \n-    def _format_cookie(self, cookie, request):\n+    def _format_cookie(self, cookie: Dict[str, Any], request: Request) -> Optional[str]:\n         \"\"\"\n         Given a dict consisting of cookie components, return its string representation.\n         Decode from bytes if necessary.\n@@ -109,7 +138,7 @@ class CookiesMiddleware:\n                 if key in (\"name\", \"value\"):\n                     msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n                     logger.warning(msg)\n-                    return\n+                    return None\n                 continue\n             if isinstance(cookie[key], (bool, float, int, str)):\n                 decoded[key] = str(cookie[key])\n@@ -129,12 +158,15 @@ class CookiesMiddleware:\n             cookie_str += f\"; {key.capitalize()}={value}\"\n         return cookie_str\n \n-    def _get_request_cookies(self, jar, request):\n+    def _get_request_cookies(\n+        self, jar: CookieJar, request: Request\n+    ) -> Sequence[Cookie]:\n         \"\"\"\n         Extract cookies from the Request.cookies attribute\n         \"\"\"\n         if not request.cookies:\n             return []\n+        cookies: Iterable[Dict[str, Any]]\n         if isinstance(request.cookies, dict):\n             cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n         else:\n\n@@ -3,19 +3,32 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+from __future__ import annotations\n \n+from typing import TYPE_CHECKING, Iterable, Tuple, Union\n+\n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n+from scrapy.http import Response\n from scrapy.utils.python import without_none_values\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class DefaultHeadersMiddleware:\n-    def __init__(self, headers):\n-        self._headers = headers\n+    def __init__(self, headers: Iterable[Tuple[str, str]]):\n+        self._headers: Iterable[Tuple[str, str]] = headers\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         headers = without_none_values(crawler.settings[\"DEFAULT_REQUEST_HEADERS\"])\n         return cls(headers.items())\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         for k, v in self._headers:\n             request.headers.setdefault(k, v)\n+        return None\n\n@@ -3,23 +3,35 @@ Download timeout middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+from __future__ import annotations\n \n-from scrapy import signals\n+from typing import TYPE_CHECKING, Union\n+\n+from scrapy import Request, Spider, signals\n+from scrapy.crawler import Crawler\n+from scrapy.http import Response\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class DownloadTimeoutMiddleware:\n-    def __init__(self, timeout=180):\n-        self._timeout = timeout\n+    def __init__(self, timeout: float = 180):\n+        self._timeout: float = timeout\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         o = cls(crawler.settings.getfloat(\"DOWNLOAD_TIMEOUT\"))\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         self._timeout = getattr(spider, \"download_timeout\", self._timeout)\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         if self._timeout:\n             request.meta.setdefault(\"download_timeout\", self._timeout)\n+        return None\n\n@@ -4,31 +4,44 @@ HTTP basic auth downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Union\n+\n from w3lib.http import basic_auth_header\n \n-from scrapy import signals\n+from scrapy import Request, Spider, signals\n+from scrapy.crawler import Crawler\n+from scrapy.http import Response\n from scrapy.utils.url import url_is_from_any_domain\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class HttpAuthMiddleware:\n     \"\"\"Set Basic HTTP Authorization header\n     (http_user and http_pass spider class attributes)\"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         o = cls()\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         usr = getattr(spider, \"http_user\", \"\")\n         pwd = getattr(spider, \"http_pass\", \"\")\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n-            self.domain = spider.http_auth_domain\n+            self.domain = spider.http_auth_domain  # type: ignore[attr-defined]\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         auth = getattr(self, \"auth\", None)\n         if auth and b\"Authorization\" not in request.headers:\n             if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                 request.headers[b\"Authorization\"] = auth\n+        return None\n\n@@ -1,5 +1,7 @@\n+from __future__ import annotations\n+\n from email.utils import formatdate\n-from typing import Optional, Type, TypeVar\n+from typing import TYPE_CHECKING, Optional, Union\n \n from twisted.internet import defer\n from twisted.internet.error import (\n@@ -23,7 +25,9 @@ from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.misc import load_object\n \n-HttpCacheMiddlewareTV = TypeVar(\"HttpCacheMiddlewareTV\", bound=\"HttpCacheMiddleware\")\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class HttpCacheMiddleware:\n@@ -49,9 +53,7 @@ class HttpCacheMiddleware:\n         self.stats = stats\n \n     @classmethod\n-    def from_crawler(\n-        cls: Type[HttpCacheMiddlewareTV], crawler: Crawler\n-    ) -> HttpCacheMiddlewareTV:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.settings, crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n@@ -64,7 +66,9 @@ class HttpCacheMiddleware:\n     def spider_closed(self, spider: Spider) -> None:\n         self.storage.close_spider(spider)\n \n-    def process_request(self, request: Request, spider: Spider) -> Optional[Response]:\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         if request.meta.get(\"dont_cache\", False):\n             return None\n \n@@ -74,7 +78,9 @@ class HttpCacheMiddleware:\n             return None\n \n         # Look for cached response and check if expired\n-        cachedresponse = self.storage.retrieve_response(spider, request)\n+        cachedresponse: Optional[Response] = self.storage.retrieve_response(\n+            spider, request\n+        )\n         if cachedresponse is None:\n             self.stats.inc_value(\"httpcache/miss\", spider=spider)\n             if self.ignore_missing:\n@@ -96,7 +102,7 @@ class HttpCacheMiddleware:\n \n     def process_response(\n         self, request: Request, response: Response, spider: Spider\n-    ) -> Response:\n+    ) -> Union[Request, Response]:\n         if request.meta.get(\"dont_cache\", False):\n             return response\n \n@@ -111,7 +117,7 @@ class HttpCacheMiddleware:\n             response.headers[\"Date\"] = formatdate(usegmt=True)\n \n         # Do not validate first-hand responses\n-        cachedresponse = request.meta.pop(\"cached_response\", None)\n+        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n         if cachedresponse is None:\n             self.stats.inc_value(\"httpcache/firsthand\", spider=spider)\n             self._cache_response(spider, response, request, cachedresponse)\n@@ -127,8 +133,8 @@ class HttpCacheMiddleware:\n \n     def process_exception(\n         self, request: Request, exception: Exception, spider: Spider\n-    ) -> Optional[Response]:\n-        cachedresponse = request.meta.pop(\"cached_response\", None)\n+    ) -> Union[Request, Response, None]:\n+        cachedresponse: Optional[Response] = request.meta.pop(\"cached_response\", None)\n         if cachedresponse is not None and isinstance(\n             exception, self.DOWNLOAD_EXCEPTIONS\n         ):\n\n@@ -1,12 +1,22 @@\n+from __future__ import annotations\n+\n import io\n import zlib\n+from typing import TYPE_CHECKING, List, Optional, Union\n \n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.gz import gunzip\n \n-ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n \n try:\n     import brotli\n@@ -27,19 +37,24 @@ class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n-    def __init__(self, stats=None):\n+    def __init__(self, stats: Optional[StatsCollector] = None):\n         self.stats = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n         return cls(stats=crawler.stats)\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n+        return None\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if request.method == \"HEAD\":\n             return response\n         if isinstance(response, Response):\n@@ -70,7 +85,7 @@ class HttpCompressionMiddleware:\n \n         return response\n \n-    def _decode(self, body, encoding):\n+    def _decode(self, body: bytes, encoding: bytes) -> bytes:\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n             body = gunzip(body)\n \n\n@@ -1,16 +1,30 @@\n-import base64\n-from urllib.parse import unquote, urlunparse\n-from urllib.request import _parse_proxy, getproxies, proxy_bypass\n+from __future__ import annotations\n \n+import base64\n+from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n+from urllib.parse import unquote, urlunparse\n+from urllib.request import (  # type: ignore[attr-defined]\n+    _parse_proxy,\n+    getproxies,\n+    proxy_bypass,\n+)\n+\n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class HttpProxyMiddleware:\n-    def __init__(self, auth_encoding=\"latin-1\"):\n-        self.auth_encoding = auth_encoding\n-        self.proxies = {}\n+    def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n+        self.auth_encoding: Optional[str] = auth_encoding\n+        self.proxies: Dict[str, Tuple[Optional[bytes], str]] = {}\n         for type_, url in getproxies().items():\n             try:\n                 self.proxies[type_] = self._get_proxy(url, type_)\n@@ -20,19 +34,19 @@ class HttpProxyMiddleware:\n                 continue\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"HTTPPROXY_ENABLED\"):\n             raise NotConfigured\n-        auth_encoding = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n+        auth_encoding: Optional[str] = crawler.settings.get(\"HTTPPROXY_AUTH_ENCODING\")\n         return cls(auth_encoding)\n \n-    def _basic_auth_header(self, username, password):\n+    def _basic_auth_header(self, username: str, password: str) -> bytes:\n         user_pass = to_bytes(\n             f\"{unquote(username)}:{unquote(password)}\", encoding=self.auth_encoding\n         )\n         return base64.b64encode(user_pass)\n \n-    def _get_proxy(self, url, orig_type):\n+    def _get_proxy(self, url: str, orig_type: str) -> Tuple[Optional[bytes], str]:\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n \n@@ -43,7 +57,9 @@ class HttpProxyMiddleware:\n \n         return creds, proxy_url\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         creds, proxy_url = None, None\n         if \"proxy\" in request.meta:\n             if request.meta[\"proxy\"] is not None:\n@@ -54,13 +70,16 @@ class HttpProxyMiddleware:\n             if (\n                 # 'no_proxy' is only supported by http schemes\n                 scheme not in (\"http\", \"https\")\n-                or not proxy_bypass(parsed.hostname)\n+                or (parsed.hostname and not proxy_bypass(parsed.hostname))\n             ) and scheme in self.proxies:\n                 creds, proxy_url = self.proxies[scheme]\n \n         self._set_proxy_and_creds(request, proxy_url, creds)\n+        return None\n \n-    def _set_proxy_and_creds(self, request, proxy_url, creds):\n+    def _set_proxy_and_creds(\n+        self, request: Request, proxy_url: Optional[str], creds: Optional[bytes]\n+    ) -> None:\n         if proxy_url:\n             request.meta[\"proxy\"] = proxy_url\n         elif request.meta.get(\"proxy\") is not None:\n\n@@ -1,17 +1,29 @@\n+from __future__ import annotations\n+\n import logging\n+from typing import TYPE_CHECKING, Any, List, Union, cast\n from urllib.parse import urljoin, urlparse\n \n from w3lib.url import safe_url_string\n \n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n-from scrapy.http import HtmlResponse\n+from scrapy.http import HtmlResponse, Response\n+from scrapy.settings import BaseSettings\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n-def _build_redirect_request(source_request, *, url, **kwargs):\n+def _build_redirect_request(\n+    source_request: Request, *, url: str, **kwargs: Any\n+) -> Request:\n     redirect_request = source_request.replace(\n         url=url,\n         **kwargs,\n@@ -26,20 +38,22 @@ def _build_redirect_request(source_request, *, url, **kwargs):\n \n \n class BaseRedirectMiddleware:\n-    enabled_setting = \"REDIRECT_ENABLED\"\n+    enabled_setting: str = \"REDIRECT_ENABLED\"\n \n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         if not settings.getbool(self.enabled_setting):\n             raise NotConfigured\n \n-        self.max_redirect_times = settings.getint(\"REDIRECT_MAX_TIMES\")\n-        self.priority_adjust = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n+        self.max_redirect_times: int = settings.getint(\"REDIRECT_MAX_TIMES\")\n+        self.priority_adjust: int = settings.getint(\"REDIRECT_PRIORITY_ADJUST\")\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def _redirect(self, redirected, request, spider, reason):\n+    def _redirect(\n+        self, redirected: Request, request: Request, spider: Spider, reason: Any\n+    ) -> Request:\n         ttl = request.meta.setdefault(\"redirect_ttl\", self.max_redirect_times)\n         redirects = request.meta.get(\"redirect_times\", 0) + 1\n \n@@ -67,7 +81,9 @@ class BaseRedirectMiddleware:\n         )\n         raise IgnoreRequest(\"max redirections reached\")\n \n-    def _redirect_request_using_get(self, request, redirect_url):\n+    def _redirect_request_using_get(\n+        self, request: Request, redirect_url: str\n+    ) -> Request:\n         redirect_request = _build_redirect_request(\n             request,\n             url=redirect_url,\n@@ -85,7 +101,9 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     and meta-refresh html tag.\n     \"\"\"\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if (\n             request.meta.get(\"dont_redirect\", False)\n             or response.status in getattr(spider, \"handle_httpstatus_list\", [])\n@@ -98,6 +116,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n         if \"Location\" not in response.headers or response.status not in allowed_status:\n             return response\n \n+        assert response.headers[\"Location\"] is not None\n         location = safe_url_string(response.headers[\"Location\"])\n         if response.headers[\"Location\"].startswith(b\"//\"):\n             request_scheme = urlparse(request.url).scheme\n@@ -116,12 +135,14 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n     enabled_setting = \"METAREFRESH_ENABLED\"\n \n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         super().__init__(settings)\n-        self._ignore_tags = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n-        self._maxdelay = settings.getint(\"METAREFRESH_MAXDELAY\")\n+        self._ignore_tags: List[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n+        self._maxdelay: int = settings.getint(\"METAREFRESH_MAXDELAY\")\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if (\n             request.meta.get(\"dont_redirect\", False)\n             or request.method == \"HEAD\"\n@@ -130,7 +151,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n             return response\n \n         interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n-        if url and interval < self._maxdelay:\n+        if url and cast(float, interval) < self._maxdelay:\n             redirected = self._redirect_request_using_get(request, url)\n             return self._redirect(redirected, request, spider, \"meta refresh\")\n \n\n@@ -9,22 +9,30 @@ RETRY_HTTP_CODES - which HTTP response codes to retry\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n+from __future__ import annotations\n+\n import warnings\n from logging import Logger, getLogger\n-from typing import Optional, Type, Union\n+from typing import TYPE_CHECKING, Any, Optional, Tuple, Type, Union\n \n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.http import Response\n from scrapy.http.request import Request\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.spiders import Spider\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n retry_logger = getLogger(__name__)\n \n \n-def backwards_compatibility_getattr(self, name):\n+def backwards_compatibility_getattr(self: Any, name: str) -> Tuple[Any, ...]:\n     if name == \"EXCEPTIONS_TO_RETRY\":\n         warnings.warn(\n             \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n@@ -54,7 +62,7 @@ def get_retry_request(\n     priority_adjust: Optional[int] = None,\n     logger: Logger = retry_logger,\n     stats_base_key: str = \"retry\",\n-):\n+) -> Optional[Request]:\n     \"\"\"\n     Returns a new :class:`~scrapy.Request` object to retry the specified\n     request, or ``None`` if retries of the specified request have been\n@@ -134,7 +142,7 @@ def get_retry_request(\n \n \n class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n         self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n@@ -153,10 +161,12 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n             )\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler.settings)\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         if request.meta.get(\"dont_retry\", False):\n             return response\n         if response.status in self.retry_http_codes:\n@@ -164,13 +174,21 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n             return self._retry(request, reason, spider) or response\n         return response\n \n-    def process_exception(self, request, exception, spider):\n+    def process_exception(\n+        self, request: Request, exception: Exception, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n             \"dont_retry\", False\n         ):\n             return self._retry(request, exception, spider)\n+        return None\n \n-    def _retry(self, request, reason, spider):\n+    def _retry(\n+        self,\n+        request: Request,\n+        reason: Union[str, Exception, Type[Exception]],\n+        spider: Spider,\n+    ) -> Optional[Request]:\n         max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n         priority_adjust = request.meta.get(\"priority_adjust\", self.priority_adjust)\n         return get_retry_request(\n\n@@ -4,65 +4,87 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n+from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n \n from twisted.internet.defer import Deferred, maybeDeferred\n+from twisted.python.failure import Failure\n \n+from scrapy import Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n+from scrapy.robotstxt import RobotParser\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import load_object\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n class RobotsTxtMiddleware:\n-    DOWNLOAD_PRIORITY = 1000\n+    DOWNLOAD_PRIORITY: int = 1000\n \n-    def __init__(self, crawler):\n+    def __init__(self, crawler: Crawler):\n         if not crawler.settings.getbool(\"ROBOTSTXT_OBEY\"):\n             raise NotConfigured\n-        self._default_useragent = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n-        self._robotstxt_useragent = crawler.settings.get(\"ROBOTSTXT_USER_AGENT\", None)\n-        self.crawler = crawler\n-        self._parsers = {}\n-        self._parserimpl = load_object(crawler.settings.get(\"ROBOTSTXT_PARSER\"))\n+        self._default_useragent: str = crawler.settings.get(\"USER_AGENT\", \"Scrapy\")\n+        self._robotstxt_useragent: Optional[str] = crawler.settings.get(\n+            \"ROBOTSTXT_USER_AGENT\", None\n+        )\n+        self.crawler: Crawler = crawler\n+        self._parsers: Dict[str, Union[RobotParser, Deferred, None]] = {}\n+        self._parserimpl: RobotParser = load_object(\n+            crawler.settings.get(\"ROBOTSTXT_PARSER\")\n+        )\n \n         # check if parser dependencies are met, this should throw an error otherwise.\n         self._parserimpl.from_crawler(self.crawler, b\"\")\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def process_request(self, request, spider):\n+    def process_request(self, request: Request, spider: Spider) -> Optional[Deferred]:\n         if request.meta.get(\"dont_obey_robotstxt\"):\n-            return\n+            return None\n         if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n-            return\n-        d = maybeDeferred(self.robot_parser, request, spider)\n+            return None\n+        d: Deferred = maybeDeferred(self.robot_parser, request, spider)\n         d.addCallback(self.process_request_2, request, spider)\n         return d\n \n-    def process_request_2(self, rp, request, spider):\n+    def process_request_2(\n+        self, rp: Optional[RobotParser], request: Request, spider: Spider\n+    ) -> None:\n         if rp is None:\n             return\n \n-        useragent = self._robotstxt_useragent\n+        useragent: Union[str, bytes, None] = self._robotstxt_useragent\n         if not useragent:\n             useragent = request.headers.get(b\"User-Agent\", self._default_useragent)\n+            assert useragent is not None\n         if not rp.allowed(request.url, useragent):\n             logger.debug(\n                 \"Forbidden by robots.txt: %(request)s\",\n                 {\"request\": request},\n                 extra={\"spider\": spider},\n             )\n+            assert self.crawler.stats\n             self.crawler.stats.inc_value(\"robotstxt/forbidden\")\n             raise IgnoreRequest(\"Forbidden by robots.txt\")\n \n-    def robot_parser(self, request, spider):\n+    def robot_parser(\n+        self, request: Request, spider: Spider\n+    ) -> Union[RobotParser, Deferred, None]:\n         url = urlparse_cached(request)\n         netloc = url.netloc\n \n@@ -75,24 +97,27 @@ class RobotsTxtMiddleware:\n                 meta={\"dont_obey_robotstxt\": True},\n                 callback=NO_CALLBACK,\n             )\n+            assert self.crawler.engine\n+            assert self.crawler.stats\n             dfd = self.crawler.engine.download(robotsreq)\n             dfd.addCallback(self._parse_robots, netloc, spider)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n             self.crawler.stats.inc_value(\"robotstxt/request_count\")\n \n-        if isinstance(self._parsers[netloc], Deferred):\n-            d = Deferred()\n+        parser = self._parsers[netloc]\n+        if isinstance(parser, Deferred):\n+            d: Deferred = Deferred()\n \n-            def cb(result):\n+            def cb(result: Any) -> Any:\n                 d.callback(result)\n                 return result\n \n-            self._parsers[netloc].addCallback(cb)\n+            parser.addCallback(cb)\n             return d\n-        return self._parsers[netloc]\n+        return parser\n \n-    def _logerror(self, failure, request, spider):\n+    def _logerror(self, failure: Failure, request: Request, spider: Spider) -> Failure:\n         if failure.type is not IgnoreRequest:\n             logger.error(\n                 \"Error downloading %(request)s: %(f_exception)s\",\n@@ -102,20 +127,24 @@ class RobotsTxtMiddleware:\n             )\n         return failure\n \n-    def _parse_robots(self, response, netloc, spider):\n+    def _parse_robots(self, response: Response, netloc: str, spider: Spider) -> None:\n+        assert self.crawler.stats\n         self.crawler.stats.inc_value(\"robotstxt/response_count\")\n         self.crawler.stats.inc_value(\n             f\"robotstxt/response_status_count/{response.status}\"\n         )\n         rp = self._parserimpl.from_crawler(self.crawler, response.body)\n         rp_dfd = self._parsers[netloc]\n+        assert isinstance(rp_dfd, Deferred)\n         self._parsers[netloc] = rp\n         rp_dfd.callback(rp)\n \n-    def _robots_error(self, failure, netloc):\n+    def _robots_error(self, failure: Failure, netloc: str) -> None:\n         if failure.type is not IgnoreRequest:\n             key = f\"robotstxt/exception_count/{failure.type}\"\n+            assert self.crawler.stats\n             self.crawler.stats.inc_value(key)\n         rp_dfd = self._parsers[netloc]\n+        assert isinstance(rp_dfd, Deferred)\n         self._parsers[netloc] = None\n         rp_dfd.callback(None)\n\n@@ -1,11 +1,23 @@\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Dict, Union\n+\n from twisted.web import http\n \n+from scrapy import Request, Spider\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.python import global_object_name, to_bytes\n from scrapy.utils.request import request_httprepr\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n-def get_header_size(headers):\n+\n+def get_header_size(headers: Dict[str, Union[list, tuple]]) -> int:\n     size = 0\n     for key, value in headers.items():\n         if isinstance(value, (list, tuple)):\n@@ -14,30 +26,36 @@ def get_header_size(headers):\n     return size + len(b\"\\r\\n\") * (len(headers.keys()) - 1)\n \n \n-def get_status_size(response_status):\n+def get_status_size(response_status: int) -> int:\n     return len(to_bytes(http.RESPONSES.get(response_status, b\"\"))) + 15\n     # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n \n \n class DownloaderStats:\n-    def __init__(self, stats):\n-        self.stats = stats\n+    def __init__(self, stats: StatsCollector):\n+        self.stats: StatsCollector = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"DOWNLOADER_STATS\"):\n             raise NotConfigured\n+        assert crawler.stats\n         return cls(crawler.stats)\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         self.stats.inc_value(\"downloader/request_count\", spider=spider)\n         self.stats.inc_value(\n             f\"downloader/request_method_count/{request.method}\", spider=spider\n         )\n         reqlen = len(request_httprepr(request))\n         self.stats.inc_value(\"downloader/request_bytes\", reqlen, spider=spider)\n+        return None\n \n-    def process_response(self, request, response, spider):\n+    def process_response(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> Union[Request, Response]:\n         self.stats.inc_value(\"downloader/response_count\", spider=spider)\n         self.stats.inc_value(\n             f\"downloader/response_status_count/{response.status}\", spider=spider\n@@ -52,9 +70,12 @@ class DownloaderStats:\n         self.stats.inc_value(\"downloader/response_bytes\", reslen, spider=spider)\n         return response\n \n-    def process_exception(self, request, exception, spider):\n+    def process_exception(\n+        self, request: Request, exception: Exception, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         ex_class = global_object_name(exception.__class__)\n         self.stats.inc_value(\"downloader/exception_count\", spider=spider)\n         self.stats.inc_value(\n             f\"downloader/exception_type_count/{ex_class}\", spider=spider\n         )\n+        return None\n\n@@ -1,23 +1,36 @@\n \"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n \n-from scrapy import signals\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Union\n+\n+from scrapy import Request, Spider, signals\n+from scrapy.crawler import Crawler\n+from scrapy.http import Response\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class UserAgentMiddleware:\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    def __init__(self, user_agent=\"Scrapy\"):\n+    def __init__(self, user_agent: str = \"Scrapy\"):\n         self.user_agent = user_agent\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         o = cls(crawler.settings[\"USER_AGENT\"])\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         self.user_agent = getattr(spider, \"user_agent\", self.user_agent)\n \n-    def process_request(self, request, spider):\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Union[Request, Response, None]:\n         if self.user_agent:\n             request.headers.setdefault(b\"User-Agent\", self.user_agent)\n+        return None\n\n@@ -1,8 +1,12 @@\n import re\n import time\n+from http.cookiejar import Cookie\n from http.cookiejar import CookieJar as _CookieJar\n from http.cookiejar import DefaultCookiePolicy\n+from typing import Sequence\n \n+from scrapy import Request\n+from scrapy.http import Response\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n@@ -24,7 +28,7 @@ class CookieJar:\n         wrsp = WrappedResponse(response)\n         return self.jar.extract_cookies(wrsp, wreq)\n \n-    def add_cookie_header(self, request):\n+    def add_cookie_header(self, request: Request) -> None:\n         wreq = WrappedRequest(request)\n         self.policy._now = self.jar._now = int(time.time())\n \n@@ -75,7 +79,7 @@ class CookieJar:\n     def set_policy(self, pol):\n         return self.jar.set_policy(pol)\n \n-    def make_cookies(self, response, request):\n+    def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n         wreq = WrappedRequest(request)\n         wrsp = WrappedResponse(response)\n         return self.jar.make_cookies(wrsp, wreq)\n@@ -83,7 +87,7 @@ class CookieJar:\n     def set_cookie(self, cookie):\n         self.jar.set_cookie(cookie)\n \n-    def set_cookie_if_ok(self, cookie, request):\n+    def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n         self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))\n \n \n\n@@ -1,11 +1,20 @@\n+from __future__ import annotations\n+\n import logging\n import sys\n from abc import ABCMeta, abstractmethod\n+from typing import TYPE_CHECKING, Union\n from warnings import warn\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -31,7 +40,7 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n class RobotParser(metaclass=ABCMeta):\n     @classmethod\n     @abstractmethod\n-    def from_crawler(cls, crawler, robotstxt_body):\n+    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n         It must return a new instance of the parser backend.\n \n@@ -44,14 +53,14 @@ class RobotParser(metaclass=ABCMeta):\n         pass\n \n     @abstractmethod\n-    def allowed(self, url, user_agent):\n+    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n         \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n \n         :param url: Absolute URL\n-        :type url: str\n+        :type url: str or bytes\n \n         :param user_agent: User agent\n-        :type user_agent: str\n+        :type user_agent: str or bytes\n         \"\"\"\n         pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
