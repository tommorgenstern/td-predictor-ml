{"custom_id": "keras#2bd4c295d6cae124f2cffce0e7343dc2c18d7eee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 130 | Lines Deleted: 129 | Files Changed: 17 | Hunks: 94 | Methods Changed: 21 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 259 | Churn Cumulative: 5915 | Contributors (this commit): 41 | Commits (past 90d): 124 | Contributors (cumulative): 91 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,8 @@\n # -*- coding: utf-8 -*-\n from __future__ import print_function\n from keras.models import Sequential, slice_X\n-from keras.layers.core import Activation, Dense, RepeatVector\n+from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n from keras.layers import recurrent\n-from sklearn.utils import shuffle\n import numpy as np\n \n \"\"\"\n@@ -25,18 +24,15 @@ and\n http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n Theoretically it introduces shorter term dependencies between source and target.\n \n-\n Two digits inverted:\n + One layer JZS1 (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n \n Three digits inverted:\n + One layer JZS1 (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n \n-\n Four digits inverted:\n + One layer JZS1 (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n \n-\n Five digits inverted:\n + One layer JZS1 (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n \n@@ -122,23 +118,32 @@ for i, sentence in enumerate(expected):\n     y[i] = ctable.encode(sentence, maxlen=DIGITS + 1)\n \n # Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n-X, y = shuffle(X, y)\n+indices = np.arange(len(y))\n+np.random.shuffle(indices)\n+X = X[indices]\n+y = y[indices]\n # Explicitly set apart 10% for validation data that we never train over\n split_at = len(X) - len(X) / 10\n (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n (y_train, y_val) = (y[:split_at], y[split_at:])\n \n+print(X_train.shape)\n+print(y_train.shape)\n+\n print('Build model...')\n model = Sequential()\n # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n-model.add(RNN(len(chars), HIDDEN_SIZE))\n+# note: in a situation where your input sequences have a variable length,\n+# use input_shape=(None, nb_feature).\n+model.add(RNN(HIDDEN_SIZE, input_shape=(None, len(chars))))\n # For the decoder's input, we repeat the encoded input for each time step\n model.add(RepeatVector(DIGITS + 1))\n # The decoder RNN could be multiple layers stacked or a single layer\n for _ in xrange(LAYERS):\n-    model.add(RNN(HIDDEN_SIZE, HIDDEN_SIZE, return_sequences=True))\n+    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n+\n # For each of step of the output sequence, decide which character should be chosen\n-model.add(Dense(HIDDEN_SIZE, len(chars)))\n+model.add(TimeDistributedDense(len(chars)))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n@@ -148,7 +153,7 @@ for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=1, validation_data=(X_val, y_val), show_accuracy=True)\n+    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1, validation_data=(X_val, y_val), show_accuracy=True)\n     ###\n     # Select 10 samples from the validation set at random so we can visualize errors\n     for i in xrange(10):\n\n@@ -181,15 +181,15 @@ print('Build model...')\n \n sentrnn = Sequential()\n sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True))\n-sentrnn.add(RNN(EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, return_sequences=False))\n+sentrnn.add(RNN(SENT_HIDDEN_SIZE, return_sequences=False))\n \n qrnn = Sequential()\n qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE))\n-qrnn.add(RNN(EMBED_HIDDEN_SIZE, QUERY_HIDDEN_SIZE, return_sequences=False))\n+qrnn.add(RNN(QUERY_HIDDEN_SIZE, return_sequences=False))\n \n model = Sequential()\n model.add(Merge([sentrnn, qrnn], mode='concat'))\n-model.add(Dense(SENT_HIDDEN_SIZE + QUERY_HIDDEN_SIZE, vocab_size, activation='softmax'))\n+model.add(Dense(vocab_size, activation='softmax'))\n \n model.compile(optimizer='adam', loss='categorical_crossentropy', class_mode='categorical')\n \n\n@@ -28,16 +28,10 @@ nb_classes = 10\n nb_epoch = 200\n data_augmentation = True\n \n-# shape of the image (SHAPE x SHAPE)\n-shapex, shapey = 32, 32\n-# number of convolutional filters to use at each layer\n-nb_filters = [32, 64]\n-# level of pooling to perform at each layer (POOL x POOL)\n-nb_pool = [2, 2]\n-# level of convolution to perform at each layer (CONV x CONV)\n-nb_conv = [3, 3]\n+# input image dimensions\n+img_rows, img_cols = 32, 32\n # the CIFAR10 images are RGB\n-image_dimensions = 3\n+img_channels = 3\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n@@ -51,28 +45,26 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(nb_filters[0], image_dimensions, nb_conv[0], nb_conv[0], border_mode='full'))\n+model.add(Convolution2D(32, 3, 3, border_mode='full',\n+                        input_shape=(img_channels, img_rows, img_cols)))\n model.add(Activation('relu'))\n-model.add(Convolution2D(nb_filters[0], nb_filters[0], nb_conv[0], nb_conv[0]))\n+model.add(Convolution2D(32, 3, 3))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(nb_pool[0], nb_pool[0])))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n-model.add(Convolution2D(nb_filters[1], nb_filters[0], nb_conv[0], nb_conv[0], border_mode='full'))\n+model.add(Convolution2D(64, 3, 3, border_mode='full'))\n model.add(Activation('relu'))\n-model.add(Convolution2D(nb_filters[1], nb_filters[1], nb_conv[1], nb_conv[1]))\n+model.add(Convolution2D(64, 3, 3))\n model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(nb_pool[1], nb_pool[1])))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n-# the image dimensions are the original dimensions divided by any pooling\n-# each pixel has a number of filters, determined by the last Convolution2D layer\n-model.add(Dense(nb_filters[-1] * (shapex / nb_pool[0] / nb_pool[1]) * (shapey / nb_pool[0] / nb_pool[1]), 512))\n+model.add(Dense(512))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-\n-model.add(Dense(512, nb_classes))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n \n # let's train the model using SGD + momentum (how original).\n\n@@ -25,7 +25,7 @@ max_features = 5000\n maxlen = 100\n batch_size = 32\n embedding_dims = 100\n-nb_filters = 250\n+nb_filter = 250\n filter_length = 3\n hidden_dims = 250\n nb_epoch = 3\n@@ -47,35 +47,29 @@ model = Sequential()\n \n # we start off with an efficient embedding layer which maps\n # our vocab indices into embedding_dims dimensions\n-model.add(Embedding(max_features, embedding_dims))\n+model.add(Embedding(max_features, embedding_dims, max_lenght=maxlen))\n model.add(Dropout(0.25))\n \n-# we add a Convolution1D, which will learn nb_filters\n+# we add a Convolution1D, which will learn nb_filter\n # word group filters of size filter_length:\n-model.add(Convolution1D(input_dim=embedding_dims,\n-                        nb_filter=nb_filters,\n+model.add(Convolution1D(nb_filter=nb_filter,\n                         filter_length=filter_length,\n                         border_mode=\"valid\",\n                         activation=\"relu\",\n                         subsample_length=1))\n-\n # we use standard max pooling (halving the output of the previous layer):\n model.add(MaxPooling1D(pool_length=2))\n \n # We flatten the output of the conv layer, so that we can add a vanilla dense layer:\n model.add(Flatten())\n \n-# Computing the output shape of a conv layer can be tricky;\n-# for a good tutorial, see: http://cs231n.github.io/convolutional-networks/\n-output_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2\n-\n # We add a vanilla hidden layer:\n-model.add(Dense(output_size, hidden_dims))\n+model.add(Dense(hidden_dims))\n model.add(Dropout(0.25))\n model.add(Activation('relu'))\n \n # We project onto a single unit output layer, and squash it with a sigmoid:\n-model.add(Dense(hidden_dims, 1))\n+model.add(Dense(1))\n model.add(Activation('sigmoid'))\n \n model.compile(loss='binary_crossentropy', optimizer='rmsprop', class_mode=\"binary\")\n\n@@ -49,9 +49,9 @@ print('X_test shape:', X_test.shape)\n print('Build model...')\n model = Sequential()\n model.add(Embedding(max_features, 128))\n-model.add(LSTM(128, 128))  # try using a GRU instead, for fun\n+model.add(LSTM(128))  # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n-model.add(Dense(128, 1))\n+model.add(Dense(1))\n model.add(Activation('sigmoid'))\n \n # try using different optimizers and different optimizer configs\n\n@@ -78,7 +78,6 @@ def make_submission(y_prob, ids, encoder, fname):\n             f.write('\\n')\n     print(\"Wrote submission to file {}.\".format(fname))\n \n-\n print(\"Loading data...\")\n X, labels = load_data('train.csv', train=True)\n X, scaler = preprocess_data(X)\n@@ -96,31 +95,29 @@ print(dims, 'dims')\n print(\"Building model...\")\n \n model = Sequential()\n-model.add(Dense(dims, 512, init='glorot_uniform'))\n-model.add(PReLU((512,)))\n+model.add(Dense(512, input_shape=(dims,)))\n+model.add(PReLU())\n model.add(BatchNormalization((512,)))\n model.add(Dropout(0.5))\n \n-model.add(Dense(512, 512, init='glorot_uniform'))\n-model.add(PReLU((512,)))\n-model.add(BatchNormalization((512,)))\n+model.add(Dense(512))\n+model.add(PReLU())\n+model.add(BatchNormalization())\n model.add(Dropout(0.5))\n \n-model.add(Dense(512, 512, init='glorot_uniform'))\n-model.add(PReLU((512,)))\n-model.add(BatchNormalization((512,)))\n+model.add(Dense(512))\n+model.add(PReLU())\n+model.add(BatchNormalization())\n model.add(Dropout(0.5))\n \n-model.add(Dense(512, nb_classes, init='glorot_uniform'))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n \n print(\"Training model...\")\n-\n model.fit(X, y, nb_epoch=20, batch_size=128, validation_split=0.15)\n \n print(\"Generating submission...\")\n-\n proba = model.predict_proba(X_test)\n make_submission(proba, ids, encoder, fname='keras-otto.csv')\n\n@@ -4,7 +4,8 @@ from keras.layers.core import Dense, Activation, Dropout\n from keras.layers.recurrent import LSTM\n from keras.datasets.data_utils import get_file\n import numpy as np\n-import random, sys\n+import random\n+import sys\n \n '''\n     Example script to generate text from Nietzsche's writings.\n@@ -50,17 +51,18 @@ for i, sentence in enumerate(sentences):\n # build the model: 2 stacked LSTM\n print('Build model...')\n model = Sequential()\n-model.add(LSTM(len(chars), 512, return_sequences=True))\n+model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n model.add(Dropout(0.2))\n-model.add(LSTM(512, 512, return_sequences=False))\n+model.add(LSTM(512, return_sequences=False))\n model.add(Dropout(0.2))\n-model.add(Dense(512, len(chars)))\n+model.add(Dense(len(chars)))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-# helper function to sample an index from a probability array\n+\n def sample(a, temperature=1.0):\n+    # helper function to sample an index from a probability array\n     a = np.log(a) / temperature\n     a = np.exp(a) / np.sum(np.exp(a))\n     return np.argmax(np.random.multinomial(1, a, 1))\n\n@@ -22,20 +22,20 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 12\n \n-# shape of the image (SHAPE x SHAPE)\n-shapex, shapey = 28, 28\n+# input image dimensions\n+img_rows, img_cols = 28, 28\n # number of convolutional filters to use\n nb_filters = 32\n-# level of pooling to perform (POOL x POOL)\n+# size of pooling area for max pooling\n nb_pool = 2\n-# level of convolution to perform (CONV x CONV)\n+# convolution kernel size\n nb_conv = 3\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n-X_train = X_train.reshape(X_train.shape[0], 1, shapex, shapey)\n-X_test = X_test.reshape(X_test.shape[0], 1, shapex, shapey)\n+X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n+X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n X_train = X_train.astype(\"float32\")\n X_test = X_test.astype(\"float32\")\n X_train /= 255\n@@ -50,22 +50,20 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(nb_filters, 1, nb_conv, nb_conv, border_mode='full'))\n+model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n+                        border_mode='full',\n+                        input_shape=(1, img_rows, img_cols)))\n model.add(Activation('relu'))\n-model.add(Convolution2D(nb_filters, nb_filters, nb_conv, nb_conv))\n+model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n model.add(Activation('relu'))\n model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n-# the resulting image after conv and pooling is the original shape\n-# divided by the pooling with a number of filters for each \"pixel\"\n-# (the number of filters is determined by the last Conv2D)\n-model.add(Dense(nb_filters * (shapex / nb_pool) * (shapey / nb_pool), 128))\n+model.add(Dense(128))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-\n-model.add(Dense(128, nb_classes))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n\n@@ -55,11 +55,12 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n print('Evaluate IRNN...')\n model = Sequential()\n-model.add(SimpleRNN(input_dim=1, output_dim=hidden_units,\n+model.add(SimpleRNN(output_dim=hidden_units,\n                     init=lambda shape: normal(shape, scale=0.001),\n                     inner_init=lambda shape: identity(shape, scale=1.0),\n-                    activation='relu', truncate_gradient=BPTT_truncate))\n-model.add(Dense(hidden_units, nb_classes))\n+                    activation='relu', truncate_gradient=BPTT_truncate,\n+                    input_shape=(None, 1)))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n@@ -73,8 +74,8 @@ print('IRNN test accuracy:', scores[1])\n \n print('Compare to LSTM...')\n model = Sequential()\n-model.add(LSTM(1, hidden_units))\n-model.add(Dense(hidden_units, nb_classes))\n+model.add(LSTM(hidden_units, input_shape=(None, 1)))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n\n@@ -37,13 +37,13 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n-model.add(Dense(784, 128))\n+model.add(Dense(128, input_shape=(784,)))\n model.add(Activation('relu'))\n model.add(Dropout(0.2))\n-model.add(Dense(128, 128))\n+model.add(Dense(128))\n model.add(Activation('relu'))\n model.add(Dropout(0.2))\n-model.add(Dense(128, 10))\n+model.add(Dense(10))\n model.add(Activation('softmax'))\n \n rms = RMSprop()\n\n@@ -45,10 +45,10 @@ print('Y_test shape:', Y_test.shape)\n \n print(\"Building model...\")\n model = Sequential()\n-model.add(Dense(max_words, 512))\n+model.add(Dense(512, input_shape=(max_words,)))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-model.add(Dense(512, nb_classes))\n+model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n@@ -160,7 +160,6 @@ word_index = tokenizer.word_index\n reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n \n \n-\n def embed_word(w):\n     i = word_index.get(w)\n     if (not i) or (i < skip_top) or (i >= max_features):\n\n@@ -25,15 +25,19 @@ class PReLU(MaskedLayer):\n             Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n                 http://arxiv.org/pdf/1502.01852v1.pdf\n     '''\n-    def __init__(self, input_shape, init='zero', weights=None, **kwargs):\n-        super(PReLU, self).__init__(**kwargs)\n+    def __init__(self, init='zero', weights=None, **kwargs):\n         self.init = initializations.get(init)\n+        self.initial_weights = weights\n+        super(PReLU, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_shape = self.input_shape[1:]\n         self.alphas = self.init(input_shape)\n         self.params = [self.alphas]\n-        self.input_shape = input_shape\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def get_output(self, train):\n         X = self.get_input(train)\n@@ -43,7 +47,6 @@ class PReLU(MaskedLayer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_shape\": self.input_shape,\n                 \"init\": self.init.__name__}\n \n \n@@ -55,19 +58,23 @@ class ParametricSoftplus(MaskedLayer):\n             Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n-    def __init__(self, input_shape, alpha_init=0.2,\n-                 beta_init=5.0, weights=None, **kwargs):\n-\n-        super(ParametricSoftplus, self).__init__(**kwargs)\n+    def __init__(self, alpha_init=0.2, beta_init=5.0,\n+                 weights=None, **kwargs):\n         self.alpha_init = alpha_init\n         self.beta_init = beta_init\n-        self.alphas = sharedX(alpha_init * np.ones(input_shape))\n-        self.betas = sharedX(beta_init * np.ones(input_shape))\n+        self.initial_weights = weights\n+        super(ParametricSoftplus, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_shape = self.input_shape[1:]\n+        self.alphas = sharedX(self.alpha_init * np.ones(input_shape))\n+        self.betas = sharedX(self.beta_init * np.ones(input_shape))\n         self.params = [self.alphas, self.betas]\n         self.input_shape = input_shape\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def get_output(self, train):\n         X = self.get_input(train)\n@@ -75,7 +82,6 @@ class ParametricSoftplus(MaskedLayer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_shape\": self.input_shape,\n                 \"alpha_init\": self.alpha_init,\n                 \"beta_init\": self.beta_init}\n \n\n@@ -51,7 +51,7 @@ def pool_output_length(input_length, pool_size, ignore_border, stride):\n class Convolution1D(Layer):\n     input_ndim = 3\n \n-    def __init__(self, input_dim, nb_filter, filter_length,\n+    def __init__(self, nb_filter, filter_length,\n                  init='uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n@@ -82,9 +82,9 @@ class Convolution1D(Layer):\n     def build(self):\n         input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n-        self.W_shape = (nb_filter, input_dim, filter_length, 1)\n+        self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)\n         self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((nb_filter,))\n+        self.b = shared_zeros((self.nb_filter,))\n         self.params = [self.W, self.b]\n         self.regularizers = []\n \n@@ -190,9 +190,9 @@ class Convolution2D(Layer):\n     def build(self):\n         stack_size = self.input_shape[1]\n         self.input = T.tensor4()\n-        self.W_shape = (nb_filter, stack_size, nb_row, nb_col)\n+        self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)\n         self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((nb_filter,))\n+        self.b = shared_zeros((self.nb_filter,))\n         self.params = [self.W, self.b]\n         self.regularizers = []\n \n\n@@ -21,6 +21,7 @@ class Layer(object):\n     def __init__(self, **kwargs):\n         if 'input_shape' in kwargs:\n             self.set_input_shape(kwargs['input_shape'])\n+        if not hasattr(self, 'params'):\n             self.params = []\n \n     def init_updates(self):\n@@ -59,7 +60,7 @@ class Layer(object):\n         elif hasattr(self, '_input_shape'):\n             return self._input_shape\n         else:\n-            raise Exception('Layer is not connected.')\n+            raise Exception('Layer is not connected. Did you forget to set \"input_shape\"?')\n \n     def set_input_shape(self, input_shape):\n         if type(input_shape) not in [tuple, list]:\n@@ -283,7 +284,7 @@ class Merge(Layer):\n         elif self.mode == 'concat':\n             output_shape = list(input_shapes[0])\n             for shape in input_shapes[1:]:\n-                output_shape[self.concat_axis] += shape[concat_axis]\n+                output_shape[self.concat_axis] += shape[self.concat_axis]\n             return tuple(output_shape)\n \n     def get_params(self):\n@@ -528,7 +529,7 @@ class Dense(Layer):\n \n         self.input = T.matrix()\n         self.W = self.init((input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n+        self.b = shared_zeros((self.output_dim,))\n \n         self.params = [self.W, self.b]\n \n\n@@ -19,38 +19,40 @@ class Embedding(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, input_dim, output_dim, init='uniform',\n+    def __init__(self, input_dim, output_dim, init='uniform', max_lenght=None,\n                  W_regularizer=None, activity_regularizer=None, W_constraint=None,\n                  mask_zero=False, weights=None, **kwargs):\n-\n-        super(Embedding, self).__init__(**kwargs)\n-        self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n-\n-        self.input = T.imatrix()\n-        self.W = self.init((self.input_dim, self.output_dim))\n+        self.init = initializations.get(init)\n+        self.max_lenght = max_lenght\n         self.mask_zero = mask_zero\n \n-        self.params = [self.W]\n-\n         self.W_constraint = constraints.get(W_constraint)\n         self.constraints = [self.W_constraint]\n \n-        self.regularizers = []\n-\n         self.W_regularizer = regularizers.get(W_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.initial_weights = weights\n+        kwargs['input_shape'] = (self.input_dim,)\n+        super(Embedding, self).__init__(**kwargs)\n+\n+    def build(self):\n+        self.input = T.imatrix()\n+        self.W = self.init((self.input_dim, self.output_dim))\n+        self.params = [self.W]\n+        self.regularizers = []\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n         if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n \n     def get_output_mask(self, train=None):\n         X = self.get_input(train)\n@@ -61,7 +63,7 @@ class Embedding(Layer):\n \n     @property\n     def output_shape(self):\n-        return (self.input_shape[0], None, self.output_dim)\n+        return (self.input_shape[0], self.max_lenght, self.output_dim)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -73,6 +75,8 @@ class Embedding(Layer):\n                 \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n+                \"max_lenght\": self.max_lenght,\n+                \"mask_zero\": self.mask_zero,\n                 \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                 \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                 \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None}\n\n@@ -21,6 +21,7 @@ class BatchNormalization(Layer):\n         self.epsilon = epsilon\n         self.mode = mode\n         self.momentum = momentum\n+        self.initial_weights = weights\n         super(BatchNormalization, self).__init__(**kwargs)\n \n     def build(self):\n@@ -34,8 +35,9 @@ class BatchNormalization(Layer):\n         self.params = [self.gamma, self.beta]\n         self.running_mean = shared_zeros(input_shape)\n         self.running_std = shared_ones((input_shape))\n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def get_weights(self):\n         return super(BatchNormalization, self).get_weights() + [self.running_mean.get_value(), self.running_std.get_value()]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c60e2dfbdbb49b383b159fba7f46810c7dcbbf34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 367 | Lines Deleted: 210 | Files Changed: 23 | Hunks: 213 | Methods Changed: 85 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 577 | Churn Cumulative: 8512 | Contributors (this commit): 40 | Commits (past 90d): 205 | Contributors (cumulative): 111 | DMM Complexity: 0.9256756756756757\n\nDIFF:\n@@ -47,7 +47,7 @@ model = Sequential()\n \n # we start off with an efficient embedding layer which maps\n # our vocab indices into embedding_dims dimensions\n-model.add(Embedding(max_features, embedding_dims, max_lenght=maxlen))\n+model.add(Embedding(max_features, embedding_dims, max_length=maxlen))\n model.add(Dropout(0.25))\n \n # we add a Convolution1D, which will learn nb_filter\n\n@@ -15,8 +15,10 @@ class LeakyReLU(MaskedLayer):\n         return T.nnet.relu(X, self.alpha)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"alpha\": self.alpha}\n+        base_config = super(LeakyReLU, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class PReLU(MaskedLayer):\n@@ -46,8 +48,10 @@ class PReLU(MaskedLayer):\n         return pos + neg\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"init\": self.init.__name__}\n+        base_config = super(PReLU, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class ParametricSoftplus(MaskedLayer):\n@@ -81,9 +85,11 @@ class ParametricSoftplus(MaskedLayer):\n         return T.nnet.softplus(self.betas * X) * self.alphas\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"alpha_init\": self.alpha_init,\n                   \"beta_init\": self.beta_init}\n+        base_config = super(ParametricSoftplus, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class ThresholdedLinear(MaskedLayer):\n@@ -103,11 +109,13 @@ class ThresholdedLinear(MaskedLayer):\n         return T.switch(abs(X) < self.theta, 0, X)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"theta\": self.theta}\n+        base_config = super(ThresholdedLinear, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n-class ThresholdedReLu(MaskedLayer):\n+class ThresholdedReLU(MaskedLayer):\n     '''\n         Thresholded Rectified Activation\n \n@@ -116,7 +124,7 @@ class ThresholdedReLu(MaskedLayer):\n             http://arxiv.org/pdf/1402.3337.pdf\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n-        super(ThresholdedReLu, self).__init__(**kwargs)\n+        super(ThresholdedReLU, self).__init__(**kwargs)\n         self.theta = theta\n \n     def get_output(self, train):\n@@ -124,5 +132,7 @@ class ThresholdedReLu(MaskedLayer):\n         return T.switch(X > self.theta, X, 0)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"theta\": self.theta}\n+        base_config = super(ThresholdedReLU, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -37,7 +37,6 @@ class Sequential(Layer):\n             self.layers[-1].set_previous(self.layers[-2])\n             if not hasattr(self.layers[0], 'input'):\n                 self.set_input()\n-        layer.init_updates()\n \n         params, regularizers, constraints, updates = layer.get_params()\n         self.params += params\n@@ -217,7 +216,6 @@ class Graph(Layer):\n                                  'merge_mode': merge_mode,\n                                  'concat_axis': concat_axis,\n                                  'create_output': create_output})\n-        layer.init_updates()\n         params, regularizers, constraints, updates = layer.get_params()\n         self.params += params\n         self.regularizers += regularizers\n\n@@ -55,7 +55,7 @@ class Convolution1D(Layer):\n                  init='uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None, input_dim=None, input_length=None, **kwargs):\n \n         if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n@@ -77,6 +77,11 @@ class Convolution1D(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(Convolution1D, self).__init__(**kwargs)\n \n     def build(self):\n@@ -143,7 +148,7 @@ class Convolution1D(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"nb_filter\": self.nb_filter,\n                   \"filter_length\": self.filter_length,\n                   \"init\": self.init.__name__,\n@@ -154,7 +159,11 @@ class Convolution1D(Layer):\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(Convolution1D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Convolution2D(Layer):\n@@ -253,7 +262,7 @@ class Convolution2D(Layer):\n         return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"nb_filter\": self.nb_filter,\n                   \"nb_row\": self.nb_row,\n                   \"nb_col\": self.nb_col,\n@@ -266,6 +275,8 @@ class Convolution2D(Layer):\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n                   \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+        base_config = super(Convolution2D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class MaxPooling1D(Layer):\n@@ -297,10 +308,12 @@ class MaxPooling1D(Layer):\n         return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"stride\": self.stride,\n                   \"pool_length\": self.pool_length,\n                   \"ignore_border\": self.ignore_border}\n+        base_config = super(MaxPooling1D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class MaxPooling2D(Layer):\n@@ -328,10 +341,12 @@ class MaxPooling2D(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"pool_size\": self.pool_size,\n                   \"ignore_border\": self.ignore_border,\n                   \"stride\": self.stride}\n+        base_config = super(MaxPooling2D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class UpSample1D(Layer):\n@@ -353,8 +368,10 @@ class UpSample1D(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"length\": self.length}\n+        base_config = super(UpSample1D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class UpSample2D(Layer):\n@@ -377,8 +394,10 @@ class UpSample2D(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"size\": self.size}\n+        base_config = super(UpSample2D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class ZeroPadding1D(Layer):\n@@ -403,7 +422,7 @@ class ZeroPadding1D(Layer):\n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = padding\n-        self.input = T.tensor4()\n+        self.input = T.tensor3()\n \n     @property\n     def output_shape(self):\n@@ -412,12 +431,18 @@ class ZeroPadding1D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = T.zeros(self.output_shape)\n+        input_shape = X.shape\n+        output_shape = (input_shape[0],\n+                        input_shape[1] + 2 * self.padding,\n+                        input_shape[2])\n+        output = T.zeros(output_shape)\n         return T.set_subtensor(output[:, self.padding:X.shape[1] + self.padding, :], X)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"padding\": self.padding}\n+        base_config = super(ZeroPadding1D, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class ZeroPadding2D(Layer):\n@@ -455,17 +480,19 @@ class ZeroPadding2D(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         input_shape = X.shape\n-        out_shape = (input_shape[0],\n+        output_shape = (input_shape[0],\n                         input_shape[1],\n                         input_shape[2] + 2 * self.padding[0],\n                         input_shape[3] + 2 * self.padding[1])\n-        out = T.zeros(out_shape)\n+        output = T.zeros(output_shape)\n         indices = (slice(None),\n                    slice(None),\n                    slice(self.padding[0], input_shape[2] + self.padding[0]),\n                    slice(self.padding[1], input_shape[3] + self.padding[1]))\n-        return T.set_subtensor(out[indices], X)\n+        return T.set_subtensor(output[indices], X)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"padding\": self.padding}\n+        base_config = super(ZeroPadding2D, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -19,18 +19,20 @@ from six.moves import zip\n \n class Layer(object):\n     def __init__(self, **kwargs):\n+        for kwarg in kwargs:\n+            assert kwarg in {'input_shape'}, \"Keyword argument not understood: \" + kwarg\n         if 'input_shape' in kwargs:\n             self.set_input_shape(kwargs['input_shape'])\n         if not hasattr(self, 'params'):\n             self.params = []\n \n-    def init_updates(self):\n-        self.updates = []\n-\n     def set_previous(self, layer, connection_map={}):\n         assert self.nb_input == layer.nb_output == 1, \"Cannot connect layers: input count and output count should be 1.\"\n-        if not self.supports_masked_input() and layer.get_output_mask() is not None:\n-            raise Exception(\"Cannot connect non-masking layer to layer with masked output\")\n+        if hasattr(self, 'input_ndim'):\n+            assert self.input_ndim == len(layer.output_shape), \"Incompatible shapes: layer expected input with ndim=\" +\\\n+                str(self.input_ndim) + \" but previous layer has output_shape \" + str(layer.output_shape)\n+        if layer.get_output_mask() is not None:\n+            assert self.supports_masked_input(), \"Cannot connect non-masking layer to layer with masked output\"\n         self.previous = layer\n         self.build()\n \n@@ -114,7 +116,8 @@ class Layer(object):\n         return None\n \n     def set_weights(self, weights):\n-        assert len(self.params) == len(weights), 'Provided weight array does not match layer weights.'\n+        assert len(self.params) == len(weights), 'Provided weight array does not match layer weights (' + \\\n+            str(len(self.params)) + ' layer params vs. ' + str(len(weights)) + ' provided weights)'\n         for p, w in zip(self.params, weights):\n             if p.eval().shape != w.shape:\n                 raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (p.eval().shape, w.shape))\n@@ -127,7 +130,10 @@ class Layer(object):\n         return weights\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__}\n+        config = {\"name\": self.__class__.__name__}\n+        if hasattr(self, '_input_shape'):\n+            config['input_shape'] = self._input_shape[1:]\n+        return config\n \n     def get_params(self):\n         consts = []\n@@ -207,8 +213,10 @@ class Masking(MaskedLayer):\n         return X * T.shape_padright(T.any((1. - T.eq(X, self.mask_value)), axis=-1))\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"mask_value\": self.mask_value}\n+        base_config = super(Masking, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class TimeDistributedMerge(Layer):\n@@ -246,8 +254,10 @@ class TimeDistributedMerge(Layer):\n             raise Exception('Unknown merge mode')\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"mode\": self.mode}\n+        base_config = super(TimeDistributedMerge, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Merge(Layer):\n@@ -352,10 +362,12 @@ class Merge(Layer):\n             weights = weights[nb_param:]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"layers\": [l.get_config() for l in self.layers],\n                   \"mode\": self.mode,\n                   \"concat_axis\": self.concat_axis}\n+        base_config = super(Merge, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Dropout(MaskedLayer):\n@@ -378,8 +390,10 @@ class Dropout(MaskedLayer):\n         return X\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"p\": self.p}\n+        base_config = super(Dropout, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Activation(MaskedLayer):\n@@ -397,10 +411,12 @@ class Activation(MaskedLayer):\n         return self.activation(X)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"target\": self.target,\n                   \"beta\": self.beta}\n+        base_config = super(Activation, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Reshape(Layer):\n@@ -423,8 +439,10 @@ class Reshape(Layer):\n         return theano.tensor.reshape(X, new_shape)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"dims\": self.dims}\n+        base_config = super(Reshape, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Permute(Layer):\n@@ -449,8 +467,10 @@ class Permute(Layer):\n         return X.dimshuffle((0,) + self.dims)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"dims\": self.dims}\n+        base_config = super(Permute, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Flatten(Layer):\n@@ -496,8 +516,10 @@ class RepeatVector(Layer):\n         return stacked.dimshuffle((1, 0, 2))\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"n\": self.n}\n+        base_config = super(RepeatVector, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class Dense(Layer):\n@@ -508,7 +530,7 @@ class Dense(Layer):\n \n     def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.output_dim = output_dim\n@@ -522,6 +544,10 @@ class Dense(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_dim,)\n         super(Dense, self).__init__(**kwargs)\n \n     def build(self):\n@@ -560,7 +586,7 @@ class Dense(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"activation\": self.activation.__name__,\n@@ -568,7 +594,10 @@ class Dense(Layer):\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim}\n+        base_config = super(Dense, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class ActivityRegularization(Layer):\n@@ -589,9 +618,11 @@ class ActivityRegularization(Layer):\n         return self.get_input(train)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"l1\": self.l1,\n                   \"l2\": self.l2}\n+        base_config = super(ActivityRegularization, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class TimeDistributedDense(MaskedLayer):\n@@ -606,7 +637,7 @@ class TimeDistributedDense(MaskedLayer):\n \n     def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None, input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -620,10 +651,15 @@ class TimeDistributedDense(MaskedLayer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(TimeDistributedDense, self).__init__(**kwargs)\n \n     def build(self):\n-        input_dim = self.input_shape[1]\n+        input_dim = self.input_shape[2]\n \n         self.input = T.tensor3()\n         self.W = self.init((input_dim, self.output_dim))\n@@ -659,7 +695,7 @@ class TimeDistributedDense(MaskedLayer):\n         return output.dimshuffle(1, 0, 2)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"activation\": self.activation.__name__,\n@@ -667,7 +703,11 @@ class TimeDistributedDense(MaskedLayer):\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(TimeDistributedDense, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class AutoEncoder(Layer):\n@@ -762,7 +802,7 @@ class MaxoutDense(Layer):\n \n     def __init__(self, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n         self.output_dim = output_dim\n         self.nb_feature = nb_feature\n         self.init = initializations.get(init)\n@@ -776,6 +816,9 @@ class MaxoutDense(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n+        self.input_dim = input_dim\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_dim,)\n         super(MaxoutDense, self).__init__(**kwargs)\n \n     def build(self):\n@@ -815,7 +858,7 @@ class MaxoutDense(Layer):\n         return output\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"nb_feature\": self.nb_feature,\n@@ -823,4 +866,7 @@ class MaxoutDense(Layer):\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim}\n+        base_config = super(MaxoutDense, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -19,13 +19,13 @@ class Embedding(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, input_dim, output_dim, init='uniform', max_lenght=None,\n+    def __init__(self, input_dim, output_dim, init='uniform', input_length=None,\n                  W_regularizer=None, activity_regularizer=None, W_constraint=None,\n                  mask_zero=False, weights=None, **kwargs):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n-        self.max_lenght = max_lenght\n+        self.input_length = input_length\n         self.mask_zero = mask_zero\n \n         self.W_constraint = constraints.get(W_constraint)\n@@ -63,7 +63,7 @@ class Embedding(Layer):\n \n     @property\n     def output_shape(self):\n-        return (self.input_shape[0], self.max_lenght, self.output_dim)\n+        return (self.input_shape[0], self.input_length, self.output_dim)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -71,7 +71,7 @@ class Embedding(Layer):\n         return out\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"input_dim\": self.input_dim,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n@@ -80,6 +80,8 @@ class Embedding(Layer):\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None}\n+        base_config = super(Embedding, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class WordContextProduct(Layer):\n@@ -142,8 +144,10 @@ class WordContextProduct(Layer):\n         return self.activation(dot)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"input_dim\": self.input_dim,\n                   \"proj_dim\": self.proj_dim,\n                   \"init\": self.init.__name__,\n                   \"activation\": self.activation.__name__}\n+        base_config = super(WordContextProduct, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -24,8 +24,10 @@ class GaussianNoise(MaskedLayer):\n                                         dtype=theano.config.floatX)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"sigma\": self.sigma}\n+        base_config = super(GaussianNoise, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class GaussianDropout(MaskedLayer):\n@@ -49,5 +51,7 @@ class GaussianDropout(MaskedLayer):\n         return X\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"p\": self.p}\n+        base_config = super(GaussianDropout, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -35,6 +35,15 @@ class BatchNormalization(Layer):\n         self.params = [self.gamma, self.beta]\n         self.running_mean = shared_zeros(input_shape)\n         self.running_std = shared_ones((input_shape))\n+\n+        # initialize self.updates: batch mean/std computation\n+        X = self.get_input(train=True)\n+        m = X.mean(axis=0)\n+        std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n+        mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n+        std_update = self.momentum * self.running_std + (1-self.momentum) * std\n+        self.updates = [(self.running_mean, mean_update), (self.running_std, std_update)]\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -47,14 +56,6 @@ class BatchNormalization(Layer):\n         self.running_std.set_value(floatX(weights[-1]))\n         super(BatchNormalization, self).set_weights(weights[:-2])\n \n-    def init_updates(self):\n-        X = self.get_input(train=True)\n-        m = X.mean(axis=0)\n-        std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n-        mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n-        std_update = self.momentum * self.running_std + (1-self.momentum) * std\n-        self.updates = [(self.running_mean, mean_update), (self.running_std, std_update)]\n-\n     def get_output(self, train):\n         X = self.get_input(train)\n \n@@ -70,10 +71,12 @@ class BatchNormalization(Layer):\n         return out\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"epsilon\": self.epsilon,\n                   \"mode\": self.mode,\n                   \"momentum\": self.momentum}\n+        base_config = super(BatchNormalization, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class LRN2D(Layer):\n@@ -105,8 +108,10 @@ class LRN2D(Layer):\n         return X / scale\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"alpha\": self.alpha,\n                   \"k\": self.k,\n                   \"beta\": self.beta,\n                   \"n\": self.n}\n+        base_config = super(LRN2D, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -55,7 +55,7 @@ class SimpleRNN(Recurrent):\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n-                 truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 truncate_gradient=-1, return_sequences=False, input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -63,6 +63,11 @@ class SimpleRNN(Recurrent):\n         self.activation = activations.get(activation)\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(SimpleRNN, self).__init__(**kwargs)\n \n     def build(self):\n@@ -109,13 +114,17 @@ class SimpleRNN(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(SimpleRNN, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class SimpleDeepRNN(Recurrent):\n@@ -131,7 +140,8 @@ class SimpleDeepRNN(Recurrent):\n     def __init__(self, output_dim, depth=3,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -141,6 +151,11 @@ class SimpleDeepRNN(Recurrent):\n         self.depth = depth\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(SimpleDeepRNN, self).__init__(**kwargs)\n \n     def build(self):\n@@ -195,7 +210,7 @@ class SimpleDeepRNN(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"depth\": self.depth,\n                   \"init\": self.init.__name__,\n@@ -203,7 +218,11 @@ class SimpleDeepRNN(Recurrent):\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(SimpleDeepRNN, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class GRU(Recurrent):\n@@ -231,7 +250,8 @@ class GRU(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -240,6 +260,11 @@ class GRU(Recurrent):\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(GRU, self).__init__(**kwargs)\n \n     def build(self):\n@@ -299,14 +324,18 @@ class GRU(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(GRU, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class LSTM(Recurrent):\n@@ -337,7 +366,8 @@ class LSTM(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n                  activation='tanh', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -347,6 +377,11 @@ class LSTM(Recurrent):\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(LSTM, self).__init__(**kwargs)\n \n     def build(self):\n@@ -419,7 +454,7 @@ class LSTM(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n@@ -427,7 +462,11 @@ class LSTM(Recurrent):\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(LSTM, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class JZS1(Recurrent):\n@@ -453,7 +492,8 @@ class JZS1(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -462,6 +502,11 @@ class JZS1(Recurrent):\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(JZS1, self).__init__(**kwargs)\n \n     def build(self):\n@@ -527,14 +572,18 @@ class JZS1(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(JZS1, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class JZS2(Recurrent):\n@@ -560,7 +609,8 @@ class JZS2(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -568,6 +618,12 @@ class JZS2(Recurrent):\n         self.inner_activation = activations.get(inner_activation)\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(JZS2, self).__init__(**kwargs)\n \n     def build(self):\n@@ -634,14 +690,18 @@ class JZS2(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(JZS2, self).get_config()\n+        return dict(base_config.items() + config.items())\n \n \n class JZS3(Recurrent):\n@@ -667,7 +727,8 @@ class JZS3(Recurrent):\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n+                 weights=None, truncate_gradient=-1, return_sequences=False,\n+                 input_dim=None, input_length=None, **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -676,6 +737,11 @@ class JZS3(Recurrent):\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n         super(JZS3, self).__init__(**kwargs)\n \n     def build(self):\n@@ -735,11 +801,15 @@ class JZS3(Recurrent):\n         return outputs[-1]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n+        config = {\"name\": self.__class__.__name__,\n                   \"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n                   \"inner_activation\": self.inner_activation.__name__,\n                   \"truncate_gradient\": self.truncate_gradient,\n-                \"return_sequences\": self.return_sequences}\n+                  \"return_sequences\": self.return_sequences,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(JZS3, self).get_config()\n+        return dict(base_config.items() + config.items())\n\n@@ -16,9 +16,6 @@ class TestConvolutions(unittest.TestCase):\n \n         weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)), np.ones(nb_filter)]\n \n-        self.assertRaises(Exception, convolutional.Convolution1D,\n-                          input_dim, nb_filter, filter_length, border_mode='foo')\n-\n         input = np.ones((nb_samples, nb_steps, input_dim))\n         for weight in [None, weights_in]:\n             for border_mode in ['valid', 'full', 'same']:\n@@ -29,10 +26,10 @@ class TestConvolutions(unittest.TestCase):\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution1D(\n-                                    input_dim, nb_filter, filter_length, weights=weight,\n+                                    nb_filter, filter_length, weights=weight,\n                                     border_mode=border_mode, W_regularizer=W_regularizer,\n                                     b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n-                                    subsample_length=subsample_length)\n+                                    subsample_length=subsample_length, input_shape=(None, input_dim))\n \n                             layer.input = theano.shared(value=input)\n                             for train in [True, False]:\n@@ -70,9 +67,6 @@ class TestConvolutions(unittest.TestCase):\n \n         weights_in = [np.ones((nb_filter, stack_size, nb_row, nb_col)), np.ones(nb_filter)]\n \n-        self.assertRaises(Exception, convolutional.Convolution2D,\n-                          nb_filter, stack_size, nb_row, nb_col, border_mode='foo')\n-\n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n         for weight in [None, weights_in]:\n             for border_mode in ['valid', 'full', 'same']:\n@@ -83,10 +77,10 @@ class TestConvolutions(unittest.TestCase):\n                         for b_regularizer in [None, 'l2']:\n                             for act_regularizer in [None, 'l2']:\n                                 layer = convolutional.Convolution2D(\n-                                    nb_filter, stack_size, nb_row, nb_col, weights=weight,\n+                                    nb_filter, nb_row, nb_col, weights=weight,\n                                     border_mode=border_mode, W_regularizer=W_regularizer,\n                                     b_regularizer=b_regularizer, activity_regularizer=act_regularizer,\n-                                    subsample=subsample)\n+                                    subsample=subsample, input_shape=(stack_size, None, None))\n \n                                 layer.input = theano.shared(value=input)\n                                 for train in [True, False]:\n\n@@ -84,7 +84,7 @@ class TestConfigParams(unittest.TestCase):\n         self._runner(layer)\n \n     def test_dense(self):\n-        layer = core.Dense(10, 10)\n+        layer = core.Dense(10, input_shape=(10,))\n         self._runner(layer)\n \n     def test_act_reg(self):\n@@ -92,7 +92,7 @@ class TestConfigParams(unittest.TestCase):\n         self._runner(layer)\n \n     def test_time_dist_dense(self):\n-        layer = core.TimeDistributedDense(10, 10)\n+        layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n         self._runner(layer)\n \n     def test_time_dist_merge(self):\n\n@@ -12,9 +12,8 @@ def _runner(layer_class):\n     All the recurrent layers share the same interface, so we can run through them with a single\n     function.\n     \"\"\"\n-    for weights in [None, [np.ones((input_dim, output_dim))]]:\n     for ret_seq in [True, False]:\n-            layer = layer_class(input_dim, output_dim, return_sequences=ret_seq, weights=weights)\n+        layer = layer_class(output_dim, return_sequences=ret_seq, weights=None, input_shape=(None, input_dim))\n         layer.input = theano.shared(value=np.ones((nb_samples, timesteps, input_dim)))\n         config = layer.get_config()\n \n\n@@ -6,6 +6,7 @@ import theano.tensor as T\n \n import numpy\n \n+\n def list_assert_equal(a, b, round_to=7):\n     '''\n     This will do a pairwise, rounded equality test across two lists of\n@@ -15,14 +16,15 @@ def list_assert_equal(a, b, round_to=7):\n     for i, j in pairs:\n         assert round(i, round_to) == round(j, round_to)\n \n+\n def get_standard_values():\n     '''\n     These are just a set of floats used for testing the activation\n     functions, and are useful in multiple tests.\n     '''\n-\n     return [0, 0.1, 0.5, 0.9, 1.0]\n \n+\n def test_softmax():\n \n     from keras.activations import softmax as s\n@@ -49,6 +51,7 @@ def test_softmax():\n \n     list_assert_equal(result, expected)\n \n+\n def test_relu():\n     '''\n     Relu implementation doesn't depend on the value being\n@@ -73,7 +76,6 @@ def test_relu():\n \n \n def test_tanh():\n-\n     from keras.activations import tanh as t\n     test_values = get_standard_values()\n \n\n@@ -15,15 +15,15 @@ class TestBatchNormalization(unittest.TestCase):\n         self.input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n     def test_setup(self):\n-        norm_m0 = normalization.BatchNormalization((10, 10))\n-        norm_m1 = normalization.BatchNormalization((10, 10), mode=1)\n+        norm_m0 = normalization.BatchNormalization(input_shape=(10, 10))\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10, 10), mode=1)\n \n         # mode 3 does not exist\n-        self.assertRaises(Exception, normalization.BatchNormalization((10, 10), mode=3))\n+        self.assertRaises(Exception, normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n \n     def test_mode_0(self):\n         model = Sequential()\n-        norm_m0 = normalization.BatchNormalization((10,))\n+        norm_m0 = normalization.BatchNormalization(input_shape=(10,))\n         model.add(norm_m0)\n         model.compile(loss='mse', optimizer='sgd')\n \n@@ -37,8 +37,7 @@ class TestBatchNormalization(unittest.TestCase):\n         self.assertAlmostEqual(out.std().eval(), 1.0, places=1)\n \n     def test_mode_1(self):\n-        norm_m1 = normalization.BatchNormalization((10,), mode=1)\n-        norm_m1.init_updates()\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n             norm_m1.input = inp\n@@ -54,12 +53,11 @@ class TestBatchNormalization(unittest.TestCase):\n         Test batch normalization with various input shapes\n         \"\"\"\n         for inp in self.input_shapes:\n-            norm_m0 = normalization.BatchNormalization(inp.shape, mode=0)\n-            norm_m0.init_updates()\n+            norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n             norm_m0.input = inp\n             out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n \n-            norm_m1 = normalization.BatchNormalization(inp.shape, mode=1)\n+            norm_m1 = normalization.BatchNormalization(input_shape=inp.shape, mode=1)\n             norm_m1.input = inp\n             out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n \n@@ -67,9 +65,8 @@ class TestBatchNormalization(unittest.TestCase):\n         \"\"\"\n         Test weight initialization\n         \"\"\"\n-\n-        norm_m1 = normalization.BatchNormalization((10,), mode=1, weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n-        norm_m1.init_updates()\n+        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1,\n+                                                   weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n             norm_m1.input = inp\n@@ -84,15 +81,14 @@ class TestBatchNormalization(unittest.TestCase):\n         assert_allclose(norm_m1.beta.eval(), np.ones(10))\n \n     def test_config(self):\n-        norm = normalization.BatchNormalization((10, 10), mode=1, epsilon=0.1)\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1, epsilon=0.1, momentum=0.9)\n         conf = norm.get_config()\n         conf_target = {\"input_shape\": (10, 10), \"name\": normalization.BatchNormalization.__name__,\n-                       \"epsilon\": 0.1, \"mode\": 1}\n-\n+                       \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n         self.assertDictEqual(conf, conf_target)\n \n     def test_save_weights(self):\n-        norm = normalization.BatchNormalization((10, 10), mode=1, epsilon=0.1)\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1, epsilon=0.1)\n         weights = norm.get_weights()\n         assert(len(weights) == 4)\n         norm.set_weights(weights)\n\n@@ -14,9 +14,9 @@ class TestEmbedding(unittest.TestCase):\n \n     def test_unitnorm_constraint(self):\n         lookup = Sequential()\n-        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm()))\n+        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm(), input_length=1))\n         lookup.add(Flatten())\n-        lookup.add(Dense(2, 1))\n+        lookup.add(Dense(1))\n         lookup.add(Activation('sigmoid'))\n         lookup.compile(loss='binary_crossentropy', optimizer='sgd', class_mode='binary')\n         lookup.train_on_batch(self.X1, np.array([[1], [0]], dtype='int32'))\n\n@@ -25,9 +25,9 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n \n         graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n@@ -47,12 +47,12 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2-0', input='input1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2-0', input='input1')\n         graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n \n-        graph.add_node(Dense(4, 16), name='dense3', input='dense2')\n-        graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n+        graph.add_node(Dense(16), name='dense3', input='dense2')\n+        graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n \n         graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n@@ -74,9 +74,9 @@ class TestGraph(unittest.TestCase):\n         graph.add_input(name='input1', ndim=2)\n         graph.add_input(name='input2', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input2')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input2')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n \n         graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n         graph.compile('rmsprop', {'output1': 'mse'})\n@@ -97,9 +97,9 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n \n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n@@ -119,9 +119,9 @@ class TestGraph(unittest.TestCase):\n         graph.save_weights('temp.h5', overwrite=True)\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n         graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n@@ -135,9 +135,9 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(1), name='dense3', input='dense1')\n \n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n@@ -167,15 +167,15 @@ class TestGraph(unittest.TestCase):\n \n         graph = containers.Graph()\n         graph.add_input(name='input1', ndim=2)\n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n         graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n \n         seq = Sequential()\n-        seq.add(Dense(32, 32, name='first_seq_dense'))\n+        seq.add(Dense(32, input_shape=(32,)))\n         seq.add(graph)\n-        seq.add(Dense(4, 4, name='last_seq_dense'))\n+        seq.add(Dense(4))\n \n         seq.compile('rmsprop', 'mse')\n \n@@ -193,10 +193,10 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n \n-        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-        graph.add_node(Dense(4, 4), name='output1', inputs=['dense2', 'dense3'], merge_mode='sum', create_output=True)\n+        graph.add_node(Dense(16, input_shape=(32,)), name='dense1', input='input1')\n+        graph.add_node(Dense(4), name='dense2', input='input1')\n+        graph.add_node(Dense(4), name='dense3', input='dense1')\n+        graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'], merge_mode='sum', create_output=True)\n         graph.compile('rmsprop', {'output1': 'mse'})\n \n         history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n@@ -218,11 +218,11 @@ class TestGraph(unittest.TestCase):\n         graph = Graph()\n         graph.add_input(name='input1', ndim=2)\n         graph.add_input(name='input2', ndim=2)\n-        graph.add_node(Dense(nb_units, nb_units),\n+        graph.add_node(Dense(nb_units, input_shape=(nb_units,)),\n                        name='dense1', input='input1')\n-        graph.add_node(Dense(nb_units, nb_classes),\n+        graph.add_node(Dense(nb_classes, input_shape=(nb_units,)),\n                        name='dense2', input='input2')\n-        graph.add_node(Dense(nb_units, nb_classes),\n+        graph.add_node(Dense(nb_classes),\n                        name='dense3', input='dense1')\n         graph.add_output(name='output', inputs=['dense2', 'dense3'],\n                          merge_mode='sum')\n\n@@ -14,8 +14,8 @@ class TestLossMasking(unittest.TestCase):\n             [[[1, 1], [2, 1], [3, 1], [5, 5]],\n              [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n         model = Sequential()\n-        model.add(Masking(mask_value=0))\n-        model.add(TimeDistributedDense(2, 1, init='one'))\n+        model.add(Masking(mask_value=0, input_shape=(None, 2)))\n+        model.add(TimeDistributedDense(1, init='one'))\n         model.compile(loss='mse', optimizer='sgd')\n         y = model.predict(X)\n         loss = model.fit(X, 4*y, nb_epoch=1, batch_size=2, verbose=1).history['loss'][0]\n\n@@ -42,9 +42,9 @@ sample_weight[y_train == weighted_class] = high_weight\n \n def create_sequential_model():\n     model = Sequential()\n-    model.add(Dense(784, 50))\n+    model.add(Dense(50, input_shape=(784,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(50, 10))\n+    model.add(Dense(10))\n     model.add(Activation('softmax'))\n     return model\n \n@@ -52,8 +52,8 @@ def create_sequential_model():\n def create_graph_model():\n     model = Graph()\n     model.add_input(name='input')\n-    model.add_node(Dense(784, 50, activation='relu'), name='d1', input='input')\n-    model.add_node(Dense(50, 10, activation='softmax'), name='d2', input='d1')\n+    model.add_node(Dense(50, activation='relu', input_shape=(784,)), name='d1', input='input')\n+    model.add_node(Dense(10, activation='softmax'), name='d2', input='d1')\n     model.add_output(name='output', input='d2')\n     return model\n \n\n@@ -18,9 +18,9 @@ y_test = to_categorical(y_test)\n \n def get_model(input_dim, nb_hidden, output_dim):\n     model = Sequential()\n-    model.add(Dense(input_dim, nb_hidden))\n+    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_hidden, output_dim))\n+    model.add(Dense(output_dim))\n     model.add(Activation('softmax'))\n     return model\n \n\n@@ -35,9 +35,9 @@ test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n def create_model(weight_reg=None, activity_reg=None):\n     model = Sequential()\n-    model.add(Dense(784, 50))\n+    model.add(Dense(50, input_shape=(784,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(50, 10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n+    model.add(Dense(10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n     model.add(Activation('softmax'))\n     return model\n \n\n@@ -30,9 +30,9 @@ class TestSequential(unittest.TestCase):\n     def test_sequential(self):\n         print('Test sequential')\n         model = Sequential()\n-        model.add(Dense(input_dim, nb_hidden))\n+        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         model.add(Activation('relu'))\n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -57,9 +57,9 @@ class TestSequential(unittest.TestCase):\n         print('test weight saving')\n         model.save_weights('temp.h5', overwrite=True)\n         model = Sequential()\n-        model.add(Dense(input_dim, nb_hidden))\n+        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         model.add(Activation('relu'))\n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         model.load_weights('temp.h5')\n@@ -79,17 +79,17 @@ class TestSequential(unittest.TestCase):\n     def test_merge_sum(self):\n         print('Test merge: sum')\n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n \n         right = Sequential()\n-        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         right.add(Activation('relu'))\n \n         model = Sequential()\n         model.add(Merge([left, right], mode='sum'))\n \n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -113,14 +113,14 @@ class TestSequential(unittest.TestCase):\n         print('test weight saving')\n         model.save_weights('temp.h5', overwrite=True)\n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n         right = Sequential()\n-        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         right.add(Activation('relu'))\n         model = Sequential()\n         model.add(Merge([left, right], mode='sum'))\n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n         model.load_weights('temp.h5')\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -132,17 +132,17 @@ class TestSequential(unittest.TestCase):\n     def test_merge_concat(self):\n         print('Test merge: concat')\n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n \n         right = Sequential()\n-        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         right.add(Activation('relu'))\n \n         model = Sequential()\n         model.add(Merge([left, right], mode='concat'))\n \n-        model.add(Dense(nb_hidden * 2, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -166,17 +166,17 @@ class TestSequential(unittest.TestCase):\n         print('test weight saving')\n         model.save_weights('temp.h5', overwrite=True)\n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n \n         right = Sequential()\n-        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         right.add(Activation('relu'))\n \n         model = Sequential()\n         model.add(Merge([left, right], mode='concat'))\n \n-        model.add(Dense(nb_hidden * 2, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -190,26 +190,26 @@ class TestSequential(unittest.TestCase):\n         print('Test merge recursivity')\n \n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n \n         right = Sequential()\n-        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         right.add(Activation('relu'))\n \n         righter = Sequential()\n-        righter.add(Dense(input_dim, nb_hidden))\n+        righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         righter.add(Activation('relu'))\n \n         intermediate = Sequential()\n         intermediate.add(Merge([left, right], mode='sum'))\n-        intermediate.add(Dense(nb_hidden, nb_hidden))\n+        intermediate.add(Dense(nb_hidden))\n         intermediate.add(Activation('relu'))\n \n         model = Sequential()\n         model.add(Merge([intermediate, righter], mode='sum'))\n \n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -240,13 +240,13 @@ class TestSequential(unittest.TestCase):\n     def test_merge_overlap(self):\n         print('Test merge overlap')\n         left = Sequential()\n-        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n \n         model = Sequential()\n         model.add(Merge([left, left], mode='sum'))\n \n-        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -278,17 +278,18 @@ class TestSequential(unittest.TestCase):\n \n     def test_count_params(self):\n         print('test count params')\n-        nb_units = 100\n+        input_dim = 20\n+        nb_units = 10\n         nb_classes = 2\n \n-        n = nb_units * nb_units + nb_units\n+        n = input_dim * nb_units + nb_units\n         n += nb_units * nb_units + nb_units\n         n += nb_units * nb_classes + nb_classes\n \n         model = Sequential()\n-        model.add(Dense(nb_units, nb_units))\n-        model.add(Dense(nb_units, nb_units))\n-        model.add(Dense(nb_units, nb_classes))\n+        model.add(Dense(nb_units, input_shape=(input_dim,)))\n+        model.add(Dense(nb_units))\n+        model.add(Dense(nb_classes))\n         model.add(Activation('softmax'))\n \n         self.assertEqual(n, model.count_params())\n\n@@ -10,12 +10,12 @@ from keras.layers.recurrent import SimpleRNN\n def check_layer_output_shape(layer, input_data):\n     ndim = len(input_data.shape)\n     layer.input = ndim_tensor(ndim)\n-    layer._input_shape = input_data.shape\n-    expected_output_shape = layer.output_shape\n+    layer.set_input_shape(input_data.shape[1:])\n+    expected_output_shape = layer.output_shape[1:]\n \n     function = theano.function([layer.input], [layer.get_output()])\n     output = function(input_data)[0]\n-    assert output.shape == expected_output_shape\n+    assert output.shape[1:] == expected_output_shape\n \n \n class TestShapeInference(unittest.TestCase):\n@@ -43,12 +43,12 @@ class TestShapeInference(unittest.TestCase):\n         check_layer_output_shape(layer, input_data)\n \n     def test_Dense(self):\n-        layer = Dense(2, 3)\n+        layer = Dense(3)\n         input_data = np.random.random((2, 2))\n         check_layer_output_shape(layer, input_data)\n \n     def test_TimeDistributedDense(self):\n-        layer = TimeDistributedDense(3, 2)\n+        layer = TimeDistributedDense(2)\n         input_data = np.random.random((2, 2, 3))\n         check_layer_output_shape(layer, input_data)\n \n@@ -62,7 +62,7 @@ class TestShapeInference(unittest.TestCase):\n                     if subsample_length > 1 and border_mode == 'same':\n                         continue\n                     for input_data_shape in [(2, 3, 2), (2, 4, 2)]:\n-                        layer = Convolution1D(input_dim=2, nb_filter=1, filter_length=filter_length,\n+                        layer = Convolution1D(nb_filter=1, filter_length=filter_length,\n                                               border_mode=border_mode, subsample_length=subsample_length)\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n@@ -74,7 +74,7 @@ class TestShapeInference(unittest.TestCase):\n                     if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n                         continue\n                     for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n-                        layer = Convolution2D(nb_filter=1, stack_size=1, nb_row=nb_row, nb_col=nb_row,\n+                        layer = Convolution2D(nb_filter=1, nb_row=nb_row, nb_col=nb_row,\n                                               border_mode=border_mode, subsample=subsample)\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n@@ -123,7 +123,7 @@ class TestShapeInference(unittest.TestCase):\n     def test_SimpleRNN(self):\n         # all recurrent layers inherit output_shape\n         # from the same base recurrent layer\n-        layer = SimpleRNN(3, 2)\n+        layer = SimpleRNN(2)\n         input_data = np.random.random((2, 2, 3))\n         check_layer_output_shape(layer, input_data)\n \n\n@@ -11,7 +11,7 @@ from keras.utils.np_utils import to_categorical\n import unittest\n \n \n-class TestRegularizers(unittest.TestCase):\n+class TestTasks(unittest.TestCase):\n     def test_vector_clf(self):\n         nb_hidden = 10\n \n@@ -27,9 +27,9 @@ class TestRegularizers(unittest.TestCase):\n         y_test = to_categorical(y_test)\n \n         model = Sequential()\n-        model.add(Dense(X_train.shape[-1], nb_hidden))\n+        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n         model.add(Activation('relu'))\n-        model.add(Dense(nb_hidden, y_train.shape[-1]))\n+        model.add(Dense(y_train.shape[-1]))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n@@ -47,9 +47,9 @@ class TestRegularizers(unittest.TestCase):\n         print('y_test:', y_test.shape)\n \n         model = Sequential()\n-        model.add(Dense(X_train.shape[-1], nb_hidden))\n+        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n         model.add(Activation('tanh'))\n-        model.add(Dense(nb_hidden, y_train.shape[-1]))\n+        model.add(Dense(y_train.shape[-1]))\n         model.compile(loss='hinge', optimizer='adagrad')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.history['val_loss'][-1] < 0.9)\n@@ -67,7 +67,7 @@ class TestRegularizers(unittest.TestCase):\n         y_test = to_categorical(y_test)\n \n         model = Sequential()\n-        model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n+        model.add(GRU(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n@@ -83,7 +83,7 @@ class TestRegularizers(unittest.TestCase):\n         print('y_test:', y_test.shape)\n \n         model = Sequential()\n-        model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n+        model.add(GRU(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n         model.compile(loss='hinge', optimizer='adam')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.history['val_loss'][-1] < 0.8)\n@@ -98,7 +98,7 @@ class TestRegularizers(unittest.TestCase):\n         print('y_test:', y_test.shape)\n \n         model = Sequential()\n-        model.add(TimeDistributedDense(X_train.shape[-1], y_train.shape[-1]))\n+        model.add(TimeDistributedDense(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n         model.compile(loss='hinge', optimizer='rmsprop')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.history['val_loss'][-1] < 0.8)\n@@ -116,13 +116,14 @@ class TestRegularizers(unittest.TestCase):\n         y_test = to_categorical(y_test)\n \n         model = Sequential()\n-        model.add(Convolution2D(32, 3, 32, 32))\n+        model.add(Convolution2D(32, 32, 32, input_shape=(3, 32, 32)))\n         model.add(Activation('sigmoid'))\n         model.add(Flatten())\n-        model.add(Dense(32, y_test.shape[-1]))\n+        model.add(Dense(y_test.shape[-1]))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='sgd')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        print(history.history['val_acc'][-1])\n         self.assertTrue(history.history['val_acc'][-1] > 0.9)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
