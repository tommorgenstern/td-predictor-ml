{"custom_id": "scrapy#c3257dc610ccdd963fae8dda330fa337deb53054", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 103 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ class ResponseTypes:\n         cls = Response\n         if b'Content-Type' in headers:\n             cls = self.from_content_type(\n-                content_type=headers[b'Content-type'],\n+                content_type=headers[b'Content-Type'],\n                 content_encoding=headers.get(b'Content-Encoding')\n             )\n         if cls is Response and b'Content-Disposition' in headers:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f510fd47d8d10217f7b18b531205dd8c252eaef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1611 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -85,11 +85,9 @@ def add_http_if_no_scheme(url):\n \n \n def guess_scheme(url):\n-    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or http:// otherwise.\"\"\"\n-    parts = urlparse(url)\n-    if parts.scheme:\n-        return url\n-    # Note: this does not match Windows filepath\n+    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n+    http:// otherwise.\"\"\"\n+    # POSIX path\n     if re.match(r'''^                   # start with...\n                     (\n                         \\.              # ...a single dot,\n@@ -99,7 +97,10 @@ def guess_scheme(url):\n                     )?      # optional match of \".\", \"..\" or \".blabla\"\n                     /       # at least one \"/\" for a file path,\n                     .       # and something after the \"/\"\n-                    ''', parts.path, flags=re.VERBOSE):\n+                    ''', url, flags=re.VERBOSE):\n+        return any_to_uri(url)\n+    # Windows drive-letter path\n+    elif re.match(r'''^[a-z]:\\\\''', url, flags=re.IGNORECASE):\n         return any_to_uri(url)\n     else:\n         return add_http_if_no_scheme(url)\n\n@@ -94,7 +94,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_local_file(self):\n-        filepath = join(tests_datadir, 'test_site/index.html')\n+        filepath = join(tests_datadir, 'test_site', 'index.html')\n         _, out, _ = yield self.execute([filepath, '-c', 'item'])\n         assert b'{}' in out\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5b23f4b00962df76d8302ebf869ef4a4319e142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 55 | Churn Cumulative: 1139 | Contributors (this commit): 57 | Commits (past 90d): 9 | Contributors (cumulative): 74 | DMM Complexity: 0.21052631578947367\n\nDIFF:\n@@ -241,6 +241,7 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n+        self.storage_batch = self.settings.getint('FEED_STORAGE_BATCH')\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n@@ -250,19 +251,7 @@ class FeedExporter:\n     def open_spider(self, spider):\n         for uri, feed in self.feeds.items():\n             uri = uri % self._get_uri_params(spider, feed['uri_params'])\n-            storage = self._get_storage(uri)\n-            file = storage.open(spider)\n-            exporter = self._get_exporter(\n-                file=file,\n-                format=feed['format'],\n-                fields_to_export=feed['fields'],\n-                encoding=feed['encoding'],\n-                indent=feed['indent'],\n-            )\n-            slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'])\n-            self.slots.append(slot)\n-            if slot.store_empty:\n-                slot.start_exporting()\n+            self.slots.append(self._start_new_batch(None, uri, feed, spider))\n \n     def close_spider(self, spider):\n         deferred_list = []\n@@ -285,11 +274,48 @@ class FeedExporter:\n             deferred_list.append(d)\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n+    def _start_new_batch(self, previous_batch_slot, uri, feed, spider):\n+        \"\"\"\n+        Redirect the output data stream to a new file.\n+        Execute multiple times if 'FEED_STORAGE_BATCH' setting is greater than zero.\n+        \"\"\"\n+        if previous_batch_slot is not None:\n+            previous_batch_slot.exporter.finish_exporting()\n+            previous_batch_slot.storage.store(previous_batch_slot.file)\n+        storage = self._get_storage(uri)\n+        file = storage.open(spider)\n+        exporter = self._get_exporter(\n+            file=file,\n+            format=feed['format'],\n+            fields_to_export=feed['fields'],\n+            encoding=feed['encoding'],\n+            indent=feed['indent']\n+        )\n+        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'])\n+        if slot.store_empty:\n+            slot.start_exporting()\n+        return slot\n+\n+    def _get_uri_of_partial(self, slot, feed, spider):\n+        \"\"\"Get uri for each partial using datetime.now().isoformat()\"\"\"\n+        uri = (slot.uri % self._get_uri_params(spider, feed['uri_params'])).split('.')[0] + '.'\n+        uri = uri + datetime.now().isoformat() + '.' + feed['format']\n+        return uri\n+\n     def item_scraped(self, item, spider):\n-        for slot in self.slots:\n+        slots = []\n+        for idx, slot in enumerate(self.slots):\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n+            if self.storage_batch and slot.itemcount % self.storage_batch == 0:\n+                uri = self._get_uri_of_partial(slot, self.feeds[slot.uri], spider)\n+                slots.append(self._start_new_batch(slot, uri, self.feeds[slot.uri], spider))\n+                self.feeds[uri] = self.feeds[slot.uri]\n+                self.feeds.pop(slot.uri)\n+                self.slots[idx] = None\n+        self.slots = [slot for slot in self.slots if slot is not None]\n+        self.slots.extend(slots)\n \n     def _load_components(self, setting_prefix):\n         conf = without_none_values(self.settings.getwithbase(setting_prefix))\n\n@@ -146,6 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n+FEED_STORAGE_BATCH = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b4566ff93843cdf17ada069dc09261a99971d26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 545 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -249,6 +249,8 @@ class FeedExporter:\n                 raise NotConfigured\n \n     def open_spider(self, spider):\n+        if self.storage_batch:\n+            self.feeds = {self._get_uri_of_partial(uri, feed, spider): feed for uri, feed in self.feeds.items()}\n         for uri, feed in self.feeds.items():\n             uri = uri % self._get_uri_params(spider, feed['uri_params'])\n             self.slots.append(self._start_new_batch(None, uri, feed, spider))\n@@ -296,11 +298,11 @@ class FeedExporter:\n             slot.start_exporting()\n         return slot\n \n-    def _get_uri_of_partial(self, slot, feed, spider):\n+    def _get_uri_of_partial(self, template_uri, feed, spider):\n         \"\"\"Get uri for each partial using datetime.now().isoformat()\"\"\"\n-        uri = (slot.uri % self._get_uri_params(spider, feed['uri_params'])).split('.')[0] + '.'\n-        uri = uri + datetime.now().isoformat() + '.' + feed['format']\n-        return uri\n+        template_uri = (template_uri % self._get_uri_params(spider, feed['uri_params']))\n+        uri_name = template_uri.split('.')[0]\n+        return '{}.{}.{}'.format(uri_name, datetime.now().isoformat(), feed[\"format\"])\n \n     def item_scraped(self, item, spider):\n         slots = []\n@@ -309,11 +311,12 @@ class FeedExporter:\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             if self.storage_batch and slot.itemcount % self.storage_batch == 0:\n-                uri = self._get_uri_of_partial(slot, self.feeds[slot.uri], spider)\n+                uri = self._get_uri_of_partial(slot.uri, self.feeds[slot.uri], spider)\n                 slots.append(self._start_new_batch(slot, uri, self.feeds[slot.uri], spider))\n                 self.feeds[uri] = self.feeds[slot.uri]\n                 self.feeds.pop(slot.uri)\n                 self.slots[idx] = None\n+\n         self.slots = [slot for slot in self.slots if slot is not None]\n         self.slots.extend(slots)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0723e3f4f9777a87d0df3b2e2fddfeac9099dd3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 75 | Churn Cumulative: 1227 | Contributors (this commit): 57 | Commits (past 90d): 12 | Contributors (cumulative): 74 | DMM Complexity: 0.55\n\nDIFF:\n@@ -180,14 +180,16 @@ class FTPFeedStorage(BlockingFeedStorage):\n \n \n class _FeedSlot:\n-    def __init__(self, file, exporter, storage, uri, format, store_empty):\n+    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, template_uri):\n         self.file = file\n         self.exporter = exporter\n         self.storage = storage\n         # feed params\n-        self.uri = uri\n+        self.batch_id = batch_id\n         self.format = format\n         self.store_empty = store_empty\n+        self.template_uri = template_uri\n+        self.uri = uri\n         # flags\n         self.itemcount = 0\n         self._exporting = False\n@@ -241,19 +243,28 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch = self.settings.getint('FEED_STORAGE_BATCH')\n+        self.storage_batch_size = self.settings.getint('FEED_STORAGE_BATCH_SIZE')\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n+            if not self._batch_deliveries_supported(uri):\n+                raise NotConfigured\n             if not self._exporter_supported(feed['format']):\n                 raise NotConfigured\n \n     def open_spider(self, spider):\n-        if self.storage_batch:\n-            self.feeds = {self._get_uri_of_partial(uri, feed, spider): feed for uri, feed in self.feeds.items()}\n         for uri, feed in self.feeds.items():\n-            uri = uri % self._get_uri_params(spider, feed['uri_params'])\n-            self.slots.append(self._start_new_batch(None, uri, feed, spider))\n+            batch_id = 1\n+            uri_params = self._get_uri_params(spider, feed['uri_params'])\n+            uri_params['batch_id'] = batch_id\n+            self.slots.append(self._start_new_batch(\n+                previous_batch_slot=None,\n+                uri=uri % uri_params,\n+                feed=feed,\n+                spider=spider,\n+                batch_id=batch_id,\n+                template_uri=uri\n+            ))\n \n     def close_spider(self, spider):\n         deferred_list = []\n@@ -276,10 +287,17 @@ class FeedExporter:\n             deferred_list.append(d)\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n-    def _start_new_batch(self, previous_batch_slot, uri, feed, spider):\n+    def _start_new_batch(self, previous_batch_slot, uri, feed, spider, batch_id, template_uri):\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if 'FEED_STORAGE_BATCH' setting is greater than zero.\n+        :param previous_batch_slot: slot of previous batch. We need to call slot.storage.store\n+        to get the file properly closed.\n+        :param uri: uri of the new batch to start\n+        :param feed: dict with parameters of feed\n+        :param spider: user spider\n+        :param batch_id: sequential batch id starting at 1\n+        :param template_uri: template uri which contains %(time)s or %(batch_id)s to create new uri\n         \"\"\"\n         if previous_batch_slot is not None:\n             previous_batch_slot.exporter.finish_exporting()\n@@ -293,30 +311,30 @@ class FeedExporter:\n             encoding=feed['encoding'],\n             indent=feed['indent']\n         )\n-        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'])\n+        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'], batch_id, template_uri)\n         if slot.store_empty:\n             slot.start_exporting()\n         return slot\n \n-    def _get_uri_of_partial(self, template_uri, feed, spider):\n-        \"\"\"Get uri for each partial using datetime.now().isoformat()\"\"\"\n-        template_uri = (template_uri % self._get_uri_params(spider, feed['uri_params']))\n-        uri_name = template_uri.split('.')[0]\n-        return '{}.{}.{}'.format(uri_name, datetime.now().isoformat(), feed[\"format\"])\n-\n     def item_scraped(self, item, spider):\n         slots = []\n         for idx, slot in enumerate(self.slots):\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n-            if self.storage_batch and slot.itemcount % self.storage_batch == 0:\n-                uri = self._get_uri_of_partial(slot.uri, self.feeds[slot.uri], spider)\n-                slots.append(self._start_new_batch(slot, uri, self.feeds[slot.uri], spider))\n-                self.feeds[uri] = self.feeds[slot.uri]\n-                self.feeds.pop(slot.uri)\n+            if self.storage_batch_size and slot.itemcount % self.storage_batch_size == 0:\n+                batch_id = slot.batch_id + 1\n+                uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'])\n+                uri_params['batch_id'] = batch_id\n+                self.slots.append(self._start_new_batch(\n+                    previous_batch_slot=slot,\n+                    uri=slot.template_uri % uri_params,\n+                    feed=self.feeds[slot.template_uri],\n+                    spider=spider,\n+                    batch_id=batch_id,\n+                    template_uri=slot.template_uri\n+                ))\n                 self.slots[idx] = None\n-\n         self.slots = [slot for slot in self.slots if slot is not None]\n         self.slots.extend(slots)\n \n@@ -335,6 +353,17 @@ class FeedExporter:\n             return True\n         logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n \n+    def _batch_deliveries_supported(self, uri):\n+        \"\"\"\n+        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(time)s or %(batch_id)s\n+        to distinguish different files of partial output\n+        \"\"\"\n+        if not self.storage_batch_size:\n+            return True\n+        if '%(time)s' in uri or '%(batch_id)s' in uri:\n+            return True\n+        logger.error('%(time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+\n     def _storage_supported(self, uri):\n         scheme = urlparse(uri).scheme\n         if scheme in self.storages:\n@@ -364,7 +393,7 @@ class FeedExporter:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n-        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n+        ts = datetime.utcnow().isoformat().replace(':', '-')\n         params['time'] = ts\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n\n@@ -146,7 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_STORAGE_BATCH = 0\n+FEED_STORAGE_BATCH_SIZE = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d11411b402ae68874c6ccc2883836be0b9cf8326", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 1260 | Contributors (this commit): 57 | Commits (past 90d): 14 | Contributors (cumulative): 74 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,6 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -243,7 +242,7 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch_size = self.settings.getint('FEED_STORAGE_BATCH_SIZE')\n+        self.storage_batch_size = self.settings.get('FEED_STORAGE_BATCH_SIZE', None)\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n@@ -263,7 +262,7 @@ class FeedExporter:\n                 feed=feed,\n                 spider=spider,\n                 batch_id=batch_id,\n-                template_uri=uri\n+                template_uri=uri,\n             ))\n \n     def close_spider(self, spider):\n@@ -290,7 +289,7 @@ class FeedExporter:\n     def _start_new_batch(self, previous_batch_slot, uri, feed, spider, batch_id, template_uri):\n         \"\"\"\n         Redirect the output data stream to a new file.\n-        Execute multiple times if 'FEED_STORAGE_BATCH' setting is greater than zero.\n+        Execute multiple times if 'FEED_STORAGE_BATCH' setting is specified.\n         :param previous_batch_slot: slot of previous batch. We need to call slot.storage.store\n         to get the file properly closed.\n         :param uri: uri of the new batch to start\n@@ -309,9 +308,18 @@ class FeedExporter:\n             format=feed['format'],\n             fields_to_export=feed['fields'],\n             encoding=feed['encoding'],\n-            indent=feed['indent']\n+            indent=feed['indent'],\n+        )\n+        slot = _FeedSlot(\n+            file=file,\n+            exporter=exporter,\n+            storage=storage,\n+            uri=uri,\n+            format=feed['format'],\n+            store_empty=feed['store_empty'],\n+            batch_id=batch_id,\n+            template_uri=template_uri,\n         )\n-        slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'], batch_id, template_uri)\n         if slot.store_empty:\n             slot.start_exporting()\n         return slot\n@@ -326,13 +334,13 @@ class FeedExporter:\n                 batch_id = slot.batch_id + 1\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'])\n                 uri_params['batch_id'] = batch_id\n-                self.slots.append(self._start_new_batch(\n+                slots.append(self._start_new_batch(\n                     previous_batch_slot=slot,\n                     uri=slot.template_uri % uri_params,\n                     feed=self.feeds[slot.template_uri],\n                     spider=spider,\n                     batch_id=batch_id,\n-                    template_uri=slot.template_uri\n+                    template_uri=slot.template_uri,\n                 ))\n                 self.slots[idx] = None\n         self.slots = [slot for slot in self.slots if slot is not None]\n@@ -358,11 +366,10 @@ class FeedExporter:\n         If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(time)s or %(batch_id)s\n         to distinguish different files of partial output\n         \"\"\"\n-        if not self.storage_batch_size:\n+        if self.storage_batch_size is None or '%(time)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        if '%(time)s' in uri or '%(batch_id)s' in uri:\n-            return True\n-        logger.error('%(time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+        logger.warning('%(time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+        return False\n \n     def _storage_supported(self, uri):\n         scheme = urlparse(uri).scheme\n\n@@ -146,7 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_STORAGE_BATCH_SIZE = 0\n+FEED_STORAGE_BATCH_SIZE = None\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39d0d13d3f7bd671d5b29646b209c62e23373fab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 195 | Churn Cumulative: 1853 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 0.6222222222222222\n\nDIFF:\n@@ -6,6 +6,7 @@ import shutil\n import string\n import tempfile\n import warnings\n+from abc import ABC, abstractmethod\n from io import BytesIO\n from pathlib import Path\n from string import ascii_letters, digits\n@@ -21,8 +22,9 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.crawler import CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n from scrapy.exporters import CsvItemExporter\n-from scrapy.extensions.feedexport import (BlockingFeedStorage, FileFeedStorage, FTPFeedStorage,\n+from scrapy.extensions.feedexport import (BlockingFeedStorage, FeedExporter, FileFeedStorage, FTPFeedStorage,\n                                           IFeedStorage, S3FeedStorage, StdoutFeedStorage)\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -76,6 +78,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n             name = 'test_spider'\n+\n         crawler = get_crawler(settings_dict=settings)\n         spider = TestSpider.from_crawler(crawler)\n         return spider\n@@ -129,6 +132,7 @@ class BlockingFeedStorageTest(unittest.TestCase):\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n             name = 'test_spider'\n+\n         crawler = get_crawler(settings_dict=settings)\n         spider = TestSpider.from_crawler(crawler)\n         return spider\n@@ -390,23 +394,63 @@ class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n     pass\n \n \n-class FeedExportTest(unittest.TestCase):\n+class FeedExportTestBase(ABC, unittest.TestCase):\n+    __test__ = False\n \n     class MyItem(scrapy.Item):\n         foo = scrapy.Field()\n         egg = scrapy.Field()\n         baz = scrapy.Field()\n \n+    def _random_temp_filename(self, inter_dir=''):\n+        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n+        filename = ''.join(chars)\n+        return os.path.join(self.temp_dir, inter_dir, filename)\n+\n     def setUp(self):\n         self.temp_dir = tempfile.mkdtemp()\n \n     def tearDown(self):\n         shutil.rmtree(self.temp_dir, ignore_errors=True)\n \n-    def _random_temp_filename(self):\n-        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n-        filename = ''.join(chars)\n-        return os.path.join(self.temp_dir, filename)\n+    @defer.inlineCallbacks\n+    def exported_data(self, items, settings):\n+        \"\"\"\n+        Return exported data which a spider yielding ``items`` would return.\n+        \"\"\"\n+\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+\n+            def parse(self, response):\n+                for item in items:\n+                    yield item\n+\n+        data = yield self.run_and_export(TestSpider, settings)\n+        defer.returnValue(data)\n+\n+    @defer.inlineCallbacks\n+    def exported_no_data(self, settings):\n+        \"\"\"\n+        Return exported data which a spider yielding no ``items`` would return.\n+        \"\"\"\n+\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+\n+            def parse(self, response):\n+                pass\n+\n+        data = yield self.run_and_export(TestSpider, settings)\n+        defer.returnValue(data)\n+\n+    @abstractmethod\n+    def run_and_export(self, spider_cls, settings):\n+        pass\n+\n+\n+class FeedExportTest(FeedExportTestBase):\n+    __test__ = True\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -417,7 +461,6 @@ class FeedExportTest(unittest.TestCase):\n             urljoin('file:', pathname2url(str(file_path))): feed\n             for file_path, feed in FEEDS.items()\n         }\n-\n         content = {}\n         try:\n             with MockServer() as s:\n@@ -435,35 +478,6 @@ class FeedExportTest(unittest.TestCase):\n \n         defer.returnValue(content)\n \n-    @defer.inlineCallbacks\n-    def exported_data(self, items, settings):\n-        \"\"\"\n-        Return exported data which a spider yielding ``items`` would return.\n-        \"\"\"\n-        class TestSpider(scrapy.Spider):\n-            name = 'testspider'\n-\n-            def parse(self, response):\n-                for item in items:\n-                    yield item\n-\n-        data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n-\n-    @defer.inlineCallbacks\n-    def exported_no_data(self, settings):\n-        \"\"\"\n-        Return exported data which a spider yielding no ``items`` would return.\n-        \"\"\"\n-        class TestSpider(scrapy.Spider):\n-            name = 'testspider'\n-\n-            def parse(self, response):\n-                pass\n-\n-        data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n-\n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n@@ -970,3 +984,112 @@ class FeedExportTest(unittest.TestCase):\n         }\n         data = yield self.exported_no_data(settings)\n         self.assertEqual(data['csv'], b'')\n+\n+\n+class PartialDeliveriesTest(FeedExportTestBase):\n+    __test__ = True\n+    _file_mark = '_%(time)s_#%(batch_id)s'\n+\n+    @defer.inlineCallbacks\n+    def run_and_export(self, spider_cls, settings):\n+        \"\"\" Run spider with specified settings; return exported data. \"\"\"\n+\n+        FEEDS = settings.get('FEEDS') or {}\n+        settings['FEEDS'] = {\n+            urljoin('file:', file_path): feed\n+            for file_path, feed in FEEDS.items()\n+        }\n+        from collections import defaultdict\n+        content = defaultdict(list)\n+        try:\n+            with MockServer() as s:\n+                runner = CrawlerRunner(Settings(settings))\n+                spider_cls.start_urls = [s.url('/')]\n+                yield runner.crawl(spider_cls)\n+\n+            for path, feed in FEEDS.items():\n+                dir_name = os.path.dirname(path)\n+                for file in sorted(os.listdir(dir_name)):\n+                    with open(os.path.join(dir_name, file), 'rb') as f:\n+                        data = f.read()\n+                        content[feed['format']].append(data)\n+        finally:\n+            pass\n+        defer.returnValue(content)\n+\n+    @defer.inlineCallbacks\n+    def assertPartialExported(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n+            },\n+        })\n+        data = yield self.exported_data(items, settings)\n+        data['jl'] = b''.join(data['jl'])\n+        parsed = [json.loads(to_unicode(line)) for line in data['jl'].splitlines()]\n+\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        self.assertEqual(rows, parsed)\n+\n+    @defer.inlineCallbacks\n+    def test_partial_deliveries(self):\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+        ]\n+        rows = [\n+            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n+            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'},\n+            {'foo': 'bar3', 'baz': 'quux3'}\n+        ]\n+        settings = {\n+            'FEED_STORAGE_BATCH_SIZE': 1\n+        }\n+        yield self.assertPartialExported(items, rows, settings=settings)\n+\n+    def test_wrong_path(self):\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'xml'},\n+            },\n+            'FEED_STORAGE_BATCH_SIZE': 1\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        self.assertRaises(NotConfigured, FeedExporter, crawler)\n+\n+    @defer.inlineCallbacks\n+    def test_export_no_items_not_store_empty(self):\n+        for fmt in ('json', 'jsonlines', 'xml', 'csv'):\n+            settings = {\n+                'FEEDS': {\n+                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n+                },\n+                'FEED_STORAGE_BATCH_SIZE': 1\n+            }\n+            data = yield self.exported_no_data(settings)\n+            data[fmt] = b''.join(data[fmt])\n+            self.assertEqual(data[fmt], b'')\n+\n+    @defer.inlineCallbacks\n+    def test_export_no_items_store_empty(self):\n+        formats = (\n+            ('json', b'[]'),\n+            ('jsonlines', b''),\n+            ('xml', b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n+            ('csv', b''),\n+        )\n+\n+        for fmt, expctd in formats:\n+            settings = {\n+                'FEEDS': {\n+                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n+                },\n+                'FEED_STORE_EMPTY': True,\n+                'FEED_EXPORT_INDENT': None,\n+                'FEED_STORAGE_BATCH_SIZE': 1\n+            }\n+            data = yield self.exported_no_data(settings)\n+            data[fmt] = b''.join(data[fmt])\n+            self.assertEqual(data[fmt], expctd)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffa8a533e74478a5c81fbf453f2c65601bb1d244", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 674 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -253,15 +253,12 @@ class FeedExporter:\n \n     def open_spider(self, spider):\n         for uri, feed in self.feeds.items():\n-            batch_id = 1\n-            uri_params = self._get_uri_params(spider, feed['uri_params'])\n-            uri_params['batch_id'] = batch_id\n+            uri_params = self._get_uri_params(spider, feed['uri_params'], None)\n             self.slots.append(self._start_new_batch(\n                 previous_batch_slot=None,\n                 uri=uri % uri_params,\n                 feed=feed,\n                 spider=spider,\n-                batch_id=batch_id,\n                 template_uri=uri,\n             ))\n \n@@ -286,7 +283,7 @@ class FeedExporter:\n             deferred_list.append(d)\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n-    def _start_new_batch(self, previous_batch_slot, uri, feed, spider, batch_id, template_uri):\n+    def _start_new_batch(self, previous_batch_slot, uri, feed, spider, template_uri):\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if 'FEED_STORAGE_BATCH' setting is specified.\n@@ -295,12 +292,15 @@ class FeedExporter:\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n-        :param batch_id: sequential batch id starting at 1\n         :param template_uri: template uri which contains %(time)s or %(batch_id)s to create new uri\n         \"\"\"\n         if previous_batch_slot is not None:\n+            previous_batch_id = previous_batch_slot.batch_id\n             previous_batch_slot.exporter.finish_exporting()\n             previous_batch_slot.storage.store(previous_batch_slot.file)\n+        else:\n+            previous_batch_id = 0\n+\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n         exporter = self._get_exporter(\n@@ -317,7 +317,7 @@ class FeedExporter:\n             uri=uri,\n             format=feed['format'],\n             store_empty=feed['store_empty'],\n-            batch_id=batch_id,\n+            batch_id=previous_batch_id + 1,\n             template_uri=template_uri,\n         )\n         if slot.store_empty:\n@@ -331,15 +331,12 @@ class FeedExporter:\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             if self.storage_batch_size and slot.itemcount % self.storage_batch_size == 0:\n-                batch_id = slot.batch_id + 1\n-                uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'])\n-                uri_params['batch_id'] = batch_id\n+                uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'], slot)\n                 slots.append(self._start_new_batch(\n                     previous_batch_slot=slot,\n                     uri=slot.template_uri % uri_params,\n                     feed=self.feeds[slot.template_uri],\n                     spider=spider,\n-                    batch_id=batch_id,\n                     template_uri=slot.template_uri,\n                 ))\n                 self.slots[idx] = None\n@@ -396,12 +393,12 @@ class FeedExporter:\n     def _get_storage(self, uri):\n         return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n \n-    def _get_uri_params(self, spider, uri_params):\n+    def _get_uri_params(self, spider, uri_params, slot):\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n-        ts = datetime.utcnow().isoformat().replace(':', '-')\n-        params['time'] = ts\n+        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n+        params['time'] = datetime.utcnow().isoformat().replace(':', '-')\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n         return params\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#963580463b96315eb58319e6d35b4cd52672371a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 44 | Files Changed: 1 | Hunks: 25 | Methods Changed: 19 | Complexity Δ (Sum/Max): 36/36 | Churn Δ: 203 | Churn Cumulative: 2056 | Contributors (this commit): 21 | Commits (past 90d): 7 | Contributors (cumulative): 21 | DMM Complexity: 0.28421052631578947\n\nDIFF:\n@@ -7,6 +7,7 @@ import string\n import tempfile\n import warnings\n from abc import ABC, abstractmethod\n+from collections import defaultdict\n from io import BytesIO\n from pathlib import Path\n from string import ascii_letters, digits\n@@ -444,10 +445,31 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         data = yield self.run_and_export(TestSpider, settings)\n         defer.returnValue(data)\n \n+    @defer.inlineCallbacks\n+    def assertExported(self, items, header, rows, settings=None, ordered=True):\n+        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n+        yield self.assertExportedJsonLines(items, rows, settings)\n+        yield self.assertExportedXml(items, rows, settings)\n+        yield self.assertExportedPickle(items, rows, settings)\n+        yield self.assertExportedMarshal(items, rows, settings)\n+        yield self.assertExportedMultiple(items, rows, settings)\n+\n     @abstractmethod\n     def run_and_export(self, spider_cls, settings):\n         pass\n \n+    def _load_until_eof(self, data, load_func):\n+        result = []\n+        with tempfile.TemporaryFile() as temp:\n+            temp.write(data)\n+            temp.seek(0)\n+            while True:\n+                try:\n+                    result.append(load_func(temp))\n+                except EOFError:\n+                    break\n+        return result\n+\n \n class FeedExportTest(FeedExportTestBase):\n     __test__ = True\n@@ -478,6 +500,22 @@ class FeedExportTest(FeedExportTestBase):\n \n         defer.returnValue(content)\n \n+    @defer.inlineCallbacks\n+    def exported_data(self, items, settings):\n+        \"\"\"\n+        Return exported data which a spider yielding ``items`` would return.\n+        \"\"\"\n+\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+\n+            def parse(self, response):\n+                for item in items:\n+                    yield item\n+\n+        data = yield self.run_and_export(TestSpider, settings)\n+        defer.returnValue(data)\n+\n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n@@ -543,18 +581,6 @@ class FeedExportTest(FeedExportTestBase):\n         json_rows = json.loads(to_unicode(data['json']))\n         self.assertEqual(rows, json_rows)\n \n-    def _load_until_eof(self, data, load_func):\n-        result = []\n-        with tempfile.TemporaryFile() as temp:\n-            temp.write(data)\n-            temp.seek(0)\n-            while True:\n-                try:\n-                    result.append(load_func(temp))\n-                except EOFError:\n-                    break\n-        return result\n-\n     @defer.inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n@@ -583,15 +609,6 @@ class FeedExportTest(FeedExportTestBase):\n         result = self._load_until_eof(data['marshal'], load_func=marshal.load)\n         self.assertEqual(expected, result)\n \n-    @defer.inlineCallbacks\n-    def assertExported(self, items, header, rows, settings=None, ordered=True):\n-        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n-        yield self.assertExportedJsonLines(items, rows, settings)\n-        yield self.assertExportedXml(items, rows, settings)\n-        yield self.assertExportedPickle(items, rows, settings)\n-        yield self.assertExportedMarshal(items, rows, settings)\n-        yield self.assertExportedMultiple(items, rows, settings)\n-\n     @defer.inlineCallbacks\n     def test_export_items(self):\n         # feed exporters use field names from Item\n@@ -615,7 +632,7 @@ class FeedExportTest(FeedExportTestBase):\n                 },\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(data[fmt], b'')\n+            self.assertEqual(b'', data[fmt])\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -635,7 +652,7 @@ class FeedExportTest(FeedExportTestBase):\n                 'FEED_EXPORT_INDENT': None,\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(data[fmt], expctd)\n+            self.assertEqual(expctd, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n@@ -734,7 +751,8 @@ class FeedExportTest(FeedExportTestBase):\n         formats = {\n             'json': u'[{\"foo\": \"Test\\\\u00d6\"}]'.encode('utf-8'),\n             'jsonlines': u'{\"foo\": \"Test\\\\u00d6\"}\\n'.encode('utf-8'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('utf-8'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode(\n+                'utf-8'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n         }\n \n@@ -751,7 +769,8 @@ class FeedExportTest(FeedExportTestBase):\n         formats = {\n             'json': u'[{\"foo\": \"Test\\xd6\"}]'.encode('latin-1'),\n             'jsonlines': u'{\"foo\": \"Test\\xd6\"}\\n'.encode('latin-1'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('latin-1'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode(\n+                'latin-1'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n         }\n \n@@ -772,7 +791,8 @@ class FeedExportTest(FeedExportTestBase):\n \n         formats = {\n             'json': u'[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode('latin-1'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode(\n+                'latin-1'),\n             'csv': u'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n         }\n \n@@ -988,7 +1008,7 @@ class FeedExportTest(FeedExportTestBase):\n \n class PartialDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n-    _file_mark = '_%(time)s_#%(batch_id)s'\n+    _file_mark = '_%(time)s_#%(batch_id)s_'\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -999,7 +1019,6 @@ class PartialDeliveriesTest(FeedExportTestBase):\n             urljoin('file:', file_path): feed\n             for file_path, feed in FEEDS.items()\n         }\n-        from collections import defaultdict\n         content = defaultdict(list)\n         try:\n             with MockServer() as s:\n@@ -1014,26 +1033,120 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                         data = f.read()\n                         content[feed['format']].append(data)\n         finally:\n-            pass\n+            self.tearDown()\n         defer.returnValue(content)\n \n     @defer.inlineCallbacks\n-    def assertPartialExported(self, items, rows, settings=None):\n+    def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n                 os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n             },\n         })\n-        data = yield self.exported_data(items, settings)\n-        data['jl'] = b''.join(data['jl'])\n-        parsed = [json.loads(to_unicode(line)) for line in data['jl'].splitlines()]\n-\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n-        self.assertEqual(rows, parsed)\n+        data = yield self.exported_data(items, settings)\n+        for batch in data['jl']:\n+            got_batch = [json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()]\n+            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n-    def test_partial_deliveries(self):\n+    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n+            },\n+        })\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        data = yield self.exported_data(items, settings)\n+        for batch in data['csv']:\n+            got_batch = csv.DictReader(to_unicode(batch).splitlines())\n+            self.assertEqual(list(header), got_batch.fieldnames)\n+            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n+            self.assertEqual(expected_batch, list(got_batch))\n+\n+    @defer.inlineCallbacks\n+    def assertExportedXml(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n+            },\n+        })\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        data = yield self.exported_data(items, settings)\n+        for batch in data['xml']:\n+            root = lxml.etree.fromstring(batch)\n+            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n+\n+    @defer.inlineCallbacks\n+    def assertExportedMultiple(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n+                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n+            },\n+        })\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        data = yield self.exported_data(items, settings)\n+        # XML\n+        xml_rows = rows.copy()\n+        for batch in data['xml']:\n+            root = lxml.etree.fromstring(batch)\n+            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+            expected_batch, xml_rows = xml_rows[:batch_size], xml_rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n+        # JSON\n+        json_rows = rows.copy()\n+        for batch in data['json']:\n+            got_batch = json.loads(batch)\n+            expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n+\n+    @defer.inlineCallbacks\n+    def assertExportedPickle(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n+            },\n+        })\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        data = yield self.exported_data(items, settings)\n+        import pickle\n+        for batch in data['pickle']:\n+            got_batch = self._load_until_eof(batch, load_func=pickle.load)\n+            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n+\n+    @defer.inlineCallbacks\n+    def assertExportedMarshal(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n+            },\n+        })\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        data = yield self.exported_data(items, settings)\n+        import marshal\n+        for batch in data['marshal']:\n+            got_batch = self._load_until_eof(batch, load_func=marshal.load)\n+            expected_batch, rows = rows[:batch_size], rows[batch_size:]\n+            self.assertEqual(expected_batch, got_batch)\n+\n+    @defer.inlineCallbacks\n+    def test_export_items(self):\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n             self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n@@ -1042,14 +1155,16 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         rows = [\n             {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n             {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'},\n-            {'foo': 'bar3', 'baz': 'quux3'}\n+            {'foo': 'bar3', 'baz': 'quux3', 'egg': ''}\n         ]\n         settings = {\n-            'FEED_STORAGE_BATCH_SIZE': 1\n+            'FEED_STORAGE_BATCH_SIZE': 2\n         }\n-        yield self.assertPartialExported(items, rows, settings=settings)\n+        header = self.MyItem.fields.keys()\n+        yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n+        \"\"\"If path without %(time)s or %(batch_id)s an exception must be raised\"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n@@ -1069,8 +1184,8 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 'FEED_STORAGE_BATCH_SIZE': 1\n             }\n             data = yield self.exported_no_data(settings)\n-            data[fmt] = b''.join(data[fmt])\n-            self.assertEqual(data[fmt], b'')\n+            data = dict(data)\n+            self.assertEqual(b'', data[fmt][0])\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -1088,8 +1203,8 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 },\n                 'FEED_STORE_EMPTY': True,\n                 'FEED_EXPORT_INDENT': None,\n-                'FEED_STORAGE_BATCH_SIZE': 1\n+                'FEED_STORAGE_BATCH_SIZE': 1,\n             }\n             data = yield self.exported_no_data(settings)\n-            data[fmt] = b''.join(data[fmt])\n-            self.assertEqual(data[fmt], expctd)\n+            data = dict(data)\n+            self.assertEqual(expctd, data[fmt][0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
