{"custom_id": "scrapy#e6ab8bc9a5460c5af29a330598f7afa5a6efb7e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 4 | Churn Cumulative: 962 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -183,8 +183,8 @@ class HttpTestCase(unittest.TestCase):\n     download_handler_cls = HTTPDownloadHandler\n \n     # only used for HTTPS tests\n-    keyfile = 'keys/cert.pem'\n-    certfile = 'keys/cert.pem'\n+    keyfile = 'keys/localhost.key'\n+    certfile = 'keys/localhost.crt'\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d14e392f1096d94669c00456855b64164dba6bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 188 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -203,7 +203,7 @@ class MockServer():\n         time.sleep(0.2)\n \n \n-def ssl_context_factory(keyfile='keys/cert.pem', certfile='keys/cert.pem'):\n+def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt'):\n     return ssl.DefaultOpenSSLContextFactory(\n          os.path.join(os.path.dirname(__file__), keyfile),\n          os.path.join(os.path.dirname(__file__), certfile),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63b8caf5debf84e8da7f299782d15d0a41bf8a14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 14 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 620 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -466,28 +466,23 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'json',\n                 'indent': -1,\n-                'expected': b\"\"\"\n-[\n+                'expected': b\"\"\"[\n {\"foo\": [\"bar\"]},\n {\"key\": \"value\"}\n-]\n-\"\"\",\n+]\"\"\",\n             },\n             {\n                 'format': 'json',\n                 'indent': 0,\n-                'expected': b\"\"\"\n-[\n+                'expected': b\"\"\"[\n {\"foo\": [\"bar\"]},\n {\"key\": \"value\"}\n-]\n-\"\"\",\n+]\"\"\",\n             },\n             {\n                 'format': 'json',\n                 'indent': 2,\n-                'expected': b\"\"\"\n-[\n+                'expected': b\"\"\"[\n {\n   \"foo\": [\n     \"bar\"\n@@ -501,8 +496,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'json',\n                 'indent': 4,\n-                'expected': b\"\"\"\n-[\n+                'expected': b\"\"\"[\n {\n     \"foo\": [\n         \"bar\"\n@@ -516,8 +510,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'json',\n                 'indent': 5,\n-                'expected': b\"\"\"\n-[\n+                'expected': b\"\"\"[\n {\n      \"foo\": [\n           \"bar\"\n@@ -533,15 +526,13 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': None,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>\"\"\",\n             },\n             {\n                 'format': 'xml',\n                 'indent': -1,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n <item><foo><value>bar</value></foo></item>\n <item><key>value</key></item>\n@@ -550,8 +541,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': 0,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n <item><foo><value>bar</value></foo></item>\n <item><key>value</key></item>\n@@ -560,8 +550,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': 2,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n   <item>\n     <foo>\n@@ -576,8 +565,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': 4,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n     <item>\n         <foo>\n@@ -592,8 +580,7 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': 5,\n-                'expected': b\"\"\"\n-<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+                'expected': b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n      <item>\n           <foo>\n@@ -611,4 +598,4 @@ class FeedExportTest(unittest.TestCase):\n             settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT': row['indent']}\n             data = yield self.exported_data(items, settings)\n             print(row['format'], row['indent'])\n-            self.assertEqual(row['expected'].strip(), data)\n+            self.assertEqual(row['expected'], data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26f723e4e63185b35b2f11285f50cfc3ac52b8fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 785 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n         if clickable and clickable[0] not in formdata and not clickable[0] is None:\n             values.append(clickable)\n \n-    values.extend(formdata.items())\n+    values.extend((k, v) for k, v in formdata.items() if v is not None)\n     return values\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffef828a8deb86520e3bd6a50d76b2a4ecf3ae71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 609 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -425,6 +425,17 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'one'], [b'1'])\n         self.assertEqual(fs[b'two'], [b'2'])\n \n+    def test_from_response_drop_params(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"get.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"3\">\n+            </form>\"\"\")\n+        req = self.request_class.from_response(response, formdata={'two': None})\n+        fs = _qs(req)\n+        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertNotIn(b'two', fs)\n+\n     def test_from_response_override_method(self):\n         response = _buildresponse(\n                 '''<html><body>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df7a5c4aa4f5898de3c70cef17c3c5031f7e05a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,4 @@\n+from scrapy.cmdline import execute\n+\n+if __name__ == '__main__':\n+    execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b74b98fa3e7149c27bd3a541940457864a28d5d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 524 | Contributors (this commit): 36 | Commits (past 90d): 8 | Contributors (cumulative): 39 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,7 +35,6 @@ class MemoryUsage(object):\n         self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n         self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n-        self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n         self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n         self.mail = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n\n@@ -219,7 +219,6 @@ MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\n MEMUSAGE_ENABLED = True\n MEMUSAGE_LIMIT_MB = 0\n MEMUSAGE_NOTIFY_MAIL = []\n-MEMUSAGE_REPORT = False\n MEMUSAGE_WARNING_MB = 0\n \n METAREFRESH_ENABLED = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a452c038cc0547924051a7cd0786215ec7c2104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 166 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -56,9 +56,9 @@ def _embed_standard_shell(namespace={}, banner=''):\n     return wrapper\n \n DEFAULT_PYTHON_SHELLS = OrderedDict([\n+    ('ptpython', _embed_ptpython_shell),\n     ('ipython', _embed_ipython_shell),\n     ('bpython', _embed_bpython_shell),\n-    ('ptpython', _embed_ptpython_shell),\n     ('python', _embed_standard_shell),\n ])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c8dc158697453b610f168f97ce94945892163674", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 1557 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool, ResponseFailed\n+    HTTPConnectionPool, ResponseFailed, URI\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n@@ -228,10 +228,33 @@ class TunnelingAgent(Agent):\n             headers, bodyProducer, requestPath)\n \n \n+class ScrapyProxyAgent(Agent):\n+\n+    def __init__(self, reactor, proxyURI,\n+                 connectTimeout=None, bindAddress=None, pool=None):\n+        super(ScrapyProxyAgent, self).__init__(reactor,\n+                                               connectTimeout=connectTimeout,\n+                                               bindAddress=bindAddress,\n+                                               pool=pool)\n+        self._proxyURI = URI.fromBytes(proxyURI)\n+\n+    def request(self, method, uri, headers=None, bodyProducer=None):\n+        \"\"\"\n+        Issue a new request via the configured proxy.\n+        \"\"\"\n+        # Cache *all* connections under the same key, since we are only\n+        # connecting to a single destination, the proxy:\n+        proxyEndpoint = self._getEndpoint(self._proxyURI)\n+        key = (\"http-proxy\", self._proxyURI.host, self._proxyURI.port)\n+        return self._requestWithEndpoint(key, proxyEndpoint, method,\n+                                         URI.fromBytes(uri), headers,\n+                                         bodyProducer, uri)\n+\n+\n class ScrapyAgent(object):\n \n     _Agent = Agent\n-    _ProxyAgent = ProxyAgent\n+    _ProxyAgent = ScrapyProxyAgent\n     _TunnelingAgent = TunnelingAgent\n \n     def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n@@ -260,9 +283,8 @@ class ScrapyAgent(object):\n                     contextFactory=self._contextFactory, connectTimeout=timeout,\n                     bindAddress=bindaddress, pool=self._pool)\n             else:\n-                endpoint = TCP4ClientEndpoint(reactor, proxyHost, proxyPort,\n-                    timeout=timeout, bindAddress=bindaddress)\n-                return self._ProxyAgent(endpoint)\n+                return self._ProxyAgent(reactor, proxyURI=proxy,\n+                    connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n \n         return self._Agent(reactor, contextFactory=self._contextFactory,\n             connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60727dedf605fad2ed4be844cb2ec44e305257f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 24 | Churn Cumulative: 871 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -27,6 +27,10 @@ class Request(object_ref):\n         assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n         self.priority = priority\n \n+        if callback is not None and not callable(callback):\n+            raise TypeError('callback must be a function, got %s' % type(callback).__name__)\n+        if errback is not None and not callable(errback):\n+            raise TypeError('errback must be a function, got %s' % type(errback).__name__)\n         assert callback or not errback, \"Cannot use errback without a callback\"\n         self.callback = callback\n         self.errback = errback\n\n@@ -235,6 +235,26 @@ class RequestTest(unittest.TestCase):\n         self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n         self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n \n+    def test_callback_is_callable(self):\n+        def a_function():\n+            pass\n+        r = self.request_class('http://example.com')\n+        self.assertIsNone(r.callback)\n+        r = self.request_class('http://example.com', a_function)\n+        self.assertIs(r.callback, a_function)\n+        with self.assertRaises(TypeError):\n+            self.request_class('http://example.com', 'a_function')\n+\n+    def test_errback_is_callable(self):\n+        def a_function():\n+            pass\n+        r = self.request_class('http://example.com')\n+        self.assertIsNone(r.errback)\n+        r = self.request_class('http://example.com', a_function, errback=a_function)\n+        self.assertIs(r.errback, a_function)\n+        with self.assertRaises(TypeError):\n+            self.request_class('http://example.com', a_function, errback='a_function')\n+\n \n class FormRequestTest(RequestTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e162c1ff40bf74802c1a9f7b93fdbb56a5ff4a13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1559 | Contributors (this commit): 16 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -283,7 +283,7 @@ class ScrapyAgent(object):\n                     contextFactory=self._contextFactory, connectTimeout=timeout,\n                     bindAddress=bindaddress, pool=self._pool)\n             else:\n-                return self._ProxyAgent(reactor, proxyURI=proxy,\n+                return self._ProxyAgent(reactor, proxyURI=to_bytes(proxy, encoding='ascii'),\n                     connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n \n         return self._Agent(reactor, contextFactory=self._contextFactory,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b092c66809ec11245d03063961ca64e488580e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1570 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,11 @@ from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool, ResponseFailed, URI\n+    HTTPConnectionPool, ResponseFailed\n+try:\n+    from twisted.web.client import URI\n+except ImportError:\n+    from twisted.web.client import _URI as URI\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n@@ -244,7 +248,12 @@ class ScrapyProxyAgent(Agent):\n         \"\"\"\n         # Cache *all* connections under the same key, since we are only\n         # connecting to a single destination, the proxy:\n+        if twisted_version >= (15, 0, 0):\n             proxyEndpoint = self._getEndpoint(self._proxyURI)\n+        else:\n+            proxyEndpoint = self._getEndpoint(self._proxyURI.scheme,\n+                                              self._proxyURI.host,\n+                                              self._proxyURI.port)\n         key = (\"http-proxy\", self._proxyURI.host, self._proxyURI.port)\n         return self._requestWithEndpoint(key, proxyEndpoint, method,\n                                          URI.fromBytes(uri), headers,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f6f486769d7761054274509de08fcf455d492de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 609 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -142,7 +142,8 @@ class Command(ScrapyCommand):\n                 logger.error('Unable to find spider for: %(url)s',\n                              {'url': url})\n \n-        request = Request(url, opts.callback)\n+        # Request requires callback argument as callable or None, not string\n+        request = Request(url, None)\n         _start_requests = lambda s: [self.prepare_request(s, request, opts)]\n         self.spidercls.start_requests = _start_requests\n \n@@ -164,7 +165,9 @@ class Command(ScrapyCommand):\n             # determine real callback\n             cb = response.meta['_callback']\n             if not cb:\n-                if opts.rules and self.first_response == response:\n+                if opts.callback:\n+                    cb = opts.callback\n+                elif opts.rules and self.first_response == response:\n                     cb = self.get_callback_from_rules(spider, response)\n \n                     if not cb:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b6f68b9ee2830534c25b824103b13d502005fe1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 39 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -17,8 +17,8 @@ class RequestSerializationTest(unittest.TestCase):\n \n     def test_all_attributes(self):\n         r = Request(\"http://www.example.com\",\n-            callback='parse_item',\n-            errback='handle_error',\n+            callback=self.spider.parse_item,\n+            errback=self.spider.handle_error,\n             method=\"POST\",\n             body=b\"some body\",\n             headers={'content-encoding': 'text/html; charset=latin-1'},\n@@ -27,7 +27,7 @@ class RequestSerializationTest(unittest.TestCase):\n             priority=20,\n             meta={'a': 'b'},\n             flags=['testFlag'])\n-        self._assert_serializes_ok(r)\n+        self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_latin1_body(self):\n         r = Request(\"http://www.example.com\", body=b\"\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b33e0d5a54a90588e02396e7a2162d4ea12ae7dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 1034 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,7 +135,7 @@ class TextResponse(Response):\n         * an attribute Selector (not SelectorList) - e.g.\n           ``response.css('a::attr(href)')[0]`` or\n           ``response.xpath('//img/@src')[0]``.\n-        * a Selector for ``<a>`` element, e.g.\n+        * a Selector for ``<a>`` or ``<link>`` element, e.g.\n           ``response.css('a.my_link')[0]``.\n           \n         See :ref:`response-follow-example` for usage examples.\n@@ -165,10 +165,11 @@ def _url_from_selector(sel):\n         return strip_html5_whitespace(sel.root)\n     if not hasattr(sel.root, 'tag'):\n         raise ValueError(\"Unsupported selector: %s\" % sel)\n-    if sel.root.tag != 'a':\n-        raise ValueError(\"Only <a> elements are supported; got <%s>\" %\n+    if sel.root.tag not in ('a', 'link'):\n+        raise ValueError(\"Only <a> and <link> elements are supported; got <%s>\" %\n                          sel.root.tag)\n     href = sel.root.get('href')\n     if href is None:\n-        raise ValueError(\"<a> element has no href attribute: %s\" % sel)\n+        raise ValueError(\"<%s> element has no href attribute: %s\" %\n+                         (sel.root.tag, sel))\n     return strip_html5_whitespace(href)\n\n@@ -162,7 +162,6 @@ class BaseResponseTest(unittest.TestCase):\n     def test_follow_whitespace_link(self):\n         self._assert_followed_url(Link('http://example.com/foo '),\n                                   'http://example.com/foo%20')\n-\n     def _assert_followed_url(self, follow_obj, target_url, response=None):\n         if response is None:\n             response = self._links_response()\n@@ -402,6 +401,13 @@ class TextResponseTest(BaseResponseTest):\n             for sel, url in zip(sellist, urls):\n                 self._assert_followed_url(sel, url, response=resp)\n \n+        # select <link> elements\n+        self._assert_followed_url(\n+            Selector(text='<link href=\"foo\"></link>').css('link')[0],\n+            'http://example.com/foo',\n+            response=resp\n+        )\n+\n         # href attributes should work\n         for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n             for sel, url in zip(sellist, urls):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a08cf3b9606bf77ef02a37dca1e2bc76f74558f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 615 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ import optparse\n import cProfile\n import inspect\n import pkg_resources\n+import gc\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -165,4 +166,9 @@ def _run_command_profiled(cmd, args, opts):\n         p.dump_stats(opts.profile)\n \n if __name__ == '__main__':\n+    try:\n         execute()\n+    finally:\n+        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect()\n+        # on exit: http://doc.pypy.org/en/latest/cpython_differences.html?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n+        gc.collect()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6014856df5717496b57aaac6c8a2e64b125ac32b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 155 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         il = TestItemLoader()\n         il.add_value('name', [u'$10'])\n         try:\n-            float('$10')\n+            float(u'$10')\n         except Exception as e:\n             expected_exc_str = str(e)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3d17659b33432ba8cd2e5b2da57105c1cc0da22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 52 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import pickle\n+\n from queuelib.tests import test_queue as t\n from scrapy.squeues import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue\n from scrapy.item import Item, Field\n@@ -14,6 +16,22 @@ class TestLoader(ItemLoader):\n     default_item_class = TestItem\n     name_out = staticmethod(_test_procesor)\n \n+def nonserializable_object_test(self):\n+    try:\n+        pickle.dumps(lambda x: x)\n+    except Exception:\n+        # Trigger Twisted bug #7989\n+        import twisted.persisted.styles  # NOQA\n+        q = self.queue()\n+        self.assertRaises(ValueError, q.push, lambda x: x)\n+    else:\n+        # Use a different unpickleable object\n+        class A(object): pass\n+        a = A()\n+        a.__reduce__ = a.__reduce_ex__ = None\n+        q = self.queue()\n+        self.assertRaises(ValueError, q.push, a)\n+\n class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n \n     chunksize = 100000\n@@ -30,11 +48,7 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n         self.assertEqual(q.pop(), 123)\n         self.assertEqual(q.pop(), {'a': 'dict'})\n \n-    def test_nonserializable_object(self):\n-        # Trigger Twisted bug #7989\n-        import twisted.persisted.styles  # NOQA\n-        q = self.queue()\n-        self.assertRaises(ValueError, q.push, lambda x: x)\n+    test_nonserializable_object = nonserializable_object_test\n \n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n@@ -110,11 +124,7 @@ class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n         self.assertEqual(q.pop(), 123)\n         self.assertEqual(q.pop(), 'a')\n \n-    def test_nonserializable_object(self):\n-        # Trigger Twisted bug #7989\n-        import twisted.persisted.styles  # NOQA\n-        q = self.queue()\n-        self.assertRaises(ValueError, q.push, lambda x: x)\n+    test_nonserializable_object = nonserializable_object_test\n \n \n class PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5abb70c8d71365adbf3a035b1f6d07f5fbbbf446", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 189 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import gc\n import functools\n import operator\n import unittest\n@@ -144,6 +145,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertNotEqual(v, wk[_Weakme()])\n         self.assertEqual(v, wk[k])\n         del k\n+        for _ in range(100):\n+            if wk._weakdict:\n+                gc.collect()\n         self.assertFalse(len(wk._weakdict))\n \n     @unittest.skipUnless(six.PY2, \"deprecated function\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c67047e77914dfe0d6666e4dc535c684aa77090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 196 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import functools\n import operator\n import unittest\n from itertools import count\n+import platform\n import six\n \n from scrapy.utils.python import (\n@@ -212,10 +213,16 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(object), [])\n \n+        if platform.python_implementation() == 'CPython':\n             # TODO: how do we fix this to return the actual argument names?\n             self.assertEqual(get_func_args(six.text_type.split), [])\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n+        else:\n+            self.assertEqual(get_func_args(six.text_type.split), ['sep', 'maxsplit'])\n+            self.assertEqual(get_func_args(\" \".join), ['list'])\n+            self.assertEqual(get_func_args(operator.itemgetter(2)), ['obj'])\n+\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19ca986aa1340b08658caaac0ce677aa22be9814", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 24 | Churn Cumulative: 1208 | Contributors (this commit): 17 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ import optparse\n import cProfile\n import inspect\n import pkg_resources\n-import gc\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -12,6 +11,7 @@ from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n+from scrapy.utils.python import garbage_collect\n from scrapy.settings.deprecated import check_deprecated_settings\n \n def _iter_command_classes(module_name):\n@@ -171,4 +171,4 @@ if __name__ == '__main__':\n     finally:\n         # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect()\n         # on exit: http://doc.pypy.org/en/latest/cpython_differences.html?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n-        gc.collect()\n+        garbage_collect()\n\n@@ -1,6 +1,7 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n+import gc\n import os\n import re\n import inspect\n@@ -8,6 +9,8 @@ import weakref\n import errno\n import six\n from functools import partial, wraps\n+import sys\n+import time\n \n from scrapy.utils.decorators import deprecated\n \n@@ -355,3 +358,20 @@ def global_object_name(obj):\n     'scrapy.http.request.Request'\n     \"\"\"\n     return \"%s.%s\" % (obj.__module__, obj.__name__)\n+\n+\n+if sys.platform.startswith('java'):\n+    def garbage_collect():\n+        # Some JVM GCs will execute finalizers in a different thread, meaning\n+        # we need to wait for that to complete before we go on looking for the\n+        # effects of that.\n+        gc.collect()\n+        time.sleep(0.1)\n+elif hasattr(sys, \"pypy_version_info\"):\n+    def garbage_collect():\n+        # Collecting weakreferences can take two collections on PyPy.\n+        gc.collect()\n+        gc.collect()\n+else:\n+    def garbage_collect():\n+        gc.collect()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8df0900713a1a56cffca8774bc137a6e781877e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 165 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.python import to_bytes, to_unicode, garbage_collect\n \n \n logger = logging.getLogger(__name__)\n@@ -362,6 +362,7 @@ class LeveldbCacheStorage(object):\n         # avoid them being removed in storages with timestamp-based autoremoval.\n         self.db.CompactRange()\n         del self.db\n+        garbage_collect()\n \n     def retrieve_response(self, spider, request):\n         data = self._read_data(spider, request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea08b952801b9af9bc45ae9918fcf5ab9bcb5aaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 598 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -360,14 +360,7 @@ def global_object_name(obj):\n     return \"%s.%s\" % (obj.__module__, obj.__name__)\n \n \n-if sys.platform.startswith('java'):\n-    def garbage_collect():\n-        # Some JVM GCs will execute finalizers in a different thread, meaning\n-        # we need to wait for that to complete before we go on looking for the\n-        # effects of that.\n-        gc.collect()\n-        time.sleep(0.1)\n-elif hasattr(sys, \"pypy_version_info\"):\n+if hasattr(sys, \"pypy_version_info\"):\n     def garbage_collect():\n         # Collecting weakreferences can take two collections on PyPy.\n         gc.collect()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0a9236357dd74381b5a014e3de15a3a52de4f7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 450 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,31 @@\n from os.path import dirname, join\n-from setuptools import setup, find_packages\n+from pkg_resources import parse_version\n+from setuptools import setup, find_packages, __version__ as setuptools_version\n \n \n with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:\n     version = f.read().decode('ascii').strip()\n \n \n+def has_environment_marker_platform_impl_support():\n+    \"\"\"Code extracted from 'pytest/setup.py'\n+    https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31\n+\n+    The first known release to support environment marker with range operators\n+    it is 18.5, see:\n+    https://setuptools.readthedocs.io/en/latest/history.html#id235\n+    \"\"\"\n+    return parse_version(setuptools_version) >= parse_version('18.5')\n+\n+\n+extras_require = {}\n+\n+if has_environment_marker_platform_impl_support():\n+    extras_require[':platform_python_implementation == \"PyPy\"'] = [\n+        'PyPyDispatcher>=2.1.0',\n+    ]\n+\n+\n setup(\n     name='Scrapy',\n     version=version,\n@@ -53,4 +73,5 @@ setup(\n         'PyDispatcher>=2.0.5',\n         'service_identity',\n     ],\n+    extras_require=extras_require,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#793b2376f83caa01d3e883f7642147de8e174c01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 799 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -164,7 +164,7 @@ class Shell(object):\n \n def inspect_response(response, spider):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n-    Shell(spider.crawler).start(response=response)\n+    Shell(spider.crawler).start(response=response, spider=spider)\n \n \n def _request_deferred(request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f08d9a64884a05a541fe9649e3349cce5aa47be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 561 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -226,6 +226,27 @@ class MySpider(scrapy.Spider):\n         self.assertNotIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n \n+    def test_runspider_dnscache_disabled(self):\n+        # see https://github.com/scrapy/scrapy/issues/2811\n+        # The spider below should not be able to connect to localhost:12345,\n+        # which is intended,\n+        # but this should not be because of DNS lookup error\n+        # assumption: localhost will resolve in all cases (true?)\n+        log = self.get_log(\"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+    start_urls = ['http://localhost:12345']\n+\n+    def parse(self, response):\n+        return {'test': 'value'}\n+\"\"\",\n+                           args=('-s', 'DNSCACHE_ENABLED=False'))\n+        print(log)\n+        self.assertNotIn(\"DNSLookupError\", log)\n+        self.assertIn(\"INFO: Spider opened\", log)\n+\n     def test_runspider_log_short_names(self):\n         log1 = self.get_log(self.debug_log_spider,\n                             args=('-s', 'LOG_SHORT_NAMES=1'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0ded6b7759c9c2ec4b8946cdfc4abab97bb14d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 57 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ class CachingThreadedResolver(ThreadedResolver):\n         # to enforce Scrapy's DNS_TIMEOUT setting's value\n         timeout = (self.timeout,)\n         d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n+        if dnscache.limit:\n             d.addCallback(self._cache_result, name)\n         return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb0bd691d9dd42725af6291a15e357f0146de17f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 246 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -28,9 +28,9 @@ class Request(object_ref):\n         self.priority = priority\n \n         if callback is not None and not callable(callback):\n-            raise TypeError('callback must be a function, got %s' % type(callback).__name__)\n+            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n         if errback is not None and not callable(errback):\n-            raise TypeError('errback must be a function, got %s' % type(errback).__name__)\n+            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n         assert callback or not errback, \"Cannot use errback without a callback\"\n         self.callback = callback\n         self.errback = errback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33dfac50185b1940dda89bec3e3480a7e76e9ca7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 73 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class QPSSpider(Spider):\n \n         slots = int(self.slots)\n         if slots > 1:\n-            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in xrange(slots)]\n+            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in range(slots)]\n         else:\n             urls = [url]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#815d6160cfac5c0b764d01fc9d4ebc4ab7793aac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 14/8 | Churn Δ: 133 | Churn Cumulative: 1444 | Contributors (this commit): 23 | Commits (past 90d): 39 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,7 +21,7 @@ from w3lib.url import file_uri_to_path\n from scrapy import signals\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import without_none_values\n from scrapy.utils.boto import is_botocore\n@@ -181,6 +181,7 @@ class FeedExporter(object):\n     @classmethod\n     def from_crawler(cls, crawler):\n         o = cls(crawler.settings)\n+        o.crawler = crawler\n         crawler.signals.connect(o.open_spider, signals.spider_opened)\n         crawler.signals.connect(o.close_spider, signals.spider_closed)\n         crawler.signals.connect(o.item_scraped, signals.item_scraped)\n@@ -253,11 +254,16 @@ class FeedExporter(object):\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                          {'scheme': scheme})\n \n+    def _get_instance(self, objcls, *args, **kwargs):\n+        return create_instance(\n+            objcls, self.settings, getattr(self, 'crawler', None),\n+            *args, **kwargs)\n+\n     def _get_exporter(self, *args, **kwargs):\n-        return self.exporters[self.format](*args, **kwargs)\n+        return self._get_instance(self.exporters[self.format], *args, **kwargs)\n \n     def _get_storage(self, uri):\n-        return self.storages[urlparse(uri).scheme](uri)\n+        return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n \n     def _get_uri_params(self, spider):\n         params = {}\n\n@@ -3,7 +3,7 @@ import logging\n import pprint\n \n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n \n logger = logging.getLogger(__name__)\n@@ -32,12 +32,7 @@ class MiddlewareManager(object):\n         for clspath in mwlist:\n             try:\n                 mwcls = load_object(clspath)\n-                if crawler and hasattr(mwcls, 'from_crawler'):\n-                    mw = mwcls.from_crawler(crawler)\n-                elif hasattr(mwcls, 'from_settings'):\n-                    mw = mwcls.from_settings(settings)\n-                else:\n-                    mw = mwcls()\n+                mw = create_instance(mwcls, settings, crawler)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n\n@@ -117,3 +117,27 @@ def md5sum(file):\n def rel_has_nofollow(rel):\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n     return True if rel is not None and 'nofollow' in rel.split() else False\n+\n+def create_instance(objcls, settings, crawler, *args, **kwargs):\n+    \"\"\"Construct a class instance using its ``from_crawler`` or\n+    ``from_settings`` constructors, if available.\n+\n+    At least one of ``settings`` and ``crawler`` needs to be different from\n+    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n+    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n+    tried.\n+\n+    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n+\n+    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n+    \"\"\"\n+    if settings is None:\n+        if crawler is None:\n+            raise ValueError(\"Specifiy at least one of settings and crawler.\")\n+        settings = crawler.settings\n+    if crawler and hasattr(objcls, 'from_crawler'):\n+        return objcls.from_crawler(crawler, *args, **kwargs)\n+    elif hasattr(objcls, 'from_settings'):\n+        return objcls.from_settings(settings, *args, **kwargs)\n+    else:\n+        return objcls(*args, **kwargs)\n\n@@ -16,6 +16,7 @@ from tests.mockserver import MockServer\n from w3lib.url import path_to_file_uri\n \n import scrapy\n+from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n     IFeedStorage, FileFeedStorage, FTPFeedStorage,\n     S3FeedStorage, StdoutFeedStorage,\n@@ -159,6 +160,23 @@ class StdoutFeedStorageTest(unittest.TestCase):\n         self.assertEqual(out.getvalue(), b\"content\")\n \n \n+class FromCrawlerMixin(object):\n+    init_with_crawler = False\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        cls.init_with_crawler = True\n+        return cls(*args, **kwargs)\n+\n+\n+class FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):\n+    pass\n+\n+\n+class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n+    pass\n+\n+\n class FeedExportTest(unittest.TestCase):\n \n     class MyItem(scrapy.Item):\n@@ -599,3 +617,15 @@ class FeedExportTest(unittest.TestCase):\n             data = yield self.exported_data(items, settings)\n             print(row['format'], row['indent'])\n             self.assertEqual(row['expected'], data)\n+\n+    @defer.inlineCallbacks\n+    def test_init_exporters_storages_with_crawler(self):\n+        settings = {\n+            'FEED_EXPORTERS': {'csv': 'tests.test_feedexport.'\n+                                      'FromCrawlerCsvItemExporter'},\n+            'FEED_STORAGES': {'file': 'tests.test_feedexport.'\n+                                      'FromCrawlerFileFeedStorage'},\n+        }\n+        yield self.exported_data({}, settings)\n+        self.assertTrue(FromCrawlerCsvItemExporter.init_with_crawler)\n+        self.assertTrue(FromCrawlerFileFeedStorage.init_with_crawler)\n\n@@ -3,7 +3,9 @@ import os\n import unittest\n \n from scrapy.item import Item, Field\n-from scrapy.utils.misc import load_object, arg_to_iter, walk_modules\n+from scrapy.utils.misc import arg_to_iter, create_instance, load_object, walk_modules\n+\n+from tests import mock\n \n __doctests__ = ['scrapy.utils.misc']\n \n@@ -74,5 +76,59 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertEqual(list(arg_to_iter({'a':1})), [{'a': 1}])\n         self.assertEqual(list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")])\n \n+    def test_create_instance(self):\n+        settings = mock.MagicMock()\n+        crawler = mock.MagicMock(spec_set=['settings'])\n+        args = (True, 100.)\n+        kwargs = {'key': 'val'}\n+\n+        def _test_with_settings(mock, settings):\n+            create_instance(mock, settings, None, *args, **kwargs)\n+            if hasattr(mock, 'from_crawler'):\n+                self.assertEqual(mock.from_crawler.call_count, 0)\n+            if hasattr(mock, 'from_settings'):\n+                mock.from_settings.assert_called_once_with(settings, *args,\n+                                                           **kwargs)\n+                self.assertEqual(mock.call_count, 0)\n+            else:\n+                mock.assert_called_once_with(*args, **kwargs)\n+\n+        def _test_with_crawler(mock, settings, crawler):\n+            create_instance(mock, settings, crawler, *args, **kwargs)\n+            if hasattr(mock, 'from_crawler'):\n+                mock.from_crawler.assert_called_once_with(crawler, *args,\n+                                                          **kwargs)\n+                if hasattr(mock, 'from_settings'):\n+                    self.assertEqual(mock.from_settings.call_count, 0)\n+                self.assertEqual(mock.call_count, 0)\n+            elif hasattr(mock, 'from_settings'):\n+                mock.from_settings.assert_called_once_with(settings, *args,\n+                                                           **kwargs)\n+                self.assertEqual(mock.call_count, 0)\n+            else:\n+                mock.assert_called_once_with(*args, **kwargs)\n+\n+        # Check usage of correct constructor using four mocks:\n+        #   1. with no alternative constructors\n+        #   2. with from_settings() constructor\n+        #   3. with from_crawler() constructor\n+        #   4. with from_settings() and from_crawler() constructor\n+        spec_sets = ([], ['from_settings'], ['from_crawler'],\n+                     ['from_settings', 'from_crawler'])\n+        for specs in spec_sets:\n+            m = mock.MagicMock(spec_set=specs)\n+            _test_with_settings(m, settings)\n+            m.reset_mock()\n+            _test_with_crawler(m, settings, crawler)\n+\n+        # Check adoption of crawler settings\n+        m = mock.MagicMock(spec_set=['from_settings'])\n+        create_instance(m, None, crawler, *args, **kwargs)\n+        m.from_settings.assert_called_once_with(crawler.settings, *args,\n+                                                **kwargs)\n+\n+        with self.assertRaises(ValueError):\n+            create_instance(m, None, None)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
