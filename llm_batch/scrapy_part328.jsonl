{"custom_id": "scrapy#8284de5e7613c47e69d40d4f6a2a1dc846b50dd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 21 | Files Changed: 12 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 6095 | Contributors (this commit): 36 | Commits (past 90d): 20 | Contributors (cumulative): 105 | DMM Complexity: None\n\nDIFF:\n@@ -121,7 +121,7 @@ def feed_complete_default_values_from_settings(feed, settings):\n     out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n     out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n-    out.setdefault(\"item_export_kwargs\", dict())\n+    out.setdefault(\"item_export_kwargs\", {})\n     if settings[\"FEED_EXPORT_INDENT\"] is None:\n         out.setdefault(\"indent\", None)\n     else:\n\n@@ -79,7 +79,7 @@ def create_deprecated_class(\n         # for implementation details\n         def __instancecheck__(cls, inst):\n             return any(cls.__subclasscheck__(c)\n-                       for c in {type(inst), inst.__class__})\n+                       for c in (type(inst), inst.__class__))\n \n         def __subclasscheck__(cls, sub):\n             if cls is not DeprecatedClass.deprecated_class:\n\n@@ -40,9 +40,9 @@ def generate_keys():\n \n     subject = issuer = Name(\n         [\n-            NameAttribute(NameOID.COUNTRY_NAME, u\"IE\"),\n-            NameAttribute(NameOID.ORGANIZATION_NAME, u\"Scrapy\"),\n-            NameAttribute(NameOID.COMMON_NAME, u\"localhost\"),\n+            NameAttribute(NameOID.COUNTRY_NAME, \"IE\"),\n+            NameAttribute(NameOID.ORGANIZATION_NAME, \"Scrapy\"),\n+            NameAttribute(NameOID.COMMON_NAME, \"localhost\"),\n         ]\n     )\n     cert = (\n@@ -54,7 +54,7 @@ def generate_keys():\n         .not_valid_before(datetime.utcnow())\n         .not_valid_after(datetime.utcnow() + timedelta(days=10))\n         .add_extension(\n-            SubjectAlternativeName([DNSName(u\"localhost\")]),\n+            SubjectAlternativeName([DNSName(\"localhost\")]),\n             critical=False,\n         )\n         .sign(key, SHA256(), default_backend())\n\n@@ -219,7 +219,7 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     certfile = 'keys/localhost.crt'\n \n     scheme = 'https'\n-    host = u'127.0.0.1'\n+    host = '127.0.0.1'\n \n     expected_http_proxy_request_body = b'/'\n \n\n@@ -152,7 +152,7 @@ class CrawlerRun:\n         self.itemerror = []\n         self.itemresp = []\n         self.headers = {}\n-        self.bytes = defaultdict(lambda: list())\n+        self.bytes = defaultdict(list)\n         self.signals_caught = {}\n         self.spider_class = spider_class\n \n\n@@ -362,14 +362,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item=dict(text=u'W\\u0275\\u200Brd'),\n+                item=dict(text='W\\u0275\\u200Brd'),\n                 expected=None,\n                 encoding='windows-1251',\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item=dict(text=u'W\\u0275\\u200Brd'),\n+            item=dict(text='W\\u0275\\u200Brd'),\n             include_headers_line=False,\n             expected='W&#629;&#8203;rd\\r\\n',\n             encoding='windows-1251',\n\n@@ -201,7 +201,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         self.site = Site(root, timeout=None)\n \n         # Start server for testing\n-        self.hostname = u'localhost'\n+        self.hostname = 'localhost'\n         context_factory = ssl_context_factory(self.key_file, self.certificate_file)\n \n         server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n\n@@ -703,7 +703,7 @@ class DeprecatedUtilityFunctionsTestCase(unittest.TestCase):\n             return None\n \n         with warnings.catch_warnings(record=True) as w:\n-            wrap_loader_context(function, context=dict())\n+            wrap_loader_context(function, context={})\n \n             assert len(w) == 1\n             assert issubclass(w[0].category, ScrapyDeprecationWarning)\n\n@@ -57,7 +57,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         },\n     }\n \n-    checks = list()\n+    checks = []\n \n     def start_requests(self):\n         data = {'key': 'value', 'number': 123}\n\n@@ -22,7 +22,7 @@ MockSlot = collections.namedtuple('MockSlot', ['active'])\n \n class MockDownloader:\n     def __init__(self):\n-        self.slots = dict()\n+        self.slots = {}\n \n     def _get_slot_key(self, request, spider):\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n@@ -31,7 +31,7 @@ class MockDownloader:\n         return urlparse_cached(request).hostname or ''\n \n     def increment(self, slot_key):\n-        slot = self.slots.setdefault(slot_key, MockSlot(active=list()))\n+        slot = self.slots.setdefault(slot_key, MockSlot(active=[]))\n         slot.active.append(1)\n \n     def decrement(self, slot_key):\n@@ -114,7 +114,7 @@ class BaseSchedulerInMemoryTester(SchedulerHandler):\n         for url, priority in _PRIORITIES:\n             self.scheduler.enqueue_request(Request(url, priority=priority))\n \n-        priorities = list()\n+        priorities = []\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n@@ -167,7 +167,7 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         self.close_scheduler()\n         self.create_scheduler()\n \n-        priorities = list()\n+        priorities = []\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n@@ -259,7 +259,7 @@ class DownloaderAwareSchedulerTestMixin:\n             self.close_scheduler()\n             self.create_scheduler()\n \n-        dequeued_slots = list()\n+        dequeued_slots = []\n         requests = []\n         downloader = self.mock_crawler.engine.downloader\n         while self.scheduler.has_pending_requests():\n\n@@ -176,7 +176,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"store_empty\": True,\n             \"uri_params\": (1, 2, 3, 4),\n             \"batch_item_count\": 2,\n-            \"item_export_kwargs\": dict(),\n+            \"item_export_kwargs\": {},\n         })\n \n     def test_feed_complete_default_values_from_settings_non_empty(self):\n@@ -199,7 +199,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"store_empty\": True,\n             \"uri_params\": None,\n             \"batch_item_count\": 2,\n-            \"item_export_kwargs\": dict(),\n+            \"item_export_kwargs\": {},\n         })\n \n \n\n@@ -27,7 +27,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n     def test_load_object_exceptions(self):\n         self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n-        self.assertRaises(TypeError, load_object, dict())\n+        self.assertRaises(TypeError, load_object, {})\n \n     def test_walk_modules(self):\n         mods = walk_modules('tests.test_utils_misc.test_walk_modules')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#890f884de46602352de48fa844edd9959c62e473", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1597 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -156,7 +156,7 @@ class Scraper:\n             callback = result.request.callback or spider._parse\n             warn_on_generator_with_return_value(spider, callback)\n             dfd = defer_succeed(result)\n-            dfd.addCallback(callback, **result.request.cb_kwargs)\n+            dfd.addCallbacks(callback=callback, callbackKeywords=result.request.cb_kwargs)\n         else:  # result is a Failure\n             result.request = request\n             warn_on_generator_with_return_value(spider, request.errback)\n\n@@ -60,7 +60,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n     checks = []\n \n     def start_requests(self):\n-        data = {'key': 'value', 'number': 123}\n+        data = {'key': 'value', 'number': 123, 'callback': 'some_callback'}\n         yield Request(self.mockserver.url('/first'), self.parse_first, cb_kwargs=data)\n         yield Request(self.mockserver.url('/general_with'), self.parse_general, cb_kwargs=data)\n         yield Request(self.mockserver.url('/general_without'), self.parse_general)\n@@ -88,7 +88,8 @@ class KeywordArgumentsSpider(MockServerSpider):\n         if response.url.endswith('/general_with'):\n             self.checks.append(kwargs['key'] == 'value')\n             self.checks.append(kwargs['number'] == 123)\n-            self.crawler.stats.inc_value('boolean_checks', 2)\n+            self.checks.append(kwargs['callback'] == 'some_callback')\n+            self.crawler.stats.inc_value('boolean_checks', 3)\n         elif response.url.endswith('/general_without'):\n             self.checks.append(kwargs == {})\n             self.crawler.stats.inc_value('boolean_checks')\n@@ -110,7 +111,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         TypeError: parse_takes_less() got an unexpected keyword argument 'number'\n         \"\"\"\n \n-    def parse_takes_more(self, response, key, number, other):\n+    def parse_takes_more(self, response, key, number, callback, other):\n         \"\"\"\n         Should raise\n         TypeError: parse_takes_more() missing 1 required positional argument: 'other'\n@@ -161,11 +162,13 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n         self.assertTrue(\n             str(exceptions['takes_less'].exc_info[1]).endswith(\n                 \"parse_takes_less() got an unexpected keyword argument 'number'\"\n-            )\n+            ),\n+            msg=\"Exception message: \" + str(exceptions['takes_less'].exc_info[1]),\n         )\n         self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n         self.assertTrue(\n             str(exceptions['takes_more'].exc_info[1]).endswith(\n                 \"parse_takes_more() missing 1 required positional argument: 'other'\"\n-            )\n+            ),\n+            msg=\"Exception message: \" + str(exceptions['takes_more'].exc_info[1]),\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b081f18a2f232a1bfd04c829ae6894cac93a89a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/9 | Churn Δ: 106 | Churn Cumulative: 133 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.847457627118644\n\nDIFF:\n@@ -3,10 +3,14 @@ HTTP basic auth downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n+import warnings\n \n from w3lib.http import basic_auth_header\n \n from scrapy import signals\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.url import url_is_from_any_domain\n \n \n class HttpAuthMiddleware:\n@@ -24,8 +28,23 @@ class HttpAuthMiddleware:\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n+            if not hasattr(spider, 'http_auth_domain'):\n+                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '\n+                              'problems if the spider makes requests to several different domains. http_auth_domain '\n+                              'will be set to the domain of the first request, please set it to the correct value '\n+                              'explicitly.',\n+                              category=ScrapyDeprecationWarning)\n+                self.domain_unset = True\n+            else:\n+                self.domain = spider.http_auth_domain\n+                self.domain_unset = False\n \n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n         if auth and b'Authorization' not in request.headers:\n+            domain = urlparse_cached(request).hostname\n+            if self.domain_unset:\n+                self.domain = domain\n+                self.domain_unset = False\n+            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                 request.headers[b'Authorization'] = auth\n\n@@ -1,13 +1,60 @@\n import unittest\n \n+from w3lib.http import basic_auth_header\n+\n from scrapy.http import Request\n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spiders import Spider\n \n \n+class TestSpiderLegacy(Spider):\n+    http_user = 'foo'\n+    http_pass = 'bar'\n+\n+\n class TestSpider(Spider):\n     http_user = 'foo'\n     http_pass = 'bar'\n+    http_auth_domain = 'example.com'\n+\n+\n+class TestSpiderAny(Spider):\n+    http_user = 'foo'\n+    http_pass = 'bar'\n+    http_auth_domain = None\n+\n+\n+class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.spider = TestSpiderLegacy('foo')\n+\n+    def test_auth(self):\n+        mw = HttpAuthMiddleware()\n+        mw.spider_opened(self.spider)\n+\n+        # initial request, sets the domain and sends the header\n+        req = Request('http://example.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+        # subsequent request to the same domain, should send the header\n+        req = Request('http://example.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+        # subsequent request to a different domain, shouldn't send the header\n+        req = Request('http://example-noauth.com/')\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertNotIn('Authorization', req.headers)\n+\n+    def test_auth_already_set(self):\n+        mw = HttpAuthMiddleware()\n+        mw.spider_opened(self.spider)\n+        req = Request('http://example.com/',\n+                      headers=dict(Authorization='Digest 123'))\n+        assert mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n \n \n class HttpAuthMiddlewareTest(unittest.TestCase):\n@@ -20,13 +67,45 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n     def tearDown(self):\n         del self.mw\n \n-    def test_auth(self):\n-        req = Request('http://scrapytest.org/')\n+    def test_no_auth(self):\n+        req = Request('http://example-noauth.com/')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')\n+        self.assertNotIn('Authorization', req.headers)\n+\n+    def test_auth_domain(self):\n+        req = Request('http://example.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+    def test_auth_subdomain(self):\n+        req = Request('http://foo.example.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n \n     def test_auth_already_set(self):\n-        req = Request('http://scrapytest.org/',\n+        req = Request('http://example.com/',\n+                      headers=dict(Authorization='Digest 123'))\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n+\n+\n+class HttpAuthAnyMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.mw = HttpAuthMiddleware()\n+        self.spider = TestSpiderAny('foo')\n+        self.mw.spider_opened(self.spider)\n+\n+    def tearDown(self):\n+        del self.mw\n+\n+    def test_auth(self):\n+        req = Request('http://example.com/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))\n+\n+    def test_auth_already_set(self):\n+        req = Request('http://example.com/',\n                       headers=dict(Authorization='Digest 123'))\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers['Authorization'], b'Digest 123')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c858cec91b013853a73e6215b74c90b609bc2da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 541 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,8 +80,8 @@ class CookiesMiddleware:\n                     logger.warning(msg.format(request, cookie, key))\n                     return\n                 continue\n-            if isinstance(cookie[key], str):\n-                decoded[key] = cookie[key]\n+            if isinstance(cookie[key], (bool, float, int, str)):\n+                decoded[key] = str(cookie[key])\n             else:\n                 try:\n                     decoded[key] = cookie[key].decode(\"utf8\")\n\n@@ -347,3 +347,24 @@ class CookiesMiddlewareTest(TestCase):\n         self.assertCookieValEqual(req1.headers['Cookie'], 'key=value1')\n         self.assertCookieValEqual(req2.headers['Cookie'], 'key=value2')\n         self.assertCookieValEqual(req3.headers['Cookie'], 'key=')\n+\n+    def test_primitive_type_cookies(self):\n+        # Boolean\n+        req1 = Request('http://example.org', cookies={'a': True})\n+        assert self.mw.process_request(req1, self.spider) is None\n+        self.assertCookieValEqual(req1.headers['Cookie'], b'a=True')\n+\n+        # Float\n+        req2 = Request('http://example.org', cookies={'a': 9.5})\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertCookieValEqual(req2.headers['Cookie'], b'a=9.5')\n+\n+        # Integer\n+        req3 = Request('http://example.org', cookies={'a': 10})\n+        assert self.mw.process_request(req3, self.spider) is None\n+        self.assertCookieValEqual(req3.headers['Cookie'], b'a=10')\n+\n+        # String\n+        req4 = Request('http://example.org', cookies={'a': 'b'})\n+        assert self.mw.process_request(req4, self.spider) is None\n+        self.assertCookieValEqual(req4.headers['Cookie'], b'a=b')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#029cab72e8a9b20ebb6b9540e9e01747f0e83dba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 240 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -105,7 +105,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         self.checks.append(default == 99)\n         self.crawler.stats.inc_value('boolean_checks', 4)\n \n-    def parse_takes_less(self, response, key):\n+    def parse_takes_less(self, response, key, callback):\n         \"\"\"\n         Should raise\n         TypeError: parse_takes_less() got an unexpected keyword argument 'number'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3f1bf79e883fe3662df827ee47cfc93a372ff02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 55 | Files Changed: 12 | Hunks: 36 | Methods Changed: 19 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 106 | Churn Cumulative: 6996 | Contributors (this commit): 81 | Commits (past 90d): 20 | Contributors (cumulative): 182 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,7 +29,7 @@ twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n # Check minimum required Python version\n if sys.version_info < (3, 6):\n-    print(\"Scrapy %s requires Python 3.6+\" % __version__)\n+    print(f\"Scrapy {__version__} requires Python 3.6+\")\n     sys.exit(1)\n \n \n\n@@ -135,11 +135,13 @@ def load_context_factory_from_settings(settings, crawler):\n             settings=settings,\n             crawler=crawler,\n         )\n-        msg = \"\"\"\n-            '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n-            e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n-            Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n-            settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n+        msg = (\n+            f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n+            \"a `method` argument (type OpenSSL.SSL method, e.g. \"\n+            \"OpenSSL.SSL.SSLv23_METHOD) and/or a `tls_verbose_logging` \"\n+            \"argument and/or a `tls_ciphers` argument. Please, upgrade your \"\n+            \"context factory class to handle them or ignore them.\"\n+        )\n         warnings.warn(msg)\n \n     return context_factory\n\n@@ -65,14 +65,14 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n                 verifyHostname(connection, self._hostnameASCII)\n             except (CertificateError, VerificationError) as e:\n                 logger.warning(\n-                    'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n-                        self._hostnameASCII, e))\n+                    'Remote certificate is not valid for hostname \"%s\"; %s',\n+                    self._hostnameASCII, e)\n \n             except ValueError as e:\n                 logger.warning(\n                     'Ignoring error while verifying certificate '\n-                    'from host \"{}\" (exception: {})'.format(\n-                        self._hostnameASCII, repr(e)))\n+                    'from host \"%s\" (exception: %r)',\n+                    self._hostnameASCII, e)\n \n \n DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')\n\n@@ -70,7 +70,7 @@ class HttpCacheMiddleware:\n             self.stats.inc_value('httpcache/miss', spider=spider)\n             if self.ignore_missing:\n                 self.stats.inc_value('httpcache/ignore', spider=spider)\n-                raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n+                raise IgnoreRequest(f\"Ignored request not in cache: {request}\")\n             return None  # first time request\n \n         # Return cached response only if not expired\n\n@@ -38,11 +38,10 @@ def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n         kwargs['feed_options'] = feed_options\n     else:\n         warnings.warn(\n-            \"{} does not support the 'feed_options' keyword argument. Add a \"\n+            f\"{builder.__qualname__} does not support the 'feed_options' keyword argument. Add a \"\n             \"'feed_options' parameter to its signature to remove this \"\n             \"warning. This parameter will become mandatory in a future \"\n-            \"version of Scrapy.\"\n-            .format(builder.__qualname__),\n+            \"version of Scrapy.\",\n             category=ScrapyDeprecationWarning\n         )\n     return builder(*preargs, uri, *args, **kwargs)\n@@ -356,32 +355,28 @@ class FeedExporter:\n             # properly closed.\n             return defer.maybeDeferred(slot.storage.store, slot.file)\n         slot.finish_exporting()\n-        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n-        log_args = {'format': slot.format,\n-                    'itemcount': slot.itemcount,\n-                    'uri': slot.uri}\n+        logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n \n-        # Use `largs=log_args` to copy log_args into function's scope\n-        # instead of using `log_args` from the outer scope\n         d.addCallback(\n-            self._handle_store_success, log_args, logfmt, spider, type(slot.storage).__name__\n+            self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n         )\n         d.addErrback(\n-            self._handle_store_error, log_args, logfmt, spider, type(slot.storage).__name__\n+            self._handle_store_error, logmsg, spider, type(slot.storage).__name__\n         )\n         return d\n \n-    def _handle_store_error(self, f, largs, logfmt, spider, slot_type):\n+    def _handle_store_error(self, f, logmsg, spider, slot_type):\n         logger.error(\n-            logfmt % \"Error storing\", largs,\n+            \"Error storing %s\", logmsg,\n             exc_info=failure_to_exc_info(f), extra={'spider': spider}\n         )\n         self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n \n-    def _handle_store_success(self, f, largs, logfmt, spider, slot_type):\n+    def _handle_store_success(self, f, logmsg, spider, slot_type):\n         logger.info(\n-            logfmt % \"Stored\", largs, extra={'spider': spider}\n+            \"Stored %s\", logmsg,\n+            extra={'spider': spider}\n         )\n         self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n \n@@ -474,10 +469,10 @@ class FeedExporter:\n         for uri_template, values in self.feeds.items():\n             if values['batch_item_count'] and not re.search(r'%\\(batch_time\\)s|%\\(batch_id\\)', uri_template):\n                 logger.error(\n-                    '%(batch_time)s or %(batch_id)d must be in the feed URI ({}) if FEED_EXPORT_BATCH_ITEM_COUNT '\n+                    '%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT '\n                     'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '\n-                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count'\n-                    ''.format(uri_template)\n+                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count',\n+                    uri_template\n                 )\n                 return False\n         return True\n@@ -526,7 +521,7 @@ class FeedExporter:\n             instance = build_instance(feedcls)\n             method_name = '__new__'\n         if instance is None:\n-            raise TypeError(\"%s.%s returned None\" % (feedcls.__qualname__, method_name))\n+            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n         return instance\n \n     def _get_uri_params(self, spider, uri_params, slot=None):\n\n@@ -226,7 +226,7 @@ class DbmCacheStorage:\n         dbpath = os.path.join(self.cachedir, f'{spider.name}.db')\n         self.db = self.dbmodule.open(dbpath, 'c')\n \n-        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n+        logger.debug(\"Using DBM cache storage in %(cachepath)s\", {'cachepath': dbpath}, extra={'spider': spider})\n \n     def close_spider(self, spider):\n         self.db.close()\n@@ -280,7 +280,7 @@ class FilesystemCacheStorage:\n         self._open = gzip.open if self.use_gzip else open\n \n     def open_spider(self, spider):\n-        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n+        logger.debug(\"Using filesystem cache storage in %(cachedir)s\", {'cachedir': self.cachedir},\n                      extra={'spider': spider})\n \n     def close_spider(self, spider):\n\n@@ -50,7 +50,7 @@ def load_object(path):\n             return path\n         else:\n             raise TypeError(\"Unexpected argument type, expected string \"\n-                            \"or object, got: %s\" % type(path))\n+                            f\"or object, got: {type(path)}\")\n \n     try:\n         dot = path.rindex('.')\n\n@@ -86,7 +86,7 @@ class SimpleSpider(MetaSpider):\n         self.start_urls = [url]\n \n     def parse(self, response):\n-        self.logger.info(\"Got response %d\" % response.status)\n+        self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefSpider(SimpleSpider):\n@@ -95,7 +95,7 @@ class AsyncDefSpider(SimpleSpider):\n \n     async def parse(self, response):\n         await defer.succeed(42)\n-        self.logger.info(\"Got response %d\" % response.status)\n+        self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefAsyncioSpider(SimpleSpider):\n@@ -105,7 +105,7 @@ class AsyncDefAsyncioSpider(SimpleSpider):\n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n         status = await get_from_asyncio_queue(response.status)\n-        self.logger.info(\"Got response %d\" % status)\n+        self.logger.info(f\"Got response {status}\")\n \n \n class AsyncDefAsyncioReturnSpider(SimpleSpider):\n@@ -115,7 +115,7 @@ class AsyncDefAsyncioReturnSpider(SimpleSpider):\n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n         status = await get_from_asyncio_queue(response.status)\n-        self.logger.info(\"Got response %d\" % status)\n+        self.logger.info(f\"Got response {status}\")\n         return [{'id': 1}, {'id': 2}]\n \n \n@@ -126,7 +126,7 @@ class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):\n     async def parse(self, response):\n         await asyncio.sleep(0.1)\n         status = await get_from_asyncio_queue(response.status)\n-        self.logger.info(\"Got response %d\" % status)\n+        self.logger.info(f\"Got response {status}\")\n         return {\"foo\": 42}\n \n \n@@ -138,7 +138,7 @@ class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n         await asyncio.sleep(0.2)\n         req_id = response.meta.get('req_id', 0)\n         status = await get_from_asyncio_queue(response.status)\n-        self.logger.info(\"Got response %d, req_id %d\" % (status, req_id))\n+        self.logger.info(f\"Got response {status}, req_id {req_id}\")\n         if req_id > 0:\n             return\n         reqs = []\n@@ -155,7 +155,7 @@ class AsyncDefAsyncioGenSpider(SimpleSpider):\n     async def parse(self, response):\n         await asyncio.sleep(0.2)\n         yield {'foo': 42}\n-        self.logger.info(\"Got response %d\" % response.status)\n+        self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n@@ -166,7 +166,7 @@ class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n         for i in range(10):\n             await asyncio.sleep(0.1)\n             yield {'foo': i}\n-        self.logger.info(\"Got response %d\" % response.status)\n+        self.logger.info(f\"Got response {response.status}\")\n \n \n class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n\n@@ -41,7 +41,7 @@ class TestCloseSpider(TestCase):\n         yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_errorcount')\n-        key = 'spider_exceptions/{name}'.format(name=crawler.spider.exception_cls.__name__)\n+        key = f'spider_exceptions/{crawler.spider.exception_cls.__name__}'\n         errorcount = crawler.stats.get_value(key)\n         self.assertTrue(errorcount >= close_on)\n \n\n@@ -498,7 +498,7 @@ class GenspiderCommandTest(CommandTest):\n                          self.find_in_file(join(self.proj_mod_path,\n                                                 'spiders', 'test_name.py'),\n                                            r'allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n-        self.assertEqual('http://%s/' % domain,\n+        self.assertEqual(f'http://{domain}/',\n                          self.find_in_file(join(self.proj_mod_path,\n                                                 'spiders', 'test_name.py'),\n                                            r'start_urls\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n@@ -708,7 +708,7 @@ class MySpider(scrapy.Spider):\n         ])\n         import asyncio\n         loop = asyncio.new_event_loop()\n-        self.assertIn(\"Using asyncio event loop: %s.%s\" % (loop.__module__, loop.__class__.__name__), log)\n+        self.assertIn(f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\", log)\n \n     def test_output(self):\n         spider_code = \"\"\"\n\n@@ -248,7 +248,7 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.body, b'/')\n \n-        http_proxy = '%s?noconnect' % self.getURL('')\n+        http_proxy = f\"{self.getURL('')}?noconnect\"\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n         with self.assertWarnsRegex(\n             Warning,\n\n@@ -1217,18 +1217,17 @@ class FormRequestTest(RequestTest):\n                           response, formcss=\"input[name='abc']\")\n \n     def test_from_response_valid_form_methods(self):\n-        body = \"\"\"<form action=\"post.php\" method=\"%s\">\n-            <input type=\"hidden\" name=\"one\" value=\"1\">\n-            </form>\"\"\"\n+        form_methods = [[method, method] for method in self.request_class.valid_form_methods]\n+        form_methods.append(['UNKNOWN', 'GET'])\n \n-        for method in self.request_class.valid_form_methods:\n-            response = _buildresponse(body % method)\n+        for method, expected in form_methods:\n+            response = _buildresponse(\n+                f'<form action=\"post.php\" method=\"{method}\">'\n+                '<input type=\"hidden\" name=\"one\" value=\"1\">'\n+                '</form>'\n+            )\n             r = self.request_class.from_response(response)\n-            self.assertEqual(r.method, method)\n-\n-        response = _buildresponse(body % 'UNKNOWN')\n-        r = self.request_class.from_response(response)\n-        self.assertEqual(r.method, 'GET')\n+            self.assertEqual(r.method, expected)\n \n \n def _buildresponse(body, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6fbd6f941f00037ae4718805352e0fa86a781e41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 11 | Churn Cumulative: 1841 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,9 @@ class ItemLoader(itemloaders.ItemLoader):\n \n     def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n         if selector is None and response is not None:\n+            try:\n                 selector = self.default_selector_class(response)\n+            except AttributeError:\n+                selector = None\n         context.update(response=response)\n         super().__init__(item=item, selector=selector, parent=parent, **context)\n\n@@ -4,7 +4,7 @@ import attr\n from itemadapter import ItemAdapter\n from itemloaders.processors import Compose, Identity, MapCompose, TakeFirst\n \n-from scrapy.http import HtmlResponse\n+from scrapy.http import HtmlResponse, Response\n from scrapy.item import Item, Field\n from scrapy.loader import ItemLoader\n from scrapy.selector import Selector\n@@ -305,6 +305,12 @@ class SelectortemLoaderTest(unittest.TestCase):\n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), ['Marta'])\n     \n+    def test_init_method_with_base_response(self):\n+        \"\"\"Selector should be None after initialization\"\"\"\n+        response = Response(\"https://scrapy.org\")\n+        l = TestItemLoader(response=response)\n+        self.assertIs(l.selector, None)\n+\n     def test_init_method_with_response(self):\n         l = TestItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a263280bad53a26490381f293a454be6c25ea30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 24 | Files Changed: 14 | Hunks: 22 | Methods Changed: 18 | Complexity Δ (Sum/Max): -48/0 | Churn Δ: 48 | Churn Cumulative: 5162 | Contributors (this commit): 66 | Commits (past 90d): 19 | Contributors (cumulative): 175 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-A spider that generate light requests to meassure QPS troughput\n+A spider that generate light requests to meassure QPS throughput\n \n usage:\n \n\n@@ -2,7 +2,7 @@\n An extension to retry failed requests that are potentially caused by temporary\n problems such as a connection timeout or HTTP 500 error.\n \n-You can change the behaviour of this middleware by modifing the scraping settings:\n+You can change the behaviour of this middleware by modifying the scraping settings:\n RETRY_TIMES - how many times to retry a failed page\n RETRY_HTTP_CODES - which HTTP response codes to retry\n \n\n@@ -30,7 +30,7 @@ class BaseItemExporter:\n         self._configure(kwargs, dont_fail=dont_fail)\n \n     def _configure(self, options, dont_fail=False):\n-        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n+        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses ``__init__`` methods)\n         \"\"\"\n\n@@ -88,7 +88,7 @@ class LxmlParserLinkExtractor:\n     def _process_links(self, links):\n         \"\"\" Normalize and filter extracted links\n \n-        The subclass should override it if neccessary\n+        The subclass should override it if necessary\n         \"\"\"\n         return self._deduplicate_if_needed(links)\n \n\n@@ -85,7 +85,7 @@ class S3FilesStore:\n     AWS_USE_SSL = None\n     AWS_VERIFY = None\n \n-    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n+    POLICY = 'private'  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n     HEADERS = {\n         'Cache-Control': 'max-age=172800',\n     }\n@@ -142,7 +142,7 @@ class S3FilesStore:\n             **extra)\n \n     def _headers_to_botocore_kwargs(self, headers):\n-        \"\"\" Convert headers to botocore keyword agruments.\n+        \"\"\" Convert headers to botocore keyword arguments.\n         \"\"\"\n         # This is required while we need to support both boto and botocore.\n         mapping = CaselessDict({\n@@ -190,7 +190,7 @@ class GCSFilesStore:\n     CACHE_CONTROL = 'max-age=172800'\n \n     # The bucket's default object ACL will be applied to the object.\n-    # Overriden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n+    # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n     POLICY = None\n \n     def __init__(self, uri):\n@@ -291,7 +291,7 @@ class FilesPipeline(MediaPipeline):\n     \"\"\"Abstract pipeline that implement the file downloading\n \n     This pipeline tries to minimize network transfers and file processing,\n-    doing stat of the files and determining if file is new, uptodate or\n+    doing stat of the files and determining if file is new, up-to-date or\n     expired.\n \n     ``new`` files are those that pipeline never processed and needs to be\n\n@@ -43,7 +43,7 @@ class XMLFeedSpider(Spider):\n         return response\n \n     def parse_node(self, response, selector):\n-        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n+        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n         if hasattr(self, 'parse_item'):  # backward compatibility\n             return self.parse_item(response, selector)\n         raise NotImplementedError\n@@ -113,7 +113,7 @@ class CSVFeedSpider(Spider):\n         return response\n \n     def parse_row(self, response, row):\n-        \"\"\"This method must be overriden with your custom spider functionality\"\"\"\n+        \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n         raise NotImplementedError\n \n     def parse_rows(self, response):\n\n@@ -14,7 +14,7 @@ def _embed_ipython_shell(namespace={}, banner=''):\n     @wraps(_embed_ipython_shell)\n     def wrapper(namespace=namespace, banner=''):\n         config = load_default_config()\n-        # Always use .instace() to ensure _instance propagation to all parents\n+        # Always use .instance() to ensure _instance propagation to all parents\n         # this is needed for <TAB> completion works well for new imports\n         # and clear the instance to always have the fresh env\n         # on repeated breaks like with inspect_response()\n\n@@ -41,7 +41,7 @@ class CaselessDict(dict):\n         return key.lower()\n \n     def normvalue(self, value):\n-        \"\"\"Method to normalize values prior to be setted\"\"\"\n+        \"\"\"Method to normalize values prior to be set\"\"\"\n         return value\n \n     def get(self, key, def_val=None):\n\n@@ -34,7 +34,7 @@ def defer_succeed(result) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n \n-    It delays by 100ms so reactor has a chance to go trough readers and writers\n+    It delays by 100ms so reactor has a chance to go through readers and writers\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     from twisted.internet import reactor\n\n@@ -48,7 +48,7 @@ def request_fingerprint(\n     the fingerprint.\n \n     For this reason, request headers are ignored by default when calculating\n-    the fingeprint. If you want to include specific headers use the\n+    the fingerprint. If you want to include specific headers use the\n     include_headers argument, which is a list of Request headers to include.\n \n     Also, servers usually ignore fragments in urls when handling requests,\n@@ -78,7 +78,7 @@ def request_fingerprint(\n \n \n def request_authenticate(request: Request, username: str, password: str) -> None:\n-    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n+    \"\"\"Authenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n     request.headers['Authorization'] = basic_auth_header(username, password)\n\n@@ -19,7 +19,7 @@ class BaseResponseTest(unittest.TestCase):\n     response_class = Response\n \n     def test_init(self):\n-        # Response requires url in the consturctor\n+        # Response requires url in the constructor\n         self.assertRaises(Exception, self.response_class)\n         self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n         self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n@@ -392,7 +392,7 @@ class TextResponseTest(BaseResponseTest):\n     def test_declared_encoding_invalid(self):\n         \"\"\"Check that unknown declared encodings are ignored\"\"\"\n         r = self.response_class(\"http://www.example.com\",\n-                                headers={\"Content-type\": [\"text/html; charset=UKNOWN\"]},\n+                                headers={\"Content-type\": [\"text/html; charset=UNKNOWN\"]},\n                                 body=b\"\\xc2\\xa3\")\n         self.assertEqual(r._declared_encoding(), None)\n         self._assert_response_values(r, 'utf-8', \"\\xa3\")\n\n@@ -106,9 +106,9 @@ class CrawlTestCase(TestCase):\n         \"\"\"\n         Downloader middleware which returns a response with an specific 'request' attribute.\n \n-        * The spider callback should receive the overriden response.request\n-        * Handlers listening to the response_received signal should receive the overriden response.request\n-        * The \"crawled\" log message should show the overriden response.request\n+        * The spider callback should receive the overridden response.request\n+        * Handlers listening to the response_received signal should receive the overridden response.request\n+        * The \"crawled\" log message should show the overridden response.request\n         \"\"\"\n         signal_params = {}\n \n@@ -144,7 +144,7 @@ class CrawlTestCase(TestCase):\n         An exception is raised but caught by the next middleware, which\n         returns a Response with a specific 'request' attribute.\n \n-        The spider callback should receive the overriden response.request\n+        The spider callback should receive the overridden response.request\n         \"\"\"\n         url = self.mockserver.url(\"/status?n=200\")\n         runner = CrawlerRunner(settings={\n\n@@ -23,7 +23,7 @@ class MustbeDeferredTest(unittest.TestCase):\n \n         dfd = mustbe_deferred(_append, 1)\n         dfd.addCallback(self.assertEqual, [1, 2])  # it is [1] with maybeDeferred\n-        steps.append(2)  # add another value, that should be catched by assertEqual\n+        steps.append(2)  # add another value, that should be caught by assertEqual\n         return dfd\n \n     def test_unfired_deferred(self):\n@@ -37,7 +37,7 @@ class MustbeDeferredTest(unittest.TestCase):\n \n         dfd = mustbe_deferred(_append, 1)\n         dfd.addCallback(self.assertEqual, [1, 2])  # it is [1] with maybeDeferred\n-        steps.append(2)  # add another value, that should be catched by assertEqual\n+        steps.append(2)  # add another value, that should be caught by assertEqual\n         return dfd\n \n \n\n@@ -36,7 +36,7 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n             self.assertEqual(result.read().decode('utf8'), rendered)\n \n         os.remove(render_path)\n-        assert not os.path.exists(render_path)  # Failure of test iself\n+        assert not os.path.exists(render_path)  # Failure of test itself\n \n \n if '__main__' == __name__:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5b057cfd4472e970b9b51757a1b823b2f585b09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen, _process_iterable_universal\n@@ -31,7 +32,10 @@ async def process_iterable(iterable):\n async def process_iterable_awaiting(iterable):\n     async for i in iterable:\n         yield i * 2\n-        await get_web_client_agent_req('http://example.com')\n+        d = Deferred()\n+        from twisted.internet import reactor\n+        reactor.callLater(0, d.callback, 42)\n+        await d\n \n \n class ProcessIterableUniversalTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a642b73e1a1ba355bae9d5b9d1e87a9da0696293", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 76 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen, _process_iterable_universal\n from scrapy.utils.defer import deferred_f_from_coro_f\n-from scrapy.utils.test import get_web_client_agent_req\n \n \n class AsyncgenUtilsTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
