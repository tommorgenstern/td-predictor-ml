{"custom_id": "keras#5dc27d0a986e3830ea720f9b8ee88dd1dbfd3601", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 148 | Churn Cumulative: 3107 | Contributors (this commit): 17 | Commits (past 90d): 25 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1265,11 +1265,10 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n         min_index = K.cast(min_index, 'int32')\n \n         # Compute sensitivity at that index.\n+        denom = self.true_positives[min_index] + self.false_negatives[min_index]\n         return K.switch(\n-            K.greater((self.true_positives[min_index] +\n-                       self.false_negatives[min_index]), 0),\n-            (self.true_positives[min_index] /\n-                (self.true_positives[min_index] + self.false_negatives[min_index])),\n+            K.greater(denom, 0),\n+            self.true_positives[min_index] / denom,\n             K.zeros_like(self.true_positives[min_index]))\n \n     def get_config(self):\n@@ -1281,6 +1280,79 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class SpecificityAtSensitivity(SensitivitySpecificityBase):\n+    \"\"\"Computes the specificity at a given sensitivity.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such (tp / (tp + fn)).\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such (tn / (tn + fp)).\n+\n+    This metric creates four local variables, `true_positives`, `true_negatives`,\n+    `false_positives` and `false_negatives` that are used to compute the\n+    specificity at the given sensitivity. The threshold for the given sensitivity\n+    value is computed and used to evaluate the corresponding specificity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    For additional information about specificity and sensitivity, see the\n+    following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n+\n+    Usage with the compile API:\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[keras.metrics.SpecificityAtSensitivity()])\n+    ```\n+\n+    # Arguments\n+        sensitivity: A scalar value in range `[0, 1]`.\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use for matching the given specificity.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, sensitivity, num_thresholds=200, name=None, dtype=None):\n+        if sensitivity < 0 or sensitivity > 1:\n+            raise ValueError('`sensitivity` must be in the range [0, 1].')\n+        self.sensitivity = sensitivity\n+        self.num_thresholds = num_thresholds\n+        super(SpecificityAtSensitivity, self).__init__(\n+            sensitivity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n+\n+    def result(self):\n+        # Calculate sensitivities at all the thresholds.\n+        sensitivities = K.switch(\n+            K.greater(self.true_positives + self.false_negatives, 0),\n+            (self.true_positives / (self.true_positives + self.false_negatives)),\n+            K.zeros_like(self.thresholds))\n+\n+        # Find the index of the threshold where the sensitivity is closest to the\n+        # given specificity.\n+        min_index = K.argmin(\n+            K.abs(sensitivities - self.value), axis=0)\n+        min_index = K.cast(min_index, 'int32')\n+\n+        # Compute specificity at that index.\n+        denom = (self.true_negatives[min_index] + self.false_positives[min_index])\n+        return K.switch(\n+            K.greater(denom, 0),\n+            self.true_negatives[min_index] / denom,\n+            K.zeros_like(self.true_negatives[min_index]))\n+\n+    def get_config(self):\n+        config = {\n+            'num_thresholds': self.num_thresholds,\n+            'sensitivity': self.sensitivity\n+        }\n+        base_config = super(SpecificityAtSensitivity, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class AUC(Metric):\n     \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n \n\n@@ -262,7 +262,7 @@ class TestSensitivityAtSpecificity(object):\n         assert s_obj2.num_thresholds == 100\n \n     def test_unweighted_all_correct(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.7)\n+        s_obj = metrics.SensitivityAtSpecificity(0.7, num_thresholds=1)\n         inputs = np.random.randint(0, 2, size=(100, 1))\n         y_pred = K.constant(inputs, dtype='float32')\n         y_true = K.constant(inputs)\n@@ -310,6 +310,72 @@ class TestSensitivityAtSpecificity(object):\n             metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n \n \n+class TestSpecificityAtSensitivity(object):\n+\n+    def test_config(self):\n+        s_obj = metrics.SpecificityAtSensitivity(\n+            0.4, num_thresholds=100, name='specificity_at_sensitivity_1')\n+        assert s_obj.name == 'specificity_at_sensitivity_1'\n+        assert len(s_obj.weights) == 4\n+        assert s_obj.sensitivity == 0.4\n+        assert s_obj.num_thresholds == 100\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())\n+        assert s_obj2.name == 'specificity_at_sensitivity_1'\n+        assert len(s_obj2.weights) == 4\n+        assert s_obj2.sensitivity == 0.4\n+        assert s_obj2.num_thresholds == 100\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.7, num_thresholds=1)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = K.constant(inputs, dtype='float32')\n+        y_true = K.constant(inputs)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+\n+    def test_unweighted_high_sensitivity(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.8)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.1, 0.45, 0.5, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(0.4, K.eval(result))\n+\n+    def test_unweighted_low_sensitivity(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(0.6, K.eval(result))\n+\n+    def test_weighted(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values, dtype='float32')\n+        weights = K.constant(weight_values)\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        assert np.isclose(0.4, K.eval(result))\n+\n+    def test_invalid_sensitivity(self):\n+        with pytest.raises(Exception):\n+            metrics.SpecificityAtSensitivity(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with pytest.raises(Exception):\n+            metrics.SpecificityAtSensitivity(0.4, num_thresholds=-1)\n+\n+\n class TestAUC(object):\n \n     def setup(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab124c3c77957ac0bb56f9442a79a84cc365dbcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 643 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 8 | Methods Changed: 20 | Complexity Δ (Sum/Max): 50/25 | Churn Δ: 647 | Churn Cumulative: 4068 | Contributors (this commit): 17 | Commits (past 90d): 36 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1353,6 +1353,216 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class Precision(Metric):\n+    \"\"\"Computes the precision of the predictions with respect to the labels.\n+\n+    For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n+    then the precision value is 2/(2+1) ie. 0.66. If the weights were specified as\n+    [0, 0, 1, 0] then the precision value would be 1.\n+\n+    The metric creates two local variables, `true_positives` and `false_positives`\n+    that are used to compute the precision. This value is ultimately returned as\n+    `precision`, an idempotent operation that simply divides `true_positives`\n+    by the sum of `true_positives` and `false_positives`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `top_k` is set, we'll calculate precision as how often on average a class\n+    among the top-k classes with the highest predicted values of a batch entry is\n+    correct and can be found in the label for that entry.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold and/or in the\n+    top-k highest predictions, and computing the fraction of them for which\n+    `class_id` is indeed a correct label.\n+\n+    Usage with the compile API:\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.Precision()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) A float value or a python list/tuple of float\n+            threshold values in [0, 1]. A threshold is compared with prediction\n+            values to determine the truth value of predictions (i.e., above the\n+            threshold is `true`, below is `false`). One metric value is generated\n+            for each threshold value. If neither thresholds nor top_k are set, the\n+            default is to calculate precision with `thresholds=0.5`.\n+        top_k: (Optional) Unset by default. An int value specifying the top-k\n+            predictions to consider when calculating precision.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 thresholds=None,\n+                 top_k=None,\n+                 class_id=None,\n+                 name=None,\n+                 dtype=None):\n+        super(Precision, self).__init__(name=name, dtype=dtype)\n+        self.init_thresholds = thresholds\n+        self.top_k = top_k\n+        self.class_id = class_id\n+\n+        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=default_threshold)\n+        self.true_positives = self.add_weight(\n+            'true_positives',\n+            shape=(len(self.thresholds),),\n+            initializer='zeros')\n+        self.false_positives = self.add_weight(\n+            'false_positives',\n+            shape=(len(self.thresholds),),\n+            initializer='zeros')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            top_k=self.top_k,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight)\n+\n+    def result(self):\n+        denom = (self.true_positives + self.false_positives)\n+        result = K.switch(\n+            K.greater(denom, 0),\n+            self.true_positives / denom,\n+            K.zeros_like(self.true_positives))\n+\n+        return result[0] if len(self.thresholds) == 1 else result\n+\n+    def reset_states(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        K.batch_set_value(\n+            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+\n+    def get_config(self):\n+        config = {\n+            'thresholds': self.init_thresholds,\n+            'top_k': self.top_k,\n+            'class_id': self.class_id\n+        }\n+        base_config = super(Precision, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Recall(Metric):\n+    \"\"\"Computes the recall of the predictions with respect to the labels.\n+\n+    For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n+    then the recall value is 2/(2+1) ie. 0.66. If the weights were specified as\n+    [0, 0, 1, 0] then the recall value would be 1.\n+\n+    This metric creates two local variables, `true_positives` and\n+    `false_negatives`, that are used to compute the recall. This value is\n+    ultimately returned as `recall`, an idempotent operation that simply divides\n+    `true_positives` by the sum of `true_positives` and `false_negatives`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `top_k` is set, recall will be computed as how often on average a class\n+    among the labels of a batch entry is in the top-k predictions.\n+\n+    If `class_id` is specified, we calculate recall by considering only the\n+    entries in the batch for which `class_id` is in the label, and computing the\n+    fraction of them for which `class_id` is above the threshold and/or in the\n+    top-k predictions.\n+\n+    Usage with the compile API:\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.Recall()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) A float value or a python list/tuple of float\n+            threshold values in [0, 1]. A threshold is compared with prediction\n+            values to determine the truth value of predictions (i.e., above the\n+            threshold is `true`, below is `false`). One metric value is generated\n+            for each threshold value. If neither thresholds nor top_k are set, the\n+            default is to calculate recall with `thresholds=0.5`.\n+        top_k: (Optional) Unset by default. An int value specifying the top-k\n+            predictions to consider when calculating recall.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 thresholds=None,\n+                 top_k=None,\n+                 class_id=None,\n+                 name=None,\n+                 dtype=None):\n+        super(Recall, self).__init__(name=name, dtype=dtype)\n+        self.init_thresholds = thresholds\n+        self.top_k = top_k\n+        self.class_id = class_id\n+\n+        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=default_threshold)\n+        self.true_positives = self.add_weight(\n+            'true_positives',\n+            shape=(len(self.thresholds),),\n+            initializer='zeros')\n+        self.false_negatives = self.add_weight(\n+            'false_negatives',\n+            shape=(len(self.thresholds),),\n+            initializer='zeros')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            top_k=self.top_k,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight)\n+\n+    def result(self):\n+        denom = (self.true_positives + self.false_negatives)\n+        result = K.switch(\n+            K.greater(denom, 0),\n+            self.true_positives / denom,\n+            K.zeros_like(self.true_positives))\n+        return result[0] if len(self.thresholds) == 1 else result\n+\n+    def reset_states(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        K.batch_set_value(\n+            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+\n+    def get_config(self):\n+        config = {\n+            'thresholds': self.init_thresholds,\n+            'top_k': self.top_k,\n+            'class_id': self.class_id\n+        }\n+        base_config = super(Recall, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class AUC(Metric):\n     \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n \n@@ -1535,7 +1745,11 @@ class AUC(Metric):\n         p = self.true_positives + self.false_positives\n         dp = p[:self.num_thresholds - 1] - p[1:]\n \n-        prec_slope = dtp / K.maximum(dp, 0)\n+        dp = K.maximum(dp, 0)\n+        prec_slope = K.switch(\n+            K.greater(dp, 0),\n+            dtp / dp,\n+            K.zeros_like(dtp))\n         intercept = self.true_positives[1:] - (prec_slope * p[1:])\n \n         # Logical and\n@@ -1546,12 +1760,18 @@ class AUC(Metric):\n \n         safe_p_ratio = K.switch(\n             switch_condition,\n-            p[:self.num_thresholds - 1] / K.maximum(p[1:], 0),\n+            K.switch(\n+                K.greater(p[1:], 0),\n+                p[:self.num_thresholds - 1] / p[1:],\n+                K.zeros_like(p[:self.num_thresholds - 1])),\n             K.ones_like(p[1:]))\n \n         numer = prec_slope * (dtp + intercept * K.log(safe_p_ratio))\n         denom = K.maximum(self.true_positives[1:] + self.false_negatives[1:], 0)\n-        return K.sum((numer / denom))\n+        return K.sum(K.switch(\n+            K.greater(denom, 0),\n+            numer / denom,\n+            K.zeros_like(numer)))\n \n     def result(self):\n         if (self.curve == metrics_utils.AUCCurve.PR and\n@@ -1612,6 +1832,111 @@ class AUC(Metric):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class MeanIoU(Metric):\n+    \"\"\"Computes the mean Intersection-Over-Union metric.\n+\n+    Mean Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation, which first computes the IOU for each semantic class and then\n+    computes the average over classes. IOU is defined as follows:\n+    IOU = true_positive / (true_positive + false_positive + false_negative).\n+\n+    The predictions are accumulated in a confusion matrix, weighted by\n+    `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[eras.metrics.MeanIoU(num_classes=2)])\n+    ```\n+\n+    # Arguments\n+        num_classes: The possible number of labels the prediction task can have.\n+            This value must be provided, since a confusion matrix of dimension =\n+            [num_classes, num_classes] will be allocated.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, num_classes, name=None, dtype=None):\n+        super(MeanIoU, self).__init__(name=name, dtype=dtype)\n+        self.num_classes = num_classes\n+\n+        # Variable to accumulate the predictions in the confusion matrix. Setting\n+        # the type to be `float64` as required by confusion_matrix_ops.\n+        self.total_cm = self.add_weight(\n+            'total_confusion_matrix',\n+            shape=(num_classes, num_classes),\n+            initializer='zeros',\n+            dtype='float64')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = K.cast(y_true, self._dtype)\n+        y_pred = K.cast(y_pred, self._dtype)\n+\n+        # Flatten the input if its rank > 1.\n+        if y_pred.shape.ndims > 1:\n+            y_pred = K.reshape(y_pred, [-1])\n+\n+        if y_true.shape.ndims > 1:\n+            y_true = K.reshape(y_true, [-1])\n+\n+        if sample_weight is not None and sample_weight.shape.ndims > 1:\n+            sample_weight = K.reshape(sample_weight, [-1])\n+\n+        # Accumulate the prediction to current confusion matrix.\n+        current_cm = confusion_matrix.confusion_matrix(\n+            y_true,\n+            y_pred,\n+            self.num_classes,\n+            weights=sample_weight,\n+            dtype=dtypes.float64)\n+\n+        return self.total_cm.assign_add(current_cm)\n+\n+    def result(self):\n+        \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n+        sum_over_row = K.cast(\n+            K.sum(self.total_cm, axis=0), dtype=self._dtype)\n+        sum_over_col = K.cast(\n+            K.sum(self.total_cm, axis=1), dtype=self._dtype)\n+        true_positives = K.cast(\n+            array_ops.diag_part(self.total_cm), dtype=self._dtype)\n+\n+        # sum_over_row + sum_over_col =\n+        #     2 * true_positives + false_positives + false_negatives.\n+        denominator = sum_over_row + sum_over_col - true_positives\n+\n+        # The mean is only computed over classes that appear in the\n+        # label or prediction tensor. If the denominator is 0, we need to\n+        # ignore the class.\n+        num_valid_entries = K.sum(\n+            K.cast(K.not_equal(denominator, 0), dtype=self._dtype))\n+\n+        denominator = K.maximum(denominator, 0)\n+        iou = K.switch(\n+            K.greater(denominator, 0),\n+            true_positives / denominator,\n+            K.zeros_like(true_positives))\n+        return K.switch(\n+            K.greater(num_valid_entries, 0),\n+            K.sum(iou) / num_valid_entries,\n+            K.zeros_like(K.sum(iou)))\n+\n+    def reset_states(self):\n+        K.set_value(self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n+\n+    def get_config(self):\n+        config = {'num_classes': self.num_classes}\n+        base_config = super(MeanIoU, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n def accuracy(y_true, y_pred):\n     if not K.is_tensor(y_pred):\n         y_pred = K.constant(y_pred)\n\n@@ -68,6 +68,22 @@ def result_wrapper(result_fn):\n     return decorated\n \n \n+def filter_top_k(x, k):\n+    \"\"\"Filters top-k values in the last dim of x and set the rest to NEG_INF.\n+    Used for computing top-k prediction values in dense labels (which has the same\n+    shape as predictions) for recall and precision top-k metrics.\n+    # Arguments\n+        x: tensor with any dimensions.\n+        k: the number of values to keep.\n+    # Returns\n+        tensor with same shape and dtype as x.\n+    \"\"\"\n+    _, top_k_idx = nn_ops.top_k(x, k, sorted=False)\n+    top_k_mask = K.sum(\n+        K.one_hot(top_k_idx, x.shape[-1], axis=-1), axis=-2)\n+    return x * top_k_mask + NEG_INF * (1 - top_k_mask)\n+\n+\n def to_list(x):\n     if isinstance(x, list):\n         return x\n@@ -237,7 +253,7 @@ def update_confusion_matrix_variables(variables_to_update,\n                 y_pred, y_true=y_true, sample_weight=sample_weight))\n \n     if top_k is not None:\n-        y_pred = _filter_top_k(y_pred, top_k)\n+        y_pred = filter_top_k(y_pred, top_k)\n     if class_id is not None:\n         y_true = y_true[..., class_id]\n         y_pred = y_pred[..., class_id]\n\n@@ -590,3 +590,301 @@ class TestAUC(object):\n     def test_invalid_summation_method(self):\n         with pytest.raises(Exception):\n             metrics.AUC(summation_method='Invalid')\n+\n+\n+class TestPrecisionTest(object):\n+\n+    def test_config(self):\n+        p_obj = metrics.Precision(\n+            name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)\n+        assert p_obj.name == 'my_precision'\n+        assert len(p_obj.weights) == 2\n+        assert ([v.name for v in p_obj.weights] ==\n+                ['true_positives:0', 'false_positives:0'])\n+        assert p_obj.thresholds == [0.4, 0.9]\n+        assert p_obj.top_k == 15\n+        assert p_obj.class_id == 12\n+\n+        # Check save and restore config\n+        p_obj2 = metrics.Precision.from_config(p_obj.get_config())\n+        assert p_obj2.name == 'my_precision'\n+        assert len(p_obj2.weights) == 2\n+        assert p_obj2.thresholds == [0.4, 0.9]\n+        assert p_obj2.top_k == 15\n+        assert p_obj2.class_id == 12\n+\n+    def test_unweighted(self):\n+        p_obj = metrics.Precision()\n+        y_pred = K.constant([1, 0, 1, 0], shape=(1, 4))\n+        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+\n+    def test_unweighted_all_incorrect(self):\n+        p_obj = metrics.Precision(thresholds=[0.5])\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = K.constant(inputs)\n+        y_true = K.constant(1 - inputs)\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(0, K.eval(result))\n+\n+    def test_weighted(self):\n+        p_obj = metrics.Precision()\n+        y_pred = K.constant([[1, 0, 1, 0], [1, 0, 1, 0]])\n+        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])\n+        result = p_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))\n+        weighted_tp = 3.0 + 4.0\n+        weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)\n+        expected_precision = weighted_tp / weighted_positives\n+        assert np.isclose(expected_precision, K.eval(result))\n+\n+    def test_unweighted_with_threshold(self):\n+        p_obj = metrics.Precision(thresholds=[0.5, 0.7])\n+        y_pred = K.constant([1, 0, 0.6, 0], shape=(1, 4))\n+        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n+        result = p_obj(y_true, y_pred)\n+        assert np.allclose([0.5, 0.], K.eval(result), 0)\n+\n+    def test_weighted_with_threshold(self):\n+        p_obj = metrics.Precision(thresholds=[0.5, 1.])\n+        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))\n+        y_pred = K.constant([[1, 0], [0.6, 0]],\n+                            shape=(2, 2),\n+                            dtype='float32')\n+        weights = K.constant([[4, 0], [3, 1]],\n+                             shape=(2, 2),\n+                             dtype='float32')\n+        result = p_obj(y_true, y_pred, sample_weight=weights)\n+        weighted_tp = 0 + 3.\n+        weighted_positives = (0 + 3.) + (4. + 0.)\n+        expected_precision = weighted_tp / weighted_positives\n+        assert np.allclose([expected_precision, 0], K.eval(result), 1e-3)\n+\n+    def test_unweighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1. / 3, K.eval(result))\n+\n+    def test_weighted_top_k(self):\n+        p_obj = metrics.Precision(top_k=3)\n+        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        K.eval(\n+            p_obj(\n+                y_true1,\n+                y_pred1,\n+                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+\n+        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+        result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+\n+        tp = (2 + 5) + (3 + 3)\n+        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n+        expected_precision = tp / predicted_positives\n+        assert np.isclose(expected_precision, K.eval(result))\n+\n+    def test_unweighted_class_id(self):\n+        p_obj = metrics.Precision(class_id=2)\n+\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n+\n+        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n+\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(1, K.eval(p_obj.false_positives))\n+\n+    def test_unweighted_top_k_and_class_id(self):\n+        p_obj = metrics.Precision(class_id=2, top_k=2)\n+\n+        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n+\n+        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n+\n+    def test_unweighted_top_k_and_threshold(self):\n+        p_obj = metrics.Precision(thresholds=.7, top_k=2)\n+\n+        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        result = p_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(p_obj.true_positives))\n+        assert np.isclose(0, K.eval(p_obj.false_positives))\n+\n+\n+class TestRecall(object):\n+\n+    def test_config(self):\n+        r_obj = metrics.Recall(\n+            name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)\n+        assert r_obj.name == 'my_recall'\n+        assert len(r_obj.weights) == 2\n+        assert ([v.name for v in r_obj.weights] ==\n+                ['true_positives:0', 'false_negatives:0'])\n+        assert r_obj.thresholds == [0.4, 0.9]\n+        assert r_obj.top_k == 15\n+        assert r_obj.class_id == 12\n+\n+        # Check save and restore config\n+        r_obj2 = metrics.Recall.from_config(r_obj.get_config())\n+        assert r_obj2.name == 'my_recall'\n+        assert len(r_obj2.weights) == 2\n+        assert r_obj2.thresholds == [0.4, 0.9]\n+        assert r_obj2.top_k == 15\n+        assert r_obj2.class_id == 12\n+\n+    def test_unweighted(self):\n+        r_obj = metrics.Recall()\n+        y_pred = K.constant([1, 0, 1, 0], shape=(1, 4))\n+        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+\n+    def test_unweighted_all_incorrect(self):\n+        r_obj = metrics.Recall(thresholds=[0.5])\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = K.constant(inputs)\n+        y_true = K.constant(1 - inputs)\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0, K.eval(result))\n+\n+    def test_weighted(self):\n+        r_obj = metrics.Recall()\n+        y_pred = K.constant([[1, 0, 1, 0], [0, 1, 0, 1]])\n+        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])\n+        result = r_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))\n+        weighted_tp = 3.0 + 1.0\n+        weighted_t = (2.0 + 3.0) + (4.0 + 1.0)\n+        expected_recall = weighted_tp / weighted_t\n+        assert np.isclose(expected_recall, K.eval(result))\n+\n+    def test_unweighted_with_threshold(self):\n+        r_obj = metrics.Recall(thresholds=[0.5, 0.7])\n+        y_pred = K.constant([1, 0, 0.6, 0], shape=(1, 4))\n+        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n+        result = r_obj(y_true, y_pred)\n+        assert np.allclose([0.5, 0.], K.eval(result), 0)\n+\n+    def test_weighted_with_threshold(self):\n+        r_obj = metrics.Recall(thresholds=[0.5, 1.])\n+        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))\n+        y_pred = K.constant([[1, 0], [0.6, 0]],\n+                            shape=(2, 2),\n+                            dtype='float32')\n+        weights = K.constant([[1, 4], [3, 2]],\n+                             shape=(2, 2),\n+                             dtype='float32')\n+        result = r_obj(y_true, y_pred, sample_weight=weights)\n+        weighted_tp = 0 + 3.\n+        weighted_positives = (0 + 3.) + (4. + 0.)\n+        expected_recall = weighted_tp / weighted_positives\n+        assert np.allclose([expected_recall, 0], K.eval(result), 1e-3)\n+\n+    def test_unweighted_top_k(self):\n+        r_obj = metrics.Recall(top_k=3)\n+        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+\n+    def test_weighted_top_k(self):\n+        r_obj = metrics.Recall(top_k=3)\n+        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+        K.eval(\n+            r_obj(\n+                y_true1,\n+                y_pred1,\n+                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+\n+        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+        result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+\n+        tp = (2 + 5) + (3 + 3)\n+        positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n+        expected_recall = tp / positives\n+        assert np.isclose(expected_recall, K.eval(result))\n+\n+    def test_unweighted_class_id(self):\n+        r_obj = metrics.Recall(class_id=2)\n+\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(0, K.eval(r_obj.false_negatives))\n+\n+        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+\n+        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+\n+    def test_unweighted_top_k_and_class_id(self):\n+        r_obj = metrics.Recall(class_id=2, top_k=2)\n+\n+        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(0, K.eval(r_obj.false_negatives))\n+\n+        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.5, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+\n+    def test_unweighted_top_k_and_threshold(self):\n+        r_obj = metrics.Recall(thresholds=.7, top_k=2)\n+\n+        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+        y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n+        result = r_obj(y_true, y_pred)\n+        assert np.isclose(0.25, K.eval(result))\n+        assert np.isclose(1, K.eval(r_obj.true_positives))\n+        assert np.isclose(3, K.eval(r_obj.false_negatives))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dcb7a16e87a7aacfef1cdf94bc3e17510a764fb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 128 | Files Changed: 1 | Hunks: 26 | Methods Changed: 5 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 256 | Churn Cumulative: 1148 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -663,82 +663,82 @@ class TestPrecisionTest(object):\n         expected_precision = weighted_tp / weighted_positives\n         assert np.allclose([expected_precision, 0], K.eval(result), 1e-3)\n \n-    def test_unweighted_top_k(self):\n-        p_obj = metrics.Precision(top_k=3)\n-        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1. / 3, K.eval(result))\n+    # def test_unweighted_top_k(self):\n+    #     p_obj = metrics.Precision(top_k=3)\n+    #     y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1. / 3, K.eval(result))\n \n-    def test_weighted_top_k(self):\n-        p_obj = metrics.Precision(top_k=3)\n-        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        K.eval(\n-            p_obj(\n-                y_true1,\n-                y_pred1,\n-                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+    # def test_weighted_top_k(self):\n+    #     p_obj = metrics.Precision(top_k=3)\n+    #     y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+    #     y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+    #     K.eval(\n+    #         p_obj(\n+    #             y_true1,\n+    #             y_pred1,\n+    #             sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n \n-        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-        result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+    #     y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+    #     y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+    #     result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n \n-        tp = (2 + 5) + (3 + 3)\n-        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n-        expected_precision = tp / predicted_positives\n-        assert np.isclose(expected_precision, K.eval(result))\n+    #     tp = (2 + 5) + (3 + 3)\n+    #     predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n+    #     expected_precision = tp / predicted_positives\n+    #     assert np.isclose(expected_precision, K.eval(result))\n \n-    def test_unweighted_class_id(self):\n-        p_obj = metrics.Precision(class_id=2)\n+    # def test_unweighted_class_id(self):\n+    #     p_obj = metrics.Precision(class_id=2)\n \n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(1, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(0.5, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(1, K.eval(p_obj.false_positives))\n \n-    def test_unweighted_top_k_and_class_id(self):\n-        p_obj = metrics.Precision(class_id=2, top_k=2)\n+    # def test_unweighted_top_k_and_class_id(self):\n+    #     p_obj = metrics.Precision(class_id=2, top_k=2)\n \n-        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n \n-    def test_unweighted_top_k_and_threshold(self):\n-        p_obj = metrics.Precision(thresholds=.7, top_k=2)\n+    # def test_unweighted_top_k_and_threshold(self):\n+    #     p_obj = metrics.Precision(thresholds=.7, top_k=2)\n \n-        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n+    #     y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+    #     result = p_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(p_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(p_obj.false_positives))\n \n \n class TestRecall(object):\n@@ -812,79 +812,79 @@ class TestRecall(object):\n         expected_recall = weighted_tp / weighted_positives\n         assert np.allclose([expected_recall, 0], K.eval(result), 1e-3)\n \n-    def test_unweighted_top_k(self):\n-        r_obj = metrics.Recall(top_k=3)\n-        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n+    # def test_unweighted_top_k(self):\n+    #     r_obj = metrics.Recall(top_k=3)\n+    #     y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(0.5, K.eval(result))\n \n-    def test_weighted_top_k(self):\n-        r_obj = metrics.Recall(top_k=3)\n-        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        K.eval(\n-            r_obj(\n-                y_true1,\n-                y_pred1,\n-                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n+    # def test_weighted_top_k(self):\n+    #     r_obj = metrics.Recall(top_k=3)\n+    #     y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n+    #     y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n+    #     K.eval(\n+    #         r_obj(\n+    #             y_true1,\n+    #             y_pred1,\n+    #             sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n \n-        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-        result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n+    #     y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n+    #     y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n+    #     result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n \n-        tp = (2 + 5) + (3 + 3)\n-        positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n-        expected_recall = tp / positives\n-        assert np.isclose(expected_recall, K.eval(result))\n+    #     tp = (2 + 5) + (3 + 3)\n+    #     positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n+    #     expected_recall = tp / positives\n+    #     assert np.isclose(expected_recall, K.eval(result))\n \n-    def test_unweighted_class_id(self):\n-        r_obj = metrics.Recall(class_id=2)\n+    # def test_unweighted_class_id(self):\n+    #     r_obj = metrics.Recall(class_id=2)\n \n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(0, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(r_obj.false_negatives))\n \n-        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(0.5, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(0.5, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-    def test_unweighted_top_k_and_class_id(self):\n-        r_obj = metrics.Recall(class_id=2, top_k=2)\n+    # def test_unweighted_top_k_and_class_id(self):\n+    #     r_obj = metrics.Recall(class_id=2, top_k=2)\n \n-        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(0, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(1, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(0, K.eval(r_obj.false_negatives))\n \n-        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n+    #     y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(0.5, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(1, K.eval(r_obj.false_negatives))\n \n-    def test_unweighted_top_k_and_threshold(self):\n-        r_obj = metrics.Recall(thresholds=.7, top_k=2)\n+    # def test_unweighted_top_k_and_threshold(self):\n+    #     r_obj = metrics.Recall(thresholds=.7, top_k=2)\n \n-        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.25, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(3, K.eval(r_obj.false_negatives))\n+    #     y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n+    #     y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n+    #     result = r_obj(y_true, y_pred)\n+    #     assert np.isclose(0.25, K.eval(result))\n+    #     assert np.isclose(1, K.eval(r_obj.true_positives))\n+    #     assert np.isclose(3, K.eval(r_obj.false_negatives))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
