{"custom_id": "scrapy#bde96a5ad98caed74cbaaeaf1dfe4b215093c03f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 14 | Churn Cumulative: 1335 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -349,10 +349,16 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         ])\n \n     def data_received(self, event: DataReceived) -> None:\n+        try:\n             self.streams[event.stream_id].receive_data(event.data, event.flow_controlled_length)\n+        except KeyError:\n+            logger.debug(f'Ignoring server-initiated event {event}')\n \n     def response_received(self, event: ResponseReceived) -> None:\n+        try:\n             self.streams[event.stream_id].receive_headers(event.headers)\n+        except KeyError:\n+            logger.debug(f'Ignoring server-initiated event {event}')\n \n     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n         self.metadata['settings_acknowledged'] = True\n@@ -365,11 +371,19 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n \n     def stream_ended(self, event: StreamEnded) -> None:\n+        try:\n             stream = self.pop_stream(event.stream_id)\n+        except KeyError:\n+            logger.debug(f'Ignoring server-initiated event {event}')\n+        else:\n             stream.close(StreamCloseReason.ENDED, from_protocol=True)\n \n     def stream_reset(self, event: StreamReset) -> None:\n+        try:\n             stream = self.pop_stream(event.stream_id)\n+        except KeyError:\n+            logger.debug(f'Ignoring server-initiated event {event}')\n+        else:\n             stream.close(StreamCloseReason.RESET, from_protocol=True)\n \n     def window_updated(self, event: WindowUpdated) -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08f5ed712f4fdafec122f887c3ef06629f35ee0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1344 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,7 @@ from h2.events import (\n     SettingsAcknowledged, StreamEnded, StreamReset, UnknownFrameReceived,\n     WindowUpdated\n )\n-from h2.exceptions import H2Error\n+from h2.exceptions import FrameTooLargeError, H2Error\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory\n@@ -261,6 +261,13 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             events = self.conn.receive_data(data)\n             self._handle_events(events)\n         except H2Error as e:\n+            if isinstance(e, FrameTooLargeError):\n+                # hyper-h2 does not drop the connection in this scenario, we\n+                # need to abort the connection manually.\n+                self._conn_lost_errors += [e]\n+                self.transport.abortConnection()\n+                return\n+\n             # Save this error as ultimately the connection will be dropped\n             # internally by hyper-h2. Saved error will be passed to all the streams\n             # closed with the connection.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#075ab156b0ac7bf56828fd40a843f3e9b63a3de3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/2 | Churn Δ: 31 | Churn Cumulative: 1066 | Contributors (this commit): 28 | Commits (past 90d): 3 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,11 @@\n-\"\"\"\n-Helpers using Python 3.6+ syntax (ignore SyntaxError on import).\n-\"\"\"\n+import warnings\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.python import collect_asyncgen  # noqa: F401\n \n \n-async def collect_asyncgen(result):\n-    results = []\n-    async for x in result:\n-        results.append(x)\n-    return results\n+warnings.warn(\n+    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.python` instead.\",\n+    category=ScrapyDeprecationWarning,\n+    stacklevel=2,\n+)\n\n@@ -355,3 +355,10 @@ class MutableChain:\n     @deprecated(\"scrapy.utils.python.MutableChain.__next__\")\n     def next(self):\n         return self.__next__()\n+\n+\n+async def collect_asyncgen(result):\n+    results = []\n+    async for x in result:\n+        results.append(x)\n+    return results\n\n@@ -4,17 +4,14 @@ import logging\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n-try:\n-    from scrapy.utils.py36 import collect_asyncgen\n-except SyntaxError:\n-    collect_asyncgen = None\n+from scrapy.utils.python import collect_asyncgen\n \n \n logger = logging.getLogger(__name__)\n \n \n def iterate_spider_output(result):\n-    if collect_asyncgen and hasattr(inspect, 'isasyncgen') and inspect.isasyncgen(result):\n+    if inspect.isasyncgen(result):\n         d = deferred_from_coro(collect_asyncgen(result))\n         d.addCallback(iterate_spider_output)\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4075e1eadd0a9e83356d18b9ca73090e4a0bc770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 65 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 65 | Churn Cumulative: 525 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,36 +0,0 @@\n-\"\"\"\n-Transitional module for moving to the w3lib library.\n-\n-For new code, always import from w3lib.http instead of this module\n-\"\"\"\n-\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.decorators import deprecated\n-from w3lib.http import *  # noqa: F401\n-\n-\n-warnings.warn(\"Module `scrapy.utils.http` is deprecated, \"\n-              \"Please import from `w3lib.http` instead.\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-\n-@deprecated\n-def decode_chunked_transfer(chunked_body):\n-    \"\"\"Parsed body received with chunked transfer encoding, and return the\n-    decoded body.\n-\n-    For more info see:\n-    https://en.wikipedia.org/wiki/Chunked_transfer_encoding\n-\n-    \"\"\"\n-    body, h, t = '', '', chunked_body\n-    while t:\n-        h, t = t.split('\\r\\n', 1)\n-        if h == '0':\n-            break\n-        size = int(h, 16)\n-        body += t[:size]\n-        t = t[size + 2:]\n-    return body\n\n@@ -1,14 +0,0 @@\n-\"\"\"\n-Transitional module for moving to the w3lib library.\n-\n-For new code, always import from w3lib.html instead of this module\n-\"\"\"\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from w3lib.html import *  # noqa: F401\n-\n-\n-warnings.warn(\"Module `scrapy.utils.markup` is deprecated. \"\n-              \"Please import from `w3lib.html` instead.\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -1,15 +0,0 @@\n-\"\"\"\n-Transitional module for moving to the w3lib library.\n-\n-For new code, always import from w3lib.form instead of this module\n-\"\"\"\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from w3lib.form import *  # noqa: F401\n-\n-\n-warnings.warn(\"Module `scrapy.utils.multipart` is deprecated. \"\n-              \"If you're using `encode_multipart` function, please use \"\n-              \"`urllib3.filepost.encode_multipart_formdata` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51ca4d0138e5c9cf637074f59c839ff9b5839db6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 54 | Files Changed: 2 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 70 | Churn Cumulative: 362 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n+import struct\n from gzip import GzipFile\n from io import BytesIO\n-import re\n-import struct\n \n from scrapy.utils.decorators import deprecated\n \n@@ -42,17 +41,5 @@ def gunzip(data):\n     return b''.join(output_list)\n \n \n-_is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n-_is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n-\n-\n-@deprecated\n-def is_gzipped(response):\n-    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n-    ctype = response.headers.get('Content-Type', b'')\n-    cenc = response.headers.get('Content-Encoding', b'').lower()\n-    return _is_gzipped(ctype) or _is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')\n-\n-\n def gzip_magic_number(response):\n     return response.body[:3] == b'\\x1f\\x8b\\x08'\n\n@@ -3,10 +3,11 @@ from os.path import join\n \n from w3lib.encoding import html_to_unicode\n \n-from scrapy.utils.gz import gunzip, is_gzipped\n-from scrapy.http import Response, Headers\n+from scrapy.utils.gz import gunzip, gzip_magic_number\n+from scrapy.http import Response\n from tests import tests_datadir\n \n+\n SAMPLEDIR = join(tests_datadir, 'compressed')\n \n \n@@ -14,8 +15,12 @@ class GunzipTest(unittest.TestCase):\n \n     def test_gunzip_basic(self):\n         with open(join(SAMPLEDIR, 'feed-sample1.xml.gz'), 'rb') as f:\n-            text = gunzip(f.read())\n-            self.assertEqual(len(text), 9950)\n+            r1 = Response(\"http://www.example.com\", body=f.read())\n+            self.assertTrue(gzip_magic_number(r1))\n+\n+            r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n+            self.assertFalse(gzip_magic_number(r2))\n+            self.assertEqual(len(r2.body), 9950)\n \n     def test_gunzip_truncated(self):\n         with open(join(SAMPLEDIR, 'truncated-crc-error.gz'), 'rb') as f:\n@@ -28,46 +33,16 @@ class GunzipTest(unittest.TestCase):\n \n     def test_gunzip_truncated_short(self):\n         with open(join(SAMPLEDIR, 'truncated-crc-error-short.gz'), 'rb') as f:\n-            text = gunzip(f.read())\n-            assert text.endswith(b'</html>')\n+            r1 = Response(\"http://www.example.com\", body=f.read())\n+            self.assertTrue(gzip_magic_number(r1))\n \n-    def test_is_x_gzipped_right(self):\n-        hdrs = Headers({\"Content-Type\": \"application/x-gzip\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertTrue(is_gzipped(r1))\n-\n-    def test_is_gzipped_right(self):\n-        hdrs = Headers({\"Content-Type\": \"application/gzip\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertTrue(is_gzipped(r1))\n-\n-    def test_is_gzipped_not_quite(self):\n-        hdrs = Headers({\"Content-Type\": \"application/gzippppp\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertFalse(is_gzipped(r1))\n-\n-    def test_is_gzipped_case_insensitive(self):\n-        hdrs = Headers({\"Content-Type\": \"Application/X-Gzip\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertTrue(is_gzipped(r1))\n-\n-        hdrs = Headers({\"Content-Type\": \"application/X-GZIP ; charset=utf-8\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertTrue(is_gzipped(r1))\n+            r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n+            assert r2.body.endswith(b'</html>')\n+            self.assertFalse(gzip_magic_number(r2))\n \n     def test_is_gzipped_empty(self):\n         r1 = Response(\"http://www.example.com\")\n-        self.assertFalse(is_gzipped(r1))\n-\n-    def test_is_gzipped_wrong(self):\n-        hdrs = Headers({\"Content-Type\": \"application/javascript\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertFalse(is_gzipped(r1))\n-\n-    def test_is_gzipped_with_charset(self):\n-        hdrs = Headers({\"Content-Type\": \"application/x-gzip;charset=utf-8\"})\n-        r1 = Response(\"http://www.example.com\", headers=hdrs)\n-        self.assertTrue(is_gzipped(r1))\n+        self.assertFalse(gzip_magic_number(r1))\n \n     def test_gunzip_illegal_eof(self):\n         with open(join(SAMPLEDIR, 'unexpected-eof.gz'), 'rb') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#462014bc5738a5ed18ec4c15a91384eb17e57096", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 589 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,14 +1,10 @@\n import os\n import json\n import logging\n-import warnings\n from os.path import join, exists\n \n-from queuelib import PriorityQueue\n-\n from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.job import job_dir\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n \n \n logger = logging.getLogger(__name__)\n@@ -56,14 +52,6 @@ class Scheduler:\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n         dupefilter = create_instance(dupefilter_cls, settings, crawler)\n         pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n-        if pqclass is PriorityQueue:\n-            warnings.warn(\"SCHEDULER_PRIORITY_QUEUE='queuelib.PriorityQueue'\"\n-                          \" is no longer supported because of API changes; \"\n-                          \"please use 'scrapy.pqueues.ScrapyPriorityQueue'\",\n-                          ScrapyDeprecationWarning)\n-            from scrapy.pqueues import ScrapyPriorityQueue\n-            pqclass = ScrapyPriorityQueue\n-\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n         logunser = settings.getbool('SCHEDULER_DEBUG')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a93df9efd20a4bcd34d2ee5e6bdf6d365d70409", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 16 | Churn Cumulative: 917 | Contributors (this commit): 25 | Commits (past 90d): 5 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,5 @@\n+async def collect_asyncgen(result):\n+    results = []\n+    async for x in result:\n+        results.append(x)\n+    return results\n\n@@ -1,11 +1,11 @@\n import warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.python import collect_asyncgen  # noqa: F401\n+from scrapy.utils.asyncgen import collect_asyncgen  # noqa: F401\n \n \n warnings.warn(\n-    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.python` instead.\",\n+    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.asyncgen` instead.\",\n     category=ScrapyDeprecationWarning,\n     stacklevel=2,\n )\n\n@@ -355,10 +355,3 @@ class MutableChain:\n     @deprecated(\"scrapy.utils.python.MutableChain.__next__\")\n     def next(self):\n         return self.__next__()\n-\n-\n-async def collect_asyncgen(result):\n-    results = []\n-    async for x in result:\n-        results.append(x)\n-    return results\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#18b05af87783d71f9c8f9f1ebe027083037e6f86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 19 | Churn Cumulative: 32 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,19 +0,0 @@\n-import unittest\n-\n-from scrapy.utils.http import decode_chunked_transfer\n-\n-\n-class ChunkedTest(unittest.TestCase):\n-\n-    def test_decode_chunked_transfer(self):\n-        \"\"\"Example taken from: http://en.wikipedia.org/wiki/Chunked_transfer_encoding\"\"\"\n-        chunked_body = \"25\\r\\n\" + \"This is the data in the first chunk\\r\\n\\r\\n\"\n-        chunked_body += \"1C\\r\\n\" + \"and this is the second one\\r\\n\\r\\n\"\n-        chunked_body += \"3\\r\\n\" + \"con\\r\\n\"\n-        chunked_body += \"8\\r\\n\" + \"sequence\\r\\n\"\n-        chunked_body += \"0\\r\\n\\r\\n\"\n-        body = decode_chunked_transfer(chunked_body)\n-        self.assertEqual(\n-            body,\n-            \"This is the data in the first chunk\\r\\nand this is the second one\\r\\nconsequence\"\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe8bb6bd905ad2a733ba5e876f4197888605902e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 167 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import logging\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import collect_asyncgen\n+from scrapy.utils.asyncgen import collect_asyncgen\n \n \n logger = logging.getLogger(__name__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dc3e6350c230028addd8e73833c93341bab4b42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 36 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,14 @@\n+import os\n+import re\n+from configparser import ConfigParser\n from importlib import import_module\n+\n+from twisted import version as twisted_version\n from twisted.trial import unittest\n \n \n class ScrapyUtilsTest(unittest.TestCase):\n+\n     def test_required_openssl_version(self):\n         try:\n             module = import_module('OpenSSL')\n@@ -13,6 +19,32 @@ class ScrapyUtilsTest(unittest.TestCase):\n             installed_version = [int(x) for x in module.__version__.split('.')[:2]]\n             assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n \n+    def test_pinned_twisted_version(self):\n+        \"\"\"When running tests within a Tox environment with pinned\n+        dependencies, make sure that the version of Twisted is the pinned\n+        version.\n+\n+        See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n+        \"\"\"\n+        if not os.environ.get('SCRAPY_PINNED', None):\n+            self.skipTest('Not in a pinned environment')\n+\n+        tox_config_file_path = os.path.join(\n+            os.path.dirname(__file__),\n+            '..',\n+            'tox.ini',\n+        )\n+        config_parser = ConfigParser()\n+        config_parser.read(tox_config_file_path)\n+        pattern = r'Twisted==([\\d.]+)'\n+        match = re.search(pattern, config_parser['pinned']['deps'])\n+        pinned_twisted_version_string = match[1]\n+\n+        self.assertEqual(\n+            twisted_version.short(),\n+            pinned_twisted_version_string\n+        )\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a752fa072e1acbb233191dbf04728db1fd6712a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 134 | Churn Cumulative: 219 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,105 @@ from scrapy.utils.python import global_object_name\n logger = logging.getLogger(__name__)\n \n \n+def get_retry_request(\n+    request,\n+    *,\n+    reason,\n+    spider,\n+    max_retry_times=None,\n+    priority_adjust=None,\n+):\n+    settings = spider.crawler.settings\n+    stats = spider.crawler.stats\n+    retry_times = request.meta.get('retry_times', 0) + 1\n+    request_max_retry_times = request.meta.get(\n+        'max_retry_times',\n+        max_retry_times,\n+    )\n+    if request_max_retry_times is None:\n+        request_max_retry_times = settings.getint('RETRY_TIMES')\n+    if retry_times <= request_max_retry_times:\n+        logger.debug(\n+            \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n+            {'request': request, 'retry_times': retry_times, 'reason': reason},\n+            extra={'spider': spider}\n+        )\n+        new_request = request.copy()\n+        new_request.meta['retry_times'] = retry_times\n+        new_request.dont_filter = True\n+        if priority_adjust is None:\n+            priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n+        new_request.priority = request.priority + priority_adjust\n+\n+        if isinstance(reason, Exception):\n+            reason = global_object_name(reason.__class__)\n+\n+        stats.inc_value('retry/count')\n+        stats.inc_value(f'retry/reason_count/{reason}')\n+        return new_request\n+    else:\n+        stats.inc_value('retry/max_reached')\n+        logger.error(\"Gave up retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n+                        {'request': request, 'retry_times': retry_times, 'reason': reason},\n+                        extra={'spider': spider})\n+        return None\n+\n+\n+def retry_request(\n+    request,\n+    *,\n+    reason,\n+    spider,\n+    max_retry_times=None,\n+    priority_adjust=None,\n+):\n+    new_request = get_retry_request(\n+        request,\n+        reason=reason,\n+        spider=spider,\n+        max_retry_times=max_retry_times,\n+        priority_adjust=priority_adjust,\n+    )\n+    if new_request:\n+        return [new_request]\n+    return []\n+\n+\n+class RetrySpiderMixin:\n+\n+    def get_retry_request(\n+        self,\n+        request,\n+        *,\n+        reason,\n+        max_retry_times=None,\n+        priority_adjust=None,\n+    ):\n+        return get_retry_request(\n+            request,\n+            reason=reason,\n+            spider=self,\n+            max_retry_times=max_retry_times,\n+            priority_adjust=priority_adjust,\n+        )\n+\n+    def retry_request(\n+        self,\n+        request,\n+        *,\n+        reason,\n+        max_retry_times=None,\n+        priority_adjust=None,\n+    ):\n+        return retry_request(\n+            request,\n+            reason=reason,\n+            spider=self,\n+            max_retry_times=max_retry_times,\n+            priority_adjust=priority_adjust,\n+        )\n+\n+\n class RetryMiddleware:\n \n     # IOError is raised by the HttpCompression middleware when trying to\n@@ -67,31 +166,10 @@ class RetryMiddleware:\n             return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n-        retries = request.meta.get('retry_times', 0) + 1\n-\n-        retry_times = self.max_retry_times\n-\n-        if 'max_retry_times' in request.meta:\n-            retry_times = request.meta['max_retry_times']\n-\n-        stats = spider.crawler.stats\n-        if retries <= retry_times:\n-            logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n-                         {'request': request, 'retries': retries, 'reason': reason},\n-                         extra={'spider': spider})\n-            retryreq = request.copy()\n-            retryreq.meta['retry_times'] = retries\n-            retryreq.dont_filter = True\n-            retryreq.priority = request.priority + self.priority_adjust\n-\n-            if isinstance(reason, Exception):\n-                reason = global_object_name(reason.__class__)\n-\n-            stats.inc_value('retry/count')\n-            stats.inc_value(f'retry/reason_count/{reason}')\n-            return retryreq\n-        else:\n-            stats.inc_value('retry/max_reached')\n-            logger.error(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n-                         {'request': request, 'retries': retries, 'reason': reason},\n-                         extra={'spider': spider})\n+        return get_retry_request(\n+            request,\n+            reason=reason,\n+            spider=spider,\n+            max_retry_times=self.max_retry_times,\n+            priority_adjust=self.priority_adjust,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6879c681ebf94b22ee777f3249dfe59a99559cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 38 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n \n         See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011\n         \"\"\"\n-        if not os.environ.get('SCRAPY_PINNED', None):\n+        if not os.environ.get('_SCRAPY_PINNED', None):\n             self.skipTest('Not in a pinned environment')\n \n         tox_config_file_path = os.path.join(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a80bafe5cdcb4b5e2542524fe641570f9107a121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 296 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,18 +68,10 @@ def get_project_settings():\n     if settings_module_path:\n         settings.setmodule(settings_module_path, priority='project')\n \n-    pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n-    if pickled_settings:\n-        warnings.warn(\"Use of environment variable \"\n-                      \"'SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE' \"\n-                      \"is deprecated.\", ScrapyDeprecationWarning)\n-        settings.setdict(pickle.loads(pickled_settings), priority='project')\n-\n     scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n                       k.startswith('SCRAPY_')}\n     valid_envvars = {\n         'CHECK',\n-        'PICKLED_SETTINGS_TO_OVERRIDE',\n         'PROJECT',\n         'PYTHON_SHELL',\n         'SETTINGS_MODULE',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6091f3cc03835b24def08bf358d7b19207be685d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 297 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import os\n-import pickle\n import warnings\n \n from importlib import import_module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef09e0d10fc950ac308a159988be6b50e87bd906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 112 | Files Changed: 12 | Hunks: 62 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 266 | Churn Cumulative: 4512 | Contributors (this commit): 48 | Commits (past 90d): 20 | Contributors (cumulative): 131 | DMM Complexity: 0.8529411764705882\n\nDIFF:\n@@ -22,7 +22,7 @@ __all__ = [\n \n \n # Scrapy and Twisted versions\n-__version__ = pkgutil.get_data(__package__, 'VERSION').decode('ascii').strip()\n+__version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\n version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split('.'))\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n\n@@ -3,6 +3,8 @@ Base class for Scrapy commands\n \"\"\"\n import os\n from optparse import OptionGroup\n+from typing import Any, Dict\n+\n from twisted.python import failure\n \n from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n@@ -15,7 +17,7 @@ class ScrapyCommand:\n     crawler_process = None\n \n     # default settings to be used for this command instead of global defaults\n-    default_settings = {}\n+    default_settings: Dict[str, Any] = {}\n \n     exitcode = 0\n \n\n@@ -1,5 +1,6 @@\n import json\n import logging\n+from typing import Dict\n \n from itemadapter import is_item, ItemAdapter\n from w3lib.url import is_url\n@@ -10,6 +11,7 @@ from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -17,8 +19,8 @@ class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n     spider = None\n-    items = {}\n-    requests = {}\n+    items: Dict[int, list] = {}\n+    requests: Dict[int, list] = {}\n \n     first_response = None\n \n\n@@ -1,16 +1,77 @@\n-import sys\n import re\n+import sys\n from functools import wraps\n from inspect import getmembers\n+from typing import Dict\n from unittest import TestCase\n \n from scrapy.http import Request\n-from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n+from scrapy.utils.spider import iterate_spider_output\n+\n+\n+class Contract:\n+    \"\"\" Abstract class for contracts \"\"\"\n+    request_cls = None\n+\n+    def __init__(self, method, *args):\n+        self.testcase_pre = _create_testcase(method, f'@{self.name} pre-hook')\n+        self.testcase_post = _create_testcase(method, f'@{self.name} post-hook')\n+        self.args = args\n+\n+    def add_pre_hook(self, request, results):\n+        if hasattr(self, 'pre_process'):\n+            cb = request.callback\n+\n+            @wraps(cb)\n+            def wrapper(response, **cb_kwargs):\n+                try:\n+                    results.startTest(self.testcase_pre)\n+                    self.pre_process(response)\n+                    results.stopTest(self.testcase_pre)\n+                except AssertionError:\n+                    results.addFailure(self.testcase_pre, sys.exc_info())\n+                except Exception:\n+                    results.addError(self.testcase_pre, sys.exc_info())\n+                else:\n+                    results.addSuccess(self.testcase_pre)\n+                finally:\n+                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n+\n+            request.callback = wrapper\n+\n+        return request\n+\n+    def add_post_hook(self, request, results):\n+        if hasattr(self, 'post_process'):\n+            cb = request.callback\n+\n+            @wraps(cb)\n+            def wrapper(response, **cb_kwargs):\n+                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n+                try:\n+                    results.startTest(self.testcase_post)\n+                    self.post_process(output)\n+                    results.stopTest(self.testcase_post)\n+                except AssertionError:\n+                    results.addFailure(self.testcase_post, sys.exc_info())\n+                except Exception:\n+                    results.addError(self.testcase_post, sys.exc_info())\n+                else:\n+                    results.addSuccess(self.testcase_post)\n+                finally:\n+                    return output\n+\n+            request.callback = wrapper\n+\n+        return request\n+\n+    def adjust_request_args(self, args):\n+        return args\n \n \n class ContractsManager:\n-    contracts = {}\n+    contracts: Dict[str, Contract] = {}\n \n     def __init__(self, contracts):\n         for contract in contracts:\n@@ -107,66 +168,6 @@ class ContractsManager:\n         request.errback = eb_wrapper\n \n \n-class Contract:\n-    \"\"\" Abstract class for contracts \"\"\"\n-    request_cls = None\n-\n-    def __init__(self, method, *args):\n-        self.testcase_pre = _create_testcase(method, f'@{self.name} pre-hook')\n-        self.testcase_post = _create_testcase(method, f'@{self.name} post-hook')\n-        self.args = args\n-\n-    def add_pre_hook(self, request, results):\n-        if hasattr(self, 'pre_process'):\n-            cb = request.callback\n-\n-            @wraps(cb)\n-            def wrapper(response, **cb_kwargs):\n-                try:\n-                    results.startTest(self.testcase_pre)\n-                    self.pre_process(response)\n-                    results.stopTest(self.testcase_pre)\n-                except AssertionError:\n-                    results.addFailure(self.testcase_pre, sys.exc_info())\n-                except Exception:\n-                    results.addError(self.testcase_pre, sys.exc_info())\n-                else:\n-                    results.addSuccess(self.testcase_pre)\n-                finally:\n-                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n-\n-            request.callback = wrapper\n-\n-        return request\n-\n-    def add_post_hook(self, request, results):\n-        if hasattr(self, 'post_process'):\n-            cb = request.callback\n-\n-            @wraps(cb)\n-            def wrapper(response, **cb_kwargs):\n-                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n-                try:\n-                    results.startTest(self.testcase_post)\n-                    self.post_process(output)\n-                    results.stopTest(self.testcase_post)\n-                except AssertionError:\n-                    results.addFailure(self.testcase_post, sys.exc_info())\n-                except Exception:\n-                    results.addError(self.testcase_post, sys.exc_info())\n-                else:\n-                    results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output\n-\n-            request.callback = wrapper\n-\n-        return request\n-\n-    def adjust_request_args(self, args):\n-        return args\n-\n-\n def _create_testcase(method, desc):\n     spider = method.__self__.name\n \n\n@@ -1,10 +1,16 @@\n+import re\n import time\n-from http.cookiejar import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n+from http.cookiejar import CookieJar as _CookieJar, DefaultCookiePolicy\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n \n+# Defined in the http.cookiejar module, but undocumented:\n+# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\n+IPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n+\n+\n class CookieJar:\n     def __init__(self, policy=None, check_expired_frequency=10000):\n         self.policy = policy or DefaultCookiePolicy()\n\n@@ -8,6 +8,7 @@ from abc import ABCMeta\n from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n+from typing import Dict\n from warnings import warn\n \n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n@@ -75,7 +76,7 @@ class ItemMeta(_BaseItemMeta):\n \n class DictItem(MutableMapping, BaseItem):\n \n-    fields = {}\n+    fields: Dict[str, Field] = {}\n \n     def __new__(cls, *args, **kwargs):\n         if issubclass(cls, DictItem) and not issubclass(cls, Item):\n\n@@ -9,7 +9,7 @@ from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n-from email.utils import COMMASPACE, formatdate\n+from email.utils import formatdate\n from io import BytesIO\n \n from twisted.internet import defer, ssl\n@@ -21,6 +21,11 @@ from scrapy.utils.python import to_bytes\n logger = logging.getLogger(__name__)\n \n \n+# Defined in the email.utils module, but undocumented:\n+# https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42\n+COMMASPACE = \", \"\n+\n+\n def _to_bytes_or_none(text):\n     if text is None:\n         return None\n\n@@ -3,15 +3,16 @@ RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n import warnings\n+from typing import Tuple\n from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n \n-from scrapy.http import Request, Response\n-from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.python import to_unicode\n+from scrapy.exceptions import NotConfigured\n+from scrapy.http import Request, Response\n from scrapy.utils.misc import load_object\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n \n@@ -30,7 +31,8 @@ POLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n \n class ReferrerPolicy:\n \n-    NOREFERRER_SCHEMES = LOCAL_SCHEMES\n+    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n+    name: str\n \n     def referrer(self, response_url, request_url):\n         raise NotImplementedError()\n@@ -88,7 +90,7 @@ class NoReferrerPolicy(ReferrerPolicy):\n     is to be sent along with requests made from a particular request client to any origin.\n     The header will be omitted entirely.\n     \"\"\"\n-    name = POLICY_NO_REFERRER\n+    name: str = POLICY_NO_REFERRER\n \n     def referrer(self, response_url, request_url):\n         return None\n@@ -108,7 +110,7 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n \n     This is a user agent's default behavior, if no policy is otherwise specified.\n     \"\"\"\n-    name = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n+    name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n \n     def referrer(self, response_url, request_url):\n         if not self.tls_protected(response_url) or self.tls_protected(request_url):\n@@ -125,7 +127,7 @@ class SameOriginPolicy(ReferrerPolicy):\n     Cross-origin requests, on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n-    name = POLICY_SAME_ORIGIN\n+    name: str = POLICY_SAME_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         if self.origin(response_url) == self.origin(request_url):\n@@ -141,7 +143,7 @@ class OriginPolicy(ReferrerPolicy):\n     when making both same-origin requests and cross-origin requests\n     from a particular request client.\n     \"\"\"\n-    name = POLICY_ORIGIN\n+    name: str = POLICY_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         return self.origin_referrer(response_url)\n@@ -160,7 +162,7 @@ class StrictOriginPolicy(ReferrerPolicy):\n     on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n-    name = POLICY_STRICT_ORIGIN\n+    name: str = POLICY_STRICT_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         if (\n@@ -181,7 +183,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n     is sent as referrer information when making cross-origin requests\n     from a particular request client.\n     \"\"\"\n-    name = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n+    name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         origin = self.origin(response_url)\n@@ -208,7 +210,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n     on the other hand, will contain no referrer information.\n     A Referer HTTP header will not be sent.\n     \"\"\"\n-    name = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n+    name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n \n     def referrer(self, response_url, request_url):\n         origin = self.origin(response_url)\n@@ -234,7 +236,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n     to insecure origins.\n     Carefully consider the impact of setting such a policy for potentially sensitive documents.\n     \"\"\"\n-    name = POLICY_UNSAFE_URL\n+    name: str = POLICY_UNSAFE_URL\n \n     def referrer(self, response_url, request_url):\n         return self.stripped_referrer(response_url)\n@@ -246,8 +248,8 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     with the addition that \"Referer\" is not sent if the parent request was\n     using ``file://`` or ``s3://`` scheme.\n     \"\"\"\n-    NOREFERRER_SCHEMES = LOCAL_SCHEMES + ('file', 's3')\n-    name = POLICY_SCRAPY_DEFAULT\n+    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + ('file', 's3')\n+    name: str = POLICY_SCRAPY_DEFAULT\n \n \n _policy_classes = {p.name: p for p in (\n\n@@ -1,13 +1,16 @@\n \"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n \n-import weakref\n-from urllib.parse import urlparse\n+from typing import Union\n+from urllib.parse import urlparse, ParseResult\n+from weakref import WeakKeyDictionary\n+\n+from scrapy.http import Request, Response\n \n \n-_urlparse_cache = weakref.WeakKeyDictionary()\n+_urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = WeakKeyDictionary()\n \n \n-def urlparse_cached(request_or_response):\n+def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:\n     \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n     Request or Response object\n     \"\"\"\n\n@@ -4,20 +4,27 @@ scrapy.http.Request objects\n \"\"\"\n \n import hashlib\n-import weakref\n+from typing import Dict, Iterable, Optional, Tuple, Union\n from urllib.parse import urlunparse\n+from weakref import WeakKeyDictionary\n \n from w3lib.http import basic_auth_header\n from w3lib.url import canonicalize_url\n \n+from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n-_fingerprint_cache = weakref.WeakKeyDictionary()\n+_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n+_fingerprint_cache = WeakKeyDictionary()\n \n \n-def request_fingerprint(request, include_headers=None, keep_fragments=False):\n+def request_fingerprint(\n+    request: Request,\n+    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n+    keep_fragments: bool = False,\n+):\n     \"\"\"\n     Return the request fingerprint.\n \n@@ -49,17 +56,18 @@ def request_fingerprint(request, include_headers=None, keep_fragments=False):\n     (for instance when handling requests with a headless browser).\n \n     \"\"\"\n+    headers: Optional[Tuple[bytes, ...]] = None\n     if include_headers:\n-        include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n+        headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n     cache = _fingerprint_cache.setdefault(request, {})\n-    cache_key = (include_headers, keep_fragments)\n+    cache_key = (headers, keep_fragments)\n     if cache_key not in cache:\n         fp = hashlib.sha1()\n         fp.update(to_bytes(request.method))\n         fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n         fp.update(request.body or b'')\n-        if include_headers:\n-            for hdr in include_headers:\n+        if headers:\n+            for hdr in headers:\n                 if hdr in request.headers:\n                     fp.update(hdr)\n                     for v in request.headers.getlist(hdr):\n@@ -68,14 +76,14 @@ def request_fingerprint(request, include_headers=None, keep_fragments=False):\n     return cache[cache_key]\n \n \n-def request_authenticate(request, username, password):\n+def request_authenticate(request: Request, username: str, password: str) -> None:\n     \"\"\"Autenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n     request.headers['Authorization'] = basic_auth_header(username, password)\n \n \n-def request_httprepr(request):\n+def request_httprepr(request: Request) -> bytes:\n     \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n     This is provided only for reference since it's not the actual stream of\n     bytes that will be send when performing the request (that's controlled\n@@ -92,7 +100,7 @@ def request_httprepr(request):\n     return s\n \n \n-def referer_str(request):\n+def referer_str(request: Request) -> Optional[str]:\n     \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n     referrer = request.headers.get('Referer')\n     if referrer is None:\n\n@@ -3,19 +3,23 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n import os\n-import weakref\n import webbrowser\n import tempfile\n+from typing import Any, Callable, Iterable, Optional, Tuple, Union\n+from weakref import WeakKeyDictionary\n+\n+import scrapy\n+from scrapy.http.response import Response\n \n from twisted.web import http\n from scrapy.utils.python import to_bytes, to_unicode\n from w3lib import html\n \n \n-_baseurl_cache = weakref.WeakKeyDictionary()\n+_baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()\n \n \n-def get_base_url(response):\n+def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n         text = response.text[0:4096]\n@@ -23,10 +27,13 @@ def get_base_url(response):\n     return _baseurl_cache[response]\n \n \n-_metaref_cache = weakref.WeakKeyDictionary()\n+_metaref_cache: \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\" = WeakKeyDictionary()\n \n \n-def get_meta_refresh(response, ignore_tags=('script', 'noscript')):\n+def get_meta_refresh(\n+    response: \"scrapy.http.response.text.TextResponse\",\n+    ignore_tags: Optional[Iterable[str]] = ('script', 'noscript'),\n+) -> Union[Tuple[None, None], Tuple[float, str]]:\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n@@ -35,14 +42,15 @@ def get_meta_refresh(response, ignore_tags=('script', 'noscript')):\n     return _metaref_cache[response]\n \n \n-def response_status_message(status):\n+def response_status_message(status: Union[bytes, float, int, str]) -> str:\n     \"\"\"Return status code plus status text descriptive message\n     \"\"\"\n-    message = http.RESPONSES.get(int(status), \"Unknown Status\")\n-    return f'{status} {to_unicode(message)}'\n+    status_int = int(status)\n+    message = http.RESPONSES.get(status_int, \"Unknown Status\")\n+    return f'{status_int} {to_unicode(message)}'\n \n \n-def response_httprepr(response):\n+def response_httprepr(response: Response) -> bytes:\n     \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n     is provided only for reference, since it's not the exact stream of bytes\n     that was received (that's not exposed by Twisted).\n@@ -60,7 +68,10 @@ def response_httprepr(response):\n     return b\"\".join(values)\n \n \n-def open_in_browser(response, _openfunc=webbrowser.open):\n+def open_in_browser(\n+    response: Union[\"scrapy.http.response.html.HtmlResponse\", \"scrapy.http.response.text.TextResponse\"],\n+    _openfunc: Callable[[str], Any] = webbrowser.open,\n+) -> Any:\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n     \"\"\"\n\n@@ -9,14 +9,15 @@ and no performance penalty at all when disabled (as object_ref becomes just an\n alias to object in that case).\n \"\"\"\n \n-import weakref\n-from time import time\n-from operator import itemgetter\n from collections import defaultdict\n+from operator import itemgetter\n+from time import time\n+from typing import DefaultDict\n+from weakref import WeakKeyDictionary\n \n \n NoneType = type(None)\n-live_refs = defaultdict(weakref.WeakKeyDictionary)\n+live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n \n \n class object_ref:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
