{"custom_id": "scrapy#c40e5a7491b78eeda6bc031984a68d9f5a29efc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 197 | Lines Deleted: 198 | Files Changed: 2 | Hunks: 2 | Methods Changed: 34 | Complexity Δ (Sum/Max): 0/36 | Churn Δ: 395 | Churn Cumulative: 897 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1 +1,197 @@\n-from scrapy.core.downloader.manager import Downloader\n+\"\"\"\n+Download web pages using asynchronous IO\n+\"\"\"\n+\n+import random\n+from time import time\n+\n+from twisted.internet import reactor, defer\n+from twisted.python.failure import Failure\n+\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.conf import settings\n+from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.signal import send_catch_log\n+from scrapy import signals\n+from scrapy import log\n+from .middleware import DownloaderMiddlewareManager\n+from .handlers import DownloadHandlers\n+\n+\n+class SpiderInfo(object):\n+    \"\"\"Simple class to keep information and state for each open spider\"\"\"\n+\n+    def __init__(self, download_delay=None, max_concurrent_requests=None):\n+        if download_delay is None:\n+            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')\n+        else:\n+            self._download_delay = float(download_delay)\n+        if self._download_delay:\n+            self.max_concurrent_requests = 1\n+        elif max_concurrent_requests is None:\n+            self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n+        else:\n+            self.max_concurrent_requests =  max_concurrent_requests\n+        if self._download_delay and settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n+            # same policy as wget --random-wait\n+            self.random_delay_interval = (0.5*self._download_delay, \\\n+                1.5*self._download_delay)\n+        else:\n+            self.random_delay_interval = None\n+\n+        self.active = set()\n+        self.queue = []\n+        self.transferring = set()\n+        self.closing = False\n+        self.lastseen = 0\n+        self.next_request_calls = set()\n+\n+    def free_transfer_slots(self):\n+        return self.max_concurrent_requests - len(self.transferring)\n+\n+    def needs_backout(self):\n+        # use self.active to include requests in the downloader middleware\n+        return len(self.active) > 2 * self.max_concurrent_requests\n+\n+    def download_delay(self):\n+        if self.random_delay_interval:\n+            return random.uniform(*self.random_delay_interval)\n+        else:\n+            return self._download_delay\n+\n+    def cancel_request_calls(self):\n+        for call in self.next_request_calls:\n+            call.cancel()\n+        self.next_request_calls.clear()\n+\n+\n+class Downloader(object):\n+    \"\"\"Mantain many concurrent downloads and provide an HTTP abstraction.\n+    It supports a limited number of connections per spider and many spiders in\n+    parallel.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.sites = {}\n+        self.handlers = DownloadHandlers()\n+        self.middleware = DownloaderMiddlewareManager.from_settings(settings)\n+        self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n+\n+    def fetch(self, request, spider):\n+        \"\"\"Main method to use to request a download\n+\n+        This method includes middleware mangling. Middleware can returns a\n+        Response object, then request never reach downloader queue, and it will\n+        not be downloaded from site.\n+        \"\"\"\n+        site = self.sites[spider]\n+        if site.closing:\n+            raise IgnoreRequest('Cannot fetch on a closing spider')\n+\n+        site.active.add(request)\n+        def _deactivate(response):\n+            send_catch_log(signal=signals.response_received, \\\n+                response=response, request=request, spider=spider)\n+            site.active.remove(request)\n+            self._close_if_idle(spider)\n+            return response\n+\n+        dfd = self.middleware.download(self.enqueue, request, spider)\n+        return dfd.addBoth(_deactivate)\n+\n+    def enqueue(self, request, spider):\n+        \"\"\"Enqueue a Request for a effective download from site\"\"\"\n+        site = self.sites[spider]\n+        if site.closing:\n+            raise IgnoreRequest\n+\n+        def _downloaded(response):\n+            send_catch_log(signal=signals.response_downloaded, \\\n+                    response=response, request=request, spider=spider)\n+            return response\n+\n+        deferred = defer.Deferred().addCallback(_downloaded)\n+        site.queue.append((request, deferred))\n+        self._process_queue(spider)\n+        return deferred\n+\n+    def _process_queue(self, spider):\n+        \"\"\"Effective download requests from site queue\"\"\"\n+        site = self.sites.get(spider)\n+        if not site:\n+            return\n+\n+        # Delay queue processing if a download_delay is configured\n+        now = time()\n+        delay = site.download_delay()\n+        if delay:\n+            penalty = delay - now + site.lastseen\n+            if penalty > 0:\n+                d = defer.Deferred()\n+                d.addCallback(self._process_queue)\n+                call = reactor.callLater(penalty, d.callback, spider)\n+                site.next_request_calls.add(call)\n+                d.addBoth(lambda x: site.next_request_calls.remove(call))\n+                return\n+        site.lastseen = now\n+\n+        # Process enqueued requests if there are free slots to transfer for this site\n+        while site.queue and site.free_transfer_slots() > 0:\n+            request, deferred = site.queue.pop(0)\n+            if site.closing:\n+                dfd = defer.fail(Failure(IgnoreRequest()))\n+            else:\n+                dfd = self._download(site, request, spider)\n+            dfd.chainDeferred(deferred)\n+\n+        self._close_if_idle(spider)\n+\n+    def _close_if_idle(self, spider):\n+        site = self.sites.get(spider)\n+        if site and site.closing and not site.active:\n+            del self.sites[spider]\n+            site.closing.callback(None)\n+\n+    def _download(self, site, request, spider):\n+        # The order is very important for the following deferreds. Do not change!\n+\n+        # 1. Create the download deferred\n+        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n+\n+        # 2. After response arrives,  remove the request from transferring\n+        # state to free up the transferring slot so it can be used by the\n+        # following requests (perhaps those which came from the downloader\n+        # middleware itself)\n+        site.transferring.add(request)\n+        def finish_transferring(_):\n+            site.transferring.remove(request)\n+            self._process_queue(spider)\n+            # avoid partially downloaded responses from propagating to the\n+            # downloader middleware, to speed-up the closing process\n+            if site.closing:\n+                log.msg(\"Crawled while closing spider: %s\" % request, \\\n+                    level=log.DEBUG, spider=spider)\n+                raise IgnoreRequest\n+            return _\n+        return dfd.addBoth(finish_transferring)\n+\n+    def open_spider(self, spider):\n+        \"\"\"Allocate resources to begin processing a spider\"\"\"\n+        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n+        self.sites[spider] = SpiderInfo(\n+            download_delay=getattr(spider, 'download_delay', None),\n+            max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n+        )\n+\n+    def close_spider(self, spider):\n+        \"\"\"Free any resources associated with the given spider\"\"\"\n+        assert spider in self.sites, \"Spider not opened: %s\" % spider\n+        site = self.sites.get(spider)\n+        site.closing = defer.Deferred()\n+        site.cancel_request_calls()\n+        self._process_queue(spider)\n+        return site.closing\n+\n+    def is_idle(self):\n+        return not self.sites\n+\n\n@@ -1,197 +0,0 @@\n-\"\"\"\n-Download web pages using asynchronous IO\n-\"\"\"\n-\n-import random\n-from time import time\n-\n-from twisted.internet import reactor, defer\n-from twisted.python.failure import Failure\n-\n-from scrapy.exceptions import IgnoreRequest\n-from scrapy.conf import settings\n-from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.signal import send_catch_log\n-from scrapy import signals\n-from scrapy import log\n-from .middleware import DownloaderMiddlewareManager\n-from .handlers import DownloadHandlers\n-\n-\n-class SpiderInfo(object):\n-    \"\"\"Simple class to keep information and state for each open spider\"\"\"\n-\n-    def __init__(self, download_delay=None, max_concurrent_requests=None):\n-        if download_delay is None:\n-            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')\n-        else:\n-            self._download_delay = float(download_delay)\n-        if self._download_delay:\n-            self.max_concurrent_requests = 1\n-        elif max_concurrent_requests is None:\n-            self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n-        else:\n-            self.max_concurrent_requests =  max_concurrent_requests\n-        if self._download_delay and settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n-            # same policy as wget --random-wait\n-            self.random_delay_interval = (0.5*self._download_delay, \\\n-                1.5*self._download_delay)\n-        else:\n-            self.random_delay_interval = None\n-\n-        self.active = set()\n-        self.queue = []\n-        self.transferring = set()\n-        self.closing = False\n-        self.lastseen = 0\n-        self.next_request_calls = set()\n-\n-    def free_transfer_slots(self):\n-        return self.max_concurrent_requests - len(self.transferring)\n-\n-    def needs_backout(self):\n-        # use self.active to include requests in the downloader middleware\n-        return len(self.active) > 2 * self.max_concurrent_requests\n-\n-    def download_delay(self):\n-        if self.random_delay_interval:\n-            return random.uniform(*self.random_delay_interval)\n-        else:\n-            return self._download_delay\n-\n-    def cancel_request_calls(self):\n-        for call in self.next_request_calls:\n-            call.cancel()\n-        self.next_request_calls.clear()\n-\n-\n-class Downloader(object):\n-    \"\"\"Mantain many concurrent downloads and provide an HTTP abstraction.\n-    It supports a limited number of connections per spider and many spiders in\n-    parallel.\n-    \"\"\"\n-\n-    def __init__(self):\n-        self.sites = {}\n-        self.handlers = DownloadHandlers()\n-        self.middleware = DownloaderMiddlewareManager.from_settings(settings)\n-        self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n-\n-    def fetch(self, request, spider):\n-        \"\"\"Main method to use to request a download\n-\n-        This method includes middleware mangling. Middleware can returns a\n-        Response object, then request never reach downloader queue, and it will\n-        not be downloaded from site.\n-        \"\"\"\n-        site = self.sites[spider]\n-        if site.closing:\n-            raise IgnoreRequest('Cannot fetch on a closing spider')\n-\n-        site.active.add(request)\n-        def _deactivate(response):\n-            send_catch_log(signal=signals.response_received, \\\n-                response=response, request=request, spider=spider)\n-            site.active.remove(request)\n-            self._close_if_idle(spider)\n-            return response\n-\n-        dfd = self.middleware.download(self.enqueue, request, spider)\n-        return dfd.addBoth(_deactivate)\n-\n-    def enqueue(self, request, spider):\n-        \"\"\"Enqueue a Request for a effective download from site\"\"\"\n-        site = self.sites[spider]\n-        if site.closing:\n-            raise IgnoreRequest\n-\n-        def _downloaded(response):\n-            send_catch_log(signal=signals.response_downloaded, \\\n-                    response=response, request=request, spider=spider)\n-            return response\n-\n-        deferred = defer.Deferred().addCallback(_downloaded)\n-        site.queue.append((request, deferred))\n-        self._process_queue(spider)\n-        return deferred\n-\n-    def _process_queue(self, spider):\n-        \"\"\"Effective download requests from site queue\"\"\"\n-        site = self.sites.get(spider)\n-        if not site:\n-            return\n-\n-        # Delay queue processing if a download_delay is configured\n-        now = time()\n-        delay = site.download_delay()\n-        if delay:\n-            penalty = delay - now + site.lastseen\n-            if penalty > 0:\n-                d = defer.Deferred()\n-                d.addCallback(self._process_queue)\n-                call = reactor.callLater(penalty, d.callback, spider)\n-                site.next_request_calls.add(call)\n-                d.addBoth(lambda x: site.next_request_calls.remove(call))\n-                return\n-        site.lastseen = now\n-\n-        # Process enqueued requests if there are free slots to transfer for this site\n-        while site.queue and site.free_transfer_slots() > 0:\n-            request, deferred = site.queue.pop(0)\n-            if site.closing:\n-                dfd = defer.fail(Failure(IgnoreRequest()))\n-            else:\n-                dfd = self._download(site, request, spider)\n-            dfd.chainDeferred(deferred)\n-\n-        self._close_if_idle(spider)\n-\n-    def _close_if_idle(self, spider):\n-        site = self.sites.get(spider)\n-        if site and site.closing and not site.active:\n-            del self.sites[spider]\n-            site.closing.callback(None)\n-\n-    def _download(self, site, request, spider):\n-        # The order is very important for the following deferreds. Do not change!\n-\n-        # 1. Create the download deferred\n-        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n-\n-        # 2. After response arrives,  remove the request from transferring\n-        # state to free up the transferring slot so it can be used by the\n-        # following requests (perhaps those which came from the downloader\n-        # middleware itself)\n-        site.transferring.add(request)\n-        def finish_transferring(_):\n-            site.transferring.remove(request)\n-            self._process_queue(spider)\n-            # avoid partially downloaded responses from propagating to the\n-            # downloader middleware, to speed-up the closing process\n-            if site.closing:\n-                log.msg(\"Crawled while closing spider: %s\" % request, \\\n-                    level=log.DEBUG, spider=spider)\n-                raise IgnoreRequest\n-            return _\n-        return dfd.addBoth(finish_transferring)\n-\n-    def open_spider(self, spider):\n-        \"\"\"Allocate resources to begin processing a spider\"\"\"\n-        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n-        self.sites[spider] = SpiderInfo(\n-            download_delay=getattr(spider, 'download_delay', None),\n-            max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n-        )\n-\n-    def close_spider(self, spider):\n-        \"\"\"Free any resources associated with the given spider\"\"\"\n-        assert spider in self.sites, \"Spider not opened: %s\" % spider\n-        site = self.sites.get(spider)\n-        site.closing = defer.Deferred()\n-        site.cancel_request_calls()\n-        self._process_queue(spider)\n-        return site.closing\n-\n-    def is_idle(self):\n-        return not self.sites\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23b13f0cc89f9a8b2cae54edfa94ca9b27d0ec26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 49 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 62 | Churn Cumulative: 64 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,58 +1,31 @@\n \"\"\"\n-This module implements the Scheduler Middleware manager.\n-\n-For more information see the Scheduler Middleware doc in:\n-docs/topics/scheduler-middleware.rst\n+Scheduler Middleware manager\n \n+TODO: To be removed in Scrapy 0.11\n \"\"\"\n-from collections import defaultdict\n+\n from twisted.internet.defer import Deferred\n \n-from scrapy import log\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n-from scrapy.conf import settings\n \n-class SchedulerMiddlewareManager(object):\n+class SchedulerMiddlewareManager(MiddlewareManager):\n \n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n-        self.mw_cbs = defaultdict(list)\n-        self.load()\n+    component_name = 'scheduler middleware'\n \n-    def load(self):\n-        \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_component_list(settings['SCHEDULER_MIDDLEWARES_BASE'], \\\n-            settings['SCHEDULER_MIDDLEWARES'])\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        for mwpath in mwlist:\n-            try:\n-                cls = load_object(mwpath)\n-                mw = cls()\n-                self.enabled[cls.__name__] = mw\n-                self._add_middleware(mw)\n-            except NotConfigured, e:\n-                self.disabled[cls.__name__] = mwpath\n-                if e.args:\n-                    log.msg(e)\n-        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n-            level=log.DEBUG)\n-        self.loaded = True\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n+            settings['DOWNLOADER_MIDDLEWARES'])\n \n     def _add_middleware(self, mw):\n-        for name in ['enqueue_request', 'open_spider', 'close_spider']:\n-            mwfunc = getattr(mw, name, None)\n-            if mwfunc:\n-                self.mw_cbs[name].append(mwfunc)\n+        if hasattr(mw, 'enqueue_request'):\n+            self.methods['enqueue_request'].append(mw.enqueue_request)\n \n     def enqueue_request(self, wrappedfunc, spider, request):\n         def _enqueue_request(request):\n-            for mwfunc in self.mw_cbs['enqueue_request']:\n+            for mwfunc in self.methods['enqueue_request']:\n                 result = mwfunc(spider=spider, request=request)\n                 assert result is None or isinstance(result, Deferred), \\\n                         'Middleware %s.enqueue_request must return None or Deferred, got %s' % \\\n@@ -63,12 +36,3 @@ class SchedulerMiddlewareManager(object):\n \n         deferred = mustbe_deferred(_enqueue_request, request)\n         return deferred\n-\n-    def open_spider(self, spider):\n-        for mwfunc in self.mw_cbs['open_spider']:\n-            mwfunc(spider)\n-\n-    def close_spider(self, spider):\n-        for mwfunc in self.mw_cbs['close_spider']:\n-            mwfunc(spider)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbc0ba7236ef8828b2e566cd2aee6a00cc62c049", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,12 @@ class FieldDescriptor(object):\n         self.required = required\n         self.allow_markup = allow_markup\n     \n+    @classmethod\n+    def from_field(cls, name, field):\n+        return cls(name, field.get('description'), \\\n+            field.get('ibl_extractor', text), field.get('required', False), \\\n+            field.get('allow_markup', False))\n+\n     def __str__(self):\n         return \"FieldDescriptor(%s)\" % self.name\n \n@@ -33,6 +39,11 @@ class ItemDescriptor(object):\n         self._required_attributes = [d.name for d in attribute_descriptors \\\n                 if d.required]\n \n+    @classmethod\n+    def from_item(cls, name, description, item):\n+        a = [FieldDescriptor.from_field(n, f) for n, f in item.fields.items()]\n+        return cls(name, description, a)\n+\n     def validated(self, data):\n         \"\"\"Only return the items in the data that are valid\"\"\"\n         return [d for d in data if self._item_validates(d)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ba6fa2096346d08a3481cee52e583333e7384b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1583 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,7 +197,7 @@ class ExecutionEngine(object):\n         assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n-        self.scheduler.open_spider(spider)\n+        yield self.scheduler.open_spider(spider)\n         self.downloader.open_spider(spider)\n         yield self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n\n@@ -21,7 +21,7 @@ class Scheduler(object):\n     def __init__(self):\n         self.pending_requests = {}\n         self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n-        self.middleware = SchedulerMiddlewareManager()\n+        self.middleware = SchedulerMiddlewareManager.from_settings(settings)\n \n     def spider_is_open(self, spider):\n         \"\"\"Check if scheduler's resources were allocated for a spider\"\"\"\n@@ -39,7 +39,7 @@ class Scheduler(object):\n \n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[spider] = Priority()\n-        self.middleware.open_spider(spider)\n+        return self.middleware.open_spider(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Called when a spider has finished scraping to free any resources\n@@ -47,8 +47,8 @@ class Scheduler(object):\n         \"\"\"\n         if spider not in self.pending_requests:\n             raise RuntimeError('Scheduler spider is not open: %s' % spider)\n-        self.middleware.close_spider(spider)\n         self.pending_requests.pop(spider, None)\n+        return self.middleware.close_spider(spider)\n \n     def enqueue_request(self, spider, request):\n         \"\"\"Enqueue a request to be downloaded for a spider that is currently being scraped.\"\"\"\n\n@@ -16,10 +16,11 @@ class SchedulerMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n-            settings['DOWNLOADER_MIDDLEWARES'])\n+        return build_component_list(settings['SCHEDULER_MIDDLEWARES_BASE'], \\\n+            settings['SCHEDULER_MIDDLEWARES'])\n \n     def _add_middleware(self, mw):\n+        super(SchedulerMiddlewareManager, self)._add_middleware(mw)\n         if hasattr(mw, 'enqueue_request'):\n             self.methods['enqueue_request'].append(mw.enqueue_request)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9ebebb230e7ecda2f55944bd20ba76940711b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 41 | Churn Cumulative: 69 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,19 +0,0 @@\n-\"\"\"\n-Url Filter Middleware\n-\n-See documentation in docs/refs/spider-middleware.rst\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.utils.url import canonicalize_url\n-\n-class UrlFilterMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        disabled = getattr(spider, 'urlfilter_disabled', False)\n-        for r in result:\n-            if isinstance(r, Request) and not disabled:\n-                curl = canonicalize_url(r.url)\n-                # only assign if different to avoid re-calculating fingerprint\n-                if curl != r.url: \n-                    r.url = curl\n-            yield r\n\n@@ -1,22 +0,0 @@\n-from unittest import TestCase\n-\n-from scrapy.contrib.spidermiddleware.urlfilter import UrlFilterMiddleware\n-from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n-from scrapy.utils.url import canonicalize_url\n-\n-\n-class TestUrlFilterMiddleware(TestCase):\n-\n-    def setUp(self):\n-        self.spider = BaseSpider('foo')\n-        self.mw = UrlFilterMiddleware()\n-\n-    def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n-        req_url = 'http://scrapytest.org/?last=1&first=2'\n-        reqs = [Request(req_url)]\n-\n-        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEquals(out[0].url, canonicalize_url(req_url))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1aab2f58ef7c701f2d7468add9392238a08f2e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 1945 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -159,12 +159,7 @@ class ExecutionEngine(object):\n                     level=log.DEBUG, spider=spider)\n                 return response\n             elif isinstance(response, Request):\n-                newrequest = response\n-                dfd = mustbe_deferred(self.schedule, newrequest, spider)\n-                if newrequest.callback:\n-                    # XXX: this is a bit hacky and should be removed\n-                    dfd.addCallbacks(newrequest.callback, newrequest.errback)\n-                return dfd\n+                return mustbe_deferred(self.schedule, response, spider)\n \n         def _on_error(_failure):\n             \"\"\"handle an error processing a page\"\"\"\n\n@@ -99,7 +99,7 @@ class Request(object_ref):\n         given new values.\n         \"\"\"\n         for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', \\\n-                'encoding', 'priority', 'dont_filter']:\n+                'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -131,8 +131,8 @@ class RequestTest(unittest.TestCase):\n         # make sure copy does not propagate callbacks\n         assert r1.callback is somecallback\n         assert r1.errback is somecallback\n-        assert r2.callback is None\n-        assert r2.errback is None\n+        assert r2.callback is r1.callback\n+        assert r2.errback is r2.errback\n \n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7da79b90fe79582f567b11f93444ac446eb15970", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 41 | Files Changed: 10 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 63 | Churn Cumulative: 1416 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,13 +38,8 @@ class BaseSgmlRequestExtractor(FixedSGMLParser):\n \n     def _make_absolute_urls(self, base_url, encoding):\n         \"\"\"Makes all request's urls absolute\"\"\"\n-        for req in self.requests:\n-            url = req.url\n-            # make absolute url\n-            url = urljoin_rfc(base_url, url, encoding)\n-            url = safe_url_string(url, encoding)\n-            # replace in-place request's url\n-            req.url = url\n+        self.requests = [x.replace(url=safe_url_string(urljoin_rfc(base_url, \\\n+            x.url, encoding), encoding)) for x in self.requests]\n \n     def _fix_link_text_encoding(self, encoding):\n         \"\"\"Convert link_text to unicode for each request\"\"\"\n\n@@ -2,20 +2,16 @@\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n \n-from itertools import ifilter, imap\n+from itertools import ifilter\n \n import re\n \n class Canonicalize(object):\n     \"\"\"Canonicalize Request Processor\"\"\"\n-    def _replace_url(self, req):\n-        # replace in-place\n-        req.url = canonicalize_url(req.url)\n-        return req\n \n     def __call__(self, requests):\n         \"\"\"Canonicalize all requests' urls\"\"\"\n-        return imap(self._replace_url, requests)\n+        return (x.replace(url=canonicalize_url(x.url)) for x in requests)\n         \n \n class FilterDupes(object):\n\n@@ -0,0 +1,9 @@\n+import warnings\n+\n+def deprecated_setter(setter, attrname):\n+    def newsetter(self, value):\n+        c = self.__class__.__name__\n+        warnings.warn(\"Don't modify %s.%s attribute, use %s.replace() instead\" % \\\n+            (c, attrname, c), DeprecationWarning, stacklevel=2)\n+        return setter(self, value)\n+    return newsetter\n\n@@ -10,6 +10,8 @@ import copy\n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n from scrapy.utils.trackref import object_ref\n+from scrapy.utils.decorator import deprecated\n+from scrapy.http.common import deprecated_setter\n \n class Request(object_ref):\n \n@@ -58,7 +60,7 @@ class Request(object_ref):\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n \n-    url = property(_get_url, _set_url)\n+    url = property(_get_url, deprecated_setter(_set_url, 'url'))\n \n     def _get_body(self):\n         return self._body\n@@ -76,7 +78,7 @@ class Request(object_ref):\n         else:\n             raise TypeError(\"Request body must either str or unicode. Got: '%s'\" % type(body).__name__)\n \n-    body = property(_get_body, _set_body)\n+    body = property(_get_body, deprecated_setter(_set_body, 'body'))\n \n     @property\n     def encoding(self):\n\n@@ -33,7 +33,7 @@ class FormRequest(Request):\n             query = [(unicode_to_str(k, self.encoding), _unicode_to_str(v, self.encoding))\n                     for k, v in items]\n             self.method = 'POST'\n-            self.body = urllib.urlencode(query, doseq=1)\n+            self._set_body(urllib.urlencode(query, doseq=1))\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n \n     @classmethod\n\n@@ -9,6 +9,7 @@ import copy\n \n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n+from scrapy.http.common import deprecated_setter\n \n class Response(object_ref):\n \n@@ -40,7 +41,7 @@ class Response(object_ref):\n             raise TypeError('%s url must be str, got %s:' % (type(self).__name__, \\\n                 type(url).__name__))\n \n-    url = property(_get_url, _set_url)\n+    url = property(_get_url, deprecated_setter(_set_url, 'url'))\n \n     def _get_body(self):\n         return self._body\n@@ -57,7 +58,7 @@ class Response(object_ref):\n             raise TypeError(\"Response body must either str or unicode. Got: '%s'\" \\\n                 % type(body).__name__)\n \n-    body = property(_get_body, _set_body)\n+    body = property(_get_body, deprecated_setter(_set_body, 'body'))\n \n     def __repr__(self):\n         attrs = ['url', 'status', 'body', 'headers', 'meta', 'flags']\n\n@@ -33,9 +33,6 @@ class TextResponse(Response):\n         self._cached_ubody = None\n         super(TextResponse, self).__init__(url, status, headers, body, meta, flags)\n \n-    def _get_url(self):\n-        return self._url\n-\n     def _set_url(self, url):\n         if isinstance(url, unicode):\n             if self.encoding is None:\n@@ -45,8 +42,6 @@ class TextResponse(Response):\n         else:\n             super(TextResponse, self)._set_url(url)\n \n-    url = property(_get_url, _set_url)\n-\n     def _set_body(self, body):\n         self._body = ''\n         if isinstance(body, unicode):\n\n@@ -29,9 +29,6 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r.url, \"http://www.example.com\")\n         self.assertEqual(r.method, self.default_method)\n \n-        r.url = \"http://www.example.com/other\"\n-        assert isinstance(r.url, str)\n-\n         assert isinstance(r.headers, Headers)\n         self.assertEqual(r.headers, self.default_headers)\n         self.assertEqual(r.meta, self.default_meta)\n@@ -80,22 +77,12 @@ class RequestTest(unittest.TestCase):\n         r = self.request_class(url=\"http://www.scrapy.org/path\")\n         self.assertEqual(r.url, \"http://www.scrapy.org/path\")\n \n-        # url quoting on attribute assign\n-        r.url = \"http://www.scrapy.org/blank%20space\"\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-        r.url = \"http://www.scrapy.org/blank space\"\n-        self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n-\n         # url quoting on creation\n         r = self.request_class(url=\"http://www.scrapy.org/blank%20space\")\n         self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n         r = self.request_class(url=\"http://www.scrapy.org/blank space\")\n         self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n \n-        # url coercion to string\n-        r.url = u\"http://www.scrapy.org/test\"\n-        self.assert_(isinstance(r.url, str))\n-\n         # url encoding\n         r1 = self.request_class(url=u\"http://www.scrapy.org/price/\\xa3\", encoding=\"utf-8\")\n         r2 = self.request_class(url=u\"http://www.scrapy.org/price/\\xa3\", encoding=\"latin1\")\n\n@@ -154,9 +154,6 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xc2\\xa3')\n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n-        resp = self.response_class(url=\"http://www.example.com/price/\", encoding='utf-8')\n-        resp.url = u'http://www.example.com/price/\\xa3'\n-        self.assertEqual(resp.url, 'http://www.example.com/price/\\xc2\\xa3')\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xc2\\xa3')\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n\n@@ -51,8 +51,7 @@ class UtilsRequestTest(unittest.TestCase):\n         # cached fingerprint must be cleared on request copy\n         r1 = Request(\"http://www.example.com\")\n         fp1 = request_fingerprint(r1)\n-        r2 = r1.copy()\n-        r2.url = \"http://www.example.com/other\"\n+        r2 = r1.replace(url = \"http://www.example.com/other\")\n         fp2 = request_fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb7cc7c8bb053669e5518cd43edee5858e97f751", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 26 | Churn Cumulative: 334 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,24 @@ class SqliteDictTest(unittest.TestCase):\n         d.clear()\n         self.failIf(d.items())\n \n+    def test_in(self):\n+        d = self.dict_class()\n+        self.assertFalse('test' in d)\n+        d['test'] = 123\n+        self.assertTrue('test' in d)\n+\n+    def test_keyerror(self):\n+        d = self.dict_class()\n+        self.assertRaises(KeyError, d.__getitem__, 'test')\n+\n+    def test_replace(self):\n+        d = self.dict_class()\n+        self.assertEqual(d.get('test'), None)\n+        d['test'] = 123\n+        self.assertEqual(d.get('test'), 123)\n+        d['test'] = 456\n+        self.assertEqual(d.get('test'), 456)\n+\n \n class JsonSqliteDictTest(SqliteDictTest):\n \n\n@@ -19,12 +19,14 @@ class SqliteDict(DictMixin):\n     def __getitem__(self, key):\n         key = self.encode(key)\n         q = \"select value from %s where key=?\" % self.table\n-        value = self.conn.execute(q, (key,)).fetchone()[0]\n-        return self.decode(value)\n+        value = self.conn.execute(q, (key,)).fetchone()\n+        if value:\n+            return self.decode(value[0])\n+        raise KeyError(key)\n \n     def __setitem__(self, key, value):\n         key, value = self.encode(key), self.encode(value)\n-        q = \"insert into %s (key, value) values (?,?)\" % self.table\n+        q = \"insert or replace into %s (key, value) values (?,?)\" % self.table\n         self.conn.execute(q, (key, value))\n         self.conn.commit()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d5a7df125fedb6268cb9042f60ba49364a02e30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 41 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ class LogFormatter(object):\n         return \"Scraped %s in <%s>\" % (item, request.url)\n \n     def dropped(self, item, exception, spider):\n-        return \"Dropped %s - %s\" % (item, str(exception))\n+        return \"Dropped %s - %s\" % (item, unicode(exception))\n \n     def passed(self, item, spider):\n         return \"Passed %s\" % item\n\n@@ -22,6 +22,11 @@ class LoggingContribTest(unittest.TestCase):\n         self.assertEqual(self.formatter.crawled(req, res, self.spider),\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n+    def test_dropped(self):\n+        item = {}\n+        exception = Exception(u\"\\u2018\")\n+        self.assertEqual(self.formatter.dropped(item, exception, self.spider),\n+            u\"Dropped {} - \\u2018\")\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe77794f7648b412f220229612c6b57bff9ab81a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 81 | Churn Cumulative: 528 | Contributors (this commit): 5 | Commits (past 90d): 31 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,6 +124,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closespider.CloseSpider': 0,\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n+    'scrapy.contrib.spidercontext.SpiderContext': 0,\n }\n \n FEED_URI = None\n@@ -231,6 +232,10 @@ SPIDER_MODULES = []\n \n SPIDER_QUEUE_CLASS = 'scrapy.spiderqueue.SqliteSpiderQueue'\n \n+SPIDER_CONTEXT_ENABLED = True\n+SPIDER_CONTEXT_STORAGE_CLASS = 'scrapy.contrib.spidercontext.SqliteSpiderContextStorage'\n+\n+\n SQLITE_DB = 'scrapy.db'\n \n SQS_QUEUE = 'scrapy'\n\n@@ -0,0 +1,67 @@\n+import sqlite3\n+\n+from zope.interface import Interface, implements\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.sqlite import JsonSqliteDict\n+from scrapy import log, signals\n+\n+class ISpiderContextStorage(Interface):\n+\n+    def get(spider):\n+        \"\"\"Get the context for the given spider, or None if the spider has no\n+        context stored.\"\"\"\n+\n+    def put(spider, context):\n+        \"\"\"Store the context for the given spider\"\"\"\n+\n+\n+class SqliteSpiderContextStorage(object):\n+\n+    implements(ISpiderContextStorage)\n+    sqlite_dict_class = JsonSqliteDict\n+\n+    def __init__(self, database=None, table='contexts'):\n+        try:\n+            self.d = self.sqlite_dict_class(database, table)\n+        except sqlite3.Error, e:\n+            self.d = self.sqlite_dict_class(':memory:', table)\n+            log.msg(\"Cannot open SQLite %r - using in-memory context storage \" \\\n+                \"instead. Error was: %r\" % (database, str(e)), log.WARNING)\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(settings['SQLITE_DB'])\n+\n+    def get(self, spider):\n+        if spider.name in self.d:\n+            return self.d[spider.name]\n+\n+    def put(self, spider, context):\n+        self.d[spider.name] = context\n+\n+\n+class SpiderContext(object):\n+\n+    def __init__(self, storage):\n+        dispatcher.connect(self._spider_opened, signals.spider_opened)\n+        dispatcher.connect(self._spider_closed, signals.spider_closed)\n+        self.storage = storage\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        if not settings.getbool('SPIDER_CONTEXT_ENABLED'):\n+            raise NotConfigured\n+        stcls = load_object(settings['SPIDER_CONTEXT_STORAGE_CLASS'])\n+        storage = stcls.from_settings(settings)\n+        return cls(storage)\n+\n+    def _spider_opened(self, spider):\n+        spider.context = self.storage.get(spider) or {}\n+\n+    def _spider_closed(self, spider):\n+        if spider.context:\n+            self.storage.put(spider, spider.context)\n+\n\n@@ -0,0 +1,9 @@\n+from twisted.trial import unittest\n+from zope.interface.verify import verifyObject\n+\n+from scrapy.contrib.spidercontext import ISpiderContextStorage, SqliteSpiderContextStorage\n+\n+class SqliteSpiderContextStorageTest(unittest.TestCase):\n+\n+    def test_interface(self):\n+        verifyObject(ISpiderContextStorage, SqliteSpiderContextStorage())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38bbec8dcaba4a4a5548d43a711349465eda1a3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 83 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 0, 'dev')\n-__version__ = \"0.10-dev\"\n+version_info = (0, 10, 0, 'rc1')\n+__version__ = \"0.10-rc1\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c041328b45dfb062217dbe65848304a41ef3a576", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 453 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,6 +51,13 @@ class Shell(object):\n             self.populate_vars(request.url, response, request, spider)\n         start_python_console(self.vars)\n \n+    def _schedule(self, request, spider):\n+        if spider is None:\n+            spider = create_spider_for_request(self.crawler.spiders, request, \\\n+                BaseSpider('default'), log_multiple=True)\n+        self.crawler.engine.open_spider(spider)\n+        return self.crawler.engine.schedule(request, spider)\n+\n     def fetch(self, request_or_url, spider=None):\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n@@ -58,14 +65,10 @@ class Shell(object):\n         else:\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n-        if spider is None:\n-            spider = create_spider_for_request(self.crawler.spiders, request, \\\n-                BaseSpider('default'), log_multiple=True)\n-        self.crawler.engine.open_spider(spider)\n         response = None\n         try:\n             response = threads.blockingCallFromThread(reactor, \\\n-                self.crawler.engine.schedule, request, spider)\n+                self._schedule, request, spider)\n         except:\n             log.err(Failure(), \"Error fetching response\", spider=spider)\n         self.populate_vars(url, response, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9ea9dcf1a8575f8e89db97b4cab45383d9bb3ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 140 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n+from time import time # used in global tests code\n+\n from scrapy.project import crawler\n \n def get_engine_status(engine=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad2b979e0f21561c48dc7ac593faea003128f8b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 509 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,7 @@ class TextResponse(Response):\n             super(TextResponse, self)._set_body(body)\n \n     def replace(self, *args, **kwargs):\n-        kwargs.setdefault('encoding', getattr(self, '_encoding', None))\n+        kwargs.setdefault('encoding', self.encoding)\n         return Response.replace(self, *args, **kwargs)\n \n     @property\n\n@@ -287,13 +287,16 @@ class XmlResponseTest(TextResponseTest):\n         r4 = r3.replace(body=body2)\n         self._assert_response_values(r4, 'utf-8', body2)\n \n-        # make sure replace() rediscovers the encoding (if not given explicitly) when changing the body\n+    def test_replace_encoding(self):\n+        # make sure replace() keeps the previous encoding unless overridden explicitly\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n-        r5 = self.response_class(\"http://www.example.com\", body=body)\n         body2 = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n+        r5 = self.response_class(\"http://www.example.com\", body=body)\n         r6 = r5.replace(body=body2)\n+        r7 = r5.replace(body=body2, encoding='utf-8')\n         self._assert_response_values(r5, 'iso-8859-1', body)\n-        self._assert_response_values(r6, 'utf-8', body2)\n+        self._assert_response_values(r6, 'iso-8859-1', body2)\n+        self._assert_response_values(r7, 'utf-8', body2)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
