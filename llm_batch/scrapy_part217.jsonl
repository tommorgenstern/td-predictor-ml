{"custom_id": "scrapy#2a6524ee3a21283db01dae8bf3a11832eca79614", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 823 | Contributors (this commit): 29 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,6 @@ import json\n import logging\n from os.path import join, exists\n \n-from queuelib import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n@@ -13,9 +12,11 @@ logger = logging.getLogger(__name__)\n \n class Scheduler(object):\n \n-    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None):\n+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n+                 logunser=False, stats=None, pqclass=None):\n         self.df = dupefilter\n         self.dqdir = self._dqdir(jobdir)\n+        self.pqclass = pqclass\n         self.dqclass = dqclass\n         self.mqclass = mqclass\n         self.logunser = logunser\n@@ -26,17 +27,19 @@ class Scheduler(object):\n         settings = crawler.settings\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n         dupefilter = dupefilter_cls.from_settings(settings)\n+        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n         logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')\n-        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser, crawler.stats)\n+        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n+                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)\n \n     def has_pending_requests(self):\n         return len(self) > 0\n \n     def open(self, spider):\n         self.spider = spider\n-        self.mqs = PriorityQueue(self._newmq)\n+        self.mqs = self.pqclass(self._newmq)\n         self.dqs = self._dq() if self.dqdir else None\n         return self.df.open()\n \n@@ -112,7 +115,7 @@ class Scheduler(object):\n                 prios = json.load(f)\n         else:\n             prios = ()\n-        q = PriorityQueue(self._newdq, startprios=prios)\n+        q = self.pqclass(self._newdq, startprios=prios)\n         if q:\n             logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                         {'queuesize': len(q)}, extra={'spider': self.spider})\n\n@@ -234,6 +234,7 @@ ROBOTSTXT_OBEY = False\n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\n SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n+SCHEDULER_PRIORITY_QUEUE = 'queuelib.PriorityQueue'\n \n SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e288d4a711f061a01a951d2e1d9892f737211e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 46 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,8 @@ IGNORED_EXTENSIONS = [\n     'm4a',\n \n     # office suites\n-    'xls', 'xlsx', 'ppt', 'pptx', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',\n+    'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg',\n+    'odp',\n \n     # other\n     'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65c7c05060fd2d1fc161d4904243d5e0b31e202b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 354 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,14 +47,8 @@ def get_meta_refresh(response):\n \n def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n-\n-    >>> response_status_message(200)\n-    '200 OK'\n-\n-    >>> response_status_message(404)\n-    '404 Not Found'\n     \"\"\"\n-    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))\n+    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n \n \n def response_httprepr(response):\n\n@@ -5,7 +5,7 @@ from six.moves.urllib.parse import urlparse\n from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (response_httprepr, open_in_browser,\n-                                   get_meta_refresh, get_base_url)\n+                                   get_meta_refresh, get_base_url, response_status_message)\n \n __doctests__ = ['scrapy.utils.response']\n \n@@ -78,3 +78,8 @@ class ResponseUtilsTest(unittest.TestCase):\n         resp2 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n         <html><body>blahablsdfsal&amp;</body></html>\"\"\")\n         self.assertEqual(get_base_url(resp2), \"http://www.example.com\")\n+\n+    def test_response_status_message(self):\n+        self.assertEqual(response_status_message(200), '200 OK')\n+        self.assertEqual(response_status_message(404), '404 Not Found')\n+        self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5045a4f1681b8c4a152210a9245e59af30744422", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 147 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -147,7 +147,7 @@ class S3FilesStore(object):\n                 Bucket=self.bucket,\n                 Key=key_name,\n                 Body=buf,\n-                Metadata={k: str(v) for k, v in six.iteritems(meta)},\n+                Metadata={k: str(v) for k, v in six.iteritems(meta or {})},\n                 ACL=self.POLICY,\n                 **extra)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc8cd45a48fb21d33e7f6edaec2aa8275c6ea942", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 150 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -364,8 +364,9 @@ class FilesPipeline(MediaPipeline):\n     def file_downloaded(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n         buf = BytesIO(response.body)\n-        self.store.persist_file(path, buf, info)\n         checksum = md5sum(buf)\n+        buf.seek(0)\n+        self.store.persist_file(path, buf, info)\n         return checksum\n \n     def item_completed(self, results, item, info):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a583e4d531306b7628b42f8a32c7db892ad86cf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 139 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -328,7 +328,7 @@ class FilesystemCacheStorage(object):\n         metapath = os.path.join(rpath, 'pickled_meta')\n         if not os.path.exists(metapath):\n             return  # not found\n-        mtime = os.stat(rpath).st_mtime\n+        mtime = os.stat(metapath).st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n         with self._open(metapath, 'rb') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1075587dbd15e5ccb9a83c4ca14086c1e135fe12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 188 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ def is_setting_index(node):\n     if node.tagname == 'index':\n         # index entries for setting directives look like:\n         # [(u'pair', u'SETTING_NAME; setting', u'std:setting-SETTING_NAME', '')]\n-        entry_type, info, refid, _ = node['entries'][0]\n+        entry_type, info, refid = node['entries'][0][:3]\n         return entry_type == 'pair' and info.endswith('; setting')\n     return False\n \n@@ -30,7 +30,7 @@ def get_setting_target(node):\n \n def get_setting_name_and_refid(node):\n     \"\"\"Extract setting name from directive index node\"\"\"\n-    entry_type, info, refid, _ = node['entries'][0]\n+    entry_type, info, refid = node['entries'][0][:3]\n     return info.replace('; setting', ''), refid\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3787fec460f134a91228d01a8fc0277354e4ee0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 729 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,7 +8,6 @@ from w3lib import html\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse\n-from scrapy.utils.response import _noscript_re, _script_re\n \n \n logger = logging.getLogger(__name__)\n@@ -89,8 +88,7 @@ def _has_ajaxcrawlable_meta(text):\n     if 'content' not in text:\n         return False\n \n-    text = _script_re.sub(u'', text)\n-    text = _noscript_re.sub(u'', text)\n-    text = html.remove_comments(html.replace_entities(text))\n+    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n+    text = html.replace_entities(text)\n+    text = html.remove_comments(text)\n     return _ajax_crawlable_re.search(text) is not None\n-\n\n@@ -3,7 +3,6 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n import os\n-import re\n import weakref\n import webbrowser\n import tempfile\n@@ -31,17 +30,13 @@ def get_base_url(response):\n     return _baseurl_cache[response]\n \n \n-_noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)\n-_script_re = re.compile(u'<script.*?>.*?</script>', re.IGNORECASE | re.DOTALL)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n-        text = _noscript_re.sub(u'', text)\n-        text = _script_re.sub(u'', text)\n         _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n-            response.encoding)\n+            response.encoding, ignore_tags=('script', 'noscript'))\n     return _metaref_cache[response]\n \n \n\n@@ -42,7 +42,7 @@ setup(\n     ],\n     install_requires=[\n         'Twisted>=10.0.0',\n-        'w3lib>=1.8.0',\n+        'w3lib>=1.13.0',\n         'queuelib',\n         'lxml',\n         'pyOpenSSL',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8ca4675720b2393fdbd2305c03d94c58752c55a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 825 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n-from scrapy.utils.python import isbinarytext, to_bytes, to_native_str\n+from scrapy.utils.python import binary_is_text, to_bytes, to_native_str\n \n \n class ResponseTypes(object):\n@@ -94,7 +94,7 @@ class ResponseTypes(object):\n         cannot be guess using more straightforward methods.\"\"\"\n         chunk = body[:5000]\n         chunk = to_bytes(chunk)\n-        if isbinarytext(chunk):\n+        if not binary_is_text(chunk):\n             return self.from_mimetype('application/octet-stream')\n         elif b\"<html>\" in chunk.lower():\n             return self.from_mimetype('text/html')\n\n@@ -174,17 +174,25 @@ def memoizemethod_noargs(method):\n         return cache[self]\n     return new_method\n \n+\n _BINARYCHARS = {six.b(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n _BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}\n \n-\n+@deprecated(\"scrapy.utils.python.binary_is_text\")\n def isbinarytext(text):\n-    \"\"\"Return True if the given text is considered binary, or False\n-    otherwise, by looking for binary bytes at their chars\n+    \"\"\" This function is deprecated.\n+    Please use scrapy.utils.python.binary_is_text, which was created to be more\n+    clear about the functions behavior: it is behaving inverted to this one. \"\"\"\n+    return not binary_is_text(text)\n+\n+\n+def binary_is_text(data):\n+    \"\"\" Returns `True` if the given ``data`` argument (a ``bytes`` object)\n+    does not contain unprintable control characters.\n     \"\"\"\n-    if not isinstance(text, bytes):\n-        raise TypeError(\"text must be bytes, got '%s'\" % type(text).__name__)\n-    return any(c in _BINARYCHARS for c in text)\n+    if not isinstance(data, bytes):\n+        raise TypeError(\"data must be bytes, got '%s'\" % type(data).__name__)\n+    return all(c not in _BINARYCHARS for c in data)\n \n \n def get_func_args(func, stripself=False):\n\n@@ -5,7 +5,7 @@ from itertools import count\n import six\n \n from scrapy.utils.python import (\n-    memoizemethod_noargs, isbinarytext, equal_attributes,\n+    memoizemethod_noargs, binary_is_text, equal_attributes,\n     WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode,\n     without_none_values)\n \n@@ -71,18 +71,18 @@ class MemoizedMethodTest(unittest.TestCase):\n         assert one is not three\n \n \n-class IsBinaryTextTest(unittest.TestCase):\n-    def test_isbinarytext(self):\n-        assert not isbinarytext(b\"hello\")\n+class BinaryIsTextTest(unittest.TestCase):\n+    def test_binaryistext(self):\n+        assert binary_is_text(b\"hello\")\n \n     def test_utf_16_strings_contain_null_bytes(self):\n-        assert not isbinarytext(u\"hello\".encode('utf-16'))\n+        assert binary_is_text(u\"hello\".encode('utf-16'))\n \n     def test_one_with_encoding(self):\n-        assert not isbinarytext(b\"<div>Price \\xa3</div>\")\n+        assert binary_is_text(b\"<div>Price \\xa3</div>\")\n \n     def test_real_binary_bytes(self):\n-        assert isbinarytext(b\"\\x02\\xa3\")\n+        assert not binary_is_text(b\"\\x02\\xa3\")\n \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9d48f8a8ed79be0f7ee16caa24dea3b8ae9e54d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 13 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 77 | Churn Cumulative: 175 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -183,6 +183,35 @@ class FilesPipelineTestCaseFields(unittest.TestCase):\n             self.assertEqual(item['stored_file'], [results[0][1]])\n \n \n+class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.pipeline = FilesPipeline(self.tempdir)\n+        self.default_settings = Settings()\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_expires(self):\n+        another_pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir,\n+                                                                'FILES_EXPIRES': 42}))\n+        self.assertEqual(self.pipeline.expires, self.default_settings.getint('FILES_EXPIRES'))\n+        self.assertEqual(another_pipeline.expires, 42)\n+\n+    def test_files_urls_field(self):\n+        another_pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir,\n+                                                                'FILES_URLS_FIELD': 'funny_field'}))\n+        self.assertEqual(self.pipeline.files_urls_field, self.default_settings.get('FILES_URLS_FIELD'))\n+        self.assertEqual(another_pipeline.files_urls_field, 'funny_field')\n+\n+    def test_files_result_field(self):\n+        another_pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir,\n+                                                                'FILES_RESULT_FIELD': 'funny_field'}))\n+        self.assertEqual(self.pipeline.files_result_field, self.default_settings.get('FILES_RESULT_FIELD'))\n+        self.assertEqual(another_pipeline.files_result_field, 'funny_field')\n+\n+\n class TestS3FilesStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n\n@@ -205,6 +205,54 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n             self.assertEqual(item['stored_image'], [results[0][1]])\n \n \n+class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.pipeline = ImagesPipeline(self.tempdir)\n+        self.default_settings = Settings()\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_expires(self):\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_EXPIRES': 42}))\n+        self.assertEqual(self.pipeline.expires, self.default_settings.getint('IMAGES_EXPIRES'))\n+        self.assertEqual(another_pipeline.expires, 42)\n+\n+    def test_images_urls_field(self):\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_URLS_FIELD': 'funny_field'}))\n+        self.assertEqual(self.pipeline.images_urls_field, self.default_settings.get('IMAGES_URLS_FIELD'))\n+        self.assertEqual(another_pipeline.images_urls_field, 'funny_field')\n+\n+    def test_images_result_field(self):\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_RESULT_FIELD': 'funny_field'}))\n+        self.assertEqual(self.pipeline.images_result_field, self.default_settings.get('IMAGES_RESULT_FIELD'))\n+        self.assertEqual(another_pipeline.images_result_field, 'funny_field')\n+\n+    def test_min_width(self):\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_MIN_WIDTH': 42}))\n+        self.assertEqual(self.pipeline.min_width, self.default_settings.getint('IMAGES_MIN_WIDTH'))\n+        self.assertEqual(another_pipeline.min_width, 42)\n+\n+    def test_min_height(self):\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_MIN_HEIGHT': 42}))\n+        self.assertEqual(self.pipeline.min_height, self.default_settings.getint('IMAGES_MIN_HEIGHT'))\n+        self.assertEqual(another_pipeline.min_height, 42)\n+\n+    def test_thumbs(self):\n+        custom_thumbs = {'small': (50, 50), 'big': (270, 270)}\n+        another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n+                                                                'IMAGES_THUMBS': custom_thumbs}))\n+        self.assertEqual(self.pipeline.thumbs, self.default_settings.get('IMAGES_THUMBS'))\n+        self.assertEqual(another_pipeline.thumbs, custom_thumbs)\n+\n+\n def _create_image(format, *a, **kw):\n     buf = TemporaryFile()\n     Image.new(*a, **kw).save(buf, format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7fc17866feb6c858673eefc690f5b10048a8b9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 642 | Contributors (this commit): 27 | Commits (past 90d): 15 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -213,14 +213,11 @@ class FilesPipeline(MediaPipeline):\n     \"\"\"\n \n     MEDIA_NAME = \"file\"\n-    EXPIRES = 90\n     STORE_SCHEMES = {\n         '': FSFilesStore,\n         'file': FSFilesStore,\n         's3': S3FilesStore,\n     }\n-    DEFAULT_FILES_URLS_FIELD = 'file_urls'\n-    DEFAULT_FILES_RESULT_FIELD = 'files'\n \n     def __init__(self, store_uri, download_func=None):\n         if not store_uri:\n@@ -235,9 +232,9 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         s3store.POLICY = settings['FILES_STORE_S3_ACL']\n \n-        cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD', cls.DEFAULT_FILES_URLS_FIELD)\n-        cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD', cls.DEFAULT_FILES_RESULT_FIELD)\n-        cls.EXPIRES = settings.getint('FILES_EXPIRES', 90)\n+        cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD')\n+        cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD')\n+        cls.EXPIRES = settings.getint('FILES_EXPIRES')\n         store_uri = settings['FILES_STORE']\n         return cls(store_uri)\n \n\n@@ -36,24 +36,19 @@ class ImagesPipeline(FilesPipeline):\n     \"\"\"\n \n     MEDIA_NAME = 'image'\n-    MIN_WIDTH = 0\n-    MIN_HEIGHT = 0\n-    THUMBS = {}\n-    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n-    DEFAULT_IMAGES_RESULT_FIELD = 'images'\n \n     @classmethod\n     def from_settings(cls, settings):\n-        cls.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n-        cls.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n-        cls.EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n-        cls.THUMBS = settings.get('IMAGES_THUMBS', {})\n+        cls.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH')\n+        cls.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT')\n+        cls.EXPIRES = settings.getint('IMAGES_EXPIRES')\n+        cls.THUMBS = settings.get('IMAGES_THUMBS')\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n \n-        cls.IMAGES_URLS_FIELD = settings.get('IMAGES_URLS_FIELD', cls.DEFAULT_IMAGES_URLS_FIELD)\n-        cls.IMAGES_RESULT_FIELD = settings.get('IMAGES_RESULT_FIELD', cls.DEFAULT_IMAGES_RESULT_FIELD)\n+        cls.IMAGES_URLS_FIELD = settings.get('IMAGES_URLS_FIELD')\n+        cls.IMAGES_RESULT_FIELD = settings.get('IMAGES_RESULT_FIELD')\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri)\n \n\n@@ -159,6 +159,9 @@ FEED_EXPORTERS_BASE = {\n }\n \n FILES_STORE_S3_ACL = 'private'\n+FILES_EXPIRES = 90\n+FILES_URLS_FIELD = 'file_urls'\n+FILES_RESULT_FIELD = 'files'\n \n HTTPCACHE_ENABLED = False\n HTTPCACHE_DIR = 'httpcache'\n@@ -175,6 +178,13 @@ HTTPCACHE_GZIP = False\n \n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n+IMAGES_MIN_WIDTH = 0\n+IMAGES_MIN_HEIGHT = 0\n+IMAGES_EXPIRES = 90\n+IMAGES_THUMBS = {}\n+IMAGES_URLS_FIELD = 'image_urls'\n+IMAGES_RESULT_FIELD = 'images'\n+\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8228a0c49113a7cd4ab1099fae746268183655fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 288 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ except ImportError:\n from twisted.internet import defer, threads\n \n from scrapy.pipelines.media import MediaPipeline\n+from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n@@ -219,10 +220,18 @@ class FilesPipeline(MediaPipeline):\n         's3': S3FilesStore,\n     }\n \n-    def __init__(self, store_uri, download_func=None):\n+    def __init__(self, store_uri, download_func=None, settings=None):\n         if not store_uri:\n             raise NotConfigured\n+        \n+        if isinstance(settings, dict) or settings is None:\n+            settings = Settings(settings)\n+        \n         self.store = self._get_store(store_uri)\n+        self.expires = settings.getint('FILES_EXPIRES')\n+        self.files_urls_field = settings.get('FILES_URLS_FIELD')\n+        self.files_result_field = settings.get('FILES_RESULT_FIELD')\n+\n         super(FilesPipeline, self).__init__(download_func=download_func)\n \n     @classmethod\n@@ -232,11 +241,8 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         s3store.POLICY = settings['FILES_STORE_S3_ACL']\n \n-        cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD')\n-        cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD')\n-        cls.EXPIRES = settings.getint('FILES_EXPIRES')\n         store_uri = settings['FILES_STORE']\n-        return cls(store_uri)\n+        return cls(store_uri, settings=settings)\n \n     def _get_store(self, uri):\n         if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n@@ -257,7 +263,7 @@ class FilesPipeline(MediaPipeline):\n \n             age_seconds = time.time() - last_modified\n             age_days = age_seconds / 60 / 60 / 24\n-            if age_days > self.EXPIRES:\n+            if age_days > self.expires:\n                 return  # returning None force download\n \n             referer = referer_str(request)\n@@ -356,7 +362,7 @@ class FilesPipeline(MediaPipeline):\n \n     ### Overridable Interface\n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]\n+        return [Request(x) for x in item.get(self.files_urls_field, [])]\n \n     def file_downloaded(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n@@ -367,8 +373,8 @@ class FilesPipeline(MediaPipeline):\n         return checksum\n \n     def item_completed(self, results, item, info):\n-        if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:\n-            item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n+        if isinstance(item, dict) or self.files_result_field in item.fields:\n+            item[self.files_result_field] = [x for ok, x in results if ok]\n         return item\n \n     def file_path(self, request, response=None, info=None):\n\n@@ -91,7 +91,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         patchers = [\n             mock.patch.object(FSFilesStore, 'stat_file', return_value={\n                 'checksum': 'abc',\n-                'last_modified': time.time() - (FilesPipeline.EXPIRES * 60 * 60 * 24 * 2)}),\n+                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),\n             mock.patch.object(FilesPipeline, 'get_media_requests',\n                               return_value=[_prepare_request_object(item_url)]),\n             mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#537083524e2a9ccf545776b23b3248220dc9f16a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ from PIL import Image\n from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n from scrapy.http import Request\n+from scrapy.settings import Settings\n from scrapy.exceptions import DropItem\n #TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.pipelines.files import FileException, FilesPipeline\n@@ -37,20 +38,27 @@ class ImagesPipeline(FilesPipeline):\n \n     MEDIA_NAME = 'image'\n \n+    def __init__(self, store_uri, download_func=None, settings=None):\n+        super(ImagesPipeline, self).__init__(store_uri, settings=settings, download_func=download_func)\n+        \n+        if isinstance(settings, dict) or settings is None:\n+            settings = Settings(settings)\n+\n+        self.expires = settings.getint('IMAGES_EXPIRES')\n+        self.images_urls_field = settings.get('IMAGES_URLS_FIELD')\n+        self.images_result_field = settings.get('IMAGES_RESULT_FIELD')\n+        self.min_width = settings.getint('IMAGES_MIN_WIDTH')\n+        self.min_height = settings.getint('IMAGES_MIN_HEIGHT')\n+        self.thumbs = settings.get('IMAGES_THUMBS')\n+\n     @classmethod\n     def from_settings(cls, settings):\n-        cls.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH')\n-        cls.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT')\n-        cls.EXPIRES = settings.getint('IMAGES_EXPIRES')\n-        cls.THUMBS = settings.get('IMAGES_THUMBS')\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n \n-        cls.IMAGES_URLS_FIELD = settings.get('IMAGES_URLS_FIELD')\n-        cls.IMAGES_RESULT_FIELD = settings.get('IMAGES_RESULT_FIELD')\n         store_uri = settings['IMAGES_STORE']\n-        return cls(store_uri)\n+        return cls(store_uri, settings=settings)\n \n     def file_downloaded(self, response, request, info):\n         return self.image_downloaded(response, request, info)\n@@ -73,14 +81,14 @@ class ImagesPipeline(FilesPipeline):\n         orig_image = Image.open(BytesIO(response.body))\n \n         width, height = orig_image.size\n-        if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:\n+        if width < self.min_width or height < self.min_height:\n             raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n-                                 (width, height, self.MIN_WIDTH, self.MIN_HEIGHT))\n+                                 (width, height, self.min_width, self.min_height))\n \n         image, buf = self.convert_image(orig_image)\n         yield path, image, buf\n \n-        for thumb_id, size in six.iteritems(self.THUMBS):\n+        for thumb_id, size in six.iteritems(self.thumbs):\n             thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n@@ -102,11 +110,11 @@ class ImagesPipeline(FilesPipeline):\n         return image, buf\n \n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]\n+        return [Request(x) for x in item.get(self.images_urls_field, [])]\n \n     def item_completed(self, results, item, info):\n-        if isinstance(item, dict) or self.IMAGES_RESULT_FIELD in item.fields:\n-            item[self.IMAGES_RESULT_FIELD] = [x for ok, x in results if ok]\n+        if isinstance(item, dict) or self.images_result_field in item.fields:\n+            item[self.images_result_field] = [x for ok, x in results if ok]\n         return item\n \n     def file_path(self, request, response=None, info=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9250a5bffa91c24dbea5c5d64c3c7cd9992a6ee7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 50 | Churn Cumulative: 744 | Contributors (this commit): 30 | Commits (past 90d): 20 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import os\n import sys\n import logging\n import posixpath\n-from tempfile import TemporaryFile\n+from tempfile import NamedTemporaryFile\n from datetime import datetime\n import six\n from six.moves.urllib.parse import urlparse\n@@ -47,7 +47,11 @@ class IFeedStorage(Interface):\n class BlockingFeedStorage(object):\n \n     def open(self, spider):\n-        return TemporaryFile(prefix='feed-')\n+        path = spider.crawler.settings['FEED_TEMPDIR']\n+        if path and not os.path.isdir(path):\n+            raise OSError('Not a Directory: ' + str(path))\n+\n+        return NamedTemporaryFile(prefix='feed-', dir=path)\n \n     def store(self, file):\n         return threads.deferToThread(self._store_in_thread, file)\n\n@@ -134,6 +134,7 @@ EXTENSIONS_BASE = {\n     'scrapy.extensions.throttle.AutoThrottle': 0,\n }\n \n+FEED_TEMPDIR = None\n FEED_URI = None\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n\n@@ -18,9 +18,9 @@ from w3lib.url import path_to_file_uri\n import scrapy\n from scrapy.extensions.feedexport import (\n     IFeedStorage, FileFeedStorage, FTPFeedStorage,\n-    S3FeedStorage, StdoutFeedStorage\n-)\n-from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n+    S3FeedStorage, StdoutFeedStorage,\n+    BlockingFeedStorage)\n+from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n from scrapy.utils.python import to_native_str\n \n \n@@ -87,6 +87,41 @@ class FTPFeedStorageTest(unittest.TestCase):\n             self.assertEqual(fp.read(), b\"new content\")\n \n \n+class BlockingFeedStorageTest(unittest.TestCase):\n+\n+    def get_test_spider(self, settings=None):\n+        class TestSpider(scrapy.Spider):\n+            name = 'test_spider'\n+        crawler = get_crawler(settings_dict=settings)\n+        spider = TestSpider.from_crawler(crawler)\n+        return spider\n+\n+    def test_default_temp_dir(self):\n+        b = BlockingFeedStorage()\n+\n+        tmp = b.open(self.get_test_spider())\n+        tmp_path = os.path.dirname(tmp.name)\n+        self.assertEqual(tmp_path, tempfile.gettempdir())\n+\n+    def test_temp_file(self):\n+        b = BlockingFeedStorage()\n+\n+        tests_path = os.path.dirname(os.path.abspath(__file__))\n+        spider = self.get_test_spider({'FEED_TEMPDIR': tests_path})\n+        tmp = b.open(spider)\n+        tmp_path = os.path.dirname(tmp.name)\n+        self.assertEqual(tmp_path, tests_path)\n+\n+    def test_invalid_folder(self):\n+        b = BlockingFeedStorage()\n+\n+        tests_path = os.path.dirname(os.path.abspath(__file__))\n+        invalid_path = os.path.join(tests_path, 'invalid_path')\n+        spider = self.get_test_spider({'FEED_TEMPDIR': invalid_path})\n+\n+        self.assertRaises(OSError, b.open, spider=spider)\n+\n+\n class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#103f6eaa88f2fe618b58525ac04278bf5e4bb831", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 67 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -39,5 +39,5 @@ def sitemap_urls_from_robots(robots_text):\n     robots.txt file\n     \"\"\"\n     for line in robots_text.splitlines():\n-        if line.lstrip().startswith('Sitemap:'):\n+        if line.lstrip().lower().startswith('sitemap:'):\n             yield line.split(':', 1)[1].strip()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1656fbcffa28f3b69862109631741a9c0118ac80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 217 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n import unittest\n from scrapy.linkextractors.regex import RegexLinkExtractor\n from scrapy.http import HtmlResponse\n@@ -81,9 +82,14 @@ class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n         ])\n \n+        # document encoding does not affect URL path component, only query part\n+        # >>> u'sample_ñ.html'.encode('utf8')\n+        # 'sample_\\xc3\\xb1.html'\n+        # >>> u\"sample_á.html\".encode('utf8')\n+        # 'sample_\\xc3\\xa1.html'\n         self.assertEqual(lx.extract_links(response_latin1), [\n-            Link(url='http://example.com/sample_%F1.html', text=''),\n-            Link(url='http://example.com/sample_%E1.html', text='sample \\xe1 text'.decode('latin1')),\n+            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n+            Link(url='http://example.com/sample_%C3%A1.html', text='sample \\xe1 text'.decode('latin1')),\n         ])\n \n     def test_matches(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b5243a2636f520b0d00d90e47256960fe8f1805", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 228 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,12 +84,19 @@ class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n \n         # document encoding does not affect URL path component, only query part\n         # >>> u'sample_ñ.html'.encode('utf8')\n-        # 'sample_\\xc3\\xb1.html'\n+        # b'sample_\\xc3\\xb1.html'\n         # >>> u\"sample_á.html\".encode('utf8')\n-        # 'sample_\\xc3\\xa1.html'\n+        # b'sample_\\xc3\\xa1.html'\n+        # >>> u\"sample_ö.html\".encode('utf8')\n+        # b'sample_\\xc3\\xb6.html'\n+        # >>> u\"£32\".encode('latin1')\n+        # b'\\xa332'\n+        # >>> u\"µ\".encode('latin1')\n+        # b'\\xb5'\n         self.assertEqual(lx.extract_links(response_latin1), [\n             Link(url='http://example.com/sample_%C3%B1.html', text=''),\n             Link(url='http://example.com/sample_%C3%A1.html', text='sample \\xe1 text'.decode('latin1')),\n+            Link(url='http://example.com/sample_%C3%B6.html?price=%A332&%B5=unit', text=''),\n         ])\n \n     def test_matches(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86e4442d177ea781f53f0f105c5ad814560a0bc4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 1366 | Contributors (this commit): 13 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -127,7 +127,8 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n             self._tunnelReadyDeferred.errback(\n-                TunnelError('Could not open CONNECT tunnel.'))\n+                TunnelError('Could not open CONNECT tunnel with proxy %s:%s' % (\n+                    self._host, self._port)))\n \n     def connectFailed(self, reason):\n         \"\"\"Propagates the errback to the appropriate deferred.\"\"\"\n@@ -193,6 +194,14 @@ class TunnelingAgent(Agent):\n                 self._contextFactory, self._connectTimeout,\n                 self._bindAddress)\n \n+    def _requestWithEndpoint(self, key, endpoint, method, parsedURI,\n+            headers, bodyProducer, requestPath):\n+        # proxy host and port are required for HTTP pool `key`\n+        # otherwise, same remote host connection request could reuse\n+        # a cached tunneled connection to a different proxy\n+        key = key + self._proxyConf\n+        return super(TunnelingAgent, self)._requestWithEndpoint(key, endpoint, method, parsedURI,\n+            headers, bodyProducer, requestPath)\n \n \n class ScrapyAgent(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d42a98d3b590515bae30fb698e7aba2d7511608e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 227 | Lines Deleted: 64 | Files Changed: 2 | Hunks: 40 | Methods Changed: 43 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 291 | Churn Cumulative: 825 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import six\n from w3lib.url import safe_url_string\n \n from scrapy.http.headers import Headers\n-from scrapy.utils.python import to_native_str, to_bytes\n+from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n from scrapy.http.common import obsolete_setter\n@@ -50,8 +50,8 @@ class Request(object_ref):\n         if not isinstance(url, six.string_types):\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n \n-        url = to_native_str(url, self.encoding)\n-        self._url = escape_ajax(safe_url_string(url))\n+        s = safe_url_string(url, self.encoding)\n+        self._url = escape_ajax(s)\n \n         if ':' not in self._url:\n             raise ValueError('Missing scheme in request url: %s' % self._url)\n\n@@ -1,10 +1,13 @@\n+# -*- coding: utf-8 -*-\n import cgi\n import unittest\n import re\n \n import six\n from six.moves import xmlrpc_client as xmlrpclib\n-from six.moves.urllib.parse import urlparse\n+from six.moves.urllib.parse import urlparse, parse_qs, unquote\n+if six.PY3:\n+    from urllib.parse import unquote_to_bytes\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n@@ -86,12 +89,60 @@ class RequestTest(unittest.TestCase):\n         r = self.request_class(url=\"http://www.scrapy.org/blank space\")\n         self.assertEqual(r.url, \"http://www.scrapy.org/blank%20space\")\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n     def test_url_encoding(self):\n-        r1 = self.request_class(url=u\"http://www.scrapy.org/price/\\xa3\", encoding=\"utf-8\")\n-        r2 = self.request_class(url=u\"http://www.scrapy.org/price/\\xa3\", encoding=\"latin1\")\n-        self.assertEqual(r1.url, \"http://www.scrapy.org/price/%C2%A3\")\n-        self.assertEqual(r2.url, \"http://www.scrapy.org/price/%A3\")\n+        r = self.request_class(url=u\"http://www.scrapy.org/price/£\")\n+        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n+\n+    def test_url_encoding_other(self):\n+        # encoding affects only query part of URI, not path\n+        # path part should always be UTF-8 encoded before percent-escaping\n+        r = self.request_class(url=u\"http://www.scrapy.org/price/£\", encoding=\"utf-8\")\n+        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n+\n+        r = self.request_class(url=u\"http://www.scrapy.org/price/£\", encoding=\"latin1\")\n+        self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n+\n+    def test_url_encoding_query(self):\n+        r1 = self.request_class(url=u\"http://www.scrapy.org/price/£?unit=µ\")\n+        self.assertEqual(r1.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n+\n+        # should be same as above\n+        r2 = self.request_class(url=u\"http://www.scrapy.org/price/£?unit=µ\", encoding=\"utf-8\")\n+        self.assertEqual(r2.url, \"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5\")\n+\n+    def test_url_encoding_query_latin1(self):\n+        # encoding is used for encoding query-string before percent-escaping;\n+        # path is still UTF-8 encoded before percent-escaping\n+        r3 = self.request_class(url=u\"http://www.scrapy.org/price/µ?currency=£\", encoding=\"latin1\")\n+        self.assertEqual(r3.url, \"http://www.scrapy.org/price/%C2%B5?currency=%A3\")\n+\n+    def test_url_encoding_nonutf8_untouched(self):\n+        # percent-escaping sequences that do not match valid UTF-8 sequences\n+        # should be kept untouched (just upper-cased perhaps)\n+        #\n+        # See https://tools.ietf.org/html/rfc3987#section-3.2\n+        #\n+        # \"Conversions from URIs to IRIs MUST NOT use any character encoding\n+        # other than UTF-8 in steps 3 and 4, even if it might be possible to\n+        # guess from the context that another character encoding than UTF-8 was\n+        # used in the URI.  For example, the URI\n+        # \"http://www.example.org/r%E9sum%E9.html\" might with some guessing be\n+        # interpreted to contain two e-acute characters encoded as iso-8859-1.\n+        # It must not be converted to an IRI containing these e-acute\n+        # characters.  Otherwise, in the future the IRI will be mapped to\n+        # \"http://www.example.org/r%C3%A9sum%C3%A9.html\", which is a different\n+        # URI from \"http://www.example.org/r%E9sum%E9.html\".\n+        r1 = self.request_class(url=u\"http://www.scrapy.org/price/%a3\")\n+        self.assertEqual(r1.url, \"http://www.scrapy.org/price/%a3\")\n+\n+        r2 = self.request_class(url=u\"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n+        self.assertEqual(r2.url, \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n+\n+        r3 = self.request_class(url=u\"http://www.scrapy.org/résumé/%a3\")\n+        self.assertEqual(r3.url, \"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3\")\n+\n+        r4 = self.request_class(url=u\"http://www.example.org/r%E9sum%E9.html\")\n+        self.assertEqual(r4.url, \"http://www.example.org/r%E9sum%E9.html\")\n \n     def test_body(self):\n         r1 = self.request_class(url=\"http://www.example.com/\")\n@@ -198,19 +249,46 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class(\"http://www.example.com\", formdata={})\n         self.assertEqual(r1.body, b'')\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n-    def test_default_encoding(self):\n+    def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n-        data = {'one': 'two', 'price': '\\xc2\\xa3 100'}\n+        data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n         r2 = self.request_class(\"http://www.example.com\", formdata=data)\n         self.assertEqual(r2.method, 'POST')\n         self.assertEqual(r2.encoding, 'utf-8')\n         self.assertQueryEqual(r2.body, b'price=%C2%A3+100&one=two')\n         self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n \n-    def test_custom_encoding(self):\n-        data = {'price': u'\\xa3 100'}\n-        r3 = self.request_class(\"http://www.example.com\", formdata=data, encoding='latin1')\n+    def test_default_encoding_textual_data(self):\n+        # using default encoding (utf-8)\n+        data = {u'µ one': u'two', u'price': u'£ 100'}\n+        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n+        self.assertEqual(r2.method, 'POST')\n+        self.assertEqual(r2.encoding, 'utf-8')\n+        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&%C2%B5+one=two')\n+        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+\n+    def test_default_encoding_mixed_data(self):\n+        # using default encoding (utf-8)\n+        data = {u'\\u00b5one': b'two', b'price\\xc2\\xa3': u'\\u00a3 100'}\n+        r2 = self.request_class(\"http://www.example.com\", formdata=data)\n+        self.assertEqual(r2.method, 'POST')\n+        self.assertEqual(r2.encoding, 'utf-8')\n+        self.assertQueryEqual(r2.body, b'%C2%B5one=two&price%C2%A3=%C2%A3+100')\n+        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+\n+    def test_custom_encoding_bytes(self):\n+        data = {b'\\xb5 one': b'two', b'price': b'\\xa3 100'}\n+        r2 = self.request_class(\"http://www.example.com\", formdata=data,\n+                                    encoding='latin1')\n+        self.assertEqual(r2.method, 'POST')\n+        self.assertEqual(r2.encoding, 'latin1')\n+        self.assertQueryEqual(r2.body, b'price=%A3+100&%B5+one=two')\n+        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')\n+\n+    def test_custom_encoding_textual_data(self):\n+        data = {'price': u'£ 100'}\n+        r3 = self.request_class(\"http://www.example.com\", formdata=data,\n+                                    encoding='latin1')\n         self.assertEqual(r3.encoding, 'latin1')\n         self.assertEqual(r3.body, b'price=%A3+100')\n \n@@ -231,15 +309,78 @@ class FormRequestTest(RequestTest):\n             url=\"http://www.example.com/this/list.html\")\n         req = self.request_class.from_response(response,\n                 formdata={'one': ['two', 'three'], 'six': 'seven'})\n+\n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n         self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n         fs = _qs(req)\n-        self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n-        self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n+        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})\n+        self.assertEqual(set(fs[b'one']), {b'two', b'three'})\n         self.assertEqual(fs[b'test2'], [b'xxx'])\n         self.assertEqual(fs[b'six'], [b'seven'])\n \n+    def test_from_response_post_nonascii_bytes_utf8(self):\n+        response = _buildresponse(\n+            b\"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test \\xc2\\xa3\" value=\"val2\">\n+            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xc2\\xb5\">\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response,\n+                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req, to_unicode=True)\n+        self.assertEqual(set(fs[u'test £']), {u'val1', u'val2'})\n+        self.assertEqual(set(fs[u'one']), {u'two', u'three'})\n+        self.assertEqual(fs[u'test2'], [u'xxx µ'])\n+        self.assertEqual(fs[u'six'], [u'seven'])\n+\n+    def test_from_response_post_nonascii_bytes_latin1(self):\n+        response = _buildresponse(\n+            b\"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test \\xa3\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test \\xa3\" value=\"val2\">\n+            <input type=\"hidden\" name=\"test2\" value=\"xxx \\xb5\">\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\",\n+            encoding='latin1',\n+            )\n+        req = self.request_class.from_response(response,\n+                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req, to_unicode=True, encoding='latin1')\n+        self.assertEqual(set(fs[u'test £']), {u'val1', u'val2'})\n+        self.assertEqual(set(fs[u'one']), {u'two', u'three'})\n+        self.assertEqual(fs[u'test2'], [u'xxx µ'])\n+        self.assertEqual(fs[u'six'], [u'seven'])\n+\n+    def test_from_response_post_nonascii_unicode(self):\n+        response = _buildresponse(\n+            u\"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"test £\" value=\"val1\">\n+            <input type=\"hidden\" name=\"test £\" value=\"val2\">\n+            <input type=\"hidden\" name=\"test2\" value=\"xxx µ\">\n+            </form>\"\"\",\n+            url=\"http://www.example.com/this/list.html\")\n+        req = self.request_class.from_response(response,\n+                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+\n+        self.assertEqual(req.method, 'POST')\n+        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n+        self.assertEqual(req.url, \"http://www.example.com/this/post.php\")\n+        fs = _qs(req, to_unicode=True)\n+        self.assertEqual(set(fs[u'test £']), {u'val1', u'val2'})\n+        self.assertEqual(set(fs[u'one']), {u'two', u'three'})\n+        self.assertEqual(fs[u'test2'], [u'xxx µ'])\n+        self.assertEqual(fs[u'six'], [u'seven'])\n+\n     def test_from_response_extra_headers(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n@@ -268,10 +409,10 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n         self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n         fs = _qs(r1)\n-        self.assertEqual(set(fs['test']), set(['val1', 'val2']))\n-        self.assertEqual(set(fs['one']), set(['two', 'three']))\n-        self.assertEqual(fs['test2'], ['xxx'])\n-        self.assertEqual(fs['six'], ['seven'])\n+        self.assertEqual(set(fs[b'test']), set([b'val1', b'val2']))\n+        self.assertEqual(set(fs[b'one']), set([b'two', b'three']))\n+        self.assertEqual(fs[b'test2'], [b'xxx'])\n+        self.assertEqual(fs[b'six'], [b'seven'])\n \n     def test_from_response_override_params(self):\n         response = _buildresponse(\n@@ -315,9 +456,9 @@ class FormRequestTest(RequestTest):\n             </form>\"\"\")\n         req = self.request_class.from_response(response)\n         fs = _qs(req)\n-        self.assertEqual(fs['clickable1'], ['clicked1'])\n-        self.assertFalse('i1' in fs, fs)  # xpath in _get_inputs()\n-        self.assertFalse('clickable2' in fs, fs)  # xpath in _get_clickable()\n+        self.assertEqual(fs[b'clickable1'], [b'clicked1'])\n+        self.assertFalse(b'i1' in fs, fs)  # xpath in _get_inputs()\n+        self.assertFalse(b'clickable2' in fs, fs)  # xpath in _get_clickable()\n \n     def test_from_response_submit_first_clickable(self):\n         response = _buildresponse(\n@@ -329,10 +470,10 @@ class FormRequestTest(RequestTest):\n             </form>\"\"\")\n         req = self.request_class.from_response(response, formdata={'two': '2'})\n         fs = _qs(req)\n-        self.assertEqual(fs['clickable1'], ['clicked1'])\n-        self.assertFalse('clickable2' in fs, fs)\n-        self.assertEqual(fs['one'], ['1'])\n-        self.assertEqual(fs['two'], ['2'])\n+        self.assertEqual(fs[b'clickable1'], [b'clicked1'])\n+        self.assertFalse(b'clickable2' in fs, fs)\n+        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertEqual(fs[b'two'], [b'2'])\n \n     def test_from_response_submit_not_first_clickable(self):\n         response = _buildresponse(\n@@ -345,10 +486,10 @@ class FormRequestTest(RequestTest):\n         req = self.request_class.from_response(response, formdata={'two': '2'}, \\\n                                               clickdata={'name': 'clickable2'})\n         fs = _qs(req)\n-        self.assertEqual(fs['clickable2'], ['clicked2'])\n-        self.assertFalse('clickable1' in fs, fs)\n-        self.assertEqual(fs['one'], ['1'])\n-        self.assertEqual(fs['two'], ['2'])\n+        self.assertEqual(fs[b'clickable2'], [b'clicked2'])\n+        self.assertFalse(b'clickable1' in fs, fs)\n+        self.assertEqual(fs[b'one'], [b'1'])\n+        self.assertEqual(fs[b'two'], [b'2'])\n \n     def test_from_response_dont_submit_image_as_input(self):\n         response = _buildresponse(\n@@ -359,7 +500,7 @@ class FormRequestTest(RequestTest):\n             </form>\"\"\")\n         req = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v']})\n+        self.assertEqual(fs, {b'i1': [b'i1v']})\n \n     def test_from_response_dont_submit_reset_as_input(self):\n         response = _buildresponse(\n@@ -371,7 +512,7 @@ class FormRequestTest(RequestTest):\n             </form>\"\"\")\n         req = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v'], 'i2': ['i2v']})\n+        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})\n \n     def test_from_response_multiple_clickdata(self):\n         response = _buildresponse(\n@@ -382,11 +523,11 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"two\" value=\"clicked2\">\n             </form>\"\"\")\n         req = self.request_class.from_response(response, \\\n-                clickdata={'name': 'clickable', 'value': 'clicked2'})\n+                clickdata={u'name': u'clickable', u'value': u'clicked2'})\n         fs = _qs(req)\n-        self.assertEqual(fs['clickable'], ['clicked2'])\n-        self.assertEqual(fs['one'], ['clicked1'])\n-        self.assertEqual(fs['two'], ['clicked2'])\n+        self.assertEqual(fs[b'clickable'], [b'clicked2'])\n+        self.assertEqual(fs[b'one'], [b'clicked1'])\n+        self.assertEqual(fs[b'two'], [b'clicked2'])\n \n     def test_from_response_unicode_clickdata(self):\n         response = _buildresponse(\n@@ -397,9 +538,24 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n             </form>\"\"\")\n         req = self.request_class.from_response(response, \\\n-                clickdata={'name': u'price in \\u00a3'})\n-        fs = _qs(req)\n-        self.assertTrue(fs[to_native_str(u'price in \\u00a3')])\n+                clickdata={u'name': u'price in \\u00a3'})\n+        fs = _qs(req, to_unicode=True)\n+        self.assertTrue(fs[u'price in \\u00a3'])\n+\n+    def test_from_response_unicode_clickdata_latin1(self):\n+        response = _buildresponse(\n+            u\"\"\"<form action=\"get.php\" method=\"GET\">\n+            <input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n+            <input type=\"submit\" name=\"price in \\u00a5\" value=\"\\u00a5 2000\">\n+            <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n+            <input type=\"hidden\" name=\"yensign\" value=\"\\u00a5\">\n+            </form>\"\"\",\n+            encoding='latin1')\n+        req = self.request_class.from_response(response, \\\n+                clickdata={u'name': u'price in \\u00a5'})\n+        fs = _qs(req, to_unicode=True, encoding='latin1')\n+        self.assertTrue(fs[u'price in \\u00a5'])\n+\n \n     def test_from_response_multiple_forms_clickdata(self):\n         response = _buildresponse(\n@@ -413,18 +569,18 @@ class FormRequestTest(RequestTest):\n             </form>\n             \"\"\")\n         req = self.request_class.from_response(response, formname='form2', \\\n-                clickdata={'name': 'clickable'})\n+                clickdata={u'name': u'clickable'})\n         fs = _qs(req)\n-        self.assertEqual(fs['clickable'], ['clicked2'])\n-        self.assertEqual(fs['field2'], ['value2'])\n-        self.assertFalse('field1' in fs, fs)\n+        self.assertEqual(fs[b'clickable'], [b'clicked2'])\n+        self.assertEqual(fs[b'field2'], [b'value2'])\n+        self.assertFalse(b'field1' in fs, fs)\n \n     def test_from_response_override_clickable(self):\n         response = _buildresponse('''<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>''')\n         req = self.request_class.from_response(response, \\\n                 formdata={'clickme': 'two'}, clickdata={'name': 'clickme'})\n         fs = _qs(req)\n-        self.assertEqual(fs['clickme'], ['two'])\n+        self.assertEqual(fs[b'clickme'], [b'two'])\n \n     def test_from_response_dont_click(self):\n         response = _buildresponse(\n@@ -436,8 +592,8 @@ class FormRequestTest(RequestTest):\n             </form>\"\"\")\n         r1 = self.request_class.from_response(response, dont_click=True)\n         fs = _qs(r1)\n-        self.assertFalse('clickable1' in fs, fs)\n-        self.assertFalse('clickable2' in fs, fs)\n+        self.assertFalse(b'clickable1' in fs, fs)\n+        self.assertFalse(b'clickable2' in fs, fs)\n \n     def test_from_response_ambiguous_clickdata(self):\n         response = _buildresponse(\n@@ -468,8 +624,8 @@ class FormRequestTest(RequestTest):\n             \"\"\")\n         req = self.request_class.from_response(response, clickdata={'nr': 1})\n         fs = _qs(req)\n-        self.assertIn('clickable2', fs)\n-        self.assertNotIn('clickable1', fs)\n+        self.assertIn(b'clickable2', fs)\n+        self.assertNotIn(b'clickable1', fs)\n \n     def test_from_response_invalid_nr_index_clickdata(self):\n         response = _buildresponse(\n@@ -490,7 +646,7 @@ class FormRequestTest(RequestTest):\n                                   \"\"\"</form></body></html>\"\"\")\n         req = self.request_class.from_response(response, formdata={'bar': 'buz'})\n         fs = _qs(req)\n-        self.assertEqual(fs, {'foo': ['xxx'], 'bar': ['buz']})\n+        self.assertEqual(fs, {b'foo': [b'xxx'], b'bar': [b'buz']})\n \n     def test_from_response_errors_formnumber(self):\n         response = _buildresponse(\n@@ -634,7 +790,7 @@ class FormRequestTest(RequestTest):\n             <select name=\"i7\"/>\n             </form>''')\n         req = self.request_class.from_response(res)\n-        fs = _qs(req)\n+        fs = _qs(req, to_unicode=True)\n         self.assertEqual(fs, {'i1': ['i1v2'], 'i2': ['i2v1'], 'i4': ['i4v2', 'i4v3']})\n \n     def test_from_response_radio(self):\n@@ -651,7 +807,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['iv2'], 'i2': ['on']})\n+        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})\n \n     def test_from_response_checkbox(self):\n         res = _buildresponse(\n@@ -667,7 +823,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['iv2'], 'i2': ['on']})\n+        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})\n \n     def test_from_response_input_text(self):\n         res = _buildresponse(\n@@ -680,7 +836,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': [''], 'i4': ['i4v1']})\n+        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b''], b'i4': [b'i4v1']})\n \n     def test_from_response_input_hidden(self):\n         res = _buildresponse(\n@@ -692,7 +848,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})\n+        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b'']})\n \n     def test_from_response_input_textarea(self):\n         res = _buildresponse(\n@@ -704,7 +860,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v'], 'i2': [''], 'i3': ['']})\n+        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b''], b'i3': [b'']})\n \n     def test_from_response_descendants(self):\n         res = _buildresponse(\n@@ -725,7 +881,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(set(fs), set(['h2', 'i2', 'i1', 'i3', 'h1', 'i5', 'i4']))\n+        self.assertEqual(set(fs), set([b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4']))\n \n     def test_from_response_xpath(self):\n         response = _buildresponse(\n@@ -873,12 +1029,18 @@ def _buildresponse(body, **kwargs):\n     kwargs.setdefault('encoding', 'utf-8')\n     return HtmlResponse(**kwargs)\n \n-def _qs(req):\n+def _qs(req, encoding='utf-8', to_unicode=False):\n     if req.method == 'POST':\n         qs = req.body\n     else:\n         qs = req.url.partition('?')[2]\n-    return cgi.parse_qs(qs, True)\n+    if six.PY2:\n+        uqs = unquote(to_native_str(qs, encoding))\n+    elif six.PY3:\n+        uqs = unquote_to_bytes(qs)\n+    if to_unicode:\n+        uqs = uqs.decode(encoding)\n+    return parse_qs(uqs, True)\n \n \n class XmlRpcRequestTest(RequestTest):\n@@ -890,7 +1052,9 @@ class XmlRpcRequestTest(RequestTest):\n     def _test_request(self, **kwargs):\n         r = self.request_class('http://scrapytest.org/rpc2', **kwargs)\n         self.assertEqual(r.headers[b'Content-Type'], b'text/xml')\n-        self.assertEqual(r.body, to_bytes(xmlrpclib.dumps(**kwargs)))\n+        self.assertEqual(r.body,\n+                         to_bytes(xmlrpclib.dumps(**kwargs),\n+                                  encoding=kwargs.get('encoding', 'utf-8')))\n         self.assertEqual(r.method, 'POST')\n         self.assertEqual(r.encoding, kwargs.get('encoding', 'utf-8'))\n         self.assertTrue(r.dont_filter, True)\n@@ -899,14 +1063,13 @@ class XmlRpcRequestTest(RequestTest):\n         self._test_request(params=('value',))\n         self._test_request(params=('username', 'password'), methodname='login')\n         self._test_request(params=('response', ), methodresponse='login')\n-        self._test_request(params=(u'pas\\xa3',), encoding='utf-8')\n+        self._test_request(params=(u'pas£',), encoding='utf-8')\n         self._test_request(params=(None,), allow_none=1)\n         self.assertRaises(TypeError, self._test_request)\n         self.assertRaises(TypeError, self._test_request, params=(None,))\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n     def test_latin1(self):\n-        self._test_request(params=(u'pas\\xa3',), encoding='latin')\n+        self._test_request(params=(u'pas£',), encoding='latin1')\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
