{"custom_id": "keras#a8d14a571908d1bae359c884d444d4831a14287d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 720 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ aligned image pairs. However, obtaining paired examples isn't always feasible.\n CycleGAN tries to learn this mapping without requiring paired input-output images,\n using cycle-consistent adversarial networks.\n \n-- [Paper](https://arxiv.org/pdf/1703.10593.pdf)\n+- [Paper](https://arxiv.org/abs/1703.10593)\n - [Original implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf6b569cb39bb07045c1221d9b81eba0f2857d21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Title: Endpoint layer pattern\n Author: [fchollet](https://twitter.com/fchollet)\n Date created: 2019/05/10\n-Last modified: 2019/05/10\n+Last modified: 2023/11/22\n Description: Demonstration of the \"endpoint layer\" pattern (layer that handles loss management).\n Accelerator: GPU\n \"\"\"\n@@ -11,6 +11,9 @@ Accelerator: GPU\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import tensorflow as tf\n import keras\n import numpy as np\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#045d1041cf04395647c4a147b3203f271153dde0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 23 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 726 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Title: Traffic forecasting using graph neural networks and LSTM\n Author: [Arash Khodadadi](https://www.linkedin.com/in/arash-khodadadi-08a02490/)\n Date created: 2021/12/28\n-Last modified: 2021/12/28\n+Last modified: 2023/11/22\n Description: This example demonstrates how to do timeseries forecasting over graphs.\n Accelerator: GPU\n \"\"\"\n@@ -40,16 +40,18 @@ Joint Conference on Artificial Intelligence, 2018.\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import pandas as pd\n import numpy as np\n-import os\n import typing\n import matplotlib.pyplot as plt\n \n import tensorflow as tf\n import keras\n from keras import layers\n-from keras.utils import timeseries_dataset_from_array\n+from keras import ops\n \n \"\"\"\n ## Data preparation\n@@ -222,7 +224,7 @@ where `N` is the number of roads.\n \n \"\"\"\n We use the Keras built-in function\n-[`timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array).\n+`keras.utils.timeseries_dataset_from_array`.\n The function `create_tf_dataset()` below takes as input a `numpy.ndarray` and returns a\n `tf.data.Dataset`. In this function `input_sequence_length=T` and `forecast_horizon=h`.\n \n@@ -281,7 +283,7 @@ def create_tf_dataset(\n         A tf.data.Dataset instance.\n     \"\"\"\n \n-    inputs = timeseries_dataset_from_array(\n+    inputs = keras.utils.timeseries_dataset_from_array(\n         np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n         None,\n         sequence_length=input_sequence_length,\n@@ -295,7 +297,7 @@ def create_tf_dataset(\n         else input_sequence_length + forecast_horizon - 1\n     )\n     target_seq_length = forecast_horizon if multi_horizon else 1\n-    targets = timeseries_dataset_from_array(\n+    targets = keras.utils.timeseries_dataset_from_array(\n         data_array[target_offset:],\n         None,\n         sequence_length=target_seq_length,\n@@ -437,7 +439,7 @@ class GraphConv(layers.Layer):\n         self.graph_info = graph_info\n         self.aggregation_type = aggregation_type\n         self.combination_type = combination_type\n-        self.weight = tf.Variable(\n+        self.weight = keras.Variable(\n             initial_value=keras.initializers.GlorotUniform()(\n                 shape=(in_feat, out_feat), dtype=\"float32\"\n             ),\n@@ -445,7 +447,7 @@ class GraphConv(layers.Layer):\n         )\n         self.activation = layers.Activation(activation)\n \n-    def aggregate(self, neighbour_representations: tf.Tensor):\n+    def aggregate(self, neighbour_representations):\n         aggregation_func = {\n             \"sum\": tf.math.unsorted_segment_sum,\n             \"mean\": tf.math.unsorted_segment_mean,\n@@ -461,7 +463,7 @@ class GraphConv(layers.Layer):\n \n         raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n \n-    def compute_nodes_representation(self, features: tf.Tensor):\n+    def compute_nodes_representation(self, features):\n         \"\"\"Computes each node's representation.\n \n         The nodes' representations are obtained by multiplying the features tensor with\n@@ -474,30 +476,29 @@ class GraphConv(layers.Layer):\n         Returns:\n             A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n         \"\"\"\n-        return tf.matmul(features, self.weight)\n+        return ops.matmul(features, self.weight)\n \n-    def compute_aggregated_messages(self, features: tf.Tensor):\n+    def compute_aggregated_messages(self, features):\n         neighbour_representations = tf.gather(\n             features, self.graph_info.edges[1]\n         )\n         aggregated_messages = self.aggregate(neighbour_representations)\n-        return tf.matmul(aggregated_messages, self.weight)\n+        return ops.matmul(aggregated_messages, self.weight)\n \n     def update(\n-        self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor\n+        self, nodes_representation, aggregated_messages\n     ):\n         if self.combination_type == \"concat\":\n-            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n+            h = ops.concatenate([nodes_representation, aggregated_messages], axis=-1)\n         elif self.combination_type == \"add\":\n             h = nodes_representation + aggregated_messages\n         else:\n             raise ValueError(\n                 f\"Invalid combination type: {self.combination_type}.\"\n             )\n-\n         return self.activation(h)\n \n-    def call(self, features: tf.Tensor):\n+    def call(self, features):\n         \"\"\"Forward pass.\n \n         Args:\n@@ -562,19 +563,19 @@ class LSTMGC(layers.Layer):\n         \"\"\"Forward pass.\n \n         Args:\n-            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n+            inputs: tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n \n         Returns:\n             A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n         \"\"\"\n \n         # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n-        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n+        inputs = ops.transpose(inputs, [2, 0, 1, 3])\n \n         gcn_out = self.graph_conv(\n             inputs\n         )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n-        shape = tf.shape(gcn_out)\n+        shape = ops.shape(gcn_out)\n         num_nodes, batch_size, input_seq_len, out_feat = (\n             shape[0],\n             shape[1],\n@@ -583,7 +584,7 @@ class LSTMGC(layers.Layer):\n         )\n \n         # LSTM takes only 3D tensors as input\n-        gcn_out = tf.reshape(\n+        gcn_out = ops.reshape(\n             gcn_out, (batch_size * num_nodes, input_seq_len, out_feat)\n         )\n         lstm_out = self.lstm(\n@@ -593,10 +594,10 @@ class LSTMGC(layers.Layer):\n         dense_output = self.dense(\n             lstm_out\n         )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n-        output = tf.reshape(\n+        output = ops.reshape(\n             dense_output, (num_nodes, batch_size, self.output_seq_len)\n         )\n-        return tf.transpose(\n+        return ops.transpose(\n             output, [1, 2, 0]\n         )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3fd2f955fa29ed05b16c40f81231cbf91fc8a47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 734 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -439,10 +439,10 @@ class GraphConv(layers.Layer):\n         self.graph_info = graph_info\n         self.aggregation_type = aggregation_type\n         self.combination_type = combination_type\n-        self.weight = keras.Variable(\n-            initial_value=keras.initializers.GlorotUniform()(\n-                shape=(in_feat, out_feat), dtype=\"float32\"\n-            ),\n+        self.weight = self.add_weight(\n+            initializer=keras.initializers.GlorotUniform(),\n+            shape=(in_feat, out_feat),\n+            dtype=\"float32\",\n             trainable=True,\n         )\n         self.activation = layers.Activation(activation)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c7dadf012d565bfe0eb37d4c9cb113082ef3b38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 11 | Churn Cumulative: 11 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,14 +2,13 @@\n Title: Timeseries forecasting for weather prediction\n Authors: [Prabhanshu Attri](https://prabhanshu.com/github), [Yashika Sharma](https://github.com/yashika51), [Kristi Takach](https://github.com/ktakattack), [Falak Shah](https://github.com/falaktheoptimist)\n Date created: 2020/06/23\n-Last modified: 2020/07/20\n+Last modified: 2023/11/22\n Description: This notebook demonstrates how to do timeseries forecasting using a LSTM model.\n Accelerator: GPU\n \"\"\"\n \n \"\"\"\n ## Setup\n-This example requires TensorFlow 2.3 or higher.\n \"\"\"\n \n import pandas as pd\n@@ -389,11 +388,3 @@ for x, y in dataset_val.take(5):\n         12,\n         \"Single Step Prediction\",\n     )\n-\n-\n-\"\"\"\n-**Example available on HuggingFace**\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Time%20Series-black.svg)](https://huggingface.co/keras-io/timeseries_forecasting_for_weather) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Time%20Series-black.svg)](https://huggingface.co/spaces/keras-io/timeseries_forecasting_for_weather) |\n-\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46853c8b611c15be60c98e31edf47b0e1769f365", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 19 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,25 +143,6 @@ def show_raw_visualization(data):\n \n show_raw_visualization(df)\n \n-\"\"\"\n-This heat map shows the correlation between different features.\n-\"\"\"\n-\n-\n-def show_heatmap(data):\n-    plt.matshow(data.corr())\n-    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n-    plt.gca().xaxis.tick_bottom()\n-    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n-\n-    cb = plt.colorbar()\n-    cb.ax.tick_params(labelsize=14)\n-    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n-    plt.show()\n-\n-\n-show_heatmap(df)\n-\n \n \"\"\"\n ## Data Preprocessing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0f29d14a868fa86e124b091d42bb37995a31dd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 63 | Files Changed: 6 | Hunks: 35 | Methods Changed: 28 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 118 | Churn Cumulative: 7692 | Contributors (this commit): 7 | Commits (past 90d): 104 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -287,12 +287,12 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     )\n \n \n-def cumprod(x, axis=None):\n-    return jnp.cumprod(x, axis=axis)\n+def cumprod(x, axis=None, dtype=None):\n+    return jnp.cumprod(x, axis=axis, dtype=dtype)\n \n \n-def cumsum(x, axis=None):\n-    return jnp.cumsum(x, axis=axis)\n+def cumsum(x, axis=None, dtype=None):\n+    return jnp.cumsum(x, axis=axis, dtype=dtype)\n \n \n def diag(x, k=0):\n\n@@ -345,14 +345,14 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     )\n \n \n-def cumprod(x, axis=None):\n+def cumprod(x, axis=None, dtype=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n-    return np.cumprod(x, axis=axis)\n+    return np.cumprod(x, axis=axis, dtype=dtype or x.dtype)\n \n \n-def cumsum(x, axis=None):\n+def cumsum(x, axis=None, dtype=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n-    return np.cumsum(x, axis=axis)\n+    return np.cumsum(x, axis=axis, dtype=dtype or x.dtype)\n \n \n def diag(x, k=0):\n\n@@ -574,12 +574,12 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     )\n \n \n-def cumprod(x, axis=None):\n-    return tfnp.cumprod(x, axis=axis)\n+def cumprod(x, axis=None, dtype=None):\n+    return tfnp.cumprod(x, axis=axis, dtype=dtype or x.dtype)\n \n \n-def cumsum(x, axis=None):\n-    return tfnp.cumsum(x, axis=axis)\n+def cumsum(x, axis=None, dtype=None):\n+    return tfnp.cumsum(x, axis=axis, dtype=dtype or x.dtype)\n \n \n def diag(x, k=0):\n\n@@ -458,20 +458,24 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=-1):\n     return cast(torch.cross(x1, x2, dim=axis), result_dtype)\n \n \n-def cumprod(x, axis=None):\n+def cumprod(x, axis=None, dtype=None):\n     x = convert_to_tensor(x)\n     if axis is None:\n         x = x.flatten()\n         axis = 0\n-    return torch.cumprod(x, dim=axis)\n+    return torch.cumprod(\n+        x, dim=axis, dtype=x.dtype if dtype is None else to_torch_dtype(dtype)\n+    )\n \n \n-def cumsum(x, axis=None):\n+def cumsum(x, axis=None, dtype=None):\n     x = convert_to_tensor(x)\n     if axis is None:\n         x = x.flatten()\n         axis = 0\n-    return torch.cumsum(x, dim=axis)\n+    return torch.cumsum(\n+        x, dim=axis, dtype=x.dtype if dtype is None else to_torch_dtype(dtype)\n+    )\n \n \n def diag(x, k=0):\n\n@@ -1779,12 +1779,13 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n \n \n class Cumprod(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=None, dtype=None):\n         super().__init__()\n         self.axis = axis\n+        self.dtype = dtype\n \n     def call(self, x):\n-        return backend.numpy.cumprod(x, axis=self.axis)\n+        return backend.numpy.cumprod(x, axis=self.axis, dtype=self.dtype)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -1792,34 +1793,35 @@ class Cumprod(Operation):\n                 output_shape = (None,)\n             else:\n                 output_shape = (int(np.prod(x.shape)),)\n-            return KerasTensor(output_shape, dtype=\"int32\")\n-        return KerasTensor(x.shape, dtype=x.dtype)\n+        else:\n+            output_shape = x.shape\n+        return KerasTensor(output_shape, self.dtype or x.dtype)\n \n \n @keras_export([\"keras.ops.cumprod\", \"keras.ops.numpy.cumprod\"])\n-def cumprod(x, axis=None):\n+def cumprod(x, axis=None, dtype=None):\n     \"\"\"Return the cumulative product of elements along a given axis.\n \n     Args:\n         x: Input tensor.\n         axis: Axis along which the cumulative product is computed.\n             By default the input is flattened.\n+        dtype: dtype of returned tensor. Defaults to x.dtype.\n \n     Returns:\n         Output tensor.\n     \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Cumprod(axis=axis).symbolic_call(x)\n-    return backend.numpy.cumprod(x, axis=axis)\n+    return Cumprod(axis=axis, dtype=dtype)(x)\n \n \n class Cumsum(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=None, dtype=None):\n         super().__init__()\n         self.axis = axis\n+        self.dtype = dtype\n \n     def call(self, x):\n-        return backend.numpy.cumsum(x, axis=self.axis)\n+        return backend.numpy.cumsum(x, axis=self.axis, dtype=self.dtype)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -1827,25 +1829,25 @@ class Cumsum(Operation):\n                 output_shape = (None,)\n             else:\n                 output_shape = (int(np.prod(x.shape)),)\n-            return KerasTensor(output_shape, dtype=\"int32\")\n-        return KerasTensor(x.shape, dtype=x.dtype)\n+        else:\n+            output_shape = x.shape\n+        return KerasTensor(output_shape, self.dtype or x.dtype)\n \n \n @keras_export([\"keras.ops.cumsum\", \"keras.ops.numpy.cumsum\"])\n-def cumsum(x, axis=None):\n+def cumsum(x, axis=None, dtype=None):\n     \"\"\"Returns the cumulative sum of elements along a given axis.\n \n     Args:\n         x: Input tensor.\n         axis: Axis along which the cumulative sum is computed.\n             By default the input is flattened.\n+        dtype: dtype of returned tensor. Defaults to x.dtype.\n \n     Returns:\n         Output tensor.\n     \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Cumsum(axis=axis).symbolic_call(x)\n-    return backend.numpy.cumsum(x, axis=axis)\n+    return Cumsum(axis=axis, dtype=dtype)(x)\n \n \n class Diag(Operation):\n\n@@ -3172,48 +3172,34 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.count_nonzero(x, axis=1),\n         )\n \n-    def test_cumprod(self):\n+    @parameterized.product(\n+        axis=[None, 0, 1, -1],\n+        dtype=[None, \"int32\", \"float32\"],\n+    )\n+    def test_cumprod(self, axis, dtype):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n-        self.assertAllClose(knp.cumprod(x), np.cumprod(x))\n         self.assertAllClose(\n-            knp.cumprod(x, axis=0),\n-            np.cumprod(x, axis=0),\n+            knp.cumprod(x, axis=axis, dtype=dtype),\n+            np.cumprod(x, axis=axis, dtype=dtype or x.dtype),\n         )\n         self.assertAllClose(\n-            knp.cumprod(x, axis=None),\n-            np.cumprod(x, axis=None),\n+            knp.Cumprod(axis=axis, dtype=dtype)(x),\n+            np.cumprod(x, axis=axis, dtype=dtype or x.dtype),\n         )\n \n-        self.assertAllClose(knp.Cumprod()(x), np.cumprod(x))\n-        self.assertAllClose(\n-            knp.Cumprod(axis=0)(x),\n-            np.cumprod(x, axis=0),\n+    @parameterized.product(\n+        axis=[None, 0, 1, -1],\n+        dtype=[None, \"int32\", \"float32\"],\n     )\n-        self.assertAllClose(\n-            knp.Cumprod(axis=None)(x),\n-            np.cumprod(x, axis=None),\n-        )\n-\n-    def test_cumsum(self):\n+    def test_cumsum(self, axis, dtype):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n-        self.assertAllClose(knp.cumsum(x), np.cumsum(x))\n         self.assertAllClose(\n-            knp.cumsum(x, axis=0),\n-            np.cumsum(x, axis=0),\n+            knp.cumsum(x, axis=axis, dtype=dtype),\n+            np.cumsum(x, axis=axis, dtype=dtype or x.dtype),\n         )\n         self.assertAllClose(\n-            knp.cumsum(x, axis=1),\n-            np.cumsum(x, axis=1),\n-        )\n-\n-        self.assertAllClose(knp.Cumsum()(x), np.cumsum(x))\n-        self.assertAllClose(\n-            knp.Cumsum(axis=0)(x),\n-            np.cumsum(x, axis=0),\n-        )\n-        self.assertAllClose(\n-            knp.Cumsum(axis=1)(x),\n-            np.cumsum(x, axis=1),\n+            knp.Cumsum(axis=axis, dtype=dtype)(x),\n+            np.cumsum(x, axis=axis, dtype=dtype or x.dtype),\n         )\n \n     def test_diag(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47cee523c4d1a9a271db78f0f36e3f6fb33b818a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 156 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -509,8 +509,12 @@ def moments(x, axes, keepdims=False, synchronized=False):\n     # but less numerically stable.\n     # Note: stop_gradient does not change the gradient to the mean, because that\n     # gradient is zero.\n-    variance = jnp.mean(jnp.square(x), axis=axes, keepdims=True) - jnp.square(\n-        jax.lax.stop_gradient(mean)\n+    # The substraction operation does not guarantee a non-negative\n+    # result given float precision, so we clamp it to 0.\n+    variance = jnp.maximum(\n+        jnp.mean(jnp.square(x), axis=axes, keepdims=True)\n+        - jnp.square(jax.lax.stop_gradient(mean)),\n+        0.0,\n     )\n \n     if not keepdims:\n\n@@ -684,10 +684,10 @@ def _compute_moments_sync(x, axes, keepdims):\n     )\n     count_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, local_count)\n \n-    mean = y_sum / count_sum\n-    y_squared_mean = y_squared_sum / count_sum\n+    mean = tf.math.divide_no_nan(y_sum, count_sum)\n+    y_squared_mean = tf.math.divide_no_nan(y_squared_sum, count_sum)\n     # var = E(x^2) - E(x)^2\n-    variance = y_squared_mean - tf.square(mean)\n+    variance = tf.maximum(y_squared_mean - tf.square(mean), 0.0)\n     if not keepdims:\n         mean = tf.squeeze(mean, axes)\n         variance = tf.squeeze(variance, axes)\n@@ -715,9 +715,13 @@ def _compute_moments(x, axes, keepdims):\n     # but less numerically stable.\n     # Note: stop_gradient does not change the gradient to the mean, because that\n     # gradient is zero.\n-    variance = tf.reduce_mean(\n-        tf.square(x), axis=axes, keepdims=True\n-    ) - tf.square(tf.stop_gradient(mean))\n+    # The substraction operation does not guarantee a non-negative\n+    # result given float precision, so we clamp it to 0.\n+    variance = tf.maximum(\n+        tf.reduce_mean(tf.square(x), axis=axes, keepdims=True)\n+        - tf.square(tf.stop_gradient(mean)),\n+        0.0,\n+    )\n \n     if not keepdims:\n         mean = tf.squeeze(mean, axes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbe397c0989eac38aa445d4c29e8469a07c694f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 388 | Lines Deleted: 65 | Files Changed: 8 | Hunks: 53 | Methods Changed: 26 | Complexity Δ (Sum/Max): 59/31 | Churn Δ: 453 | Churn Cumulative: 10686 | Contributors (this commit): 20 | Commits (past 90d): 122 | Contributors (cumulative): 52 | DMM Complexity: 0.48031496062992124\n\nDIFF:\n@@ -753,9 +753,6 @@ def where(condition, x1, x2):\n def divide(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n-    x1 = cast(x1, dtype)\n-    x2 = cast(x2, dtype)\n     return jnp.divide(x1, x2)\n \n \n\n@@ -8,11 +8,16 @@ from keras.backend.numpy.core import convert_to_tensor\n \n \n def add(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = x1.astype(dtype)\n-    x2 = x2.astype(dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return np.add(x1, x2)\n \n \n@@ -31,10 +36,17 @@ def einsum(subscripts, *operands, **kwargs):\n \n \n def subtract(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    return np.subtract(x1, x2).astype(dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return np.subtract(x1, x2)\n \n \n def matmul(x1, x2):\n@@ -45,10 +57,17 @@ def matmul(x1, x2):\n \n \n def multiply(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    return np.multiply(x1, x2).astype(dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return np.multiply(x1, x2)\n \n \n def mean(x, axis=None, keepdims=False):\n@@ -596,15 +615,17 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n \n \n def maximum(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    # TODO: np.maximum doesn't support dtype=bfloat16, so we need to cast it\n-    # first\n-    if dtype == \"bfloat16\":\n-        x1 = x1.astype(dtype)\n-        x2 = x2.astype(dtype)\n-    return np.maximum(x1, x2, dtype=dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return np.maximum(x1, x2)\n \n \n def median(x, axis=None, keepdims=False):\n@@ -622,15 +643,17 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n \n def minimum(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    # TODO: np.minimum doesn't support dtype=bfloat16, so we need to cast it\n-    # first\n-    if dtype == \"bfloat16\":\n-        x1 = x1.astype(dtype)\n-        x2 = x2.astype(dtype)\n-    return np.minimum(x1, x2, dtype=dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n+    return np.minimum(x1, x2)\n \n \n def mod(x1, x2):\n@@ -878,11 +901,17 @@ def where(condition, x1, x2):\n \n \n def divide(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n-    x1 = x1.astype(dtype)\n-    x2 = x2.astype(dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+        float,\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return np.divide(x1, x2)\n \n \n\n@@ -17,11 +17,16 @@ from keras.backend.tensorflow.core import convert_to_tensor\n \n @sparse.elementwise_binary_union(tf.sparse.add)\n def add(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.add(x1, x2)\n \n \n@@ -90,11 +95,16 @@ def einsum(subscripts, *operands, **kwargs):\n \n @sparse.elementwise_binary_union(sparse.sparse_subtract)\n def subtract(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.subtract(x1, x2)\n \n \n@@ -191,11 +201,16 @@ def matmul(x1, x2):\n \n @sparse.elementwise_binary_intersection\n def multiply(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.multiply(x1, x2)\n \n \n@@ -915,11 +930,16 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n \n @sparse.elementwise_binary_union(tf.sparse.maximum, densify_mixed=True)\n def maximum(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.maximum(x1, x2)\n \n \n@@ -954,11 +974,16 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n @sparse.elementwise_binary_union(tf.sparse.minimum, densify_mixed=True)\n def minimum(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.minimum(x1, x2)\n \n \n@@ -1390,11 +1415,17 @@ def where(condition, x1, x2):\n \n @sparse.elementwise_division\n def divide(x1, x2):\n+    if not isinstance(x1, (int, float)):\n         x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n         x2 = convert_to_tensor(x2)\n-    dtype = dtypes.result_type(x1.dtype, x2.dtype, float)\n-    x1 = tf.cast(x1, dtype)\n-    x2 = tf.cast(x2, dtype)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+        float,\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return tfnp.divide(x1, x2)\n \n \n\n@@ -803,7 +803,16 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n \n \n def maximum(x1, x2):\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return torch.maximum(x1, x2)\n \n \n@@ -881,7 +890,16 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n \n def minimum(x1, x2):\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(\n+        getattr(x1, \"dtype\", type(x1)),\n+        getattr(x2, \"dtype\", type(x2)),\n+    )\n+    x1 = convert_to_tensor(x1, dtype)\n+    x2 = convert_to_tensor(x2, dtype)\n     return torch.minimum(x1, x2)\n \n \n@@ -1280,7 +1298,10 @@ def where(condition, x1, x2):\n \n \n def divide(x1, x2):\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    if not isinstance(x1, (int, float)):\n+        x1 = convert_to_tensor(x1)\n+    if not isinstance(x2, (int, float)):\n+        x2 = convert_to_tensor(x2)\n     return torch.divide(x1, x2)\n \n \n\n@@ -45,9 +45,7 @@ class UnitNormalization(Layer):\n         x = ops.cast(inputs, self.compute_dtype)\n \n         square_sum = ops.sum(ops.square(x), axis=self.axis, keepdims=True)\n-        x_inv_norm = ops.rsqrt(\n-            ops.maximum(square_sum, ops.cast(1e-12, self.compute_dtype))\n-        )\n+        x_inv_norm = ops.rsqrt(ops.maximum(square_sum, 1e-12))\n         return ops.multiply(x, x_inv_norm)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -298,19 +298,13 @@ class Normalization(Layer):\n                 self.mean,\n                 ops.multiply(\n                     inputs,\n-                    ops.maximum(\n-                        ops.sqrt(self.variance),\n-                        ops.cast(backend.epsilon(), self.compute_dtype),\n-                    ),\n+                    ops.maximum(ops.sqrt(self.variance), backend.epsilon()),\n                 ),\n             )\n         else:\n             return ops.divide(\n                 ops.subtract(inputs, self.mean),\n-                ops.maximum(\n-                    ops.sqrt(self.variance),\n-                    ops.cast(backend.epsilon(), self.compute_dtype),\n-                ),\n+                ops.maximum(ops.sqrt(self.variance), backend.epsilon()),\n             )\n \n     def compute_output_shape(self, input_shape):\n\n@@ -5549,8 +5549,8 @@ class Divide(Operation):\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n         output_dtype = dtypes.result_type(\n-            getattr(x1, \"dtype\", backend.floatx()),\n-            getattr(x2, \"dtype\", backend.floatx()),\n+            getattr(x1, \"dtype\", type(x1)),\n+            getattr(x2, \"dtype\", type(x2)),\n             float,\n         )\n         x1_sparse = getattr(x1, \"sparse\", False)\n\n@@ -4343,6 +4343,48 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_add_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.add doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((1,), dtype=dtype)\n+            x_jax = jnp.ones((1,), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.add(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.add(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Add().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.add(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.add(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Add().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(named_product(dtype=INT_DTYPES))\n     def test_bincount(self, dtype):\n         import jax.numpy as jnp\n@@ -4428,6 +4470,48 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             knp.Subtract().symbolic_call(x1, x2).dtype, expected_dtype\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_subtract_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.subtract doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((1,), dtype=dtype)\n+            x_jax = jnp.ones((1,), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.subtract(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.subtract(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Subtract().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.subtract(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.subtract(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Subtract().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n@@ -4468,6 +4552,48 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             knp.Multiply().symbolic_call(x1, x2).dtype, expected_dtype\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_multiply_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.multiply doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((1,), dtype=dtype)\n+            x_jax = jnp.ones((1,), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.multiply(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.multiply(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Multiply().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.multiply(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.multiply(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Multiply().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_mean(self, dtype):\n         import jax.numpy as jnp\n@@ -5112,18 +5238,23 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n     def test_divide(self, dtypes):\n+        import jax.experimental\n         import jax.numpy as jnp\n \n+        # We have to disable x64 for jax since jnp.divide doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n             dtype1, dtype2 = dtypes\n             x1 = knp.ones((1,), dtype=dtype1)\n             x2 = knp.ones((1,), dtype=dtype2)\n             x1_jax = jnp.ones((1,), dtype=dtype1)\n             x2_jax = jnp.ones((1,), dtype=dtype2)\n             expected_dtype = standardize_dtype(jnp.divide(x1_jax, x2_jax).dtype)\n-        if dtype1 is not None and \"float\" not in dtype1:\n-            if dtype2 is not None and \"float\" not in dtype2:\n-                if \"int64\" in (dtype1, dtype2) or \"uint32\" in (dtype1, dtype2):\n-                    expected_dtype = backend.floatx()\n+            if \"float64\" in (dtype1, dtype2):\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n \n             self.assertEqual(\n                 standardize_dtype(knp.divide(x1, x2).dtype), expected_dtype\n@@ -5132,6 +5263,46 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n                 knp.Divide().symbolic_call(x1, x2).dtype, expected_dtype\n             )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_divide_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.divide doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`. We also need to downcast\n+        # the expected dtype from 64 bit to 32 bit when using jax backend.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((), dtype=dtype)\n+            x_jax = jnp.ones((), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.divide(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.divide(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Divide().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.divide(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.divide(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Divide().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n@@ -5807,6 +5978,47 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_maximum_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.maximum doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((), dtype=dtype)\n+            x_jax = jnp.ones((), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.maximum(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.maximum(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Maximum().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.maximum(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.maximum(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Maximum().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_median(self, dtype):\n         import jax.numpy as jnp\n@@ -5883,6 +6095,47 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_minimum_python_types(self, dtype):\n+        import jax.experimental\n+        import jax.numpy as jnp\n+\n+        # We have to disable x64 for jax since jnp.minimum doesn't respect\n+        # JAX_DEFAULT_DTYPE_BITS=32 in `./conftest.py`.\n+        with jax.experimental.disable_x64():\n+            x = knp.ones((), dtype=dtype)\n+            x_jax = jnp.ones((), dtype=dtype)\n+\n+            # python int\n+            expected_dtype = standardize_dtype(jnp.minimum(x_jax, 1).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            elif dtype == \"int64\":\n+                expected_dtype = \"int64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.minimum(x, 1).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Minimum().symbolic_call(x, 1).dtype, expected_dtype\n+            )\n+\n+            # python float\n+            expected_dtype = standardize_dtype(jnp.minimum(x_jax, 1.0).dtype)\n+            if dtype == \"float64\":\n+                expected_dtype = \"float64\"\n+            if backend.backend() == \"jax\":\n+                expected_dtype = expected_dtype.replace(\"64\", \"32\")\n+\n+            self.assertEqual(\n+                standardize_dtype(knp.minimum(x, 1.0).dtype), expected_dtype\n+            )\n+            self.assertEqual(\n+                knp.Minimum().symbolic_call(x, 1.0).dtype, expected_dtype\n+            )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
