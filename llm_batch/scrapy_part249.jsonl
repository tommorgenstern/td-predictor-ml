{"custom_id": "scrapy#69a1ee79aa43bb1444e5b2a800a2e1702db6866e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 253 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -93,7 +93,7 @@ class Request(object_ref):\n         given new values.\n         \"\"\"\n         for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n-                  'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n+                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'kwargs']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a67f1ce512ffa111a400e2718ebbf3e7bd32d0ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 106 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -32,7 +32,8 @@ def request_to_dict(request, spider=None):\n         '_encoding': request._encoding,\n         'priority': request.priority,\n         'dont_filter': request.dont_filter,\n-        'flags': request.flags\n+        'flags': request.flags,\n+        'kwargs': request.kwargs,\n     }\n     if type(request) is not Request:\n         d['_class'] = request.__module__ + '.' + request.__class__.__name__\n@@ -64,7 +65,9 @@ def request_from_dict(d, spider=None):\n         encoding=d['_encoding'],\n         priority=d['priority'],\n         dont_filter=d['dont_filter'],\n-        flags=d.get('flags'))\n+        flags=d.get('flags'),\n+        kwargs=d.get('kwargs'),\n+    )\n \n \n def _find_method(obj, func):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#770a501fb32b7582acaa5900ac2f41ea46a321cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -11/2 | Churn Δ: 15 | Churn Cumulative: 1284 | Contributors (this commit): 19 | Commits (past 90d): 11 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,8 +40,14 @@ class Request(object_ref):\n         self.dont_filter = dont_filter\n \n         self._meta = dict(meta) if meta else None\n+        self._kwargs = dict(kwargs) if kwargs else None\n         self.flags = [] if flags is None else list(flags)\n-        self.kwargs = dict(kwargs) if kwargs else None\n+\n+    @property\n+    def kwargs(self):\n+        if self._kwargs is None:\n+            self._kwargs = {}\n+        return self._kwargs\n \n     @property\n     def meta(self):\n\n@@ -177,6 +177,7 @@ class RequestTest(unittest.TestCase):\n         r1 = self.request_class(\"http://www.example.com\", flags=['f1', 'f2'],\n                                 callback=somecallback, errback=somecallback)\n         r1.meta['foo'] = 'bar'\n+        r1.kwargs['key'] = 'value'\n         r2 = r1.copy()\n \n         # make sure copy does not propagate callbacks\n@@ -189,6 +190,10 @@ class RequestTest(unittest.TestCase):\n         assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n         self.assertEqual(r1.flags, r2.flags)\n \n+        # make sure kwargs dict is shallow copied\n+        assert r1.kwargs is not r2.kwargs, \"kwargs must be a shallow copy, not identical\"\n+        self.assertEqual(r1.kwargs, r2.kwargs)\n+\n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n\n@@ -26,6 +26,7 @@ class RequestSerializationTest(unittest.TestCase):\n             encoding='latin-1',\n             priority=20,\n             meta={'a': 'b'},\n+            kwargs={'k': 'v'},\n             flags=['testFlag'])\n         self._assert_serializes_ok(r, spider=self.spider)\n \n@@ -52,6 +53,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1.headers, r2.headers)\n         self.assertEqual(r1.cookies, r2.cookies)\n         self.assertEqual(r1.meta, r2.meta)\n+        self.assertEqual(r1.kwargs, r2.kwargs)\n         self.assertEqual(r1._encoding, r2._encoding)\n         self.assertEqual(r1.priority, r2.priority)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57e7c769779b9d37058d1d8839215a9c269b8c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 42 | Churn Cumulative: 491 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,40 @@ class MetaSpider(MockServerSpider):\n         self.meta['close_reason'] = reason\n \n \n+class KeywordArgumentsSpider(MockServerSpider):\n+\n+    name = 'kwargs'\n+    checks = set()\n+\n+    def start_requests(self):\n+        data = {'key': 'value', 'number': 123}\n+        yield Request(self.mockserver.url('/first'), self.parse_first, kwargs=data)\n+        yield Request(self.mockserver.url('/general_with'), self.parse_general, kwargs=data)\n+        yield Request(self.mockserver.url('/general_without'), self.parse_general)\n+        yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)\n+\n+    def parse_first(self, response, key, number):\n+        self.checks.add(key == 'value')\n+        self.checks.add(number == 123)\n+        yield response.follow(\n+            self.mockserver.url('/two'),\n+            self.parse_second,\n+            kwargs={'new_key': 'new_value'})\n+\n+    def parse_second(self, response, new_key):\n+        self.checks.add(new_key == 'new_value')\n+\n+    def parse_general(self, response, **kwargs):\n+        if response.url.endswith('/general_with'):\n+            self.checks.add(kwargs['key'] == 'value')\n+            self.checks.add(kwargs['number'] == 123)\n+        elif response.url.endswith('/general_without'):\n+            self.checks.add(kwargs == {})\n+\n+    def parse_no_kwargs(self, response):\n+        pass\n+\n+\n class FollowAllSpider(MetaSpider):\n \n     name = 'follow'\n\n@@ -8,7 +8,7 @@ from twisted.trial.unittest import TestCase\n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n from scrapy.utils.python import to_unicode\n-from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n+from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, KeywordArgumentsSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n \n@@ -23,6 +23,12 @@ class CrawlTestCase(TestCase):\n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n \n+    @defer.inlineCallbacks\n+    def test_callback_kwargs(self):\n+        crawler = self.runner.create_crawler(KeywordArgumentsSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(crawler.spider.checks, set([True]))\n+\n     @defer.inlineCallbacks\n     def test_follow_all(self):\n         crawler = self.runner.create_crawler(FollowAllSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac111088c6470d55e4d698247a76a4938f4a6dc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 27 | Churn Cumulative: 840 | Contributors (this commit): 23 | Commits (past 90d): 1 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import six\n-from six.moves.urllib.parse import urljoin, urlencode\n+from six.moves.urllib.parse import urljoin, urlencode, urlsplit\n \n import lxml.html\n from parsel.selector import create_root_node\n@@ -33,7 +33,30 @@ class FormRequest(Request):\n                 self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                 self._set_body(querystr)\n             else:\n-                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\n+                if urlsplit(self.url).query:\n+                    queries = (urlsplit(self.url).query + '&' + querystr).split('&')\n+                else:\n+                    queries = querystr.split('&')\n+                query_dict = {}\n+                for i in range(len(queries)):\n+                    query_list = queries[i].split('=')\n+                    query_dict[query_list[0]] = query_list[1]\n+                querystr = ''\n+                query_key = list(query_dict.keys())\n+                for i in range(len(query_dict)):\n+                    querystr += (query_key[i] + '=' + query_dict[query_key[i]])\n+                    if i!=len(query_dict)-1:\n+                        querystr += '&'\n+                if urlsplit(self.url).fragment:\n+                    if urlsplit(self.url).query:\n+                        self._set_url(self.url[:self.url.index('?')+1]+querystr+'#'+urlsplit(self.url).fragment)\n+                    else:\n+                        self._set_url(self.url+'?'+querystr+'#'+urlsplit(self.url).fragment)\n+                else:\n+                    if urlsplit(self.url).query:\n+                        self._set_url(self.url[:self.url.index('?')+1]+querystr)\n+                    else:\n+                        self._set_url(self.url+'?'+querystr)\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5e454809e240839e76f050b4587abcd80c33765", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 995 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -274,6 +274,19 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class(\"http://www.example.com\", formdata={})\n         self.assertEqual(r1.body, b'')\n         \n+    def test_formdata_overrides_querystring_duplicates(self):\n+        data1 = {'a' : '1', 'b' : '2'}\n+        data2 = [('a','one'), ('a','two'), ('c','three')]\n+    \n+        fs1 = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data1))\n+        self.assertEqual(fs1[b'a'], [b'1'])\n+        self.assertEqual(fs1[b'b'], [b'2'])\n+\n+        fs2 = _qs(self.request_class('http://www.example.com?a=1&b=2&c=3', method='GET', formdata=data2))\n+        self.assertEqual(fs2[b'a'], [b'two'])\n+        self.assertEqual(fs2[b'b'], [b'2'])\n+        self.assertEqual(fs2[b'c'], [b'three'])\n+\n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n         data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bea5d307628a11d4010cdabb87b2a29a6c3c772", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 857 | Contributors (this commit): 23 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,27 +34,30 @@ class FormRequest(Request):\n                 self._set_body(querystr)\n             else:\n                 if urlsplit(self.url).query:\n-                    queries = (urlsplit(self.url).query + '&' + querystr).split('&')\n+                    queries = (urlsplit(self.url).query).split('&')\n                 else:\n                     queries = querystr.split('&')\n                 query_dict = {}\n+                duplicate_key_passed=[]\n+                for i in range(len(querystr.split('&'))):\n+                    query_list = querystr.split('&')[i].split('=')\n+                    duplicate_key_passed.append(query_list[0])\n                 for i in range(len(queries)):\n                     query_list = queries[i].split('=')\n+                    if duplicate_key_passed.count(query_list[0])<=1:\n                         query_dict[query_list[0]] = query_list[1]  \n-                querystr = ''\n+                query_str = ''\n                 query_key = list(query_dict.keys())\n                 for i in range(len(query_dict)):\n-                    querystr += (query_key[i] + '=' + query_dict[query_key[i]])\n-                    if i!=len(query_dict)-1:\n-                        querystr += '&'\n+                    query_str += (query_key[i] + '=' + query_dict[query_key[i]] + '&')       \n                 if urlsplit(self.url).fragment:\n                     if urlsplit(self.url).query:\n-                        self._set_url(self.url[:self.url.index('?')+1]+querystr+'#'+urlsplit(self.url).fragment)\n+                        self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr + '#' + urlsplit(self.url).fragment)\n                     else:\n                         self._set_url(self.url + '?' + querystr + '#' + urlsplit(self.url).fragment)\n                 else:\n                     if urlsplit(self.url).query:\n-                        self._set_url(self.url[:self.url.index('?')+1]+querystr)\n+                        self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr)\n                     else:\n                         self._set_url(self.url + '?' + querystr)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a4bbd6d029f61dc60594fb8bca631c3a494f855", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 859 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ class FormRequest(Request):\n                     duplicate_key_passed.append(query_list[0])\n                 for i in range(len(queries)):\n                     query_list = queries[i].split('=')\n-                    if duplicate_key_passed.count(query_list[0])<=1:\n+                    if duplicate_key_passed.count(query_list[0])==0:\n                         query_dict[query_list[0]] = query_list[1]  \n                 query_str = ''\n                 query_key = list(query_dict.keys())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4abcdcb306d8782e6b792ad36e40391a0bcbd0f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1004 | Contributors (this commit): 16 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -276,17 +276,10 @@ class FormRequestTest(RequestTest):\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n         data1 = {'a' : '1', 'b' : '2'}\n-        data2 = [('a','one'), ('a','two'), ('c','three')]\n-    \n-        fs1 = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data1))\n+        fs = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data1))\n         self.assertEqual(fs1[b'a'], [b'1'])\n         self.assertEqual(fs1[b'b'], [b'2'])\n \n-        fs2 = _qs(self.request_class('http://www.example.com?a=1&b=2&c=3', method='GET', formdata=data2))\n-        self.assertEqual(fs2[b'a'], [b'two'])\n-        self.assertEqual(fs2[b'b'], [b'2'])\n-        self.assertEqual(fs2[b'c'], [b'three'])\n-\n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n         data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#023290dabc55c92ea2143a98f7753abaf21f74c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1012 | Contributors (this commit): 16 | Commits (past 90d): 10 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -275,10 +275,10 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(r1.body, b'')\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n-        data1 = {'a' : '1', 'b' : '2'}\n-        fs = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data1))\n-        self.assertEqual(fs1[b'a'], [b'1'])\n-        self.assertEqual(fs1[b'b'], [b'2'])\n+        data = {'a' : '1', 'b' : '2'}\n+        fs = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data))\n+        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(fs[b'b'], [b'2'])\n \n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f914f6d8c369a18e1f856c01b7d1ad2a63f6e49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 17 | Churn Cumulative: 1111 | Contributors (this commit): 16 | Commits (past 90d): 18 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ This module implements the JSONRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+import copy\n import json\n import warnings\n \n@@ -13,7 +14,10 @@ from scrapy.http.request import Request\n \n class JSONRequest(Request):\n     def __init__(self, *args, **kwargs):\n-        dumps_kwargs = kwargs.pop('dumps_kwargs', {})\n+        dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n+        dumps_kwargs['sort_keys'] = True\n+        self._dumps_kwargs = dumps_kwargs\n+\n         body_passed = kwargs.get('body', None) is not None\n         data = kwargs.pop('data', None)\n         data_passed = data is not None\n@@ -22,7 +26,7 @@ class JSONRequest(Request):\n             warnings.warn('Both body and data passed. data will be ignored')\n \n         elif not body_passed and data_passed:\n-            kwargs['body'] = self.dump(data, **dumps_kwargs)\n+            kwargs['body'] = self._dumps(data)\n \n             if 'method' not in kwargs:\n                 kwargs['method'] = 'POST'\n@@ -30,7 +34,6 @@ class JSONRequest(Request):\n         super(JSONRequest, self).__init__(*args, **kwargs)\n         self.headers.setdefault('Content-Type', 'application/json')\n         self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n-        self._dumps_kwargs = dumps_kwargs\n \n     def replace(self, *args, **kwargs):\n         body_passed = kwargs.get('body', None) is not None\n@@ -41,10 +44,10 @@ class JSONRequest(Request):\n             warnings.warn('Both body and data passed. data will be ignored')\n \n         elif not body_passed and data_passed:\n-            kwargs['body'] = self.dump(data, **self._dumps_kwargs)\n+            kwargs['body'] = self._dumps(data)\n \n         return super(JSONRequest, self).replace(*args, **kwargs)\n \n-    def dump(self, data, **kwargs):\n+    def _dumps(self, data):\n         \"\"\"Convert to JSON \"\"\"\n-        return json.dumps(data, sort_keys=True, **kwargs)\n+        return json.dumps(data, **self._dumps_kwargs)\n\n@@ -1292,7 +1292,7 @@ class JSONRequestTest(RequestTest):\n             self.assertEqual(kwargs['sort_keys'], True)\n \n     def test_replace_dumps_kwargs(self):\n-        \"\"\" Test that dumps_kwargs are provided json.dumps when replace is called \"\"\"\n+        \"\"\" Test that dumps_kwargs are provided to json.dumps when replace is called \"\"\"\n         data1 = {\n             'name1': 'value1',\n         }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f1f4df9666f38f81adda5a41f3cac646a4f18dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 16 | Churn Cumulative: 1030 | Contributors (this commit): 16 | Commits (past 90d): 12 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -275,8 +275,22 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(r1.body, b'')\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n+        #Without fragment\n         data = {'a' : '1', 'b' : '2'}\n-        fs = _qs(self.request_class('http://www.example.com?a=0&a=2&b=1', method='GET', formdata=data))\n+        fs = _qs(self.request_class('http://www.example.com/?a=0&a=2&b=1', method='GET', formdata=data))\n+        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(fs[b'b'], [b'2'])\n+\n+        #With fragment\n+        data = (('a', '1'), ('b', '2'))\n+        url = self.request_class('http://www.example.com/?a=0&a=2&b=1#fragment', method='GET', formdata=data).url.split('#')[0]\n+        fs = _qs(self.request_class(url, method='GET', formdata=data))\n+        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(fs[b'b'], [b'2'])\n+\n+        #Witout query\n+        data = {'a' : '1', 'b' : '2'}\n+        fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e67fa8fc1eaa2189cec08ce4000b3efc254ee1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 41 | Churn Cumulative: 900 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,33 +33,26 @@ class FormRequest(Request):\n                 self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                 self._set_body(querystr)\n             else:\n-                if urlsplit(self.url).query:\n-                    queries = (urlsplit(self.url).query).split('&')\n-                else:\n-                    queries = querystr.split('&')\n+                url_split = urlsplit(self.url)\n+                formdata_key_list = []\n+                for k in querystr.split('&'):\n+                    formdata_key_list.append(k.split('=')[0])\n+\n+                if url_split.query:\n+                    queries = (url_split.query).split('&')\n                     query_dict = {}\n-                duplicate_key_passed=[]\n-                for i in range(len(querystr.split('&'))):\n-                    query_list = querystr.split('&')[i].split('=')\n-                    duplicate_key_passed.append(query_list[0])\n-                for i in range(len(queries)):\n-                    query_list = queries[i].split('=')\n-                    if duplicate_key_passed.count(query_list[0])==0:\n-                        query_dict[query_list[0]] = query_list[1]  \n+\n+                    for x in queries:\n+                        query = x.split('=')\n+                        if formdata_key_list.count(query[0])==0:\n+                            query_dict[query[0]] = query[1]\n                         query_str = ''\n-                query_key = list(query_dict.keys())\n-                for i in range(len(query_dict)):\n-                    query_str += (query_key[i] + '=' + query_dict[query_key[i]] + '&')       \n-                if urlsplit(self.url).fragment:\n-                    if urlsplit(self.url).query:\n-                        self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr + '#' + urlsplit(self.url).fragment)\n+                        for k, v in query_dict:\n+                            query_str += (k + '=' + v + '&')\n+\n+                    self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n                 else:\n-                        self._set_url(self.url + '?' + querystr + '#' + urlsplit(self.url).fragment)\n-                else:\n-                    if urlsplit(self.url).query:\n-                        self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr)\n-                    else:\n-                        self._set_url(self.url + '?' + querystr)\n+                    self._set_url(self.url + '?' + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f86c93f366509ba05faf3b2cdc12f5e3ef6693a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 1053 | Contributors (this commit): 16 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -275,20 +275,27 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(r1.body, b'')\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n-        #Without fragment\n-        data = {'a' : '1', 'b' : '2'}\n-        fs = _qs(self.request_class('http://www.example.com/?a=0&a=2&b=1', method='GET', formdata=data))\n-        self.assertEqual(fs[b'a'], [b'1'])\n-        self.assertEqual(fs[b'b'], [b'2'])\n-\n-        #With fragment\n+        #Both fragment and query in url\n         data = (('a', '1'), ('b', '2'))\n         url = self.request_class('http://www.example.com/?a=0&a=2&b=1#fragment', method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n \n-        #Witout query\n+        #only query in url\n+        data = {'a' : '1', 'b' : '2'}\n+        fs = _qs(self.request_class('http://www.example.com/?a=0&a=2&b=1', method='GET', formdata=data))\n+        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(fs[b'b'], [b'2'])\n+\n+        #only fragment in url\n+        data = (('a', '1'), ('b', '2'))\n+        url = self.request_class('http://www.example.com/#fragment', method='GET', formdata=data).url.split('#')[0]\n+        fs = _qs(self.request_class(url, method='GET', formdata=data))\n+        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(fs[b'b'], [b'2'])\n+\n+        #None of both in url\n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n         self.assertEqual(fs[b'a'], [b'1'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6be73f06c33a27cb3291a1e46d80574b4d23b10b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1069 | Contributors (this commit): 16 | Commits (past 90d): 14 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -277,23 +277,11 @@ class FormRequestTest(RequestTest):\n     def test_formdata_overrides_querystring_duplicates(self):\n         #Both fragment and query in url\n         data = (('a', '1'), ('b', '2'))\n-        url = self.request_class('http://www.example.com/?a=0&a=2&b=1#fragment', method='GET', formdata=data).url.split('#')[0]\n-        fs = _qs(self.request_class(url, method='GET', formdata=data))\n-        self.assertEqual(fs[b'a'], [b'1'])\n-        self.assertEqual(fs[b'b'], [b'2'])\n-\n-        #only query in url\n-        data = {'a' : '1', 'b' : '2'}\n-        fs = _qs(self.request_class('http://www.example.com/?a=0&a=2&b=1', method='GET', formdata=data))\n-        self.assertEqual(fs[b'a'], [b'1'])\n-        self.assertEqual(fs[b'b'], [b'2'])\n-\n-        #only fragment in url\n-        data = (('a', '1'), ('b', '2'))\n-        url = self.request_class('http://www.example.com/#fragment', method='GET', formdata=data).url.split('#')[0]\n+        url = self.request_class('http://www.example.com/?a=0&a=2&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n+        self.assertEqual(fs[b'c'], [b'3'])\n \n         #None of both in url\n         data = {'a' : '1', 'b' : '2'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9f68acb6dd7f5aca2bee95353278e5a40c32b8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 911 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,15 +41,14 @@ class FormRequest(Request):\n                 if url_split.query:\n                     queries = (url_split.query).split('&')\n                     query_dict = {}\n-\n                     for x in queries:\n-                        query = x.split('=')\n-                        if formdata_key_list.count(query[0])==0:\n-                            query_dict[query[0]] = query[1]\n+                        k = x.split('=')[0]\n+                        v = x.split('=')[1]\n+                        if formdata_key_list.count(k)==0:\n+                            query_dict[k] = v\n                         query_str = ''\n-                        for k, v in query_dict:\n+                        for k, v in query_dict.items():\n                             query_str += (k + '=' + v + '&')\n-\n                     self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n                 else:\n                     self._set_url(self.url + '?' + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdf12f775062fda8aa8bf03f7b4faade4faac16d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 9 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,7 @@ import os\n import logging\n \n from scrapy.utils.job import job_dir\n-from scrapy.utils.request import request_fingerprint\n-\n+from scrapy.utils.request import referer_str, request_fingerprint\n \n class BaseDupeFilter(object):\n \n@@ -61,8 +60,9 @@ class RFPDupeFilter(BaseDupeFilter):\n \n     def log(self, request, spider):\n         if self.debug:\n-            msg = \"Filtered duplicate request: %(request)s\"\n-            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n+            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n+            args = {'request': request, 'referer': referer_str(request) }\n+            self.logger.debug(msg, args, extra={'spider': spider})\n         elif self.logdupes:\n             msg = (\"Filtered duplicate request: %(request)s\"\n                    \" - no more duplicates will be shown\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8eade7d8640e112faf8677f4666bbe3ab10c7234", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 57 | Churn Cumulative: 174 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import hashlib\n import tempfile\n import unittest\n import shutil\n+from testfixtures import LogCapture\n \n from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n@@ -9,7 +10,7 @@ from scrapy.core.scheduler import Scheduler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.job import job_dir\n from scrapy.utils.test import get_crawler\n-\n+from tests.spiders import SimpleSpider\n \n class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n \n@@ -126,3 +127,57 @@ class RFPDupeFilterTest(unittest.TestCase):\n         assert case_insensitive_dupefilter.request_seen(r2)\n \n         case_insensitive_dupefilter.close('finished')\n+\n+    def test_log(self):\n+        with LogCapture() as l:\n+            settings = {'DUPEFILTER_DEBUG': False,\n+                        'DUPEFILTER_CLASS': __name__  + '.FromCrawlerRFPDupeFilter'}\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            scheduler = Scheduler.from_crawler(crawler)\n+            spider = SimpleSpider.from_crawler(crawler)\n+\n+            dupefilter = scheduler.df\n+            dupefilter.open()\n+\n+            r1 = Request('http://scrapytest.org/index.html')\n+            r2 = Request('http://scrapytest.org/index.html')\n+            \n+            dupefilter.log(r1, spider)\n+            dupefilter.log(r2, spider)\n+\n+            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            l.check_present(('scrapy.dupefilters', 'DEBUG', \n+                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n+                ' - no more duplicates will be shown'\n+                ' (see DUPEFILTER_DEBUG to show all duplicates)')))\n+\n+            dupefilter.close('finished')\n+\n+    def test_log_debug(self):\n+        with LogCapture() as l:\n+            settings = {'DUPEFILTER_DEBUG': True,\n+                        'DUPEFILTER_CLASS': __name__  + '.FromCrawlerRFPDupeFilter'}\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            scheduler = Scheduler.from_crawler(crawler)\n+            spider = SimpleSpider.from_crawler(crawler)\n+\n+            dupefilter = scheduler.df\n+            dupefilter.open()\n+\n+            r1 = Request('http://scrapytest.org/index.html')\n+            r2 = Request('http://scrapytest.org/index.html',\n+                headers={'Referer': 'http://scrapytest.org/INDEX.html'}\n+            )\n+            \n+            dupefilter.log(r1, spider)\n+            dupefilter.log(r2, spider)\n+\n+            assert crawler.stats.get_value('dupefilter/filtered') == 2\n+            l.check_present(('scrapy.dupefilters', 'DEBUG',\n+                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n+                ' (referer: None)')))\n+            l.check_present(('scrapy.dupefilters', 'DEBUG',\n+                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n+                ' (referer: http://scrapytest.org/INDEX.html)')))\n+\n+            dupefilter.close('finished')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dee841b8b904a3961481f7b1999d819d2c17c5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 21 | Churn Cumulative: 932 | Contributors (this commit): 23 | Commits (past 90d): 6 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import six\n-from six.moves.urllib.parse import urljoin, urlencode, urlsplit\n+from six.moves.urllib.parse import urljoin, urlencode, urlsplit, parse_qsl\n \n import lxml.html\n from parsel.selector import create_root_node\n@@ -37,21 +37,10 @@ class FormRequest(Request):\n                 formdata_key_list = []\n                 for k in querystr.split('&'):\n                     formdata_key_list.append(k.split('=')[0])\n-\n-                if url_split.query:\n-                    queries = (url_split.query).split('&')\n-                    query_dict = {}\n-                    for x in queries:\n-                        k = x.split('=')[0]\n-                        v = x.split('=')[1]\n-                        if formdata_key_list.count(k)==0:\n-                            query_dict[k] = v\n-                        query_str = ''\n-                        for k, v in query_dict.items():\n-                            query_str += (k + '=' + v + '&')\n-                    self._set_url(self.url[:self.url.index('?')+1] + query_str + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n-                else:\n-                    self._set_url(self.url + '?' + querystr + ('#' + url_split.fragment if url_split.fragment else ''))\n+                items = []\n+                items += [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_key_list]\n+                query_str = _urlencode(items, self.encoding)\n+                self._set_url(urljoin(self.url,'?'+ (query_str + '&' if query_str else '') + querystr + ('#'+ url_split.fragment if url_split.fragment else '')))\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#722a30ac2bc35bdc7abb52d10417ed1191a3aee0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1075 | Contributors (this commit): 16 | Commits (past 90d): 15 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -276,10 +276,10 @@ class FormRequestTest(RequestTest):\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n         #Both fragment and query in url\n-        data = (('a', '1'), ('b', '2'))\n-        url = self.request_class('http://www.example.com/?a=0&a=2&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n+        data = (('a', 'one'), ('a', 'two'), ('b', '2'))\n+        url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n-        self.assertEqual(fs[b'a'], [b'1'])\n+        self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n         self.assertEqual(fs[b'c'], [b'3'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9aa5391327dd34f8d840e7ce2bca1eb8583d932", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 99 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ from scrapy.http.request import Request\n class JSONRequest(Request):\n     def __init__(self, *args, **kwargs):\n         dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n-        dumps_kwargs['sort_keys'] = True\n+        dumps_kwargs.setdefault('sort_keys', True)\n         self._dumps_kwargs = dumps_kwargs\n \n         body_passed = kwargs.get('body', None) is not None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65d631329a1434ec013f24341e4b8520241aec70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 223 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'prefs': print_live_refs,\n             'hpy': hpy,\n             'help': \"This is Scrapy telnet console. For more info see: \"\n-                    \"https://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n+                    \"https://docs.scrapy.org/en/latest/topics/telnetconsole.html\",\n         }\n         self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n         return telnet_vars\n\n@@ -1,7 +1,7 @@\n \"\"\"\n tests: this package contains all Scrapy unittests\n \n-see https://doc.scrapy.org/en/latest/contributing.html#running-tests\n+see https://docs.scrapy.org/en/latest/contributing.html#running-tests\n \"\"\"\n \n import os\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a55c4269d3389df4e486657caa58985f8e37e5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 74 | Churn Cumulative: 941 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,6 +146,32 @@ class S3FeedStorage(BlockingFeedStorage):\n             key.close()\n \n \n+\n+class GCSFeedStorage(BlockingFeedStorage):\n+\n+    project_id = None\n+    bucket_name = None\n+    blob_name = None\n+\n+    def __init__(self, uri, project_id):\n+        self.project_id = project_id\n+        u = urlparse(uri)\n+        self.bucket_name = u.hostname\n+        self.blob_name = u.path[1:]  # remove first \"/\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, uri):\n+        return cls(uri, crawler.settings['GCS_PROJECT_ID'])\n+\n+    def _store_in_thread(self, file):\n+        file.seek(0)\n+        from google.cloud.storage import Client\n+        client = Client(project=self.project_id)\n+        bucket = client.get_bucket(self.bucket_name)\n+        blob = bucket.blob(self.blob_name)\n+        blob.upload_from_file(file)\n+\n+\n class FTPFeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n\n@@ -21,7 +21,7 @@ from w3lib.url import path_to_file_uri\n import scrapy\n from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n-    IFeedStorage, FileFeedStorage, FTPFeedStorage,\n+    IFeedStorage, FileFeedStorage, FTPFeedStorage, GCSFeedStorage,\n     S3FeedStorage, StdoutFeedStorage,\n     BlockingFeedStorage)\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n@@ -187,6 +187,52 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(content, expected_content)\n \n \n+class GCSFeedStorageTest(unittest.TestCase):\n+\n+    @mock.patch('scrapy.conf.settings',\n+                new={'GCS_PROJECT_ID': 'conf_id' }, create=True)\n+    def test_parse_settings(self):\n+        try:\n+            from google.cloud.storage import Client\n+        except ImportError:\n+            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+\n+        settings = {'GCS_PROJECT_ID': '123' }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = GCSFeedStorage.from_crawler(crawler, 'gcs://mybucket/export.csv')\n+        assert storage.project_id == '123'\n+        assert storage.bucket_name == 'mybucket'\n+        assert storage.blob_name == 'export.csv'\n+\n+    @defer.inlineCallbacks\n+    def test_store(self):\n+        try:\n+            from google.cloud.storage import Client, Bucket, Blob\n+        except ImportError:\n+            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+\n+        uri = 'gcs://mybucket/export.csv'\n+        project_id = 'myproject-123'\n+        with mock.patch('google.cloud.storage.Client') as m:\n+            client_mock = mock.create_autospec(Client)\n+            m.return_value = client_mock\n+\n+            bucket_mock = mock.create_autospec(Bucket)\n+            client_mock.get_bucket.return_value = bucket_mock\n+\n+            blob_mock = mock.create_autospec(Blob)\n+            bucket_mock.blob.return_value = blob_mock\n+\n+            f = mock.Mock()\n+            storage = GCSFeedStorage(uri, project_id)\n+            yield storage.store(f)\n+\n+            f.seek.assert_called_once_with(0)\n+            m.assert_called_once_with(project=project_id)\n+            client_mock.get_bucket.assert_called_once_with('mybucket')\n+            bucket_mock.blob.assert_called_once_with('export.csv')\n+            blob_mock.upload_from_file.assert_called_once_with(f)\n+\n class StdoutFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4059851e7b6c8d712b2bc73dbff99be2d569d21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 1072 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,7 @@ import os\n \n from importlib import import_module\n from twisted.trial.unittest import SkipTest\n+from tests import mock\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore\n@@ -91,3 +92,18 @@ def assert_samelines(testcase, text1, text2, msg=None):\n     line endings between platforms\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n+\n+def mock_google_cloud_storage():\n+    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n+    classes and set their proper return values.\n+    \"\"\"\n+    from google.cloud.storage import Client, Bucket, Blob\n+    client_mock = mock.create_autospec(Client)\n+\n+    bucket_mock = mock.create_autospec(Bucket)\n+    client_mock.get_bucket.return_value = bucket_mock\n+\n+    blob_mock = mock.create_autospec(Blob)\n+    bucket_mock.blob.return_value = blob_mock\n+\n+    return (client_mock, bucket_mock, blob_mock)\n\\ No newline at end of file\n\n@@ -24,7 +24,8 @@ from scrapy.extensions.feedexport import (\n     IFeedStorage, FileFeedStorage, FTPFeedStorage, GCSFeedStorage,\n     S3FeedStorage, StdoutFeedStorage,\n     BlockingFeedStorage)\n-from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n+from scrapy.utils.test import (assert_aws_environ, get_s3_content_and_delete,\n+    get_crawler, mock_google_cloud_storage)\n from scrapy.utils.python import to_native_str\n \n \n@@ -207,22 +208,16 @@ class GCSFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store(self):\n         try:\n-            from google.cloud.storage import Client, Bucket, Blob\n+            from google.cloud.storage import Client\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n         uri = 'gcs://mybucket/export.csv'\n         project_id = 'myproject-123'\n+        (client_mock, bucket_mock, blob_mock) = mock_google_cloud_storage()\n         with mock.patch('google.cloud.storage.Client') as m:    \n-            client_mock = mock.create_autospec(Client)\n             m.return_value = client_mock\n \n-            bucket_mock = mock.create_autospec(Bucket)\n-            client_mock.get_bucket.return_value = bucket_mock\n-\n-            blob_mock = mock.create_autospec(Blob)\n-            bucket_mock.blob.return_value = blob_mock\n-\n             f = mock.Mock()\n             storage = GCSFeedStorage(uri, project_id)\n             yield storage.store(f)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
