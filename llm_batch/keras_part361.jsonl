{"custom_id": "keras#a74b7478888d347c7b9732724de433f55904c292", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 104 | Churn Cumulative: 491 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -50,6 +50,110 @@ keras_export(v1=['keras.initializers.glorot_normal'], allow_multiple_exports=Tru\n                   'keras.initializers.random_normal',\n                   'keras.initializers.normal'])\n class RandomNormal(tf.compat.v1.random_normal_initializer):\n+  \"\"\"Initializer that generates a normal distribution.\n+\n+  Args:\n+    mean: a python scalar or a scalar tensor. Mean of the random values to\n+      generate.\n+    stddev: a python scalar or a scalar tensor. Standard deviation of the random\n+      values to generate.\n+    seed: A Python integer. Used to create random seeds. See\n+      `tf.compat.v1.set_random_seed` for behavior.\n+    dtype: Default data type, used if no `dtype` argument is provided when\n+      calling the initializer. Only floating point types are supported.\n+\n+  @compatibility(TF2)\n+  Although it is a legacy compat.v1 api,\n+  `tf.compat.v1.keras.initializers.RandomNormal` is compatible with eager\n+  execution and `tf.function`.\n+\n+  To switch to native TF2, switch to using\n+  `tf.keras.initializers.RandomNormal` (not from `compat.v1`) and\n+  if you need to change the default dtype use\n+  `tf.keras.backend.set_floatx(float_dtype)`\n+  or pass the dtype when calling the initializer, rather than passing it\n+  when constructing the initializer.\n+\n+  Random seed behavior:\n+  Also be aware that if you pass a seed to the TF2 initializer\n+  API it will reuse that same seed for every single initialization\n+  (unlike the TF1 intializer)\n+\n+  #### Structural Mapping to Native TF2\n+\n+  Before:\n+\n+  ```python\n+  initializer = tf.compat.v1.keras.initializers.RandomNormal(\n+    mean=mean,\n+    stddev=stddev,\n+    seed=seed,\n+    dtype=dtype)\n+\n+  weight_one = tf.Variable(initializer(shape_one))\n+  weight_two = tf.Variable(initializer(shape_two))\n+  ```\n+\n+  After:\n+\n+  ```python\n+  initializer = tf.keras.initializers.RandomNormal(\n+    mean=mean,\n+    # seed=seed,  # Setting a seed in the native TF2 API\n+                  # causes it to produce the same initializations\n+                  # across multiple calls of the same initializer.\n+    stddev=stddev)\n+\n+  weight_one = tf.Variable(initializer(shape_one, dtype=dtype))\n+  weight_two = tf.Variable(initializer(shape_two, dtype=dtype))\n+  ```\n+\n+  #### How to Map Arguments\n+\n+  | TF1 Arg Name      | TF2 Arg Name    | Note                       |\n+  | :---------------- | :-------------- | :------------------------- |\n+  | `mean`            | `mean`          | No change to defaults |\n+  | `stddev`          | `stddev`        | No change to defaults |\n+  | `seed`            | `seed`          | Different random number generation |\n+  :                   :        : semantics (to change in a :\n+  :                   :        : future version). If set, the TF2 version :\n+  :                   :        : will use stateless random number :\n+  :                   :        : generation which will produce the exact :\n+  :                   :        : same initialization even across multiple :\n+  :                   :        : calls of the initializer instance. the :\n+  :                   :        : `compat.v1` version will generate new :\n+  :                   :        : initializations each time. Do not set :\n+  :                   :        : a seed if you need different          :\n+  :                   :        : initializations each time. Instead    :\n+  :                   :        : either set a global tf seed with      :\n+  :                   :        : `tf.random.set_seed` if you need      :\n+  :                   :        : determinism, or initialize each weight:\n+  :                   :        : with a separate initializer instance  :\n+  :                   :        : and a different seed.                 :\n+  | `dtype`           | `dtype`  | The TF2 native api only takes it    |\n+  :                   :      : as a `__call__` arg, not a constructor arg. :\n+  | `partition_info`  | -    |  (`__call__` arg in TF1) Not supported      |\n+\n+  #### Example of fixed-seed behavior differences\n+\n+  `compat.v1` Fixed seed behavior:\n+\n+  >>> initializer = tf.compat.v1.keras.initializers.TruncatedNormal(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n+\n+  After:\n+\n+  >>> initializer = tf.keras.initializers.TruncatedNormal(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+\n+  @end_compatibility\n+  \"\"\"\n \n   def __init__(self, mean=0.0, stddev=0.05, seed=None, dtype=tf.float32):\n     super(RandomNormal, self).__init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#edebe0f3b44714b30b8d3390da4fafa4df878651", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 28 | Files Changed: 5 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 62 | Churn Cumulative: 4642 | Contributors (this commit): 7 | Commits (past 90d): 20 | Contributors (cumulative): 19 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -99,12 +99,12 @@ class FalsePositivesTest(tf.test.TestCase, parameterized.TestCase):\n   def test_threshold_limit(self):\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Threshold values must be in \\[0, 1\\]. Invalid values: \\[-1, 2\\]'):\n+        r'Threshold values must be in \\[0, 1\\]. Received: \\[-1, 2\\]'):\n       metrics.FalsePositives(thresholds=[-1, 0.5, 2])\n \n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Threshold values must be in \\[0, 1\\]. Invalid values: \\[None\\]'):\n+        r'Threshold values must be in \\[0, 1\\]. Received: \\[None\\]'):\n       metrics.FalsePositives(thresholds=[None])\n \n \n@@ -1467,12 +1467,12 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_invalid_curve(self):\n     with self.assertRaisesRegex(ValueError,\n-                                'Invalid AUC curve value \"Invalid\".'):\n+                                'Invalid AUC curve value: \"Invalid\".'):\n       metrics.AUC(curve='Invalid')\n \n   def test_invalid_summation_method(self):\n     with self.assertRaisesRegex(\n-        ValueError, 'Invalid AUC summation method value \"Invalid\".'):\n+        ValueError, 'Invalid AUC summation method value: \"Invalid\".'):\n       metrics.AUC(summation_method='Invalid')\n \n   def test_extra_dims(self):\n\n@@ -129,4 +129,4 @@ def constant_value(pred):  # pylint: disable=invalid-name\n   if isinstance(pred, tf.Variable):\n     return None\n   raise TypeError(\"`pred` must be a Tensor, or a Python bool, or 1 or 0. \"\n-                  \"Found instead: %s\" % type(pred))\n+                  f\"Received: {type(pred)}\")\n\n@@ -147,7 +147,7 @@ def result_wrapper(result_fn):\n             raise RuntimeError(\n                 'The output of `metric.result()` can only be a single '\n                 'Tensor/Variable, or a dict of Tensors/Variables. '\n-                'For metric %s, got result %s.' % (metric_obj.name, raw_result))\n+                f'For metric {metric_obj.name}, got result {raw_result}.')\n     else:\n       # TODO(psv): Test distribution of metrics using different distribution\n       # strategies.\n@@ -199,8 +199,7 @@ def assert_thresholds_range(thresholds):\n     invalid_thresholds = [t for t in thresholds if t is None or t < 0 or t > 1]\n     if invalid_thresholds:\n       raise ValueError(\n-          'Threshold values must be in [0, 1]. Invalid values: {}'.format(\n-              invalid_thresholds))\n+          f'Threshold values must be in [0, 1]. Received: {invalid_thresholds}')\n \n \n def parse_init_thresholds(thresholds, default_threshold=0.5):\n@@ -229,7 +228,9 @@ class AUCCurve(Enum):\n     elif key in ('roc', 'ROC'):\n       return AUCCurve.ROC\n     else:\n-      raise ValueError('Invalid AUC curve value \"%s\".' % key)\n+      raise ValueError(\n+          f'Invalid AUC curve value: \"{key}\". '\n+          'Expected values are [\"PR\", \"ROC\"]')\n \n \n class AUCSummationMethod(Enum):\n@@ -259,7 +260,9 @@ class AUCSummationMethod(Enum):\n     elif key in ('minoring', 'Minoring'):\n       return AUCSummationMethod.MINORING\n     else:\n-      raise ValueError('Invalid AUC summation method value \"%s\".' % key)\n+      raise ValueError(\n+          f'Invalid AUC summation method value: \"{key}\". '\n+          'Expected values are [\"interpolation\", \"majoring\", \"minoring\"]')\n \n \n def _update_confusion_matrix_variables_optimized(\n@@ -561,9 +564,8 @@ def update_confusion_matrix_variables(variables_to_update,\n       key for key in variables_to_update if key in list(ConfusionMatrix)):\n     raise ValueError(\n         'Please provide at least one valid confusion matrix '\n-        'variable to update. Valid variable key options are: \"{}\". '\n-        'Received: \"{}\"'.format(\n-            list(ConfusionMatrix), variables_to_update.keys()))\n+        'variable to update. Valid variable key options are: '\n+        f'\"{list(ConfusionMatrix)}\". Received: \"{variables_to_update.keys()}\"')\n \n   variable_dtype = list(variables_to_update.values())[0].dtype\n \n@@ -599,8 +601,8 @@ def update_confusion_matrix_variables(variables_to_update,\n   ]\n   if invalid_keys:\n     raise ValueError(\n-        'Invalid keys: {}. Valid variable key options are: \"{}\"'.format(\n-            invalid_keys, list(ConfusionMatrix)))\n+        f'Invalid keys: \"{invalid_keys}\". '\n+        f'Valid variable key options are: \"{list(ConfusionMatrix)}\"')\n \n   with tf.control_dependencies([\n       tf.compat.v1.assert_greater_equal(\n@@ -806,10 +808,12 @@ def ragged_assert_compatible_and_get_flat_values(values, mask=None):\n     values = flat_values[0] if to_be_stripped else flat_values\n \n   elif is_any_ragged:\n-    raise TypeError('One of the inputs does not have acceptable types.')\n+    raise TypeError('Some of the inputs are not tf.RaggedTensor. '\n+                    f'Input received: {values}')\n   # values are empty or value are not ragged and mask is ragged.\n   elif isinstance(mask, tf.RaggedTensor):\n-    raise TypeError('Ragged mask is not allowed with non-ragged inputs.')\n+    raise TypeError('Ragged mask is not allowed with non-ragged inputs. '\n+                    f'Input received: {values}, mask received: {mask}')\n \n   return values, mask\n \n@@ -831,7 +835,8 @@ def _assert_splits_match(nested_splits_lists):\n   Raises:\n     ValueError: If the splits are not identical.\n   \"\"\"\n-  error_msg = 'Inputs must have identical ragged splits'\n+  error_msg = ('Inputs must have identical ragged splits. '\n+               f'Input received: {nested_splits_lists}')\n   for splits_list in nested_splits_lists:\n     if len(splits_list) != len(nested_splits_lists[0]):\n       raise ValueError(error_msg)\n\n@@ -73,7 +73,8 @@ def get_reachable_from_inputs(inputs, targets=None):\n     elif tf.is_tensor(x):\n       outputs = x.consumers()\n     else:\n-      raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\n+      raise TypeError(\n+          f'Expected tf.Operation, tf.Variable, or tf.Tensor. Received: {x}')\n \n     for y in outputs:\n       if y not in reachable:\n@@ -112,7 +113,7 @@ def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n   # Recursively convert.\n   if not tf.nest.is_nested(nested):\n     raise ValueError(\n-        'Received non-atomic and non-sequence element: {}'.format(nested))\n+        f'Received non-atomic and non-sequence element: {nested}')\n   if tf.__internal__.nest.is_mapping(nested):\n     values = [nested[k] for k in sorted(nested.keys())]\n   elif tf.__internal__.nest.is_attrs(nested):\n@@ -406,11 +407,12 @@ def assert_no_legacy_layers(layers):\n   if legacy_layers:\n     layer_str = '\\n'.join('  ' + str(l) for l in legacy_layers)\n     raise TypeError(\n-        'The following are legacy tf.layers.Layers:\\n{}\\nTo use keras as a '\n+        f'The following are legacy tf.layers.Layers:\\n{layer_str}\\n'\n+        'To use keras as a '\n         'framework (for instance using the Network, Model, or Sequential '\n         'classes), please use the tf.keras.layers implementation instead. '\n         '(Or, if writing custom layers, subclass from tf.keras.layers rather '\n-        'than tf.layers)'.format(layer_str))\n+        'than tf.layers)')\n \n \n @tf_contextlib.contextmanager\n@@ -521,7 +523,7 @@ def _astuple(attrs):\n   cls = type(attrs)\n   fields = getattr(cls, '__attrs_attrs__', None)\n   if fields is None:\n-    raise ValueError('%r is not an attrs-decorated class.' % cls)\n+    raise ValueError(f'{cls} is not an attrs-decorated class.')\n   values = []\n   for field in fields:\n     values.append(getattr(attrs, field.name))\n\n@@ -121,11 +121,10 @@ def swap_class(cls, v2_cls, v1_cls, use_v2):\n def disallow_legacy_graph(cls_name, method_name):\n   if not tf.compat.v1.executing_eagerly_outside_functions():\n     error_msg = (\n-        \"Calling `{cls_name}.{method_name}` in graph mode is not supported \"\n-        \"when the `{cls_name}` instance was constructed with eager mode \"\n-        \"enabled. Please construct your `{cls_name}` instance in graph mode or\"\n-        \" call `{cls_name}.{method_name}` with eager mode enabled.\")\n-    error_msg = error_msg.format(cls_name=cls_name, method_name=method_name)\n+        f\"Calling `{cls_name}.{method_name}` in graph mode is not supported \"\n+        f\"when the `{cls_name}` instance was constructed with eager mode \"\n+        f\"enabled. Please construct your `{cls_name}` instance in graph mode or\"\n+        f\" call `{cls_name}.{method_name}` with eager mode enabled.\")\n     raise ValueError(error_msg)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a4074b3e9e1c4d7a7133ba1e5600008c7309a39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 45 | Files Changed: 5 | Hunks: 31 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 97 | Churn Cumulative: 8325 | Contributors (this commit): 58 | Commits (past 90d): 14 | Contributors (cumulative): 86 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -67,9 +67,9 @@ class LeakyReLU(Layer):\n   def __init__(self, alpha=0.3, **kwargs):\n     super(LeakyReLU, self).__init__(**kwargs)\n     if alpha is None:\n-      raise ValueError('The alpha value of a Leaky ReLU layer '\n-                       'cannot be None, needs a float. '\n-                       'Got %s' % alpha)\n+      raise ValueError(\n+          'The alpha value of a Leaky ReLU layer cannot be None, '\n+          f'Expecting a float. Received: {alpha}')\n     self.supports_masking = True\n     self.alpha = backend.cast_to_floatx(alpha)\n \n@@ -206,8 +206,9 @@ class ELU(Layer):\n   def __init__(self, alpha=1.0, **kwargs):\n     super(ELU, self).__init__(**kwargs)\n     if alpha is None:\n-      raise ValueError('Alpha of an ELU layer cannot be None, '\n-                       'requires a float. Got %s' % alpha)\n+      raise ValueError(\n+          'Alpha of an ELU layer cannot be None, expecting a float. '\n+          f'Received: {alpha}')\n     self.supports_masking = True\n     self.alpha = backend.cast_to_floatx(alpha)\n \n@@ -250,11 +251,12 @@ class ThresholdedReLU(Layer):\n   def __init__(self, theta=1.0, **kwargs):\n     super(ThresholdedReLU, self).__init__(**kwargs)\n     if theta is None:\n-      raise ValueError('Theta of a Thresholded ReLU layer cannot be '\n-                       'None, requires a float. Got %s' % theta)\n+      raise ValueError(\n+          'Theta of a Thresholded ReLU layer cannot be None, expecting a float.'\n+          f' Received: {theta}')\n     if theta < 0:\n       raise ValueError('The theta value of a Thresholded ReLU layer '\n-                       'should be >=0, got %s' % theta)\n+                       f'should be >=0. Received: {theta}')\n     self.supports_masking = True\n     self.theta = backend.cast_to_floatx(theta)\n \n@@ -410,13 +412,13 @@ class ReLU(Layer):\n     super(ReLU, self).__init__(**kwargs)\n     if max_value is not None and max_value < 0.:\n       raise ValueError('max_value of a ReLU layer cannot be a negative '\n-                       'value. Got: %s' % max_value)\n+                       f'value. Received: {max_value}')\n     if negative_slope is None or negative_slope < 0.:\n       raise ValueError('negative_slope of a ReLU layer cannot be a negative '\n-                       'value. Got: %s' % negative_slope)\n+                       f'value. Received: {negative_slope}')\n     if threshold is None or threshold < 0.:\n       raise ValueError('threshold of a ReLU layer cannot be a negative '\n-                       'value. Got: %s' % threshold)\n+                       f'value. Received: {threshold}')\n \n     self.supports_masking = True\n     if max_value is not None:\n\n@@ -81,7 +81,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n   def test_relu_with_invalid_max_value(self):\n     with self.assertRaisesRegex(\n         ValueError, 'max_value of a ReLU layer cannot be a negative '\n-        'value. Got: -10'):\n+        'value. Received: -10'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'max_value': -10},\n@@ -91,7 +91,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n   def test_relu_with_invalid_negative_slope(self):\n     with self.assertRaisesRegex(\n         ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n-        'value. Got: None'):\n+        'value. Received: None'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'negative_slope': None},\n@@ -100,7 +100,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(\n         ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n-        'value. Got: -10'):\n+        'value. Received: -10'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'negative_slope': -10},\n@@ -110,7 +110,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n   def test_relu_with_invalid_threshold(self):\n     with self.assertRaisesRegex(\n         ValueError, 'threshold of a ReLU layer cannot be a negative '\n-        'value. Got: None'):\n+        'value. Received: None'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'threshold': None},\n@@ -119,7 +119,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(\n         ValueError, 'threshold of a ReLU layer cannot be a negative '\n-        'value. Got: -10'):\n+        'value. Received: -10'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'threshold': -10},\n@@ -140,7 +140,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n     # Test case for GitHub issue 46993.\n     with self.assertRaisesRegex(\n         ValueError, 'The alpha value of a Leaky ReLU layer '\n-        'cannot be None, needs a float. Got None'):\n+        'cannot be None. Expecting a float. Received: None'):\n       testing_utils.layer_test(\n           keras.layers.LeakyReLU,\n           kwargs={'alpha': None},\n@@ -151,7 +151,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n     # Test case for GitHub issue 46993.\n     with self.assertRaisesRegex(\n         ValueError, 'Alpha of an ELU layer cannot be None, '\n-        'requires a float. Got None'):\n+        'expecting a float. Received: None'):\n       testing_utils.layer_test(\n           keras.layers.ELU,\n           kwargs={'alpha': None},\n@@ -161,7 +161,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n   def test_threshold_relu_with_invalid_theta(self):\n     with self.assertRaisesRegex(\n         ValueError, 'Theta of a Thresholded ReLU layer cannot '\n-        'be None, requires a float. Got None'):\n+        'be None, expecting a float. Received: None'):\n       testing_utils.layer_test(\n           keras.layers.ThresholdedReLU,\n           kwargs={'theta': None},\n@@ -170,7 +170,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(\n         ValueError, 'The theta value of a Thresholded ReLU '\n-        'layer should be >=0, got -10'):\n+        'layer should be >=0. Received: -10'):\n       testing_utils.layer_test(\n           keras.layers.ThresholdedReLU,\n           kwargs={'theta': -10},\n\n@@ -119,9 +119,9 @@ class Embedding(Layer):\n       else:\n         kwargs['input_shape'] = (None,)\n     if input_dim <= 0 or output_dim <= 0:\n-      raise ValueError('Both `input_dim` and `output_dim` should be positive, '\n-                       'found input_dim {} and output_dim {}'.format(\n-                           input_dim, output_dim))\n+      raise ValueError(\n+          'Both `input_dim` and `output_dim` should be positive, '\n+          f'Received input_dim = {input_dim} and output_dim = {output_dim}')\n     if (not base_layer_utils.v2_dtype_behavior_enabled() and\n         'dtype' not in kwargs):\n       # In TF1, the dtype defaults to the input dtype which is typically int32,\n@@ -171,15 +171,15 @@ class Embedding(Layer):\n       else:\n         in_lens = [self.input_length]\n       if len(in_lens) != len(input_shape) - 1:\n-        raise ValueError('\"input_length\" is %s, '\n-                         'but received input has shape %s' % (str(\n-                             self.input_length), str(input_shape)))\n+        raise ValueError(\n+            f'\"input_length\" is {self.input_length}, but received input has '\n+            f'shape {input_shape}')\n       else:\n         for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n           if s1 is not None and s2 is not None and s1 != s2:\n-            raise ValueError('\"input_length\" is %s, '\n-                             'but received input has shape %s' % (str(\n-                                 self.input_length), str(input_shape)))\n+            raise ValueError(\n+                f'\"input_length\" is {self.input_length}, but received input '\n+                f'has shape {input_shape}')\n           elif s1 is None:\n             in_lens[i] = s2\n       return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n\n@@ -119,8 +119,7 @@ class TimeDistributed(Wrapper):\n     if not isinstance(layer, Layer):\n       raise ValueError(\n           'Please initialize `TimeDistributed` layer with a '\n-          '`tf.keras.layers.Layer` instance. You passed: {input}'.format(\n-              input=layer))\n+          f'`tf.keras.layers.Layer` instance. Received: {layer}')\n     super(TimeDistributed, self).__init__(layer, **kwargs)\n     self.supports_masking = True\n \n@@ -174,7 +173,7 @@ class TimeDistributed(Wrapper):\n     if any(dim < 3 for dim in input_dims):\n       raise ValueError(\n           '`TimeDistributed` Layer should be passed an `input_shape ` '\n-          'with at least 3 dimensions, received: ' + str(input_shape))\n+          f'with at least 3 dimensions, received: {input_shape}')\n     # Don't enforce the batch or time dimension.\n     self.input_spec = tf.nest.map_structure(\n         lambda x: InputSpec(shape=[None, None] + x.as_list()[2:]), input_shape)\n@@ -246,7 +245,7 @@ class TimeDistributed(Wrapper):\n                                   input_row_lenghts)\n       elif any(is_ragged_input):\n         raise ValueError('All inputs has to be either ragged or not, '\n-                         'but not mixed. You passed: {}'.format(inputs))\n+                         f'but not mixed. Received: {inputs}')\n       else:\n         input_length = tf_utils.convert_shapes(input_shape)\n         input_length = tf.nest.flatten(input_length)[1]\n@@ -453,12 +452,13 @@ class Bidirectional(Wrapper):\n     if not isinstance(layer, Layer):\n       raise ValueError(\n           'Please initialize `Bidirectional` layer with a '\n-          '`Layer` instance. You passed: {input}'.format(input=layer))\n+          f'`tf.keras.layers.Layer` instance. Received: {layer}')\n     if backward_layer is not None and not isinstance(backward_layer, Layer):\n-      raise ValueError('`backward_layer` need to be a `Layer` instance. '\n-                       'You passed: {input}'.format(input=backward_layer))\n+      raise ValueError(\n+          '`backward_layer` need to be a `tf.keras.layers.Layer` instance. '\n+          f'Received: {backward_layer}')\n     if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n-      raise ValueError('Invalid merge mode. '\n+      raise ValueError(f'Invalid merge mode. Received: {merge_mode}. '\n                        'Merge mode should be one of '\n                        '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n     # We don't want to track `layer` since we're already tracking the two copies\n@@ -511,8 +511,11 @@ class Bidirectional(Wrapper):\n   def _verify_layer_config(self):\n     \"\"\"Ensure the forward and backward layers have valid common property.\"\"\"\n     if self.forward_layer.go_backwards == self.backward_layer.go_backwards:\n-      raise ValueError('Forward layer and backward layer should have different '\n-                       '`go_backwards` value.')\n+      raise ValueError(\n+          'Forward layer and backward layer should have different '\n+          '`go_backwards` value.'\n+          f'forward_layer.go_backwards = {self.forward_layer.go_backwards},'\n+          f'backward_layer.go_backwards = {self.backward_layer.go_backwards}')\n \n     common_attributes = ('stateful', 'return_sequences', 'return_state')\n     for a in common_attributes:\n@@ -521,8 +524,8 @@ class Bidirectional(Wrapper):\n       if forward_value != backward_value:\n         raise ValueError(\n             'Forward layer and backward layer are expected to have the same '\n-            'value for attribute {attr}, got {forward} and {backward}'.format(\n-                attr=a, forward=forward_value, backward=backward_value))\n+            f'value for attribute \"{a}\", got \"{forward_value}\" for forward '\n+            f'layer and \"{backward_value}\" for backward layer')\n \n   def _recreate_layer_from_config(self, layer, go_backwards=False):\n     # When recreating the layer from its config, it is possible that the layer\n@@ -593,7 +596,7 @@ class Bidirectional(Wrapper):\n             'When passing `initial_state` to a Bidirectional RNN, '\n             'the state should be a list containing the states of '\n             'the underlying RNNs. '\n-            'Found: ' + str(initial_state))\n+            f'Received: {initial_state}')\n \n       kwargs['initial_state'] = initial_state\n       additional_inputs += initial_state\n@@ -723,7 +726,8 @@ class Bidirectional(Wrapper):\n       output = [y, y_rev]\n     else:\n       raise ValueError(\n-          'Unrecognized value for `merge_mode`: %s' % (self.merge_mode))\n+          f'Unrecognized value for `merge_mode`. Received: {self.merge_mode}'\n+          'Expected values are [\"concat\", \"sum\", \"ave\", \"mul\"]')\n \n     if self.return_state:\n       if self.merge_mode is None:\n\n@@ -561,7 +561,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     x = tf.constant(np.zeros((1, 1)).astype('float32'))\n     with self.assertRaisesRegex(\n         ValueError,\n-        'Please initialize `Bidirectional` layer with a `Layer` instance.'):\n+        'Please initialize `Bidirectional` layer with a '\n+        '`tf.keras.layers.Layer` instance.'):\n       keras.layers.Bidirectional(x)\n \n   def test_bidirectional_weight_loading(self):\n@@ -1136,7 +1137,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n       kwargs = {attr: True}\n       backward_layer = rnn(units, go_backwards=True, **kwargs)\n       with self.assertRaisesRegex(\n-          ValueError, 'expected to have the same value for attribute ' + attr):\n+          ValueError, 'expected to have the same value for attribute \"' + attr):\n         keras.layers.Bidirectional(\n             forward_layer, merge_mode='concat', backward_layer=backward_layer)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdadb494d5a22e3e6d9caf82e4e337e6c7d7aada", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1011 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,8 +82,10 @@ def get_layer_index_bound_by_layer_name(model, layer_names):\n       lower_index.append(idx)\n     if re.match(layer_names[1], layer.name):\n       upper_index.append(idx)\n-  if len(lower_index) == 0 or len(upper_index) == 0:\n-    raise ValueError('Passed layer_range does not match to model layers')\n+  if not lower_index or not upper_index:\n+    raise ValueError(\n+        'Passed layer_names does not match to layers in the model. '\n+        f'Recieved: {layer_names}')\n   if min(lower_index) > max(upper_index):\n     return [min(upper_index), max(lower_index)]\n   return [min(lower_index), max(upper_index)]\n@@ -162,14 +164,18 @@ def model_to_dot(model,\n \n   if layer_range:\n     if len(layer_range) != 2:\n-      raise ValueError('layer_range must be of shape (2,)')\n+      raise ValueError(\n+          'layer_range must be of shape (2,). Received: '\n+          f'layer_range = {layer_range} of length {len(layer_range)}')\n     if (not isinstance(layer_range[0], str) or\n         not isinstance(layer_range[1], str)):\n-      raise ValueError('layer_range should contain string type only')\n+      raise ValueError(\n+          'layer_range should contain string type only. '\n+          f'Received: {layer_range}')\n     layer_range = get_layer_index_bound_by_layer_name(model, layer_range)\n     if layer_range[0] < 0 or layer_range[1] > len(model.layers):\n-      raise ValueError('Both values in layer_range should be in',\n-                       'range (%d, %d)' % (0, len(model.layers)))\n+      raise ValueError('Both values in layer_range should be in range (0, '\n+                       f'{len(model.layers)}. Recieved: {layer_range}')\n \n   sub_n_first_node = {}\n   sub_n_last_node = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95b1aceaa2d23cb0259807a4561f12dc1d53b9ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 41 | Files Changed: 7 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 92 | Churn Cumulative: 8317 | Contributors (this commit): 44 | Commits (past 90d): 21 | Contributors (cumulative): 66 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -268,13 +268,13 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(self.evaluate(loss), 16, 1e-2)\n \n   def test_invalid_reduction(self):\n-    with self.assertRaisesRegex(ValueError, 'Invalid Reduction Key Foo.'):\n+    with self.assertRaisesRegex(ValueError, 'Invalid Reduction Key: Foo.'):\n       losses.MeanSquaredError(reduction='Foo')\n \n     mse_obj = losses.MeanSquaredError()\n     y = tf.constant([1])\n     mse_obj.reduction = 'Bar'\n-    with self.assertRaisesRegex(ValueError, 'Invalid Reduction Key Bar.'):\n+    with self.assertRaisesRegex(ValueError, 'Invalid Reduction Key: Bar.'):\n       mse_obj(y, y)\n \n   def test_deserialization_error(self):\n\n@@ -31,7 +31,8 @@ def convert_data_format(data_format, ndim):\n     elif ndim == 5:\n       return 'NDHWC'\n     else:\n-      raise ValueError('Input rank not supported:', ndim)\n+      raise ValueError(\n+          f'Input rank not supported: {ndim}. Expected values are [3, 4, 5]')\n   elif data_format == 'channels_first':\n     if ndim == 3:\n       return 'NCW'\n@@ -40,9 +41,12 @@ def convert_data_format(data_format, ndim):\n     elif ndim == 5:\n       return 'NCDHW'\n     else:\n-      raise ValueError('Input rank not supported:', ndim)\n+      raise ValueError(\n+          f'Input rank not supported: {ndim}. Expected values are [3, 4, 5]')\n   else:\n-    raise ValueError('Invalid data_format:', data_format)\n+    raise ValueError(\n+        f'Invalid data_format: {data_format}. '\n+        'Expected values are [\"channels_first\", \"channels_last\"]')\n \n \n def normalize_tuple(value, n, name):\n@@ -65,22 +69,22 @@ def normalize_tuple(value, n, name):\n   if isinstance(value, int):\n     return (value,) * n\n   else:\n+    error_msg = (f'The `{name}` argument must be a tuple of {n} '\n+                 f'integers. Received: {value}')\n+\n     try:\n       value_tuple = tuple(value)\n     except TypeError:\n-      raise ValueError('The `' + name + '` argument must be a tuple of ' +\n-                       str(n) + ' integers. Received: ' + str(value))\n+      raise ValueError(error_msg)\n     if len(value_tuple) != n:\n-      raise ValueError('The `' + name + '` argument must be a tuple of ' +\n-                       str(n) + ' integers. Received: ' + str(value))\n+      raise ValueError(error_msg)\n     for single_value in value_tuple:\n       try:\n         int(single_value)\n       except (ValueError, TypeError):\n-        raise ValueError('The `' + name + '` argument must be a tuple of ' +\n-                         str(n) + ' integers. Received: ' + str(value) + ' '\n-                         'including element ' + str(single_value) + ' of type' +\n-                         ' ' + str(type(single_value)))\n+        error_msg += (f'including element {single_value} of '\n+                      f'type {type(single_value)}')\n+        raise ValueError(error_msg)\n     return value_tuple\n \n \n@@ -189,8 +193,7 @@ def normalize_data_format(value):\n   data_format = value.lower()\n   if data_format not in {'channels_first', 'channels_last'}:\n     raise ValueError('The `data_format` argument must be one of '\n-                     '\"channels_first\", \"channels_last\". Received: ' +\n-                     str(value))\n+                     f'\"channels_first\", \"channels_last\". Received: {value}')\n   return data_format\n \n \n@@ -201,7 +204,7 @@ def normalize_padding(value):\n   if padding not in {'valid', 'same', 'causal'}:\n     raise ValueError('The `padding` argument must be a list/tuple or one of '\n                      '\"valid\", \"same\" (or \"causal\", only for `Conv1D). '\n-                     'Received: ' + str(padding))\n+                     f'Received: {padding}')\n   return padding\n \n \n@@ -252,9 +255,8 @@ def conv_kernel_mask(input_shape, kernel_shape, strides, padding):\n     NotImplementedError: if `padding` is not in {`\"same\"`, `\"valid\"`}.\n   \"\"\"\n   if padding not in {'same', 'valid'}:\n-    raise NotImplementedError('Padding type %s not supported. '\n-                              'Only \"valid\" and \"same\" '\n-                              'are implemented.' % padding)\n+    raise NotImplementedError(f'Padding type {padding} not supported. '\n+                              'Only \"valid\" and \"same\" are implemented.')\n \n   in_dims = len(input_shape)\n   if isinstance(kernel_shape, int):\n@@ -266,8 +268,8 @@ def conv_kernel_mask(input_shape, kernel_shape, strides, padding):\n   stride_dims = len(strides)\n   if kernel_dims != in_dims or stride_dims != in_dims:\n     raise ValueError('Number of strides, input and kernel dimensions must all '\n-                     'match. Received: %d, %d, %d.' %\n-                     (stride_dims, in_dims, kernel_dims))\n+                     f'match. Received: stride_dims={stride_dims}, '\n+                     f'in_dims={in_dims}, kernel_dims={kernel_dims}')\n \n   output_shape = conv_output_shape(input_shape, kernel_shape, strides, padding)\n \n@@ -334,9 +336,8 @@ def conv_kernel_idxs(input_shape, kernel_shape, strides, padding, filters_in,\n       NotImplementedError: if `padding` is neither `\"same\"` nor `\"valid\"`.\n   \"\"\"\n   if padding not in ('same', 'valid'):\n-    raise NotImplementedError('Padding type %s not supported. '\n-                              'Only \"valid\" and \"same\" '\n-                              'are implemented.' % padding)\n+    raise NotImplementedError(f'Padding type {padding} not supported. '\n+                              'Only \"valid\" and \"same\" are implemented.')\n \n   in_dims = len(input_shape)\n   if isinstance(kernel_shape, int):\n@@ -348,8 +349,8 @@ def conv_kernel_idxs(input_shape, kernel_shape, strides, padding, filters_in,\n   stride_dims = len(strides)\n   if kernel_dims != in_dims or stride_dims != in_dims:\n     raise ValueError('Number of strides, input and kernel dimensions must all '\n-                     'match. Received: %d, %d, %d.' %\n-                     (stride_dims, in_dims, kernel_dims))\n+                     f'match. Received: stride_dims={stride_dims}, '\n+                     f'in_dims={in_dims}, kernel_dims={kernel_dims}')\n \n   output_shape = conv_output_shape(input_shape, kernel_shape, strides, padding)\n   output_axes_ticks = [range(dim) for dim in output_shape]\n@@ -359,9 +360,9 @@ def conv_kernel_idxs(input_shape, kernel_shape, strides, padding, filters_in,\n   elif data_format == 'channels_last':\n     concat_idxs = lambda spatial_idx, filter_idx: spatial_idx + (filter_idx,)\n   else:\n-    raise ValueError('Data format %s not recognized.'\n-                     '`data_format` must be \"channels_first\" or '\n-                     '\"channels_last\".' % data_format)\n+    raise ValueError(\n+        f'Data format `{data_format}` not recognized.'\n+        '`data_format` must be \"channels_first\" or \"channels_last\".')\n \n   for output_position in itertools.product(*output_axes_ticks):\n     input_axes_ticks = conv_connected_inputs(input_shape, kernel_shape,\n\n@@ -225,7 +225,9 @@ def get_file(fname=None,\n   if not fname:\n     fname = os.path.basename(urlsplit(origin).path)\n     if not fname:\n-      raise ValueError(\"Invalid origin '{}'\".format(origin))\n+      raise ValueError(\n+          f\"Can't parse the file name from the origin provided: '{origin}'.\"\n+          \"Please specify the `fname` as the input param.\")\n \n   if untar:\n     if fname.endswith('.tar.gz'):\n\n@@ -87,11 +87,14 @@ class DatasetCreator:\n \n   def __init__(self, dataset_fn, input_options=None):\n     if not callable(dataset_fn):\n-      raise TypeError('`dataset_fn` for `DatasetCreator` must be a `callable`.')\n+      raise TypeError(\n+          '`dataset_fn` for `DatasetCreator` must be a `callable`. '\n+          f'Received: {dataset_fn}')\n     if input_options and (not isinstance(input_options,\n                                          tf.distribute.InputOptions)):\n-      raise TypeError('`input_options` for `DatasetCreator` must be a '\n-                      '`tf.distribute.InputOptions`.')\n+      raise TypeError(\n+          '`input_options` for `DatasetCreator` must be a '\n+          f'`tf.distribute.InputOptions`. Received: {input_options}')\n \n     self.dataset_fn = dataset_fn\n     self.input_options = input_options\n@@ -101,6 +104,7 @@ class DatasetCreator:\n     # the callable.\n     dataset = self.dataset_fn(*args, **kwargs)\n     if not isinstance(dataset, tf.data.Dataset):\n-      raise TypeError('The `callable` provided to `DatasetCreator` must return '\n-                      'a Dataset.')\n+      raise TypeError(\n+          'The `callable` provided to `DatasetCreator` must return '\n+          f'a Dataset. It returns \"{dataset}\"')\n     return dataset\n\n@@ -21,8 +21,8 @@ def _to_matrix(u):\n   \"\"\"If input tensor is a vector (i.e., has rank 1), converts it to matrix.\"\"\"\n   u_rank = len(u.shape)\n   if u_rank not in [1, 2]:\n-    raise ValueError('The input tensor should have rank 1 or 2. Given rank: {}'\n-                     .format(u_rank))\n+    raise ValueError('The input tensor should have rank 1 or 2. '\n+                     f'Received rank: {u_rank}')\n   if u_rank == 1:\n     return tf.expand_dims(u, 0)\n   return u\n@@ -36,8 +36,8 @@ def _align_matrices(x, y):\n   y_shape = y_matrix.shape\n   if y_shape[1] != x_shape[1]:  # dimensions do not match.\n     raise ValueError(\n-        'The outermost dimensions of the input tensors should match. Given: {} '\n-        'vs {}.'.format(y_shape[1], x_shape[1]))\n+        'The outermost dimensions of the input tensors should match. '\n+        f'Received y = {y_shape[1]} vs x = {x_shape[1]}.')\n \n   x_tile = tf.tile(\n       tf.expand_dims(x_matrix, 1), [1, y_shape[0], 1])\n\n@@ -80,7 +80,8 @@ class ReductionV2:\n   @classmethod\n   def validate(cls, key):\n     if key not in cls.all():\n-      raise ValueError('Invalid Reduction Key %s.' % key)\n+      raise ValueError(\n+          f'Invalid Reduction Key: {key}. Expected keys are \"{cls.all()}\"')\n \n \n def remove_squeezable_dimensions(\n\n@@ -38,7 +38,9 @@ class _ObjectIdentityWrapper:\n \n   def _assert_type(self, other):\n     if not isinstance(other, _ObjectIdentityWrapper):\n-      raise TypeError(\"Cannot compare wrapped object with unwrapped object\")\n+      raise TypeError(\n+          \"Cannot compare wrapped object with unwrapped object. \"\n+          f\"Expect the object to be `_ObjectIdentityWrapper`. Got: {other}\")\n \n   def __lt__(self, other):\n     self._assert_type(other)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#961acd563851cf1cf90d20f08c30b3e392d068ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 9011 | Contributors (this commit): 64 | Commits (past 90d): 7 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -104,14 +104,14 @@ def smart_resize(x, size, interpolation='bilinear'):\n   \"\"\"\n   if len(size) != 2:\n     raise ValueError('Expected `size` to be a tuple of 2 integers, '\n-                     'but got: %s' % (size,))\n+                     f'but got: {size}.')\n   img = tf.convert_to_tensor(x)\n   if img.shape.rank is not None:\n     if img.shape.rank < 3 or img.shape.rank > 4:\n       raise ValueError(\n           'Expected an image array with shape `(height, width, channels)`, '\n-          'or `(batch_size, height, width, channels)` but '\n-          'got input with incorrect rank, of shape %s' % (img.shape,))\n+          'or `(batch_size, height, width, channels)`, but '\n+          f'got input with incorrect rank, of shape {img.shape}.')\n   shape = tf.shape(img)\n   height, width = shape[-3], shape[-2]\n   target_height, target_width = size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7be2cef8fa736313db81c7b3d18391eb90acf5d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 457 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -134,8 +134,9 @@ class ClassificationOutput(ExportOutput):\n       raise ValueError('Classification classes must be a string Tensor; '\n                        'got {}'.format(classes))\n     if scores is None and classes is None:\n-      raise ValueError('At least one of scores and classes must be set.')\n-\n+      raise ValueError('Cannot create a ClassificationOutput with empty '\n+                       'arguments. At least one of `scores` and `classes` '\n+                       'must be defined.')\n     self._scores = scores\n     self._classes = classes\n \n@@ -149,12 +150,22 @@ class ClassificationOutput(ExportOutput):\n \n   def as_signature_def(self, receiver_tensors):\n     if len(receiver_tensors) != 1:\n-      raise ValueError('Classification input must be a single string Tensor; '\n-                       'got {}'.format(receiver_tensors))\n+      raise ValueError(\n+          'Classification signatures can only accept a single tensor input of '\n+          'type tf.string. Please check to make sure that you have structured '\n+          'the serving_input_receiver_fn so that it creates a single string '\n+          'placeholder. If your model function expects multiple inputs, then '\n+          'use `tf.io.parse_example()` to parse the string into multiple '\n+          f'tensors.\\n Received: {receiver_tensors}')\n     (_, examples), = receiver_tensors.items()\n     if tf.as_dtype(examples.dtype) != tf.string:\n-      raise ValueError('Classification input must be a single string Tensor; '\n-                       'got {}'.format(receiver_tensors))\n+      raise ValueError(\n+          'Classification signatures can only accept a single tensor input of '\n+          'type tf.string. Please check to make sure that you have structured '\n+          'the serving_input_receiver_fn so that it creates a single string '\n+          'placeholder. If your model function expects multiple inputs, then '\n+          'use `tf.io.parse_example()` to parse the string into multiple '\n+          f'tensors.\\n Received: {receiver_tensors}')\n     return tf.compat.v1.saved_model.classification_signature_def(\n         examples, self.classes, self.scores)\n \n@@ -182,12 +193,22 @@ class RegressionOutput(ExportOutput):\n \n   def as_signature_def(self, receiver_tensors):\n     if len(receiver_tensors) != 1:\n-      raise ValueError('Regression input must be a single string Tensor; '\n-                       'got {}'.format(receiver_tensors))\n+      raise ValueError(\n+          'Regression signatures can only accept a single tensor input of '\n+          'type tf.string. Please check to make sure that you have structured '\n+          'the serving_input_receiver_fn so that it creates a single string '\n+          'placeholder. If your model function expects multiple inputs, then '\n+          'use `tf.io.parse_example()` to parse the string into multiple '\n+          f'tensors.\\n Received: {receiver_tensors}')\n     (_, examples), = receiver_tensors.items()\n     if tf.as_dtype(examples.dtype) != tf.string:\n-      raise ValueError('Regression input must be a single string Tensor; '\n-                       'got {}'.format(receiver_tensors))\n+      raise ValueError(\n+          'Regression signatures can only accept a single tensor input of '\n+          'type tf.string. Please check to make sure that you have structured '\n+          'the serving_input_receiver_fn so that it creates a single string '\n+          'placeholder. If your model function expects multiple inputs, then '\n+          'use `tf.io.parse_example()` to parse the string into multiple '\n+          f'tensors.\\n Received: {receiver_tensors}')\n     return tf.compat.v1.saved_model.regression_signature_def(examples, self.value)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06ba37b8662dea768b3bc8201942f1eb877708e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 49 | Files Changed: 6 | Hunks: 27 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 109 | Churn Cumulative: 1727 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -159,15 +159,15 @@ def image_dataset_from_directory(directory,\n           'directory. If you wish to infer the labels from the subdirectory '\n           'names in the target directory, pass `labels=\"inferred\"`. '\n           'If you wish to get a dataset that only contains images '\n-          '(no labels), pass `label_mode=None`.')\n+          f'(no labels), pass `labels=None`. Received: labels={labels}')\n     if class_names:\n-      raise ValueError('You can only pass `class_names` if the labels are '\n-                       'inferred from the subdirectory names in the target '\n-                       'directory (`labels=\"inferred\"`).')\n+      raise ValueError('You can only pass `class_names` if '\n+                       f'`labels=\"inferred\"`. Received: labels={labels}, and '\n+                       f'class_names={class_names}')\n   if label_mode not in {'int', 'categorical', 'binary', None}:\n     raise ValueError(\n         '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n-        'or None. Received: %s' % (label_mode,))\n+        f'or None. Received: label_mode={label_mode}')\n   if labels is None or label_mode is None:\n     labels = None\n     label_mode = None\n@@ -180,7 +180,7 @@ def image_dataset_from_directory(directory,\n   else:\n     raise ValueError(\n         '`color_mode` must be one of {\"rbg\", \"rgba\", \"grayscale\"}. '\n-        'Received: %s' % (color_mode,))\n+        f'Received: color_mode={color_mode}')\n   interpolation = image_preprocessing.get_interpolation(interpolation)\n   dataset_utils.check_validation_split_arg(\n       validation_split, subset, shuffle, seed)\n@@ -198,13 +198,14 @@ def image_dataset_from_directory(directory,\n \n   if label_mode == 'binary' and len(class_names) != 2:\n     raise ValueError(\n-        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n-        'Found the following classes: %s' % (class_names,))\n+        f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+        f'class_names. Received: class_names={class_names}')\n \n   image_paths, labels = dataset_utils.get_training_or_validation_split(\n       image_paths, labels, validation_split, subset)\n   if not image_paths:\n-    raise ValueError(f'No images found. Allowed formats: {ALLOWLIST_FORMATS}')\n+    raise ValueError(f'No images found in directory {directory}. '\n+                     f'Allowed formats: {ALLOWLIST_FORMATS}')\n \n   dataset = paths_and_labels_to_dataset(\n       image_paths=image_paths,\n\n@@ -308,7 +308,7 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n           directory, color_mode='other')\n \n     with self.assertRaisesRegex(\n-        ValueError, 'only pass `class_names` if the labels are inferred'):\n+        ValueError, 'only pass `class_names` if `labels=\"inferred\"`'):\n       _ = image_dataset.image_dataset_from_directory(\n           directory, labels=[0, 0, 1, 1, 1],\n           class_names=['class_0', 'class_1', 'class_2'])\n@@ -324,7 +324,7 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       _ = image_dataset.image_dataset_from_directory(\n           directory, class_names=['class_0', 'class_2'])\n \n-    with self.assertRaisesRegex(ValueError, 'there must exactly 2 classes'):\n+    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):\n       _ = image_dataset.image_dataset_from_directory(\n           directory, label_mode='binary')\n \n\n@@ -120,15 +120,15 @@ def text_dataset_from_directory(directory,\n           'directory. If you wish to infer the labels from the subdirectory '\n           'names in the target directory, pass `labels=\"inferred\"`. '\n           'If you wish to get a dataset that only contains text samples '\n-          '(no labels), pass `labels=None`.')\n+          f'(no labels), pass `labels=None`. Received: labels={labels}')\n     if class_names:\n-      raise ValueError('You can only pass `class_names` if the labels are '\n-                       'inferred from the subdirectory names in the target '\n-                       'directory (`labels=\"inferred\"`).')\n+      raise ValueError('You can only pass `class_names` if '\n+                       f'`labels=\"inferred\"`. Received: labels={labels}, and '\n+                       f'class_names={class_names}')\n   if label_mode not in {'int', 'categorical', 'binary', None}:\n     raise ValueError(\n         '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n-        'or None. Received: %s' % (label_mode,))\n+        f'or None. Received: label_mode={label_mode}')\n   if labels is None or label_mode is None:\n     labels = None\n     label_mode = None\n@@ -148,13 +148,14 @@ def text_dataset_from_directory(directory,\n \n   if label_mode == 'binary' and len(class_names) != 2:\n     raise ValueError(\n-        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n-        'Found the following classes: %s' % (class_names,))\n+        f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+        f'class_names. Received: class_names={class_names}')\n \n   file_paths, labels = dataset_utils.get_training_or_validation_split(\n       file_paths, labels, validation_split, subset)\n   if not file_paths:\n-    raise ValueError('No text files found.')\n+    raise ValueError(f'No text files found in directory {directory}. '\n+                     f'Allowed format: .txt')\n \n   dataset = paths_and_labels_to_dataset(\n       file_paths=file_paths,\n\n@@ -195,7 +195,7 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n \n   def test_text_dataset_from_directory_no_files(self):\n     directory = self._prepare_directory(num_classes=2, count=0)\n-    with self.assertRaisesRegex(ValueError, 'No text files found.'):\n+    with self.assertRaisesRegex(ValueError, 'No text files found'):\n       _ = text_dataset.text_dataset_from_directory(directory)\n \n   def test_text_dataset_from_directory_errors(self):\n@@ -210,7 +210,7 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n           directory, label_mode='other')\n \n     with self.assertRaisesRegex(\n-        ValueError, 'only pass `class_names` if the labels are inferred'):\n+        ValueError, 'only pass `class_names` if `labels=\"inferred\"`'):\n       _ = text_dataset.text_dataset_from_directory(\n           directory, labels=[0, 0, 1, 1, 1],\n           class_names=['class_0', 'class_1', 'class_2'])\n@@ -226,7 +226,7 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       _ = text_dataset.text_dataset_from_directory(\n           directory, class_names=['class_0', 'class_2'])\n \n-    with self.assertRaisesRegex(ValueError, 'there must exactly 2 classes'):\n+    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):\n       _ = text_dataset.text_dataset_from_directory(\n           directory, label_mode='binary')\n \n\n@@ -142,33 +142,42 @@ def timeseries_dataset_from_array(\n     break\n   ```\n   \"\"\"\n-  if start_index and (start_index < 0 or start_index >= len(data)):\n-    raise ValueError('start_index must be higher than 0 and lower than the '\n-                     'length of the data. Got: start_index=%s '\n-                     'for data of length %s.' % (start_index, len(data)))\n+  if start_index:\n+    if start_index < 0:\n+      raise ValueError(f'`start_index` must be 0 or greater. Received: '\n+                       f'start_index={start_index}')\n+    if start_index >= len(data):\n+      raise ValueError(f'`start_index` must be lower than the length of the '\n+                       f'data. Received: start_index={start_index}, for data '\n+                       f'of length {len(data)}')\n   if end_index:\n     if start_index and end_index <= start_index:\n-      raise ValueError('end_index must be higher than start_index. Got: '\n-                       'start_index=%s, end_index=%s.' %\n-                       (start_index, end_index))\n+      raise ValueError(f'`end_index` must be higher than `start_index`. '\n+                       f'Received: start_index={start_index}, and '\n+                       f'end_index={end_index} ')\n     if end_index >= len(data):\n-      raise ValueError('end_index must be lower than the length of the data. '\n-                       'Got: end_index=%s' % (end_index,))\n+      raise ValueError(f'`end_index` must be lower than the length of the '\n+                       f'data. Received: end_index={end_index}, for data of '\n+                       f'length {len(data)}')\n     if end_index <= 0:\n-      raise ValueError('end_index must be higher than 0. '\n-                       'Got: end_index=%s' % (end_index,))\n+      raise ValueError('`end_index` must be higher than 0. '\n+                       f'Received: end_index={end_index}')\n \n   # Validate strides\n-  if sampling_rate <= 0 or sampling_rate >= len(data):\n-    raise ValueError(\n-        'sampling_rate must be higher than 0 and lower than '\n-        'the length of the data. Got: '\n-        'sampling_rate=%s for data of length %s.' % (sampling_rate, len(data)))\n-  if sequence_stride <= 0 or sequence_stride >= len(data):\n-    raise ValueError(\n-        'sequence_stride must be higher than 0 and lower than '\n-        'the length of the data. Got: sequence_stride=%s '\n-        'for data of length %s.' % (sequence_stride, len(data)))\n+  if sampling_rate <= 0:\n+    raise ValueError(f'`sampling_rate` must be higher than 0. Received: '\n+                     f'sampling_rate={sampling_rate}')\n+  if sampling_rate >= len(data):\n+    raise ValueError(f'`sampling_rate` must be lower than the length of the '\n+                     f'data. Received: sampling_rate={sampling_rate}, for data '\n+                     f'of length {len(data)}')\n+  if sequence_stride <= 0:\n+    raise ValueError(f'`sequence_stride` must be higher than 0. Received: '\n+                     f'sequence_stride={sequence_stride}')\n+  if sequence_stride >= len(data):\n+    raise ValueError(f'`sequence_stride` must be lower than the length of the '\n+                     f'data. Received: sequence_stride={sequence_stride}, for '\n+                     f'data of length {len(data)}')\n \n   if start_index is None:\n     start_index = 0\n\n@@ -152,25 +152,25 @@ class TimeseriesDatasetTest(tf.test.TestCase):\n \n   def test_errors(self):\n     # bad start index\n-    with self.assertRaisesRegex(ValueError, 'start_index must be '):\n+    with self.assertRaisesRegex(ValueError, '`start_index` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, start_index=-1)\n-    with self.assertRaisesRegex(ValueError, 'start_index must be '):\n+    with self.assertRaisesRegex(ValueError, '`start_index` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, start_index=11)\n     # bad end index\n-    with self.assertRaisesRegex(ValueError, 'end_index must be '):\n+    with self.assertRaisesRegex(ValueError, '`end_index` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, end_index=-1)\n-    with self.assertRaisesRegex(ValueError, 'end_index must be '):\n+    with self.assertRaisesRegex(ValueError, '`end_index` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, end_index=11)\n     # bad sampling_rate\n-    with self.assertRaisesRegex(ValueError, 'sampling_rate must be '):\n+    with self.assertRaisesRegex(ValueError, '`sampling_rate` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, sampling_rate=0)\n     # bad sequence stride\n-    with self.assertRaisesRegex(ValueError, 'sequence_stride must be '):\n+    with self.assertRaisesRegex(ValueError, '`sequence_stride` must be '):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, sequence_stride=0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
