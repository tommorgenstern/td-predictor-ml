{"custom_id": "scrapy#034152961d187930eb659bef9e87ceb3e589c103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 707 | Contributors (this commit): 10 | Commits (past 90d): 21 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,11 +2,12 @@ from __future__ import absolute_import\n import os\n import csv\n import json\n+import warnings\n from io import BytesIO\n import tempfile\n import shutil\n-from six.moves.urllib.parse import urlparse\n-import warnings\n+from six.moves.urllib.parse import urljoin, urlparse\n+from six.moves.urllib.request import pathname2url\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n@@ -226,9 +227,10 @@ class FeedExportTest(unittest.TestCase):\n     def run_and_export(self, spider_cls, settings=None):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n         tmpdir = tempfile.mkdtemp()\n-        res_name = tmpdir + '/res'\n+        res_path = os.path.join(tmpdir, 'res')\n+        res_uri = urljoin('file:', pathname2url(res_path))\n         defaults = {\n-            'FEED_URI': 'file://' + res_name,\n+            'FEED_URI': res_uri,\n             'FEED_FORMAT': 'csv',\n         }\n         defaults.update(settings or {})\n@@ -238,7 +240,7 @@ class FeedExportTest(unittest.TestCase):\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(res_name, 'rb') as f:\n+            with open(res_path, 'rb') as f:\n                 defer.returnValue(f.read())\n \n         finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22505a34a9d0095dfc6e133bbb79d3b7aa651082", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 71 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,7 +52,8 @@ class CmdlineTest(unittest.TestCase):\n             stats.print_stats()\n             out.seek(0)\n             stats = out.read()\n-            self.assertIn('scrapy/commands/version.py', stats)\n+            self.assertIn(os.path.join('scrapy', 'commands', 'version.py'),\n+                          stats)\n             self.assertIn('tottime', stats)\n         finally:\n             shutil.rmtree(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb09148c91f118e3a71243e527a0d39294ba59cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 637 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -284,10 +284,6 @@ class MySpider(scrapy.Spider):\n         log = self.get_log('', name='myspider.txt')\n         self.assertIn('Unable to load', log)\n \n-\n-class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n-    command = 'parse'\n-\n     def test_start_requests_errors(self):\n         log = self.get_log(\"\"\"\n import scrapy\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a21abac743865b628e52f11882db7b7ab70e0342", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1081 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -915,6 +915,7 @@ class BaseFTPTestCase(unittest.TestCase):\n \n     def test_ftp_local_filename(self):\n         f, local_fname = tempfile.mkstemp()\n+        local_fname = to_bytes(local_fname)\n         os.close(f)\n         meta = {\"ftp_local_filename\": local_fname}\n         meta.update(self.req_meta)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e532e3dd8b4dd07b1a287d6321c1e6ae6786c64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 215 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -209,7 +209,7 @@ class MockServer():\n         time.sleep(0.2)\n \n     def url(self, path, is_secure=False):\n-        host = self.http_address\n+        host = self.http_address.replace('0.0.0.0', '127.0.0.1')\n         if is_secure:\n             host = self.https_address\n         return host + path\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca53a8699a8cfc2a0a3be4a65d91bada8a43f94c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 867 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,8 +107,8 @@ class Command(ScrapyCommand):\n                 string.Template(path).substitute(project_name=project_name))\n             render_templatefile(tplfile, project_name=project_name,\n                 ProjectName=string_camelcase(project_name))\n-        print(\"New Scrapy project %r, using template directory %r, created in:\" % \\\n-              (project_name, self.templates_dir))\n+        print(\"New Scrapy project '%s', using template directory '%s', \"\n+              \"created in:\" % (project_name, self.templates_dir))\n         print(\"    %s\\n\" % abspath(project_dir))\n         print(\"You can start your first spider with:\")\n         print(\"    cd %s\" % project_dir)\n\n@@ -113,7 +113,8 @@ class StartprojectTemplatesTest(ProjectTest):\n \n         args = ['--set', 'TEMPLATES_DIR=%s' % self.tmpl]\n         p, out, err = self.proc('startproject', self.project_name, *args)\n-        self.assertIn(\"New Scrapy project %r, using template directory\" % self.project_name, out)\n+        self.assertIn(\"New Scrapy project '%s', using template directory\"\n+                      % self.project_name, out)\n         self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb281757500b2e114fb554aa3bf618e3a8f8eb71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 218 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n from tests import get_testdata\n \n-FOOBAR_NL = u\"foo\" + os.linesep + u\"bar\"\n+FOOBAR_NL = u\"foo\\nbar\"\n \n \n class XmliterTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d93d960319e22badccd68499df11f2a728dbbc04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 113 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,8 +25,12 @@ def inside_a_project():\n \n class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_outside_project(self):\n-        self.assertEqual('.scrapy/somepath', data_path('somepath'))\n-        self.assertEqual('/absolute/path', data_path('/absolute/path'))\n+        self.assertEqual(\n+            os.path.join('.scrapy', 'somepath'),\n+            data_path('somepath')\n+        )\n+        abspath = os.path.join(os.path.sep, 'absolute', 'path')\n+        self.assertEqual(abspath, data_path(abspath))\n \n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n@@ -35,4 +39,5 @@ class ProjectUtilsTest(unittest.TestCase):\n                 os.path.realpath(expected),\n                 os.path.realpath(data_path('somepath'))\n             )\n-            self.assertEqual('/absolute/path', data_path('/absolute/path'))\n+            abspath = os.path.join(os.path.sep, 'absolute', 'path')\n+            self.assertEqual(abspath, data_path(abspath))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96517cb7de93da990f4f7dcb1cb7b7129e8c2064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 15 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 233 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n from os.path import join, abspath\n from twisted.trial import unittest\n from twisted.internet import defer\n@@ -7,6 +8,11 @@ from scrapy.utils.python import to_native_str\n from tests.test_commands import CommandTest\n \n \n+def _textmode(bstr):\n+    \"\"\"Normalize input the same as writing to a file\n+    and reading from it in text mode\"\"\"\n+    return to_native_str(bstr).replace(os.linesep, '\\n')\n+\n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     command = 'parse'\n \n@@ -97,7 +103,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '-a', 'test_arg=1',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n+        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_request_with_meta(self):\n@@ -106,13 +112,13 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '--meta', raw_json_string,\n                                            '-c', 'parse_request_with_meta',\n                                            self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n+        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '-m', raw_json_string,\n                                            '-c', 'parse_request_with_meta',\n                                            self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n+        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n \n     @defer.inlineCallbacks\n@@ -120,7 +126,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                           '-c', 'parse_request_without_meta',\n                                            self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n+        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n \n     @defer.inlineCallbacks\n@@ -129,29 +135,29 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '--pipelines',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"INFO: It Works!\", to_native_str(stderr))\n+        self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_parse_items_no_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, self.url('/html')]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_wrong_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]\n         )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"Cannot find callback\"\"\", to_native_str(stderr))\n+        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"Cannot find callback\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_matching_rule_callback_set(self):\n@@ -159,7 +165,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/html')]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_matching_rule_default_callback(self):\n@@ -167,7 +173,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', 'goodcrawl'+self.spider_name, '-r', self.url('/text')]\n         )\n-        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", to_native_str(out))\n+        self.assertIn(\"\"\"[{}, {'nomatch': 'default'}]\"\"\", _textmode(out))\n \n     @defer.inlineCallbacks\n     def test_spider_with_no_rules_attribute(self):\n@@ -175,15 +181,15 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-r', self.url('/html')]\n         )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", to_native_str(stderr))\n+        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"No CrawlSpider rules found\"\"\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_crawlspider_missing_callback(self):\n         status, out, stderr = yield self.execute(\n             ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/html')]\n         )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n \n     @defer.inlineCallbacks\n     def test_crawlspider_no_matching_rule(self):\n@@ -191,5 +197,5 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         status, out, stderr = yield self.execute(\n             ['--spider', 'badcrawl'+self.spider_name, '-r', self.url('/enc-gb18030')]\n         )\n-        self.assertRegexpMatches(to_native_str(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n-        self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", to_native_str(stderr))\n+        self.assertRegexpMatches(_textmode(out), \"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n+        self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7fe243c3e6f71660620441ec6a8ca4605d4333b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 417 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,9 @@\n import logging\n import tempfile\n import warnings\n-import unittest\n \n from twisted.internet import defer\n-import twisted.trial.unittest\n+from twisted.trial import unittest\n \n import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n@@ -94,12 +93,12 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         assert get_scrapy_root_handler() is None\n \n     def test_spider_custom_settings_log_level(self):\n-        with tempfile.NamedTemporaryFile() as log_file:\n+        log_file = self.mktemp()\n         class MySpider(scrapy.Spider):\n             name = 'spider'\n             custom_settings = {\n                 'LOG_LEVEL': 'INFO',\n-                    'LOG_FILE': log_file.name,\n+                'LOG_FILE': log_file,\n                 # disable telnet if not available to avoid an extra warning\n                 'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n             }\n@@ -113,7 +112,10 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         logging.info('info message')\n         logging.warning('warning message')\n         logging.error('error message')\n-            logged = log_file.read().decode('utf8')\n+\n+        with open(log_file, 'rb') as fo:\n+            logged = fo.read().decode('utf8')\n+\n         self.assertNotIn('debug message', logged)\n         self.assertIn('info message', logged)\n         self.assertIn('warning message', logged)\n@@ -203,7 +205,7 @@ class NoRequestsSpider(scrapy.Spider):\n         return []\n \n \n-class CrawlerRunnerHasSpider(twisted.trial.unittest.TestCase):\n+class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4eaf8690b14527d8d74ddd8103810de7c4dcdee6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 422 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,9 +143,8 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n         settings = Settings({\n             'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'\n         })\n-        with warnings.catch_warnings(record=True) as w, \\\n-                self.assertRaises(AttributeError):\n-            CrawlerRunner(settings)\n+        with warnings.catch_warnings(record=True) as w:\n+            self.assertRaises(AttributeError, CrawlerRunner, settings)\n             self.assertEqual(len(w), 1)\n             self.assertIn(\"SPIDER_LOADER_CLASS\", str(w[0].message))\n             self.assertIn(\"scrapy.interfaces.ISpiderLoader\", str(w[0].message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38608bc2495189bd7de2dc48c0036df217138377", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 725 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -244,13 +244,7 @@ class FeedExportTest(unittest.TestCase):\n                 content = f.read()\n \n         finally:\n-            # FIXME: Windows fails to remove the file because FeedExporter \n-            # keeps a reference to the temporal file even after\n-            # the spider finished.\n-            try:\n-                shutil.rmtree(tmpdir)\n-            except OSError:\n-                pass\n+            shutil.rmtree(tmpdir, ignore_errors=True)\n \n         defer.returnValue(content)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4de493efdd80d8ff85a78009dcc120ac46b9c55c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 40 | Churn Cumulative: 201 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n from unittest import TextTestResult\n \n+from twisted.internet import defer\n from twisted.python import failure\n from twisted.trial import unittest\n \n+from scrapy.crawler import CrawlerRunner\n from scrapy.spidermiddlewares.httperror import HttpError\n from scrapy.spiders import Spider\n from scrapy.http import Request\n@@ -101,6 +103,29 @@ class TestSpider(Spider):\n         pass\n \n \n+class TestSameUrlSpider(Spider):\n+\n+    name = 'test_same_url'\n+\n+    def __init__(self, *args, **kwargs):\n+        super(TestSameUrlSpider, self).__init__(*args, **kwargs)\n+        self.visited = 0\n+\n+    def parse_first(self, response):\n+        \"\"\"first callback\n+        @url http://scrapy.org\n+        \"\"\"\n+        self.visited += 1\n+        return TestItem()\n+\n+    def parse_second(self, response):\n+        \"\"\"second callback\n+        @url http://scrapy.org\n+        \"\"\"\n+        self.visited += 1\n+        return TestItem()\n+\n+\n class ContractsManagerTest(unittest.TestCase):\n     contracts = [UrlContract, ReturnsContract, ScrapesContract]\n \n@@ -177,14 +202,12 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_succeed()\n \n         # scrapes_item_fail\n-        request = self.conman.from_method(spider.scrapes_item_fail,\n-                self.results)\n+        request = self.conman.from_method(spider.scrapes_item_fail, self.results)\n         request.callback(response)\n         self.should_fail()\n \n         # scrapes_dict_item_fail\n-        request = self.conman.from_method(spider.scrapes_dict_item_fail,\n-                self.results)\n+        request = self.conman.from_method(spider.scrapes_dict_item_fail, self.results)\n         request.callback(response)\n         self.should_fail()\n \n@@ -202,3 +225,12 @@ class ContractsManagerTest(unittest.TestCase):\n \n         self.assertFalse(self.results.failures)\n         self.assertTrue(self.results.errors)\n+\n+    @defer.inlineCallbacks\n+    def test_same_url(self):\n+        TestSameUrlSpider.start_requests = lambda s: self.conman.from_spider(s, self.results)\n+\n+        crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n+        yield crawler.crawl()\n+\n+        self.assertEqual(crawler.spider.visited, 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cb4decb6ad52548b3441c877235c540cef12082", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 246 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,29 +103,6 @@ class TestSpider(Spider):\n         pass\n \n \n-class TestSameUrlSpider(Spider):\n-\n-    name = 'test_same_url'\n-\n-    def __init__(self, *args, **kwargs):\n-        super(TestSameUrlSpider, self).__init__(*args, **kwargs)\n-        self.visited = 0\n-\n-    def parse_first(self, response):\n-        \"\"\"first callback\n-        @url http://scrapy.org\n-        \"\"\"\n-        self.visited += 1\n-        return TestItem()\n-\n-    def parse_second(self, response):\n-        \"\"\"second callback\n-        @url http://scrapy.org\n-        \"\"\"\n-        self.visited += 1\n-        return TestItem()\n-\n-\n class ContractsManagerTest(unittest.TestCase):\n     contracts = [UrlContract, ReturnsContract, ScrapesContract]\n \n@@ -228,6 +205,28 @@ class ContractsManagerTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_same_url(self):\n+\n+        class TestSameUrlSpider(Spider):\n+            name = 'test_same_url'\n+\n+            def __init__(self, *args, **kwargs):\n+                super(TestSameUrlSpider, self).__init__(*args, **kwargs)\n+                self.visited = 0\n+\n+            def parse_first(self, response):\n+                \"\"\"first callback\n+                @url http://scrapy.org\n+                \"\"\"\n+                self.visited += 1\n+                return TestItem()\n+\n+            def parse_second(self, response):\n+                \"\"\"second callback\n+                @url http://scrapy.org\n+                \"\"\"\n+                self.visited += 1\n+                return TestItem()\n+\n         TestSameUrlSpider.start_requests = lambda s: self.conman.from_spider(s, self.results)\n \n         crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddd69f4c10578975658ef6ac450cbe1bed85df80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 255 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from scrapy.contracts.default import (\n     ReturnsContract,\n     ScrapesContract,\n )\n+from tests.mockserver import MockServer\n \n \n class TestItem(Item):\n@@ -213,6 +214,9 @@ class ContractsManagerTest(unittest.TestCase):\n                 super(TestSameUrlSpider, self).__init__(*args, **kwargs)\n                 self.visited = 0\n \n+            def start_requests(s):\n+                return self.conman.from_spider(s, self.results)\n+\n             def parse_first(self, response):\n                 \"\"\"first callback\n                 @url http://scrapy.org\n@@ -227,9 +231,8 @@ class ContractsManagerTest(unittest.TestCase):\n                 self.visited += 1\n                 return TestItem()\n \n-        TestSameUrlSpider.start_requests = lambda s: self.conman.from_spider(s, self.results)\n-\n         crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n-        yield crawler.crawl()\n+        with MockServer() as mockserver:\n+            yield crawler.crawl(mockserver=mockserver)\n \n         self.assertEqual(crawler.spider.visited, 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d95762db7c9abe59d8edfc8c17397a5a5bbfc661", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 44 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,7 @@ spider_closed = object()\n spider_error = object()\n request_scheduled = object()\n request_dropped = object()\n+request_reached_downloader = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#afb1458bd3b75fad78f053ac062910052971cdfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 167 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,6 +103,7 @@ class CrawlerRun(object):\n         self.respplug = []\n         self.reqplug = []\n         self.reqdropped = []\n+        self.reqreached = []\n         self.itemerror = []\n         self.itemresp = []\n         self.signals_catched = {}\n@@ -124,6 +125,7 @@ class CrawlerRun(object):\n         self.crawler.signals.connect(self.item_error, signals.item_error)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n+        self.crawler.signals.connect(self.request_reached, signals.request_reached_downloader)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n         self.crawler.crawl(start_urls=start_urls)\n         self.spider = self.crawler.spider\n@@ -155,6 +157,9 @@ class CrawlerRun(object):\n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n \n+    def request_reached(self, request, spider):\n+        self.reqreached.append((request, spider))\n+\n     def request_dropped(self, request, spider):\n         self.reqdropped.append((request, spider))\n \n@@ -212,6 +217,8 @@ class EngineTest(unittest.TestCase):\n         responses_count = len(self.run.respplug)\n         self.assertEqual(scheduled_requests_count,\n                          dropped_requests_count + responses_count)\n+        self.assertEqual(len(self.run.reqreached),\n+                         responses_count)\n \n     def _assert_dropped_requests(self):\n         self.assertEqual(len(self.run.reqdropped), 1)\n@@ -219,6 +226,7 @@ class EngineTest(unittest.TestCase):\n     def _assert_downloaded_responses(self):\n         # response tests\n         self.assertEqual(8, len(self.run.respplug))\n+        self.assertEqual(8, len(self.run.reqreached))\n \n         for response, _ in self.run.respplug:\n             if self.run.getpath(response.url) == '/item999.html':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#561ad3b63c539b560bbaed1df37d0b60043ab3b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 759 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,6 +129,9 @@ class Downloader(object):\n             return response\n \n         slot.active.add(request)\n+        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n+                                    request=request,\n+                                    spider=spider)\n         deferred = defer.Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bac43676425d25169830d4410db60a98a11911f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 38 | Churn Cumulative: 1533 | Contributors (this commit): 20 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,13 @@ class DownloadHandlers(object):\n             crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n         for scheme, clspath in six.iteritems(handlers):\n             self._schemes[scheme] = clspath\n+        for scheme in self._schemes:\n+            path = self._schemes[scheme]\n+            dhcls = load_object(path)\n+            lazy = getattr(dhcls, 'lazy', False)\n+            if lazy:\n+                continue\n+            self._load_handler(scheme, dhcls)\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n@@ -40,8 +47,12 @@ class DownloadHandlers(object):\n             return None\n \n         path = self._schemes[scheme]\n-        try:\n         dhcls = load_object(path)\n+        self._load_handler(scheme, dhcls)\n+        return self._handlers[scheme]\n+\n+    def _load_handler(self, scheme, dhcls):\n+        try:\n             dh = dhcls(self._crawler.settings)\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n@@ -54,7 +65,6 @@ class DownloadHandlers(object):\n             return None\n         else:\n             self._handlers[scheme] = dh\n-        return self._handlers[scheme]\n \n     def download_request(self, request, spider):\n         scheme = urlparse_cached(request).scheme\n\n@@ -31,6 +31,7 @@ def _get_boto_connection():\n \n \n class S3DownloadHandler(object):\n+    lazy = True\n \n     def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n             httpdownloadhandler=HTTPDownloadHandler, **kw):\n\n@@ -41,12 +41,20 @@ from scrapy.exceptions import NotConfigured\n from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n \n+\n class DummyDH(object):\n \n     def __init__(self, crawler):\n         pass\n \n \n+class DummyLazyDH(object):\n+    lazy = True\n+\n+    def __init__(self, crawler):\n+        pass\n+\n+\n class OffDH(object):\n \n     def __init__(self, crawler):\n@@ -60,8 +68,6 @@ class LoadTestCase(unittest.TestCase):\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._schemes)\n-        for scheme in handlers: # force load handlers\n-            dh._get_handler(scheme)\n         self.assertIn('scheme', dh._handlers)\n         self.assertNotIn('scheme', dh._notconfigured)\n \n@@ -70,8 +76,6 @@ class LoadTestCase(unittest.TestCase):\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._schemes)\n-        for scheme in handlers: # force load handlers\n-            dh._get_handler(scheme)\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertIn('scheme', dh._notconfigured)\n \n@@ -85,6 +89,17 @@ class LoadTestCase(unittest.TestCase):\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertIn('scheme', dh._notconfigured)\n \n+    def test_lazy_handlers(self):\n+        handlers = {'scheme': 'tests.test_downloader_handlers.DummyLazyDH'}\n+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        dh = DownloadHandlers(crawler)\n+        self.assertIn('scheme', dh._schemes)\n+        self.assertNotIn('scheme', dh._handlers)\n+        for scheme in handlers:  # force load lazy handler\n+            dh._get_handler(scheme)\n+        self.assertIn('scheme', dh._handlers)\n+        self.assertNotIn('scheme', dh._notconfigured)\n+\n \n class FileTestCase(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2de0a7203b99d8fc71e62539fdd75dad439982a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 351 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ class ContractsManager(object):\n             bound_method = spider.__getattribute__(method)\n             try:\n                 requests.append(self.from_method(bound_method, results))\n-            except:\n+            except Exception:\n                 case = _create_testcase(bound_method, 'contract')\n                 results.addError(case, sys.exc_info())\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0467737cf0bc4c5603bd63a87e220482eb724600", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 269 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -222,22 +222,20 @@ class ContractsManagerTest(unittest.TestCase):\n                 return self.conman.from_spider(s, self.results)\n \n             def parse_first(self, response):\n-                \"\"\"first callback\n-                @url {}\n-                \"\"\".format(self.mockserver.url('/status?n=200'))\n                 self.visited += 1\n                 return TestItem()\n \n             def parse_second(self, response):\n-                \"\"\"second callback\n-                @url {}\n-                \"\"\".format(self.mockserver.url('/status?n=200'))\n                 self.visited += 1\n                 return TestItem()\n \n-        crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n         with MockServer() as mockserver:\n-            yield crawler.crawl(mockserver=mockserver)\n+            mock_endpoint = mockserver.url('/status?n=200')\n+            TestSameUrlSpider.parse_first.__func__.__doc__ = '@url {}'.format(mock_endpoint)\n+            TestSameUrlSpider.parse_second.__func__.__doc__ = '@url {}'.format(mock_endpoint)\n+\n+            crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n+            yield crawler.crawl()\n \n         self.assertEqual(crawler.spider.visited, 2)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57824600a8295f0d8537de2a8d1c3eb9977ed36d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 277 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n from unittest import TextTestResult\n \n+from six import get_unbound_function\n from twisted.internet import defer\n from twisted.python import failure\n from twisted.trial import unittest\n@@ -230,9 +231,10 @@ class ContractsManagerTest(unittest.TestCase):\n                 return TestItem()\n \n         with MockServer() as mockserver:\n-            mock_endpoint = mockserver.url('/status?n=200')\n-            TestSameUrlSpider.parse_first.__func__.__doc__ = '@url {}'.format(mock_endpoint)\n-            TestSameUrlSpider.parse_second.__func__.__doc__ = '@url {}'.format(mock_endpoint)\n+            contract_doc = '@url {}'.format(mockserver.url('/status?n=200'))\n+\n+            get_unbound_function(TestSameUrlSpider.parse_first).__doc__ = contract_doc\n+            get_unbound_function(TestSameUrlSpider.parse_second).__doc__ = contract_doc\n \n             crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n             yield crawler.crawl()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#167211ffb0e7fa7756483aa9f20e1ab5589c7c4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 8 | Hunks: 16 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 34 | Churn Cumulative: 3416 | Contributors (this commit): 28 | Commits (past 90d): 13 | Contributors (cumulative): 65 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,13 +24,7 @@ class DownloadHandlers(object):\n             crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n         for scheme, clspath in six.iteritems(handlers):\n             self._schemes[scheme] = clspath\n-        for scheme in self._schemes:\n-            path = self._schemes[scheme]\n-            dhcls = load_object(path)\n-            lazy = getattr(dhcls, 'lazy', False)\n-            if lazy:\n-                continue\n-            self._load_handler(scheme, dhcls)\n+            self._load_handler(scheme, skip_lazy=True)\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n \n@@ -46,13 +40,14 @@ class DownloadHandlers(object):\n             self._notconfigured[scheme] = 'no handler available for that scheme'\n             return None\n \n-        path = self._schemes[scheme]\n-        dhcls = load_object(path)\n-        self._load_handler(scheme, dhcls)\n-        return self._handlers[scheme]\n+        return self._load_handler(scheme)\n \n-    def _load_handler(self, scheme, dhcls):\n+    def _load_handler(self, scheme, skip_lazy=False):\n+        path = self._schemes[scheme]\n         try:\n+            dhcls = load_object(path)\n+            if skip_lazy and getattr(dhcls, 'lazy', True):\n+                return None\n             dh = dhcls(self._crawler.settings)\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n@@ -65,6 +60,7 @@ class DownloadHandlers(object):\n             return None\n         else:\n             self._handlers[scheme] = dh\n+            return dh\n \n     def download_request(self, request, spider):\n         scheme = urlparse_cached(request).scheme\n\n@@ -6,6 +6,8 @@ from scrapy.utils.decorators import defers\n \n \n class DataURIDownloadHandler(object):\n+    lazy = False\n+\n     def __init__(self, settings):\n         super(DataURIDownloadHandler, self).__init__()\n \n\n@@ -2,7 +2,9 @@ from w3lib.url import file_uri_to_path\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n+\n class FileDownloadHandler(object):\n+    lazy = False\n \n     def __init__(self, settings):\n         pass\n\n@@ -60,7 +60,10 @@ class ReceivedDataProtocol(Protocol):\n         self.body.close() if self.filename else self.body.seek(0)\n \n _CODE_RE = re.compile(\"\\d+\")\n+\n+\n class FTPDownloadHandler(object):\n+    lazy = False\n \n     CODE_MAPPING = {\n         \"550\": 404,\n\n@@ -6,6 +6,7 @@ from scrapy.utils.python import to_unicode\n \n \n class HTTP10DownloadHandler(object):\n+    lazy = False\n \n     def __init__(self, settings):\n         self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n\n@@ -33,6 +33,7 @@ logger = logging.getLogger(__name__)\n \n \n class HTTP11DownloadHandler(object):\n+    lazy = False\n \n     def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n\n@@ -31,7 +31,6 @@ def _get_boto_connection():\n \n \n class S3DownloadHandler(object):\n-    lazy = True\n \n     def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n             httpdownloadhandler=HTTPDownloadHandler, **kw):\n\n@@ -43,19 +43,21 @@ from tests.spiders import SingleRequestSpider\n \n \n class DummyDH(object):\n+    lazy = False\n \n     def __init__(self, crawler):\n         pass\n \n \n class DummyLazyDH(object):\n-    lazy = True\n+    # Default is lazy for backwards compatibility\n \n     def __init__(self, crawler):\n         pass\n \n \n class OffDH(object):\n+    lazy = False\n \n     def __init__(self, crawler):\n         raise NotConfigured\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e9fa3a41ffbb97af226b72487ddf8b32af19a9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 55 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,7 +3,6 @@ Offsite Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-\n import re\n import logging\n import warnings\n@@ -35,7 +34,8 @@ class OffsiteMiddleware(object):\n                     domain = urlparse_cached(x).hostname\n                     if domain and domain not in self.domains_seen:\n                         self.domains_seen.add(domain)\n-                        logger.debug(\"Filtered offsite request to %(domain)r: %(request)s\",\n+                        logger.debug(\n+                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                             {'domain': domain, 'request': x}, extra={'spider': spider})\n                         self.stats.inc_value('offsite/domains', spider=spider)\n                     self.stats.inc_value('offsite/filtered', spider=spider)\n@@ -56,9 +56,11 @@ class OffsiteMiddleware(object):\n         url_pattern = re.compile(\"^https?://.*$\")\n         for domain in allowed_domains:\n             if url_pattern.match(domain):\n-                warnings.warn(\"allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains.\" % domain, URLWarning)\n-                \n-        regex = r'^(.*\\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains if d is not None)\n+                message = (\"allowed_domains accepts only domains, not URLs. \"\n+                           \"Ignoring URL entry %s in allowed_domains.\" % domain)\n+                warnings.warn(message, URLWarning)\n+        domains = [re.escape(d) for d in allowed_domains if d is not None]\n+        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8dbbbd13950dcb21dda759b073c64ffdca85c2d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 17 | Churn Cumulative: 645 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,6 @@ from functools import wraps\n from inspect import getmembers\n from unittest import TestCase\n \n-from scrapy import FormRequest\n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n@@ -50,14 +49,17 @@ class ContractsManager(object):\n     def from_method(self, method, results):\n         contracts = self.extract_contracts(method)\n         if contracts:\n-            # prepare request arguments\n-            kwargs = {'callback': method}\n+            request_cls = Request\n+            for contract in contracts:\n+                if contract.request_cls is not None:\n+                    request_cls = contract.request_cls\n+\n+            # calculate request args\n+            args, kwargs = get_spec(request_cls.__init__)\n+            kwargs['callback'] = method\n             for contract in contracts:\n                 kwargs = contract.adjust_request_args(kwargs)\n \n-            request_cls = kwargs.pop('request_cls', Request)\n-\n-            args, _ = get_spec(request_cls.__init__)\n             args.remove('self')\n \n             # check if all positional arguments are defined in kwargs\n@@ -98,6 +100,7 @@ class ContractsManager(object):\n \n class Contract(object):\n     \"\"\" Abstract class for contracts \"\"\"\n+    request_cls = None\n \n     def __init__(self, method, *args):\n         self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n\n@@ -27,9 +27,9 @@ class ResponseMock(object):\n \n class CustomFormContract(Contract):\n     name = 'custom_form'\n+    request_cls = FormRequest\n \n     def adjust_request_args(self, args):\n-        args['request_cls'] = FormRequest\n         args['formdata'] = {'name': 'scrapy'}\n         return args\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
