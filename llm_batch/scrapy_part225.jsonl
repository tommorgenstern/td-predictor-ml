{"custom_id": "scrapy#534772f6ea8389ec4f51d5ea4c4572080a3d3d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1493 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -13,8 +13,9 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import PotentialDataLoss\n-from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool, TCP4ClientEndpoint\n+from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n+    HTTPConnectionPool\n+from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n\n@@ -3,10 +3,10 @@ from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n+from twisted.web.client import ResponseFailed\n from scrapy import signals\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.misc import load_object\n-from scrapy.xlib.tx import ResponseFailed\n \n \n class HttpCacheMiddleware(object):\n\n@@ -17,10 +17,10 @@ from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n+from twisted.web.client import ResponseFailed\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n-from scrapy.xlib.tx import ResponseFailed\n from scrapy.core.downloader.handlers.http11 import TunnelError\n \n logger = logging.getLogger(__name__)\n\n@@ -3,10 +3,10 @@ from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n+from twisted.web.client import ResponseFailed\n \n from scrapy import twisted_version\n from scrapy.downloadermiddlewares.retry import RetryMiddleware\n-from scrapy.xlib.tx import ResponseFailed\n from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n from scrapy.utils.test import get_crawler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67cf64edbee99287bc8f663be033d1343e3a77ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 24 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,19 +1,10 @@\n-from scrapy import twisted_version\n-if twisted_version > (13, 0, 0):\n+from __future__ import absolute_import\n+\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n from twisted.web import client\n from twisted.internet import endpoints\n-if twisted_version >= (11, 1, 0):\n-    from . import client, endpoints\n-else:\n-    from scrapy.exceptions import NotSupported\n-    class _Mocked(object):\n-        def __init__(self, *args, **kw):\n-            raise NotSupported('HTTP1.1 not supported')\n-    class _Mock(object):\n-        def __getattr__(self, name):\n-            return _Mocked\n-    client = endpoints = _Mock()\n-\n \n Agent = client.Agent  # since < 11.1\n ProxyAgent = client.ProxyAgent  # since 11.1\n@@ -21,3 +12,8 @@ ResponseDone = client.ResponseDone  # since 11.1\n ResponseFailed = client.ResponseFailed  # since 11.1\n HTTPConnectionPool = client.HTTPConnectionPool  # since 12.1\n TCP4ClientEndpoint = endpoints.TCP4ClientEndpoint  # since 10.1\n+\n+warnings.warn(\"Importing from scrapy.xlib.tx is deprecated and will\"\n+              \" no longer be supported in future Scrapy versions.\"\n+              \" Update your code to import from twisted proper.\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67bc2e0b18d990f08bfeb913be0c5bd7f299015a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 194 | Files Changed: 6 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 194 | Churn Cumulative: 14344 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,59 +0,0 @@\n-# -*- test-case-name: twisted.web.test.test_newclient -*-\n-# Copyright (c) Twisted Matrix Laboratories.\n-# See LICENSE for details.\n-\n-\"\"\"\n-An U{HTTP 1.1<http://www.w3.org/Protocols/rfc2616/rfc2616.html>} client.\n-\n-The way to use the functionality provided by this module is to:\n-\n-  - Connect a L{HTTP11ClientProtocol} to an HTTP server\n-  - Create a L{Request} with the appropriate data\n-  - Pass the request to L{HTTP11ClientProtocol.request}\n-  - The returned Deferred will fire with a L{Response} object\n-  - Create a L{IProtocol} provider which can handle the response body\n-  - Connect it to the response with L{Response.deliverBody}\n-  - When the protocol's C{connectionLost} method is called, the response is\n-    complete.  See L{Response.deliverBody} for details.\n-\n-Various other classes in this module support this usage:\n-\n-  - HTTPParser is the basic HTTP parser.  It can handle the parts of HTTP which\n-    are symmetric between requests and responses.\n-\n-  - HTTPClientParser extends HTTPParser to handle response-specific parts of\n-    HTTP.  One instance is created for each request to parse the corresponding\n-    response.\n-\"\"\"\n-\n-__metaclass__ = type\n-\n-from zope.interface import implements\n-\n-from twisted.python import log\n-from twisted.python.reflect import fullyQualifiedName\n-from twisted.python.failure import Failure\n-from twisted.internet.interfaces import IConsumer, IPushProducer\n-from twisted.internet.error import ConnectionDone\n-from twisted.internet.defer import Deferred, succeed, fail, maybeDeferred\n-from twisted.internet.defer import CancelledError\n-from twisted.internet.protocol import Protocol\n-from twisted.web.iweb import UNKNOWN_LENGTH, IResponse\n-from twisted.web.http import NO_CONTENT, NOT_MODIFIED\n-from twisted.web.http import _DataLoss, PotentialDataLoss\n-from twisted.web.http import _IdentityTransferDecoder, _ChunkedTransferDecoder\n-\n-from twisted.web._newclient import (\n-    BadHeaders, ExcessWrite, ParseError, BadResponseVersion, _WrapperException,\n-    RequestGenerationFailed, RequestTransmissionFailed, ConnectionAborted,\n-    WrongBodyLength, ResponseDone, ResponseFailed, RequestNotSent,\n-    ResponseNeverReceived, HTTPParser, HTTPClientParser, Request,\n-    LengthEnforcingConsumer, makeStatefulDispatcher, Response, ChunkedEncoder,\n-    TransportProxyProducer, HTTP11ClientProtocol\n-)\n-\n-# States HTTPParser can be in\n-STATUS = 'STATUS'\n-HEADER = 'HEADER'\n-BODY = 'BODY'\n-DONE = 'DONE'\n\n@@ -1,58 +0,0 @@\n-# -*- test-case-name: twisted.web.test.test_webclient,twisted.web.test.test_agent -*-\n-# Copyright (c) Twisted Matrix Laboratories.\n-# See LICENSE for details.\n-\n-\"\"\"\n-HTTP client.\n-\"\"\"\n-\n-from __future__ import division, absolute_import\n-\n-import os\n-\n-try:\n-    from urlparse import urlunparse\n-    from urllib import splithost, splittype\n-except ImportError:\n-    from urllib.parse import splithost, splittype\n-    from urllib.parse import urlunparse as _urlunparse\n-\n-    def urlunparse(parts):\n-        result = _urlunparse(tuple([p.decode(\"charmap\") for p in parts]))\n-        return result.encode(\"charmap\")\n-import zlib\n-\n-from zope.interface import implementer\n-\n-from twisted.python import log\n-from twisted.python.failure import Failure\n-from twisted.web import http\n-from twisted.internet import defer, protocol, task, reactor\n-from twisted.internet.interfaces import IProtocol\n-from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n-from twisted.python import failure\n-from twisted.python.components import proxyForInterface\n-from twisted.web import error\n-from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IResponse\n-from twisted.web.http_headers import Headers\n-\n-from twisted.web.client import (\n-    PartialDownloadError, FileBodyProducer,\n-    CookieAgent, GzipDecoder, ContentDecoderAgent, RedirectAgent,\n-    Agent, ProxyAgent, HTTPConnectionPool, readBody,\n-)\n-\n-\n-# The code which follows is based on the new HTTP client implementation.  It\n-# should be significantly better than anything above, though it is not yet\n-# feature equivalent.\n-\n-from twisted.web._newclient import Response\n-from twisted.web._newclient import ResponseDone, ResponseFailed\n-\n-\n-__all__ = [\n-    'PartialDownloadError',\n-    'ResponseDone', 'Response', 'ResponseFailed', 'Agent', 'CookieAgent',\n-    'ProxyAgent', 'ContentDecoderAgent', 'GzipDecoder', 'RedirectAgent',\n-    'HTTPConnectionPool', 'readBody']\n\n@@ -1,26 +0,0 @@\n-# -*- test-case-name: twisted.internet.test.test_endpoints -*-\n-# Copyright (c) Twisted Matrix Laboratories.\n-# See LICENSE for details.\n-\n-\"\"\"\n-Implementations of L{IStreamServerEndpoint} and L{IStreamClientEndpoint} that\n-wrap the L{IReactorTCP}, L{IReactorSSL}, and L{IReactorUNIX} interfaces.\n-\n-This also implements an extensible mini-language for describing endpoints,\n-parsed by the L{clientFromString} and L{serverFromString} functions.\n-\n-@since: 10.1\n-\"\"\"\n-\n-from __future__ import division, absolute_import\n-\n-from twisted.internet.endpoints import (\n-    clientFromString, serverFromString, quoteStringArgument,\n-    TCP4ServerEndpoint, TCP6ServerEndpoint,\n-    TCP4ClientEndpoint, TCP6ClientEndpoint,\n-    UNIXServerEndpoint, UNIXClientEndpoint,\n-    SSL4ServerEndpoint, SSL4ClientEndpoint,\n-    AdoptedStreamServerEndpoint, connectProtocol,\n-)\n-\n-__all__ = [\"TCP4ClientEndpoint\", \"SSL4ServerEndpoint\"]\n\n@@ -1,28 +0,0 @@\n-# Copyright (c) Twisted Matrix Laboratories.\n-# See LICENSE for details.\n-\n-\"\"\"\n-Interface documentation.\n-\n-Maintainer: Itamar Shtull-Trauring\n-\"\"\"\n-\n-from __future__ import division, absolute_import\n-\n-from twisted.internet.interfaces import (\n-    IAddress, IConnector, IResolverSimple, IReactorTCP, IReactorSSL,\n-    IReactorWin32Events, IReactorUDP, IReactorMulticast, IReactorProcess,\n-    IReactorTime, IDelayedCall, IReactorThreads, IReactorCore,\n-    IReactorPluggableResolver, IReactorDaemonize, IReactorFDSet,\n-    IListeningPort, ILoggingContext, IFileDescriptor, IReadDescriptor,\n-    IWriteDescriptor, IReadWriteDescriptor, IHalfCloseableDescriptor,\n-    ISystemHandle, IConsumer, IProducer, IPushProducer, IPullProducer,\n-    IProtocol, IProcessProtocol, IHalfCloseableProtocol,\n-    IFileDescriptorReceiver, IProtocolFactory, ITransport, ITCPTransport,\n-    IUNIXTransport,\n-    ITLSTransport, ISSLTransport, IProcessTransport, IServiceCollection,\n-    IUDPTransport, IUNIXDatagramTransport, IUNIXDatagramConnectedTransport,\n-    IMulticastTransport, IStreamClientEndpoint, IStreamServerEndpoint,\n-    IStreamServerEndpointStringParser, IStreamClientEndpointStringParser,\n-    IReactorUNIX, IReactorUNIXDatagram, IReactorSocket, IResolver\n-)\n\n@@ -1,23 +0,0 @@\n-# -*- test-case-name: twisted.web.test -*-\n-# Copyright (c) Twisted Matrix Laboratories.\n-# See LICENSE for details.\n-\n-\"\"\"\n-Interface definitions for L{twisted.web}.\n-\n-@var UNKNOWN_LENGTH: An opaque object which may be used as the value of\n-    L{IBodyProducer.length} to indicate that the length of the entity\n-    body is not known in advance.\n-\"\"\"\n-\n-from twisted.web.iweb import (\n-    IRequest, ICredentialFactory, IBodyProducer, IRenderable, ITemplateLoader,\n-    IResponse, _IRequestEncoder, _IRequestEncoderFactory, UNKNOWN_LENGTH,\n-)\n-\n-__all__ = [\n-    \"ICredentialFactory\", \"IRequest\",\n-    \"IBodyProducer\", \"IRenderable\", \"IResponse\", \"_IRequestEncoder\",\n-    \"_IRequestEncoderFactory\",\n-\n-    \"UNKNOWN_LENGTH\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5efd65255c88ebb156b07f5f09a0a0fdb66f8e7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 448 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -244,7 +244,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n             return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n \n         settings = {\n-            \"IMAGES_EXPIRES\": random.randint(1, 1000),\n+            \"IMAGES_EXPIRES\": random.randint(100, 1000),\n             \"IMAGES_STORE\": self.tempdir,\n             \"IMAGES_RESULT_FIELD\": random_string(),\n             \"IMAGES_URLS_FIELD\": random_string(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#778bed07bf771dd3942ea8cd51b7944065f4e2cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 41 | Churn Cumulative: 1457 | Contributors (this commit): 20 | Commits (past 90d): 11 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ from w3lib.url import is_url\n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.exceptions import UsageError\n+from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n class Command(ScrapyCommand):\n@@ -27,8 +28,8 @@ class Command(ScrapyCommand):\n             help=\"use this spider\")\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n             help=\"print response HTTP headers instead of body\")\n-        parser.add_option(\"--no-status-aware\", dest=\"no_status_aware\", action=\"store_true\", \\\n-            default=False, help=\"do not handle status codes like redirects and print response as-is\")\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", \\\n+            default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n@@ -52,7 +53,11 @@ class Command(ScrapyCommand):\n             raise UsageError()\n         cb = lambda x: self._print_response(x, opts)\n         request = Request(args[0], callback=cb, dont_filter=True)\n-        if opts.no_status_aware:\n+        # by default, let the framework handle redirects,\n+        # i.e. command handles all codes expect 3xx\n+        if not opts.no_redirect:\n+            request.meta['handle_httpstatus_list'] = SequenceExclude(six.moves.range(300, 400))\n+        else:\n             request.meta['handle_httpstatus_all'] = True\n \n         spidercls = DefaultSpider\n\n@@ -36,8 +36,8 @@ class Command(ScrapyCommand):\n             help=\"evaluate the code in the shell, print the result and exit\")\n         parser.add_option(\"--spider\", dest=\"spider\",\n             help=\"use this spider\")\n-        parser.add_option(\"--no-status-aware\", dest=\"no_status_aware\", action=\"store_true\", \\\n-            default=False, help=\"do not transparently handle status codes like redirects\")\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", \\\n+            default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n@@ -70,7 +70,7 @@ class Command(ScrapyCommand):\n         self._start_crawler_thread()\n \n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n-        shell.start(url=url, handle_statuses=opts.no_status_aware)\n+        shell.start(url=url, redirect=not opts.no_redirect)\n \n     def _start_crawler_thread(self):\n         t = Thread(target=self.crawler_process.start,\n\n@@ -7,6 +7,7 @@ from __future__ import print_function\n \n import os\n import signal\n+from six.moves import range\n import warnings\n \n from twisted.internet import reactor, threads, defer\n@@ -20,6 +21,7 @@ from scrapy.item import BaseItem\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.console import start_python_console\n+from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n from scrapy.utils.conf import get_config\n@@ -40,11 +42,11 @@ class Shell(object):\n         self.code = code\n         self.vars = {}\n \n-    def start(self, url=None, request=None, response=None, spider=None, handle_statuses=True):\n+    def start(self, url=None, request=None, response=None, spider=None, redirect=True):\n         # disable accidental Ctrl-C key press from shutting down the engine\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n         if url:\n-            self.fetch(url, spider, handle_statuses=handle_statuses)\n+            self.fetch(url, spider, redirect=redirect)\n         elif request:\n             self.fetch(request, spider)\n         elif response:\n@@ -98,13 +100,15 @@ class Shell(object):\n         self.spider = spider\n         return spider\n \n-    def fetch(self, request_or_url, spider=None, handle_statuses=False, **kwargs):\n+    def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n         else:\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True, **kwargs)\n-            if handle_statuses:\n+            if redirect:\n+                request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n+            else:\n                 request.meta['handle_httpstatus_all'] = True\n         response = None\n         try:\n\n@@ -304,3 +304,13 @@ class LocalCache(OrderedDict):\n         while len(self) >= self.limit:\n             self.popitem(last=False)\n         super(LocalCache, self).__setitem__(key, value)\n+\n+\n+class SequenceExclude(object):\n+    \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n+\n+    def __init__(self, seq):\n+        self.seq = seq\n+\n+    def __contains__(self, item):\n+        return item not in self.seq\n\n@@ -21,7 +21,7 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_redirect_disabled(self):\n-        _, out, err = yield self.execute(['--no-status-aware', self.url('/redirect-no-meta-refresh')])\n+        _, out, err = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh')])\n         err = err.strip()\n         self.assertIn(b'downloader/response_status_count/302', err, err)\n         self.assertNotIn(b'downloader/response_status_count/200', err, err)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e54de24550df658690277839cbee99c9afe4bc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 527 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,6 +49,16 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])\n         assert out.strip().endswith(b'/redirected')\n \n+    @defer.inlineCallbacks\n+    def test_redirect_follow_302(self):\n+        _, out, _ = yield self.execute([self.url('/redirect-no-meta-refresh'), '-c', 'response.status'])\n+        assert out.strip().endswith(b'200')\n+\n+    @defer.inlineCallbacks\n+    def test_redirect_not_follow_302(self):\n+        _, out, _ = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh'), '-c', 'response.status'])\n+        assert out.strip().endswith(b'302')\n+\n     @defer.inlineCallbacks\n     def test_request_replace(self):\n         url = self.url('/text')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cd579a7748d0c37eac557216b857c38fbcf80df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 546 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,6 +59,25 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         _, out, _ = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh'), '-c', 'response.status'])\n         assert out.strip().endswith(b'302')\n \n+    @defer.inlineCallbacks\n+    def test_fetch_redirect_follow_302(self):\n+        \"\"\"Test that calling `fetch(url)` follows HTTP redirects by default.\"\"\"\n+        url = self.url('/redirect-no-meta-refresh')\n+        code = \"fetch('{0}')\"\n+        errcode, out, errout = yield self.execute(['-c', code.format(url)])\n+        self.assertEqual(errcode, 0, out)\n+        assert b'Redirecting (302)' in errout\n+        assert b'Crawled (200)' in errout\n+\n+    @defer.inlineCallbacks\n+    def test_fetch_redirect_not_follow_302(self):\n+        \"\"\"Test that calling `fetch(url, redirect=False)` disables automatic redirects.\"\"\"\n+        url = self.url('/redirect-no-meta-refresh')\n+        code = \"fetch('{0}', redirect=False)\"\n+        errcode, out, errout = yield self.execute(['-c', code.format(url)])\n+        self.assertEqual(errcode, 0, out)\n+        assert b'Crawled (302)' in errout\n+\n     @defer.inlineCallbacks\n     def test_request_replace(self):\n         url = self.url('/text')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#948e3cd00328f9a23410b3a7197975f56efadd16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 187 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+import warnings\n \n from zope.interface import implementer\n \n@@ -25,8 +26,14 @@ class SpiderLoader(object):\n \n     def _load_all_spiders(self):\n         for name in self.spider_modules:\n+            try:\n                 for module in walk_modules(name):\n                     self._load_spiders(module)\n+            except ImportError as e:\n+                msg = (\"Could not load spiders from module '{}'; \"\n+                       \"Check SPIDER_MODULES setting \"\n+                       \"(exception: {})\".format(name, str(e)))\n+                warnings.warn(msg, RuntimeWarning)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -1,6 +1,7 @@\n import sys\n import os\n import shutil\n+import warnings\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n@@ -89,3 +90,14 @@ class SpiderLoaderTest(unittest.TestCase):\n         crawler = runner.create_crawler('spider1')\n         self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n         self.assertEqual(crawler.spidercls.name, 'spider1')\n+\n+    def test_bad_spider_modules_warning(self):\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n+            settings = Settings({'SPIDER_MODULES': [module]})\n+            spider_loader = SpiderLoader.from_settings(settings)\n+            self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n+\n+            spiders = spider_loader.list()\n+            self.assertEqual(spiders, [])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d1783603251923bb549d34a64d43952fe03b3bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 796 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,10 +148,13 @@ class Shell(object):\n             if self._is_relevant(v):\n                 b.append(\"  %-10s %s\" % (k, v))\n         b.append(\"Useful shortcuts:\")\n-        b.append(\"  shelp()           Shell help (print this help)\")\n         if self.inthread:\n-            b.append(\"  fetch(req_or_url) Fetch request (or URL) and \"\n-                     \"update local objects\")\n+            b.append(\"  fetch(url[, redirect=True]) \"\n+                     \"Fetch URL and update local objects \"\n+                     \"(by default, redirects are followed)\")\n+            b.append(\"  fetch(req)                  \"\n+                     \"Fetch a scrapy.Request and update local objects \")\n+        b.append(\"  shelp()           Shell help (print this help)\")\n         b.append(\"  view(response)    View response in a browser\")\n \n         return \"\\n\".join(\"[s] %s\" % l for l in b)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a75ad2bbc63e2fd351c43069a034461c1ab673cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 754 | Contributors (this commit): 32 | Commits (past 90d): 23 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,6 +191,7 @@ LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n+LOG_SHORT_NAMES = False\n \n SCHEDULER_DEBUG = False\n \n\n@@ -118,6 +118,7 @@ def _get_handler(settings):\n     )\n     handler.setFormatter(formatter)\n     handler.setLevel(settings.get('LOG_LEVEL'))\n+    if settings.getbool('LOG_SHORT_NAMES'):\n         handler.addFilter(TopLevelFormatter(['scrapy']))\n     return handler\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6eab59cbac6a35e7d92a7391541ad2f16493338b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 47 | Churn Cumulative: 519 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,11 +38,11 @@ class ProjectTest(unittest.TestCase):\n             return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd,\n                 env=self.env, **kwargs)\n \n-    def proc(self, *new_args, **kwargs):\n+    def proc(self, *new_args, **popen_kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         p = subprocess.Popen(args, cwd=self.cwd, env=self.env,\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n-                             **kwargs)\n+                             **popen_kwargs)\n \n         waited = 0\n         interval = 0.2\n@@ -182,6 +182,17 @@ class MiscCommandsTest(CommandTest):\n \n class RunSpiderCommandTest(CommandTest):\n \n+    debug_log_spider = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug(\"It Works!\")\n+        return []\n+\"\"\"\n+\n     @contextmanager\n     def _create_file(self, content, name):\n         tmpdir = self.mktemp()\n@@ -194,32 +205,23 @@ class RunSpiderCommandTest(CommandTest):\n         finally:\n             rmtree(tmpdir)\n \n-    def runspider(self, code, name='myspider.py'):\n+    def runspider(self, code, name='myspider.py', args=()):\n         with self._create_file(code, name) as fname:\n-            return self.proc('runspider', fname)\n+            return self.proc('runspider', fname, *args)\n+\n+    def get_log(self, code, name='myspider.py', args=()):\n+        p = self.runspider(code, name=name, args=args)\n+        return to_native_str(p.stderr.read())\n \n     def test_runspider(self):\n-        spider = \"\"\"\n-import scrapy\n-\n-class MySpider(scrapy.Spider):\n-    name = 'myspider'\n-\n-    def start_requests(self):\n-        self.logger.debug(\"It Works!\")\n-        return []\n-\"\"\"\n-        p = self.runspider(spider)\n-        log = to_native_str(p.stderr.read())\n-\n+        log = self.get_log(self.debug_log_spider)\n         self.assertIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n         self.assertIn(\"INFO: Closing spider (finished)\", log)\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n     def test_runspider_no_spider_found(self):\n-        p = self.runspider(\"from scrapy.spiders import Spider\\n\")\n-        log = to_native_str(p.stderr.read())\n+        log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n         self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n@@ -228,12 +230,11 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n-        p = self.runspider('', 'myspider.txt')\n-        log = to_native_str(p.stderr.read())\n+        log = self.get_log('', name='myspider.txt')\n         self.assertIn('Unable to load', log)\n \n     def test_start_requests_errors(self):\n-        p = self.runspider(\"\"\"\n+        log = self.get_log(\"\"\"\n import scrapy\n \n class BadSpider(scrapy.Spider):\n@@ -241,11 +242,11 @@ class BadSpider(scrapy.Spider):\n     def start_requests(self):\n         raise Exception(\"oops!\")\n         \"\"\", name=\"badspider.py\")\n-        log = to_native_str(p.stderr.read())\n         print(log)\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n+\n class BenchCommandTest(CommandTest):\n \n     def test_run(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e46572d6f2de1533b1df2ab206971351c22bbbbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 525 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -220,6 +220,12 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"INFO: Closing spider (finished)\", log)\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n+    def test_runspider_log_level(self):\n+        log = self.get_log(self.debug_log_spider,\n+                           args=('-s', 'LOG_LEVEL=INFO'))\n+        self.assertNotIn(\"DEBUG: It Works!\", log)\n+        self.assertIn(\"INFO: Spider opened\", log)\n+\n     def test_runspider_no_spider_found(self):\n         log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n         self.assertIn(\"No spider found in file\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05b4555f3932afb04ab3b15893a5858fca9dec04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 540 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -226,6 +226,21 @@ class MySpider(scrapy.Spider):\n         self.assertNotIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n \n+    def test_runspider_log_short_names(self):\n+        log1 = self.get_log(self.debug_log_spider,\n+                            args=('-s', 'LOG_SHORT_NAMES=1'))\n+        print(log1)\n+        self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n+        self.assertIn(\"[scrapy]\", log1)\n+        self.assertNotIn(\"[scrapy.core.engine]\", log1)\n+\n+        log2 = self.get_log(self.debug_log_spider,\n+                            args=('-s', 'LOG_SHORT_NAMES=0'))\n+        print(log2)\n+        self.assertIn(\"[myspider] DEBUG: It Works!\", log2)\n+        self.assertNotIn(\"[scrapy]\", log2)\n+        self.assertIn(\"[scrapy.core.engine]\", log2)\n+\n     def test_runspider_no_spider_found(self):\n         log = self.get_log(\"from scrapy.spiders import Spider\\n\")\n         self.assertIn(\"No spider found in file\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f457379a54cdbad60bcf24947e053340abc7e16b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 78 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+import traceback\n import warnings\n \n from zope.interface import implementer\n@@ -30,9 +31,9 @@ class SpiderLoader(object):\n                 for module in walk_modules(name):\n                     self._load_spiders(module)\n             except ImportError as e:\n-                msg = (\"Could not load spiders from module '{}'; \"\n-                       \"Check SPIDER_MODULES setting \"\n-                       \"(exception: {})\".format(name, str(e)))\n+                msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n+                       \"Check SPIDER_MODULES setting\".format(\n+                            modname=name, tb=traceback.format_exc()))\n                 warnings.warn(msg, RuntimeWarning)\n \n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7e4081414d318ddb5297fb98a120e57044dc622", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 30/30 | Churn Δ: 63 | Churn Cumulative: 63 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import copy\n import unittest\n \n-from scrapy.utils.datatypes import CaselessDict\n+from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n __doctests__ = ['scrapy.utils.datatypes']\n \n@@ -128,6 +128,67 @@ class CaselessDictTest(unittest.TestCase):\n         assert isinstance(h2, CaselessDict)\n \n \n+class SequenceExcludeTest(unittest.TestCase):\n+\n+    def test_list(self):\n+        seq = [1, 2, 3]\n+        d = SequenceExclude(seq)\n+        self.assertIn(0, d)\n+        self.assertIn(4, d)\n+        self.assertNotIn(2, d)\n+\n+    def test_range(self):\n+        seq = range(10, 20)\n+        d = SequenceExclude(seq)\n+        self.assertIn(5, d)\n+        self.assertIn(20, d)\n+        self.assertNotIn(15, d)\n+\n+    def test_six_range(self):\n+        import six.moves\n+        seq = six.moves.range(10**3, 10**6)\n+        d = SequenceExclude(seq)\n+        self.assertIn(10**2, d)\n+        self.assertIn(10**7, d)\n+        self.assertNotIn(10**4, d)\n+\n+    def test_range_step(self):\n+        seq = range(10, 20, 3)\n+        d = SequenceExclude(seq)\n+        are_not_in = [v for v in range(10, 20, 3) if v in d]\n+        self.assertEquals([], are_not_in)\n+\n+        are_not_in = [v for v in range(10, 20) if v in d]\n+        self.assertEquals([11, 12, 14, 15, 17, 18], are_not_in)\n+\n+    def test_string_seq(self):\n+        seq = \"cde\"\n+        d = SequenceExclude(seq)\n+        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n+        self.assertEquals(\"abfg\", chars)\n+\n+    def test_stringset_seq(self):\n+        seq = set(\"cde\")\n+        d = SequenceExclude(seq)\n+        chars = \"\".join(v for v in \"abcdefg\" if v in d)\n+        self.assertEquals(\"abfg\", chars)\n+\n+    def test_set(self):\n+        \"\"\"Anything that is not in the supplied sequence will evaluate as 'in' the container.\"\"\"\n+        seq = set([-3, \"test\", 1.1])\n+        d = SequenceExclude(seq)\n+        self.assertIn(0, d)\n+        self.assertIn(\"foo\", d)\n+        self.assertIn(3.14, d)\n+        self.assertIn(set(\"bar\"), d)\n+\n+        # supplied sequence is a set, so checking for list (non)inclusion fails\n+        self.assertRaises(TypeError, (0, 1, 2) in d)\n+        self.assertRaises(TypeError, d.__contains__, ['a', 'b', 'c'])\n+\n+        for v in [-3, \"test\", 1.1]:\n+            self.assertNotIn(v, d)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70a69d2199c3c08a7e16f33ea5d35fd4066eb14b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 929 | Contributors (this commit): 15 | Commits (past 90d): 9 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ class Command(ScrapyCommand):\n         # by default, let the framework handle redirects,\n         # i.e. command handles all codes expect 3xx\n         if not opts.no_redirect:\n-            request.meta['handle_httpstatus_list'] = SequenceExclude(six.moves.range(300, 400))\n+            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))\n         else:\n             request.meta['handle_httpstatus_all'] = True\n \n\n@@ -7,7 +7,6 @@ from __future__ import print_function\n \n import os\n import signal\n-from six.moves import range\n import warnings\n \n from twisted.internet import reactor, threads, defer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b3abdb7006a2875744a89d45f46e858c981d751", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 422 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ setup(\n         'pyOpenSSL',\n         'cssselect>=0.9',\n         'six>=1.5.2',\n-        'parsel>=0.9.3',\n+        'parsel>=0.9.5',\n         'PyDispatcher>=2.0.5',\n         'service_identity',\n     ],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6dec4a3ccb455511decc4c1414b9042c57a205ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1432 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -319,14 +319,13 @@ class ScrapyAgent(object):\n         expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n \n         if maxsize and expected_size > maxsize:\n-            error_message = (\"Cancelling download of {url}: expected response \"\n-                             \"size ({size}) larger than \"\n-                             \"download max size ({maxsize}).\"\n-            ).format(url=request.url, size=expected_size, maxsize=maxsize)\n+            error_msg = (\"Cancelling download of %(url)s: expected response \"\n+                         \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n+            error_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n \n-            logger.error(error_message)\n+            logger.error(error_msg, error_args)\n             txresponse._transport._producer.loseConnection()\n-            raise defer.CancelledError(error_message)\n+            raise defer.CancelledError(error_msg % error_args)\n \n         if warnsize and expected_size > warnsize:\n             logger.warning(\"Expected response size (%(size)s) larger than \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07f9985a941d2fce4d7115a35dc983bc21ded4be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 309 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -208,7 +208,7 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n             return \"\".join([chr(random.randint(97, 123)) for _ in range(10)])\n \n         settings = {\n-            \"FILES_EXPIRES\": random.randint(1, 1000),\n+            \"FILES_EXPIRES\": random.randint(100, 1000),\n             \"FILES_URLS_FIELD\": random_string(),\n             \"FILES_RESULT_FIELD\": random_string(),\n             \"FILES_STORE\": self.tempdir\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2240f00a136e264cc1856860fddba85ec97432c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1148 | Contributors (this commit): 35 | Commits (past 90d): 18 | Contributors (cumulative): 46 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-import sys\n+import sys, os\n import optparse\n import cProfile\n import inspect\n@@ -106,6 +106,12 @@ def execute(argv=None, settings=None):\n \n     if settings is None:\n         settings = get_project_settings()\n+        # set EDITOR from environment if available\n+        try:\n+            editor = os.environ['EDITOR']\n+        except KeyError: pass\n+        else:\n+            settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n \n     # --- backwards compatibility for scrapy.conf.settings singleton ---\n\n@@ -3,6 +3,7 @@ import sys, os\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n@@ -15,7 +16,8 @@ class Command(ScrapyCommand):\n         return \"Edit spider\"\n \n     def long_desc(self):\n-        return \"Edit a spider using the editor defined in EDITOR setting\"\n+        return (\"Edit a spider using the editor defined in the EDITOR environment\"\n+                \" variable or else the EDITOR setting\")\n \n     def _err(self, msg):\n         sys.stderr.write(msg + os.linesep)\n\n@@ -13,7 +13,6 @@ Scrapy developers, if you add a setting here remember to:\n \n \"\"\"\n \n-import os\n import sys\n from importlib import import_module\n from os.path import join, abspath, dirname\n@@ -111,13 +110,9 @@ DOWNLOADER_STATS = True\n \n DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'\n \n-try:\n-    EDITOR = os.environ['EDITOR']\n-except KeyError:\n+EDITOR = 'vi'\n if sys.platform == 'win32':\n     EDITOR = '%s -m idlelib.idle'\n-    else:\n-        EDITOR = 'vi'\n \n EXTENSIONS = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3406677b980656751af9a12ddf2d33ad884fbc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 423 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -36,6 +36,7 @@ setup(\n         'Programming Language :: Python :: 3.3',\n         'Programming Language :: Python :: 3.4',\n         'Programming Language :: Python :: 3.5',\n+        'Programming Language :: Python :: 3.6',\n         'Topic :: Internet :: WWW/HTTP',\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b279bc8546994f5610836e63490b4a7a262b3d77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 27 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ class Command(fetch.Command):\n             \"contents in a browser\"\n \n     def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+        super(Command, self).add_options(parser)\n         parser.add_option(\"--spider\", dest=\"spider\",\n             help=\"use this spider\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53757e51e56ace34bef8616451cca921da16791c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 19 | Churn Cumulative: 124 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import six\n \n from scrapy.http import Request\n from scrapy.utils.python import to_unicode, to_native_str\n+from scrapy.utils.misc import load_object\n \n \n def request_to_dict(request, spider=None):\n@@ -32,6 +33,8 @@ def request_to_dict(request, spider=None):\n         'priority': request.priority,\n         'dont_filter': request.dont_filter,\n     }\n+    if type(request) is not Request:\n+        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n     return d\n \n \n@@ -47,7 +50,8 @@ def request_from_dict(d, spider=None):\n     eb = d['errback']\n     if eb and spider:\n         eb = _get_method(spider, eb)\n-    return Request(\n+    request_cls = load_object(d['_class']) if '_class' in d else Request\n+    return request_cls(\n         url=to_native_str(d['url']),\n         callback=cb,\n         errback=eb,\n\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n import unittest\n \n-from scrapy.http import Request\n+from scrapy.http import Request, FormRequest\n from scrapy.spiders import Spider\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n@@ -42,6 +42,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_same_request(request, request2)\n \n     def _assert_same_request(self, r1, r2):\n+        self.assertEqual(r1.__class__, r2.__class__)\n         self.assertEqual(r1.url, r2.url)\n         self.assertEqual(r1.callback, r2.callback)\n         self.assertEqual(r1.errback, r2.errback)\n@@ -54,6 +55,12 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1.priority, r2.priority)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n \n+    def test_request_class(self):\n+        r = FormRequest(\"http://www.example.com\")\n+        self._assert_serializes_ok(r, spider=self.spider)\n+        r = CustomRequest(\"http://www.example.com\")\n+        self._assert_serializes_ok(r, spider=self.spider)\n+\n     def test_callback_serialization(self):\n         r = Request(\"http://www.example.com\", callback=self.spider.parse_item,\n                     errback=self.spider.handle_error)\n@@ -77,3 +84,7 @@ class TestSpider(Spider):\n \n     def handle_error(self, failure):\n         pass\n+\n+\n+class CustomRequest(Request):\n+    pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
