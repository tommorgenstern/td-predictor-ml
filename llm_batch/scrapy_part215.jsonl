{"custom_id": "scrapy#7748ee6bba8eacb889ce68cd6ced273255b8b9a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 25 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 71 | Churn Cumulative: 655 | Contributors (this commit): 9 | Commits (past 90d): 24 | Contributors (cumulative): 13 | DMM Complexity: 0.9565217391304348\n\nDIFF:\n@@ -68,8 +68,8 @@ class S3DownloadHandler(object):\n             except Exception as ex:\n                 raise NotConfigured(str(ex))\n         else:\n+            if not self.anon:\n                 SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n-            # TODO - anon\n                 self._signer = SignerCls(botocore.credentials.Credentials(\n                     aws_access_key_id, aws_secret_access_key))\n \n\n@@ -1,5 +1,10 @@\n import os\n import six\n+import contextlib\n+try:\n+    from unittest import mock\n+except ImportError:\n+    import mock\n \n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n@@ -433,16 +438,16 @@ class HttpDownloadHandlerMock(object):\n \n \n class BaseS3TestCase(unittest.TestCase):\n-    if six.PY3:\n     try:\n         import botocore\n     except ImportError:\n-            skip = 'missing botocore library'\n-    else:\n+        if six.PY2:\n             try:\n                 import boto\n             except ImportError:\n-            skip = 'missing boto library'\n+                skip = 'missing botocore or boto library'\n+        else:\n+            skip = 'missing botocore library'\n \n \n class S3AnonTestCase(BaseS3TestCase):\n@@ -464,7 +469,7 @@ class S3AnonTestCase(BaseS3TestCase):\n             httpreq.url, 'http://aws-publicdatasets.s3.amazonaws.com/')\n \n \n-class S3TestCase(unittest.TestCase):\n+class S3TestCase(BaseS3TestCase):\n     download_handler_cls = S3DownloadHandler\n \n     # test use same example keys than amazon developer guide\n@@ -481,40 +486,61 @@ class S3TestCase(unittest.TestCase):\n         self.download_request = s3reqh.download_request\n         self.spider = Spider('foo')\n \n+    @contextlib.contextmanager\n+    def _mocked_date(self, date):\n+        try:\n+            import botocore.auth\n+        except ImportError:\n+            yield\n+        else:\n+            # We need to mock botocore.auth.formatdate, because otherwise\n+            # botocore overrides Date header with current date and time\n+            # and Authorization header is different each time\n+            with mock.patch('botocore.auth.formatdate') as mock_formatdate:\n+                mock_formatdate.return_value = date\n+                yield\n+\n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n-        req = Request('s3://johnsmith/photos/puppy.jpg',\n-                headers={'Date': 'Tue, 27 Mar 2007 19:36:42 +0000'})\n+        date ='Tue, 27 Mar 2007 19:36:42 +0000'\n+        req = Request('s3://johnsmith/photos/puppy.jpg', headers={'Date': date})\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n \n     def test_request_signing2(self):\n         # puts an object into the johnsmith bucket.\n+        date = 'Tue, 27 Mar 2007 21:15:45 +0000'\n         req = Request('s3://johnsmith/photos/puppy.jpg', method='PUT', headers={\n             'Content-Type': 'image/jpeg',\n-            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n+            'Date': date,\n             'Content-Length': '94328',\n             })\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n \n     def test_request_signing3(self):\n         # lists the content of the johnsmith bucket.\n+        date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n         req = Request('s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy', \\\n                 method='GET', headers={\n                     'User-Agent': 'Mozilla/5.0',\n-                    'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n+                    'Date': date,\n                     })\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n \n     def test_request_signing4(self):\n         # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n-        req = Request('s3://johnsmith/?acl', \\\n-                method='GET', headers={'Date': 'Tue, 27 Mar 2007 19:44:46 +0000'})\n+        date = 'Tue, 27 Mar 2007 19:44:46 +0000'\n+        req = Request('s3://johnsmith/?acl',\n+            method='GET', headers={'Date': date})\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n@@ -522,22 +548,27 @@ class S3TestCase(unittest.TestCase):\n     def test_request_signing5(self):\n         # deletes an object from the 'johnsmith' bucket using the\n         # path-style and Date alternative.\n+        date = 'Tue, 27 Mar 2007 21:20:27 +0000'\n         req = Request('s3://johnsmith/photos/puppy.jpg', \\\n                 method='DELETE', headers={\n-                    'Date': 'Tue, 27 Mar 2007 21:20:27 +0000',\n+                    'Date': date,\n                     'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n                     })\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n-                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n+        # botocore does not override Date with x-amz-date\n+        self.assertIn(httpreq.headers['Authorization'], [\n+                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=',\n+                b'AWS 0PN5J17HBGZHT7JJ3X82:otYM2krxnuHhAofO4oqIV7wcfdU='])\n \n     def test_request_signing6(self):\n         # uploads an object to a CNAME style virtual hosted bucket with metadata.\n+        date = 'Tue, 27 Mar 2007 21:06:08 +0000'\n         req = Request('s3://static.johnsmith.net:8080/db-backup.dat.gz', \\\n                 method='PUT', headers={\n                     'User-Agent': 'curl/7.15.5',\n                     'Host': 'static.johnsmith.net:8080',\n-                    'Date': 'Tue, 27 Mar 2007 21:06:08 +0000',\n+                    'Date': date,\n                     'x-amz-acl': 'public-read',\n                     'content-type': 'application/x-download',\n                     'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',\n@@ -548,19 +579,21 @@ class S3TestCase(unittest.TestCase):\n                     'Content-Encoding': 'gzip',\n                     'Content-Length': '5913339',\n                     })\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n \n     def test_request_signing7(self):\n         # ensure that spaces are quoted properly before signing\n+        date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n         req = Request(\n             (\"s3://johnsmith/photos/my puppy.jpg\"\n              \"?response-content-disposition=my puppy.jpg\"),\n             method='GET',\n-            headers={\n-                'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n-            })\n+            headers={'Date': date},\n+            )\n+        with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(\n             httpreq.headers['Authorization'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6bea3bf2eb4793555366a3341fe41456704b860", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 472 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -546,6 +546,11 @@ class S3TestCase(BaseS3TestCase):\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n \n     def test_request_signing5(self):\n+        try: import botocore\n+        except ImportError: pass\n+        else:\n+            raise unittest.SkipTest(\n+                'botocore does not support overriding date with x-amz-date')\n         # deletes an object from the 'johnsmith' bucket using the\n         # path-style and Date alternative.\n         date = 'Tue, 27 Mar 2007 21:20:27 +0000'\n@@ -557,9 +562,8 @@ class S3TestCase(BaseS3TestCase):\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         # botocore does not override Date with x-amz-date\n-        self.assertIn(httpreq.headers['Authorization'], [\n-                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=',\n-                b'AWS 0PN5J17HBGZHT7JJ3X82:otYM2krxnuHhAofO4oqIV7wcfdU='])\n+        self.assertEqual(httpreq.headers['Authorization'],\n+                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n \n     def test_request_signing6(self):\n         # uploads an object to a CNAME style virtual hosted bucket with metadata.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcb92b50dc1d1106ee418d6d7a701d87cde4010c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 672 | Contributors (this commit): 9 | Commits (past 90d): 27 | Contributors (cumulative): 13 | DMM Complexity: 0.5\n\nDIFF:\n@@ -68,6 +68,9 @@ class S3DownloadHandler(object):\n             except Exception as ex:\n                 raise NotConfigured(str(ex))\n         else:\n+            kw.pop('anon', None)\n+            if kw:\n+                raise TypeError('Unexpected keyword arguments: %s' % kw)\n             if not self.anon:\n                 SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n                 self._signer = SignerCls(botocore.credentials.Credentials(\n\n@@ -500,6 +500,10 @@ class S3TestCase(BaseS3TestCase):\n                 mock_formatdate.return_value = date\n                 yield\n \n+    def test_extra_kw(self):\n+        with self.assertRaises(TypeError):\n+            S3DownloadHandler(Settings(), extra_kw=True)\n+\n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n         date ='Tue, 27 Mar 2007 19:36:42 +0000'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19b2910ad145cb4b86ed621e7045f0afdf810d7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 40 | Churn Cumulative: 705 | Contributors (this commit): 11 | Commits (past 90d): 23 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ This module contains some assorted functions used in tests\n import os\n \n from importlib import import_module\n+import six\n from twisted.trial.unittest import SkipTest\n \n \n@@ -12,14 +13,22 @@ def assert_aws_environ():\n     \"\"\"Asserts the current environment is suitable for running AWS testsi.\n     Raises SkipTest with the reason if it's not.\n     \"\"\"\n-    try:\n-        import boto\n-    except ImportError as e:\n-        raise SkipTest(str(e))\n-\n+    skip_if_no_boto()\n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n         raise SkipTest(\"AWS keys not found\")\n \n+def skip_if_no_boto():\n+    try:\n+        import botocore\n+    except ImportError:\n+        if six.PY2:\n+            try:\n+                import boto\n+            except ImportError:\n+                raise SkipTest('missing botocore or boto library')\n+        else:\n+            raise SkipTest('missing botocore library')\n+\n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n\n@@ -28,7 +28,7 @@ from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n@@ -437,22 +437,10 @@ class HttpDownloadHandlerMock(object):\n         return request\n \n \n-class BaseS3TestCase(unittest.TestCase):\n-    try:\n-        import botocore\n-    except ImportError:\n-        if six.PY2:\n-            try:\n-                import boto\n-            except ImportError:\n-                skip = 'missing botocore or boto library'\n-        else:\n-            skip = 'missing botocore library'\n-\n-\n-class S3AnonTestCase(BaseS3TestCase):\n+class S3AnonTestCase(unittest.TestCase):\n \n     def setUp(self):\n+        skip_if_no_boto()\n         self.s3reqh = S3DownloadHandler(Settings(),\n                 httpdownloadhandler=HttpDownloadHandlerMock,\n                 #anon=True, # is implicit\n@@ -469,7 +457,7 @@ class S3AnonTestCase(BaseS3TestCase):\n             httpreq.url, 'http://aws-publicdatasets.s3.amazonaws.com/')\n \n \n-class S3TestCase(BaseS3TestCase):\n+class S3TestCase(unittest.TestCase):\n     download_handler_cls = S3DownloadHandler\n \n     # test use same example keys than amazon developer guide\n@@ -480,6 +468,7 @@ class S3TestCase(BaseS3TestCase):\n     AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n \n     def setUp(self):\n+        skip_if_no_boto()\n         s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID,\n                 self.AWS_SECRET_ACCESS_KEY,\n                 httpdownloadhandler=HttpDownloadHandlerMock)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d2f067458ce8a3e39d717afae2034dace5db54c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 125 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -102,8 +102,10 @@ class S3FeedStorageTest(unittest.TestCase):\n         file.write(\"content\")\n         yield storage.store(file)\n         u = urlparse(uri)\n-        key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n+        bucket = connect_s3().get_bucket(u.hostname, validate=False)\n+        key = bucket.get_key(u.path)\n         self.assertEqual(key.get_contents_as_string(), \"content\")\n+        bucket.delete_key(u.path)\n \n \n class StdoutFeedStorageTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ada45a9bb6adcfe5546a515a0574e7efe94c720", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 40 | Files Changed: 5 | Hunks: 18 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/4 | Churn Δ: 111 | Churn Cumulative: 678 | Contributors (this commit): 12 | Commits (past 90d): 16 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,17 +1,13 @@\n-import six\n from six.moves.urllib.parse import unquote\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_unicode\n+from scrapy.utils.boto import is_botocore\n from .http import HTTPDownloadHandler\n \n \n-def get_s3_connection():\n-    try:\n+def _get_boto_connection():\n     from boto.s3.connection import S3Connection\n-    except ImportError:\n-        return None\n \n     class _v19_S3Connection(S3Connection):\n         \"\"\"A dummy S3Connection wrapper that doesn't do any synchronous download\"\"\"\n@@ -53,21 +49,9 @@ class S3DownloadHandler(object):\n         self.anon = kw.get('anon')\n \n         self._signer = None\n-        try:\n+        if is_botocore():\n             import botocore.auth\n             import botocore.credentials\n-        except ImportError:\n-            if six.PY3:\n-                raise NotConfigured(\"missing botocore library\")\n-            _S3Connection = get_s3_connection()\n-            if _S3Connection is None:\n-                raise NotConfigured(\"missing botocore or boto library\")\n-            try:\n-                self.conn = _S3Connection(\n-                    aws_access_key_id, aws_secret_access_key, **kw)\n-            except Exception as ex:\n-                raise NotConfigured(str(ex))\n-        else:\n             kw.pop('anon', None)\n             if kw:\n                 raise TypeError('Unexpected keyword arguments: %s' % kw)\n@@ -75,6 +59,13 @@ class S3DownloadHandler(object):\n                 SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n                 self._signer = SignerCls(botocore.credentials.Credentials(\n                     aws_access_key_id, aws_secret_access_key))\n+        else:\n+            _S3Connection = _get_boto_connection()\n+            try:\n+                self.conn = _S3Connection(\n+                    aws_access_key_id, aws_secret_access_key, **kw)\n+            except Exception as ex:\n+                raise NotConfigured(str(ex))\n \n         self._download_http = httpdownloadhandler(settings).download_request\n \n\n@@ -24,6 +24,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import without_none_values\n+from scrapy.utils.boto import is_botocore\n \n logger = logging.getLogger(__name__)\n \n@@ -90,19 +91,28 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n         from scrapy.conf import settings\n-        try:\n-            import boto\n-        except ImportError:\n-            raise NotConfigured\n-        self.connect_s3 = boto.connect_s3\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or settings['AWS_ACCESS_KEY_ID']\n         self.secret_key = u.password or settings['AWS_SECRET_ACCESS_KEY']\n-        self.keyname = u.path\n+        self.is_botocore = is_botocore()\n+        self.keyname = u.path[1:]  # remove first \"/\"\n+        if self.is_botocore:\n+            import botocore.session\n+            session = botocore.session.get_session()\n+            self.s3_client = session.create_client(\n+                's3', aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key)\n+        else:\n+            import boto\n+            self.connect_s3 = boto.connect_s3\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n+        if self.is_botocore:\n+            self.s3_client.put_object(\n+                Bucket=self.bucketname, Key=self.keyname, Body=file)\n+        else:\n             conn = self.connect_s3(self.access_key, self.secret_key)\n             bucket = conn.get_bucket(self.bucketname, validate=False)\n             key = bucket.new_key(self.keyname)\n\n@@ -0,0 +1,20 @@\n+\"\"\"Boto/botocore helpers\"\"\"\n+\n+import six\n+\n+from scrapy.exceptions import NotConfigured\n+\n+\n+def is_botocore():\n+    try:\n+        import botocore\n+        return True\n+    except ImportError:\n+        if six.PY2:\n+            try:\n+                import boto\n+                return False\n+            except ImportError:\n+                raise NotConfigured('missing botocore or boto library')\n+        else:\n+            raise NotConfigured('missing botocore library')\n\n@@ -5,9 +5,11 @@ This module contains some assorted functions used in tests\n import os\n \n from importlib import import_module\n-import six\n from twisted.trial.unittest import SkipTest\n \n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.boto import is_botocore\n+\n \n def assert_aws_environ():\n     \"\"\"Asserts the current environment is suitable for running AWS testsi.\n@@ -19,15 +21,9 @@ def assert_aws_environ():\n \n def skip_if_no_boto():\n     try:\n-        import botocore\n-    except ImportError:\n-        if six.PY2:\n-            try:\n-                import boto\n-            except ImportError:\n-                raise SkipTest('missing botocore or boto library')\n-        else:\n-            raise SkipTest('missing botocore library')\n+        is_botocore()\n+    except NotConfigured as e:\n+        raise SkipTest(e.message)\n \n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n\n@@ -22,6 +22,7 @@ from scrapy.extensions.feedexport import (\n )\n from scrapy.utils.test import assert_aws_environ\n from scrapy.utils.python import to_native_str\n+from scrapy.utils.boto import is_botocore\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -95,17 +96,30 @@ class S3FeedStorageTest(unittest.TestCase):\n         uri = os.environ.get('FEEDTEST_S3_URI')\n         if not uri:\n             raise unittest.SkipTest(\"No S3 URI available for testing\")\n-        from boto import connect_s3\n         storage = S3FeedStorage(uri)\n         verifyObject(IFeedStorage, storage)\n         file = storage.open(scrapy.Spider(\"default\"))\n         file.write(\"content\")\n         yield storage.store(file)\n         u = urlparse(uri)\n-        bucket = connect_s3().get_bucket(u.hostname, validate=False)\n-        key = bucket.get_key(u.path)\n-        self.assertEqual(key.get_contents_as_string(), \"content\")\n-        bucket.delete_key(u.path)\n+        content = self._get_content_and_delete(u.hostname, u.path[1:])\n+        self.assertEqual(content, \"content\")\n+\n+    def _get_content_and_delete(self, bucket, path):\n+        if is_botocore():\n+            import botocore.session\n+            session = botocore.session.get_session()\n+            client = session.create_client('s3')\n+            key = client.get_object(Bucket=bucket, Key=path)\n+            content = key['Body'].read()\n+            client.delete_object(Bucket=bucket, Key=path)\n+        else:\n+            from boto import connect_s3\n+            bucket = connect_s3().get_bucket(bucket, validate=False)\n+            key = bucket.get_key(path)\n+            content = key.get_contents_as_string()\n+            bucket.delete_key(path)\n+        return content\n \n \n class StdoutFeedStorageTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1470e85a2987ad53c3b3a89d801a846a053e133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 154 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -99,11 +99,12 @@ class S3FeedStorageTest(unittest.TestCase):\n         storage = S3FeedStorage(uri)\n         verifyObject(IFeedStorage, storage)\n         file = storage.open(scrapy.Spider(\"default\"))\n-        file.write(\"content\")\n+        expected_content = b\"content: \\xe2\\x98\\x83\"\n+        file.write(expected_content)\n         yield storage.store(file)\n         u = urlparse(uri)\n         content = self._get_content_and_delete(u.hostname, u.path[1:])\n-        self.assertEqual(content, \"content\")\n+        self.assertEqual(content, expected_content)\n \n     def _get_content_and_delete(self, bucket, path):\n         if is_botocore():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32cd8c91654a8442495cc00aebf5322d1fbc644b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 69 | Churn Cumulative: 474 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n+from __future__ import absolute_import\n import os\n \n from importlib import import_module\n@@ -25,6 +26,25 @@ def skip_if_no_boto():\n     except NotConfigured as e:\n         raise SkipTest(e.message)\n \n+def get_s3_content_and_delete(bucket, path):\n+    \"\"\" Get content from s3 key, and delete key afterwards.\n+    \"\"\"\n+    if is_botocore():\n+        import botocore.session\n+        session = botocore.session.get_session()\n+        client = session.create_client('s3')\n+        key = client.get_object(Bucket=bucket, Key=path)\n+        content = key['Body'].read()\n+        client.delete_object(Bucket=bucket, Key=path)\n+    else:\n+        import boto\n+        # assuming boto=2.2.2\n+        bucket = boto.connect_s3().get_bucket(bucket, validate=False)\n+        key = bucket.get_key(path)\n+        content = key.get_contents_as_string()\n+        bucket.delete_key(path)\n+    return content\n+\n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n\n@@ -20,7 +20,7 @@ from scrapy.extensions.feedexport import (\n     IFeedStorage, FileFeedStorage, FTPFeedStorage,\n     S3FeedStorage, StdoutFeedStorage\n )\n-from scrapy.utils.test import assert_aws_environ\n+from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n from scrapy.utils.python import to_native_str\n from scrapy.utils.boto import is_botocore\n \n@@ -93,7 +93,7 @@ class S3FeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store(self):\n         assert_aws_environ()\n-        uri = os.environ.get('FEEDTEST_S3_URI')\n+        uri = os.environ.get('S3_TEST_FILE_URI')\n         if not uri:\n             raise unittest.SkipTest(\"No S3 URI available for testing\")\n         storage = S3FeedStorage(uri)\n@@ -103,25 +103,9 @@ class S3FeedStorageTest(unittest.TestCase):\n         file.write(expected_content)\n         yield storage.store(file)\n         u = urlparse(uri)\n-        content = self._get_content_and_delete(u.hostname, u.path[1:])\n+        content = get_s3_content_and_delete(u.hostname, u.path[1:])\n         self.assertEqual(content, expected_content)\n \n-    def _get_content_and_delete(self, bucket, path):\n-        if is_botocore():\n-            import botocore.session\n-            session = botocore.session.get_session()\n-            client = session.create_client('s3')\n-            key = client.get_object(Bucket=bucket, Key=path)\n-            content = key['Body'].read()\n-            client.delete_object(Bucket=bucket, Key=path)\n-        else:\n-            from boto import connect_s3\n-            bucket = connect_s3().get_bucket(bucket, validate=False)\n-            key = bucket.get_key(path)\n-            content = key.get_contents_as_string()\n-            bucket.delete_key(path)\n-        return content\n-\n \n class StdoutFeedStorageTest(unittest.TestCase):\n \n\n@@ -4,15 +4,18 @@ import hashlib\n import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n+from six.moves.urllib.parse import urlparse\n+from six import BytesIO\n \n from twisted.trial import unittest\n from twisted.internet import defer\n \n-from scrapy.pipelines.files import FilesPipeline, FSFilesStore\n+from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n+from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n \n from tests import mock\n \n@@ -179,6 +182,28 @@ class FilesPipelineTestCaseFields(unittest.TestCase):\n             self.assertEqual(item['stored_file'], [results[0][1]])\n \n \n+class TestS3FilesStore(unittest.TestCase):\n+    @defer.inlineCallbacks\n+    def test_persist(self):\n+        assert_aws_environ()\n+        uri = os.environ.get('S3_TEST_FILE_URI')\n+        if not uri:\n+            raise unittest.SkipTest(\"No S3 URI available for testing\")\n+        data = b\"TestS3FilesStore: \\xe2\\x98\\x83\"\n+        buf = BytesIO(data)\n+        meta = {'foo': 'bar'}\n+        path = ''\n+        store = S3FilesStore(uri)\n+        yield store.persist_file(path, buf, info=None, meta=meta)\n+        s = yield store.stat_file(path, info=None)\n+        self.assertIn('last_modified', s)\n+        self.assertIn('checksum', s)\n+        self.assertEqual(s['checksum'], b'3187896a9657a28163abb31667df64c8')\n+        u = urlparse(uri)\n+        content = get_s3_content_and_delete(u.hostname, u.path[1:])\n+        self.assertEqual(content, data)\n+\n+\n class ItemWithFiles(Item):\n     file_urls = Field()\n     files = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfc567f48e934dc5b148c681c11a73f974e0d37c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 42 | Churn Cumulative: 278 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.11538461538461539\n\nDIFF:\n@@ -28,6 +28,7 @@ from scrapy.utils.misc import md5sum\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n+from scrapy.utils.boto import is_botocore\n \n logger = logging.getLogger(__name__)\n \n@@ -86,16 +87,26 @@ class S3FilesStore(object):\n     }\n \n     def __init__(self, uri):\n-        try:\n+        self.is_botocore = is_botocore()\n+        if self.is_botocore:\n+            import botocore.session\n+            session = botocore.session.get_session()\n+            self.s3_client = session.create_client(\n+                's3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n+                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)\n+        else:\n             from boto.s3.connection import S3Connection\n             self.S3Connection = S3Connection\n-        except ImportError:\n-            raise NotConfigured(\"missing boto library\")\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n     def stat_file(self, path, info):\n         def _onsuccess(boto_key):\n+            if self.is_botocore:\n+                checksum = boto_key['ETag'].strip('\"')\n+                last_modified = boto_key['LastModified']\n+                modified_stamp = time.mktime(last_modified.timetuple())\n+            else:\n                 checksum = boto_key.etag.strip('\"')\n                 last_modified = boto_key.last_modified\n                 modified_tuple = parsedate_tz(last_modified)\n@@ -111,14 +122,30 @@ class S3FilesStore(object):\n         return c.get_bucket(self.bucket, validate=False)\n \n     def _get_boto_key(self, path):\n-        b = self._get_boto_bucket()\n         key_name = '%s%s' % (self.prefix, path)\n+        if self.is_botocore:\n+            return threads.deferToThread(\n+                self.s3_client.head_object,\n+                Bucket=self.bucket,\n+                Key=key_name)\n+        else:\n+            b = self._get_boto_bucket()\n             return threads.deferToThread(b.get_key, key_name)\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         \"\"\"Upload file to S3 storage\"\"\"\n-        b = self._get_boto_bucket()\n         key_name = '%s%s' % (self.prefix, path)\n+        buf.seek(0)\n+        if self.is_botocore:\n+            return threads.deferToThread(\n+                self.s3_client.put_object,\n+                Bucket=self.bucket,\n+                Key=key_name,\n+                Body=buf,\n+                Metadata={k: str(v) for k, v in six.iteritems(meta)},\n+                ACL=self.POLICY)\n+        else:\n+            b = self._get_boto_bucket()\n             k = b.new_key(key_name)\n             if meta:\n                 for metakey, metavalue in six.iteritems(meta):\n@@ -126,8 +153,8 @@ class S3FilesStore(object):\n             h = self.HEADERS.copy()\n             if headers:\n                 h.update(headers)\n-        buf.seek(0)\n-        return threads.deferToThread(k.set_contents_from_string, buf.getvalue(),\n+            return threads.deferToThread(\n+                k.set_contents_from_string, buf.getvalue(),\n                 headers=h, policy=self.POLICY)\n \n \n\n@@ -22,7 +22,6 @@ from scrapy.extensions.feedexport import (\n )\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n from scrapy.utils.python import to_native_str\n-from scrapy.utils.boto import is_botocore\n \n \n class FileFeedStorageTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cb7a567ea0f0623fb77bd19b652d89625189f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -198,7 +198,7 @@ class TestS3FilesStore(unittest.TestCase):\n         s = yield store.stat_file(path, info=None)\n         self.assertIn('last_modified', s)\n         self.assertIn('checksum', s)\n-        self.assertEqual(s['checksum'], b'3187896a9657a28163abb31667df64c8')\n+        self.assertEqual(s['checksum'], '3187896a9657a28163abb31667df64c8')\n         u = urlparse(uri)\n         content = get_s3_content_and_delete(u.hostname, u.path[1:])\n         self.assertEqual(content, data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77ebb136840acaa700b6c0ada5e028217a503cfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 501 | Contributors (this commit): 7 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -490,8 +490,8 @@ class S3TestCase(unittest.TestCase):\n                 yield\n \n     def test_extra_kw(self):\n-        with self.assertRaises(TypeError):\n-            S3DownloadHandler(Settings(), extra_kw=True)\n+        self.assertRaises(\n+            TypeError, S3DownloadHandler, Settings(), extra_kw=True)\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1ecb8cd38bd8922780731682b696adc97974240", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 509 | Contributors (this commit): 7 | Commits (past 90d): 24 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -490,8 +490,12 @@ class S3TestCase(unittest.TestCase):\n                 yield\n \n     def test_extra_kw(self):\n-        self.assertRaises(\n-            TypeError, S3DownloadHandler, Settings(), extra_kw=True)\n+        try:\n+            S3DownloadHandler(Settings(), extra_kw=True)\n+        except Exception as e:\n+            self.assertIsInstance(e, (TypeError, NotConfigured))\n+        else:\n+            assert False\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49313a6988bb9161f9036098c454a180d48ca4b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 21 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n \"\"\"Boto/botocore helpers\"\"\"\n \n+from __future__ import absolute_import\n import six\n \n from scrapy.exceptions import NotConfigured\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cabed6f183cfb2ab778c57be8c75802fec5e54d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 99 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,7 +59,8 @@ class ResponseTypes(object):\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n+            filename = to_native_str(content_disposition,\n+                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:\n\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n import unittest\n from scrapy.responsetypes import responsetypes\n \n@@ -20,8 +21,14 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_content_disposition(self):\n         mappings = [\n-            ('attachment; filename=\"data.xml\"', XmlResponse),\n-            ('attachment; filename=data.xml', XmlResponse),\n+            (b'attachment; filename=\"data.xml\"', XmlResponse),\n+            (b'attachment; filename=data.xml', XmlResponse),\n+            (u'attachment;filename=data£.tar.gz'.encode('utf-8'), Response),\n+            (u'attachment;filename=dataµ.tar.gz'.encode('latin-1'), Response),\n+            (u'attachment;filename=data高.doc'.encode('gbk'), Response),\n+            (u'attachment;filename=دورهdata.html'.encode('cp720'), HtmlResponse),\n+            (u'attachment;filename=日本語版Wikipedia.xml'.encode('iso2022_jp'), XmlResponse),\n+\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_disposition(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06da7af9e293d00087799ac819f9ac4caa4901e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 41 | Churn Cumulative: 197 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,7 @@ from time import sleep\n from os.path import exists, join, abspath\n from shutil import rmtree, copytree\n from tempfile import mkdtemp\n-import six\n+from contextlib import contextmanager\n \n from twisted.trial import unittest\n from twisted.internet import defer\n@@ -154,12 +154,24 @@ class MiscCommandsTest(CommandTest):\n \n class RunSpiderCommandTest(CommandTest):\n \n-    def test_runspider(self):\n+    @contextmanager\n+    def _create_file(self, content, name):\n         tmpdir = self.mktemp()\n         os.mkdir(tmpdir)\n-        fname = abspath(join(tmpdir, 'myspider.py'))\n+        fname = abspath(join(tmpdir, name))\n         with open(fname, 'w') as f:\n-            f.write(\"\"\"\n+            f.write(content)\n+        try:\n+            yield fname\n+        finally:\n+            rmtree(tmpdir)\n+\n+    def runspider(self, code, name='myspider.py'):\n+        with self._create_file(code, name) as fname:\n+            return self.proc('runspider', fname)\n+\n+    def test_runspider(self):\n+        spider = \"\"\"\n import scrapy\n \n class MySpider(scrapy.Spider):\n@@ -168,23 +180,17 @@ class MySpider(scrapy.Spider):\n     def start_requests(self):\n         self.logger.debug(\"It Works!\")\n         return []\n-\"\"\")\n-        p = self.proc('runspider', fname)\n+\"\"\"\n+        p = self.runspider(spider)\n         log = to_native_str(p.stderr.read())\n+\n         self.assertIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n         self.assertIn(\"INFO: Closing spider (finished)\", log)\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n     def test_runspider_no_spider_found(self):\n-        tmpdir = self.mktemp()\n-        os.mkdir(tmpdir)\n-        fname = abspath(join(tmpdir, 'myspider.py'))\n-        with open(fname, 'w') as f:\n-            f.write(\"\"\"\n-from scrapy.spiders import Spider\n-\"\"\")\n-        p = self.proc('runspider', fname)\n+        p = self.runspider(\"from scrapy.spiders import Spider\\n\")\n         log = to_native_str(p.stderr.read())\n         self.assertIn(\"No spider found in file\", log)\n \n@@ -194,12 +200,7 @@ from scrapy.spiders import Spider\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n-        tmpdir = self.mktemp()\n-        os.mkdir(tmpdir)\n-        fname = abspath(join(tmpdir, 'myspider.txt'))\n-        with open(fname, 'w') as f:\n-            f.write(\"\")\n-        p = self.proc('runspider', fname)\n+        p = self.runspider(\"\", \"myspider.txt\")\n         log = to_native_str(p.stderr.read())\n         self.assertIn(\"Unable to load\", log)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#617631f2646d349c4bdc288b3d7b41ad483f5ec6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 284 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -82,7 +82,7 @@ class S3DownloadHandler(object):\n             awsrequest = botocore.awsrequest.AWSRequest(\n                 method=request.method,\n                 url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),\n-                headers=request.headers.to_native_string_dict(),\n+                headers=request.headers.to_unicode_dict(),\n                 data=request.body)\n             self._signer.add_auth(awsrequest)\n             request = request.replace(\n\n@@ -79,7 +79,10 @@ class Headers(CaselessDict):\n     def to_string(self):\n         return headers_dict_to_raw(self)\n \n-    def to_native_string_dict(self):\n+    def to_unicode_dict(self):\n+        \"\"\" Return headers as a CaselessDict with unicode keys\n+        and unicode values. Multiple values are joined with ','.\n+        \"\"\"\n         return CaselessDict(\n             (to_unicode(key, encoding=self.encoding),\n              to_unicode(b','.join(value), encoding=self.encoding))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d61fbcc8b5fee1c2407d42fb72bd72bb18d40e25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 58 | Churn Cumulative: 459 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: 0.9047619047619048\n\nDIFF:\n@@ -29,6 +29,7 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n+from scrapy.utils.datatypes import CaselessDict\n \n logger = logging.getLogger(__name__)\n \n@@ -137,13 +138,17 @@ class S3FilesStore(object):\n         key_name = '%s%s' % (self.prefix, path)\n         buf.seek(0)\n         if self.is_botocore:\n+            extra = self._headers_to_botocore_kwargs(self.HEADERS)\n+            if headers:\n+                extra.update(self._headers_to_botocore_kwargs(headers))\n             return threads.deferToThread(\n                 self.s3_client.put_object,\n                 Bucket=self.bucket,\n                 Key=key_name,\n                 Body=buf,\n                 Metadata={k: str(v) for k, v in six.iteritems(meta)},\n-                ACL=self.POLICY)\n+                ACL=self.POLICY,\n+                **extra)\n         else:\n             b = self._get_boto_bucket()\n             k = b.new_key(key_name)\n@@ -157,6 +162,35 @@ class S3FilesStore(object):\n                 k.set_contents_from_string, buf.getvalue(),\n                 headers=h, policy=self.POLICY)\n \n+    def _headers_to_botocore_kwargs(self, headers):\n+        \"\"\" Convert headers to botocore keyword agruments.\n+        \"\"\"\n+        # This is required while we need to support both boto and botocore.\n+        mapping = CaselessDict({\n+            'Content-Type': 'ContentType',\n+            'Cache-Control': 'CacheControl',\n+            'Content-Disposition': 'ContentDisposition',\n+            'Content-Encoding': 'ContentEncoding',\n+            'Content-Language': 'ContentLanguage',\n+            'Content-Length': 'ContentLength',\n+            'Content-MD5': 'ContentMD5',\n+            'Expires': 'Expires',\n+            'X-Amz-Grant-Full-Control': 'GrantFullControl',\n+            'X-Amz-Grant-Read': 'GrantRead',\n+            'X-Amz-Grant-Read-ACP': 'GrantReadACP',\n+            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',\n+            })\n+        extra = {}\n+        for key, value in six.iteritems(headers):\n+            try:\n+                kwarg = mapping[key]\n+            except KeyError:\n+                raise TypeError(\n+                    'Header \"%s\" is not supported by botocore' % key)\n+            else:\n+                extra[kwarg] = value\n+        return extra\n+\n \n class FilesPipeline(MediaPipeline):\n     \"\"\"Abstract pipeline that implement the file downloading\n\n@@ -26,7 +26,7 @@ def skip_if_no_boto():\n     except NotConfigured as e:\n         raise SkipTest(e.message)\n \n-def get_s3_content_and_delete(bucket, path):\n+def get_s3_content_and_delete(bucket, path, with_key=False):\n     \"\"\" Get content from s3 key, and delete key afterwards.\n     \"\"\"\n     if is_botocore():\n@@ -43,7 +43,7 @@ def get_s3_content_and_delete(bucket, path):\n         key = bucket.get_key(path)\n         content = key.get_contents_as_string()\n         bucket.delete_key(path)\n-    return content\n+    return (content, key) if with_key else content\n \n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n\n@@ -16,6 +16,7 @@ from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n+from scrapy.utils.boto import is_botocore\n \n from tests import mock\n \n@@ -194,14 +195,27 @@ class TestS3FilesStore(unittest.TestCase):\n         meta = {'foo': 'bar'}\n         path = ''\n         store = S3FilesStore(uri)\n-        yield store.persist_file(path, buf, info=None, meta=meta)\n+        yield store.persist_file(\n+            path, buf, info=None, meta=meta,\n+            headers={'Content-Type': 'image/png'})\n         s = yield store.stat_file(path, info=None)\n         self.assertIn('last_modified', s)\n         self.assertIn('checksum', s)\n         self.assertEqual(s['checksum'], '3187896a9657a28163abb31667df64c8')\n         u = urlparse(uri)\n-        content = get_s3_content_and_delete(u.hostname, u.path[1:])\n+        content, key = get_s3_content_and_delete(\n+            u.hostname, u.path[1:], with_key=True)\n         self.assertEqual(content, data)\n+        if is_botocore():\n+            self.assertEqual(key['Metadata'], {'foo': 'bar'})\n+            self.assertEqual(\n+                key['CacheControl'], S3FilesStore.HEADERS['Cache-Control'])\n+            self.assertEqual(key['ContentType'], 'image/png')\n+        else:\n+            self.assertEqual(key.metadata, {'foo': 'bar'})\n+            self.assertEqual(\n+                key.cache_control, S3FilesStore.HEADERS['Cache-Control'])\n+            self.assertEqual(key.content_type, 'image/png')\n \n \n class ItemWithFiles(Item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
