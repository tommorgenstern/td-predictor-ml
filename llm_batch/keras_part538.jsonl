{"custom_id": "keras#bb783b7446797bf77adf9dc45490c5427142f24f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 687 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -95,11 +95,10 @@ def image_dataset_from_directory(\n           - None (no labels).\n       class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n           list of class names (must match names of subdirectories). Used\n-          to control the order of the classes\n-          (otherwise alphanumerical order is used).\n+          to control the order of the classes (otherwise alphanumerical order\n+          is used).\n       color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n-          Whether the images will be converted to\n-          have 1, 3, or 4 channels.\n+          Whether the images will be converted to have 1, 3, or 4 channels.\n       batch_size: Size of the batches of data. Default: 32.\n             If `None`, the data will not be batched\n             (the dataset will yield individual samples).\n@@ -123,11 +122,12 @@ def image_dataset_from_directory(\n       follow_links: Whether to visit subdirectories pointed to by symlinks.\n           Defaults to False.\n       crop_to_aspect_ratio: If True, resize the images without aspect\n-        ratio distortion. When the original aspect ratio differs from the target\n-        aspect ratio, the output image will be cropped so as to return the\n-        largest possible window in the image (of size `image_size`) that matches\n-        the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n-        aspect ratio may not be preserved.\n+            ratio distortion. When the original aspect ratio differs from the\n+            target aspect ratio, the output image will be cropped so as to\n+            return the largest possible window in the image\n+            (of size `image_size`) that matches the target aspect ratio. By\n+            default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n+            preserved.\n       **kwargs: Legacy keyword arguments.\n \n     Returns:\n@@ -268,6 +268,7 @@ def image_dataset_from_directory(\n         )\n         train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n         val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n+\n         if batch_size is not None:\n             if shuffle:\n                 # Shuffle locally at each iteration\n@@ -285,6 +286,7 @@ def image_dataset_from_directory(\n         # Users may need to reference `class_names`.\n         train_dataset.class_names = class_names\n         val_dataset.class_names = class_names\n+\n         # Include file paths for images as attribute.\n         train_dataset.file_paths = image_paths_train\n         val_dataset.file_paths = image_paths_val\n@@ -321,6 +323,7 @@ def image_dataset_from_directory(\n \n         # Users may need to reference `class_names`.\n         dataset.class_names = class_names\n+\n         # Include file paths for images as attribute.\n         dataset.file_paths = image_paths\n     return dataset\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f0183616aa4e67af26d8ee9eea6fe73d0476eb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 695 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -80,8 +80,8 @@ def image_dataset_from_directory(\n             (labels are generated from the directory structure),\n             None (no labels),\n             or a list/tuple of integer labels of the same size as the number of\n-            image files found in the directory. Labels should be sorted according\n-            to the alphanumeric order of the image file paths\n+            image files found in the directory. Labels should be sorted\n+            according to the alphanumeric order of the image file paths\n             (obtained via `os.walk(directory)` in Python).\n       label_mode: String describing the encoding of `labels`. Options are:\n           - 'int': means that the labels are encoded as integers\n@@ -136,8 +136,8 @@ def image_dataset_from_directory(\n         - If `label_mode` is None, it yields `float32` tensors of shape\n             `(batch_size, image_size[0], image_size[1], num_channels)`,\n             encoding images (see below for rules regarding `num_channels`).\n-        - Otherwise, it yields a tuple `(images, labels)`, where `images`\n-            has shape `(batch_size, image_size[0], image_size[1], num_channels)`,\n+        - Otherwise, it yields a tuple `(images, labels)`, where `images` has\n+            shape `(batch_size, image_size[0], image_size[1], num_channels)`,\n             and `labels` follows the format described below.\n \n     Rules regarding labels format:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74c043bdbab09800278c457104f5bc0677145b5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 21 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 56 | Churn Cumulative: 696 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,7 +34,7 @@ class Loss:\n             else:\n                 mask = None\n \n-            return reduce_weighted_loss(\n+            return reduce_weighted_values(\n                 losses,\n                 sample_weight=sample_weight,\n                 mask=mask,\n@@ -78,56 +78,56 @@ def squeeze_to_same_rank(x1, x2):\n     return x1, x2\n \n \n-def reduce_loss(losses, reduction=\"sum_over_batch_size\"):\n+def reduce_values(values, reduction=\"sum_over_batch_size\"):\n     if (\n         reduction is None\n-        or tuple(losses.shape) == ()\n-        or tuple(losses.shape) == (0,)\n+        or tuple(values.shape) == ()\n+        or tuple(values.shape) == (0,)\n     ):\n-        return losses\n-    loss = ops.sum(losses)\n+        return values\n+    loss = ops.sum(values)\n     if reduction == \"sum_over_batch_size\":\n-        loss /= ops.cast(ops.shape(losses)[0], loss.dtype)\n+        loss /= ops.cast(ops.shape(values)[0], loss.dtype)\n     return loss\n \n \n-def reduce_weighted_loss(\n-    losses,\n+def reduce_weighted_values(\n+    values,\n     sample_weight=None,\n     mask=None,\n     reduction=\"sum_over_batch_size\",\n ):\n     reduction = standardize_reduction(reduction)\n \n-    losses = ops.convert_to_tensor(losses)\n+    values = ops.convert_to_tensor(values)\n     if sample_weight is not None:\n-        sample_weight = ops.convert_to_tensor(sample_weight, dtype=losses.dtype)\n+        sample_weight = ops.convert_to_tensor(sample_weight, dtype=values.dtype)\n     if mask is not None:\n-        mask = ops.convert_to_tensor(mask, dtype=losses.dtype)\n+        mask = ops.convert_to_tensor(mask, dtype=values.dtype)\n \n     # Merge mask and sample weight into sample weight.\n     sample_weight = apply_mask(\n-        sample_weight, mask, dtype=losses.dtype, reduction=reduction\n+        sample_weight, mask, dtype=values.dtype, reduction=reduction\n     )\n \n     # Convert any non float dtypes to floats, to avoid loss of precision\n     # for dtype like int or bool.\n-    dtype = backend.standardize_dtype(losses.dtype)\n+    dtype = backend.standardize_dtype(values.dtype)\n     if not dtype_utils.is_float(dtype):\n-        input_dtype = losses.dtype\n-        losses = ops.cast(losses, \"float32\")\n+        input_dtype = values.dtype\n+        values = ops.cast(values, \"float32\")\n         input_casted = True\n     else:\n         input_casted = False\n \n     if sample_weight is not None:\n-        sample_weight = ops.cast(sample_weight, losses.dtype)\n+        sample_weight = ops.cast(sample_weight, values.dtype)\n         # Update dimensions of `sample_weight` to match `losses`.\n-        losses, sample_weight = squeeze_to_same_rank(losses, sample_weight)\n-        losses *= sample_weight\n+        values, sample_weight = squeeze_to_same_rank(values, sample_weight)\n+        values *= sample_weight\n \n     # Apply reduction function to the individual weighted losses.\n-    loss = reduce_loss(losses, reduction)\n+    loss = reduce_values(values, reduction)\n \n     if input_casted:\n         # Convert the result back to the input type.\n\n@@ -29,7 +29,7 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n     values_ndim = len(values.shape)\n     if values_ndim > 1:\n         return reduce_fn(values, axis=list(range(1, values_ndim)))\n-    return values\n+    return values, sample_weight\n \n \n @keras_core_export(\"keras_core.metrics.Sum\")\n@@ -59,7 +59,7 @@ class Sum(Metric):\n         self.total = self.add_variable(shape=(), initializer=initializers.Zeros(), dtype=self.dtype)\n \n     def update_state(self, values, sample_weight=None):\n-        values = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.sum, dtype=self.dtype)\n+        values, _ = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.sum, dtype=self.dtype)\n         self.total.assign(self.total + ops.sum(values))\n \n     def reset_state(self):\n@@ -85,7 +85,7 @@ class Mean(Metric):\n \n     Example:\n \n-    >>> m = tf.keras.metrics.Mean()\n+    >>> m = Mean()\n     >>> m.update_state([1, 3, 5, 7])\n     >>> m.result()\n     4.0\n@@ -101,9 +101,12 @@ class Mean(Metric):\n         self.count = self.add_variable(shape=(), initializer=initializers.Zeros(), dtype=\"int64\")\n \n     def update_state(self, values, sample_weight=None):\n-        values = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.mean, dtype=self.dtype)\n+        values, sample_weight = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.mean, dtype=self.dtype)\n         self.total.assign(self.total + ops.sum(values))\n-        num_samples = ops.shape(values)[0]\n+        if sample_weight is not None:\n+            num_samples = ops.sum(ops.ones(shape=(values.shape[0],)) * sample_weight)\n+        else:\n+            num_samples = values.shape[0]\n         self.count.assign(self.count + ops.cast(num_samples, dtype=\"int64\"))\n \n     def reset_state(self):\n\n@@ -31,7 +31,8 @@ from keras_core.metrics import reduction_metrics\n \n \n def mean_square_error(y_true, y_pred):\n-    return (y_true - y_pred) ** 2\n+    ndim = len(y_pred.shape)\n+    return ops.mean((y_true - y_pred) ** 2, axis=list(range(1, ndim)))\n \n \n class MeanSquareError(reduction_metrics.MeanMetricWrapper):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b851873765da919bd790b9ec833152d45e0320e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 157 | Churn Cumulative: 584 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.811965811965812\n\nDIFF:\n@@ -220,12 +220,12 @@ class CompileMetrics(metrics_module.Metric):\n                     raise ValueError(\n                         f\"When there is only a single output, the `{argument_name}` argument \"\n                         \"must be a list of metric objects. \"\n-                        f\"Received instead: {argument_name}={metrics} of type {type(metrics)}\"\n+                        f\"Received instead:\\n{argument_name}={metrics} of type {type(metrics)}\"\n                     )\n                 if not all(is_function_like(m) for m in metrics):\n                     raise ValueError(\n                         f\"Expected all entries in the `{argument_name}` list to be \"\n-                        f\"metric objects. Received instead: {argument_name}={metrics}\"\n+                        f\"metric objects. Received instead:\\n{argument_name}={metrics}\"\n                     )\n                 flat_metrics.append(\n                     MetricsList([get_metric(m, y_true[0], y_pred[0]) for m in metrics if m is not None])\n@@ -237,9 +237,16 @@ class CompileMetrics(metrics_module.Metric):\n                         \"For a model with multiple outputs, \"\n                         f\"when providing the `{argument_name}` argument as a list, \"\n                         \"it should have as many entries as the model has outputs. \"\n-                        f\"Received {argument_name}={metrics} of length {len(metrics)} \"\n+                        f\"Received:\\n{argument_name}={metrics}\\nof length {len(metrics)} \"\n                         f\"whereas the model has {len(y_pred)} outputs.\"\n                     )\n+                if not all(isinstance(mls, list) for mls in metrics):\n+                    raise ValueError(\n+                        \"For a model with multiple outputs, \"\n+                        f\"when providing the `{argument_name}` argument as a list, \"\n+                        \"each list entry should itself be a list (the list of metrics \"\n+                        f\"corresponding to that output). Received:\\n{argument_name}={metrics}\"\n+                    )\n                 for mls, yt, yp in zip(metrics, y_true, y_pred):\n                     if not all(is_function_like(e) for e in mls):\n                         raise ValueError(\n@@ -251,12 +258,26 @@ class CompileMetrics(metrics_module.Metric):\n                         MetricsList([get_metric(m, yt, yp) for m in mls if m is not None])\n                     )\n             elif isinstance(metrics, dict):\n+                if output_names is None:\n+                    raise ValueError(\n+                        f\"Argument `{argument_name}` can only be provided as a dict \"\n+                        \"when the model also returns a dict of outputs. Received \"\n+                        f\"{argument_name}={metrics}\"\n+                    )\n                 for name in metrics.keys():\n                     if name not in output_names:\n                         raise ValueError(\n-                            \"In the dict argument `{argument_name}`, key \"\n+                            f\"In the dict argument `{argument_name}`, key \"\n                             f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received: {argument_name}={metrics}\"\n+                            f\"Received:\\n{argument_name}={metrics}\"\n+                        )\n+                    if not isinstance(metrics[name], list):\n+                        raise ValueError(\n+                            \"For a model with multiple outputs, \"\n+                            f\"when providing the `{argument_name}` argument as a dict, \"\n+                            \"each dict entry should be a list (the list of metrics \"\n+                            \"corresponding to that output). \"\n+                            f\"At key '{name}', received invalid type:\\n{metrics[name]}\"\n                         )\n                     if not all(is_function_like(e) for e in metrics[name]):\n                         raise ValueError(\n@@ -329,8 +350,9 @@ class CompileMetrics(metrics_module.Metric):\n                     if name not in unique_name_counters:\n                         unique_name_counters[name] = 1\n                     else:\n-                        name = f\"{m.name}_{unique_name_counters[m.name]}\"\n-                        unique_name_counters[m.name] += 1\n+                        index = unique_name_counters[name]\n+                        unique_name_counters[name] += 1\n+                        name = f\"{name}_{index}\"\n                     results[name] = m.result()\n         return results\n \n\n@@ -15,24 +15,125 @@ class TestCompileMetrics(testing.TestCase):\n         y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))\n         compile_metrics.build(y_true, y_pred)\n         # Test eager build\n-        y_true, y_pred = np.ones((3, 4)), np.ones((3, 4))\n+        y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n+        y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n+        sample_weight = np.array([1, 0., 1])\n         compile_metrics.build(y_true, y_pred)\n \n         # Test update / result / reset flow\n-        sample_weight = np.array([1, 0, 1])\n         compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n-        y_pred += 1\n+        y_pred = np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]])\n         compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 2)\n-        self.assertAllClose(result[\"mean_square_error\"], 0.5)\n-        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.5)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.055833336)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0725)\n \n+        compile_metrics.reset_state()\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 2)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n \n-    # def test_metric_identifier_conversion(self):\n-    #     compile_metrics = CompileMetrics(\n-    #         metrics=[\n+    def test_list_output_case(self):\n+        compile_metrics = CompileMetrics(\n+            metrics=[\n+                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+            ],\n+            weighted_metrics=[\n+                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+            ],\n+        )\n+        # Test symbolic build\n+        y_true = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        y_pred = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        compile_metrics.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = [\n+            np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+            np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+        ]\n+        y_pred = [\n+            np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n+            np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n+        ]\n+        sample_weight = np.array([1, 0., 1])\n+        compile_metrics.build(y_true, y_pred)\n+\n+        # Test update / result / reset flow\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        y_pred = [\n+            np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n+            np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n+        ]\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 8)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.055833336)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0725)\n+\n+        compile_metrics.reset_state()\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 8)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n+\n+    def test_dict_output_case(self):\n+        compile_metrics = CompileMetrics(\n+            metrics={\n+                \"output_1\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                \"output_2\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+            },\n+            weighted_metrics={\n+                \"output_1\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                \"output_2\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+            },\n+        )\n+        # Test symbolic build\n+        y_true = {\n+            \"output_1\": backend.KerasTensor((3, 4)),\n+            \"output_2\": backend.KerasTensor((3, 4))\n+        }\n+        y_pred = {\n+            \"output_1\": backend.KerasTensor((3, 4)),\n+            \"output_2\": backend.KerasTensor((3, 4))\n+        }\n+        compile_metrics.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = {\n+            \"output_1\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+            \"output_2\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+        }\n+        y_pred = {\n+            \"output_1\": np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n+            \"output_2\": np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n+        }\n+        sample_weight = np.array([1, 0., 1])\n+        compile_metrics.build(y_true, y_pred)\n+\n+        # Test update / result / reset flow\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        y_pred = {\n+            \"output_1\": np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n+            \"output_2\": np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n+        }\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 8)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.055833336)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0725)\n+\n+        compile_metrics.reset_state()\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 8)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n \n-    #         ],\n-    #     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90df7ef9fce2c3e5e7b169a2f0aa359bcf9da121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 61 | Files Changed: 5 | Hunks: 51 | Methods Changed: 15 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 206 | Churn Cumulative: 3735 | Contributors (this commit): 3 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: 0.8024691358024691\n\nDIFF:\n@@ -1,9 +1,9 @@\n-from keras_core.losses import loss\n+from keras_core import backend\n+from keras_core import initializers\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n+from keras_core.losses import loss\n from keras_core.metrics.metric import Metric\n-from keras_core import initializers\n-from keras_core import backend\n \n \n def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n@@ -12,11 +12,11 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n     if sample_weight is not None:\n         sample_weight = ops.cast(sample_weight, dtype=dtype)\n         if mask is not None:\n-            sample_weight = loss.apply_mask(sample_weight, mask, dtype=dtype, reduction=\"sum\")\n-        # Update dimensions of weights to match with values if possible.\n-        values, sample_weight = loss.squeeze_to_same_rank(\n-            values, sample_weight\n+            sample_weight = loss.apply_mask(\n+                sample_weight, mask, dtype=dtype, reduction=\"sum\"\n             )\n+        # Update dimensions of weights to match with values if possible.\n+        values, sample_weight = loss.squeeze_to_same_rank(values, sample_weight)\n         # Reduce values to same ndim as weight array\n         weight_ndim = len(sample_weight.shape)\n         values_ndim = len(values.shape)\n@@ -56,14 +56,18 @@ class Sum(Metric):\n \n     def __init__(self, name=\"sum\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.total = self.add_variable(shape=(), initializer=initializers.Zeros(), dtype=self.dtype)\n+        self.total = self.add_variable(\n+            shape=(), initializer=initializers.Zeros(), dtype=self.dtype\n+        )\n \n     def update_state(self, values, sample_weight=None):\n-        values, _ = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.sum, dtype=self.dtype)\n+        values, _ = reduce_to_samplewise_values(\n+            values, sample_weight, reduce_fn=ops.sum, dtype=self.dtype\n+        )\n         self.total.assign(self.total + ops.sum(values))\n \n     def reset_state(self):\n-        self.total.assign(0.)\n+        self.total.assign(0.0)\n \n     def result(self):\n         return ops.identity(self.total)\n@@ -95,26 +99,37 @@ class Mean(Metric):\n     2.0\n     ```\n     \"\"\"\n+\n     def __init__(self, name=\"sum\", dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.total = self.add_variable(shape=(), initializer=initializers.Zeros(), dtype=self.dtype)\n-        self.count = self.add_variable(shape=(), initializer=initializers.Zeros(), dtype=\"int64\")\n+        self.total = self.add_variable(\n+            shape=(), initializer=initializers.Zeros(), dtype=self.dtype\n+        )\n+        self.count = self.add_variable(\n+            shape=(), initializer=initializers.Zeros(), dtype=\"int64\"\n+        )\n \n     def update_state(self, values, sample_weight=None):\n-        values, sample_weight = reduce_to_samplewise_values(values, sample_weight, reduce_fn=ops.mean, dtype=self.dtype)\n+        values, sample_weight = reduce_to_samplewise_values(\n+            values, sample_weight, reduce_fn=ops.mean, dtype=self.dtype\n+        )\n         self.total.assign(self.total + ops.sum(values))\n         if sample_weight is not None:\n-            num_samples = ops.sum(ops.ones(shape=(values.shape[0],)) * sample_weight)\n+            num_samples = ops.sum(\n+                ops.ones(shape=(values.shape[0],)) * sample_weight\n+            )\n         else:\n             num_samples = values.shape[0]\n         self.count.assign(self.count + ops.cast(num_samples, dtype=\"int64\"))\n \n     def reset_state(self):\n-        self.total.assign(0.)\n+        self.total.assign(0.0)\n         self.count.assign(0)\n \n     def result(self):\n-        return self.total / (ops.cast(self.count, dtype=self.dtype) + backend.epsilon())\n+        return self.total / (\n+            ops.cast(self.count, dtype=self.dtype) + backend.epsilon()\n+        )\n \n \n @keras_core_export(\"keras_core.metrics.MeanMetricWrapper\")\n@@ -152,13 +167,13 @@ class MeanMetricWrapper(Mean):\n         mask = getattr(y_pred, \"_keras_mask\", None)\n         values = self._fn(y_true, y_pred, **self._fn_kwargs)\n         if sample_weight is not None and mask is not None:\n-            sample_weight = loss.apply_mask(sample_weight, mask, dtype=self.dtype, reduction=\"sum\")\n+            sample_weight = loss.apply_mask(\n+                sample_weight, mask, dtype=self.dtype, reduction=\"sum\"\n+            )\n         return super().update_state(values, sample_weight=sample_weight)\n \n     def get_config(self):\n-        config = {\n-            k: v for k, v in self._fn_kwargs.items()\n-        }\n+        config = {k: v for k, v in self._fn_kwargs.items()}\n         config[\"fn\"] = self._fn\n         base_config = super().get_config()\n         return {**base_config.items(), **config.items()}\n\n@@ -1,9 +1,8 @@\n from keras_core import backend\n from keras_core import initializers\n from keras_core import operations as ops\n-from keras_core.metrics.metric import Metric\n from keras_core.metrics import reduction_metrics\n-\n+from keras_core.metrics.metric import Metric\n \n # class MeanSquareError(Metric):\n #     def __init__(self, name=\"mean_square_error\", dtype=None):\n@@ -40,7 +39,4 @@ class MeanSquareError(reduction_metrics.MeanMetricWrapper):\n         super().__init__(fn=mean_square_error, name=name, dtype=dtype)\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"dtype\": self.dtype\n-        }\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n\n@@ -822,7 +822,11 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([5, 3, 2])\n         self.assertEqual(knp.get_item(x, 3).shape, (3, 2))\n \n-        x = KerasTensor([2,])\n+        x = KerasTensor(\n+            [\n+                2,\n+            ]\n+        )\n         self.assertEqual(knp.get_item(x, 0).shape, ())\n \n         x = KerasTensor([None, None])\n\n@@ -53,9 +53,7 @@ class MetricsList(metrics_module.Metric):\n \n     def update_state(self, y_true, y_pred, sample_weight=None):\n         for m in self.metrics:\n-            m.update_state(\n-                y_true, y_pred, sample_weight=sample_weight\n-            )\n+            m.update_state(y_true, y_pred, sample_weight=sample_weight)\n \n     def reset_state(self):\n         for m in self.metrics:\n@@ -228,7 +226,13 @@ class CompileMetrics(metrics_module.Metric):\n                         f\"metric objects. Received instead:\\n{argument_name}={metrics}\"\n                     )\n                 flat_metrics.append(\n-                    MetricsList([get_metric(m, y_true[0], y_pred[0]) for m in metrics if m is not None])\n+                    MetricsList(\n+                        [\n+                            get_metric(m, y_true[0], y_pred[0])\n+                            for m in metrics\n+                            if m is not None\n+                        ]\n+                    )\n                 )\n         else:\n             if isinstance(metrics, (list, tuple)):\n@@ -255,7 +259,13 @@ class CompileMetrics(metrics_module.Metric):\n                             f\"Found the following sublist with unknown types: {mls}\"\n                         )\n                     flat_metrics.append(\n-                        MetricsList([get_metric(m, yt, yp) for m in mls if m is not None])\n+                        MetricsList(\n+                            [\n+                                get_metric(m, yt, yp)\n+                                for m in mls\n+                                if m is not None\n+                            ]\n+                        )\n                     )\n             elif isinstance(metrics, dict):\n                 if output_names is None:\n@@ -290,7 +300,11 @@ class CompileMetrics(metrics_module.Metric):\n                     if name in metrics:\n                         flat_metrics.append(\n                             MetricsList(\n-                                [get_metric(m, yt, yp) for m in metrics[name] if m is not None]\n+                                [\n+                                    get_metric(m, yt, yp)\n+                                    for m in metrics[name]\n+                                    if m is not None\n+                                ]\n                             )\n                         )\n                     else:\n@@ -366,8 +380,20 @@ class CompileMetrics(metrics_module.Metric):\n \n class CompileLoss(losses_module.Loss):\n     def __init__(self, loss, loss_weights):\n+        if loss and not isinstance(loss, (list, tuple, dict)):\n+            raise ValueError(\n+                \"Expected `loss` argument to be a list, tuple, or dict. \"\n+                f\"Received instead: loss={loss} of type {type(loss)}\"\n+            )\n+        if loss_weights and not isinstance(loss_weights, (list, tuple, dict)):\n+            raise ValueError(\n+                \"Expected `loss_weights` argument to be a list, tuple, or dict. \"\n+                f\"Received instead: loss_weights={loss_weights} \"\n+                f\"of type {type(loss_weights)}\"\n+            )\n         self._user_loss = loss\n         self._user_loss_weights = loss_weights\n+        self.built = False\n \n     def build(self, y_true, y_pred):\n         # TODO\n\n@@ -1,29 +1,36 @@\n import numpy as np\n+\n from keras_core import backend\n-from keras_core import testing\n from keras_core import metrics as metrics_module\n+from keras_core import testing\n from keras_core.trainers.compile_utils import CompileMetrics\n \n-class TestCompileMetrics(testing.TestCase):\n \n+class TestCompileMetrics(testing.TestCase):\n     def test_single_output_case(self):\n         compile_metrics = CompileMetrics(\n             metrics=[metrics_module.MeanSquareError()],\n             weighted_metrics=[metrics_module.MeanSquareError()],\n         )\n         # Test symbolic build\n-        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))\n+        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n+            (3, 4)\n+        )\n         compile_metrics.build(y_true, y_pred)\n         # Test eager build\n         y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n         y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n-        sample_weight = np.array([1, 0., 1])\n+        sample_weight = np.array([1, 0.0, 1])\n         compile_metrics.build(y_true, y_pred)\n \n         # Test update / result / reset flow\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         y_pred = np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]])\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 2)\n@@ -34,18 +41,30 @@ class TestCompileMetrics(testing.TestCase):\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 2)\n-        self.assertAllClose(result[\"mean_square_error\"], 0.)\n-        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.0)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0)\n \n     def test_list_output_case(self):\n         compile_metrics = CompileMetrics(\n             metrics=[\n-                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n-                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n+                [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n             ],\n             weighted_metrics=[\n-                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n-                [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n+                [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n             ],\n         )\n         # Test symbolic build\n@@ -61,16 +80,20 @@ class TestCompileMetrics(testing.TestCase):\n             np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n             np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n         ]\n-        sample_weight = np.array([1, 0., 1])\n+        sample_weight = np.array([1, 0.0, 1])\n         compile_metrics.build(y_true, y_pred)\n \n         # Test update / result / reset flow\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         y_pred = [\n             np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n             np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n         ]\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 8)\n@@ -81,28 +104,40 @@ class TestCompileMetrics(testing.TestCase):\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 8)\n-        self.assertAllClose(result[\"mean_square_error\"], 0.)\n-        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.0)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0)\n \n     def test_dict_output_case(self):\n         compile_metrics = CompileMetrics(\n             metrics={\n-                \"output_1\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n-                \"output_2\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                \"output_1\": [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n+                \"output_2\": [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n             },\n             weighted_metrics={\n-                \"output_1\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n-                \"output_2\": [metrics_module.MeanSquareError(), metrics_module.MeanSquareError()],\n+                \"output_1\": [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n+                \"output_2\": [\n+                    metrics_module.MeanSquareError(),\n+                    metrics_module.MeanSquareError(),\n+                ],\n             },\n         )\n         # Test symbolic build\n         y_true = {\n             \"output_1\": backend.KerasTensor((3, 4)),\n-            \"output_2\": backend.KerasTensor((3, 4))\n+            \"output_2\": backend.KerasTensor((3, 4)),\n         }\n         y_pred = {\n             \"output_1\": backend.KerasTensor((3, 4)),\n-            \"output_2\": backend.KerasTensor((3, 4))\n+            \"output_2\": backend.KerasTensor((3, 4)),\n         }\n         compile_metrics.build(y_true, y_pred)\n         # Test eager build\n@@ -114,16 +149,20 @@ class TestCompileMetrics(testing.TestCase):\n             \"output_1\": np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n             \"output_2\": np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]]),\n         }\n-        sample_weight = np.array([1, 0., 1])\n+        sample_weight = np.array([1, 0.0, 1])\n         compile_metrics.build(y_true, y_pred)\n \n         # Test update / result / reset flow\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         y_pred = {\n             \"output_1\": np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n             \"output_2\": np.array([[0.3, 0.2], [0.1, 0.4], [0.2, 0.3]]),\n         }\n-        compile_metrics.update_state(y_true, y_pred, sample_weight=sample_weight)\n+        compile_metrics.update_state(\n+            y_true, y_pred, sample_weight=sample_weight\n+        )\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 8)\n@@ -134,6 +173,10 @@ class TestCompileMetrics(testing.TestCase):\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n         self.assertEqual(len(result), 8)\n-        self.assertAllClose(result[\"mean_square_error\"], 0.)\n-        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.)\n+        self.assertAllClose(result[\"mean_square_error\"], 0.0)\n+        self.assertAllClose(result[\"weighted_mean_square_error\"], 0.0)\n \n+\n+class TestCompileLoss(testing.TestCase):\n+    def test_single_output_case(self):\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#742b86407db23331455adbd6286d980223ac8d56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 394 | Lines Deleted: 179 | Files Changed: 4 | Hunks: 17 | Methods Changed: 25 | Complexity Δ (Sum/Max): 60/40 | Churn Δ: 573 | Churn Cumulative: 1847 | Contributors (this commit): 3 | Commits (past 90d): 30 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,6 @@ from keras_core.backend.common import get_stateless_scope\n from keras_core.backend.common import in_stateless_scope\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.keras_tensor import KerasTensor\n-from keras_core.backend.tensorflow.trainer import Trainer\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = True\n\n@@ -37,17 +37,51 @@ class Functional(Function, Model):\n             for k, v in inputs.items():\n                 if not isinstance(v, backend.KerasTensor):\n                     raise ValueError(\n-                        \"When providing an input dict, all values in the dict \"\n+                        \"When providing `inputs` as a dict, all values in the dict \"\n                         f\"must be KerasTensors. Received: inputs={inputs} including \"\n                         f\"invalid value {v} of type {type(v)}\"\n                     )\n                 if k != v.name:\n+                    # TODO: maybe make this a warning\n                     raise ValueError(\n-                        \"When providing an input dict, all keys in the dict \"\n+                        \"When providing `inputs` as a dict, all keys in the dict \"\n                         \"must match the names of the corresponding tensors. \"\n                         f\"Received key '{k}' mapping to value {v} which has name '{v.name}'. \"\n                         f\"Change the tensor name to '{k}' (via `Input(..., name='{k}')`)\"\n                     )\n+        elif isinstance(inputs, (list, tuple)):\n+            for x in inputs:\n+                if not isinstance(x, backend.KerasTensor):\n+                    raise ValueError(\n+                        \"When providing `inputs` as a list/tuple, all values in the list/tuple \"\n+                        f\"must be KerasTensors. Received: inputs={inputs} including \"\n+                        f\"invalid value {x} of type {type(x)}\"\n+                    )\n+        elif not isinstance(inputs, backend.KerasTensor):\n+            raise ValueError(\n+                f\"Unrecognized type for `inputs`: {inputs} (of type {type(inputs)})\"\n+            )\n+        if isinstance(outputs, dict):\n+            for k, v in outputs.items():\n+                if not isinstance(v, backend.KerasTensor):\n+                    raise ValueError(\n+                        \"When providing `outputs` as a dict, all values in the dict \"\n+                        f\"must be KerasTensors. Received: outputs={outputs} including \"\n+                        f\"invalid value {v} of type {type(v)}\"\n+                    )\n+        elif isinstance(outputs, (list, tuple)):\n+            for x in outputs:\n+                if not isinstance(x, backend.KerasTensor):\n+                    raise ValueError(\n+                        \"When providing `outputs` as a list/tuple, all values in the list/tuple \"\n+                        f\"must be KerasTensors. Received: outputs={outputs} including \"\n+                        f\"invalid value {x} of type {type(x)}\"\n+                    )\n+        elif not isinstance(outputs, backend.KerasTensor):\n+            raise ValueError(\n+                f\"Unrecognized type for `outputs`: {outputs} (of type {type(outputs)})\"\n+            )\n+\n         super().__init__(inputs, outputs, name=name, **kwargs)\n         self._layers = self.layers\n         self.built = True\n\n@@ -175,13 +175,17 @@ class CompileMetrics(metrics_module.Metric):\n             output_names = sorted(list(y_pred.keys()))\n             num_outputs = len(output_names)\n         elif isinstance(y_pred, (list, tuple)):\n-            output_names = None\n             num_outputs = len(y_pred)\n+            if all(hasattr(x, \"_keras_history\") for x in y_pred):\n+                output_names = [x._keras_history.operation.name for x in y_pred]\n+            else:\n+                output_names = None\n         else:\n             output_names = None\n             num_outputs = 1\n \n         y_pred = nest.flatten(y_pred)\n+        y_true = nest.flatten(y_true)\n \n         metrics = self._user_metrics\n         weighted_metrics = self._user_weighted_metrics\n@@ -396,8 +400,145 @@ class CompileLoss(losses_module.Loss):\n         self.built = False\n \n     def build(self, y_true, y_pred):\n-        # TODO\n-        pass\n+        if isinstance(y_pred, dict):\n+            output_names = sorted(list(y_pred.keys()))\n+            num_outputs = len(output_names)\n+        elif isinstance(y_pred, (list, tuple)):\n+            num_outputs = len(y_pred)\n+            if all(hasattr(x, \"_keras_history\") for x in y_pred):\n+                output_names = [x._keras_history.operation.name for x in y_pred]\n+            else:\n+                output_names = None\n+        else:\n+            output_names = None\n+            num_outputs = 1\n+\n+        y_pred = nest.flatten(y_pred)\n+        loss = self._user_loss\n+        loss_weights = self._user_loss_weights\n+        flat_losses = []\n+        flat_loss_weights = []\n+        if num_outputs == 1:\n+            if not is_function_like(loss):\n+                raise ValueError(\n+                    f\"When there is only a single output, the `loss` argument \"\n+                    \"must be a callable. \"\n+                    f\"Received instead:\\nloss={loss} of type {type(loss)}\"\n+                )\n+            flat_losses.append(get_loss(loss, y_true, y_pred))\n+            if loss_weights:\n+                if not isinstance(loss_weights, float):\n+                    raise ValueError(\n+                        f\"When there is only a single output, the `loss_weights` argument \"\n+                        \"must be a Python float. \"\n+                        f\"Received instead:\\loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                    )\n+                flat_loss_weights.append(loss_weights)\n+            else:\n+                flat_loss_weights.append(1.0)\n+        elif isinstance(loss, (list, tuple)):\n+            if len(loss) != len(y_pred):\n+                raise ValueError(\n+                    \"For a model with multiple outputs, \"\n+                    f\"when providing the `loss` argument as a list, \"\n+                    \"it should have as many entries as the model has outputs. \"\n+                    f\"Received:\\nloss={loss}\\nof length {len(loss)} \"\n+                    f\"whereas the model has {len(y_pred)} outputs.\"\n+                )\n+            if not all(is_function_like(e) for e in loss):\n+                raise ValueError(\n+                    \"For a model with multiple outputs, \"\n+                    f\"when providing the `loss` argument as a list, \"\n+                    \"each list entry should be a callable (the loss function \"\n+                    \"corresponding to that output). \"\n+                    f\"Received: loss={loss}\"\n+                )\n+            flat_losses = [\n+                get_loss(fn, y_true, y_pred) for fn in loss if fn is not None\n+            ]\n+            if loss_weights:\n+                if not isinstance(loss_weights, (list, tuple)):\n+                    raise ValueError(\n+                        \"If the `loss` argument is provided as a list/tuple, \"\n+                        \"the `loss_weight` argument should also be provided as a list/tuple, \"\n+                        f\"of equal length. Received: loss_weights={loss_weights}\"\n+                    )\n+                if len(loss_weights) != len(y_pred):\n+                    raise ValueError(\n+                        \"For a model with multiple outputs, \"\n+                        f\"when providing the `loss_weights` argument as a list, \"\n+                        \"it should have as many entries as the model has outputs. \"\n+                        f\"Received:\\loss_weights={loss_weights}\\nof length {len(loss_weights)} \"\n+                        f\"whereas the model has {len(y_pred)} outputs.\"\n+                    )\n+                if not all(isinstance(e, float) for e in loss_weights):\n+                    raise ValueError(\n+                        \"For a model with multiple outputs, \"\n+                        f\"when providing the `loss_weights` argument as a list, \"\n+                        \"each list entry should be a Python float (the weighting coefficient \"\n+                        \"corresponding to the loss for that output). \"\n+                        f\"Received: loss_weights={loss_weights}\"\n+                    )\n+                flat_loss_weights = list(loss_weights)\n+            else:\n+                flat_loss_weights = [1.0 for _ in loss]\n+        elif isinstance(loss, dict):\n+            if output_names is None:\n+                raise ValueError(\n+                    f\"Argument `loss` can only be provided as a dict \"\n+                    \"when the model also returns a dict of outputs. Received \"\n+                    f\"loss={loss}\"\n+                )\n+            for name in loss.keys():\n+                if name not in output_names:\n+                    raise ValueError(\n+                        f\"In the dict argument `loss`, key \"\n+                        f\"'{name}' does not correspond to any model output. \"\n+                        f\"Received:\\nloss={loss}\"\n+                    )\n+                if not is_function_like(loss[name]):\n+                    raise ValueError(\n+                        \"For a model with multiple outputs, \"\n+                        f\"when providing the `loss` argument as a dict, \"\n+                        \"each dict entry should be a callable (the loss \"\n+                        \"function corresponding to that output). \"\n+                        f\"At key '{name}', received invalid type:\\n{loss[name]}\"\n+                    )\n+            for name, yt, yp in zip(output_names, y_true, y_pred):\n+                if name in loss:\n+                    if loss[name]:\n+                        flat_losses.append(get_metric(loss[name], yt, yp))\n+                    else:\n+                        flat_losses.append(None)\n+                else:\n+                    flat_losses.append(None)\n+            if loss_weights:\n+                if not isinstance(loss_weights, dict):\n+                    raise ValueError(\n+                        \"If the `loss` argument is provided as a dict, \"\n+                        \"the `loss_weight` argument should also be provided as a dict. \"\n+                        f\"Received: loss_weights={loss_weights}\"\n+                    )\n+                for name in loss_weights.keys():\n+                    if name not in output_names:\n+                        raise ValueError(\n+                            f\"In the dict argument `loss_weights`, key \"\n+                            f\"'{name}' does not correspond to any model output. \"\n+                            f\"Received:\\loss_weights={loss_weights}\"\n+                        )\n+                    if not isinstance(loss_weights[name], float):\n+                        raise ValueError(\n+                            \"For a model with multiple outputs, \"\n+                            f\"when providing the `loss_weights` argument as a dict, \"\n+                            \"each dict entry should be a Python float (the weighting coefficient \"\n+                            \"corresponding to the loss for that output). \"\n+                            f\"At key '{name}', received invalid type:\\n{loss_weights[name]}\"\n+                        )\n+                for name in output_names:\n+                    if name in loss_weights:\n+                        flat_loss_weights.append(loss_weights[name])\n+                    else:\n+                        flat_loss_weights.append(1.0)\n \n     def call(self, y_true, y_pred):\n         if not self.built:\n@@ -412,6 +553,9 @@ class CompileLoss(losses_module.Loss):\n             if loss:\n                 value = w * ops.cast(loss(y_t, y_p), dtype=backend.floatx())\n                 loss_values.append(value)\n+        if loss_values:\n+            return ops.sum(loss_values)\n+        return None\n \n     def get_config(self):\n         raise NotImplementedError\n\n@@ -1,3 +1,9 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.trainers.compile_utils import CompileLoss\n+from keras_core.trainers.compile_utils import CompileMetrics\n+\n+\n class Trainer:\n     def compile(\n         self,\n@@ -11,14 +17,205 @@ class Trainer:\n     ):\n         # TODO: get from module\n         self.optimizer = optimizer\n-\n+        if loss is not None:\n             self._compile_loss = CompileLoss(loss, loss_weights)\n+        else:\n+            self._compile_loss = None\n+        if metrics is not None:\n             self._compile_metrics = CompileMetrics(metrics, weighted_metrics)\n-\n-        # TODO: immediately build the loss/metrics if we have a graph network\n+        else:\n+            self._compile_metrics = None\n         self.jit_compile = jit_compile\n         self.run_eagerly = run_eagerly\n \n+    @property\n+    def jit_compile(self):\n+        return self._jit_compile\n+\n+    @jit_compile.setter\n+    def jit_compile(self, value):\n+        self._jit_compile = value\n+\n+    @property\n+    def run_eagerly(self):\n+        return self._run_eagerly\n+\n+    @run_eagerly.setter\n+    def run_eagerly(self, value):\n+        self._run_eagerly = value\n+\n+    @property\n+    def metrics(self):\n+        metrics = self._metrics[:]\n+        if self._compile_metrics is not None:\n+            metrics += [self.compile_metric]\n+        return metrics\n+\n+    def reset_metrics(self):\n+        for m in self.metrics:\n+            m.reset_state()\n+\n+    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n+        \"\"\"Compute the total loss, validate it, and return it.\n+\n+        Subclasses can optionally override this method to provide custom loss\n+        computation logic.\n+\n+        Example:\n+\n+        ```python\n+        class MyModel(Model):\n+          def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.loss_tracker = metrics.Mean(name='loss')\n+\n+          def compute_loss(self, x, y, y_pred, sample_weight):\n+            loss = ops.means((y_pred - y) ** 2)\n+            loss += ops.sum(self.losses)\n+            self.loss_tracker.update_state(loss)\n+            return loss\n+\n+          def reset_metrics(self):\n+            self.loss_tracker.reset_state()\n+\n+          @property\n+          def metrics(self):\n+            return [self.loss_tracker]\n+\n+        inputs = layers.Input(shape=(10,), name='my_input')\n+        outputs = layers.Dense(10)(inputs)\n+        model = MyModel(inputs, outputs)\n+        model.add_loss(ops.sum(outputs))\n+\n+        optimizer = SGD()\n+        model.compile(optimizer, loss='mse', steps_per_execution=10)\n+        dataset = ...\n+        model.fit(dataset, epochs=2, steps_per_epoch=10)\n+        print(f\"Custom loss: {model.loss_tracker.result()}\")\n+        ```\n+\n+        Args:\n+            x: Input data.\n+            y: Target data.\n+            y_pred: Predictions returned by the model (output of `model(x)`)\n+            sample_weight: Sample weights for weighting the loss function.\n+\n+        Returns:\n+            The total loss as a scalar tensor, or `None` if no loss results\n+            (which is the case when called by `Model.test_step`).\n+        \"\"\"\n+        del x  # The default implementation does not use `x`.\n+        losses = []\n+        if self._compile_metrics is not None:\n+            loss = self.compile_loss(y, y_pred, sample_weight)\n+            if loss is not None:\n+                losses.append(loss)\n+        for l in self.losses:\n+            losses.append(ops.cast(l, dtype=backend.floatx()))\n+        if len(losses) == 0:\n+            raise ValueError(\n+                \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n+            )\n+        if len(losses) == 1:\n+            return losses[0]\n+        return ops.sum(losses)\n+\n+    def compute_metrics(self, x, y, y_pred, sample_weight):\n+        \"\"\"Update metric states and collect all metrics to be returned.\n+\n+        Subclasses can optionally override this method to provide custom metric\n+        updating and collection logic.\n+\n+        Example:\n+\n+        ```python\n+        class MyModel(Sequential):\n+          def compute_metrics(self, x, y, y_pred, sample_weight):\n+            # This super call updates `self.compiled_metrics` and returns\n+            # results for all metrics listed in `self.metrics`.\n+            metric_results = super().compute_metrics(x, y, y_pred, sample_weight)\n+\n+            # Note that `self.custom_metric` is not listed in `self.metrics`.\n+            self.custom_metric.update_state(x, y, y_pred, sample_weight)\n+            metric_results['custom_metric_name'] = self.custom_metric.result()\n+            return metric_results\n+        ```\n+\n+        Args:\n+            x: Input data.\n+            y: Target data.\n+            y_pred: Predictions returned by the model (output of `model.call(x)`)\n+            sample_weight: Sample weights for weighting the loss function.\n+\n+        Returns:\n+            A `dict` containing values that will be passed to\n+            `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n+            values of the metrics listed in `self.metrics` are returned. Example:\n+            `{'loss': 0.2, 'accuracy': 0.7}`.\n+        \"\"\"\n+        del x  # The default implementation does not use `x`.\n+        if self._compile_metrics is not None:\n+            self._compile_metrics.update_state(y, y_pred, sample_weight)\n+        return self.get_metrics_result()\n+\n+    def get_metrics_result(self):\n+        \"\"\"Returns the model's metrics values as a dict.\n+\n+        If any of the metric result is a dict (containing multiple metrics),\n+        each of them gets added to the top level returned dict of this method.\n+\n+        Returns:\n+            A `dict` containing values of the metrics listed in `self.metrics`.\n+            Example:\n+            `{'loss': 0.2, 'accuracy': 0.7}`.\n+        \"\"\"\n+        return_metrics = {}\n+        for metric in self.metrics:\n+            result = metric.result()\n+            if isinstance(result, dict):\n+                return_metrics.update(result)\n+            else:\n+                return_metrics[metric.name] = result\n+        return return_metrics\n+\n+    def call(self, inputs):\n+        raise NotImplementedError\n+\n+    def train_step(self, data):\n+        raise NotImplementedError\n+\n+    def test_step(self, data):\n+        raise NotImplementedError\n+\n+    def predict_step(self, data):\n+        raise NotImplementedError\n+\n+    def make_train_function(self):\n+        raise NotImplementedError\n+\n+    def make_test_function(self):\n+        raise NotImplementedError\n+\n+    def make_predict_function(self):\n+        raise NotImplementedError\n+\n+    def fit(self, x, y=None):\n+        raise NotImplementedError\n+\n+    def evaluate(self, x, y=None):\n+        raise NotImplementedError\n+\n+    def predict(self, x, y=None):\n+        raise NotImplementedError\n+\n+    def get_compile_config(self):\n+        # TODO\n+        raise NotImplementedError\n+\n+    def compile_from_config(self):\n+        # TODO\n+        raise NotImplementedError\n+\n     # @property\n     # def num_inputs(self):\n     #     \"\"\"Return the number of input tensors expected to be taken by the first argument of `call`.\"\"\"\n@@ -77,174 +274,15 @@ class Trainer:\n     #         x._keras_history.operation.name for x in self._inputs_struct\n     #     ]\n \n-    @property\n-    def jit_compile(self):\n-        return self._jit_compile\n \n-    @jit_compile.setter\n-    def jit_compile(self, value):\n-        self._jit_compile = value\n-\n-    @property\n-    def run_eagerly(self):\n-        return self._run_eagerly\n-\n-    @run_eagerly.setter\n-    def run_eagerly(self, value):\n-        self._run_eagerly = value\n-\n-    @property\n-    def metrics(self):\n-        # TODO: combine sub-metrics with compile metric\n-        return self._metrics\n-\n-    def reset_metrics(self):\n-        for m in self.metrics:\n-            m.reset_state()\n-\n-    def call(self, inputs):\n-        raise NotImplementedError\n-\n-    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n-        \"\"\"Compute the total loss, validate it, and return it.\n-\n-        Subclasses can optionally override this method to provide custom loss\n-        computation logic.\n-\n-        Example:\n-\n-        ```python\n-        class MyModel(Model):\n-          def __init__(self, *args, **kwargs):\n-            super().__init__(*args, **kwargs)\n-            self.loss_tracker = metrics.Mean(name='loss')\n-\n-          def compute_loss(self, x, y, y_pred, sample_weight):\n-            loss = ops.means((y_pred - y) ** 2)\n-            loss += ops.sum(self.losses)\n-            self.loss_tracker.update_state(loss)\n-            return loss\n-\n-          def reset_metrics(self):\n-            self.loss_tracker.reset_state()\n-\n-          @property\n-          def metrics(self):\n-            return [self.loss_tracker]\n-\n-        inputs = layers.Input(shape=(10,), name='my_input')\n-        outputs = layers.Dense(10)(inputs)\n-        model = MyModel(inputs, outputs)\n-        model.add_loss(ops.sum(outputs))\n-\n-        optimizer = SGD()\n-        model.compile(optimizer, loss='mse', steps_per_execution=10)\n-        dataset = ...\n-        model.fit(dataset, epochs=2, steps_per_epoch=10)\n-        print(f\"My custom loss: {model.loss_tracker.result()}\")\n-        ```\n-\n-        Args:\n-            x: Input data.\n-            y: Target data.\n-            y_pred: Predictions returned by the model (output of `model(x)`)\n-            sample_weight: Sample weights for weighting the loss function.\n-\n-        Returns:\n-            The total loss as a scalar tensor, or `None` if no loss results\n-            (which is the case when called by `Model.test_step`).\n-        \"\"\"\n-        del x  # The default implementation does not use `x`.\n-        return self.compiled_loss(\n-            y, y_pred, sample_weight, regularization_losses=self.losses\n-        )\n-\n-    def compute_metrics(self, x, y, y_pred, sample_weight):\n-        \"\"\"Update metric states and collect all metrics to be returned.\n-\n-        Subclasses can optionally override this method to provide custom metric\n-        updating and collection logic.\n-\n-        Example:\n-\n-        ```python\n-        class MyModel(Sequential):\n-          def compute_metrics(self, x, y, y_pred, sample_weight):\n-            # This super call updates `self.compiled_metrics` and returns\n-            # results for all metrics listed in `self.metrics`.\n-            metric_results = super().compute_metrics(x, y, y_pred, sample_weight)\n-\n-            # Note that `self.custom_metric` is not listed in `self.metrics`.\n-            self.custom_metric.update_state(x, y, y_pred, sample_weight)\n-            metric_results['custom_metric_name'] = self.custom_metric.result()\n-            return metric_results\n-        ```\n-\n-        Args:\n-            x: Input data.\n-            y: Target data.\n-            y_pred: Predictions returned by the model (output of `model.call(x)`)\n-            sample_weight: Sample weights for weighting the loss function.\n-\n-        Returns:\n-            A `dict` containing values that will be passed to\n-            `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n-            values of the metrics listed in `self.metrics` are returned. Example:\n-            `{'loss': 0.2, 'accuracy': 0.7}`.\n-        \"\"\"\n-        del x  # The default implementation does not use `x`.\n-        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n-        return self.get_metrics_result()\n-\n-    def get_metrics_result(self):\n-        \"\"\"Returns the model's metrics values as a dict.\n-\n-        If any of the metric result is a dict (containing multiple metrics),\n-        each of them gets added to the top level returned dict of this method.\n-\n-        Returns:\n-            A `dict` containing values of the metrics listed in `self.metrics`.\n-            Example:\n-            `{'loss': 0.2, 'accuracy': 0.7}`.\n-        \"\"\"\n-        return_metrics = {}\n-        for metric in self.metrics:\n-            result = metric.result()\n-            if isinstance(result, dict):\n-                return_metrics.update(result)\n-            else:\n-                return_metrics[metric.name] = result\n-        return return_metrics\n-\n-    def train_step(self, data):\n-        raise NotImplementedError\n-\n-    def test_step(self, data):\n-        raise NotImplementedError\n-\n-    def predict_step(self, data):\n-        raise NotImplementedError\n-\n-    def make_train_function(self):\n-        raise NotImplementedError\n-\n-    def make_test_function(self):\n-        raise NotImplementedError\n-\n-    def make_predict_function(self):\n-        raise NotImplementedError\n-\n-    def fit(self, x, y=None):\n-        raise NotImplementedError\n-\n-    def evaluate(self, x, y=None):\n-        raise NotImplementedError\n-\n-    def predict(self, x, y=None):\n-        raise NotImplementedError\n-\n-    def get_compile_config(self):\n-        return {}\n-\n-    def compile_from_config(self):\n-        pass\n+# def get_functional_inputs_and_outputs(model):\n+#     from keras_core.models.functional import Functional\n+#     from keras_core.models.sequential import Sequential\n+#     functional = None\n+#     if isinstance(model, Functional):\n+#         functional = model\n+#     elif isinstance(model, Sequential) and model._functional:\n+#         functional = model._functional\n+#     if functional:\n+#         return functional._inputs_struct, functional._outputs_struct\n+#     return None, None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
