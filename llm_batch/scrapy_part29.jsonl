{"custom_id": "scrapy#3d85932ae0089a44ac93e5fd5863fc4272e90bae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 53 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,8 +13,8 @@ urlpatterns = patterns('',\n \n     # docs\n     url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n-    # blog\n-    url(r\"^blog/\", include(\"scrapyorg.blog.urls\")),\n+    # news\n+    url(r\"^news/\", include(\"scrapyorg.blog.urls\")),\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f2c5cf19dbd376482cd878d2fcd4e8b815e8f41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 144 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,6 @@ class Post(models.Model):\n     body            = models.TextField(_('body'))\n     tease           = models.TextField(_('tease'), blank=True)\n     status          = models.IntegerField(_('status'), choices=STATUS_CHOICES, default=2)\n-    allow_comments  = models.BooleanField(_('allow comments'), default=True)\n     publish         = models.DateTimeField(_('publish'))\n     created         = models.DateTimeField(_('created'), auto_now_add=True)\n     modified        = models.DateTimeField(_('modified'), auto_now=True)\n\n@@ -77,7 +77,6 @@ INSTALLED_APPS = (\n     'django.contrib.sites',\n     'django.contrib.admin',\n     'django.contrib.markup',\n-    'django.contrib.comments',\n     'scrapyorg.article',\n     'scrapyorg.download',\n     'scrapyorg.docs',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36277f3a328af6b1de4a63bed8203ca847b4f1a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n def setup(app):\n-    app.add_description_unit(\n+    app.add_crossref_type(\n         directivename = \"setting\",\n         rolename      = \"setting\",\n         indextemplate = \"pair: %s; setting\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af79bedf8f400193f660892efdae321d13b21e50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -36,6 +36,7 @@ class RegexLinkExtractor(LinkExtractor):\n     Url matching is always performed against the absolute urls, never the\n     relative urls found in pages.\n \n+    If no allow/deny arguments are given, match all links.\n     \"\"\"\n     \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d047409579023f9b6a15b7e9808576f7addfe89e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 185 | Lines Deleted: 35 | Files Changed: 4 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 220 | Churn Cumulative: 475 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -11,14 +11,14 @@ class Settings(object):\n     overrides = None\n     settings = None\n     defaults = None\n-    core = None\n+    global_defaults = None\n \n     def __init__(self):\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n         self.settings = self._import(SETTINGS_MODULE)\n         self.defaults = {}\n-        self.core = self._import('scrapy.conf.core_settings')\n+        self.global_defaults = self._import('scrapy.conf.default_settings')\n \n     def _import(self, modulepath):\n         return __import__(modulepath, {}, {}, [''])\n@@ -36,8 +36,8 @@ class Settings(object):\n         if opt_name in self.defaults:\n             return self.defaults[opt_name]\n \n-        if hasattr(self.core, opt_name):\n-            return getattr(self.core, opt_name)\n+        if hasattr(self.global_defaults, opt_name):\n+            return getattr(self.global_defaults, opt_name)\n \n     def get(self, name, default=None):\n         return self[name] if self[name] is not None else default\n\n@@ -1,30 +0,0 @@\n-import scrapy\n-# Scrapy core settings\n-\n-BOT_NAME = 'scrapy'\n-BOT_VERSION = scrapy.__version__\n-\n-ENGINE_DEBUG = False\n-\n-# Download configuration options\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-DOWNLOAD_TIMEOUT = 180      # 3mins\n-CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n-REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n-CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid (a negative value means \"never expires\")\n-\n-LOG_ENABLED = True  #\n-LOGLEVEL = 'DEBUG'   # default loglevel\n-LOGFILE = None      # None means sys.stderr by default\n-LOG_STDOUT = False   #\n-\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-MEMORYSTORE = 'scrapy.core.scheduler.MemoryStore'\n-PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n-\n-EXTENSIONS = []\n-\n-# contrib.middleware.retry.RetryMiddleware default settings\n-RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n\n@@ -0,0 +1,179 @@\n+\"\"\"\n+This module contains the default values for all settings used by Scrapy. \n+\n+For more information about these settings you can read the settings\n+documentation in docs/reference/settings.rst\n+\n+Scrapy developers, if you add a setting here remember to:\n+\n+* add it in alphabetical order\n+* group similar settings without leaving blank lines\n+* add its documentation to the available settings documentation\n+  (docs/reference/settings.rst)\n+\n+\"\"\"\n+\n+from os.path import join, abspath, dirname\n+\n+ADAPTORS_DEBUG = False\n+\n+BOT_NAME = 'scrapybot'\n+BOT_VERSION = '1.0'\n+\n+CACHE2_DIR = ''\n+CACHE2_IGNORE_MISSING = False\n+CACHE2_SECTORIZE = True\n+CACHE2_EXPIRATION_SECS = 0\n+\n+CLOSEDOMAIN_TIMEOUT = 0\n+CLOSEDOMAIN_NOTIFY = ['pablo@insophia.com']\n+\n+CLUSTER_LOGDIR = ''\n+\n+CLUSTER_MASTER_PORT = 8790\n+CLUSTER_MASTER_ENABLED = 0\n+CLUSTER_MASTER_POLL_INTERVAL = 60\n+CLUSTER_MASTER_NODES = {}\n+CLUSTER_MASTER_CACHEFILE = \"\"\n+\n+CLUSTER_WORKER_ENABLED = 0\n+CLUSTER_WORKER_MAXPROC = 4\n+CLUSTER_WORKER_PORT = 8789\n+\n+COMMANDS_MODULE = ''\n+COMMANDS_SETTINGS_MODULE = ''\n+\n+CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n+\n+DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+\n+DEPTH_LIMIT = 0\n+DEPTH_STATS = True\n+\n+DOWNLOAD_TIMEOUT = 180      # 3mins\n+\n+DOWNLOADER_MIDDLEWARES = [\n+    # Engine side\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n+    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats',\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    # Downloader side\n+]\n+\n+DOWNLOADER_STATS = True\n+\n+ENABLED_SPIDERS_FILE = ''\n+\n+ENGINE_DEBUG = False\n+\n+EXTENSIONS = [\n+    'scrapy.stats.corestats.CoreStats',\n+    'scrapy.xpath.extension.ResponseLibxml2',\n+    'scrapy.management.web.WebConsole',\n+    'scrapy.management.telnet.TelnetConsole',\n+    'scrapy.contrib.webconsole.schedstats.SchedulerStats',\n+    'scrapy.contrib.webconsole.livestats.LiveStats',\n+    'scrapy.contrib.webconsole.spiderctl.Spiderctl',\n+    'scrapy.contrib.webconsole.enginestatus.EngineStatus',\n+    'scrapy.contrib.webconsole.stats.StatsDump',\n+    'scrapy.contrib.webconsole.spiderstats.SpiderStats',\n+    'scrapy.contrib.spider.reloader.SpiderReloader',\n+    'scrapy.contrib.memusage.MemoryUsage',\n+    'scrapy.contrib.memdebug.MemoryDebugger',\n+    'scrapy.contrib.pbcluster.ClusterWorker',\n+    'scrapy.contrib.pbcluster.ClusterMasterWeb',\n+    'scrapy.contrib.pbcluster.ClusterCrawler',\n+    'scrapy.contrib.closedomain.CloseDomain',\n+    'scrapy.contrib.debug.StackTraceDebug',\n+    'scrapy.contrib.response.soup.ResponseSoup',\n+]\n+\n+GROUPSETTINGS_ENABLED = False\n+GROUPSETTINGS_MODULE = ''\n+\n+# Item pipelines are typically set in specific commands settings\n+ITEM_PIPELINES = []\n+\n+LOG_ENABLED = True\n+LOG_STDOUT = False\n+LOGLEVEL = 'DEBUG'\n+LOGFILE = None\n+\n+MAIL_HOST = 'localhost'\n+MAIL_FROM = 'scrapy@localhost'\n+\n+MEMDEBUG_ENABLED = False        # enable memory debugging\n+MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown\n+\n+MEMORYSTORE = 'scrapy.core.scheduler.MemoryStore'\n+\n+MEMUSAGE_ENABLED = 1\n+MEMUSAGE_LIMIT_MB = 0\n+MEMUSAGE_NOTIFY_MAIL = []\n+MEMUSAGE_REPORT = False\n+MEMUSAGE_WARNING_MB = 0\n+\n+MYSQL_CONNECTION_SETTINGS = {}\n+\n+NEWSPIDER_MODULE = ''\n+\n+PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n+\n+REQUESTS_QUEUE_SIZE = 0\n+REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n+\n+# contrib.middleware.retry.RetryMiddleware default settings\n+RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n+RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n+\n+SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n+SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n+\n+SPIDER_MODULES = []\n+\n+SPIDERPROFILER_ENABLED = False\n+\n+SPIDER_MIDDLEWARES = [\n+    # Engine side\n+    'scrapy.contrib.itemsampler.ItemSamplerMiddleware',\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    # Spider side\n+]\n+\n+STATS_ENABLED = True\n+STATS_CLEANUP = False\n+STATS_DEBUG = False\n+\n+# deprecated settings - the stats web service should be moved to the web console\n+STATS_WSPORT = 8089\n+STATS_WSTIMEOUT = 15\n+\n+TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n+\n+URLLENGTH_LIMIT = 2083\n+\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n+TELNETCONSOLE_ENABLED = 1\n+#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n+\n+WEBCONSOLE_ENABLED = True\n+WEBCONSOLE_PORT = None\n+WEBCONSOLE_LOGFILE = None\n+\n+# this setting is used by the cluster master to pass additional settings to\n+# workers at connection time\n+GLOBAL_CLUSTER_SETTINGS = [] \n\n@@ -11,7 +11,8 @@ once the spider has finished crawling all regular (non failed) pages. Once\n there is no more failed pages to retry this middleware sends a signal\n (retry_complete), so other extensions could connect to that signal.\n \n-Default values are located in scrapy.conf.core_settings\n+Default values are located in scrapy.conf.default_settings, like any other\n+setting\n \n About HTTP errors to consider:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#710247e0968c6cab28c2e7eaa0dcbdd41ad92d7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 42 | Churn Cumulative: 244 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,9 @@ DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n DOWNLOAD_TIMEOUT = 600\n \n+# use this spider class as default when no spider was found for a given url\n+#DEFAULT_SPIDER = 'scrapy.contrib.spiders.generic.GenericSpider'\n+\n # uncomment if you want to add your own custom scrapy commands\n #COMMANDS_MODULE = '$project_name.commands'\n #COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'\n\n@@ -0,0 +1,20 @@\n+from scrapy.contrib.spiders import CrawlSpider, Rule\n+from scrapy.link.extractors import RegexLinkExtractor\n+\n+class GenericSpider(CrawlSpider):\n+    \"\"\"\n+    This generic crawler crawls an entire site. It can also be used as a default crawler\n+    (see setting \"DEFAULT_SPIDER\")\n+    \"\"\"\n+    \n+    def __init__(self, domain_name):\n+        self.domain_name = domain_name\n+        self.rules = (\n+            Rule(RegexLinkExtractor(allow_domains=(domain_name,)), self.parse_note, follow=True),\n+        )\n+        super(GenericSpider, self).__init__()\n+        \n+    def parse_note(self, response):\n+        pass\n+\n+# not a singleton\n\\ No newline at end of file\n\n@@ -12,6 +12,7 @@ from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n+from scrapy.utils.misc import load_class\n \n class SpiderManager(object):\n     \"\"\"Spider locator and manager\"\"\"\n@@ -49,7 +50,14 @@ class SpiderManager(object):\n                 for p in plist:\n                     if url_is_from_spider(url, p):\n                         return p\n-        return self._alldict.get(self.default_domain)\n+        spider = self._alldict.get(self.default_domain)\n+        if not spider:                          # create a custom spider\n+            spiderclassname = settings.get('DEFAULT_SPIDER')\n+            if spiderclassname:\n+                spider = load_class(spiderclassname)(domain)\n+                self.add_spider(spider)\n+            \n+        return spider\n \n     def asdict(self, include_disabled=True):\n         self._load_on_demand()\n@@ -75,21 +83,24 @@ class SpiderManager(object):\n         self._invaliddict = {}\n         self._alldict = {}\n         self._enableddict = {}\n-        enabled_spiders = self._enabled_spiders()\n+        self._enabled_spiders = self._enabled_spiders()\n \n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n         for module in modules:\n             for spider in self._getspiders(ISpider, module):\n+                self.add_spider(spider)\n+        self.loaded = True\n+\n+    def add_spider(self, spider):\n         try:\n             ISpider.validateInvariants(spider)\n             self._alldict[spider.domain_name] = spider\n-                    if spider.domain_name in enabled_spiders:\n+            if spider.domain_name in self._enabled_spiders:\n                 self._enableddict[spider.domain_name] = spider\n         except Exception, e:\n             self._invaliddict[spider.domain_name] = spider\n             # we can't use the log module here because it may not be available yet\n             print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n-        self.loaded = True\n \n     def reload(self, skip_domains=None):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1433af5561db083709d73fbf0734df73b10813ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 165 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -83,7 +83,7 @@ class SpiderManager(object):\n         self._invaliddict = {}\n         self._alldict = {}\n         self._enableddict = {}\n-        self._enabled_spiders = self._enabled_spiders()\n+        self._enabled_spiders_set = self._enabled_spiders()\n \n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n         for module in modules:\n@@ -95,7 +95,7 @@ class SpiderManager(object):\n         try:\n             ISpider.validateInvariants(spider)\n             self._alldict[spider.domain_name] = spider\n-            if spider.domain_name in self._enabled_spiders:\n+            if spider.domain_name in self._enabled_spiders_set:\n                 self._enableddict[spider.domain_name] = spider\n         except Exception, e:\n             self._invaliddict[spider.domain_name] = spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99143ff3572c613bf5989e35d6e2232d9698deb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 129 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,6 +26,7 @@ class RegexLinkExtractor(LinkExtractor):\n     deny - ignore urls that match any of these regexes\n     allow_domains - only extract urls from these domains\n     deny_domains - ignore urls from these dmoains\n+    restrict_xpaths - restrict url extraction to given xpaths contents\n     tags - look for urls in this tags\n     attrs - look for urls in this attrs\n     canonicalize - canonicalize all extracted urls using scrapy.utils.url.canonicalize_url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7bd73e0c094891e5a2d7634c916df471bbc92eef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 17 | Churn Cumulative: 528 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import unittest\n import libxml2\n \n from scrapy.http import Response\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, XPathSelectorList\n \n class XPathTestCase(unittest.TestCase):\n \n@@ -220,6 +220,15 @@ class XPathTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n+    def test_selectorlist_add(self):\n+        '''Adding XPathSelectorLists should return another XPathSelectorList'''\n+        hxs = HtmlXPathSelector(text='<html><body><tag1></tag1><tag2></body></html>')\n+        xlist1 = XPathSelectorList([hxs.x('/'), hxs.x('//tag1')])\n+        xlist2 = XPathSelectorList([hxs.x('/html')])\n+        xlist3 = [1, 2, 3]\n+        self.assertTrue(isinstance(xlist1 + xlist2, XPathSelectorList))\n+        self.assertTrue(isinstance(xlist2 + xlist3, list))\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -102,6 +102,12 @@ class XPathSelectorList(list):\n     def __getslice__(self, i, j):\n         return XPathSelectorList(list.__getslice__(self, i, j))\n \n+    def __add__(self, other):\n+        if isinstance(other, XPathSelectorList):\n+            return XPathSelectorList(super(XPathSelectorList, self).__add__(other))\n+        else:\n+            return super(XPathSelectorList, self).__add__(other)\n+\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5757e54a2869aab03d25eed83896267ad614dced", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 28/23 | Churn Δ: 113 | Churn Cumulative: 440 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 6 | DMM Complexity: 0.49382716049382713\n\nDIFF:\n@@ -91,3 +91,6 @@ class Link(object):\n \n     def __eq__(self, other):\n         return self.url == other.url and self.text == other.text\n+\n+    def __repr__(self):\n+        return '<Link url=%r text=%r >' % (self.url, self.text)\n\n@@ -5,11 +5,13 @@ This module provides some LinkExtractors, which extend that base LinkExtractor\n \"\"\"\n \n import re\n+import urlparse\n \n-from scrapy.link import LinkExtractor\n+from scrapy.link import LinkExtractor, Link\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n from scrapy.utils.response import new_response_from_xpaths\n-from scrapy.utils.misc import dict_updatedefault\n+from scrapy.utils.python import unicode_to_str\n+from scrapy.xpath.selector import HtmlXPathSelector\n \n _re_type = type(re.compile(\"\", 0))\n \n@@ -83,3 +85,63 @@ class RegexLinkExtractor(LinkExtractor):\n         allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]\n         denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []\n         return any(allowed) and not any(denied)\n+\n+class ImageLinkExtractor(object):\n+    def __init__(self, locations=None):\n+        self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()\n+\n+    def extract_from_selector(self, selector, parent=None):\n+        ret = []\n+        def _add_link(url_sel, alt_sel=None):\n+            url = url_sel.extract()\n+            alt = alt_sel.extract() if alt_sel else ('', )\n+            if url:\n+                ret.append(Link(unicode_to_str(url[0]), alt[0]))\n+\n+        if selector.xmlNode.type == 'element':\n+            if selector.xmlNode.name == 'img':\n+                _add_link(selector.x('@src'), selector.x('@alt') or selector.x('@title'))\n+            else:\n+                children = selector.x('child::*')\n+                if len(children):\n+                    for child in children:\n+                        ret.extend(self.extract_from_selector(child, parent=selector))\n+                elif selector.xmlNode.name == 'a' and not parent:\n+                    _add_link(selector.x('@href'), selector.x('@title'))\n+        else:\n+            _add_link(selector)\n+\n+        return ret\n+\n+    def extract_links(self, response, locations=None, unique=True):\n+        xs = HtmlXPathSelector(response)\n+        base_url = xs.x('//base/@href').extract()\n+        base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)\n+\n+        if hasattr(locations, '__iter__'):\n+            locations = self.locations + tuple(locations)\n+        elif self.locations:\n+            locations = self.locations\n+        else:\n+            locations = ('//img', )\n+\n+        links = []\n+        for location in locations:\n+            selector_res = xs.x(location)\n+            for selector in selector_res:\n+                links.extend(self.extract_from_selector(selector))\n+\n+        seen, ret = set(), []\n+        for link in links:\n+            link.url = urlparse.urljoin(base_url, link.url)\n+            if unique:\n+                if link.url in seen:\n+                    continue\n+                else:\n+                    seen.add(link.url)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def matches(self, url):\n+        return False\n\n@@ -1,8 +1,9 @@\n+import os\n import unittest\n \n-from scrapy.http import Response\n+from scrapy.http.response import Response, ResponseBody\n from scrapy.link import LinkExtractor, Link\n-from scrapy.link.extractors import RegexLinkExtractor\n+from scrapy.link.extractors import RegexLinkExtractor, ImageLinkExtractor\n \n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n@@ -63,5 +64,44 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n \n+class ImageLinkExtractorTestCase(unittest.TestCase):\n+    def setUp(self):\n+        body = open(os.path.join(os.path.dirname(__file__), 'sample_data/image_linkextractor.html'), 'r').read()\n+        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))\n+\n+    def test_urls_type(self):\n+        '''Test that the resulting urls are regular strings and not a unicode objects'''\n+        lx = ImageLinkExtractor()\n+        links = lx.extract_links(self.response)\n+        self.assertTrue(all(isinstance(link.url, str) for link in links))\n+\n+    def test_extraction(self):\n+        '''Test the extractor's behaviour among different situations'''\n+        lx = ImageLinkExtractor()\n+\n+        links_1 = lx.extract_links(self.response) # using default locations (//img)\n+        self.assertEqual(links_1,\n+            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+\n+        links_2 = lx.extract_links(self.response, unique=False) # using default locations and unique=False\n+        self.assertEqual(links_2,\n+            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4'),\n+              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])\n+\n+        links_3 = lx.extract_links(self.response, locations=('//div[@id=\"wrapper\"]', ))\n+        self.assertEqual(links_3,\n+            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+\n+        links_4 = lx.extract_links(self.response, locations=('//a', ))\n+        self.assertEqual(links_4,\n+            [ Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c82c799d07bdd175405fa23a2b993bed3bf7b667", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 204 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -87,6 +87,15 @@ class RegexLinkExtractor(LinkExtractor):\n         return any(allowed) and not any(denied)\n \n class ImageLinkExtractor(object):\n+    '''ImageLinkExtractor objects are intended to extract image links from HTML pages\n+    given certain xpath locations.\n+\n+    These locations can be passed in a list/tuple either when instanciating the LinkExtractor,\n+    or whenever you call extract_links.\n+    If no locations are specified in any of these places, a default pattern '//img' will be used.\n+    If locations are specified when instanciating the LinkExtractor, and also when calling extract_links,\n+    both locations will be used for that call of extract_links'''\n+\n     def __init__(self, locations=None):\n         self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91a23e61bf6e61f5fbdad4d32125fee6ecc358f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 545 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -85,7 +85,7 @@ class CrawlSpider(BaseSpider):\n         requests = []\n         seen = set()\n         for rule in self._rules:\n-            links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]\n+            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n             if links and rule.process_links:\n                 links = rule.process_links(links)\n             seen = seen.union(links)\n\n@@ -7,7 +7,7 @@ from scrapy.utils.url import urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n     \"\"\"LinkExtractor are used to extract links from web pages. They are\n-    instantiated and later \"applied\" to a Response using the extract_urls\n+    instantiated and later \"applied\" to a Response using the extract_links\n     method which must receive a Response object and return a dict whoose keys\n     are the (absolute) urls to follow, and its values any arbitrary data. In\n     this case the values are the text of the hyperlink.\n@@ -16,7 +16,7 @@ class LinkExtractor(FixedSGMLParser):\n     functionality for extracting links to follow, but you could override this\n     class or create a new one if you need some additional functionality. The\n     only requisite is that the new (or overrided) class must provide a\n-    extract_urls method that receives a Response and returns a dict with the\n+    extract_links method that receives a Response and returns a dict with the\n     links to follow as its keys.\n \n     The constructor arguments are:\n@@ -35,7 +35,7 @@ class LinkExtractor(FixedSGMLParser):\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.current_link = None\n \n-    def extract_urls(self, response, unique=False):\n+    def extract_links(self, response, unique=False):\n         self.reset()\n         self.unique = unique\n         self.feed(response.body.to_string())\n\n@@ -54,11 +54,11 @@ class RegexLinkExtractor(LinkExtractor):\n         attr_func = lambda x: x in attrs\n         LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n \n-    def extract_urls(self, response, unique=True):\n+    def extract_links(self, response, unique=True):\n         if self.restrict_xpaths:\n             response = new_response_from_xpaths(response, self.restrict_xpaths)\n \n-        links = LinkExtractor.extract_urls(self, response, unique)\n+        links = LinkExtractor.extract_links(self, response, unique)\n         links = [link for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n\n@@ -17,7 +17,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_urls(response), \n+        self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), \n                           Link(url='http://example.org/about.html', text='About us'),\n                           Link(url='http://example.org/othercat.html', text='Other category'), \n@@ -30,7 +30,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_urls(response), \n+        self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n     def test_matches(self):\n\n@@ -22,7 +22,7 @@ class TestSpider(BaseSpider):\n     def parse(self, response):\n         xlink = LinkExtractor()\n         itemre = re.compile(self.itemurl_re)\n-        for link in xlink.extract_urls(response):\n+        for link in xlink.extract_links(response):\n             if itemre.search(link.url):\n                 yield Request(url=link.url, callback=self.parse_item)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9e48dc5ed6a2b07cf39c9e7d528d86c6d7146bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 64 | Churn Cumulative: 191 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,44 +64,44 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n \n-class ImageLinkExtractorTestCase(unittest.TestCase):\n-    def setUp(self):\n-        body = open(os.path.join(os.path.dirname(__file__), 'sample_data/image_linkextractor.html'), 'r').read()\n-        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))\n+#class ImageLinkExtractorTestCase(unittest.TestCase):\n+#    def setUp(self):\n+#        body = open(os.path.join(os.path.dirname(__file__), 'sample_data/image_linkextractor.html'), 'r').read()\n+#        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))\n \n-    def test_urls_type(self):\n-        '''Test that the resulting urls are regular strings and not a unicode objects'''\n-        lx = ImageLinkExtractor()\n-        links = lx.extract_links(self.response)\n-        self.assertTrue(all(isinstance(link.url, str) for link in links))\n+#    def test_urls_type(self):\n+#        '''Test that the resulting urls are regular strings and not a unicode objects'''\n+#        lx = ImageLinkExtractor()\n+#        links = lx.extract_links(self.response)\n+#        self.assertTrue(all(isinstance(link.url, str) for link in links))\n \n-    def test_extraction(self):\n-        '''Test the extractor's behaviour among different situations'''\n-        lx = ImageLinkExtractor()\n+#    def test_extraction(self):\n+#        '''Test the extractor's behaviour among different situations'''\n+#        lx = ImageLinkExtractor()\n \n-        links_1 = lx.extract_links(self.response) # using default locations (//img)\n-        self.assertEqual(links_1,\n-            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+#        links_1 = lx.extract_links(self.response) # using default locations (//img)\n+#        self.assertEqual(links_1,\n+#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n \n-        links_2 = lx.extract_links(self.response, unique=False) # using default locations and unique=False\n-        self.assertEqual(links_2,\n-            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4'),\n-              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])\n+#        links_2 = lx.extract_links(self.response, unique=False) # using default locations and unique=False\n+#        self.assertEqual(links_2,\n+#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4'),\n+#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])\n \n-        links_3 = lx.extract_links(self.response, locations=('//div[@id=\"wrapper\"]', ))\n-        self.assertEqual(links_3,\n-            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+#        links_3 = lx.extract_links(self.response, locations=('//div[@id=\"wrapper\"]', ))\n+#        self.assertEqual(links_3,\n+#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n \n-        links_4 = lx.extract_links(self.response, locations=('//a', ))\n-        self.assertEqual(links_4,\n-            [ Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])\n+#        links_4 = lx.extract_links(self.response, locations=('//a', ))\n+#        self.assertEqual(links_4,\n+#            [ Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebd5f465aa163ef82f0e0068598d1d05764c5549", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 42 | Files Changed: 5 | Hunks: 21 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 79 | Churn Cumulative: 740 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,18 +35,20 @@ class LinkExtractor(FixedSGMLParser):\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.current_link = None\n \n-    def extract_links(self, response, unique=False):\n+    def _extract_links(self, response_text, response_url, unique):\n         self.reset()\n         self.unique = unique\n-        self.feed(response.body.to_string())\n+        self.feed(response_text)\n         self.close()\n \n-        base_url = self.base_url if self.base_url else response.url\n-        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n         for link in self.links:\n             link.url = urljoin(base_url, link.url).strip()\n-            ret.append(link)\n-        return ret\n+            yield link\n+\n+    def extract_links(self, response, unique=False):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body.to_string(), response.url, unique)\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n\n@@ -9,9 +9,8 @@ import urlparse\n \n from scrapy.link import LinkExtractor, Link\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n-from scrapy.utils.response import new_response_from_xpaths\n from scrapy.utils.python import unicode_to_str\n-from scrapy.xpath.selector import HtmlXPathSelector\n+from scrapy.xpath import HtmlXPathSelector\n \n _re_type = type(re.compile(\"\", 0))\n \n@@ -56,23 +55,25 @@ class RegexLinkExtractor(LinkExtractor):\n \n     def extract_links(self, response, unique=True):\n         if self.restrict_xpaths:\n-            response = new_response_from_xpaths(response, self.restrict_xpaths)\n-\n+            hxs = HtmlXPathSelector(response)\n+            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)\n+            links = self._extract_links(html_slice, response.url, unique)\n+        else:\n             links = LinkExtractor.extract_links(self, response, unique)\n-        links = [link for link in links if _is_valid_url(link.url)]\n+\n+        links = (link for link in links if _is_valid_url(link.url))\n \n         if self.allow_res:\n-            links = [link for link in links if _matches(link.url, self.allow_res)]\n+            links = (link for link in links if _matches(link.url, self.allow_res))\n         if self.deny_res:\n-            links = [link for link in links if not _matches(link.url, self.deny_res)]\n+            links = (link for link in links if not _matches(link.url, self.deny_res))\n         if self.allow_domains:\n-            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]\n+            links = (link for link in links if url_is_from_any_domain(link.url, self.allow_domains))\n         if self.deny_domains:\n-            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]\n+            links = (link for link in links if not url_is_from_any_domain(link.url, self.deny_domains))\n \n         if self.canonicalize:\n-            for link in links:\n-                link.url = canonicalize_url(link.url)\n+            links = (canonicalize_url(link.url) for link in links)\n \n         return links\n \n\n@@ -17,7 +17,8 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n+        links = [link for link in lx.extract_links(response)]\n+        self.assertEqual(links,\n                          [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), \n                           Link(url='http://example.org/about.html', text='About us'),\n                           Link(url='http://example.org/othercat.html', text='Other category'), \n@@ -30,7 +31,8 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n+        links = [link for link in lx.extract_links(response)]\n+        self.assertEqual(links,\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n     def test_matches(self):\n\n@@ -1,18 +1,9 @@\n import re, csv\n \n from scrapy.xpath import XmlXPathSelector\n-from scrapy.http import Response\n from scrapy import log\n-from scrapy.utils.python import re_rsearch, str_to_unicode, unicode_to_str\n-\n-def _normalize_input(obj, unicode=True):\n-    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n-    if isinstance(obj, Response):\n-        return obj.body.to_unicode() if unicode else obj.body.to_string()\n-    elif isinstance(obj, str):\n-        return obj.decode('utf-8') if unicode else obj\n-    else:\n-        return obj if unicode else obj.encode('utf-8')\n+from scrapy.utils.python import re_rsearch, str_to_unicode\n+from scrapy.utils.response import body_or_str\n \n def xmliter(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n@@ -25,7 +16,7 @@ def xmliter(obj, nodename):\n     \"\"\"\n     HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename, re.S)\n     HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename, re.S)\n-    text = _normalize_input(obj)\n+    text = body_or_str(obj)\n \n     header_start = re.search(HEADER_START_RE, text)\n     header_start = header_start.group(1).strip() if header_start else ''\n@@ -53,7 +44,7 @@ def csviter(obj, delimiter=None, headers=None):\n     def _getrow(csv_r):\n         return [str_to_unicode(field) for field in csv_r.next()]\n \n-    lines = _normalize_input(obj, unicode=False).splitlines(True)\n+    lines = body_or_str(obj, unicode=False).splitlines(True)\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n\n@@ -3,14 +3,13 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n-from scrapy.xpath import XPathSelector\n-from scrapy.http.response import ResponseBody\n+from scrapy.http.response import Response\n \n-def new_response_from_xpaths(response, xpaths):\n-    \"\"\"Return a new response constructed by applying the given xpaths to the\n-    original response body\n-    \"\"\"\n-    xs = XPathSelector(response)\n-    parts = [''.join([n for n in xs.x(x).extract()]) for x in xpaths]\n-    new_body_content = ''.join(parts)\n-    return response.replace(body=ResponseBody(content=new_body_content, declared_encoding=response.body.get_declared_encoding()))\n+def body_or_str(obj, unicode=True):\n+    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n+    if isinstance(obj, Response):\n+        return obj.body.to_unicode() if unicode else obj.body.to_string()\n+    elif isinstance(obj, str):\n+        return obj.decode('utf-8') if unicode else obj\n+    else:\n+        return obj if unicode else obj.encode('utf-8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2979521b6346cb1746e197a00eec01863883c143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 18 | Churn Cumulative: 546 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import unittest\n import libxml2\n \n from scrapy.http import Response\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, XPathSelectorList\n+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n \n class XPathTestCase(unittest.TestCase):\n \n@@ -220,15 +220,5 @@ class XPathTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n-    def test_selectorlist_add(self):\n-        '''Adding XPathSelectorLists should return another XPathSelectorList'''\n-        hxs = HtmlXPathSelector(text='<html><body><tag1></tag1><tag2></body></html>')\n-        xlist1 = XPathSelectorList([hxs.x('/'), hxs.x('//tag1')])\n-        xlist2 = XPathSelectorList([hxs.x('/html')])\n-        xlist3 = [1, 2, 3]\n-        self.assertTrue(isinstance(xlist1 + xlist2, XPathSelectorList))\n-        self.assertTrue(isinstance(xlist2 + xlist3, list))\n-\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -102,12 +102,6 @@ class XPathSelectorList(list):\n     def __getslice__(self, i, j):\n         return XPathSelectorList(list.__getslice__(self, i, j))\n \n-    def __add__(self, other):\n-        if isinstance(other, XPathSelectorList):\n-            return XPathSelectorList(super(XPathSelectorList, self).__add__(other))\n-        else:\n-            return super(XPathSelectorList, self).__add__(other)\n-\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
