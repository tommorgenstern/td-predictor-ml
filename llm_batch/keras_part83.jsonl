{"custom_id": "keras#cf3b3dff329cb2b50724ef44977abfd0a841ad77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 46 | Churn Cumulative: 490 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,13 +13,13 @@ class Regularizer(object):\n         return loss\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__}\n+        return {'name': self.__class__.__name__}\n \n \n class WeightRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n-        self.l1 = l1\n-        self.l2 = l2\n+        self.l1 = K.cast_to_floatx(l1)\n+        self.l2 = K.cast_to_floatx(l2)\n \n     def set_param(self, p):\n         self.p = p\n@@ -30,29 +30,30 @@ class WeightRegularizer(Regularizer):\n         return loss\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"l1\": self.l1,\n-                \"l2\": self.l2}\n+        return {'name': self.__class__.__name__,\n+                'l1': self.l1,\n+                'l2': self.l2}\n \n \n class ActivityRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n-        self.l1 = l1\n-        self.l2 = l2\n+        self.l1 = K.cast_to_floatx(l1)\n+        self.l2 = K.cast_to_floatx(l2)\n \n     def set_layer(self, layer):\n         self.layer = layer\n+        layer.uses_learning_phase = True\n \n     def __call__(self, loss):\n-        output = self.layer.get_output(True)\n-        loss += self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n-        loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n-        return loss\n+        output = self.layer.output\n+        regularized_loss = self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n+        regularized_loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n+        return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"l1\": self.l1,\n-                \"l2\": self.l2}\n+        return {'name': self.__class__.__name__,\n+                'l1': self.l1,\n+                'l2': self.l2}\n \n \n def l1(l=0.01):\n\n@@ -1,10 +1,14 @@\n import pytest\n import numpy as np\n-np.random.seed(1337) # for reproducibility\n+np.random.seed(1337)\n \n from keras.models import Sequential\n-from keras.layers.core import Merge, Dense, Activation, Flatten, ActivityRegularization\n-from keras.layers.embeddings import Embedding\n+from keras.layers import Merge\n+from keras.layers import Dense\n+from keras.layers import Activation\n+from keras.layers import Flatten\n+from keras.layers import ActivityRegularization\n+from keras.layers import Embedding\n from keras.datasets import mnist\n from keras.utils import np_utils\n from keras import regularizers\n@@ -44,7 +48,10 @@ def create_model(weight_reg=None, activity_reg=None):\n \n \n def test_W_reg():\n-    for reg in [regularizers.identity(), regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n+    for reg in [regularizers.identity(),\n+                regularizers.l1(),\n+                regularizers.l2(),\n+                regularizers.l1l2()]:\n         model = create_model(weight_reg=reg)\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         model.fit(X_train, Y_train, batch_size=batch_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7448dcea6591f632026e56f3246593782867923d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 29 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,10 +28,11 @@ def get_model(input_dim, nb_hidden, output_dim):\n \n def _test_optimizer(optimizer, target=0.9):\n     model = get_model(X_train.shape[1], 10, y_train.shape[1])\n-    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer=optimizer,\n+                  metrics=['accuracy'])\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n-                        validation_data=(X_test, y_test),\n-                        show_accuracy=True, verbose=2)\n+                        validation_data=(X_test, y_test), verbose=2)\n     config = optimizer.get_config()\n     assert type(config) == dict\n     assert history.history['val_acc'][-1] > target\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a066cf8680372f107f4da35b1518998a851e02b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 227 | Files Changed: 4 | Hunks: 65 | Methods Changed: 23 | Complexity Δ (Sum/Max): -9/6 | Churn Δ: 358 | Churn Cumulative: 3577 | Contributors (this commit): 12 | Commits (past 90d): 18 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1427,7 +1427,8 @@ class Container(Layer):\n     def __init__(self, input, output, name=None):\n         # handle name argument\n         if not name:\n-            name = self.__class__.__name__.lower() + '_' + str(id(self))\n+            prefix = self.__class__.__name__.lower()\n+            name = prefix + '_' + str(K.get_uid(prefix))\n         self.name = name\n \n         # Container-specific properties\n@@ -1487,16 +1488,18 @@ class Container(Layer):\n             if len(layer.inbound_nodes) > 1 or (layer.inbound_nodes and layer.inbound_nodes[0].inbound_layers):\n                 cls_name = self.__class__.__name__\n                 raise Exception(cls_name + ' inputs must come from '\n-                                'a Keras `Input` layer, '\n+                                'a Keras Input layer, '\n                                 'they cannot be the output of '\n                                 'a previous non-Input layer. '\n-                                'Here, the tensor specified as '\n+                                'Here, a tensor specified as '\n                                 'input to \"' + self.name +\n                                 '\" was not an Input tensor, '\n                                 'it was generated by layer ' +\n                                 layer.name + '.\\n'\n                                 'Note that input tensors are '\n-                                'instantiated via `tensor = Input(shape)`.')\n+                                'instantiated via `tensor = Input(shape)`.\\n'\n+                                'The tensor that caused the issue was: ' +\n+                                str(x.name))\n         for x in self.outputs:\n             if not hasattr(x, '_keras_history'):\n                 cls_name = self.__class__.__name__\n\n@@ -22,22 +22,21 @@ class LeakyReLU(Layer):\n     '''\n     def __init__(self, alpha=0.3, **kwargs):\n         self.supports_masking = True\n-        self.alpha = alpha\n+        self.alpha = K.cast_to_floatx(alpha)\n         super(LeakyReLU, self).__init__(**kwargs)\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        return K.relu(X, alpha=self.alpha)\n+    def call(self, x, mask=None):\n+        return K.relu(x, alpha=self.alpha)\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'alpha': self.alpha}\n+        config = {'alpha': self.alpha}\n         base_config = super(LeakyReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class PReLU(Layer):\n-    '''\n+    '''Parametric Rectified Linear Unit.\n+\n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n         (tuple of integers, does not include the samples axis)\n@@ -59,9 +58,8 @@ class PReLU(Layer):\n         self.initial_weights = weights\n         super(PReLU, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape[1:]\n-        self.alphas = self.init(input_shape,\n+    def build(self, input_shape):\n+        self.alphas = self.init(input_shape[1:],\n                                 name='{}_alphas'.format(self.name))\n         self.trainable_weights = [self.alphas]\n \n@@ -69,21 +67,20 @@ class PReLU(Layer):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        pos = K.relu(X)\n-        neg = self.alphas * (X - abs(X)) * 0.5\n+    def call(self, x, mask=None):\n+        pos = K.relu(x)\n+        neg = self.alphas * (x - abs(x)) * 0.5\n         return pos + neg\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'init': self.init.__name__}\n+        config = {'init': self.init.__name__}\n         base_config = super(PReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class ELU(Layer):\n-    '''\n+    '''Exponential Linear Unit.\n+\n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n         (tuple of integers, does not include the samples axis)\n@@ -100,18 +97,16 @@ class ELU(Layer):\n     '''\n     def __init__(self, alpha=1.0, **kwargs):\n         self.supports_masking = True\n-        self.alpha = alpha\n+        self.alpha = K.cast_to_floatx(alpha)\n         super(ELU, self).__init__(**kwargs)\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        pos = K.relu(X)\n-        neg = (X - abs(X)) * 0.5\n+    def call(self, x, mask=None):\n+        pos = K.relu(x)\n+        neg = (x - abs(x)) * 0.5\n         return pos + self.alpha * (K.exp(neg) - 1.)\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'alpha': self.alpha}\n+        config = {'alpha': self.alpha}\n         base_config = super(ELU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -138,13 +133,13 @@ class ParametricSoftplus(Layer):\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n                  weights=None, **kwargs):\n         self.supports_masking = True\n-        self.alpha_init = alpha_init\n-        self.beta_init = beta_init\n+        self.alpha_init = K.cast_to_floatx(alpha_init)\n+        self.beta_init = K.cast_to_floatx(beta_init)\n         self.initial_weights = weights\n         super(ParametricSoftplus, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape[1:]\n+    def build(self, input_shape):\n+        input_shape = input_shape[1:]\n         self.alphas = K.variable(self.alpha_init * np.ones(input_shape),\n                                  name='{}_alphas'.format(self.name))\n         self.betas = K.variable(self.beta_init * np.ones(input_shape),\n@@ -155,13 +150,11 @@ class ParametricSoftplus(Layer):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        return K.softplus(self.betas * X) * self.alphas\n+    def call(self, x, mask=None):\n+        return K.softplus(self.betas * x) * self.alphas\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'alpha_init': self.alpha_init,\n+        config = {'alpha_init': self.alpha_init,\n                   'beta_init': self.beta_init}\n         base_config = super(ParametricSoftplus, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -186,22 +179,20 @@ class ThresholdedLinear(Layer):\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         self.supports_masking = True\n-        self.theta = theta\n+        self.theta = K.cast_to_floatx(theta)\n         super(ThresholdedLinear, self).__init__(**kwargs)\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        return K.switch(K.abs(X) < self.theta, 0, X)\n+    def call(self, x, mask=None):\n+        return x * K.cast(x > self.theta, K.floatx())\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'theta': self.theta}\n+        config = {'theta': self.theta}\n         base_config = super(ThresholdedLinear, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class ThresholdedReLU(Layer):\n-    '''Thresholded Rectified Activation.\n+    '''Thresholded Rectified Linear Unit.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -219,22 +210,20 @@ class ThresholdedReLU(Layer):\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         self.supports_masking = True\n-        self.theta = theta\n+        self.theta = K.cast_to_floatx(theta)\n         super(ThresholdedReLU, self).__init__(**kwargs)\n \n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        return K.switch(X > self.theta, X, 0)\n+    def call(self, x, mask=None):\n+        return x * K.cast(x > self.theta, K.floatx())\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'theta': self.theta}\n+        config = {'theta': self.theta}\n         base_config = super(ThresholdedReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class SReLU(Layer):\n-    '''SReLU\n+    '''S-shaped Rectified Linear Unit.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -256,39 +245,44 @@ class SReLU(Layer):\n     def __init__(self, t_left_init='zero', a_left_init='glorot_uniform',\n                  t_right_init='glorot_uniform', a_right_init='one', **kwargs):\n         self.supports_masking = True\n-        self.t_left_init = initializations.get(t_left_init)\n-        self.a_left_init = initializations.get(a_left_init)\n-        self.t_right_init = initializations.get(t_right_init)\n-        self.a_right_init = initializations.get(a_right_init)\n+        self.t_left_init = t_left_init\n+        self.a_left_init = a_left_init\n+        self.t_right_init = t_right_init\n+        self.a_right_init = a_right_init\n         super(SReLU, self).__init__(**kwargs)\n \n-    def build(self):\n-        input_shape = self.input_shape[1:]\n-        self.t_left = self.t_left_init(input_shape,\n+    def build(self, input_shape):\n+        input_shape = input_shape[1:]\n+\n+        t_left_init = initializations.get(self.t_left_init)\n+        a_left_init = initializations.get(self.a_left_init)\n+        t_right_init = initializations.get(self.t_right_init)\n+        a_right_init = initializations.get(self.a_right_init)\n+\n+        self.t_left = t_left_init(input_shape,\n                                   name='{}_t_left'.format(self.name))\n-        self.a_left = self.a_left_init(input_shape,\n+        self.a_left = a_left_init(input_shape,\n                                   name='{}_a_left'.format(self.name))\n-        self.t_right = self.t_right_init(input_shape,\n+        self.t_right = t_right_init(input_shape,\n                                     name='{}_t_right'.format(self.name))\n-        self.a_right = self.a_right_init(input_shape,\n+        self.a_right = a_right_init(input_shape,\n                                     name='{}_a_right'.format(self.name))\n         # ensure the the right part is always to the right of the left\n         self.t_right_actual = self.t_left + abs(self.t_right)\n         self.trainable_weights = [self.t_left, self.a_left,\n                                   self.t_right, self.a_right]\n \n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        Y_left_and_center = self.t_left + K.relu(X - self.t_left,\n+    def call(self, x, mask=None):\n+        Y_left_and_center = self.t_left + K.relu(x - self.t_left,\n                                                  self.a_left,\n                                                  self.t_right_actual - self.t_left)\n-        Y_right = K.relu(X - self.t_right_actual) * self.a_right\n+        Y_right = K.relu(x - self.t_right_actual) * self.a_right\n         return Y_left_and_center + Y_right\n \n     def get_config(self):\n-        return {'name': self.__class__.__name__,\n-                'input_shape': self.input_shape,\n-                't_left_init': self.t_left_init,\n+        config = {'t_left_init': self.t_left_init,\n                   'a_left_init': self.a_left_init,\n                   't_right_init': self.t_right_init,\n                   'a_right_init': self.a_right_init}\n+        base_config = super(SReLU, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,9 +1,14 @@\n import numpy as np\n+from numpy.testing import assert_allclose\n+import inspect\n+\n from ..engine import Model, Input\n-from ..models import Sequential\n+from ..models import Sequential, model_from_json\n+from .. import backend as K\n \n \n-def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,), output_shape=(2,),\n+def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n+                  output_shape=(2,),\n                   classification=True, nb_class=2):\n     '''\n         classification=True overrides output_shape\n@@ -29,27 +34,43 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,), output_shape=(2\n     return (X[:nb_train], y[:nb_train]), (X[nb_train:], y[nb_train:])\n \n \n-def test_layer(layer, input_shape, input_dtype=None):\n+def test_layer(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n+               input_data=None, expected_output=None):\n     '''Test routine for a layer with a single input tensor\n     and single output tensor.\n     '''\n+    if input_data is None:\n+        assert input_shape\n         if not input_dtype:\n-        input_dtype = K.float()\n+            input_dtype = K.floatx()\n         input_data = (10 * np.random.random(input_shape)).astype(input_dtype)\n+    elif input_shape is None:\n+        input_shape = input_data.shape\n \n-    # test basic serialization\n-    layer_config = layer.get_config()\n-    layer = layer.__class__.from_config(layer_config)\n+    # instantiation\n+    layer = layer_cls(**kwargs)\n+\n+    # test get_weights , set_weights\n+    weights = layer.get_weights()\n+    layer.set_weights(weights)\n+\n+    # test and instantiation from weights\n+    if 'weights' in inspect.getargspec(layer_cls.__init__):\n+        kwargs['weights'] = weights\n+        layer = layer_cls(**kwargs)\n \n     # test in functional API\n     x = Input(shape=input_shape[1:], dtype=input_dtype)\n     y = layer(x)\n-    model = Model(x, y)\n+    model = Model(input=x, output=y)\n     model.compile('rmsprop', 'mse')\n \n     expected_output_shape = layer.get_output_shape_for(input_shape)\n-    actual_output_shape = model.predict(input_data).shape\n+    actual_output = model.predict(input_data)\n+    actual_output_shape = actual_output.shape\n     assert expected_output_shape == actual_output_shape\n+    if expected_output is not None:\n+        assert_allclose(actual_output, expected_output, rtol=1e-3)\n \n     # test serialization\n     model_config = model.get_config()\n@@ -62,8 +83,11 @@ def test_layer(layer, input_shape, input_dtype=None):\n     outer_model = Model(x_outer, y_outer)\n     outer_model.compile('rmsprop', 'mse')\n \n-    actual_output_shape = outer_model.predict(input_data).shape\n+    actual_output = outer_model.predict(input_data)\n+    actual_output_shape = actual_output.shape\n     assert expected_output_shape == actual_output_shape\n+    if expected_output is not None:\n+        assert_allclose(actual_output, expected_output, rtol=1e-3)\n \n     outer_model_config = outer_model.get_config()\n     outer_model = Model.from_config(outer_model_config)\n@@ -77,5 +101,15 @@ def test_layer(layer, input_shape, input_dtype=None):\n     model = Sequential()\n     model.add(layer)\n     model.compile('rmsprop', 'mse')\n-    actual_output_shape = model.predict(input_data).shape\n+    actual_output = model.predict(input_data)\n+    actual_output_shape = actual_output.shape\n     assert expected_output_shape == actual_output_shape\n+    if expected_output is not None:\n+        assert_allclose(actual_output, expected_output, rtol=1e-3)\n+\n+    # test JSON serialization\n+    json_model = model.to_json()\n+    model = model_from_json(json_model)\n+\n+    # for further checks in the caller function\n+    return actual_output\n\n@@ -1,181 +1,54 @@\n import pytest\n-from numpy.testing import assert_allclose\n-import numpy as np\n-from keras import backend as K\n-\n-\n-def get_standard_values():\n-    '''\n-    These are just a set of floats used for testing the activation\n-    functions, and are useful in multiple tests.\n-\n-    The values should all be non-negative because they and their negatives\n-    are used to test the ReLU derivates in this file.\n-    '''\n-    return np.array([[0, 0.1, 0.5, 0.9, 1.0, 10, 1e2, 0.01]], dtype=K.floatx())\n+from keras.utils.test_utils import test_layer\n \n \n def test_leaky_relu():\n-    np.random.seed(1337)\n     from keras.layers.advanced_activations import LeakyReLU\n-    inp = get_standard_values()\n     for alpha in [0., .5, -1.]:\n-        layer = LeakyReLU(alpha=alpha)\n-        layer.input = K.variable(inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, inp)\n-\n-        layer.input = K.variable(-inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, -inp * alpha)\n-\n-        config = layer.get_config()\n-        assert config['alpha'] == alpha\n+        test_layer(LeakyReLU, kwargs={'alpha': alpha},\n+                   input_shape=(2, 3, 4))\n \n \n def test_prelu():\n     from keras.layers.advanced_activations import PReLU\n-    np.random.seed(1337)\n-    inp = get_standard_values()\n-\n-    for train in [True, False]:\n-        # test with custom weights\n-        alphas = np.random.random(inp.shape)\n-        layer = PReLU(weights=alphas, input_shape=inp.flatten().shape)\n-        # calling build here causes an error, unclear if this is a bug\n-        # layer.build()\n-\n-        layer.input = K.variable(inp)\n-        outp = K.eval(layer.get_output(train))\n-        assert_allclose(inp, outp)\n-\n-        layer.input = K.variable(-inp)\n-        outp = K.eval(layer.get_output(train))\n-        assert_allclose(-alphas * inp, outp)\n-\n-        # test with default weights\n-        layer = PReLU(input_shape=inp.flatten().shape)\n-        # layer.build()\n-        layer.input = K.variable(inp)\n-        outp = K.eval(layer.get_output(train))\n-        assert_allclose(inp, outp)\n-\n-        layer.input = K.variable(-inp)\n-        outp = K.eval(layer.get_output(train))\n-\n-        assert_allclose(0., alphas * outp)\n-\n-        layer.get_config()\n+    test_layer(PReLU, kwargs={},\n+               input_shape=(2, 3, 4))\n \n \n def test_elu():\n     from keras.layers.advanced_activations import ELU\n-    np.random.seed(1337)\n-    inp = get_standard_values()\n-    for alpha in [0.1, .5, -1., 1.]:\n-        layer = ELU(alpha=alpha)\n-        layer.input = K.variable(inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, inp, rtol=1e-3)\n-\n-        layer.input = K.variable(-inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, alpha * (np.exp(-inp) - 1.), rtol=1e-3)\n-\n-        config = layer.get_config()\n-        assert config['alpha'] == alpha\n+    for alpha in [0., .5, -1.]:\n+        test_layer(ELU, kwargs={'alpha': alpha},\n+                   input_shape=(2, 3, 4))\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n-    np.random.seed(1337)\n-    inp = np.vstack((get_standard_values(), -get_standard_values()))\n-    # large values cause overflow in exp\n-    inp = inp[:-2]\n-    for alpha in [.5, -1., 1., 5]:\n-        for beta in [.5, -1., 1., 2]:\n-            layer = ParametricSoftplus(alpha_init=alpha,\n-                                       beta_init=beta,\n-                                       input_shape=inp.shape)\n-            layer.input = K.variable(inp)\n-            layer.build()\n-            for train in [True, False]:\n-                outp = K.eval(layer.get_output(train))\n-                assert_allclose(outp, alpha * np.log(1. + np.exp(beta * inp)),\n-                                atol=1e-3)\n-\n-            config = layer.get_config()\n-            assert config['alpha_init'] == alpha\n-            assert config['beta_init'] == beta\n+    for alpha in [0., .5, -1.]:\n+        test_layer(ParametricSoftplus,\n+                   kwargs={'alpha_init': 1.,\n+                           'beta_init': -1},\n+                   input_shape=(2, 3, 4))\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_thresholded_linear():\n     from keras.layers.advanced_activations import ThresholdedLinear\n-    np.random.seed(1337)\n-    inp = get_standard_values()\n-    for theta in [0., .5, 1.]:\n-        layer = ThresholdedLinear(theta=theta)\n-        layer.input = K.variable(inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, inp * (np.abs(inp) >= theta))\n-\n-        layer.input = K.variable(-inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, -inp * (np.abs(inp) >= theta))\n-\n-        config = layer.get_config()\n-        assert config['theta'] == theta\n+    test_layer(ThresholdedLinear, kwargs={'theta': 0.5},\n+               input_shape=(2, 3, 4))\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n-    np.random.seed(1337)\n-    inp = get_standard_values()\n-    for theta in [-1, 0., .5, 1.]:\n-        layer = ThresholdedReLU(theta=theta)\n-        layer.input = K.variable(inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, inp * (inp > theta))\n-\n-        layer.input = K.variable(-inp)\n-        for train in [True, False]:\n-            outp = K.eval(layer.get_output(train))\n-            assert_allclose(outp, -inp * (-inp > theta))\n-\n-        config = layer.get_config()\n-        assert config['theta'] == theta\n+    test_layer(ThresholdedReLU, kwargs={'theta': 0.5},\n+               input_shape=(2, 3, 4))\n \n \n def test_srelu():\n     from keras.layers.advanced_activations import SReLU\n-    np.random.seed(1337)\n-    inp = np.array([-2, -1., -0.5, 0., 0.5, 1., 2.])\n-    out = np.array([-1.5, -1., -0.5, 0., 0.5, 1., 3.])\n-    input_size = len(inp)\n-    for train in [True, False]:\n-        layer = SReLU(input_shape=inp.flatten().shape)\n-        ones_proto = np.ones(input_size)\n-        layer.set_weights([ones_proto * -1., ones_proto * 0.5,\n-                           ones_proto * 2., ones_proto * 2.])\n-        layer.input = K.variable(inp)\n-        outp = K.eval(layer.get_output(train))\n-        assert_allclose(out, outp)\n-\n-        layer.get_config()\n+    test_layer(SReLU, kwargs={},\n+               input_shape=(2, 3, 4))\n \n \n if __name__ == '__main__':\n-    pytest.main([__file__])\n+    # pytest.main([__file__])\n+    test_srelu()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf4dab3501c62836f94ea17d2f0e198348f5293d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 474 | Files Changed: 9 | Hunks: 39 | Methods Changed: 54 | Complexity Δ (Sum/Max): -19/2 | Churn Δ: 631 | Churn Cumulative: 15516 | Contributors (this commit): 49 | Commits (past 90d): 70 | Contributors (cumulative): 75 | DMM Complexity: 0.0\n\nDIFF:\n@@ -89,7 +89,6 @@ nonneg = NonNeg\n unitnorm = UnitNorm\n \n from .utils.generic_utils import get_from_module\n-\n-\n def get(identifier, kwargs=None):\n-    return get_from_module(identifier, globals(), 'constraint', instantiate=True, kwargs=kwargs)\n+    return get_from_module(identifier, globals(), 'constraint',\n+                           instantiate=True, kwargs=kwargs)\n\n@@ -1014,7 +1014,7 @@ class Merge(Layer):\n             a list of layer instances. Must be more\n             than one layer/tensor.\n         mode: string or lambda/function. If string, must be one\n-            of: 'sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot'.\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n         concat_axis: integer, axis to use in mode `concat`.\n\n@@ -820,17 +820,15 @@ class Highway(Layer):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def call(self, train=False):\n-        X = self.get_input(train)\n-        transform_weight = activations.sigmoid(K.dot(X, self.W_carry) + self.b_carry)\n-        act = self.activation(K.dot(X, self.W) + self.b)\n+    def call(self, x, mask=None):\n+        transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n+        act = self.activation(K.dot(x, self.W) + self.b)\n         act *= transform_weight\n-        output = act + (1 - transform_weight) * X\n+        output = act + (1 - transform_weight) * x\n         return output\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'init': self.init.__name__,\n+        config = {'init': self.init.__name__,\n                   'transform_bias': self.transform_bias,\n                   'activation': self.activation.__name__,\n                   'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n@@ -968,8 +966,7 @@ class TimeDistributedDense(Layer):\n         return y\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'output_dim': self.output_dim,\n+        config = {'output_dim': self.output_dim,\n                   'init': self.init.__name__,\n                   'activation': self.activation.__name__,\n                   'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n\n@@ -18,6 +18,14 @@ def get_from_module(identifier, module_params, module_name,\n             return res(**kwargs)\n         else:\n             return res\n+    elif type(identifier) is dict:\n+        name = identifier.pop('name')\n+        res = module_params.get(name)\n+        if res:\n+            return res(**identifier)\n+        else:\n+            raise Exception('Invalid ' + str(module_name) + ': ' +\n+                            str(identifier))\n     return identifier\n \n \n\n@@ -34,7 +34,7 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n     return (X[:nb_train], y[:nb_train]), (X[nb_train:], y[nb_train:])\n \n \n-def test_layer(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n+def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n                input_data=None, expected_output=None):\n     '''Test routine for a layer with a single input tensor\n     and single output tensor.\n@@ -63,7 +63,7 @@ def test_layer(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     x = Input(shape=input_shape[1:], dtype=input_dtype)\n     y = layer(x)\n     model = Model(input=x, output=y)\n-    model.compile('rmsprop', 'mse')\n+    model.compile('rmsprop', 'mse', mode='FAST_COMPILE')\n \n     expected_output_shape = layer.get_output_shape_for(input_shape)\n     actual_output = model.predict(input_data)\n\n@@ -9,33 +9,6 @@ from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n \n \n-def test_lambda_serialization():\n-    from keras.layers import Lambda\n-    from keras.utils.layer_utils import layer_from_config\n-    ld = Lambda(lambda x: x + 1)\n-    config = ld.get_config()\n-    ld = Lambda.from_config(config)\n-\n-    def f(x):\n-        return x + 1\n-    ld = Lambda(f)\n-    config = ld.get_config()\n-    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n-\n-    ld = Lambda(lambda x: K.concatenate([K.square(x), x]),\n-                output_shape=lambda s: tuple(list(s)[:-1] + [2 * s[-1]]))\n-    config = ld.get_config()\n-    ld = Lambda.from_config(config)\n-\n-    def f(x):\n-        return K.concatenate([K.square(x), x])\n-    def f_shape(s):\n-        return tuple(list(s)[:-1] + [2 * s[-1]])\n-    ld = Lambda(f, output_shape=f_shape)\n-    config = ld.get_config()\n-    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n-\n-\n def test_learning_phase():\n     a = Input(shape=(32,), name='input_a')\n     b = Input(shape=(32,), name='input_b')\n\n@@ -1,31 +1,31 @@\n import pytest\n-from keras.utils.test_utils import test_layer\n+from keras.utils.test_utils import layer_test\n \n \n def test_leaky_relu():\n     from keras.layers.advanced_activations import LeakyReLU\n     for alpha in [0., .5, -1.]:\n-        test_layer(LeakyReLU, kwargs={'alpha': alpha},\n+        layer_test(LeakyReLU, kwargs={'alpha': alpha},\n                    input_shape=(2, 3, 4))\n \n \n def test_prelu():\n     from keras.layers.advanced_activations import PReLU\n-    test_layer(PReLU, kwargs={},\n+    layer_test(PReLU, kwargs={},\n                input_shape=(2, 3, 4))\n \n \n def test_elu():\n     from keras.layers.advanced_activations import ELU\n     for alpha in [0., .5, -1.]:\n-        test_layer(ELU, kwargs={'alpha': alpha},\n+        layer_test(ELU, kwargs={'alpha': alpha},\n                    input_shape=(2, 3, 4))\n \n \n def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n     for alpha in [0., .5, -1.]:\n-        test_layer(ParametricSoftplus,\n+        layer_test(ParametricSoftplus,\n                    kwargs={'alpha_init': 1.,\n                            'beta_init': -1},\n                    input_shape=(2, 3, 4))\n@@ -33,19 +33,19 @@ def test_parametric_softplus():\n \n def test_thresholded_linear():\n     from keras.layers.advanced_activations import ThresholdedLinear\n-    test_layer(ThresholdedLinear, kwargs={'theta': 0.5},\n+    layer_test(ThresholdedLinear, kwargs={'theta': 0.5},\n                input_shape=(2, 3, 4))\n \n \n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n-    test_layer(ThresholdedReLU, kwargs={'theta': 0.5},\n+    layer_test(ThresholdedReLU, kwargs={'theta': 0.5},\n                input_shape=(2, 3, 4))\n \n \n def test_srelu():\n     from keras.layers.advanced_activations import SReLU\n-    test_layer(SReLU, kwargs={},\n+    layer_test(SReLU, kwargs={},\n                input_shape=(2, 3, 4))\n \n \n\n@@ -1,212 +0,0 @@\n-\"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n-\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras.layers.core import Dense\n-from keras.models import Sequential, Graph\n-\n-\n-def test_layer_call():\n-    \"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n-    nb_samples, input_dim, output_dim = 3, 10, 5\n-    layer = Dense(output_dim, input_dim=input_dim)\n-    W = np.asarray(K.eval(layer.W)).astype(K.floatx())\n-    X = K.placeholder(ndim=2)\n-    Y = layer(X)\n-    f = K.function([X], [Y])\n-\n-    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    y = f([x])[0].astype(K.floatx())\n-    t = np.dot(x, W).astype(K.floatx())\n-    assert_allclose(t, y, rtol=.2)\n-\n-\n-def test_sequential_call():\n-    \"\"\"Test keras.models.Sequential.__call__\"\"\"\n-    nb_samples, input_dim, output_dim = 3, 10, 5\n-    model = Sequential()\n-    model.add(Dense(output_dim=output_dim, input_dim=input_dim))\n-    model.compile('sgd', 'mse')\n-\n-    # test flat model\n-    X = K.placeholder(ndim=2)\n-    Y = model(X)\n-    f = K.function([X], [Y])\n-\n-    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model.predict(x)\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-    # test nested model\n-    model2 = Sequential()\n-    model2.add(model)\n-    model2.compile('sgd', 'mse')\n-\n-    Y2 = model2(X)\n-    f = K.function([X], [Y2])\n-\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model2.predict(x)\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-\n-def test_graph_call():\n-    \"\"\"Test keras.models.Graph.__call__\"\"\"\n-    nb_samples, input_dim, output_dim = 3, 10, 5\n-    model = Graph()\n-    model.add_input('input', input_shape=(input_dim, ))\n-    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                   input='input', name='output', create_output=True)\n-\n-    model.compile('sgd', {'output': 'mse'})\n-\n-    # test flat model\n-    X = K.placeholder(ndim=2)\n-    Y = model(X)\n-    f = K.function([X], [Y])\n-\n-    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model.predict({'input': x})['output']\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-    # test nested Graph models\n-    model2 = Graph()\n-    model2.add_input('input', input_shape=(input_dim, ))\n-    model2.add_node(model, input='input', name='output', create_output=True)\n-    # need to turn off cache because we're reusing model\n-    model2.cache_enabled = False\n-    model2.compile('sgd', {'output': 'mse'})\n-\n-    Y2 = model2(X)\n-    f = K.function([X], [Y2])\n-\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model2.predict({'input': x})['output']\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-\n-def test_graph_multiple_in_out_call():\n-    \"\"\"Test keras.models.Graph.__call__ with multiple inputs\"\"\"\n-    nb_samples, input_dim, output_dim = 3, 10, 5\n-    model = Graph()\n-    model.add_input('input1', input_shape=(input_dim, ))\n-    model.add_input('input2', input_shape=(input_dim, ))\n-    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                   inputs=['input1', 'input2'], merge_mode='sum', name='output', create_output=True)\n-\n-    model.compile('sgd', {'output': 'mse'})\n-\n-    # test flat model\n-    X1 = K.placeholder(ndim=2)\n-    X2 = K.placeholder(ndim=2)\n-    Y = model({'input1': X1, 'input2': X2})['output']\n-    f = K.function([X1, X2], [Y])\n-\n-    x1 = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    x2 = np.ones((nb_samples, input_dim)).astype(K.floatx()) * -2\n-    y1 = f([x1, x2])[0].astype(K.floatx())\n-    y2 = model.predict({'input1': x1, 'input2': x2})['output']\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-    # test with single input, multiple outputs\n-    model2 = Graph()\n-    model2.add_input('input', input_shape=(input_dim, ))\n-    model2.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                    input='input', name='output1', create_output=True)\n-    model2.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                    input='input', name='output2', create_output=True)\n-\n-    model2.compile('sgd', {'output1': 'mse', 'output2': 'mse'})\n-\n-    # test flat model\n-    X = K.placeholder(ndim=2)\n-    Y = model2(X)\n-    f = K.function([X], [Y['output1'], Y['output2']])\n-\n-    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    out = f([x])\n-    y1a = out[0].astype(K.floatx())\n-    y1b = out[1].astype(K.floatx())\n-    y2 = model2.predict({'input': x})\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1a, y2['output1'])\n-    assert_allclose(y1b, y2['output2'])\n-\n-    # test with multiple inputs, multiple outputs\n-    model3 = Graph()\n-    model3.add_input('input1', input_shape=(input_dim, ))\n-    model3.add_input('input2', input_shape=(input_dim, ))\n-    model3.add_shared_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                           inputs=['input1', 'input2'], name='output',\n-                           outputs=['output1', 'output2'], create_output=True)\n-    model3.compile('sgd', {'output1': 'mse', 'output2': 'mse'})\n-\n-    # test flat model\n-    Y = model3({'input1': X1, 'input2': X2})\n-    f = K.function([X1, X2], [Y['output1'], Y['output2']])\n-\n-    x1 = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-    x2 = np.ones((nb_samples, input_dim)).astype(K.floatx()) * -2\n-    out = f([x1, x2])\n-    y1a = out[0].astype(K.floatx())\n-    y1b = out[1].astype(K.floatx())\n-    y2 = model3.predict({'input1': x1, 'input2': x2})\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1a, y2['output1'])\n-    assert_allclose(y1b, y2['output2'])\n-\n-\n-def test_nested_call():\n-    \"\"\"Test nested Sequential and Graph models\"\"\"\n-    nb_samples, input_dim, output_dim = 3, 10, 5\n-    X = K.placeholder(ndim=2)\n-    x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n-\n-    # test Graph model nested inside Sequential model\n-    model = Graph()\n-    model.add_input('input', input_shape=(input_dim, ))\n-    model.add_node(Dense(output_dim=output_dim, input_dim=input_dim),\n-                   input='input', name='output', create_output=True)\n-\n-    model2 = Sequential()\n-    model2.add(model)\n-    model2.compile('sgd', 'mse')\n-\n-    Y2 = model2(X)\n-    f = K.function([X], [Y2])\n-\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model2.predict(x)\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-    # test Sequential model inside Graph model\n-    model3 = Sequential()\n-    model3.add(Dense(output_dim=output_dim, input_dim=input_dim))\n-\n-    model4 = Graph()\n-    model4.add_input('input', input_shape=(input_dim, ))\n-    model4.add_node(model3, input='input', name='output', create_output=True)\n-    model4.compile('sgd', {'output': 'mse'})\n-\n-    Y2 = model4(X)\n-    f = K.function([X], [Y2])\n-\n-    y1 = f([x])[0].astype(K.floatx())\n-    y2 = model4.predict({'input': x})['output']\n-    # results of __call__ should match model.predict\n-    assert_allclose(y1, y2)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,258 +1,176 @@\n import pytest\n import numpy as np\n-from keras.models import Sequential\n from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras.layers import core\n-from keras.layers import containers\n+from keras.utils.test_utils import layer_test\n \n \n-def test_input_output():\n-    nb_samples = 10\n-    input_dim = 5\n-    layer = core.Layer()\n-\n-    # Once an input is provided, it should be reachable through the\n-    # appropriate getters\n-    input = np.ones((nb_samples, input_dim))\n-    layer.input = K.variable(input)\n-    for train in [True, False]:\n-        assert_allclose(K.eval(layer.get_input(train)), input)\n-        assert_allclose(K.eval(layer.get_output(train)), input)\n-\n-\n-def test_connections():\n-    nb_samples = 10\n-    input_dim = 5\n-    layer1 = core.Layer()\n-    layer2 = core.Layer()\n-\n-    input = np.ones((nb_samples, input_dim))\n-    layer1.input = K.variable(input)\n-\n-    # After connecting, input of layer1 should be passed through\n-    layer2.set_previous(layer1)\n-    for train in [True, False]:\n-        assert_allclose(K.eval(layer2.get_input(train)), input)\n-        assert_allclose(K.eval(layer2.get_output(train)), input)\n-\n-\n-def test_base():\n-    layer = core.Layer()\n-    _runner(layer)\n-\n-\n-def test_masked():\n-    layer = core.MaskedLayer()\n-    _runner(layer)\n+def test_masking():\n+    layer_test(core.Masking,\n+               kwargs={},\n+               input_shape=(3, 2, 3))\n \n \n def test_merge():\n-    layer_1 = core.Layer()\n-    layer_2 = core.Layer()\n-    layer_1.set_input_shape((None,))\n-    layer_2.set_input_shape((None,))\n-    layer = core.Merge([layer_1, layer_2])\n-    _runner(layer)\n+    # test modes: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+    # test lambda with output_shape lambda\n+    # test function with output_shape function\n+    pass\n \n \n def test_dropout():\n-    layer = core.Dropout(0.5)\n-    _runner(layer)\n+    layer_test(core.Dropout,\n+               kwargs={'p': 0.5},\n+               input_shape=(3, 2))\n \n \n def test_activation():\n-    layer = core.Activation('linear')\n-    _runner(layer)\n+    # with string argument\n+    layer_test(core.Activation,\n+               kwargs={'activation': 'relu'},\n+               input_shape=(3, 2))\n+\n+    # with function argument\n+    layer_test(core.Activation,\n+               kwargs={'activation': K.relu},\n+               input_shape=(3, 2))\n \n \n def test_reshape():\n-    layer = core.Reshape(dims=(10, 10))\n-    _runner(layer)\n+    layer_test(core.Reshape,\n+               kwargs={'target_shape': (8, 1)},\n+               input_shape=(3, 2, 4))\n+\n+\n+def test_permute():\n+    layer_test(core.Permute,\n+               kwargs={'dims': (2, 1)},\n+               input_shape=(3, 2, 4))\n \n \n def test_flatten():\n-    layer = core.Flatten()\n-    _runner(layer)\n+    layer_test(core.Flatten,\n+               kwargs={},\n+               input_shape=(3, 2, 4))\n \n \n def test_repeat_vector():\n-    layer = core.RepeatVector(10)\n-    _runner(layer)\n+    layer_test(core.RepeatVector,\n+               kwargs={'n': 3},\n+               input_shape=(3, 2))\n+\n+\n+def test_lambda():\n+    from keras.utils.layer_utils import layer_from_config\n+    Lambda = core.Lambda\n+\n+    layer_test(Lambda,\n+               kwargs={'function': lambda x: x + 1},\n+               input_shape=(3, 2))\n+\n+    # test serialization with function\n+    def f(x):\n+        return x + 1\n+\n+    ld = Lambda(f)\n+    config = ld.get_config()\n+    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n+\n+    ld = Lambda(lambda x: K.concatenate([K.square(x), x]),\n+                output_shape=lambda s: tuple(list(s)[:-1] + [2 * s[-1]]))\n+    config = ld.get_config()\n+    ld = Lambda.from_config(config)\n+\n+    # test serialization with output_shape function\n+    def f(x):\n+        return K.concatenate([K.square(x), x])\n+\n+    def f_shape(s):\n+        return tuple(list(s)[:-1] + [2 * s[-1]])\n+\n+    ld = Lambda(f, output_shape=f_shape)\n+    config = ld.get_config()\n+    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n \n \n def test_dense():\n-    layer = core.Dense(10, input_shape=(10,))\n-    _runner(layer)\n+    from keras import regularizers\n+    from keras import constraints\n+\n+    layer_test(core.Dense,\n+               kwargs={'output_dim': 3},\n+               input_shape=(3, 2))\n+\n+    layer_test(core.Dense,\n+               kwargs={'output_dim': 3,\n+                       'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2))\n \n \n-def test_act_reg():\n-    layer = core.ActivityRegularization(0.5, 0.5)\n-    _runner(layer)\n-\n-\n-def test_time_dist_dense():\n-    layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n-    _runner(layer)\n-\n-\n-def test_time_dist_merge():\n-    layer = core.TimeDistributedMerge()\n-    _runner(layer)\n-\n-\n-def test_highway():\n-    layer = core.Highway(input_shape=(10,))\n-    _runner(layer)\n-\n-\n-def test_autoencoder():\n-    layer_1 = core.Layer()\n-    layer_2 = core.Layer()\n-\n-    layer = core.AutoEncoder(layer_1, layer_2)\n-    _runner(layer)\n-\n-\n-def test_autoencoder_advanced():\n-    encoder = containers.Sequential([core.Dense(5, input_shape=(10,))])\n-    decoder = containers.Sequential([core.Dense(10, input_shape=(5,))])\n-    X_train = np.random.random((100, 10))\n-    X_test = np.random.random((100, 10))\n-\n-    model = Sequential()\n-    model.add(core.Dense(output_dim=10, input_dim=10))\n-    autoencoder = core.AutoEncoder(encoder=encoder, decoder=decoder,\n-                                   output_reconstruction=True)\n-    model.add(autoencoder)\n-\n-    # training the autoencoder:\n-    model.compile(optimizer='sgd', loss='mse')\n-    assert autoencoder.output_reconstruction\n-\n-    model.fit(X_train, X_train, nb_epoch=1, batch_size=32)\n-\n-    # predicting compressed representations of inputs:\n-    autoencoder.output_reconstruction = False  # the autoencoder has to be recompiled after modifying this property\n-    assert not autoencoder.output_reconstruction\n-    model.compile(optimizer='sgd', loss='mse')\n-    representations = model.predict(X_test)\n-    assert representations.shape == (100, 5)\n-\n-    # the model is still trainable, although it now expects compressed representations as targets:\n-    model.fit(X_test, representations, nb_epoch=1, batch_size=32)\n-\n-    # to keep training against the original inputs, just switch back output_reconstruction to True:\n-    autoencoder.output_reconstruction = True\n-    model.compile(optimizer='sgd', loss='mse')\n-    model.fit(X_train, X_train, nb_epoch=1)\n-\n-    reconstructions = model.predict(X_test)\n-    assert reconstructions.shape == (100, 10)\n+def test_activity_regularization():\n+    layer_test(core.ActivityRegularization,\n+               kwargs={'l1': 0.01, 'l2': 0.01},\n+               input_shape=(3, 2, 3))\n \n \n def test_maxout_dense():\n-    layer = core.MaxoutDense(10, 10, input_shape=(20,))\n-    _runner(layer)\n+    from keras import regularizers\n+    from keras import constraints\n+\n+    layer_test(core.MaxoutDense,\n+               kwargs={'output_dim': 3},\n+               input_shape=(3, 2))\n+\n+    layer_test(core.MaxoutDense,\n+               kwargs={'output_dim': 3,\n+                       'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2))\n \n \n-def test_naming():\n-    layer = core.Dense(2, input_dim=2)\n-    assert layer.name == 'dense'\n+def test_highway():\n+    from keras import regularizers\n+    from keras import constraints\n \n-    model = Sequential()\n-    model.add(core.Dense(2, input_dim=2, name='my_dense'))\n-    model.add(core.Dense(2, name='my_dense'))\n+    layer_test(core.Highway,\n+               kwargs={},\n+               input_shape=(3, 2))\n \n-    assert model.layers[0].name == 'my_dense'\n-    assert model.layers[1].name == 'my_dense'\n-\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.random.random((2, 2)), np.random.random((2, 2)))\n+    layer_test(core.Highway,\n+               kwargs={'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2))\n \n \n-def test_sequences():\n-    '''Test masking sequences with zeroes as padding'''\n-    # integer inputs, one per timestep, like embeddings\n-    layer = core.Masking(input_shape=(4, 1))\n-    func = K.function([layer.get_input(True)], [layer.get_output_mask()])\n-    input_data = np.array([[[1], [2], [3], [0]],\n-                           [[0], [4], [5], [0]]], dtype=np.int32)\n+def test_timedistributeddense():\n+    from keras import regularizers\n+    from keras import constraints\n \n-    # This is the expected output mask, one dimension less\n-    expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n+    layer_test(core.TimeDistributedDense,\n+               kwargs={'output_dim': 2, 'input_length': 2},\n+               input_shape=(3, 2, 3))\n \n-    # get mask for this input\n-    output = func([input_data])[0]\n-    assert np.all(output == expected), 'Output not as expected'\n+    layer_test(core.TimeDistributedDense,\n+               kwargs={'output_dim': 3,\n+                       'W_regularizer': regularizers.l2(0.01),\n+                       'b_regularizer': regularizers.l1(0.01),\n+                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'W_constraint': constraints.MaxNorm(1),\n+                       'b_constraint': constraints.MaxNorm(1)},\n+               input_shape=(3, 2, 3))\n \n \n-def test_non_zero():\n-    '''Test masking with non-zero mask value'''\n-    layer = core.Masking(5, input_shape=(4, 2))\n-    func = K.function([layer.input], [layer.get_output_mask()])\n-    input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                           [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                          dtype=np.int32)\n-    output = func([input_data])[0]\n-    expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n-    assert np.all(output == expected), 'Output not as expected'\n-\n-\n-def test_non_zero_output():\n-    '''Test output of masking layer with non-zero mask value'''\n-    layer = core.Masking(5, input_shape=(4, 2))\n-    func = K.function([layer.input], [layer.get_output()])\n-\n-    input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                           [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                          dtype=np.int32)\n-    output = func([input_data])[0]\n-    expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n-                         [[1, 5], [5, 0], [0, 0], [0, 0]]])\n-    assert np.all(output == expected), 'Output not as expected'\n-\n-\n-def _runner(layer):\n-    assert isinstance(layer, core.Layer)\n-    layer.build()\n-    conf = layer.get_config()\n-    assert (type(conf) == dict)\n-\n-    param = layer.get_params()\n-    # Typically a list or a tuple, but may be any iterable\n-    assert hasattr(param, '__iter__')\n-\n-    # Test the setter for the trainable attribute\n-    layer.trainable = True\n-    layer.trainable = False\n-\n-\n-def test_siamese_all():\n-    right_input_layer = core.Dense(7, input_dim=3)\n-    left_input_layer = core.Dense(7, input_dim=3)\n-\n-    shared_layer = core.Dense(5,input_dim=7)\n-    for mode in ['sum', 'mul', 'ave', 'concat']:\n-        siamese_layer = core.Siamese(shared_layer, [left_input_layer, right_input_layer], merge_mode=mode)\n-        siamese_layer.output_shape\n-        siamese_layer.get_output()\n-\n-\n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n-def test_siamese_theano_only():\n-    right_input_layer = core.Dense(7, input_dim=3)\n-    left_input_layer = core.Dense(7, input_dim=3)\n-\n-    shared_layer = core.Dense(5,input_dim=7)\n-\n-    for mode in ['dot', 'cos']:\n-        siamese_layer = core.Siamese(shared_layer, [left_input_layer, right_input_layer], merge_mode=mode,\n-                                     dot_axes=([1], [1]))\n-        siamese_layer.output_shape\n-        siamese_layer.get_output()\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
