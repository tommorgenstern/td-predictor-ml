{"custom_id": "keras#5899b057aeb3b26217e87a8379f0de6a1c29c50f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 30 | Churn Cumulative: 21810 | Contributors (this commit): 125 | Commits (past 90d): 44 | Contributors (cumulative): 136 | DMM Complexity: 0.9444444444444444\n\nDIFF:\n@@ -281,6 +281,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._init_batch_counters()\n     self._base_model_initialized = True\n \n+    # `jit_compile` starts off with None as default and gets overwritten by the\n+    # value specified in `Model.compile`, and this is effective for `fit`,\n+    # `evaluate`, and `predict`.\n+    self._jit_compile = None\n+\n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _init_batch_counters(self):\n     # Untracked Variables, used to keep track of mini-batches seen in `fit`,\n@@ -1507,6 +1512,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           model._test_counter.assign_add(1)  # pylint: disable=protected-access\n         return outputs\n \n+      if self._jit_compile:\n+        run_step = tf.function(\n+            run_step, jit_compile=True, experimental_relax_shapes=True)\n+\n       data = next(iterator)\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n       outputs = reduce_per_replica(\n@@ -1791,6 +1800,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           model._predict_counter.assign_add(1)  # pylint: disable=protected-access\n         return outputs\n \n+      if self._jit_compile:\n+        run_step = tf.function(\n+            run_step, jit_compile=True, experimental_relax_shapes=True)\n+\n       data = next(iterator)\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n       outputs = reduce_per_replica(\n\n@@ -124,7 +124,7 @@ class TrainingTest(test_combinations.TestCase):\n     model.fit(model_input, np.random.random((10, 1)), epochs=1, batch_size=10)\n \n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-  def test_compile_fit_with_mirrored_strategy(self):\n+  def test_compile_fit_evaluate_predict_with_mirrored_strategy(self):\n     # Test with jit_compile = True\n     strategy = tf.distribute.MirroredStrategy()\n     with strategy.scope():\n@@ -132,6 +132,8 @@ class TrainingTest(test_combinations.TestCase):\n     model.compile('sgd', loss='mse', run_eagerly=False, jit_compile=True)\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model.fit(x, y, epochs=2)\n+    model.evaluate(x, y)\n+    model.predict(x)\n \n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_verify_xla_compile_with_jit_compile(self):\n@@ -154,6 +156,19 @@ class TrainingTest(test_combinations.TestCase):\n         model.fit(input_array, expected_output, epochs=1)\n         model.predict(input_array)\n \n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  def test_jit_compile_for_compile_evaluate_predict(self):\n+    # Test with jit_compile = True for model.compile(), model.evaluate(),\n+    # model.predict()\n+    model = sequential.Sequential([layers_module.Dense(1)])\n+    self.assertIsNone(model._jit_compile)\n+    model.compile('sgd', loss='mse', run_eagerly=False, jit_compile=True)\n+    self.assertTrue(model._jit_compile)\n+    x, y = np.ones((10, 1)), np.ones((10, 1))\n+    model.fit(x, y, epochs=2)\n+    model.evaluate(x, y)\n+    model.predict(x)\n+\n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_fit_without_loss_at_compile(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ecef127f70db723c158dbe9ed3268b3d610ab55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 225 | Lines Deleted: 765 | Files Changed: 14 | Hunks: 86 | Methods Changed: 58 | Complexity Δ (Sum/Max): -51/5 | Churn Δ: 990 | Churn Cumulative: 38506 | Contributors (this commit): 137 | Commits (past 90d): 74 | Contributors (cumulative): 207 | DMM Complexity: 0.8659420289855072\n\nDIFF:\n@@ -346,12 +346,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # Ensures the same metric is not added multiple times in `MirroredStrategy`.\n     self._metrics_lock = threading.Lock()\n \n-    # Both graph and subclassed networks have a dtype policy. For graph\n-    # networks, the policy's compute and variable dtypes are ignored. Such\n-    # networks only use the policy if it is a PolicyV1, in which case it uses\n-    # the PolicyV1's loss_scale (Policy does not have a loss_scale). For\n-    # subclassed networks, the compute and variable dtypes are used as like any\n-    # ordinary layer.\n+    # Note that models also have a dtype policy, as they are layers. For\n+    # functional models, the policy is only used in Model.compile, which wraps\n+    # the optimizer with a LossScaleOptimizer if the policy name is\n+    # \"mixed_float16\". Subclassed models additionally use the policy's compute\n+    # and variable dtypes, as like any ordinary layer.\n     self._set_dtype_policy(dtype)\n     # Boolean indicating whether the layer automatically casts its inputs to the\n     # layer's compute_dtype.\n\n@@ -171,12 +171,11 @@ class Layer(base_layer.Layer):\n     # added using the `add_metric` API.\n     self._metrics = []\n \n-    # Both graph and subclassed networks have a dtype policy. For graph\n-    # networks, the policy's compute and variable dtypes are ignored. Such\n-    # networks only use the policy if it is a PolicyV1, in which case it uses\n-    # the PolicyV1's loss_scale (Policy does not have a loss_scale). For\n-    # subclassed networks, the compute and variable dtypes are used as like any\n-    # ordinary layer.\n+    # Note that models also have a dtype policy, as they are layers. For\n+    # functional models, the policy is only used in Model.compile, which wraps\n+    # the optimizer with a LossScaleOptimizer if the policy name is\n+    # \"mixed_float16\". Subclassed models additionally use the policy's compute\n+    # and variable dtypes, as like any ordinary layer.\n     self._set_dtype_policy(dtype)\n     # Boolean indicating whether the layer automatically casts its inputs to the\n     # layer's compute_dtype.\n\n@@ -30,7 +30,6 @@ from keras.engine import compile_utils\n from keras.engine import data_adapter\n from keras.engine import training_utils\n from keras.mixed_precision import loss_scale_optimizer as lso\n-from keras.mixed_precision import policy\n from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import hdf5_format\n@@ -623,25 +622,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   def _get_optimizer(self, optimizer):\n     \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"\n-    # The deprecated PolicyV1 has a loss_scale, which we use for backwards\n-    # compatibility to match TF 2.3 behavior. The new Policy does not have a\n-    # loss_scale, so we use dynamic loss scaling if the mixed_float16 policy is\n-    # used.\n-    if isinstance(self._dtype_policy, policy.PolicyV1):\n-      loss_scale = self._dtype_policy.loss_scale\n-    elif self._dtype_policy.name == 'mixed_float16':\n-      loss_scale = 'dynamic'\n-    else:\n-      loss_scale = None\n-\n     def _get_single_optimizer(opt):\n       opt = optimizers.get(opt)\n-      if (loss_scale is not None and\n+      if (self.dtype_policy.name == 'mixed_float16' and\n           not isinstance(opt, lso.LossScaleOptimizer)):\n-        if loss_scale == 'dynamic':\n+        # Loss scaling is necessary with mixed_float16 for models to converge to\n+        # the same accuracy as with float32.\n         opt = lso.LossScaleOptimizer(opt)\n-        else:\n-          opt = lso.LossScaleOptimizerV1(opt, loss_scale)\n       return opt\n \n     return tf.nest.map_structure(_get_single_optimizer, optimizer)\n\n@@ -36,7 +36,6 @@ from keras.engine import training_generator_v1\n from keras.engine import training_utils\n from keras.engine import training_utils_v1\n from keras.mixed_precision import loss_scale_optimizer\n-from keras.mixed_precision import policy\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.saving import saving_utils\n from keras.saving.saved_model import model_serialization\n@@ -1337,18 +1336,11 @@ class Model(training_lib.Model):\n     else:\n       self.optimizer = optimizers.get(optimizer)\n \n-    if isinstance(self._dtype_policy, policy.PolicyV1):\n-      loss_scale = self._dtype_policy.loss_scale\n-    elif self._dtype_policy.name == 'mixed_float16':\n-      loss_scale = 'dynamic'\n-    else:\n-      loss_scale = None\n-\n-    if (loss_scale is not None and\n+    if (self._dtype_policy.name == 'mixed_float16' and\n         not isinstance(self.optimizer,\n                        loss_scale_optimizer.LossScaleOptimizer)):\n       if isinstance(self.optimizer, list):\n-        raise ValueError('When a dtype policy with a loss scale is used, you '\n+        raise ValueError('When the \"mixed_float16\" dtype policy is used, you '\n                          'can only pass a single optimizer. Using policy %s '\n                          'and got optimizers: %s' %\n                          self._dtype_policy, self.optimizer)\n@@ -1358,11 +1350,7 @@ class Model(training_lib.Model):\n                          'with a loss scale  used, but got: %s. Using policy: '\n                          '%s' %\n                          (self.optimizer, self._dtype_policy))\n-      if loss_scale == 'dynamic':\n       self.optimizer = loss_scale_optimizer.LossScaleOptimizer(self.optimizer)\n-      else:\n-        self.optimizer = loss_scale_optimizer.LossScaleOptimizerV1(\n-            self.optimizer, loss_scale)\n \n   def _prepare_validation_data(self, validation_data, batch_size,\n                                validation_steps):\n\n@@ -1,41 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Contains the get_layer_policy function.\n-\n-This is a separate file from policy.py to avoid a circular dependency.\n-get_layer_policy() relies on base_layer.py, itself which relies on policy.py.\n-\"\"\"\n-\n-from keras.engine import base_layer\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.mixed_precision.experimental.get_layer_policy', v1=[])\n-def get_layer_policy(layer):\n-  \"\"\"Returns the dtype policy of a layer.\n-\n-  Warning: This function is deprecated. Use\n-  `tf.keras.layers.Layer.dtype_policy` instead.\n-\n-  Args:\n-    layer: A `tf.keras.layers.Layer`.\n-\n-  Returns:\n-    The `tf.keras.mixed_precision.Policy` of the layer.\n-  \"\"\"\n-  if not isinstance(layer, base_layer.Layer):\n-    raise ValueError('get_policy can only be called on a layer, but got: %s'\n-                     % (layer,))\n-  return layer.dtype_policy\n\n@@ -1,46 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests the get_layer_policy function.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.engine import base_layer_utils\n-from keras.layers import core\n-from keras.mixed_precision import get_layer_policy\n-from keras.mixed_precision import policy\n-\n-\n-class GetLayerPolicyTest(tf.test.TestCase):\n-\n-  def test_get_layer_policy(self):\n-    layer = core.Dense(4)\n-    self.assertEqual(get_layer_policy.get_layer_policy(layer).name, 'float32')\n-\n-    p = policy.Policy('mixed_float16')\n-    layer = core.Dense(4, dtype=p)\n-    self.assertIs(get_layer_policy.get_layer_policy(layer), p)\n-\n-    layer = core.Dense(4, dtype='float64')\n-    self.assertEqual(get_layer_policy.get_layer_policy(layer).name, 'float64')\n-\n-  def test_error(self):\n-    with self.assertRaisesRegex(\n-        ValueError, 'get_policy can only be called on a layer, but got: 1'):\n-      get_layer_policy.get_layer_policy(1)\n-\n-\n-if __name__ == '__main__':\n-  base_layer_utils.enable_v2_dtype_behavior()\n-  tf.test.main()\n\n@@ -26,7 +26,6 @@ from keras import models\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n-from keras.mixed_precision import get_layer_policy\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers.optimizer_v2 import gradient_descent\n@@ -83,8 +82,7 @@ class LayerTest(test_combinations.TestCase):\n       with strategy.scope(), policy.policy_scope(policy_name):\n         layer = mp_test_util.MultiplyLayer(assert_type=dtype)\n         self.assertEqual(layer.dtype, tf.float32)\n-        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n-                         policy_name)\n+        self.assertEqual(layer.dtype_policy.name, policy_name)\n         y = layer(x)\n         self.assertEqual(layer.v.dtype, tf.float32)\n         self.assertEqual(y.dtype, dtype)\n@@ -93,8 +91,7 @@ class LayerTest(test_combinations.TestCase):\n         self.assertEqual(layer.compute_dtype, dtype)\n         self.assertEqual(layer.dtype, tf.float32)\n         self.assertEqual(layer.variable_dtype, tf.float32)\n-        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n-                         policy_name)\n+        self.assertEqual(layer.dtype_policy.name, policy_name)\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n         self.assertEqual(self.evaluate(y), 1.)\n \n@@ -310,56 +307,52 @@ class LayerTest(test_combinations.TestCase):\n       self.assertEqual(layer.v.dtype, 'float32')\n \n   @parameterized.named_parameters(*TESTCASES)\n-  def test_config_policy_v1(self, strategy_fn):\n+  def test_from_config_policy_v1(self, strategy_fn):\n+    # Test that layers serialized in previous Keras versions with the\n+    # now-deleted PolicyV1 can be deserialized. In such cases, the PolicyV1 will\n+    # be converted to a Policy, since PolicyV1 no longer exists. Unlike Policy,\n+    # PolicyV1 had a \"loss_scale\" field, which is silently dropped when\n+    # deserialized.\n     x = tf.constant([1.], dtype=tf.float16)\n     with strategy_fn().scope():\n \n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('mixed_float16',\n-                                                               loss_scale=None))\n+      layer = mp_test_util.MultiplyLayer(dtype='mixed_float16')\n       config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n+      # Change the serialized dtype policy to a PolicyV1\n+      config['dtype'] = {'class_name': 'PolicyV1',\n                          'config': {'name': 'mixed_float16',\n-                                   'loss_scale': None}})\n+                                    'loss_scale': None}}\n       layer = mp_test_util.MultiplyLayer.from_config(config)\n       self.assertEqual(layer.dtype, 'float32')\n       self.assertEqual(layer(x).dtype, 'float16')\n       self.assertEqual(layer.v.dtype, 'float32')\n-      # Restoring a PolicyV1 silently converts it to a Policy and drops the loss\n-      # scale.\n-      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n       config = layer.get_config()\n       # The loss_scale is silently dropped\n       self.assertEqual(config['dtype'],\n                        {'class_name': 'Policy',\n                         'config': {'name': 'mixed_float16'}})\n \n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('float64',\n-                                                               loss_scale=2.))\n+      layer = mp_test_util.MultiplyLayer(dtype='float64')\n       config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n+      config['dtype'] = {'class_name': 'PolicyV1',\n                          'config': {'name': 'float64',\n                                     'loss_scale': {\n                                         'class_name': 'FixedLossScale',\n-                                       'config': {'loss_scale_value': 2.0}}}})\n+                                        'config': {'loss_scale_value': 2.0}}}}\n       layer = mp_test_util.MultiplyLayer.from_config(config)\n       self.assertEqual(layer.dtype, 'float64')\n       self.assertEqual(layer(x).dtype, 'float64')\n       self.assertEqual(layer.v.dtype, 'float64')\n-      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n       config = layer.get_config()\n       self.assertEqual(config['dtype'], 'float64')\n \n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('_infer',\n-                                                               loss_scale=2.))\n+      layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('_infer'))\n       config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n+      config['dtype'] = {'class_name': 'PolicyV1',\n                          'config': {'name': '_infer',\n                                     'loss_scale': {\n                                         'class_name': 'FixedLossScale',\n-                                       'config': {'loss_scale_value': 2.0}}}})\n+                                        'config': {'loss_scale_value': 2.0}}}}\n       layer = mp_test_util.MultiplyLayer.from_config(config)\n       self.assertEqual(layer.dtype, None)\n       self.assertEqual(layer(x).dtype, 'float16')\n\n@@ -591,7 +591,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n       raise TypeError(msg)\n     if not isinstance(dynamic, bool):\n       # Catch errors if a user incorrectly passes a string or float to the\n-      # second argument argument, as this is commonly done for\n+      # second argument argument, as this was commonly done for the now-removed\n       # LossScaleOptimizerV1.\n       raise TypeError('\"dynamic\" argument to LossScaleOptimizer.__init__ must '\n                       'be a bool, but got: %r' % (dynamic,))\n@@ -990,182 +990,6 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n   # optimizer being used.\n \n \n-@keras_export('keras.mixed_precision.experimental.LossScaleOptimizer')\n-class LossScaleOptimizerV1(LossScaleOptimizer):\n-  \"\"\"An deprecated optimizer that applies loss scaling.\n-\n-  Warning: This class is deprecated and will be removed in a future version of\n-  TensorFlow. Please use the non-experimental class\n-  `tf.keras.mixed_precision.LossScaleOptimizer` instead.\n-\n-  This class is identical to the non-experimental\n-  `keras.mixed_precision.LossScaleOptimizer` except its constructor takes\n-  different arguments. For this class (the experimental version), the\n-  constructor takes a `loss_scale` argument.  For the non-experimental class,\n-  the constructor encodes the loss scaling information in multiple arguments.\n-  Note that unlike this class, the non-experimental class does not accept a\n-  `tf.compat.v1.mixed_precision.LossScale`, which is deprecated.\n-\n-  If you currently use this class, you should switch to the non-experimental\n-  `tf.keras.mixed_precision.LossScaleOptimizer` instead. We show several\n-  examples of converting the use of the experimental class to the equivalent\n-  non-experimental class.\n-\n-  >>> # In all of the the examples below, `opt1` and `opt2` are identical\n-  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), loss_scale='dynamic')\n-  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD())\n-  >>> assert opt1.get_config() == opt2.get_config()\n-\n-  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), loss_scale=123)\n-  >>> # dynamic=False indicates to use fixed loss scaling. initial_scale=123\n-  >>> # refers to the initial loss scale, which is the single fixed loss scale\n-  >>> # when dynamic=False.\n-  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), dynamic=False, initial_scale=123)\n-  >>> assert opt1.get_config() == opt2.get_config()\n-\n-  >>> loss_scale = tf.compat.v1.mixed_precision.experimental.DynamicLossScale(\n-  ...     initial_loss_scale=2048, increment_period=500)\n-  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), loss_scale=loss_scale)\n-  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), initial_scale=2048,\n-  ...     dynamic_growth_steps=500)\n-  >>> assert opt1.get_config() == opt2.get_config()\n-\n-  Make sure to also switch from this class to the non-experimental class in\n-  isinstance checks, if you have any. If you do not do this, your model may run\n-  into hard-to-debug issues, as the experimental `LossScaleOptimizer` subclasses\n-  the non-experimental `LossScaleOptimizer`, but not vice versa. It is safe to\n-  switch isinstance checks to the non-experimental `LossScaleOptimizer` even\n-  before using the non-experimental `LossScaleOptimizer`.\n-\n-  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD(), loss_scale='dynamic')\n-  >>> # The experimental class subclasses the non-experimental class\n-  >>> isinstance(opt1, tf.keras.mixed_precision.LossScaleOptimizer)\n-  True\n-  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(\n-  ...     tf.keras.optimizers.SGD())\n-  >>> # The non-experimental class does NOT subclass the experimental class.\n-  >>> isinstance(opt2, tf.keras.mixed_precision.experimental.LossScaleOptimizer)\n-  False\n-\n-  Args:\n-    optimizer: The Optimizer instance to wrap.\n-    loss_scale: The loss scale to scale the loss and gradients. This can\n-      either be an int/float to use a fixed loss scale, the string \"dynamic\"\n-      to use dynamic loss scaling, or an instance of a LossScale. The string\n-      \"dynamic\" equivalent to passing `DynamicLossScale()`, and passing an\n-      int/float is equivalent to passing a FixedLossScale with the given loss\n-      scale. If a DynamicLossScale is passed, DynamicLossScale.multiplier must\n-      be 2 (the default).\n-  \"\"\"\n-\n-  def __init__(self, optimizer, loss_scale):\n-    warn_msg_prefix = (\n-        'tf.keras.mixed_precision.experimental.LossScaleOptimizer is '\n-        'deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer '\n-        'instead. ')\n-\n-    if isinstance(loss_scale, dict):\n-      loss_scale = keras_loss_scale_module.deserialize(loss_scale)\n-\n-    if isinstance(loss_scale, (int, float)):\n-      tf_logging.warning(\n-          warn_msg_prefix + 'For example:\\n'\n-          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n-          'opt, dynamic=False, initial_scale={})'.format(loss_scale))\n-      super(LossScaleOptimizerV1, self).__init__(optimizer, dynamic=False,\n-                                                 initial_scale=loss_scale)\n-    elif isinstance(loss_scale, tf.mixed_precision.experimental.FixedLossScale):\n-      ls_val = loss_scale._loss_scale_value  # pylint: disable=protected-access\n-      tf_logging.warning(\n-          warn_msg_prefix + 'For example:\\n'\n-          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n-          'opt, dynamic=False, initial_scale={})'.format(ls_val))\n-      super(LossScaleOptimizerV1, self).__init__(optimizer, dynamic=False,\n-                                                 initial_scale=ls_val)\n-    elif loss_scale == 'dynamic':\n-      tf_logging.warning(\n-          warn_msg_prefix + 'For example:\\n'\n-          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n-          'opt)')\n-      super(LossScaleOptimizerV1, self).__init__(optimizer)\n-    elif isinstance(loss_scale, tf.mixed_precision.experimental.DynamicLossScale):\n-      kwargs = {}\n-      extra_arguments = ''\n-      if loss_scale.initial_loss_scale != _DEFAULT_INITIAL_SCALE:\n-        kwargs['initial_scale'] = loss_scale.initial_loss_scale\n-        extra_arguments += (', initial_scale=%s' %\n-                            loss_scale.initial_loss_scale)\n-      if loss_scale.increment_period != _DEFAULT_GROWTH_STEPS:\n-        kwargs['dynamic_growth_steps'] = loss_scale.increment_period\n-        extra_arguments += (', dynamic_growth_steps=%s' %\n-                            loss_scale.increment_period)\n-      if loss_scale.multiplier != 2:\n-        raise ValueError('When passing a DynamicLossScale to \"loss_scale\", '\n-                         'DynamicLossScale.multiplier must be 2. Got: %s'\n-                         % (loss_scale,))\n-      tf_logging.warning(\n-          warn_msg_prefix +\n-          'Note that the non-experimental LossScaleOptimizer does not take a '\n-          'DynamicLossScale but instead takes the dynamic configuration '\n-          'directly in the constructor. For example:\\n'\n-          '  opt = tf.keras.mixed_precision.LossScaleOptimizer('\n-          'opt{})\\n'.format(extra_arguments))\n-      super(LossScaleOptimizerV1, self).__init__(optimizer, **kwargs)\n-    elif isinstance(loss_scale, tf.mixed_precision.experimental.LossScale):\n-      raise TypeError('Passing a LossScale that is not a FixedLossScale or a '\n-                      'DynamicLossScale is no longer supported. Got: {}'\n-                      .format(loss_scale))\n-    else:\n-      raise ValueError('Invalid value passed to loss_scale. loss_scale '\n-                       'must be the string \"dynamic\" (recommended), an int, '\n-                       'a float, a FixedLossScale, or a DynamicLossScale. Got '\n-                       'value: {}'.format(loss_scale))\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    config = config.copy()  # Make a copy, since we mutate config\n-\n-    # If loss_scale is in config, we assume we are deserializing a\n-    # LossScaleOptimizer from TF 2.3 or below. Otherwise, we assume we are\n-    # deserializing a LossScaleOptimizer from TF 2.4 or above.\n-    if 'loss_scale' in config:\n-      config['loss_scale'] = keras_loss_scale_module.deserialize(\n-          config['loss_scale'])\n-      if (isinstance(config['loss_scale'],\n-                     tf.mixed_precision.experimental.DynamicLossScale) and\n-          config['loss_scale'].multiplier != 2):\n-        raise ValueError('Cannot deserialize LossScaleOptimizer with a '\n-                         'DynamicLossScale whose multiplier is not 2. Got '\n-                         'DynamicLossScale: %s' % (config['loss_scale'],))\n-      optimizer = optimizers.deserialize(\n-          config['optimizer'], custom_objects=custom_objects)\n-      del config['optimizer']\n-      return cls(optimizer, **config)\n-\n-    # We convert the config, as generated by LossScaleOptimizer.get_config, to a\n-    # version that can be passed to LossScaleOptimizerV1.__init__\n-    if config['dynamic']:\n-      config['loss_scale'] = tf.mixed_precision.experimental.DynamicLossScale(\n-          config['initial_scale'], config['dynamic_growth_steps'], multiplier=2)\n-    else:\n-      config['loss_scale'] = tf.mixed_precision.experimental.FixedLossScale(\n-          config['initial_scale'])\n-\n-    del config['dynamic']\n-    del config['initial_scale']\n-    del config['dynamic_growth_steps']\n-    optimizer = optimizers.deserialize(\n-        config.pop('inner_optimizer'), custom_objects=custom_objects)\n-    return cls(optimizer, **config)\n-\n-\n class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n                            optimizer_experimental.Optimizer,\n                            BaseLossScaleOptimizer):\n@@ -1204,7 +1028,7 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n                       f'Optimizer, but got: {inner_optimizer}.')\n     if not isinstance(dynamic, bool):\n       # Catch errors if a user incorrectly passes a string or float to the\n-      # second argument argument, as this is commonly done for\n+      # second argument argument, as this was commonly done for the now-removed\n       # LossScaleOptimizerV1.\n       raise TypeError(f'\"dynamic\" argument to LossScaleOptimizer.__init__ must '\n                       f'be a bool, but got: {repr(dynamic)}')\n@@ -1463,8 +1287,53 @@ class FakeOptimizerForRestoration(tf.__internal__.tracking.Trackable):\n         slot_variable_position, slot_name, variable)\n \n \n-tf.__internal__.mixed_precision.register_loss_scale_wrapper(optimizer_v2.OptimizerV2,\n-                                            LossScaleOptimizerV1)\n+def _create_loss_scale_optimizer_from_v1_loss_scale(optimizer, loss_scale):\n+  \"\"\"Creates an LSO from a tf.compat.v1.mixed_precision.LossScale.\n+\n+  This is only used to pass to\n+  `tf.__internal__.mixed_precision.register_loss_scale_wrapper` below, which is\n+  called so that\n+  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite` can\n+  wrap a Keras optimizer with a LossScaleOptimizer.\n+\n+  Args:\n+    optimizer: An OptimizerV2 instance.\n+    loss_scale: A `tf.compat.v1.mixed_precision.LossScale` instance\n+\n+  Returns:\n+    A LossScaleOptimizer that wraps `optimizer` and uses the same loss scaling\n+    algorithm as `loss_scale`.\n+  \"\"\"\n+  if isinstance(loss_scale, (int, float)):\n+    return LossScaleOptimizer(optimizer, dynamic=False,\n+                              initial_scale=loss_scale)\n+  elif isinstance(loss_scale, tf.compat.v1.mixed_precision.FixedLossScale):\n+    ls_val = loss_scale._loss_scale_value  # pylint: disable=protected-access\n+    return LossScaleOptimizer(optimizer, dynamic=False,\n+                              initial_scale=ls_val)\n+  elif loss_scale == 'dynamic':\n+    return LossScaleOptimizer(optimizer)\n+  elif isinstance(loss_scale, tf.compat.v1.mixed_precision.DynamicLossScale):\n+    if loss_scale.multiplier != 2:\n+      raise ValueError(f'When passing a DynamicLossScale to \"loss_scale\", '\n+                       f'DynamicLossScale.multiplier must be 2. Got: '\n+                       f'{loss_scale}')\n+    return LossScaleOptimizer(\n+        optimizer, initial_scale=loss_scale.initial_loss_scale,\n+        dynamic_growth_steps=loss_scale.increment_period)\n+  elif isinstance(loss_scale, tf.compat.v1.mixed_precision.LossScale):\n+    raise TypeError(f'Passing a LossScale that is not a FixedLossScale or a '\n+                    f'DynamicLossScale is not supported. Got: {loss_scale}')\n+  else:\n+    raise ValueError(f'Invalid value passed to loss_scale. loss_scale '\n+                     f'must be the string \"dynamic\" (recommended), an int, '\n+                     f'a float, a FixedLossScale, or a DynamicLossScale. Got '\n+                     f'value: {loss_scale}')\n+\n+\n+tf.__internal__.mixed_precision.register_loss_scale_wrapper(\n+    optimizer_v2.OptimizerV2, _create_loss_scale_optimizer_from_v1_loss_scale,\n+    LossScaleOptimizer)\n \n \n def _multiply_gradient(gradient, scale):\n\n@@ -676,123 +676,6 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         run_fn = tf.function(run_fn)\n       strategy.experimental_run(run_fn)\n \n-  @test_combinations.generate(\n-      test_combinations.combine(strategy_fn=STRATEGY_FNS,\n-                                mode=['graph', 'eager']))\n-  def testV1Optimizer(self, strategy_fn):\n-    strategy = strategy_fn()\n-    learning_rate = 2.\n-    with strategy.scope():\n-      # Test FixedLossScale\n-      var = tf.Variable([5.0])\n-      opt = gradient_descent.SGD(learning_rate)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale=2)\n-      self.assertIsInstance(opt.loss_scale, tf.Tensor)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self.assertEqual(self.evaluate(opt.loss_scale), 2)\n-      self.assertEqual(opt.initial_scale, 2)\n-      self.assertIsNone(opt.dynamic_growth_steps)\n-      run_fn = self._run_fn_with_grad_check(\n-          strategy, var, opt, 2 / strategy.num_replicas_in_sync)\n-      run_op = strategy.experimental_run(run_fn)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self._run_if_in_graph_mode(run_op)\n-      # The loss is the identity of the variable. Therefore the gradient is 1,\n-      # and so the variable will be init_val - grad * lr == 5 - 1 * 2 == 3\n-      self.assertAllClose([3.], self.evaluate(var))\n-\n-      # Test DynamicLossScale\n-      var = tf.Variable([5.0])\n-      opt = gradient_descent.SGD(learning_rate)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, 'dynamic')\n-      self.assertEqual(opt.initial_scale, 2 ** 15)\n-      self.assertEqual(opt.dynamic_growth_steps, 2000)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self.assertEqual(self.evaluate(opt.loss_scale), 2 ** 15)\n-      for s in strategy.experimental_local_results(opt.dynamic_counter):\n-        self.assertEqual(self.evaluate(s), 0)\n-\n-      loss = lambda: var * float('NaN')\n-      run_fn = lambda: opt.minimize(loss, var_list=[var])\n-      run_op = strategy.experimental_run(run_fn)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self._run_if_in_graph_mode(run_op)\n-      self.assertAllClose([5.], self.evaluate(var))\n-      self.assertEqual(self.evaluate(opt.loss_scale), 2 ** 14)\n-      for s in strategy.experimental_local_results(opt.dynamic_counter):\n-        self.assertEqual(self.evaluate(s), 0)\n-\n-  @test_combinations.generate(\n-      test_combinations.combine(strategy_fn=STRATEGY_FNS,\n-                                mode=['graph', 'eager']))\n-  def testPassingV1LossScale(self, strategy_fn):\n-    strategy = strategy_fn()\n-    learning_rate = 2.\n-    with strategy.scope():\n-      # Test FixedLossScale\n-      var = tf.Variable([5.0])\n-      opt = gradient_descent.SGD(learning_rate)\n-      loss_scale = tf.mixed_precision.experimental.FixedLossScale(2.)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-      self.assertIsInstance(opt.loss_scale, tf.Tensor)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self.assertEqual(self.evaluate(opt.loss_scale), 2)\n-      run_fn = self._run_fn_with_grad_check(\n-          strategy, var, opt, 2 / strategy.num_replicas_in_sync)\n-      run_op = strategy.experimental_run(run_fn)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self._run_if_in_graph_mode(run_op)\n-      # The loss is the identity of the variable. Therefore the gradient is 1,\n-      # and so the variable will be init_val - grad * lr == 5 - 1 * 2 == 3\n-      self.assertAllClose([3.], self.evaluate(var))\n-\n-      # Test DynamicLossScale\n-      var = tf.Variable([5.0])\n-      opt = gradient_descent.SGD(learning_rate)\n-      loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-          initial_loss_scale=4, increment_period=1, multiplier=2)\n-      loss_scale._current_loss_scale.assign(2)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-      self.assertEqual(opt.initial_scale, 4)\n-      self.assertEqual(opt.dynamic_growth_steps, 1)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      # Current loss scale is not copied so loss scale is reinitialized to 4\n-      self.assertEqual(self.evaluate(opt.loss_scale), 4)\n-      for s in strategy.experimental_local_results(opt.dynamic_counter):\n-        self.assertEqual(self.evaluate(s), 0)\n-\n-      run_fn = self._run_fn_with_grad_check(\n-          strategy, var, opt, 4 / strategy.num_replicas_in_sync)\n-      run_op = strategy.experimental_run(run_fn)\n-      self.evaluate(tf.compat.v1.global_variables_initializer())\n-      self._run_if_in_graph_mode(run_op)\n-      self.assertAllClose([3.], self.evaluate(var))\n-\n-  def testPassingV1LossScaleErrors(self):\n-    opt = gradient_descent.SGD()\n-    loss_scale = tf.mixed_precision.experimental.DynamicLossScale(multiplier=4)\n-    with self.assertRaisesRegex(\n-        ValueError, 'When passing a DynamicLossScale to \"loss_scale\", '\n-                    'DynamicLossScale.multiplier must be 2. Got: '\n-                    'DynamicLossScale'):\n-      loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-\n-    class MyLossScale(tf.mixed_precision.experimental.LossScale):\n-\n-      def __call__(self):\n-        return 1.\n-\n-      def update(self, grads):\n-        return None, True\n-\n-      def get_config(self):\n-        return {}\n-\n-    with self.assertRaisesRegex(\n-        TypeError, 'Passing a LossScale that is not a FixedLossScale or a '\n-                   'DynamicLossScale is no longer supported. Got:'):\n-      loss_scale_optimizer.LossScaleOptimizerV1(opt, MyLossScale())\n-\n   @test_combinations.generate(opt_combinations_only())\n   def testLossScaleDelegationWithWrapper(self, opt_cls):\n     # Test learning_rate is exposed when LossScaleOptimizer wraps another\n@@ -959,31 +842,26 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(self.evaluate(slot_var).item(), -1)\n \n   @test_combinations.generate(\n-      test_combinations.combine(\n-          get_config=['v1', 'v2', 'tf2_3'], from_config=['v1', 'v2']) +\n-      test_combinations.combine(\n-          get_config='v3', from_config='v3', mode='eager'))\n-  def testGetConfigFixed(self, get_config, from_config):\n-    # Get a config from LossScaleOptimizerV1, LossScaleOptimizer,\n-    # LossScaleOptimizerV3, or the LossScaleOptimizer from TF 2.3. Then restore\n-    # the config into a LossScaleOptimizerV1, LossScaleOptimizer, or\n-    # LossScaleOptimizerV3\n-    if get_config == 'v1':\n-      opt = gradient_descent.SGD(2., momentum=0.5)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, 2)\n-      config = opt.get_config()\n-    elif get_config == 'v2':\n+      test_combinations.combine(config_version=['v2', 'tf2_3']) +\n+      test_combinations.combine(config_version='v3', mode='eager'))\n+  def testGetConfigFixed(self, config_version):\n+    # Get a config from LossScaleOptimizer, LossScaleOptimizerV3, or the\n+    # LossScaleOptimizer from TF 2.3. Then restore the config into a\n+    # LossScaleOptimizer or LossScaleOptimizerV3\n+    if config_version == 'v2':\n       opt = gradient_descent.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizer(\n           opt, dynamic=False, initial_scale=2)\n       config = opt.get_config()\n-    elif get_config == 'v3':\n+      opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n+    elif config_version == 'v3':\n       opt = sgd_experimental.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizerV3(\n           opt, dynamic=False, initial_scale=2)\n       config = opt.get_config()\n+      opt = loss_scale_optimizer.LossScaleOptimizerV3.from_config(config)\n     else:\n-      self.assertEqual(get_config, 'tf2_3')\n+      self.assertEqual(config_version, 'tf2_3')\n       config = {\n           'optimizer': {\n               'class_name': 'SGD',\n@@ -1000,14 +878,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n               'config': {'loss_scale_value': 2.0}\n           },\n       }\n-\n-    if from_config == 'v1':\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1.from_config(config)\n-    elif from_config == 'v2':\n       opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n-    else:\n-      self.assertEqual(from_config, 'v3')\n-      opt = loss_scale_optimizer.LossScaleOptimizerV3.from_config(config)\n \n     # Force hyperparameters to be created\n     opt.learning_rate  # pylint: disable=pointless-statement\n@@ -1032,33 +903,26 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self.evaluate(var), [3.])\n \n   @test_combinations.generate(\n-      test_combinations.combine(\n-          get_config=['v1', 'v2', 'tf2_3'], from_config=['v1', 'v2']) +\n-      test_combinations.combine(\n-          get_config='v3', from_config='v3', mode='eager'))\n-  def testGetConfigDynamic(self, get_config, from_config):\n-    # Get a config from LossScaleOptimizerV1, LossScaleOptimizer,\n-    # LossScaleOptimizerV3, or the LossScaleOptimizer from TF 2.3. Then restore\n-    # the config into a LossScaleOptimizerV1, LossScaleOptimizer, or\n-    # LossScaleOptimizerV3\n-    if get_config == 'v1':\n-      opt = gradient_descent.SGD(2., momentum=0.5)\n-      loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-          initial_loss_scale=2, increment_period=3)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-      config = opt.get_config()\n-    elif get_config == 'v2':\n+      test_combinations.combine(config_version=['v2', 'tf2_3']) +\n+      test_combinations.combine(config_version='v3', mode='eager'))\n+  def testGetConfigDynamic(self, config_version):\n+    # Get a config from LossScaleOptimizer, LossScaleOptimizerV3, or the\n+    # LossScaleOptimizer from TF 2.3. Then restore the config into a\n+    # LossScaleOptimizer or LossScaleOptimizerV3\n+    if config_version == 'v2':\n       opt = gradient_descent.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizer(\n           opt, initial_scale=2, dynamic_growth_steps=3)\n       config = opt.get_config()\n-    elif get_config == 'v3':\n+      opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n+    elif config_version == 'v3':\n       opt = sgd_experimental.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizerV3(\n           opt, initial_scale=2, dynamic_growth_steps=3)\n       config = opt.get_config()\n+      opt = loss_scale_optimizer.LossScaleOptimizerV3.from_config(config)\n     else:\n-      self.assertEqual(get_config, 'tf2_3')\n+      self.assertEqual(config_version, 'tf2_3')\n       config = {\n           'optimizer': {\n               'class_name': 'SGD',\n@@ -1079,14 +943,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n               }\n           },\n       }\n-\n-    if from_config == 'v1':\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1.from_config(config)\n-    elif from_config == 'v2':\n       opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n-    else:\n-      self.assertEqual(from_config, 'v3')\n-      opt = loss_scale_optimizer.LossScaleOptimizerV3.from_config(config)\n \n     # Force hyperparameters to be created\n     opt.learning_rate  # pylint: disable=pointless-statement\n@@ -1137,22 +994,22 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                       'DynamicLossScale: DynamicLossScale\\\\(')\n     with self.assertRaisesRegex(ValueError, expected_error):\n       loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n-    with self.assertRaisesRegex(ValueError, expected_error):\n-      loss_scale_optimizer.LossScaleOptimizerV1.from_config(config)\n \n   @test_combinations.generate(\n       test_combinations.combine(lso_type=['v1', 'v2']) +\n       test_combinations.combine(lso_type='v3', mode='eager'))\n   def testSerializationWithBuiltInOptimizer(self, lso_type):\n-    if lso_type == 'v1':\n-      opt = gradient_descent.SGD(2., momentum=0.5)\n-      loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-          initial_loss_scale=2., increment_period=3.)\n-      opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-    elif lso_type == 'v2':\n+    if lso_type in ('v1', 'v2'):\n       opt = gradient_descent.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizer(\n           opt, initial_scale=2., dynamic_growth_steps=3.)\n+      config = optimizers.serialize(opt)\n+      if lso_type == 'v1':\n+        # LossScaleOptimizerV1 was an older experimental version of LSO that is\n+        # now deleted. The config had the same format as LSO but the class\n+        # name was different. This tests that LSO V1 configs can still be\n+        # deserialized, which are deserialized as a (non-V1) LSO\n+        config['class_name'] = 'LossScaleOptimizerV1'\n     else:\n       opt = sgd_experimental.SGD(2., momentum=0.5)\n       opt = loss_scale_optimizer.LossScaleOptimizerV3(\n@@ -1169,7 +1026,6 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(opt.dynamic_growth_steps, 3.)\n     self.assertTrue(opt.dynamic, 4.)\n     if lso_type in ('v1', 'v2'):\n-      # Deserializing a LossScaleOptimizerV1 results in a V2 LossScaleOptimizer.\n       self.assertEqual(type(opt), loss_scale_optimizer.LossScaleOptimizer)\n     else:\n       self.assertEqual(type(opt), loss_scale_optimizer.LossScaleOptimizerV3)\n\n@@ -47,18 +47,83 @@ class MixedPrecisionTest(test_combinations.TestCase):\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n-  def test_wrap_optimizer(self):\n+  def test_wrap_optimizer_fixed_loss_scale(self):\n     opt = gradient_descent_v2.SGD(1.0)\n-    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(opt, 123.)\n-    self.assertIsInstance(\n-        opt, loss_scale_optimizer_v2.LossScaleOptimizerV1)\n+    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+        opt, 123)\n+    self.assertIsInstance(opt, loss_scale_optimizer_v2.LossScaleOptimizer)\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n     self.assertEqual(self.evaluate(opt.loss_scale), 123.)\n+    self.assertFalse(opt.dynamic)\n+    self.assertTrue(opt.initial_scale, 123.)\n+\n+    opt = gradient_descent_v2.SGD(1.0)\n+    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+        opt, tf.compat.v1.mixed_precision.FixedLossScale(123))\n+    self.assertIsInstance(opt, loss_scale_optimizer_v2.LossScaleOptimizer)\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n+    self.assertEqual(self.evaluate(opt.loss_scale), 123.)\n+    self.assertFalse(opt.dynamic)\n+    self.assertTrue(opt.initial_scale, 123.)\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_wrap_optimizer_dynamic_loss_scale(self):\n+    opt = gradient_descent_v2.SGD(1.0)\n+    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+        opt, 'dynamic')\n+    self.assertIsInstance(opt, loss_scale_optimizer_v2.LossScaleOptimizer)\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n+    self.assertEqual(self.evaluate(opt.loss_scale), 2. ** 15)\n+    self.assertTrue(opt.dynamic)\n+    self.assertTrue(opt.initial_scale, 2. ** 15)\n+    self.assertTrue(opt.dynamic_growth_steps, 2000)\n+\n+    opt = gradient_descent_v2.SGD(1.0)\n+    opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+        opt, tf.compat.v1.mixed_precision.DynamicLossScale(\n+            initial_loss_scale=4, increment_period=1000))\n+    self.assertIsInstance(opt, loss_scale_optimizer_v2.LossScaleOptimizer)\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n+    self.assertEqual(self.evaluate(opt.loss_scale), 4.)\n+    self.assertTrue(opt.dynamic)\n+    self.assertTrue(opt.initial_scale, 4.)\n+    self.assertTrue(opt.dynamic_growth_steps, 1000)\n+\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  def test_wrap_optimizer_dynamic_loss_scale_errors(self):\n+\n+    opt = gradient_descent_v2.SGD(1.0)\n+    with self.assertRaisesRegex(\n+        ValueError, 'When passing a DynamicLossScale to \"loss_scale\", '\n+                    'DynamicLossScale.multiplier must be 2. Got: '\n+                    'DynamicLossScale'):\n+      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+          opt, tf.compat.v1.mixed_precision.DynamicLossScale(multiplier=4.))\n+\n+    class MyLossScale(tf.compat.v1.mixed_precision.LossScale):\n+\n+      def __call__(self):\n+        return 1.\n+\n+      def update(self, grads):\n+        return None, True\n+\n+      def get_config(self):\n+        return {}\n+\n+    with self.assertRaisesRegex(\n+        TypeError, 'Passing a LossScale that is not a FixedLossScale or a '\n+                   'DynamicLossScale is not supported. Got:'):\n+      tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n+          opt, MyLossScale())\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def test_optimizer_errors(self):\n     opt = gradient_descent_v2.SGD(1.0)\n-    opt = loss_scale_optimizer_v2.LossScaleOptimizerV1(opt, 'dynamic')\n+    opt = loss_scale_optimizer_v2.LossScaleOptimizer(opt)\n     with self.assertRaisesRegex(\n         ValueError, '\"opt\" must not already be an instance of a '\n         'LossScaleOptimizer.'):\n\n@@ -40,7 +40,6 @@ from keras.engine import base_layer_utils\n from keras.engine import input_spec\n from keras.engine import sequential\n from keras.layers import core\n-from keras.mixed_precision import get_layer_policy\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n@@ -144,11 +143,6 @@ class KerasModelTest(test_combinations.TestCase):\n           'strategy_fn': create_mirrored_strategy,\n           'save_format': 'h5',\n           'use_regularizer': True,\n-      }, {\n-          'testcase_name': 'saved_model_v1_policy',\n-          'strategy_fn': create_mirrored_strategy,\n-          'use_v1_policy': True,\n-          'save_format': 'tf',\n       })\n   def test_model(self,\n                  strategy_fn,\n@@ -157,8 +151,7 @@ class KerasModelTest(test_combinations.TestCase):\n                  policy_name='mixed_float16',\n                  get_config=False,\n                  save_format=None,\n-                 use_input_spec=False,\n-                 use_v1_policy=False):\n+                 use_input_spec=False):\n     self._skip_if_strategy_unsupported(strategy_fn)\n     self._skip_if_save_format_unsupported(save_format)\n     if use_regularizer:\n@@ -167,8 +160,7 @@ class KerasModelTest(test_combinations.TestCase):\n     else:\n       weight_regularizer = activity_regularizer = None\n     with strategy_fn().scope():\n-      cls = policy.PolicyV1 if use_v1_policy else policy.Policy\n-      with policy.policy_scope(cls(policy_name)):\n+      with policy.policy_scope(policy_name):\n         layer = mp_test_util.MultiplyLayer(\n             assert_type=tf.float16,\n             use_operator=use_operator,\n@@ -249,13 +241,10 @@ class KerasModelTest(test_combinations.TestCase):\n \n     # Ensure various dtype-related aspects of the layer are correct\n     self.assertEqual(layer.dtype, 'float32')\n-    self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n-                     'mixed_float16')\n+    self.assertEqual(layer.dtype_policy.name, 'mixed_float16')\n     self.assertEqual(layer.v.dtype, 'float32')\n     self.assertEqual(layer(np.ones((2, 1))).dtype, 'float16')\n \n-    # Loading a model always loads with a v2 Policy, even if saved with a\n-    # PolicyV1.\n     self.assertEqual(type(model.dtype_policy), policy.Policy)\n     self.assertEqual(layer.get_config()['dtype'],\n                      {'class_name': 'Policy', 'config': {\n@@ -400,30 +389,14 @@ class KerasModelTest(test_combinations.TestCase):\n       }, {\n           'testcase_name': 'distribute',\n           'strategy_fn': create_mirrored_strategy,\n-      }, {\n-          'testcase_name': 'pass_loss_scale_to_policy',\n-          'strategy_fn': create_mirrored_strategy,\n-          'pass_loss_scale_to_policy': True,\n       }, {\n           'testcase_name': 'get_config',\n           'strategy_fn': create_mirrored_strategy,\n           'get_config': True,\n-      }, {\n-          'testcase_name': 'get_config_v1_lso',\n-          'strategy_fn': create_mirrored_strategy,\n-          'get_config': True,\n-          'use_v1_loss_scale_optimizer': True,\n-      }, {\n-          'testcase_name': 'get_config_and_pass_loss_scale_to_policy',\n-          'strategy_fn': create_mirrored_strategy,\n-          'get_config': True,\n-          'pass_loss_scale_to_policy': True,\n       })\n   def test_dynamic_loss_scaling(self,\n                                 strategy_fn,\n-                                pass_loss_scale_to_policy=False,\n-                                get_config=False,\n-                                use_v1_loss_scale_optimizer=False):\n+                                get_config=False):\n     strategy = strategy_fn()\n     initial_loss_scale = 2.\n     batch_size = 4\n@@ -433,21 +406,9 @@ class KerasModelTest(test_combinations.TestCase):\n     have_nan_gradients = backend.variable(False, dtype=tf.bool)\n     with strategy.scope():\n       opt = gradient_descent.SGD(1.)\n-      if pass_loss_scale_to_policy:\n-        loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-            initial_loss_scale=initial_loss_scale, increment_period=2)\n-        p = policy.PolicyV1('mixed_float16', loss_scale=loss_scale)\n-      elif use_v1_loss_scale_optimizer:\n-        loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-            initial_loss_scale=initial_loss_scale, increment_period=2)\n-        p = policy.Policy('mixed_float16')\n-        opt = loss_scale_optimizer.LossScaleOptimizerV1(\n-            opt, loss_scale)\n-      else:\n-        p = policy.Policy('mixed_float16')\n       opt = loss_scale_optimizer.LossScaleOptimizer(\n           opt, initial_scale=initial_loss_scale, dynamic_growth_steps=2)\n-      with policy.policy_scope(p):\n+      with policy.policy_scope('mixed_float16'):\n         x = layers.Input(\n             shape=(1,), batch_size=batch_size, dtype=tf.float16)\n         layer = mp_test_util.MultiplyLayer(assert_type=tf.float16)\n@@ -514,34 +475,46 @@ class KerasModelTest(test_combinations.TestCase):\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n-  def test_loss_scale_optimizer_overrides_policy_v1_loss_scale(self):\n-    with policy.policy_scope(policy.PolicyV1('float32', loss_scale=10.)):\n-      opt = gradient_descent.SGD(1.)\n-      opt = loss_scale_optimizer.LossScaleOptimizer(opt, dynamic=False,\n-                                                    initial_scale=5.)\n+  def test_compile_wraps_with_loss_scale_optimizer(self):\n     x = layers.Input(shape=(1,))\n     y = mp_test_util.MultiplyLayer()(x)\n-      model = models.Model(x, y)\n-      model.compile(opt, loss='mse')\n-      self.assertEqual(self.evaluate(model.optimizer.loss_scale), 5.)\n \n-  @test_combinations.generate(\n-      test_combinations.combine(mode=['graph', 'eager']))\n-  def test_policy_v1_without_loss_scale(self):\n-    with policy.policy_scope(policy.PolicyV1('mixed_float16',\n-                                             loss_scale=None)):\n-      opt = gradient_descent.SGD(1.)\n-      x = layers.Input(shape=(1,))\n-      y = mp_test_util.MultiplyLayer()(x)\n+    with policy.policy_scope('mixed_float16'):\n+      # Test optimizer is automatically wrapped with LSO\n       model = models.Model(x, y)\n-      model.compile(opt, loss='mse')\n+      model.compile(gradient_descent.SGD(1.), 'mse')\n+      self.assertIsInstance(model.optimizer,\n+                            loss_scale_optimizer.LossScaleOptimizer)\n+      self.assertEqual(backend.get_value(model.optimizer.learning_rate), 1.)\n+\n+      # Test optimizer specified as string is automatically wrapped in LSO\n+      model = models.Model(x, y)\n+      model.compile('sgd', 'mse')\n+      self.assertIsInstance(model.optimizer,\n+                            loss_scale_optimizer.LossScaleOptimizer)\n+\n+      # Test if an LSO is passed, optimizer is not automatically wrapped with\n+      # another LSO\n+      model = models.Model(x, y)\n+      optimizer = loss_scale_optimizer.LossScaleOptimizer(\n+          gradient_descent.SGD(1.), dynamic_growth_steps=2)\n+      model.compile(optimizer, 'mse')\n+      self.assertIsInstance(model.optimizer,\n+                            loss_scale_optimizer.LossScaleOptimizer)\n+      self.assertEqual(model.optimizer.dynamic_growth_steps, 2)\n+\n+    with policy.policy_scope('mixed_bfloat16'):\n+      # Test mixed_bfloat16 models are not automatically wrapped with LSO\n+      model = models.Model(x, y)\n+      model.compile(gradient_descent.SGD(1.), 'mse')\n       self.assertNotIsInstance(model.optimizer,\n                                loss_scale_optimizer.LossScaleOptimizer)\n+      self.assertIsInstance(model.optimizer, gradient_descent.SGD)\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def test_pass_invalid_optimizer_with_loss_scaling(self):\n-    with policy.policy_scope(policy.PolicyV1('float32', loss_scale=10.)):\n+    with policy.policy_scope(policy.Policy('mixed_float16')):\n       x = layers.Input(shape=(1,))\n       y = mp_test_util.MultiplyLayer()(x)\n       model = models.Model(x, y)\n@@ -760,10 +733,6 @@ class KerasModelTest(test_combinations.TestCase):\n       }, {\n           'testcase_name': 'distribute',\n           'strategy_fn': create_mirrored_strategy,\n-      }, {\n-          'testcase_name': 'use_v1_lso',\n-          'strategy_fn': create_mirrored_strategy,\n-          'use_v1_loss_scale_optimizer': True\n       }, {\n           'testcase_name': 'base_h5',\n           'strategy_fn': default_strategy_fn,\n@@ -774,7 +743,7 @@ class KerasModelTest(test_combinations.TestCase):\n           'h5': True,\n       })\n   def test_save_model_with_dynamic_loss_scaling(\n-      self, strategy_fn, h5=False, use_v1_loss_scale_optimizer=False):\n+      self, strategy_fn, h5=False):\n     # TODO(reedwm): Support and test saving model with a mixed_[b]float16 policy\n     # as well.\n     strategy = strategy_fn()\n@@ -790,11 +759,6 @@ class KerasModelTest(test_combinations.TestCase):\n       model = models.Model(inputs=x, outputs=y)\n \n       opt = gradient_descent.SGD(1.)\n-      if use_v1_loss_scale_optimizer:\n-        loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-            initial_loss_scale=1., increment_period=2.)\n-        opt = loss_scale_optimizer.LossScaleOptimizerV1(opt, loss_scale)\n-      else:\n       opt = loss_scale_optimizer.LossScaleOptimizer(opt, initial_scale=1.,\n                                                     dynamic_growth_steps=2.)\n       model.compile(\n\n@@ -20,9 +20,7 @@ import contextlib\n from keras import backend\n from keras.engine import base_layer_utils\n from keras.mixed_precision import device_compatibility_check\n-from keras.mixed_precision import loss_scale as keras_loss_scale_module\n from keras.utils import generic_utils\n-from tensorflow.python.platform import tf_logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -308,99 +306,6 @@ class Policy:\n     return cls(**config)\n \n \n-@keras_export('keras.mixed_precision.experimental.Policy', v1=[])\n-class PolicyV1(Policy):\n-  \"\"\"A deprecated dtype policy for a Keras layer.\n-\n-  Warning: This class is now deprecated and will be removed soon. Please use the\n-  non-experimental class `tf.keras.mixed_precision.Policy` instead.\n-\n-  The difference between this class and the non-experimental class is that this\n-  class has a `loss_scale` field and the non-experimental class does not. The\n-  loss scale is only used by `tf.keras.Model.compile`, which automatically wraps\n-  the optimizer with a `LossScaleOptimizer` if the optimizer is not already a\n-  `LossScaleOptimizer`. For the non-experimental Policy class, `Model.compile`\n-  instead wraps the optimizer with a `LossScaleOptimizer` if `Policy.name` is\n-  \"mixed_float16\".\n-\n-  When deserializing objects with an experimental policy using functions like\n-  `tf.keras.utils.deserialize_keras_object`, the policy will be deserialized as\n-  the non-experimental `tf.keras.mixed_precision.Policy`, and the loss scale\n-  will silently be dropped. This is so that SavedModels that are generated\n-  with an experimental policy can be restored after the experimental policy is\n-  removed.\n-  \"\"\"\n-\n-  def __init__(self, name, loss_scale='auto'):\n-    \"\"\"Constructs the policy.\n-\n-    The `name` argument determines the compute and variable dtype, the default\n-    loss scale, and has no additional effect on the Policy. The compute and\n-    variable dtypes can only be specified through `name`, and cannot be\n-    specified directly.\n-\n-    Args:\n-      name: A string. Can be one of the following values:\n-        * Any dtype name, such as 'float32' or 'float64'. Both the variable and\n-          compute dtypes will be that dtype.\n-        * 'mixed_float16' or 'mixed_bfloat16': The compute dtype is float16 or\n-          bfloat16, while the variable dtype is float32. With 'mixed_float16',\n-          a dynamic loss scale is used. These policies are used for mixed\n-          precision training.\n-      loss_scale: A `tf.compat.v1.mixed_precision.LossScale`, an int (which\n-        uses a `FixedLossScale`), the string \"dynamic\" (which uses a\n-        `DynamicLossScale`), or None (which uses no loss scale). Defaults to\n-        `\"auto\"`. In the `\"auto\"` case: 1) if `name` is `\"mixed_float16\"`, then\n-        use `loss_scale=\"dynamic\"`. 2) otherwise, do not use a loss scale. Only\n-        `tf.keras.Model`s, not layers, use the loss scale, and it is only used\n-        during `Model.fit`, `Model.train_on_batch`, and other similar methods.\n-    \"\"\"\n-    super(PolicyV1, self).__init__(name)\n-    if loss_scale == 'auto':\n-      loss_scale = 'dynamic' if name == 'mixed_float16' else None\n-      self._using_default_loss_scale = True\n-    else:\n-      self._using_default_loss_scale = False\n-    if loss_scale and self._compute_dtype not in (None, 'float16'):\n-      tf_logging.warning(\n-          'Creating a Policy with a loss scale is only useful for '\n-          'float16 policies. You passed loss_scale=%r for policy '\n-          '%s. Consider not passing any loss_scale instead.' %\n-          (loss_scale, name))\n-    self._loss_scale = keras_loss_scale_module.get(loss_scale)\n-\n-  @property\n-  def loss_scale(self):\n-    \"\"\"Returns the loss scale of this Policy.\n-\n-    Returns:\n-      A `tf.compat.v1.mixed_precision.experimental.LossScale`, or None.\n-    \"\"\"\n-    return self._loss_scale\n-\n-  def __repr__(self):\n-    return '<PolicyV1 \"%s\", loss_scale=%s>' % (self._name, self.loss_scale)\n-\n-  def get_config(self):\n-    config = {\n-        'name': self.name\n-    }\n-    if not self._using_default_loss_scale:\n-      # We only include the loss scale if the default loss scale is not used.\n-      # This allows us to change the loss scale config format without breaking\n-      # users who use the default loss scale.\n-      config['loss_scale'] = keras_loss_scale_module.serialize(self.loss_scale)\n-    return config\n-\n-  @classmethod\n-  def from_config(cls, config, custom_objects=None):\n-    if 'loss_scale' in config and isinstance(config['loss_scale'], dict):\n-      config = config.copy()\n-      config['loss_scale'] = keras_loss_scale_module.deserialize(\n-          config['loss_scale'], custom_objects=custom_objects)\n-    return cls(**config)\n-\n-\n # The current global policy in effect. If None, it means the current value of\n # floatx should be used as the policy if the V2 dtype behavior is enabled,\n # or \"_infer\" otherwise.\n@@ -408,8 +313,7 @@ class PolicyV1(Policy):\n _global_policy = None\n \n \n-@keras_export('keras.mixed_precision.global_policy',\n-              'keras.mixed_precision.experimental.global_policy', v1=[])\n+@keras_export('keras.mixed_precision.global_policy', v1=[])\n def global_policy():\n   \"\"\"Returns the global dtype policy.\n \n@@ -458,8 +362,7 @@ def _check_if_mixed_precision_graph_rewrite_is_enabled(policy):\n         'customizable.'.format(policy=policy))\n \n \n-@keras_export('keras.mixed_precision.set_global_policy',\n-              'keras.mixed_precision.experimental.set_policy', v1=[])\n+@keras_export('keras.mixed_precision.set_global_policy', v1=[])\n def set_global_policy(policy):\n   \"\"\"Sets the global dtype policy.\n \n@@ -549,7 +452,7 @@ def _policy_equivalent_to_dtype(policy):\n \n   A policy is equivalent to a single dtype if the policy's compute and variable\n   dtypes are the same and the policy's type is Policy and not a subclass of\n-  Policy (such as PolicyV1).\n+  Policy.\n \n   The \"_infer\" policy is considered equivalent to a single dtype.\n \n@@ -580,6 +483,8 @@ def deserialize(config, custom_objects=None):\n     return Policy(config)\n   if config is None:\n     return Policy('_infer')\n+  # PolicyV1 was an old version of Policy that was removed. Deserializing it\n+  # turns it into a (non-V1) Policy.\n   module_objects = {'Policy': Policy, 'PolicyV1': Policy}\n   return generic_utils.deserialize_keras_object(\n       config,\n\n@@ -56,17 +56,6 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(repr(mp_policy.Policy(policy)),\n                        '<Policy \"%s\">' % policy)\n \n-    # Test PolicyV1 repr\n-    for policy in ('float32', 'int8', 'mixed_bfloat16', '_infer'):\n-      self.assertEqual(repr(mp_policy.PolicyV1(policy)),\n-                       '<PolicyV1 \"%s\", loss_scale=None>' % policy)\n-    self.assertEqual(\n-        repr(mp_policy.PolicyV1('float16', loss_scale=2.)),\n-        '<PolicyV1 \"float16\", loss_scale=FixedLossScale(2.0)>')\n-    self.assertStartsWith(\n-        repr(mp_policy.PolicyV1('mixed_float16')),\n-        '<PolicyV1 \"mixed_float16\", loss_scale=DynamicLossScale(')\n-\n   @test_utils.enable_v2_dtype_behavior\n   def test_policy_errors(self):\n     # Test passing invalid strings\n@@ -109,30 +98,6 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         '\\'int8_with_float32_vars\\''):\n       mp_policy.Policy('int8_with_float32_vars')\n \n-  @test_utils.enable_v2_dtype_behavior\n-  def test_loss_scale(self):\n-    policy = mp_policy.PolicyV1('float32')\n-    self.assertEqual(policy.loss_scale, None)\n-\n-    policy = mp_policy.PolicyV1('float32', loss_scale=None)\n-    self.assertEqual(policy.loss_scale, None)\n-\n-    ls = tf.mixed_precision.experimental.DynamicLossScale()\n-    policy = mp_policy.PolicyV1('float32', loss_scale=ls)\n-    self.assertIs(policy.loss_scale, ls)\n-\n-    policy = mp_policy.PolicyV1('float32', loss_scale='dynamic')\n-    self.assertIsInstance(policy.loss_scale, tf.mixed_precision.experimental.DynamicLossScale)\n-\n-    policy = mp_policy.PolicyV1('mixed_float16')\n-    self.assertIsInstance(policy.loss_scale, tf.mixed_precision.experimental.DynamicLossScale)\n-\n-    policy = mp_policy.PolicyV1('mixed_float16', loss_scale=None)\n-    self.assertEqual(policy.loss_scale, None)\n-\n-    policy = mp_policy.PolicyV1('mixed_bfloat16')\n-    self.assertEqual(policy.loss_scale, None)\n-\n   @test_utils.enable_v2_dtype_behavior\n   def test_global_policy(self):\n     if base_layer_utils.v2_dtype_behavior_enabled():\n@@ -168,23 +133,6 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         'got policy: complex64'):\n       mp_policy.set_global_policy(mp_policy.Policy('complex64'))\n \n-  @test_utils.enable_v2_dtype_behavior\n-  def test_loss_scale_warning(self):\n-    with tf.compat.v1.test.mock.patch.object(tf_logging, 'warning') as mock_warn:\n-      mp_policy.PolicyV1('float32', loss_scale=2.)\n-      self.assertEqual(\n-          mock_warn.call_args_list[0][0][0],\n-          'Creating a Policy with a loss scale is only useful for float16 '\n-          'policies. You passed loss_scale=2.0 for policy float32. Consider '\n-          'not passing any loss_scale instead.')\n-\n-    for policy_name in 'float16', 'mixed_float16':\n-      # Trigger any other warnings that occur only once\n-      mp_policy.PolicyV1(policy_name, loss_scale=2.)\n-      with tf.compat.v1.test.mock.patch.object(tf_logging, 'warning') as mock_warn:\n-        mp_policy.PolicyV1(policy_name, loss_scale=2.)\n-        mock_warn.assert_not_called()\n-\n   @test_utils.enable_v2_dtype_behavior\n   def test_device_compatibility_warning(self):\n     if not tf.executing_eagerly():\n@@ -268,31 +216,6 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n                                          custom_objects={'MyPolicy': MyPolicy})\n       self.assertEqual(str(policy), str(new_policy))\n \n-    # Test V1 policies that override the loss scale\n-    for policy in (\n-        mp_policy.PolicyV1('float32', loss_scale=2.),\n-        mp_policy.PolicyV1('float32', loss_scale=None),\n-        mp_policy.PolicyV1('mixed_float16', loss_scale=2.),\n-        mp_policy.PolicyV1('mixed_float16', loss_scale=None),\n-        mp_policy.PolicyV1('mixed_bfloat16', loss_scale=2.),\n-        mp_policy.PolicyV1('mixed_bfloat16', loss_scale=None),\n-    ):\n-      config = mp_policy.serialize(policy)\n-      expected_loss_scale_config = None\n-      if policy.loss_scale:\n-        expected_loss_scale_config = {\n-            'class_name': 'FixedLossScale',\n-            'config': {'loss_scale_value': 2.}\n-        }\n-      self.assertEqual(\n-          config, {\n-              'class_name': policy.__class__.__name__,\n-              'config': {\n-                  'name': policy.name,\n-                  'loss_scale': expected_loss_scale_config\n-              }\n-          })\n-\n   @test_utils.enable_v2_dtype_behavior\n   def test_error_if_graph_rewrite_enabled(self):\n     try:\n\n@@ -93,9 +93,8 @@ def deserialize(config, custom_objects=None):\n       'ftrl': ftrl.Ftrl,\n       'lossscaleoptimizer': loss_scale_optimizer.LossScaleOptimizer,\n       'lossscaleoptimizerv3': loss_scale_optimizer.LossScaleOptimizerV3,\n-      # LossScaleOptimizerV1 deserializes into LossScaleOptimizer, as\n-      # LossScaleOptimizerV1 will be removed soon but deserializing it will\n-      # still be supported.\n+      # LossScaleOptimizerV1 was an old version of LSO that was removed.\n+      # Deserializing it turns it into a LossScaleOptimizer\n       'lossscaleoptimizerv1': loss_scale_optimizer.LossScaleOptimizer,\n   }\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
