{"custom_id": "scrapy#ae4aa2c3b24a967556f602a031520c535ebeb77d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 128 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -220,13 +220,13 @@ class WebClientTestCase(unittest.TestCase):\n         os.mkdir(name)\n         FilePath(name).child(\"file\").setContent(b\"0123456789\")\n         r = static.File(name)\n-        r.putChild(\"redirect\", util.Redirect(\"/file\"))\n-        r.putChild(\"wait\", ForeverTakingResource())\n-        r.putChild(\"error\", ErrorResource())\n-        r.putChild(\"nolength\", NoLengthResource())\n-        r.putChild(\"host\", HostHeaderResource())\n-        r.putChild(\"payload\", PayloadResource())\n-        r.putChild(\"broken\", BrokenDownloadResource())\n+        r.putChild(b\"redirect\", util.Redirect(\"/file\"))\n+        r.putChild(b\"wait\", ForeverTakingResource())\n+        r.putChild(b\"error\", ErrorResource())\n+        r.putChild(b\"nolength\", NoLengthResource())\n+        r.putChild(b\"host\", HostHeaderResource())\n+        r.putChild(b\"payload\", PayloadResource())\n+        r.putChild(b\"broken\", BrokenDownloadResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n         self.port = self._listen(self.wrapper)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5f9bc8499293aced54f54250a18e148af356267", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 148 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -240,14 +240,17 @@ class WebClientTestCase(unittest.TestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        return getPage(self.getURL(\"payload\"), body=s).addCallback(self.assertEquals, s)\n+        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n+            self.assertEquals, to_bytes(s))\n \n     def testHostHeader(self):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n         return defer.gatherResults([\n-            getPage(self.getURL(\"host\")).addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno),\n-            getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(self.assertEquals, \"www.example.com\")])\n+            getPage(self.getURL(\"host\")).addCallback(\n+                self.assertEquals, to_bytes(\"127.0.0.1:%d\" % self.portno)),\n+            getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(\n+                self.assertEquals, to_bytes(\"www.example.com\"))])\n \n \n     def test_getPage(self):\n@@ -280,7 +283,8 @@ class WebClientTestCase(unittest.TestCase):\n         called back with the contents of the page.\n         \"\"\"\n         d = getPage(self.getURL(\"host\"), timeout=100)\n-        d.addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno)\n+        d.addCallback(\n+            self.assertEquals, to_bytes(\"127.0.0.1:%d\" % self.portno))\n         return d\n \n \n@@ -309,7 +313,7 @@ class WebClientTestCase(unittest.TestCase):\n         return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n-        self.assert_('404 - No Such Resource' in pageData)\n+        self.assert_(b'404 - No Such Resource' in pageData)\n \n     def testFactoryInfo(self):\n         url = self.getURL('file')\n@@ -329,6 +333,6 @@ class WebClientTestCase(unittest.TestCase):\n \n     def _cbRedirect(self, pageData):\n         self.assertEquals(pageData,\n-                '\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n-                '    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n-                '<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n+                b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n+                b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n+                b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88f55312af0d9551f00686d79eaaf1c122740aa1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 156 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -323,10 +323,10 @@ class WebClientTestCase(unittest.TestCase):\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n-        self.assertEquals(factory.status, '200')\n-        self.assert_(factory.version.startswith('HTTP/'))\n-        self.assertEquals(factory.message, 'OK')\n-        self.assertEquals(factory.response_headers['content-length'], '10')\n+        self.assertEquals(factory.status, b'200')\n+        self.assert_(factory.version.startswith(b'HTTP/'))\n+        self.assertEquals(factory.message, b'OK')\n+        self.assertEquals(factory.response_headers[b'content-length'], b'10')\n \n     def testRedirect(self):\n         return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30c7b4e4cc5411af1015aa292e20e4c453607428", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 158 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -300,7 +300,7 @@ class WebClientTestCase(unittest.TestCase):\n         def cleanup(passthrough):\n             # Clean up the server which is hanging around not doing\n             # anything.\n-            connected = self.wrapper.protocols.keys()\n+            connected = list(six.iterkeys(self.wrapper.protocols))\n             # There might be nothing here if the server managed to already see\n             # that the connection was lost.\n             if connected:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01783561781e1620580e0f1b66f32a514ac2b960", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 160 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -220,7 +220,7 @@ class WebClientTestCase(unittest.TestCase):\n         os.mkdir(name)\n         FilePath(name).child(\"file\").setContent(b\"0123456789\")\n         r = static.File(name)\n-        r.putChild(b\"redirect\", util.Redirect(\"/file\"))\n+        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n         r.putChild(b\"wait\", ForeverTakingResource())\n         r.putChild(b\"error\", ErrorResource())\n         r.putChild(b\"nolength\", NoLengthResource())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5fb6094384f8d96e8a677b5fd9f2e6bfe2b97d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 288 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,13 +12,15 @@ from scrapy.responsetypes import responsetypes\n \n \n def _parsed_url_args(parsed):\n+    b = lambda x: to_bytes(x, encoding='ascii')\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n-    host = parsed.hostname\n+    path = to_bytes(path)  # FIXME\n+    host = b(parsed.hostname)  # FIXME\n     port = parsed.port\n-    scheme = parsed.scheme\n-    netloc = parsed.netloc\n+    scheme = b(parsed.scheme)\n+    netloc = b(parsed.netloc)  # FIXME - host + port\n     if port is None:\n-        port = 443 if scheme == 'https' else 80\n+        port = 443 if scheme == b'https' else 80\n     return scheme, netloc, host, port, path\n \n \n@@ -36,11 +38,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         self.headers = Headers() # bucket for response headers\n \n         # Method command\n-        self.sendCommand(\n-            to_bytes(self.factory.method, encoding='ascii'),\n-            # XXX - do we need to percent-encode path somewhere?\n-            # https://en.wikipedia.org/wiki/Percent-encoding#Character_data\n-            to_bytes(self.factory.path))\n+        self.sendCommand(self.factory.method, self.factory.path)\n         # Headers\n         for key, values in self.factory.headers.items():\n             for value in values:\n@@ -96,8 +94,10 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     afterFoundGet = False\n \n     def __init__(self, request, timeout=180):\n-        self.url = urldefrag(request.url)[0]\n-        self.method = request.method\n+        self._url = urldefrag(request.url)[0]\n+        # converting to bytes to comply to Twisted interface\n+        self.url = to_bytes(self._url)  # FIXME\n+        self.method = to_bytes(request.method, encoding='ascii')\n         self.body = request.body or None\n         self.headers = Headers(request.headers)\n         self.response_headers = None\n@@ -131,11 +131,11 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         request.meta['download_latency'] = self.headers_time-self.start_time\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n-        respcls = responsetypes.from_args(headers=headers, url=self.url)\n+        respcls = responsetypes.from_args(headers=headers, url=self._url)\n         # XXX - scrapy response stores body as bytes,\n         # but maybe it makes sense to be able to store unicode?\n         body = to_bytes(body)\n-        return respcls(url=self.url, status=status, headers=headers, body=body)\n+        return respcls(url=self._url, status=status, headers=headers, body=body)\n \n     def _set_connection_attributes(self, request):\n         parsed = urlparse_cached(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac2cf191d1868f19897a108a861e3170485a676a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 299 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,13 +12,12 @@ from scrapy.responsetypes import responsetypes\n \n \n def _parsed_url_args(parsed):\n-    b = lambda x: to_bytes(x, encoding='ascii')\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n-    path = to_bytes(path)  # FIXME\n-    host = b(parsed.hostname)  # FIXME\n+    path = to_bytes(path)\n+    host = to_bytes(parsed.hostname)\n     port = parsed.port\n-    scheme = b(parsed.scheme)\n-    netloc = b(parsed.netloc)  # FIXME - host + port\n+    scheme = to_bytes(parsed.scheme, encoding='ascii')\n+    netloc = to_bytes(parsed.netloc)\n     if port is None:\n         port = 443 if scheme == b'https' else 80\n     return scheme, netloc, host, port, path\n@@ -96,7 +95,7 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     def __init__(self, request, timeout=180):\n         self._url = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n-        self.url = to_bytes(self._url)  # FIXME\n+        self.url = to_bytes(self._url)\n         self.method = to_bytes(request.method, encoding='ascii')\n         self.body = request.body or None\n         self.headers = Headers(request.headers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8df35bcac6190ab92635e46009c2e099c8e6010d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 301 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -131,8 +131,6 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=self._url)\n-        # XXX - scrapy response stores body as bytes,\n-        # but maybe it makes sense to be able to store unicode?\n         body = to_bytes(body)\n         return respcls(url=self._url, status=status, headers=headers, body=body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c2241ccc7a886ea48ce12c2fb67cee5db24ce4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 479 | Contributors (this commit): 6 | Commits (past 90d): 16 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         self.factory.noPage(reason)\n \n     def handleResponse(self, response):\n-        if self.factory.method.upper() == 'HEAD':\n+        if self.factory.method.upper() == b'HEAD':\n             self.factory.page('')\n         elif self.length is not None and self.length > 0:\n             self.factory.noPage(self._connection_lost_reason)\n@@ -123,7 +123,7 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n         # Content-Length must be specified in POST method even with no body\n-        elif self.method == 'POST':\n+        elif self.method == b'POST':\n             self.headers['Content-Length'] = 0\n \n     def _build_response(self, body, request):\n\n@@ -68,6 +68,8 @@ class ParseUrlTestCase(unittest.TestCase):\n     )\n \n         for url, test in tests:\n+            test = tuple(\n+                to_bytes(x) if not isinstance(x, int) else x for x in test)\n             self.assertEquals(client._parse(url), test, url)\n \n     def test_externalUnicodeInterference(self):\n@@ -82,10 +84,10 @@ class ParseUrlTestCase(unittest.TestCase):\n             goodInput, badInput = badInput, goodInput\n         urlparse(badInput)\n         scheme, netloc, host, port, path = self._parse(goodInput)\n-        self.assertTrue(isinstance(scheme, str))\n-        self.assertTrue(isinstance(netloc, str))\n-        self.assertTrue(isinstance(host, str))\n-        self.assertTrue(isinstance(path, str))\n+        self.assertTrue(isinstance(scheme, bytes))\n+        self.assertTrue(isinstance(netloc, bytes))\n+        self.assertTrue(isinstance(host, bytes))\n+        self.assertTrue(isinstance(path, bytes))\n         self.assertTrue(isinstance(port, int))\n \n \n@@ -317,9 +319,9 @@ class WebClientTestCase(unittest.TestCase):\n \n     def testFactoryInfo(self):\n         url = self.getURL('file')\n-        scheme, netloc, host, port, path = client._parse(url)\n+        _, _, host, port, _ = client._parse(url)\n         factory = client.ScrapyHTTPClientFactory(Request(url))\n-        reactor.connectTCP(host, port, factory)\n+        reactor.connectTCP(to_unicode(host), port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32bb5b682a7cf4f2baca78914f18297c120eef2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 1408 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ from io import BytesIO\n from time import time\n from six.moves.urllib.parse import urldefrag\n \n-from zope.interface import implements\n+from zope.interface import implementer\n from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n@@ -265,8 +265,8 @@ class ScrapyAgent(object):\n         return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n \n \n+@implementer(IBodyProducer)\n class _RequestBodyProducer(object):\n-    implements(IBodyProducer)\n \n     def __init__(self, body):\n         self.body = body\n\n@@ -10,9 +10,7 @@ from twisted.web import server, static, util, resource\n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         NoLengthResource, HostHeaderResource, \\\n         PayloadResource, BrokenDownloadResource\n-from twisted.protocols.ftp import FTPRealm, FTPFactory\n from twisted.cred import portal, checkers, credentials\n-from twisted.protocols.ftp import FTPClient, ConnectionLost\n from w3lib.url import path_to_file_uri\n \n from scrapy import twisted_version\n@@ -22,7 +20,6 @@ from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownlo\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n-from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n from scrapy.spiders import Spider\n from scrapy.http import Request\n@@ -520,6 +517,9 @@ class FTPTestCase(unittest.TestCase):\n         skip = \"Twisted pre 10.2.0 doesn't allow to set home path other than /home\"\n \n     def setUp(self):\n+        from twisted.protocols.ftp import FTPRealm, FTPFactory\n+        from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n+\n         # setup dirs and test file\n         self.directory = self.mktemp()\n         os.mkdir(self.directory)\n@@ -601,6 +601,8 @@ class FTPTestCase(unittest.TestCase):\n         return self._add_test_callbacks(d, _test)\n \n     def test_invalid_credentials(self):\n+        from twisted.protocols.ftp import ConnectionLost\n+\n         request = Request(url=\"ftp://127.0.0.1:%s/file.txt\" % self.portNum,\n                 meta={\"ftp_user\": self.username, \"ftp_password\": 'invalid'})\n         d = self.download_handler.download_request(request, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3509378b8be0df7cb38d3823068288b5daa37612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 1657 | Contributors (this commit): 16 | Commits (past 90d): 9 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n+from scrapy.utils.python import to_bytes, to_unicode\n from scrapy import twisted_version\n \n logger = logging.getLogger(__name__)\n@@ -200,8 +201,8 @@ class ScrapyAgent(object):\n         agent = self._get_agent(request, timeout)\n \n         # request details\n-        url = urldefrag(request.url)[0]\n-        method = request.method\n+        url = to_bytes(urldefrag(request.url)[0])\n+        method = to_bytes(request.method)\n         headers = TxHeaders(request.headers)\n         if isinstance(agent, self._TunnelingAgent):\n             headers.removeHeader('Proxy-Authorization')\n@@ -261,8 +262,10 @@ class ScrapyAgent(object):\n         txresponse, body, flags = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n+        url = to_unicode(url)\n         respcls = responsetypes.from_args(headers=headers, url=url)\n-        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n+        return respcls(\n+            url=url, status=status, headers=headers, body=body, flags=flags)\n \n \n @implementer(IBodyProducer)\n\n@@ -4,6 +4,7 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+import six\n from six.moves.urllib.parse import urljoin\n \n from scrapy.http.headers import Headers\n@@ -34,7 +35,7 @@ class Response(object_ref):\n         return self._url\n \n     def _set_url(self, url):\n-        if isinstance(url, str):\n+        if isinstance(url, six.string_types):\n             self._url = url\n         else:\n             raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n\n@@ -88,7 +88,7 @@ class FileTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEquals(response.url, request.url)\n             self.assertEquals(response.status, 200)\n-            self.assertEquals(response.body, '0123456789')\n+            self.assertEquals(response.body, b'0123456789')\n \n         request = Request(path_to_file_uri(self.tmpname + '^'))\n         assert request.url.upper().endswith('%5E')\n@@ -107,15 +107,15 @@ class HttpTestCase(unittest.TestCase):\n     def setUp(self):\n         name = self.mktemp()\n         os.mkdir(name)\n-        FilePath(name).child(\"file\").setContent(\"0123456789\")\n+        FilePath(name).child(\"file\").setContent(b\"0123456789\")\n         r = static.File(name)\n-        r.putChild(\"redirect\", util.Redirect(\"/file\"))\n-        r.putChild(\"wait\", ForeverTakingResource())\n-        r.putChild(\"hang-after-headers\", ForeverTakingResource(write=True))\n-        r.putChild(\"nolength\", NoLengthResource())\n-        r.putChild(\"host\", HostHeaderResource())\n-        r.putChild(\"payload\", PayloadResource())\n-        r.putChild(\"broken\", BrokenDownloadResource())\n+        r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n+        r.putChild(b\"wait\", ForeverTakingResource())\n+        r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n+        r.putChild(b\"nolength\", NoLengthResource())\n+        r.putChild(b\"host\", HostHeaderResource())\n+        r.putChild(b\"payload\", PayloadResource())\n+        r.putChild(b\"broken\", BrokenDownloadResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n         self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n@@ -136,7 +136,7 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, \"0123456789\")\n+        d.addCallback(self.assertEquals, b\"0123456789\")\n         return d\n \n     def test_download_head(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b79fffa9a4c53cb6a6af2e9d9251b95c28496b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1455 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -182,7 +182,7 @@ class ScrapyAgent(object):\n             _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             omitConnectTunnel = proxyParams.find('noconnect') >= 0\n-            if  scheme == 'https' and not omitConnectTunnel:\n+            if  scheme == b'https' and not omitConnectTunnel:\n                 proxyConf = (proxyHost, proxyPort,\n                              request.headers.get('Proxy-Authorization', None))\n                 return self._TunnelingAgent(reactor, proxyConf,\n@@ -233,7 +233,7 @@ class ScrapyAgent(object):\n     def _cb_bodyready(self, txresponse, request):\n         # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n-            return txresponse, '', None\n+            return txresponse, b'', None\n \n         maxsize = request.meta.get('download_maxsize', self._maxsize)\n         warnsize = request.meta.get('download_warnsize', self._warnsize)\n\n@@ -1,5 +1,4 @@\n import os\n-import twisted\n import six\n \n from twisted.trial import unittest\n@@ -25,6 +24,7 @@ from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler\n+from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n from tests.mockserver import MockServer\n@@ -143,7 +143,7 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('file'), method='HEAD')\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, '')\n+        d.addCallback(self.assertEquals, b'')\n         return d\n \n     def test_redirect_status(self):\n@@ -175,7 +175,7 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n-            self.assertEquals(response.body, '127.0.0.1:%d' % self.portno)\n+            self.assertEquals(response.body, to_bytes('127.0.0.1:%d' % self.portno))\n             self.assertEquals(request.headers, {})\n \n         request = Request(self.getURL('host'))\n@@ -183,19 +183,19 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_host_header_seted_in_request_headers(self):\n         def _test(response):\n-            self.assertEquals(response.body, 'example.com')\n-            self.assertEquals(request.headers.get('Host'), 'example.com')\n+            self.assertEquals(response.body, b'example.com')\n+            self.assertEquals(request.headers.get('Host'), b'example.com')\n \n         request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, 'example.com')\n+        d.addCallback(self.assertEquals, b'example.com')\n         return d\n \n     def test_payload(self):\n-        body = '1'*100 # PayloadResource requires body length to be 100\n+        body = b'1'*100 # PayloadResource requires body length to be 100\n         request = Request(self.getURL('payload'), method='POST', body=body)\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6f14a39de14b15b78d3fd4a098ec246536e12c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 277 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@\n \"\"\"\n from twisted.internet import reactor\n from scrapy.utils.misc import load_object\n+from scrapy.utils.python import to_unicode\n \n \n class HTTP10DownloadHandler(object):\n@@ -17,8 +18,8 @@ class HTTP10DownloadHandler(object):\n         return factory.deferred\n \n     def _connect(self, factory):\n-        host, port = factory.host, factory.port\n-        if factory.scheme == 'https':\n+        host, port = to_unicode(factory.host), factory.port\n+        if factory.scheme == b'https':\n             return reactor.connectSSL(host, port, factory,\n                                       self.ClientContextFactory())\n         else:\n\n@@ -223,7 +223,7 @@ class Http11TestCase(HttpTestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, \"0123456789\")\n+        d.addCallback(self.assertEquals, b\"0123456789\")\n         return d\n \n     @defer.inlineCallbacks\n@@ -234,7 +234,7 @@ class Http11TestCase(HttpTestCase):\n         # response body. (regardless of headers)\n         d = self.download_request(request, Spider('foo', download_maxsize=10))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, \"0123456789\")\n+        d.addCallback(self.assertEquals, b\"0123456789\")\n         yield d\n \n         d = self.download_request(request, Spider('foo', download_maxsize=9))\n@@ -257,7 +257,7 @@ class Http11TestCase(HttpTestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo', download_maxsize=100))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, \"0123456789\")\n+        d.addCallback(self.assertEquals, b\"0123456789\")\n         return d\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4950f5988ef1df5bc6b6ec2c4a70a7956f64b539", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1474 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -181,10 +181,11 @@ class ScrapyAgent(object):\n         if proxy:\n             _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n-            omitConnectTunnel = proxyParams.find('noconnect') >= 0\n+            proxyHost = to_unicode(proxyHost)\n+            omitConnectTunnel = proxyParams.find(b'noconnect') >= 0\n             if  scheme == b'https' and not omitConnectTunnel:\n                 proxyConf = (proxyHost, proxyPort,\n-                             request.headers.get('Proxy-Authorization', None))\n+                             request.headers.get(b'Proxy-Authorization', None))\n                 return self._TunnelingAgent(reactor, proxyConf,\n                     contextFactory=self._contextFactory, connectTimeout=timeout,\n                     bindAddress=bindaddress, pool=self._pool)\n@@ -205,7 +206,7 @@ class ScrapyAgent(object):\n         method = to_bytes(request.method)\n         headers = TxHeaders(request.headers)\n         if isinstance(agent, self._TunnelingAgent):\n-            headers.removeHeader('Proxy-Authorization')\n+            headers.removeHeader(b'Proxy-Authorization')\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n \n         start_time = time()\n\n@@ -351,7 +351,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, 'http://example.com')\n+            self.assertEquals(response.body, b'http://example.com')\n \n         http_proxy = self.getURL('')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n@@ -361,7 +361,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, 'https://example.com')\n+            self.assertEquals(response.body, b'https://example.com')\n \n         http_proxy = '%s?noconnect' % self.getURL('')\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n@@ -371,7 +371,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, '/path/to/resource')\n+            self.assertEquals(response.body, b'/path/to/resource')\n \n         request = Request(self.getURL('path/to/resource'))\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f46a9d595dee801d0ea13d7cdaab8b8de952929f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 255 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -515,6 +515,8 @@ class FTPTestCase(unittest.TestCase):\n \n     if twisted_version < (10, 2, 0):\n         skip = \"Twisted pre 10.2.0 doesn't allow to set home path other than /home\"\n+    if six.PY3:\n+        skip = \"Twisted missing ftp support for PY3\"\n \n     def setUp(self):\n         from twisted.protocols.ftp import FTPRealm, FTPFactory\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2aa6c92ffca50c8f6e5d057ac2808a99785eb88f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 48 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -134,12 +134,12 @@ class Echo(LeafResource):\n class Partial(LeafResource):\n \n     def render_GET(self, request):\n-        request.setHeader(\"Content-Length\", \"1024\")\n+        request.setHeader(b\"Content-Length\", b\"1024\")\n         self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n \n     def _delayedRender(self, request):\n-        request.write(\"partial content\\n\")\n+        request.write(b\"partial content\\n\")\n         request.finish()\n \n \n@@ -147,7 +147,7 @@ class Drop(Partial):\n \n     def _delayedRender(self, request):\n         abort = getarg(request, \"abort\", 0, type=int)\n-        request.write(\"this connection will be dropped\\n\")\n+        request.write(b\"this connection will be dropped\\n\")\n         tr = request.channel.transport\n         try:\n             if abort and hasattr(tr, 'abortConnection'):\n@@ -162,13 +162,13 @@ class Root(Resource):\n \n     def __init__(self):\n         Resource.__init__(self)\n-        self.putChild(\"status\", Status())\n-        self.putChild(\"follow\", Follow())\n-        self.putChild(\"delay\", Delay())\n-        self.putChild(\"partial\", Partial())\n-        self.putChild(\"drop\", Drop())\n-        self.putChild(\"raw\", Raw())\n-        self.putChild(\"echo\", Echo())\n+        self.putChild(b\"status\", Status())\n+        self.putChild(b\"follow\", Follow())\n+        self.putChild(b\"delay\", Delay())\n+        self.putChild(b\"partial\", Partial())\n+        self.putChild(b\"drop\", Drop())\n+        self.putChild(b\"raw\", Raw())\n+        self.putChild(b\"echo\", Echo())\n \n         if six.PY2 and twisted_version > (12, 3, 0):\n             from twisted.web.test.test_webclient import PayloadResource\n@@ -181,7 +181,7 @@ class Root(Resource):\n         return self\n \n     def render(self, request):\n-        return 'Scrapy mock HTTP server\\n'\n+        return b'Scrapy mock HTTP server\\n'\n \n \n class MockServer():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81a90c3af65ce863c073e2d83a6b149a03e7d4cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 320 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,12 +170,12 @@ class Root(Resource):\n         self.putChild(b\"raw\", Raw())\n         self.putChild(b\"echo\", Echo())\n \n-        if six.PY2 and twisted_version > (12, 3, 0):\n+        if twisted_version > (12, 3, 0):\n             from twisted.web.test.test_webclient import PayloadResource\n             from twisted.web.server import GzipEncoderFactory\n             from twisted.web.resource import EncodingResourceWrapper\n-            self.putChild('payload', PayloadResource())\n-            self.putChild(\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+            self.putChild(b\"payload\", PayloadResource())\n+            self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n \n     def getChild(self, name, request):\n         return self\n\n@@ -294,27 +294,30 @@ class Http11MockServerTestCase(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_download_gzip_response(self):\n \n-        if six.PY2 and twisted_version > (12, 3, 0):\n+        if twisted_version > (12, 3, 0):\n \n             crawler = get_crawler(SingleRequestSpider)\n-            body = '1'*100 # PayloadResource requires body length to be 100\n+            body = b'1'*100 # PayloadResource requires body length to be 100\n             request = Request('http://localhost:8998/payload', method='POST', body=body, meta={'download_maxsize': 50})\n             yield crawler.crawl(seed=request)\n             failure = crawler.spider.meta['failure']\n             # download_maxsize < 100, hence the CancelledError\n             self.assertIsInstance(failure.value, defer.CancelledError)\n \n-            request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n+            request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n             request = request.replace(url='http://localhost:8998/xpayload')\n             yield crawler.crawl(seed=request)\n \n+            if six.PY2:\n                 # download_maxsize = 50 is enough for the gzipped response\n+                # See issue https://twistedmatrix.com/trac/ticket/8175\n+                raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n                 failure = crawler.spider.meta.get('failure')\n                 self.assertTrue(failure == None)\n                 reason = crawler.spider.meta['close_reason']\n                 self.assertTrue(reason, 'finished')\n         else:\n-            raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0 and python 2.x\")\n+            raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0\")\n \n \n class UriResource(resource.Resource):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94ab7bee6c5630bcb36e7b758daf31e45d493613", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 42 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n+from scrapy.utils.python import to_bytes\n from tests import mock\n \n \n@@ -68,7 +69,7 @@ class DefaultsTest(ManagerTestCase):\n \n         \"\"\"\n         req = Request('http://example.com')\n-        body = '<p>You are being redirected</p>'\n+        body = b'<p>You are being redirected</p>'\n         resp = Response(req.url, status=302, body=body, headers={\n             'Content-Length': str(len(body)),\n             'Content-Type': 'text/html',\n@@ -78,12 +79,12 @@ class DefaultsTest(ManagerTestCase):\n         ret = self._download(request=req, response=resp)\n         self.assertTrue(isinstance(ret, Request),\n                         \"Not redirected: {0!r}\".format(ret))\n-        self.assertEqual(ret.url, resp.headers['Location'],\n+        self.assertEqual(to_bytes(ret.url), resp.headers['Location'],\n                          \"Not redirected to location header\")\n \n     def test_200_and_invalid_gzipped_body_must_fail(self):\n         req = Request('http://example.com')\n-        body = '<p>You are being redirected</p>'\n+        body = b'<p>You are being redirected</p>'\n         resp = Response(req.url, status=200, body=body, headers={\n             'Content-Length': str(len(body)),\n             'Content-Type': 'text/html',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbf6cc73d96f5015e14d4075e5a0d50db2a96453", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 436 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -18,6 +18,8 @@ import sys\n from importlib import import_module\n from os.path import join, abspath, dirname\n \n+import six\n+\n AJAXCRAWL_ENABLED = False\n \n AUTOTHROTTLE_ENABLED = False\n@@ -163,7 +165,7 @@ HTTPCACHE_ALWAYS_STORE = False\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n-HTTPCACHE_DBM_MODULE = 'anydbm'\n+HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7ed1fd70df28d529b5f1df04bb850bb260dfc01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 21 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 52 | Churn Cumulative: 114 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,7 @@ class RFC2616Policy(object):\n \n     def _parse_cachecontrol(self, r):\n         if r not in self._cc_parsed:\n-            cch = r.headers.get('Cache-Control', '')\n+            cch = r.headers.get(b'Cache-Control', b'')\n             parsed = parse_cachecontrol(cch)\n             if isinstance(r, Response):\n                 for key in self.ignore_response_cache_controls:\n@@ -58,7 +58,7 @@ class RFC2616Policy(object):\n             return False\n         cc = self._parse_cachecontrol(request)\n         # obey user-agent directive \"Cache-Control: no-store\"\n-        if 'no-store' in cc:\n+        if b'no-store' in cc:\n             return False\n         # Any other is eligible for caching\n         return True\n@@ -69,7 +69,7 @@ class RFC2616Policy(object):\n         # Status code 206 is not included because cache can not deal with partial contents\n         cc = self._parse_cachecontrol(response)\n         # obey directive \"Cache-Control: no-store\"\n-        if 'no-store' in cc:\n+        if b'no-store' in cc:\n             return False\n         # Never cache 304 (Not Modified) responses\n         elif response.status == 304:\n@@ -78,14 +78,14 @@ class RFC2616Policy(object):\n         elif self.always_store:\n             return True\n         # Any hint on response expiration is good\n-        elif 'max-age' in cc or 'Expires' in response.headers:\n+        elif b'max-age' in cc or b'Expires' in response.headers:\n             return True\n         # Firefox fallbacks this statuses to one year expiration if none is set\n         elif response.status in (300, 301, 308):\n             return True\n         # Other statuses without expiration requires at least one validator\n         elif response.status in (200, 203, 401):\n-            return 'Last-Modified' in response.headers or 'ETag' in response.headers\n+            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n         # Any other is probably not eligible for caching\n         # Makes no sense to cache responses that does not contain expiration\n         # info and can not be revalidated\n@@ -95,7 +95,7 @@ class RFC2616Policy(object):\n     def is_cached_response_fresh(self, cachedresponse, request):\n         cc = self._parse_cachecontrol(cachedresponse)\n         ccreq = self._parse_cachecontrol(request)\n-        if 'no-cache' in cc or 'no-cache' in ccreq:\n+        if b'no-cache' in cc or b'no-cache' in ccreq:\n             return False\n \n         now = time()\n@@ -109,7 +109,7 @@ class RFC2616Policy(object):\n         if currentage < freshnesslifetime:\n             return True\n \n-        if 'max-stale' in ccreq and 'must-revalidate' not in cc:\n+        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n             # From RFC2616: \"Indicates that the client is willing to\n             # accept a response that has exceeded its expiration time.\n             # If max-stale is assigned a value, then the client is\n@@ -117,7 +117,7 @@ class RFC2616Policy(object):\n             # expiration time by no more than the specified number of\n             # seconds. If no value is assigned to max-stale, then the\n             # client is willing to accept a stale response of any age.\"\n-            staleage = ccreq['max-stale']\n+            staleage = ccreq[b'max-stale']\n             if staleage is None:\n                 return True\n \n@@ -136,22 +136,22 @@ class RFC2616Policy(object):\n         # as long as the old response didn't specify must-revalidate.\n         if response.status >= 500:\n             cc = self._parse_cachecontrol(cachedresponse)\n-            if 'must-revalidate' not in cc:\n+            if b'must-revalidate' not in cc:\n                 return True\n \n         # Use the cached response if the server says it hasn't changed.\n         return response.status == 304\n \n     def _set_conditional_validators(self, request, cachedresponse):\n-        if 'Last-Modified' in cachedresponse.headers:\n-            request.headers['If-Modified-Since'] = cachedresponse.headers['Last-Modified']\n+        if b'Last-Modified' in cachedresponse.headers:\n+            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n \n-        if 'ETag' in cachedresponse.headers:\n-            request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n+        if b'ETag' in cachedresponse.headers:\n+            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']\n \n     def _get_max_age(self, cc):\n         try:\n-            return max(0, int(cc['max-age']))\n+            return max(0, int(cc[b'max-age']))\n         except (KeyError, ValueError):\n             return None\n \n@@ -164,18 +164,18 @@ class RFC2616Policy(object):\n             return maxage\n \n         # Parse date header or synthesize it if none exists\n-        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n \n         # Try HTTP/1.0 Expires header\n-        if 'Expires' in response.headers:\n-            expires = rfc1123_to_epoch(response.headers['Expires'])\n+        if b'Expires' in response.headers:\n+            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n             # When parsing Expires header fails RFC 2616 section 14.21 says we\n             # should treat this as an expiration time in the past.\n             return max(0, expires - date) if expires else 0\n \n         # Fallback to heuristic using last-modified header\n         # This is not in RFC but on Firefox caching implementation\n-        lastmodified = rfc1123_to_epoch(response.headers.get('Last-Modified'))\n+        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n         if lastmodified and lastmodified <= date:\n             return (date - lastmodified) / 10\n \n@@ -192,13 +192,13 @@ class RFC2616Policy(object):\n         currentage = 0\n         # If Date header is not set we assume it is a fast connection, and\n         # clock is in sync with the server\n-        date = rfc1123_to_epoch(response.headers.get('Date')) or now\n+        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n         if now > date:\n             currentage = now - date\n \n-        if 'Age' in response.headers:\n+        if b'Age' in response.headers:\n             try:\n-                age = int(response.headers['Age'])\n+                age = int(response.headers[b'Age'])\n                 currentage = max(currentage, age)\n             except ValueError:\n                 pass\n@@ -404,16 +404,16 @@ def parse_cachecontrol(header):\n \n     http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n \n-    >>> parse_cachecontrol('public, max-age=3600') == {'public': None,\n-    ...                                                'max-age': '3600'}\n+    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n+    ...                                                 b'max-age': b'3600'}\n     True\n-    >>> parse_cachecontrol('') == {}\n+    >>> parse_cachecontrol(b'') == {}\n     True\n \n     \"\"\"\n     directives = {}\n-    for directive in header.split(','):\n-        key, sep, val = directive.strip().partition('=')\n+    for directive in header.split(b','):\n+        key, sep, val = directive.strip().partition(b'=')\n         if key:\n             directives[key.lower()] = val if sep else None\n     return directives\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
