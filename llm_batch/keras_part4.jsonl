{"custom_id": "keras#52251f523f6700c50881ebc28edf13913ef0fde5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 295 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -48,12 +48,13 @@ skip_top = 100 # ignore top 100 most common words\n nb_epoch = 1\n dim_proj = 256 # embedding space dimension\n \n-save = False\n-load = False\n+save = True\n+load_model = False\n+load_tokenizer = False\n train_model = True\n save_dir = os.path.expanduser(\"~/.keras/models\")\n-model_load_fname = \"HN_skipgram_model_full_256.pkl\"\n-model_save_fname = \"HN_skipgram_model_full_256.pkl\"\n+model_load_fname = \"HN_skipgram_model.pkl\"\n+model_save_fname = \"HN_skipgram_model.pkl\"\n tokenizer_fname = \"HN_tokenizer.pkl\"\n \n data_path = os.path.expanduser(\"~/\")+\"HNCommentsAll.1perline.json\"\n@@ -83,11 +84,9 @@ def text_generator(path=data_path):\n     f.close()\n \n # model management\n-if load:\n+if load_tokenizer:\n     print('Load tokenizer...')\n-    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname)))\n-    print('Load model...')\n-    model = six.moves.cPickle.load(open(os.path.join(save_dir, model_load_fname)))\n+    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n else:\n     print(\"Fit tokenizer...\")\n     tokenizer = text.Tokenizer(nb_words=max_features)\n@@ -96,15 +95,18 @@ else:\n         print(\"Save tokenizer...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        six.moves.cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"w\"))\n+        six.moves.cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"wb\"))\n \n # training process\n if train_model:\n-    if not load:\n+    if load_model:\n+        print('Load model...')\n+        model = six.moves.cPickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n+    else:\n         print('Build model...')\n         model = Sequential()\n         model.add(WordContextProduct(max_features, proj_dim=dim_proj, init=\"normal\"))\n-        model.compile(loss='mse', optimizer='rmsprop')\n+        model.compile(loss='hinge', optimizer='adam')\n \n     sampling_table = sequence.make_sampling_table(max_features)\n \n@@ -136,7 +138,7 @@ if train_model:\n         print(\"Saving model...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        six.moves.cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"w\"))\n+        six.moves.cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"wb\"))\n \n \n print(\"It's test time!\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d032cbf9ea40d813ea887182a214a5296172aac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -149,7 +149,7 @@ class Tokenizer(object):\n         if mode == \"tfidf\" and not self.document_count:\n             raise Exception(\"Fit the Tokenizer on some data before using tfidf mode\")\n \n-        X = np.zeros((len(sequences), nb_words))\n+        X = np.zeros((len(sequences), nb_words+1))\n         for i, seq in enumerate(sequences):\n             if not seq:\n                 pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e3ec8ef69bd01e86d2575e9b7d84b35e549e3d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 122 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,122 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+\n+import numpy as np\n+import pandas as pd\n+\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers.normalization import BatchNormalization\n+from keras.layers.advanced_activations import PReLU\n+from keras.utils import np_utils, generic_utils\n+\n+from sklearn.preprocessing import LabelEncoder\n+from sklearn.preprocessing import StandardScaler\n+\n+'''\n+    This demonstrates how to reach a score of 0.4890 (local validation)\n+    on the Kaggle Otto challenge, with a deep net using Keras.\n+\n+    Compatible Python 2.7-3.4 \n+\n+    Recommended to run on GPU: \n+        Command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kaggle_otto_nn.py\n+        On EC2 g2.2xlarge instance: 10s/epoch. 6-7 minutes total training time.\n+\n+    Best validation score at epoch 21: 0.4881 \n+\n+    Try it at home:\n+        - with/without BatchNormalization (BatchNormalization helps!)\n+        - with ReLU or with PReLU (PReLU helps!)\n+        - with smaller layers, largers layers\n+        - with more layers, less layers\n+        - with different optimizers (SGD+momentum+decay is probably better than Adam!)\n+'''\n+\n+np.random.seed(1337) # for reproducibility\n+\n+def load_data(path, train=True):\n+    df = pd.read_csv(path)\n+    X = df.values.copy()\n+    if train:\n+        np.random.shuffle(X) # https://youtu.be/uyUXoap67N8\n+        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n+        return X, labels\n+    else:\n+        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n+        return X, ids\n+\n+def preprocess_data(X, scaler=None):\n+    if not scaler:\n+        scaler = StandardScaler()\n+        scaler.fit(X)\n+    X = scaler.transform(X)\n+    return X, scaler\n+\n+def preprocess_labels(y, encoder=None, categorical=True):\n+    if not encoder:\n+        encoder = LabelEncoder()\n+        encoder.fit(labels)\n+    y = encoder.transform(labels).astype(np.int32)\n+    if categorical:\n+        y = np_utils.to_categorical(y)\n+    return y, encoder\n+\n+def make_submission(y_prob, ids, encoder, fname):\n+    with open(fname, 'w') as f:\n+        f.write('id,')\n+        f.write(','.join(encoder.classes_))\n+        f.write('\\n')\n+        for i, probs in zip(ids, y_prob):\n+            probas = ','.join([i] + [str(p) for p in probs.tolist()])\n+            f.write(probas)\n+            f.write('\\n')\n+    print(\"Wrote submission to file {}.\".format(fname))\n+\n+\n+print(\"Loading data...\")\n+X, labels = load_data('train.csv', train=True)\n+X, scaler = preprocess_data(X)\n+y, encoder = preprocess_labels(labels)\n+\n+X_test, ids = load_data('test.csv', train=False)\n+X_test, _ = preprocess_data(X_test)\n+\n+nb_classes = y.shape[1]\n+print(nb_classes, 'classes')\n+\n+dims = X.shape[1]\n+print(dims, 'dims')\n+\n+print(\"Building model...\")\n+\n+model = Sequential()\n+model.add(Dense(dims, 512, init='glorot_uniform'))\n+model.add(PReLU((512,)))\n+model.add(BatchNormalization((512,)))\n+model.add(Dropout(0.5))\n+\n+model.add(Dense(512, 512, init='glorot_uniform'))\n+model.add(PReLU((512,)))\n+model.add(BatchNormalization((512,)))\n+model.add(Dropout(0.5))\n+\n+model.add(Dense(512, 512, init='glorot_uniform'))\n+model.add(PReLU((512,)))\n+model.add(BatchNormalization((512,)))\n+model.add(Dropout(0.5))\n+\n+model.add(Dense(512, nb_classes, init='glorot_uniform'))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n+\n+print(\"Training model...\")\n+\n+model.fit(X, y, nb_epoch=20, batch_size=16, validation_split=0.15)\n+\n+print(\"Generating submission...\")\n+\n+proba = model.predict_proba(X_test)\n+make_submission(proba, ids, encoder, fname='keras-otto.csv')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73bb22469fab9e7875fe1620fff90012542cc9a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 124 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ from sklearn.preprocessing import StandardScaler\n \n     Recommended to run on GPU: \n         Command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kaggle_otto_nn.py\n-        On EC2 g2.2xlarge instance: 10s/epoch. 6-7 minutes total training time.\n+        On EC2 g2.2xlarge instance: 19s/epoch. 6-7 minutes total training time.\n \n     Best validation score at epoch 21: 0.4881 \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22012fdecc1deec6827624de1a6f4bf17e416b38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 126 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ X, scaler = preprocess_data(X)\n y, encoder = preprocess_labels(labels)\n \n X_test, ids = load_data('test.csv', train=False)\n-X_test, _ = preprocess_data(X_test)\n+X_test, _ = preprocess_data(X_test, scaler)\n \n nb_classes = y.shape[1]\n print(nb_classes, 'classes')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cde6df7a5eefec715ed95bf28cee5a53a25035a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 29 | Methods Changed: 21 | Complexity Δ (Sum/Max): 3/8 | Churn Δ: 101 | Churn Cumulative: 702 | Contributors (this commit): 10 | Commits (past 90d): 37 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,34 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams()\n \n+def l1(lam):\n+    def l1wrap(g,p):\n+        g += T.sgn(p) * lam\n+        return g\n+    return l1wrap\n+\n+def l2(lam):\n+    def l2wrap(g,p):\n+        g += p * lam\n+        return g\n+    return l2wrap\n+\n+def maxnorm(m):\n+    def maxnorm_wrap(p):\n+        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n+        desired = T.clip(norms, 0, m)\n+        p = p * (desired / (1e-7 + norms))\n+        return p\n+    return maxnorm_wrap\n+\n+def nonneg(p):\n+    p *= T.ge(p,0)\n+    return p\n+\n+def ident(g,*l):\n+    return g\n+\n+\n class Layer(object):\n     def connect(self, previous_layer):\n         self.previous_layer = previous_layer\n@@ -46,6 +74,8 @@ class Dropout(Layer):\n     def __init__(self, p):\n         self.p = p\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -69,6 +99,8 @@ class Activation(Layer):\n     def __init__(self, activation):\n         self.activation = activations.get(activation)\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -88,6 +120,8 @@ class Reshape(Layer):\n     def __init__(self, *dims):\n         self.dims = dims\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -106,6 +140,8 @@ class Flatten(Layer):\n     '''\n     def __init__(self):\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -124,6 +160,8 @@ class RepeatVector(Layer):\n     def __init__(self, n):\n         self.n = n\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -140,7 +178,7 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n+    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, regularizer=[ident, ident], constraint=[ident, ident]):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -152,6 +190,9 @@ class Dense(Layer):\n \n         self.params = [self.W, self.b]\n \n+        self.regularizer = regularizer\n+        self.constraint = constraint\n+\n         if weights is not None:\n             self.set_weights(weights)\n \n@@ -176,7 +217,7 @@ class TimeDistributedDense(Layer):\n        Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n \n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n+    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, regularizer=[ident, ident], constraint=[ident, ident]):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -188,6 +229,9 @@ class TimeDistributedDense(Layer):\n \n         self.params = [self.W, self.b]\n \n+        self.regularizer = regularizer\n+        self.constraint = constraint\n+\n         if weights is not None:\n             self.set_weights(weights)\n \n\n@@ -25,12 +25,16 @@ class Sequential(object):\n     def __init__(self):\n         self.layers = []\n         self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n \n     def add(self, layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n             self.layers[-1].connect(self.layers[-2])\n         self.params += [p for p in layer.params]\n+        self.regularizer += [r for r in layer.regularizer]\n+        self.constraint += [c for c in layer.constraint]\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\"):\n         self.optimizer = optimizers.get(optimizer)\n@@ -58,7 +62,7 @@ class Sequential(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        updates = self.optimizer.get_updates(self.params, train_loss)\n+        updates = self.optimizer.get_updates(self.params, self.regularizer, self.constraint, train_loss)\n \n         self._train = theano.function([self.X, self.y], train_loss, \n             updates=updates, allow_input_downcast=True)\n\n@@ -22,21 +22,7 @@ class Optimizer(object):\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n             grads = [clip_norm(g, c, norm) for g in grads]\n \n-        new_grads = []\n-        for p, g in zip(params, grads):\n-            if hasattr(self, 'l1') and self.l1 > 0:\n-                g += T.sgn(p) * self.l1\n-\n-            if hasattr(self, 'l2') and self.l2 > 0:\n-                g += p * self.l2\n-\n-            if hasattr(self, 'maxnorm') and self.maxnorm > 0:\n-                norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n-                desired = T.clip(norms, 0, self.maxnorm)\n-                p = p * (desired / (1e-7 + norms))\n-\n-            new_grads.append(g)\n-        return new_grads\n+        return grads\n \n \n class SGD(Optimizer):\n@@ -45,12 +31,13 @@ class SGD(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, cost):\n+    def get_updates(self, params, regularizers, constraints, cost):\n         grads = self.get_gradients(cost, params)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n-        for p, g in zip(params, grads):\n+        for p, g, r, c in zip(params, grads, regularizers, constraints):\n+            g = r(g,p)\n             m = shared_zeros(p.get_value().shape) # momentum\n             v = self.momentum * m - lr * g # velocity\n             updates.append((m, v)) \n@@ -59,6 +46,8 @@ class SGD(Optimizer):\n                 new_p = p + self.momentum * v - lr * g\n             else:\n                 new_p = p + v\n+\n+            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -68,16 +57,18 @@ class RMSprop(Optimizer):\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, cost):\n+    def get_updates(self, params, regularizers, constraints, cost):\n         grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a in zip(params, grads, accumulators):\n+        for p, g, a, r, c in zip(params, grads, accumulators, regularizers, constraints):\n+            g = r(g,p)\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -87,16 +78,18 @@ class Adagrad(Optimizer):\n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, cost):\n+    def get_updates(self, params, regularizers, constraints, cost):\n         grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a in zip(params, grads, accumulators):\n+        for p, g, a, r, c in zip(params, grads, accumulators, regularizers, constraints):\n+            g = r(g,p)\n             new_a = a + g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -108,13 +101,14 @@ class Adadelta(Optimizer):\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, cost):\n+    def get_updates(self, params, regularizers, constraints, cost):\n         grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n+        for p, g, a, d_a, r, c in zip(params, grads, accumulators, delta_accumulators, regularizers, constraints):\n+            g = r(g,p)\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n@@ -122,6 +116,7 @@ class Adadelta(Optimizer):\n             update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n+            new_p = c(new_p)\n             updates.append((p, new_p))\n \n             # update delta_accumulator\n@@ -142,7 +137,7 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, cost):\n+    def get_updates(self, params, regularizers, constraints, cost):\n         grads = self.get_gradients(cost, params)\n         updates = [(self.iterations, self.iterations+1.)]\n \n@@ -152,7 +147,8 @@ class Adam(Optimizer):\n         # the update below seems missing from the paper, but is obviously required\n         beta_2_t = self.beta_2 * (self.kappa**i) \n \n-        for p, g in zip(params, grads):\n+        for p, g, r, c in zip(params, grads, regularizers, constraints):\n+            g = r(g,p)\n             m = theano.shared(p.get_value() * 0.) # zero init of moment\n             v = theano.shared(p.get_value() * 0.) # zero init of velocity\n \n@@ -164,6 +160,7 @@ class Adam(Optimizer):\n \n             p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon)\n             \n+            p_t = c(p_t)\n             updates.append((m, m_t))\n             updates.append((v, v_t))\n             updates.append((p, p_t))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df154aeab8bc8a8e7f306de100363124a8566b74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 59 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n from __future__ import absolute_import\n from .data_utils import get_file\n import random\n-import sys\n+import sys, os\n import numpy as np\n import six.moves.cPickle\n from six.moves import range\n@@ -16,7 +16,7 @@ def load_data(test_split=0.1, seed=113):\n     X = np.zeros((nb_samples, 3, 32, 32), dtype=\"uint8\")\n     y = np.zeros((nb_samples,), dtype=\"uint8\")\n     for i in range(1, 6):\n-        fpath = path + '/data_batch_' + str(i)\n+        fpath = os.path.join(path, 'data_batch_' + str(i))\n         f = open(fpath, 'rb')\n         if sys.version_info < (3,):\n             d = six.moves.cPickle.load(f)\n\n@@ -5,7 +5,7 @@ from six.moves.urllib.request import urlretrieve\n from ..utils.generic_utils import Progbar\n \n def get_file(fname, origin, untar=False):\n-    datadir = os.path.expanduser(\"~/.keras/datasets\")\n+    datadir = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n     if not os.path.exists(datadir):\n         os.makedirs(datadir)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ebb8910a45c1dc23a31da236a3600fd9bdc0bee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 59 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,11 +4,11 @@ from __future__ import print_function\n from .data_utils import get_file\n import string\n import random\n+import os\n import six.moves.cPickle\n from six.moves import zip\n \n-def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_topic=15):\n-    import os\n+def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'), min_samples_per_topic=15):\n     import re\n     from ..preprocessing.text import Tokenizer\n \n@@ -74,8 +74,8 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n     dataset = (X, labels) \n     print('-')\n     print('Saving...')\n-    six.moves.cPickle.dump(dataset, open('datasets/data/reuters.pkl', 'w'))\n-    six.moves.cPickle.dump(tokenizer.word_index, open('datasets/data/reuters_word_index.pkl', 'w'))\n+    six.moves.cPickle.dump(dataset, open(os.path.join('datasets', 'data', 'reuters.pkl'), 'w'))\n+    six.moves.cPickle.dump(tokenizer.word_index, open(os.path.join('datasets','data', 'reuters_word_index.pkl'), 'w'))\n \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8dab51eeb80371087335fa581631808ca1b8022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 34 | Files Changed: 1 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 41 | Churn Cumulative: 206 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n+from ..regularizers import ident\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n \n@@ -12,34 +13,6 @@ from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams()\n \n-def l1(lam):\n-    def l1wrap(g,p):\n-        g += T.sgn(p) * lam\n-        return g\n-    return l1wrap\n-\n-def l2(lam):\n-    def l2wrap(g,p):\n-        g += p * lam\n-        return g\n-    return l2wrap\n-\n-def maxnorm(m):\n-    def maxnorm_wrap(p):\n-        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n-        desired = T.clip(norms, 0, m)\n-        p = p * (desired / (1e-7 + norms))\n-        return p\n-    return maxnorm_wrap\n-\n-def nonneg(p):\n-    p *= T.ge(p,0)\n-    return p\n-\n-def ident(g,*l):\n-    return g\n-\n-\n class Layer(object):\n     def connect(self, previous_layer):\n         self.previous_layer = previous_layer\n@@ -178,7 +151,7 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, regularizer=[ident, ident], constraint=[ident, ident]):\n+    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -190,8 +163,8 @@ class Dense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizer = regularizer\n-        self.constraint = constraint\n+        self.regularizer = [W_regularizer, b_regularizer]\n+        self.constraint = [W_constraint, b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -217,7 +190,7 @@ class TimeDistributedDense(Layer):\n        Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n \n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, regularizer=[ident, ident], constraint=[ident, ident]):\n+    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -229,8 +202,8 @@ class TimeDistributedDense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizer = regularizer\n-        self.constraint = constraint\n+        self.regularizer = [W_regularizer, b_regularizer]\n+        self.constraint = [W_constraint, b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#babc0b9dd873de40ba459027a27865c330b4f58b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 35 | Churn Cumulative: 35 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,16 @@\n+from __future__ import absolute_import\n+import theano\n+import theano.tensor as T\n+import numpy as np\n+\n+def maxnorm(m=2):\n+    def maxnorm_wrap(p):\n+        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n+        desired = T.clip(norms, 0, m)\n+        p = p * (desired / (1e-7 + norms))\n+        return p\n+    return maxnorm_wrap\n+\n+def nonneg(p):\n+    p *= T.ge(p,0)\n+    return p\n\\ No newline at end of file\n\n@@ -0,0 +1,19 @@\n+from __future__ import absolute_import\n+import theano\n+import theano.tensor as T\n+import numpy as np\n+\n+def l1(lam=.01):\n+    def l1wrap(g,p):\n+        g += T.sgn(p) * lam\n+        return g\n+    return l1wrap\n+\n+def l2(lam=.01):\n+    def l2wrap(g,p):\n+        g += p * lam\n+        return g\n+    return l2wrap\n+\n+def ident(g,*l):\n+    return g\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b91c1bc629a58f0f575126a7e228febcd1f77404", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -32,5 +32,4 @@ class PReLU(Layer):\n         return pos + neg\n \n     def get_config(self):\n-        return {\"name\":self.__class__.__name__,\n-            \"input_shape\":self.input_shape}\n+        return {\"name\":self.__class__.__name__}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba36814de79e20daee9776e08e9a7ded0f891191", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ class PReLU(Layer):\n     def __init__(self, input_shape):\n         self.alphas = shared_zeros(input_shape)\n         self.params = [self.alphas]\n+        self.input_shape = input_shape\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -32,4 +33,5 @@ class PReLU(Layer):\n         return pos + neg\n \n     def get_config(self):\n-        return {\"name\":self.__class__.__name__}\n+        return {\"name\":self.__class__.__name__,\n+        \"input_shape\":self.input_shape}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e172e12182868f70d39ff27b307663d6fe9422e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ def preprocess_labels(y, encoder=None, categorical=True):\n def make_submission(y_prob, ids, encoder, fname):\n     with open(fname, 'w') as f:\n         f.write('id,')\n-        f.write(','.join(encoder.classes_))\n+        f.write(','.join([str(i) for i in encoder.classes_]))\n         f.write('\\n')\n         for i, probs in zip(ids, y_prob):\n             probas = ','.join([i] + [str(p) for p in probs.tolist()])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8ddaffb0b9b971d50249c8f8e8879a09e840f57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,6 @@ setup(name='Keras',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n       license='MIT',\n+      install_requires=['theano', 'h5py'],\n       packages=find_packages(),\n-      # TODO: dependencies\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0cbed3f2d8bbc31cbb20508c5c7c8be4b1326ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 7 | Methods Changed: 12 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 14 | Churn Cumulative: 367 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ from ..layers.core import Layer\n \n class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col, \n-        init='uniform', activation='linear', weights=None, \n+        init='glorot_uniform', activation='linear', weights=None, \n         image_shape=None, border_mode='valid', subsample=(1,1)):\n \n         self.init = initializations.get(init)\n\n@@ -140,7 +140,7 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -176,7 +176,7 @@ class TimeDistributedDense(Layer):\n        Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n \n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n\n@@ -18,7 +18,7 @@ class SimpleRNN(Layer):\n         (demonstrates how to use theano.scan to build a basic RNN).\n     '''\n     def __init__(self, input_dim, output_dim, \n-        init='uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n+        init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n         truncate_gradient=-1, return_sequences=False):\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -89,7 +89,7 @@ class SimpleDeepRNN(Layer):\n         Also (probably) not a super useful model.\n     '''\n     def __init__(self, input_dim, output_dim, depth=3,\n-        init='uniform', inner_init='orthogonal', \n+        init='glorot_uniform', inner_init='orthogonal', \n         activation='sigmoid', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n         self.init = initializations.get(init)\n@@ -173,7 +173,7 @@ class GRU(Layer):\n                 http://arxiv.org/pdf/1412.3555v1.pdf\n     '''\n     def __init__(self, input_dim, output_dim=128, \n-        init='uniform', inner_init='orthogonal',\n+        init='glorot_uniform', inner_init='orthogonal',\n         activation='sigmoid', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n@@ -275,7 +275,7 @@ class LSTM(Layer):\n                 http://www.cs.toronto.edu/~graves/preprint.pdf\n     '''\n     def __init__(self, input_dim, output_dim=128, \n-        init='uniform', inner_init='orthogonal', \n+        init='glorot_uniform', inner_init='orthogonal', \n         activation='tanh', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c53c15dbd007480100920b692782e28f6f23c86f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 235 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -11,10 +11,8 @@ from keras.preprocessing.text import Tokenizer\n \n '''\n     Train and evaluate a simple MLP on the Reuters newswire topic classification task.\n-\n     GPU run command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python examples/reuters_mlp.py\n-\n     CPU run command:\n         python examples/reuters_mlp.py\n '''\n@@ -47,30 +45,16 @@ print(\"Building model...\")\n model = Sequential()\n model.add(Dense(max_words, 256, init='normal'))\n model.add(Activation('relu'))\n-model.add(BatchNormalization(input_shape=(256,))) # try without batch normalization (doesn't work as well!)\n model.add(Dropout(0.5))\n model.add(Dense(256, nb_classes, init='normal'))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n-# import cPickle\n-# model = cPickle.load(open('testsave.m.pkl'))\n-\n-for v in range(3):\n-    for sa in [True, False]:\n-        for vs in [0, 0.1]:\n-            print('='*40)\n-            print('v:%d, sa:%r, vs:%f' % (v, sa, vs))\n-            print(\"Training...\")\n-            model.fit(X_train, Y_train, nb_epoch=2, batch_size=batch_size, verbose=v, show_accuracy=sa, validation_split=vs)\n-            score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=v, show_accuracy=sa)\n+model.fit(X_train, Y_train, nb_epoch=4, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n+score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n print('Test score:', score)\n \n-            classes = model.predict_classes(X_test, batch_size=batch_size, verbose=v)\n+classes = model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n acc = np_utils.accuracy(classes, y_test)\n print('Test accuracy:', acc)\n-\n-# model.save('testsave.m')\n-\n-\n\n@@ -149,7 +149,7 @@ class Tokenizer(object):\n         if mode == \"tfidf\" and not self.document_count:\n             raise Exception(\"Fit the Tokenizer on some data before using tfidf mode\")\n \n-        X = np.zeros((len(sequences), nb_words+1))\n+        X = np.zeros((len(sequences), nb_words))\n         for i, seq in enumerate(sequences):\n             if not seq:\n                 pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb7cf7376b6c81f5d1792441f772de2771563c61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 171 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -53,8 +53,5 @@ model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n model.fit(X_train, Y_train, nb_epoch=4, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n-print('Test score:', score)\n-\n-classes = model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n-acc = np_utils.accuracy(classes, y_test)\n-print('Test accuracy:', acc)\n+print('Test score:', score[0])\n+print('Test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54691f9be052e5564ca0e5c6a503e641ea3142e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 24 | Churn Cumulative: 35 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,16 +2,22 @@ from ..layers.core import Layer\n from ..utils.theano_utils import shared_zeros\n from .. import initializations\n \n+import theano.tensor as T\n+\n class BatchNormalization(Layer):\n     '''\n         Reference: \n             Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n                 http://arxiv.org/pdf/1502.03167v3.pdf\n+\n+            mode: 0 -> featurewise normalization\n+                  1 -> samplewise normalization (may sometimes outperform featurewise mode)\n     '''\n-    def __init__(self, input_shape, epsilon=1e-6, weights=None):\n+    def __init__(self, input_shape, epsilon=1e-6, mode=0, weights=None):\n         self.init = initializations.get(\"uniform\")\n         self.input_shape = input_shape\n         self.epsilon = epsilon\n+        self.mode = mode\n \n         self.gamma = self.init((self.input_shape))\n         self.beta = shared_zeros(self.input_shape)\n@@ -22,11 +28,23 @@ class BatchNormalization(Layer):\n \n     def output(self, train):\n         X = self.get_input(train)\n-        X_normed = (X - X.mean(keepdims=True)) / (X.std(keepdims=True) + self.epsilon)\n+\n+        if self.mode == 0:\n+            m = X.mean(axis=0)\n+            # manual computation of std to prevent NaNs\n+            std = T.mean((X-m)**2 + self.epsilon, axis=0) ** 0.5\n+            X_normed = (X - m) / (std + self.epsilon)\n+\n+        elif self.mode == 1:\n+            m = X.mean(axis=-1, keepdims=True)\n+            std = X.std(axis=-1, keepdims=True)\n+            X_normed = (X - m) / (std + self.epsilon)\n+\n         out = self.gamma * X_normed + self.beta\n         return out\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n             \"input_shape\":self.input_shape,\n-            \"epsilon\":self.epsilon}\n\\ No newline at end of file\n+            \"epsilon\":self.epsilon,\n+            \"mode\":self.mode}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7da65f1d65fe363c945792d5ba978d5cc6c6a86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 39 | Files Changed: 7 | Hunks: 42 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 78 | Churn Cumulative: 720 | Contributors (this commit): 7 | Commits (past 90d): 49 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,8 +3,8 @@ from ..utils.theano_utils import shared_zeros\n \n class LeakyReLU(Layer):\n     def __init__(self, alpha=0.3):\n+        super(LeakyReLU,self).__init__()\n         self.alpha = alpha\n-        self.params = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -22,6 +22,7 @@ class PReLU(Layer):\n                 http://arxiv.org/pdf/1502.01852v1.pdf\n     '''\n     def __init__(self, input_shape):\n+        super(PReLU,self).__init__()\n         self.alphas = shared_zeros(input_shape)\n         self.params = [self.alphas]\n \n\n@@ -19,6 +19,7 @@ class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col, \n         init='uniform', activation='linear', weights=None, \n         image_shape=None, border_mode='valid', subsample=(1,1)):\n+        super(Convolution2D,self).__init__()\n \n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -63,10 +64,10 @@ class Convolution2D(Layer):\n \n class MaxPooling2D(Layer):\n     def __init__(self, poolsize=(2, 2), ignore_border=True):\n+        super(MaxPooling2D,self).__init__()\n         self.input = T.tensor4()\n         self.poolsize = poolsize\n         self.ignore_border = ignore_border\n-        self.params = []\n \n     def output(self, train):\n         X = self.get_input(train)\n\n@@ -14,6 +14,11 @@ from six.moves import zip\n srng = RandomStreams()\n \n class Layer(object):\n+    def __init__(self):\n+        self.params = []\n+        self.regularizer = []\n+        self.constraint = []\n+\n     def connect(self, previous_layer):\n         self.previous_layer = previous_layer\n \n@@ -45,10 +50,8 @@ class Dropout(Layer):\n         Hinton's dropout.\n     '''\n     def __init__(self, p):\n+        super(Dropout,self).__init__()\n         self.p = p\n-        self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -70,10 +73,8 @@ class Activation(Layer):\n         Apply an activation function to an output.\n     '''\n     def __init__(self, activation):\n+        super(Activation,self).__init__()\n         self.activation = activations.get(activation)\n-        self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -91,10 +92,8 @@ class Reshape(Layer):\n         First dimension is assumed to be nb_samples.\n     '''\n     def __init__(self, *dims):\n+        super(Reshape,self).__init__()\n         self.dims = dims\n-        self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -112,9 +111,7 @@ class Flatten(Layer):\n         First dimension is assumed to be nb_samples.\n     '''\n     def __init__(self):\n-        self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n+        super(Flatten,self).__init__()\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -131,10 +128,8 @@ class RepeatVector(Layer):\n         Return tensor of shape (nb_samples, n, dim).\n     '''\n     def __init__(self, n):\n+        super(RepeatVector,self).__init__()\n         self.n = n\n-        self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -152,6 +147,7 @@ class Dense(Layer):\n         Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n+        super(Dense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -191,6 +187,7 @@ class TimeDistributedDense(Layer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n+        super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n\n@@ -15,6 +15,7 @@ class Embedding(Layer):\n         @out_dim: size of dense representation\n     '''\n     def __init__(self, input_dim, output_dim, init='uniform', weights=None):\n+        super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n@@ -64,6 +65,7 @@ class WordContextProduct(Layer):\n     '''\n     def __init__(self, input_dim, proj_dim=128, \n         init='uniform', activation='sigmoid', weights=None):\n+        super(WordContextProduct,self).__init__()\n         self.input_dim = input_dim\n         self.proj_dim = proj_dim\n         self.init = initializations.get(init)\n\n@@ -9,6 +9,7 @@ class BatchNormalization(Layer):\n                 http://arxiv.org/pdf/1502.03167v3.pdf\n     '''\n     def __init__(self, input_shape, epsilon=1e-6, weights=None):\n+        super(BatchNormalization,self).__init__()\n         self.init = initializations.get(\"uniform\")\n         self.input_shape = input_shape\n         self.epsilon = epsilon\n\n@@ -20,6 +20,7 @@ class SimpleRNN(Layer):\n     def __init__(self, input_dim, output_dim, \n         init='uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n         truncate_gradient=-1, return_sequences=False):\n+        super(SimpleRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.input_dim = input_dim\n@@ -92,6 +93,7 @@ class SimpleDeepRNN(Layer):\n         init='uniform', inner_init='orthogonal', \n         activation='sigmoid', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n+        super(SimpleDeepRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.input_dim = input_dim\n@@ -177,6 +179,7 @@ class GRU(Layer):\n         activation='sigmoid', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n+        super(GRU,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n@@ -279,6 +282,7 @@ class LSTM(Layer):\n         activation='tanh', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n     \n+        super(LSTM,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n\n@@ -15,14 +15,18 @@ class Optimizer(object):\n     def get_updates(self, params, grads):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params):\n+    def get_gradients(self, cost, params, regularizers, constraints):\n         grads = T.grad(cost, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n             grads = [clip_norm(g, c, norm) for g in grads]\n \n-        return grads\n+        for p, g, r, c in zip(params, grads, regularizers, constraints):\n+            g = r(g,p)\n+            p = c(p)\n+\n+        return grads, params\n \n \n class SGD(Optimizer):\n@@ -32,12 +36,11 @@ class SGD(Optimizer):\n         self.iterations = shared_scalar(0)\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params)\n+        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n-        for p, g, r, c in zip(params, grads, regularizers, constraints):\n-            g = r(g,p)\n+        for p, g in zip(params, grads):\n             m = shared_zeros(p.get_value().shape) # momentum\n             v = self.momentum * m - lr * g # velocity\n             updates.append((m, v)) \n@@ -47,7 +50,6 @@ class SGD(Optimizer):\n             else:\n                 new_p = p + v\n \n-            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -58,17 +60,15 @@ class RMSprop(Optimizer):\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params)\n+        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, r, c in zip(params, grads, accumulators, regularizers, constraints):\n-            g = r(g,p)\n+        for p, g, a in zip(params, grads, accumulators):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -79,17 +79,15 @@ class Adagrad(Optimizer):\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params)\n+        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, r, c in zip(params, grads, accumulators, regularizers, constraints):\n-            g = r(g,p)\n+        for p, g, a in zip(params, grads, accumulators):\n             new_a = a + g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            new_p = c(new_p)\n             updates.append((p, new_p))\n         return updates\n \n@@ -102,13 +100,12 @@ class Adadelta(Optimizer):\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params)\n+        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, d_a, r, c in zip(params, grads, accumulators, delta_accumulators, regularizers, constraints):\n-            g = r(g,p)\n+        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n@@ -116,7 +113,6 @@ class Adadelta(Optimizer):\n             update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n-            new_p = c(new_p)\n             updates.append((p, new_p))\n \n             # update delta_accumulator\n@@ -138,7 +134,7 @@ class Adam(Optimizer):\n         self.iterations = shared_scalar(0)\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params)\n+        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n@@ -147,8 +143,7 @@ class Adam(Optimizer):\n         # the update below seems missing from the paper, but is obviously required\n         beta_2_t = self.beta_2 * (self.kappa**i) \n \n-        for p, g, r, c in zip(params, grads, regularizers, constraints):\n-            g = r(g,p)\n+        for p, g in zip(params, grads):\n             m = theano.shared(p.get_value() * 0.) # zero init of moment\n             v = theano.shared(p.get_value() * 0.) # zero init of velocity\n \n@@ -160,7 +155,6 @@ class Adam(Optimizer):\n \n             p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon)\n             \n-            p_t = c(p_t)\n             updates.append((m, m_t))\n             updates.append((v, v_t))\n             updates.append((p, p_t))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
