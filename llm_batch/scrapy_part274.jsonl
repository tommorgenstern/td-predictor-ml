{"custom_id": "scrapy#e43f37fff3cbdb5c4de7af21594c6062859a50e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 1566 | Contributors (this commit): 22 | Commits (past 90d): 14 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -69,8 +69,8 @@ class S3DownloadHandler(object):\n         self._download_http = httpdownloadhandler(crawler).download_request\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        return cls(crawler, *args, **kwargs)\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n@@ -250,7 +250,7 @@ class HttpTestCase(unittest.TestCase):\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls(get_crawler())\n+        self.download_handler = self.download_handler_cls.from_crawler(get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -490,7 +490,7 @@ class Http11TestCase(HttpTestCase):\n         return self.test_download_broken_content_allow_data_loss('broken-chunked')\n \n     def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n-        download_handler = self.download_handler_cls(\n+        download_handler = self.download_handler_cls.from_crawler(\n             get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n         )\n         request = Request(self.getURL(url))\n@@ -510,7 +510,7 @@ class Https11TestCase(Http11TestCase):\n \n     @defer.inlineCallbacks\n     def test_tls_logging(self):\n-        download_handler = self.download_handler_cls(\n+        download_handler = self.download_handler_cls.from_crawler(\n             get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n         )\n         try:\n@@ -579,7 +579,7 @@ class Https11CustomCiphers(unittest.TestCase):\n             0, self.wrapper, ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n             interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls(\n+        self.download_handler = self.download_handler_cls.from_crawler(\n             get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n         )\n         self.download_request = self.download_handler.download_request\n@@ -677,7 +677,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls(get_crawler())\n+        self.download_handler = self.download_handler_cls.from_crawler(get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -750,7 +750,7 @@ class S3AnonTestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        self.s3reqh = S3DownloadHandler(\n+        self.s3reqh = S3DownloadHandler.from_crawler(\n             crawler=get_crawler(),\n             httpdownloadhandler=HttpDownloadHandlerMock,\n             #anon=True, # is implicit\n@@ -779,10 +779,10 @@ class S3TestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        s3reqh = S3DownloadHandler(\n-            get_crawler(),\n-            self.AWS_ACCESS_KEY_ID,\n-            self.AWS_SECRET_ACCESS_KEY,\n+        s3reqh = S3DownloadHandler.from_crawler(\n+            crawler=get_crawler(),\n+            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n+            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n         )\n         self.download_request = s3reqh.download_request\n@@ -804,7 +804,7 @@ class S3TestCase(unittest.TestCase):\n \n     def test_extra_kw(self):\n         try:\n-            S3DownloadHandler(get_crawler(), extra_kw=True)\n+            S3DownloadHandler.from_crawler(get_crawler(), extra_kw=True)\n         except Exception as e:\n             self.assertIsInstance(e, (TypeError, NotConfigured))\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a9f5a0aefae83fc0b1dc161d117a265148ad1ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 31 | Churn Cumulative: 1482 | Contributors (this commit): 13 | Commits (past 90d): 21 | Contributors (cumulative): 20 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -7,10 +7,10 @@ See documentation in docs/topics/request-response.rst\n \n from contextlib import suppress\n from typing import Generator\n+from urllib.parse import urljoin\n \n import parsel\n import six\n-from six.moves.urllib.parse import urljoin\n from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n                             http_content_type_encoding, resolve_encoding)\n from w3lib.html import strip_html5_whitespace\n@@ -183,22 +183,25 @@ class TextResponse(Response):\n         In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction\n         within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted).\n \n-        Note that when using the ``css`` or ``xpath`` parameters, this method will not produce\n-        requests for selectors from which links cannot be obtained (for instance, anchor tags\n-        without an ``href`` attribute)\n+        Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or\n+        using the ``css`` or ``xpath`` parameters, this method will not produce requests for\n+        selectors from which links cannot be obtained (for instance, anchor tags without an\n+        ``href`` attribute)\n         \"\"\"\n         arg_count = len(list(filter(None, (urls, css, xpath))))\n         if arg_count != 1:\n             raise ValueError('Please supply exactly one of the following arguments: urls, css, xpath')\n         if not urls:\n             if css:\n-                selector_list = self.css(css)\n+                urls = self.css(css)\n             if xpath:\n-                selector_list = self.xpath(xpath)\n+                urls = self.xpath(xpath)\n+        if isinstance(urls, parsel.SelectorList):\n+            selectors = urls\n             urls = []\n-            for selector in selector_list:\n+            for sel in selectors:\n                 with suppress(_InvalidSelector):\n-                    urls.append(_url_from_selector(selector))\n+                    urls.append(_url_from_selector(sel))\n         return super(TextResponse, self).follow_all(\n             urls=urls,\n             callback=callback,\n\n@@ -579,8 +579,10 @@ class TextResponseTest(BaseResponseTest):\n             'http://example.com/page/4/',\n         ]\n         response = self._links_response_no_href()\n-        extracted = [r.url for r in response.follow_all(css='.pagination a')]\n-        self.assertEqual(expected, extracted)\n+        extracted1 = [r.url for r in response.follow_all(css='.pagination a')]\n+        self.assertEqual(expected, extracted1)\n+        extracted2 = [r.url for r in response.follow_all(response.css('.pagination a'))]\n+        self.assertEqual(expected, extracted2)\n \n     def test_follow_all_xpath(self):\n         expected = [\n@@ -598,8 +600,10 @@ class TextResponseTest(BaseResponseTest):\n             'http://example.com/page/4/',\n         ]\n         response = self._links_response_no_href()\n-        extracted = [r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')]\n-        self.assertEqual(expected, extracted)\n+        extracted1 = [r.url for r in response.follow_all(xpath='//div[@id=\"pagination\"]/a')]\n+        self.assertEqual(expected, extracted1)\n+        extracted2 = [r.url for r in response.follow_all(response.xpath('//div[@id=\"pagination\"]/a'))]\n+        self.assertEqual(expected, extracted2)\n \n     def test_follow_all_too_many_arguments(self):\n         response = self._links_response()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62778cf23f6e0cad5839f31f9da8b3c5778dcbdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 47 | Churn Cumulative: 1640 | Contributors (this commit): 26 | Commits (past 90d): 21 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,7 +32,6 @@ class Request(object_ref):\n             raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n         if errback is not None and not callable(errback):\n             raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n-        assert callback or not errback, \"Cannot use errback without a callback\"\n         self.callback = callback\n         self.errback = errback\n \n\n@@ -246,25 +246,41 @@ class RequestTest(unittest.TestCase):\n         self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n         self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n \n-    def test_callback_is_callable(self):\n+    def test_callback_and_errback(self):\n         def a_function():\n             pass\n-        r = self.request_class('http://example.com')\n-        self.assertIsNone(r.callback)\n-        r = self.request_class('http://example.com', a_function)\n-        self.assertIs(r.callback, a_function)\n-        with self.assertRaises(TypeError):\n-            self.request_class('http://example.com', 'a_function')\n \n-    def test_errback_is_callable(self):\n-        def a_function():\n-            pass\n-        r = self.request_class('http://example.com')\n-        self.assertIsNone(r.errback)\n-        r = self.request_class('http://example.com', a_function, errback=a_function)\n-        self.assertIs(r.errback, a_function)\n+        r1 = self.request_class('http://example.com')\n+        self.assertIsNone(r1.callback)\n+        self.assertIsNone(r1.errback)\n+\n+        r2 = self.request_class('http://example.com', callback=a_function)\n+        self.assertIs(r2.callback, a_function)\n+        self.assertIsNone(r2.errback)\n+\n+        r3 = self.request_class('http://example.com', errback=a_function)\n+        self.assertIsNone(r3.callback)\n+        self.assertIs(r3.errback, a_function)\n+\n+        r4 = self.request_class(\n+            url='http://example.com',\n+            callback=a_function,\n+            errback=a_function,\n+        )\n+        self.assertIs(r4.callback, a_function)\n+        self.assertIs(r4.errback, a_function)\n+\n+    def test_callback_and_errback_type(self):\n         with self.assertRaises(TypeError):\n-            self.request_class('http://example.com', a_function, errback='a_function')\n+            self.request_class('http://example.com', callback='a_function')\n+        with self.assertRaises(TypeError):\n+            self.request_class('http://example.com', errback='a_function')\n+        with self.assertRaises(TypeError):\n+            self.request_class(\n+                url='http://example.com',\n+                callback='a_function',\n+                errback='a_function',\n+            )\n \n     def test_from_curl(self):\n         # Note: more curated tests regarding curl conversion are in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d8d4bb7d7d998ae2324c4995bafaafdef752572", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 75 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,8 @@\n import base64\n+from urllib.request import _parse_proxy\n+\n from six.moves.urllib.parse import unquote, urlunparse\n from six.moves.urllib.request import getproxies, proxy_bypass\n-from urllib.request import _parse_proxy\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#702333478d072c3c043c64ec7ad3997befb87943", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 63 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,19 @@\n from email.utils import formatdate\n+\n from twisted.internet import defer\n-from twisted.internet.error import TimeoutError, DNSLookupError, \\\n-        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n-        ConnectionLost, TCPTimedOutError\n+from twisted.internet.error import (\n+    ConnectError,\n+    ConnectionDone,\n+    ConnectionLost,\n+    ConnectionRefusedError,\n+    DNSLookupError,\n+    TCPTimedOutError,\n+    TimeoutError,\n+)\n from twisted.web.client import ResponseFailed\n+\n from scrapy import signals\n-from scrapy.exceptions import NotConfigured, IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.utils.misc import load_object\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74627033c4a1701f3e197216f9f2801d497f5535", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 39 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import os\n-import six\n import logging\n from collections import defaultdict\n \n+import six\n+\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b35260625c3ffec9885265d9ac92771ade67ad9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 144 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from unittest import mock\n \n+from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n@@ -177,3 +178,31 @@ class ProcessExceptionInvalidOutput(ManagerTestCase):\n         dfd.addBoth(results.append)\n         self.assertIsInstance(results[0], Failure)\n         self.assertIsInstance(results[0].value, _InvalidOutput)\n+\n+\n+class MiddlewareUsingDeferreds(ManagerTestCase):\n+    \"\"\"Middlewares using Deferreds should work\"\"\"\n+\n+    def test_deferred(self):\n+        resp = Response('http://example.com/index.html')\n+\n+        class DeferredMiddleware:\n+            def cb(self, result):\n+                return result\n+\n+            def process_request(self, request, spider):\n+                d = Deferred()\n+                d.addCallback(self.cb)\n+                d.callback(resp)\n+                return d\n+\n+        self.mwman._add_middleware(DeferredMiddleware())\n+        req = Request('http://example.com/index.html')\n+        download_func = mock.MagicMock()\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n+        self.assertFalse(download_func.called)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b437bbe9fa0eb9736f35e510d486805706c783e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 22 | Churn Cumulative: 316 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,28 @@ import warnings\n warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n del warnings\n \n+# Install twisted asyncio loop\n+def _install_asyncio_reactor():\n+    global asyncio_supported\n+    try:\n+        import asyncio\n+        from twisted.internet import asyncioreactor\n+    except ImportError:\n+        pass\n+    else:\n+        from twisted.internet.error import ReactorAlreadyInstalledError\n+        try:\n+            asyncioreactor.install(asyncio.get_event_loop())\n+            asyncio_supported = True\n+        except ReactorAlreadyInstalledError:\n+            import twisted.internet.reactor\n+            if isinstance(twisted.internet.reactor,\n+                              asyncioreactor.AsyncioSelectorReactor):\n+                asyncio_supported = True\n+asyncio_supported = False\n+_install_asyncio_reactor()\n+del _install_asyncio_reactor\n+\n # Apply monkey patches to fix issues in external libraries\n from . import _monkeypatches\n del _monkeypatches\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9777639533373951652f3865a6321a4dff73246a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 267 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,9 @@ from subprocess import Popen, PIPE\n \n from OpenSSL import SSL\n from six.moves.urllib.parse import urlencode\n+\n+import scrapy  # needed before importing twisted.internet.reactor\n+\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b04b541372b219a02bb5fda2cc15cfd9fa1aac66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 27 | Files Changed: 5 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/5 | Churn Δ: 63 | Churn Cumulative: 1527 | Contributors (this commit): 27 | Commits (past 90d): 17 | Contributors (cumulative): 49 | DMM Complexity: 0.5\n\nDIFF:\n@@ -23,28 +23,6 @@ import warnings\n warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n del warnings\n \n-# Install twisted asyncio loop\n-def _install_asyncio_reactor():\n-    global asyncio_supported\n-    try:\n-        import asyncio\n-        from twisted.internet import asyncioreactor\n-    except ImportError:\n-        pass\n-    else:\n-        from twisted.internet.error import ReactorAlreadyInstalledError\n-        try:\n-            asyncioreactor.install(asyncio.get_event_loop())\n-            asyncio_supported = True\n-        except ReactorAlreadyInstalledError:\n-            import twisted.internet.reactor\n-            if isinstance(twisted.internet.reactor,\n-                              asyncioreactor.AsyncioSelectorReactor):\n-                asyncio_supported = True\n-asyncio_supported = False\n-_install_asyncio_reactor()\n-del _install_asyncio_reactor\n-\n # Apply monkey patches to fix issues in external libraries\n from . import _monkeypatches\n del _monkeypatches\n\n@@ -7,9 +7,9 @@ import inspect\n import pkg_resources\n \n import scrapy\n-from scrapy.crawler import CrawlerProcess\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n+from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_supported\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n@@ -121,6 +121,10 @@ def execute(argv=None, settings=None):\n             settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n \n+    # needs to be before _get_commands_dict() as that imports the command modules\n+    # which may import twisted.internet.reactor\n+    install_asyncio_reactor()\n+\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n@@ -142,6 +146,8 @@ def execute(argv=None, settings=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n+    # needs to be after install_asyncio_reactor() as it imports twisted.internet.reactor\n+    from scrapy.crawler import CrawlerProcess\n     cmd.crawler_process = CrawlerProcess(settings)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n\n@@ -0,0 +1,26 @@\n+#coding: utf-8\n+\n+\n+def install_asyncio_reactor():\n+    \"\"\" Tries to install AsyncioSelectorReactor\n+    \"\"\"\n+    try:\n+        import asyncio\n+        from twisted.internet import asyncioreactor\n+    except ImportError:\n+        pass\n+    else:\n+        from twisted.internet.error import ReactorAlreadyInstalledError\n+        try:\n+            asyncioreactor.install(asyncio.get_event_loop())\n+        except ReactorAlreadyInstalledError:\n+            pass\n+\n+\n+def is_asyncio_supported():\n+    try:\n+        import twisted.internet.reactor\n+        from twisted.internet import asyncioreactor\n+        return isinstance(twisted.internet.reactor, asyncioreactor.AsyncioSelectorReactor)\n+    except ImportError:\n+        return False\n\n@@ -8,8 +8,9 @@ import inspect\n from twisted.internet import defer, reactor, task\n from twisted.python import failure\n \n-from scrapy import asyncio_supported\n from scrapy.exceptions import IgnoreRequest\n+from scrapy.utils.asyncio import is_asyncio_supported\n+\n \n \n def defer_fail(_failure):\n@@ -131,7 +132,7 @@ def deferred_from_coro(o):\n     if isinstance(o, defer.Deferred):\n         return o\n     if asyncio.iscoroutine(o) or isfuture(o) or inspect.isawaitable(o):\n-        if not asyncio_supported:\n+        if not is_asyncio_supported():\n             raise TypeError('Using coroutines requires installing AsyncioSelectorReactor')\n         return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n\n@@ -7,8 +7,6 @@ from subprocess import Popen, PIPE\n from OpenSSL import SSL\n from six.moves.urllib.parse import urlencode\n \n-import scrapy  # needed before importing twisted.internet.reactor\n-\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fbe7d49dc084b7770cc4dc6bbe65eb380b5f498", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 399 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.asyncio import is_asyncio_supported\n from scrapy.utils.versions import scrapy_components_versions\n \n \n@@ -148,6 +149,8 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n+    if is_asyncio_supported():\n+        logger.debug(\"Asyncio support enabled\")\n \n \n class StreamLogger(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc19ab5439f20ba6995528542cc064ddab86273c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/2 | Churn Δ: 48 | Churn Cumulative: 1307 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,3 +27,7 @@ def pytest_collection_modifyitems(session, config, items):\n             items[:] = [item for item in items if isinstance(item, Flake8Item)]\n     except ImportError:\n         pass\n+\n+@pytest.fixture()\n+def reactor_pytest(request):\n+    request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n\n@@ -9,6 +9,7 @@ from tempfile import mkdtemp\n from contextlib import contextmanager\n from threading import Timer\n \n+from pytest import mark\n from twisted.trial import unittest\n \n import scrapy\n@@ -178,6 +179,7 @@ class MiscCommandsTest(CommandTest):\n         self.assertEqual(0, self.call('list'))\n \n \n+@mark.usefixtures('reactor_pytest')\n class RunSpiderCommandTest(CommandTest):\n \n     debug_log_spider = \"\"\"\n@@ -295,6 +297,11 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n+    def test_asyncio_supported(self):\n+        if self.reactor_pytest == 'asyncio':\n+            log = self.get_log(self.debug_log_spider)\n+            self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n+\n \n class BenchCommandTest(CommandTest):\n \n\n@@ -1,14 +1,16 @@\n import logging\n import warnings\n \n+from pytest import raises, mark\n+from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial import unittest\n-from pytest import raises\n \n import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n+from scrapy.utils.asyncio import is_asyncio_supported\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n@@ -203,6 +205,15 @@ class NoRequestsSpider(scrapy.Spider):\n         return []\n \n \n+class AsyncioSpider(scrapy.Spider):\n+    name = 'asyncio'\n+\n+    def start_requests(self):\n+        self.logger.info('Asyncio support: %s', is_asyncio_supported())\n+        return []\n+\n+\n+@mark.usefixtures('reactor_pytest')\n class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     @defer.inlineCallbacks\n@@ -245,3 +256,10 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         yield runner.crawl(NoRequestsSpider)\n \n         self.assertEqual(runner.bootstrap_failed, True)\n+\n+    @defer.inlineCallbacks\n+    def test_asyncio_supported(self):\n+        runner = CrawlerRunner()\n+        with LogCapture() as log:\n+            yield runner.crawl(AsyncioSpider)\n+            log.check_present(('asyncio', 'INFO', 'Asyncio support: %s' % (self.reactor_pytest == 'asyncio')))\n\n@@ -0,0 +1,17 @@\n+from unittest import TestCase\n+\n+from pytest import mark\n+\n+from scrapy.utils.asyncio import is_asyncio_supported, install_asyncio_reactor\n+\n+\n+@mark.usefixtures('reactor_pytest')\n+class AsyncioTest(TestCase):\n+\n+    def test_is_asyncio_supported(self):\n+        # the result should depend only on the pytest --reactor argument\n+        self.assertEquals(is_asyncio_supported(), self.reactor_pytest == 'asyncio')\n+\n+    def test_install_asyncio_reactor(self):\n+        # this should do nothing\n+        install_asyncio_reactor()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ba25ccbd3b456024b1d350645407557c81d73c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 229 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -131,7 +131,7 @@ def deferred_from_coro(o):\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, defer.Deferred):\n         return o\n-    if asyncio.iscoroutine(o) or isfuture(o) or inspect.isawaitable(o):\n+    if isfuture(o) or inspect.isawaitable(o):\n         if not is_asyncio_supported():\n             raise TypeError('Using coroutines requires installing AsyncioSelectorReactor')\n         return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#794cf71806a94ba68238a93e75c396f355159ab5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 698 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ import pkg_resources\n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.asyncio import install_asyncio_reactor, is_asyncio_supported\n+from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n\n@@ -1,6 +1,3 @@\n-#coding: utf-8\n-\n-\n def install_asyncio_reactor():\n     \"\"\" Tries to install AsyncioSelectorReactor\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed34ce14c0c06d4539d4fdeb0ad014f4e6fb5b94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 24 | Files Changed: 10 | Hunks: 25 | Methods Changed: 18 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 81 | Churn Cumulative: 3548 | Contributors (this commit): 69 | Commits (past 90d): 17 | Contributors (cumulative): 132 | DMM Complexity: 0.48148148148148145\n\nDIFF:\n@@ -9,7 +9,6 @@ import pkg_resources\n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n@@ -121,10 +120,6 @@ def execute(argv=None, settings=None):\n             settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n \n-    # needs to be before _get_commands_dict() as that imports the command modules\n-    # which may import twisted.internet.reactor\n-    install_asyncio_reactor()\n-\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n@@ -146,7 +141,8 @@ def execute(argv=None, settings=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n-    # needs to be after install_asyncio_reactor() as it imports twisted.internet.reactor\n+    # needs to be after cmd.process_options() as it imports twisted.internet.reactor\n+    # while commands may want to install the asyncio reactor\n     from scrapy.crawler import CrawlerProcess\n     cmd.crawler_process = CrawlerProcess(settings)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n\n@@ -1,5 +1,6 @@\n import os\n from scrapy.commands import ScrapyCommand\n+from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.python import without_none_values\n from scrapy.exceptions import UsageError\n@@ -26,6 +27,8 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n+        if self.settings.getbool('ASYNCIO_SUPPORT'):\n+            install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n\n@@ -2,6 +2,7 @@ import sys\n import os\n from importlib import import_module\n \n+from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -50,6 +51,8 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n+        if self.settings.getbool('ASYNCIO_SUPPORT'):\n+            install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n\n@@ -19,6 +19,8 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n+ASYNCIO_SUPPORT = False\n+\n AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n AUTOTHROTTLE_MAX_DELAY = 60.0\n\n@@ -14,7 +14,7 @@ def install_asyncio_reactor():\n             pass\n \n \n-def is_asyncio_supported():\n+def is_asyncio_reactor_installed():\n     try:\n         import twisted.internet.reactor\n         from twisted.internet import asyncioreactor\n\n@@ -9,8 +9,7 @@ from twisted.internet import defer, reactor, task\n from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.asyncio import is_asyncio_supported\n-\n+from scrapy.utils.asyncio import is_asyncio_reactor_installed\n \n \n def defer_fail(_failure):\n@@ -127,12 +126,18 @@ def isfuture(o):\n     return isinstance(o, asyncio.futures.Future)\n \n \n-def deferred_from_coro(o):\n+def deferred_from_coro(o, asyncio_enabled=False):\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, defer.Deferred):\n         return o\n     if isfuture(o) or inspect.isawaitable(o):\n-        if not is_asyncio_supported():\n+        if not asyncio_enabled:\n+            # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n+            # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n+            return defer.ensureDeferred(o)\n+        else:\n+            # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n+            if not is_asyncio_reactor_installed():\n                 raise TypeError('Using coroutines requires installing AsyncioSelectorReactor')\n             return defer.Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n\n@@ -11,7 +11,7 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncio import is_asyncio_supported\n+from scrapy.utils.asyncio import is_asyncio_reactor_installed\n from scrapy.utils.versions import scrapy_components_versions\n \n \n@@ -149,8 +149,13 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n-    if is_asyncio_supported():\n+    if settings.getbool('ASYNCIO_SUPPORT'):\n+        if is_asyncio_reactor_installed():\n             logger.debug(\"Asyncio support enabled\")\n+        else:\n+            logger.error(\"ASYNCIO_SUPPORT is on but the Twisted asyncio \"\n+                         \"reactor is not installed, this is not supported \"\n+                         \"and asyncio coroutines will not work.\")\n \n \n class StreamLogger(object):\n\n@@ -9,7 +9,6 @@ from tempfile import mkdtemp\n from contextlib import contextmanager\n from threading import Timer\n \n-from pytest import mark\n from twisted.trial import unittest\n \n import scrapy\n@@ -179,7 +178,6 @@ class MiscCommandsTest(CommandTest):\n         self.assertEqual(0, self.call('list'))\n \n \n-@mark.usefixtures('reactor_pytest')\n class RunSpiderCommandTest(CommandTest):\n \n     debug_log_spider = \"\"\"\n@@ -297,11 +295,14 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n-    def test_asyncio_supported(self):\n-        if self.reactor_pytest == 'asyncio':\n-            log = self.get_log(self.debug_log_spider)\n+    def test_asyncio_support_true(self):\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_SUPPORT=True'])\n         self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n \n+    def test_asyncio_support_false(self):\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_SUPPORT=False'])\n+        self.assertNotIn(\"DEBUG: Asyncio support enabled\", log)\n+\n \n class BenchCommandTest(CommandTest):\n \n\n@@ -10,7 +10,7 @@ import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n-from scrapy.utils.asyncio import is_asyncio_supported\n+from scrapy.utils.asyncio import is_asyncio_reactor_installed\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n@@ -209,7 +209,7 @@ class AsyncioSpider(scrapy.Spider):\n     name = 'asyncio'\n \n     def start_requests(self):\n-        self.logger.info('Asyncio support: %s', is_asyncio_supported())\n+        self.logger.info('Asyncio support: %s', is_asyncio_reactor_installed())\n         return []\n \n \n@@ -258,7 +258,25 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         self.assertEqual(runner.bootstrap_failed, True)\n \n     @defer.inlineCallbacks\n-    def test_asyncio_supported(self):\n+    def test_crawler_process_asyncio_supported_true(self):\n+        with LogCapture(level=logging.DEBUG) as log:\n+            runner = CrawlerProcess(settings={'ASYNCIO_SUPPORT': True})\n+            yield runner.crawl(NoRequestsSpider)\n+            if self.reactor_pytest == 'asyncio':\n+                self.assertIn(\"Asyncio support enabled\", str(log))\n+            else:\n+                self.assertNotIn(\"Asyncio support enabled\", str(log))\n+                self.assertIn(\"ASYNCIO_SUPPORT is on but the Twisted asyncio reactor is not installed\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_process_asyncio_supported_false(self):\n+        runner = CrawlerProcess(settings={'ASYNCIO_SUPPORT': False})\n+        with LogCapture(level=logging.DEBUG) as log:\n+            yield runner.crawl(NoRequestsSpider)\n+            self.assertNotIn(\"Asyncio support enabled\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_runner_asyncio_supported(self):\n         runner = CrawlerRunner()\n         with LogCapture() as log:\n             yield runner.crawl(AsyncioSpider)\n\n@@ -2,15 +2,15 @@ from unittest import TestCase\n \n from pytest import mark\n \n-from scrapy.utils.asyncio import is_asyncio_supported, install_asyncio_reactor\n+from scrapy.utils.asyncio import is_asyncio_reactor_installed, install_asyncio_reactor\n \n \n @mark.usefixtures('reactor_pytest')\n class AsyncioTest(TestCase):\n \n-    def test_is_asyncio_supported(self):\n+    def test_is_asyncio_reactor_installed(self):\n         # the result should depend only on the pytest --reactor argument\n-        self.assertEquals(is_asyncio_supported(), self.reactor_pytest == 'asyncio')\n+        self.assertEquals(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n         # this should do nothing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97fb61cec846641eb1c8e224ae24e55558746f4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 284 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,6 @@ See documentation in docs/topics/shell.rst\n from threading import Thread\n \n from scrapy.commands import ScrapyCommand\n-from scrapy.shell import Shell\n from scrapy.http import Request\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n from scrapy.utils.url import guess_scheme\n@@ -70,6 +69,8 @@ class Command(ScrapyCommand):\n \n         self._start_crawler_thread()\n \n+        # moved from the top-level because it imports twisted.internet.reactor\n+        from scrapy.shell import Shell\n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         shell.start(url=url, redirect=not opts.no_redirect)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b4b43f8ac3a39d5dd6137223699d3d9e7dacec2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 7 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 1925 | Contributors (this commit): 27 | Commits (past 90d): 26 | Contributors (cumulative): 60 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n del warnings\n \n # Apply monkey patches to fix issues in external libraries\n-from . import _monkeypatches\n+from scrapy import _monkeypatches\n del _monkeypatches\n \n from twisted import version as _txv\n\n@@ -4,7 +4,7 @@ from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.exceptions import ContractFail\n \n-from . import Contract\n+from scrapy.contracts import Contract\n \n \n # contracts\n\n@@ -10,8 +10,8 @@ from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n from scrapy import signals\n-from .middleware import DownloaderMiddlewareManager\n-from .handlers import DownloadHandlers\n+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.core.downloader.handlers import DownloadHandlers\n \n \n class Slot(object):\n\n@@ -1,2 +1,4 @@\n-from .http10 import HTTP10DownloadHandler\n-from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n+from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n+from scrapy.core.downloader.handlers.http11 import (\n+    HTTP11DownloadHandler as HTTPDownloadHandler,\n+)\n\n@@ -3,7 +3,7 @@ from six.moves.urllib.parse import unquote\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.boto import is_botocore\n-from .http import HTTPDownloadHandler\n+from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n \n \n def _get_boto_connection():\n\n@@ -118,4 +118,4 @@ class FilteringLinkExtractor(object):\n \n \n # Top-level imports\n-from .lxmlhtml import LxmlLinkExtractor as LinkExtractor  # noqa: F401\n+from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor  # noqa: F401\n\n@@ -4,7 +4,7 @@ from six.moves.urllib.parse import urljoin\n from w3lib.html import remove_tags, replace_entities, replace_escape_chars, get_base_url\n \n from scrapy.link import Link\n-from .sgml import SgmlLinkExtractor\n+from scrapy.linkextractors.sgml import SgmlLinkExtractor\n \n linkre = re.compile(\n         \"<a\\s.*?href=(\\\"[.#]+?\\\"|\\'[.#]+?\\'|[^\\s]+?)(>|\\s.*?>)(.*?)<[/ ]?a>\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02cdc53fb82e3cc5e51771180f1f79186b52670a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 44 | Churn Cumulative: 697 | Contributors (this commit): 14 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,12 +1,19 @@\n+from pathlib import Path\n+\n import pytest\n \n \n+def _py_files(folder):\n+    return (str(p) for p in Path(folder).rglob('*.py'))\n+\n+\n collect_ignore = [\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n+    # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n+    *_py_files(\"tests/CrawlerProcess\")\n ]\n \n-\n for line in open('tests/ignores.txt'):\n     file_path = line.strip()\n     if file_path and file_path[0] != '#':\n\n@@ -0,0 +1,15 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -1,4 +1,7 @@\n import logging\n+import os\n+import subprocess\n+import sys\n import warnings\n \n from twisted.internet import defer\n@@ -14,6 +17,7 @@ from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.extensions import telnet\n+from scrapy.utils.test import get_testenv\n \n \n class BaseCrawlerTest(unittest.TestCase):\n@@ -245,3 +249,19 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         yield runner.crawl(NoRequestsSpider)\n \n         self.assertEqual(runner.bootstrap_failed, True)\n+\n+\n+class CrawlerProcessSubprocess(unittest.TestCase):\n+    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerProcess')\n+\n+    def run_script(self, script_name):\n+        script_path = os.path.join(self.script_dir, script_name)\n+        args = (sys.executable, script_path)\n+        p = subprocess.Popen(args, env=get_testenv(),\n+                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+        stdout, stderr = p.communicate()\n+        return stderr.decode('utf-8')\n+\n+    def test_simple(self):\n+        log = self.run_script('simple.py')\n+        self.assertIn('Spider closed (finished)', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3560123090c1660fcfad7c5e7da08c9af503940f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 6 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 28 | Churn Cumulative: 2623 | Contributors (this commit): 59 | Commits (past 90d): 15 | Contributors (cumulative): 101 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n-        if self.settings.getbool('ASYNCIO_SUPPORT'):\n+        if self.settings.getbool('ASYNCIO_ENABLED'):\n             install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n\n@@ -51,7 +51,7 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n-        if self.settings.getbool('ASYNCIO_SUPPORT'):\n+        if self.settings.getbool('ASYNCIO_ENABLED'):\n             install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n\n@@ -19,7 +19,7 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n-ASYNCIO_SUPPORT = False\n+ASYNCIO_ENABLED = False\n \n AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n\n@@ -149,11 +149,11 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n-    if settings.getbool('ASYNCIO_SUPPORT'):\n+    if settings.getbool('ASYNCIO_ENABLED'):\n         if is_asyncio_reactor_installed():\n             logger.debug(\"Asyncio support enabled\")\n         else:\n-            logger.error(\"ASYNCIO_SUPPORT is on but the Twisted asyncio \"\n+            logger.error(\"ASYNCIO_ENABLED is on but the Twisted asyncio \"\n                          \"reactor is not installed, this is not supported \"\n                          \"and asyncio coroutines will not work.\")\n \n\n@@ -295,12 +295,12 @@ class BadSpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n-    def test_asyncio_support_true(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_SUPPORT=True'])\n+    def test_asyncio_enabled_true(self):\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_ENABLED=True'])\n         self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n \n-    def test_asyncio_support_false(self):\n-        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_SUPPORT=False'])\n+    def test_asyncio_enabled_false(self):\n+        log = self.get_log(self.debug_log_spider, args=['-s', 'ASYNCIO_ENABLED=False'])\n         self.assertNotIn(\"DEBUG: Asyncio support enabled\", log)\n \n \n\n@@ -262,19 +262,19 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         self.assertEqual(runner.bootstrap_failed, True)\n \n     @defer.inlineCallbacks\n-    def test_crawler_process_asyncio_supported_true(self):\n+    def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n-            runner = CrawlerProcess(settings={'ASYNCIO_SUPPORT': True})\n+            runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': True})\n             yield runner.crawl(NoRequestsSpider)\n             if self.reactor_pytest == 'asyncio':\n                 self.assertIn(\"Asyncio support enabled\", str(log))\n             else:\n                 self.assertNotIn(\"Asyncio support enabled\", str(log))\n-                self.assertIn(\"ASYNCIO_SUPPORT is on but the Twisted asyncio reactor is not installed\", str(log))\n+                self.assertIn(\"ASYNCIO_ENABLED is on but the Twisted asyncio reactor is not installed\", str(log))\n \n     @defer.inlineCallbacks\n-    def test_crawler_process_asyncio_supported_false(self):\n-        runner = CrawlerProcess(settings={'ASYNCIO_SUPPORT': False})\n+    def test_crawler_process_asyncio_enabled_false(self):\n+        runner = CrawlerProcess(settings={'ASYNCIO_ENABLED': False})\n         with LogCapture(level=logging.DEBUG) as log:\n             yield runner.crawl(NoRequestsSpider)\n             self.assertNotIn(\"Asyncio support enabled\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
