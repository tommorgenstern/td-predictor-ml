{"custom_id": "scrapy#a4b728f2b28d883239e05b5aaeafcfe1dd6aea9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 319 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.conf import settings\n class SiteInfo(object):\n     \"\"\"Object for holding data of the responses being scraped\"\"\"\n \n-    FAILURE_SIZE = 1024 # make failures equivalent to 1K responses in size\n+    MIN_RESPONSE_SIZE = 1024\n \n     def __init__(self, max_active_size=5000000):\n         self.max_active_size = max_active_size\n@@ -31,9 +31,9 @@ class SiteInfo(object):\n         deferred = defer.Deferred()\n         self.queue.append((response, request, deferred))\n         if isinstance(response, Response):\n-            self.active_size += len(response.body)\n+            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n         else:\n-            self.active_size += self.FAILURE_SIZE\n+            self.active_size += self.MIN_RESPONSE_SIZE\n         return deferred\n \n     def next_response_request_deferred(self):\n@@ -44,9 +44,9 @@ class SiteInfo(object):\n     def finish_response(self, response):\n         self.active.remove(response)\n         if isinstance(response, Response):\n-            self.active_size -= len(response.body)\n+            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n         else:\n-            self.active_size -= self.FAILURE_SIZE\n+            self.active_size -= self.MIN_RESPONSE_SIZE\n \n     def is_idle(self):\n         return not (self.queue or self.active)\n@@ -133,7 +133,6 @@ class Scraper(object):\n             _failure.value.__class__.__name__))\n \n     def handle_spider_output(self, result, request, response, spider):\n-        domain = spider.domain_name\n         if not result:\n             return defer_succeed(None)\n         dfd = parallel(iter(result), self.concurrent_items,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f19115a800ae17a673f2d872a1f5e0ce21d0e93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 125 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -118,9 +118,6 @@ ITEM_PROCESSOR = 'scrapy.item.pipeline.ItemPipelineManager'\n # Item pipelines are typically set in specific commands settings\n ITEM_PIPELINES = []\n \n-# max limit of items to process in parallel\n-ITEMPIPELINE_CONCURRENTLIMIT = 0\n-\n LOG_ENABLED = True\n LOG_STDOUT = False\n LOGLEVEL = 'DEBUG'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f31f75c0e280b9de0251bea44b648beffa50c76f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 95 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,8 +9,7 @@ __all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DateTimeField',\n \n \n class BaseField(object):\n-    def __init__(self, required=False, default=None):\n-        self.required = required\n+    def __init__(self, default=None):\n         self.default = default or self.to_python(None)\n \n     def assign(self, value):\n@@ -37,9 +36,9 @@ class Field(BaseField):\n \n \n class MultiValuedField(BaseField):\n-    def __init__(self, field_type, required=False, default=None):\n+    def __init__(self, field_type, default=None):\n         self._field = field_type()\n-        super(MultiValuedField, self).__init__(required, default)\n+        super(MultiValuedField, self).__init__(default)\n \n     def to_python(self, value):\n         if value is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60e7b80798f8ce4a09c1909db4c6e58877d79f68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 15 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 369 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -188,12 +188,12 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n-                signals.send_catch_log(signal=signals.item_dropped, \\\n-                    sender=self.__class__, item=item, spider=spider, exception=output.value)\n+                signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n+                    item=item, spider=spider, response=response, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n                     log.ERROR, domain=domain)\n         else:\n-            signals.send_catch_log(signal=signals.item_passed, \\\n-                sender=self.__class__, item=item, spider=spider, output=output)\n+            signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n+                item=item, spider=spider, response=response, output=output)\n \n\n@@ -9,57 +9,18 @@ from pydispatch import dispatcher\n \n from scrapy import log\n \n-# After the execution engine has started\n-# args: None\n engine_started = object()\n-\n-# After the execution engine has stopped\n-# args: None\n engine_stopped = object()\n-\n-# Before opening a new domain for crawling\n-# args: domain, spider\n domain_open = object()\n-\n-# After a domain has been opened for crawling\n-# args: domain, spider\n domain_opened = object()\n-\n-# When a domain has no remaining requests to process\n-# args: domain, spider\n domain_idle = object()\n-\n-# After a domain has been closed\n-# args: domain, spider, reason\n domain_closed = object()\n-\n-# New request received from spiders\n-# args: request, spider, response\n-# response is the response (fed to the spider) which generated the request\n request_received = object()\n-\n-# When request is sent in the downloader\n-# args: request, spider\n request_uploaded = object()\n-\n-# When new response is received (by the engine) from the downloader (middleware)\n-# args: response, spider\n response_received = object()\n-\n-# When response arrives from the downloader\n-# args: response, spider\n response_downloaded = object()\n-\n-# After item is returned from spiders\n-# args: item, spider, response\n item_scraped = object()\n-\n-# After item is processed by pipeline and pass all its stages\n-# args: item, spider, response, pipe_output\n item_passed = object()\n-\n-# After item is dropped by pipeline\n-# args: item, spider, response, exception\n item_dropped = object()\n \n def send_catch_log(signal, sender=None, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32c25f5a361031d6e287bc431909b2199a730daa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 13 | Methods Changed: 24 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 109 | Churn Cumulative: 212 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import decimal\n import unittest\n \n from scrapy.contrib_exp.newitem import *\n+from scrapy.contrib_exp.newitem.fields import BaseField\n \n \n class NewItemTest(unittest.TestCase):\n@@ -25,8 +26,54 @@ class NewItemTest(unittest.TestCase):\n         i.names = ['name1', 'name2']\n         assert i.names == ['name1', 'name2']\n \n+    def test_invalid_field(self):\n+        class TestItem(Item):\n+            pass\n+\n+        i = TestItem()\n+        def set_invalid_field():\n+            i.field = 'text'\n+\n+        self.assertRaises(AttributeError, set_invalid_field)\n+\n+        def get_invalid_field():\n+            return i.field\n+\n+        self.assertRaises(AttributeError, get_invalid_field)\n+\n+    def test_default_value(self):\n+        class TestItem(Item):\n+            name = StringField(default='John')\n+ \n+        i = TestItem()\n+        assert i.name == 'John'\n+\n+    def test_topython_iter(self):\n+        class TestItem(Item):\n+            name = StringField()\n+ \n+        i = TestItem()\n+        i.name = ('John', 'Doe')\n+        assert i.name == 'John Doe'\n+\n+    def test_repr(self):\n+        class TestItem(Item):\n+            name = StringField()\n+\n+        i = TestItem()\n+        i.name = 'John Doe'\n+        assert i.__repr__() == \"TestItem({'name': 'John Doe'})\"\n+\n \n class NewItemFieldsTest(unittest.TestCase):\n+    \n+    def test_base_field(self):\n+        f = BaseField()\n+\n+        assert f.default == None\n+        assert f.assign(1) == 1\n+        assert f.to_python(1) == 1\n+\n     def test_boolean_field(self):\n         class TestItem(Item):\n             field = BooleanField()\n@@ -35,11 +82,13 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i.field = True\n         assert i.field == True\n+    \n         i.field = 1\n         assert i.field == True\n \n         i.field = False\n         assert i.field == False\n+\n         i.field = 0\n         assert i.field == False\n \n@@ -49,13 +98,27 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        today = datetime.date.today()\n-        i.field = today\n-        assert i.field == today\n+        d_today = datetime.date.today()\n+        i.field = d_today\n+        assert i.field == d_today\n+\n+        dt_today = datetime.datetime.today()\n+        i.field = dt_today\n+        assert i.field == dt_today.date()\n  \n         i.field = '2009-05-21'\n         assert i.field == datetime.date(2009, 5, 21)\n \n+        def set_invalid_format():\n+            i.field = '21-05-2009'\n+\n+        self.assertRaises(ValueError, set_invalid_format)\n+\n+        def set_invalid_date():\n+            i.field = '2009-05-51'\n+\n+        self.assertRaises(ValueError, set_invalid_date)\n+\n     def test_datetime_field(self):\n         class TestItem(Item):\n             field = DateTimeField()\n@@ -71,6 +134,9 @@ class NewItemFieldsTest(unittest.TestCase):\n         assert i.field == datetime.datetime(d_today.year, d_today.month,\n                                             d_today.day)\n \n+        i.field = '2009-05-21 11:08:10.100'\n+        assert i.field == datetime.datetime(2009, 5, 21, 11, 8, 10, 100)\n+ \n         i.field = '2009-05-21 11:08:10'\n         assert i.field == datetime.datetime(2009, 5, 21, 11, 8, 10)\n \n@@ -80,6 +146,21 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = '2009-05-21'\n         assert i.field == datetime.datetime(2009, 5, 21)\n \n+        def set_invalid_usecs():\n+            i.field = '2009-05-21 11:08:10.usecs'\n+\n+        self.assertRaises(ValueError, set_invalid_usecs)\n+\n+        def set_invalid_format():\n+            i.field = '21-05-2009'\n+\n+        self.assertRaises(ValueError, set_invalid_format)\n+\n+        def set_invalid_date():\n+            i.field = '2009-05-51'\n+\n+        self.assertRaises(ValueError, set_invalid_date)\n+\n     def test_decimal_field(self):\n         class TestItem(Item):\n             field = DecimalField()\n@@ -92,6 +173,11 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = '3.14'\n         assert i.field == decimal.Decimal('3.14')\n         \n+        def set_invalid_value():\n+            i.field = 'text'\n+\n+        self.assertRaises(ValueError, set_invalid_value)\n+\n     def test_float_field(self):\n         class TestItem(Item):\n             field = FloatField()\n@@ -104,6 +190,11 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = '3.14'\n         assert i.field == 3.14\n         \n+        def set_invalid_value():\n+            i.field = 'text'\n+\n+        self.assertRaises(ValueError, set_invalid_value)\n+\n     def test_integer_field(self):\n         class TestItem(Item):\n             field = IntegerField()\n@@ -116,7 +207,12 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = '3'\n         assert i.field == 3\n \n-    def test_sting_field(self):\n+        def set_invalid_value():\n+            i.field = 'text'\n+\n+        self.assertRaises(ValueError, set_invalid_value)\n+\n+    def test_string_field(self):\n         class TestItem(Item):\n             field = StringField()\n \n@@ -125,3 +221,8 @@ class NewItemFieldsTest(unittest.TestCase):\n         i.field = 'hello'\n         assert i.field == 'hello'\n \n+        def set_invalid_value():\n+            i.field = 3 \n+\n+        self.assertRaises(ValueError, set_invalid_value)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8de5cef6e2d388046c33e0c0c14e4effeb663e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 45 | Files Changed: 42 | Hunks: 42 | Methods Changed: 3 | Complexity Δ (Sum/Max): 982/632 | Churn Δ: 87 | Churn Cumulative: 1244 | Contributors (this commit): 4 | Commits (past 90d): 149 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -11,9 +11,6 @@ if sys.version_info < (2,5):\n     print \"Scrapy %s requires Python 2.5 or above\" % __version__\n     sys.exit(1)\n \n-# add external python libraries bundled into scrapy\n-sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), \"xlib\"))\n-\n # monkey patches to fix external library issues\n from scrapy.patches import monkeypatches\n monkeypatches.apply_patches()\n\n@@ -143,7 +143,7 @@ def execute_with_args(argv):\n         p.runctx('ret = cmd.run(args, opts)', globals(), loc)\n         p.dump_stats(opts.profile)\n         try:\n-            import lsprofcalltree\n+            from scrapy.xlib import lsprofcalltree\n             fn = opts.profile + \".cachegrind\"\n             k = lsprofcalltree.KCacheGrind(p)\n             with open(fn, 'w') as f:\n\n@@ -6,7 +6,7 @@ time has expired.\n import datetime\n import pprint\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy import log\n\n@@ -3,7 +3,7 @@ from __future__ import with_statement\n import datetime\n import cPickle as pickle\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from twisted.spread import pb\n from twisted.internet import reactor\n \n\n@@ -1,6 +1,6 @@\n import datetime\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.spider import spiders \n from scrapy.management.web import banner, webconsole_discover_module\n\n@@ -5,7 +5,7 @@ configurable amount of idle time is reached\n \n from datetime import datetime\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from collections import defaultdict\n \n from scrapy.core import signals\n\n@@ -1,7 +1,7 @@\n import operator\n from itertools import groupby\n from collections import defaultdict\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.http import Response\n\n@@ -5,7 +5,7 @@ import os\n import hashlib\n import datetime\n import cPickle as pickle\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy import log\n\n@@ -7,7 +7,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n import urlparse\n import robotparser\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n\n@@ -25,7 +25,7 @@ from __future__ import with_statement\n \n import cPickle as pickle\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n\n@@ -9,7 +9,7 @@ import gc\n import socket\n \n import libxml2\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n\n@@ -9,7 +9,7 @@ import os\n import pprint\n import socket\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy import log\n\n@@ -1,4 +1,4 @@\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from twisted.internet import defer\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n\n@@ -4,7 +4,7 @@ ResponseSoup extension\n See documentation in docs/ref/extensions.rst\n \"\"\" \n \n-from BeautifulSoup import BeautifulSoup\n+from scrapy.xlib.BeautifulSoup import BeautifulSoup\n \n from scrapy.http import Response\n \n\n@@ -11,7 +11,7 @@ when needed.\n \n import datetime\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.extension import extensions\n from scrapy.core import signals\n\n@@ -4,7 +4,7 @@ Reload spider modules once they are finished scraping\n This is to release any resources held on to by scraping spiders.\n \"\"\"\n import sys\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n \n\n@@ -8,7 +8,7 @@ setting \"REQUESTS_QUEUE_SIZE\". If not specified (or 0), no limit will be\n applied. \n \"\"\"\n from itertools import imap\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n\n@@ -6,7 +6,7 @@ Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \n import pprint\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.stats import stats\n from scrapy.mail import MailSender\n\n@@ -10,7 +10,7 @@ from scrapy.item import ScrapedItem\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.utils.misc import memoize, arg_to_iter\n-from lrucache import LRUCache\n+from scrapy.xlib.lrucache import LRUCache\n \n from .site import WebSite, WebResource\n from .http import HttpResponse\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Scheduler information module for Scrapy webconsole\n \"\"\"\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import scrapyengine\n from scrapy.management.web import banner\n \n\n@@ -2,7 +2,7 @@\n Live statistics extension\n \"\"\"\n from datetime import datetime\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n from scrapy.management.web import banner, webconsole_discover_module\n\n@@ -4,7 +4,7 @@ Scheduler queue web console module\n See documentation in docs/ref/extensions.rst\n \"\"\"\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import scrapyengine\n from scrapy.management.web import banner, webconsole_discover_module\n \n\n@@ -2,7 +2,7 @@\n Extensions for allowing spider control from web console\n \"\"\"\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n\n@@ -4,7 +4,7 @@ Extension for persistent statistics by domain\n import re\n import pprint\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.spider import spiders\n from scrapy.core.exceptions import NotConfigured\n\n@@ -1,6 +1,6 @@\n import pprint\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.stats import stats\n from scrapy.management.web import banner, webconsole_discover_module\n\n@@ -1,7 +1,7 @@\n import hashlib\n from datetime import datetime\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.utils.misc import load_object\n from scrapy.core import signals\n\n@@ -8,7 +8,7 @@ http://pypi.python.org/pypi/shove\n from string import Template\n \n from shove import Shove\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import log\n from scrapy.core import signals\n\n@@ -5,7 +5,7 @@ crawler, reducing the dns lookups.\n \"\"\"\n import socket\n from scrapy.core import signals\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n class DNSCache(object):\n     \"\"\"\n\n@@ -9,7 +9,7 @@ from datetime import datetime\n from twisted.internet import reactor, task\n from twisted.internet.error import CannotListenError\n from twisted.python.failure import Failure\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import log\n from scrapy.conf import settings\n\n@@ -5,7 +5,7 @@ These signals are documented in docs/ref/signals.rst. Please don't add new\n signals here without documenting them there.\n \"\"\"\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import log\n \n\n@@ -8,7 +8,7 @@ See documentation in docs/ref/request-response.rst\n import urllib\n from cStringIO import StringIO\n \n-from ClientForm import ParseFile\n+from scrapy.xlib.ClientForm import ParseFile\n \n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n\n@@ -7,7 +7,7 @@ See documentation in docs/ref/request-response.rst\n \n import re\n \n-from BeautifulSoup import UnicodeDammit\n+from scrapy.xlib.BeautifulSoup import UnicodeDammit\n \n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod\n\n@@ -7,7 +7,7 @@ import sys\n from traceback import format_exc\n \n from twisted.python import log\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n\n@@ -9,7 +9,7 @@ import socket\n from datetime import datetime\n \n from twisted.web import server, resource\n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n\n@@ -6,7 +6,7 @@ import getpass\n import socket\n import datetime\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.stats import stats\n\n@@ -3,7 +3,7 @@ Scrapy extension for collecting scraping stats\n \"\"\"\n import pprint\n \n-from pydispatch import dispatcher\n+from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy import log\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from BeautifulSoup import BeautifulSoup\n+from scrapy.xlib.BeautifulSoup import BeautifulSoup\n \n from scrapy.http import Response\n from scrapy.contrib.response.soup import ResponseSoup\n\n@@ -58,7 +58,7 @@ class CrawlingSession(object):\n             from scrapy.core import signals\n             from scrapy.core.manager import scrapymanager\n             from scrapy.core.engine import scrapyengine\n-            from pydispatch import dispatcher\n+            from scrapy.xlib.pydispatch import dispatcher\n \n             dispatcher.connect(self.record_signal, signals.engine_started)\n             dispatcher.connect(self.record_signal, signals.engine_stopped)\n\n@@ -4,7 +4,7 @@ class SpidermonkeyTest(unittest.TestCase):\n \n     def setUp(self):\n         try:\n-            from spidermonkey import Runtime\n+            from scrapy.xlib.spidermonkey import Runtime\n             r = Runtime()\n             self.cx = r.new_context()\n         except ImportError:\n\n@@ -919,7 +919,7 @@ def _create_bs_classes(bs,\n try:\n     if sys.version_info[:2] < (2, 2):\n         raise ImportError  # BeautifulSoup uses generators\n-    import BeautifulSoup\n+    from scrapy.xlib import BeautifulSoup\n except ImportError:\n     pass\n else:\n\n@@ -27,7 +27,7 @@ Internal attributes:\n \"\"\"\n from __future__ import generators\n import types, weakref\n-from pydispatch import saferef, robustapply, errors\n+from scrapy.xlib.pydispatch import saferef, robustapply, errors\n \n __author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n __cvsid__ = \"$Id$\"\n\n@@ -1,6 +1,6 @@\n \"\"\"Module implementing error-catching version of send (sendRobust)\"\"\"\n-from pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers\n-from pydispatch.robustapply import robustApply\n+from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers\n+from scrapy.xlib.pydispatch.robustapply import robustApply\n \n def sendRobust(\n \tsignal=Any, \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eff8ea61730ec8c3f202fa8dfcf1fa16cb5345fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 335 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -157,7 +157,7 @@ class Scraper(object):\n                 item=output, spider=spider, response=response)\n             self.sites[domain].itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n-            dfd.addBoth(self._itemproc_finished, output, response, spider)\n+            dfd.addBoth(self._itemproc_finished, output, spider)\n             return dfd\n         elif output is None:\n             pass\n@@ -179,7 +179,7 @@ class Scraper(object):\n         else:\n             return spider_failure # exceptions raised in the spider code\n \n-    def _itemproc_finished(self, output, item, response, spider):\n+    def _itemproc_finished(self, output, item, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n         domain = spider.domain_name\n@@ -189,11 +189,11 @@ class Scraper(object):\n             if isinstance(ex, DropItem):\n                 log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n                 signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n-                    item=item, spider=spider, response=response, exception=output.value)\n+                    item=item, spider=spider, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n                     log.ERROR, domain=domain)\n         else:\n             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n-                item=item, spider=spider, response=response, output=output)\n+                item=item, spider=spider, output=output)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cd1fa9c32715d102a79e98eed9ad783dc8a1def", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -54,6 +54,14 @@ if root_dir != '':\n     os.chdir(root_dir)\n scrapy_dir = 'scrapy'\n \n+# Generate dropin.cache for spiders under tests\n+# if dropin.cache is missing, running tests under python site-packages fails with permission errors\n+import os\n+os.environ['SCRAPY_SETTINGS_DISABLED']='1'\n+from twisted.plugin import getPlugins, IPlugin\n+import scrapy.tests.test_spiders\n+list(getPlugins(IPlugin, scrapy.tests.test_spiders))\n+\n def is_not_module(filename):\n     return os.path.splitext(f)[1] not in ['.py', '.pyc', '.pyo']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3caf00d7a1c43c676a9af353eff51060c7e8044", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 63 | Files Changed: 4 | Hunks: 22 | Methods Changed: 17 | Complexity Δ (Sum/Max): 28/30 | Churn Δ: 84 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 8 | DMM Complexity: 0.5833333333333334\n\nDIFF:\n@@ -147,7 +147,7 @@ class ClusterMasterWeb(ClusterMaster):\n         if wc_request.args:\n             self.webconsole_control(wc_request)\n \n-        enabled_domains = set(spiders.asdict(include_disabled=False).keys())\n+        enabled_domains = set(spiders.asdict().keys())\n         print \"Enabled domains: %s\" % len(enabled_domains)\n         inactive_domains = enabled_domains - set(self.running.keys() + [p['domain'] for p in self.pending])\n \n\n@@ -36,7 +36,7 @@ class Spiderctl(object):\n         if wc_request.args:\n             changes = self.webconsole_control(wc_request)\n \n-        enabled_domains = spiders.asdict(include_disabled=False).keys()\n+        enabled_domains = spiders.asdict().keys()\n         self.scheduled = set(scrapyengine.scheduler.pending_domains_count)\n         self.not_scheduled = [d for d in enabled_domains if d not in self.scheduled\n                                                         and d not in self.running\n@@ -139,7 +139,7 @@ class Spiderctl(object):\n \n     def webconsole_control(self, wc_request):\n         args = wc_request.args\n-        enabled_domains = spiders.asdict(include_disabled=False).keys()\n+        enabled_domains = spiders.asdict().keys()\n         s = \"<hr />\\n\"\n         if \"reloadspiders\" in args:\n             scrapymanager.reload_spiders()\n\n@@ -96,10 +96,8 @@ class ExecutionManager(object):\n             signal.signal(signal.SIGBREAK, sig_handler_terminate)\n \n     def _parse_args(self, args):\n-        \"\"\" Parse crawl arguments and return a dict domains -> [requests] \"\"\"\n-        if not args:\n-            args = [p.domain_name for p in spiders.enabled]\n-\n+        \"\"\"Parse crawl arguments and return a dict of domains -> list of \n+        requests\"\"\"\n         requests, urls, sites = set(), set(), set()\n         for a in args:\n             if isinstance(a, Request):\n\n@@ -3,7 +3,6 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n import sys\n-import os\n import urlparse\n \n from twisted.plugin import getCache\n@@ -23,34 +22,24 @@ class SpiderManager(object):\n         self.spider_modules = settings.getlist('SPIDER_MODULES')\n         self.force_domain = None\n \n-    @property\n-    def all(self):\n-        self._load_on_demand()\n-        return self._alldict.values()\n+    def fromdomain(self, domain_name):\n+        return self.asdict().get(domain_name)\n \n-    @property\n-    def enabled(self):\n-        self._load_on_demand()\n-        return self._enableddict.values()\n-\n-    def fromdomain(self, domain_name, include_disabled=True): \n-        return self.asdict(include_disabled=include_disabled).get(domain_name)\n-\n-    def fromurl(self, url, include_disabled=True):\n+    def fromurl(self, url):\n         self._load_on_demand()\n         if self.force_domain:\n-            return self._alldict.get(self.force_domain)\n+            return self.asdict().get(self.force_domain)\n         domain = urlparse.urlparse(url).hostname\n         domain = str(domain).replace('www.', '')\n         if domain:\n-            if domain in self._alldict:         # try first locating by domain\n-                return self._alldict[domain]\n+            if domain in self.asdict():         # try first locating by domain\n+                return self.asdict()[domain]\n             else:                               # else search spider by spider\n-                plist = self.all if include_disabled else self.enabled\n+                plist = self.asdict().values()\n                 for p in plist:\n                     if url_is_from_spider(url, p):\n                         return p\n-        spider = self._alldict.get(self.default_domain)\n+        spider = self.asdict().get(self.default_domain)\n         if not spider:                          # create a custom spider\n             spiderclassname = settings.get('DEFAULT_SPIDER')\n             if spiderclassname:\n@@ -59,31 +48,17 @@ class SpiderManager(object):\n             \n         return spider\n \n-    def asdict(self, include_disabled=True):\n+    def asdict(self):\n         self._load_on_demand()\n-        return self._alldict if include_disabled else self._enableddict\n+        return self._spiders\n \n     def _load_on_demand(self):\n         if not self.loaded:\n             self.load()\n \n-    def _enabled_spiders(self):\n-        if settings['ENABLED_SPIDERS']:\n-            return set(settings['ENABLED_SPIDERS'])\n-        elif settings['ENABLED_SPIDERS_FILE']:\n-            if os.path.exists(settings['ENABLED_SPIDERS_FILE']):\n-                lines = open(settings['ENABLED_SPIDERS_FILE']).readlines()\n-                return set([l.strip() for l in lines if not l.strip().startswith('#')])\n-            else:\n-                return set()\n-        else:\n-            return set()\n-\n     def load(self):\n         self._invaliddict = {}\n-        self._alldict = {}\n-        self._enableddict = {}\n-        self._enabled_spiders_set = self._enabled_spiders()\n+        self._spiders = {}\n \n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n         for module in modules:\n@@ -94,37 +69,24 @@ class SpiderManager(object):\n     def add_spider(self, spider):\n         try:\n             ISpider.validateInvariants(spider)\n-            self._alldict[spider.domain_name] = spider\n-            if spider.domain_name in self._enabled_spiders_set:\n-                self._enableddict[spider.domain_name] = spider\n+            self._spiders[spider.domain_name] = spider\n         except Exception, e:\n             self._invaliddict[spider.domain_name] = spider\n             # we can't use the log module here because it may not be available yet\n             print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n \n     def reload(self, skip_domains=None):\n-        \"\"\"\n-        Reload all enabled spiders.\n-\n-        This discovers any spiders added under the spiders module/packages,\n-        removes any spiders removed, updates all enabled spiders code and\n-        updates list of enabled spiders from ENABLED_SPIDERS_FILE or\n-        ENABLED_SPIDERS setting.\n-\n-        Disabled spiders are intentionally excluded to avoid\n-        syntax/initialization errors. Currently running spiders are also\n-        excluded to avoid inconsistent behaviours.\n+        \"\"\"Reload spiders by trying to discover any spiders added under the\n+        spiders module/packages, removes any spiders removed.\n \n         If skip_domains is passed those spiders won't be reloaded.\n-\n         \"\"\"\n         skip_domains = set(skip_domains or [])\n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n         for m in modules:\n             reload(m)\n-        self.load()  # first call to update list of enabled spiders\n         reloaded = 0\n-        pdict = self.asdict(include_disabled=False)\n+        pdict = self.asdict()\n         for domain, spider in pdict.iteritems():\n             if not domain in skip_domains:\n                 reload(sys.modules[spider.__module__])\n@@ -133,12 +95,10 @@ class SpiderManager(object):\n         log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n \n     def _getspiders(self, interface, package):\n-        \"\"\"\n-        This is an override of twisted.plugin.getPlugin, because we're\n+        \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n         interested in catching exceptions thrown when loading spiders such as\n         KeyboardInterrupt\n         \"\"\"\n-\n         try:\n             allDropins = getCache(package)\n             for dropin in allDropins.itervalues():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc64360a343e9919f4763b853978d86a28f3144f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,8 +27,6 @@ class MemoryDebugger(object):\n         self.mail = MailSender()\n         self.rcpts = settings.getlist('MEMDEBUG_NOTIFY')\n \n-        self.domains_scraped = []\n-        dispatcher.connect(self.domain_opened, signals.domain_opened)\n         dispatcher.connect(self.engine_started, signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n \n@@ -70,6 +68,3 @@ class MemoryDebugger(object):\n         if self.rcpts:\n             self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % socket.gethostname(), report)\n         print report\n-\n-    def domain_opened(self, domain):\n-        self.domains_scraped.append(domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a153d47f3ef63f52b6bf4317efbcd08c0f47352", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 20/19 | Churn Δ: 17 | Churn Cumulative: 19 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -163,6 +163,20 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(x.x(\"//p:SecondTestTag\").x(\"./xmlns:price/text()\")[0].extract(), '90')\n         self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:material\").extract()[0], '<material/>')\n \n+    @libxml2debug\n+    def test_selector_invalid_xpath(self):\n+        response = XmlResponse(url=\"http://example.com\", body=\"<html></html>\")\n+        x = HtmlXPathSelector(response)\n+        xpath = \"//test[@foo='bar]\"\n+        try:\n+            x.x(xpath)\n+        except ValueError, e:\n+            assert xpath in str(e), \"Exception message does not contain invalid xpath\"\n+        except Exception:\n+            raise AssertionError(\"A invalid XPath does not raise ValueError\")\n+        else:\n+            raise AssertionError(\"A invalid XPath does not raise an exception\")\n+\n     @libxml2debug\n     def test_http_header_encoding_precedence(self):\n         # u'\\xa3'     = pound symbol in unicode\n\n@@ -45,7 +45,10 @@ class XPathSelector(object):\n         return a XPathSelectorList of the result\"\"\"\n         if hasattr(self.xmlNode, 'xpathEval'):\n             self.doc.xpathContext.setContextNode(self.xmlNode)\n+            try:\n                 xpath_result = self.doc.xpathContext.xpathEval(xpath)\n+            except libxml2.xpathError:\n+                raise ValueError(\"Invalid XPath: %s\" % xpath)\n             cls = type(self)\n             if hasattr(xpath_result, '__iter__'):\n                 return XPathSelectorList([cls(node=node, parent=self, expr=xpath, response=self.response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3fe0ef29775f3778efa24f79718e6cefdf1c3ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 88 | Files Changed: 5 | Hunks: 53 | Methods Changed: 25 | Complexity Δ (Sum/Max): 10/11 | Churn Δ: 171 | Churn Cumulative: 482 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,2 +1 @@\n from scrapy.contrib_exp.newitem.models import Item\n-from scrapy.contrib_exp.newitem.fields import *\n\n@@ -3,18 +3,10 @@ import decimal\n import re\n import time\n \n-\n-__all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DateTimeField',\n-           'DecimalField', 'FloatField', 'IntegerField', 'StringField']\n-\n-\n class BaseField(object):\n     def __init__(self, default=None):\n         self.default = default or self.to_python(None)\n \n-    def assign(self, value):\n-        return self.to_python(value)\n-\n     def to_python(self, value):\n         \"\"\"\n         Converts the input value into the expected Python data type.\n@@ -23,18 +15,6 @@ class BaseField(object):\n         return value\n \n \n-class Field(BaseField):\n-    def assign(self, value):\n-        if hasattr(value, '__iter__'):\n-            return self.to_python(self.deiter(value))\n-        else:\n-            return self.to_python(value)\n-\n-    def deiter(self, value):\n-        \"Converts the input iterable into a single value.\"\n-        return ' '.join(value)\n-\n-\n class MultiValuedField(BaseField):\n     def __init__(self, field_type, default=None):\n         self._field = field_type()\n@@ -47,15 +27,14 @@ class MultiValuedField(BaseField):\n             return [self._field.to_python(v) for v in value]\n \n \n-class BooleanField(Field):\n+class BooleanField(BaseField):\n     def to_python(self, value):\n         return bool(value)\n \n \n+class DateField(BaseField):\n     ansi_date_re = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n \n-\n-class DateField(Field):\n     def to_python(self, value):\n         if value is None:\n             return value\n@@ -64,7 +43,7 @@ class DateField(Field):\n         if isinstance(value, datetime.date):\n             return value\n \n-        if not ansi_date_re.search(value):\n+        if not self.ansi_date_re.search(value):\n             raise ValueError(\"Enter a valid date in YYYY-MM-DD format.\")\n \n         year, month, day = map(int, value.split('-'))\n@@ -74,7 +53,7 @@ class DateField(Field):\n             raise ValueError(\"Invalid date: %s\" % str(e))\n \n \n-class DateTimeField(Field):\n+class DateTimeField(BaseField):\n     def to_python(self, value):\n         if value is None:\n             return value\n@@ -111,41 +90,34 @@ class DateTimeField(Field):\n                     raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')\n \n \n-class DecimalField(Field):\n+class DecimalField(BaseField):\n     def to_python(self, value):\n-        if value is None:\n-            return value\n-        try:\n-            return decimal.Decimal(value)\n-        except decimal.InvalidOperation:\n-            raise ValueError(\"This value must be a decimal number.\")\n+        return decimal.Decimal(value) if value is not None else None\n \n \n-class FloatField(Field):\n+class FloatField(BaseField):\n     def to_python(self, value):\n-        if value is None:\n-            return value\n-        try:\n-            return float(value)\n-        except (TypeError, ValueError):\n-            raise ValueError(\"This value must be a float.\")\n+        return float(value) if value is not None else None\n \n \n-class IntegerField(Field):\n+class IntegerField(BaseField):\n     def to_python(self, value):\n-        if value is None:\n-            return value\n-        try:\n-            return int(value)\n-        except (TypeError, ValueError):\n-            raise ValueError(\"This value must be an integer.\")\n+        return int(value) if value is not None else None\n \n \n-class StringField(Field):\n+class StringField(BaseField):\n     def to_python(self, value):\n-        if isinstance(value, basestring):\n+        if hasattr(value, '__iter__'):\n+            return self.to_python(self.to_single(value))\n+        elif isinstance(value, basestring):\n             return value\n-        if value is None:\n+        elif value is None:\n             return value\n-        raise ValueError(\"This field must be a string.\")\n+        else:\n+            raise ValueError(\"StringField expects a basestring, got %s\" \\\n+                % type(value).__name__)\n+\n+    def to_single(self, value):\n+        \"Converts the input iterable into a single value.\"\n+        return ' '.join(value)\n \n\n@@ -2,40 +2,45 @@ from scrapy.item import ScrapedItem\n from scrapy.contrib_exp.newitem.fields import BaseField\n \n \n-class ItemMeta(type):\n+class _ItemMeta(type):\n \n     def __new__(meta, class_name, bases, attrs):\n         cls = type.__new__(meta, class_name, bases, attrs)\n         cls.fields = cls.fields.copy()\n-        for n, v in attrs.items():\n+        for n, v in attrs.iteritems():\n             if isinstance(v, BaseField):\n                 cls.fields[n] = v\n-\n         return cls\n \n \n class Item(ScrapedItem):\n     \"\"\" This is the base class for all scraped items. \"\"\"\n \n-    __metaclass__ = ItemMeta\n+    __metaclass__ = _ItemMeta\n \n     fields = {}\n \n-    def __init__(self):\n+    def __init__(self, values=None):\n         self._values = {}\n+        if isinstance(values, dict):\n+            for k, v in values.iteritems():\n+                setattr(self, k, v)\n+        elif values is not None:\n+            raise TypeError(\"Items must be instantiated with dicts, got %s\" % \\\n+                type(values).__name__)\n \n     def __setattr__(self, name, value):\n         if name.startswith('_'):\n-            return object.__setattr__(self, name, value)\n+            return ScrapedItem.__setattr__(self, name, value)\n \n         if name in self.fields.keys():\n-            self._values[name] = self.fields[name].assign(value)\n+            self._values[name] = self.fields[name].to_python(value)\n         else:\n             raise AttributeError(name)\n \n     def __getattribute__(self, name):\n         if name.startswith('_') or name == 'fields':\n-            return object.__getattribute__(self, name)\n+            return ScrapedItem.__getattribute__(self, name)\n         \n         if name in self.fields.keys():\n             try:\n@@ -46,9 +51,8 @@ class Item(ScrapedItem):\n             raise AttributeError(name)\n \n     def __repr__(self):\n+        \"\"\"Generate a representation of this item that can be used to\n+        reconstruct the item by evaluating it\n         \"\"\"\n-        Generate the following format so that items can be deserialized\n-        easily: ClassName({'attrib': value, ...})\n-        \"\"\"\n-        reprdict = dict((field, getattr(self, field)) for field in self.fields)\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n+        values = dict((field, getattr(self, field)) for field in self.fields)\n+        return \"%s(%s)\" % (self.__class__.__name__, repr(values))\n\n@@ -1,16 +1,16 @@\n import unittest\n import string\n from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor\n-from scrapy.contrib_exp.newitem import * \n+from scrapy.contrib_exp.newitem import Item, fields\n \n \n class BaseItem(Item):\n-    name = StringField()\n+    name = fields.StringField()\n \n \n class TestItem(BaseItem):\n-    url = StringField()\n-    summary = StringField()\n+    url = fields.StringField()\n+    summary = fields.StringField()\n \n \n class BaseAdaptor(ItemAdaptor):\n@@ -30,7 +30,7 @@ class InheritDefaultAdaptor(DefaultedAdaptor):\n \n \n class MultiValuedTestItem(Item):\n-    names = MultiValuedField(StringField)\n+    names = fields.MultiValuedField(fields.StringField)\n \n \n class MultiValuedItemAdaptor(ItemAdaptor):\n\n@@ -2,7 +2,7 @@ import datetime\n import decimal\n import unittest\n \n-from scrapy.contrib_exp.newitem import *\n+from scrapy.contrib_exp.newitem import Item, fields\n from scrapy.contrib_exp.newitem.fields import BaseField\n \n \n@@ -10,16 +10,30 @@ class NewItemTest(unittest.TestCase):\n \n     def test_simple(self):\n         class TestItem(Item):\n-            name = StringField()\n+            name = fields.StringField()\n \n         i = TestItem()\n         i.name = 'name'\n         assert i.name == 'name'\n \n+    def test_init(self):\n+        class TestItem(Item):\n+            name = fields.StringField()\n+        \n+        i = TestItem()\n+        assert i.name is None\n+\n+        i2 = TestItem({'name': 'john doe'})\n+        assert i2.name == 'john doe'\n+\n+        self.assertRaises(TypeError, TestItem, name='john doe')\n+\n+        self.assertRaises(AttributeError, TestItem, {'name': 'john doe', 'other': 'foo'})\n+\n     def test_multi(self):\n         class TestMultiItem(Item):\n-            name = StringField()\n-            names = MultiValuedField(StringField)\n+            name = fields.StringField()\n+            names = fields.MultiValuedField(fields.StringField)\n \n         i = TestMultiItem()\n         i.name = 'name'\n@@ -43,14 +57,14 @@ class NewItemTest(unittest.TestCase):\n \n     def test_default_value(self):\n         class TestItem(Item):\n-            name = StringField(default='John')\n+            name = fields.StringField(default='John')\n  \n         i = TestItem()\n         assert i.name == 'John'\n \n-    def test_topython_iter(self):\n+    def test_to_python_iter(self):\n         class TestItem(Item):\n-            name = StringField()\n+            name = fields.StringField()\n  \n         i = TestItem()\n         i.name = ('John', 'Doe')\n@@ -58,25 +72,31 @@ class NewItemTest(unittest.TestCase):\n \n     def test_repr(self):\n         class TestItem(Item):\n-            name = StringField()\n+            name = fields.StringField()\n+            number = fields.IntegerField()\n \n         i = TestItem()\n         i.name = 'John Doe'\n-        assert i.__repr__() == \"TestItem({'name': 'John Doe'})\"\n+        i.number = '123'\n+        itemrepr = repr(i)\n+        assert itemrepr == \"TestItem({'name': 'John Doe', 'number': 123})\"\n+\n+        i2 = eval(itemrepr)\n+        assert i2.name == 'John Doe'\n+        assert i2.number == 123\n \n \n class NewItemFieldsTest(unittest.TestCase):\n     \n     def test_base_field(self):\n-        f = BaseField()\n+        f = fields.BaseField()\n \n         assert f.default == None\n-        assert f.assign(1) == 1\n         assert f.to_python(1) == 1\n \n     def test_boolean_field(self):\n         class TestItem(Item):\n-            field = BooleanField()\n+            field = fields.BooleanField()\n \n         i = TestItem()\n \n@@ -94,7 +114,7 @@ class NewItemFieldsTest(unittest.TestCase):\n \n     def test_date_field(self):\n         class TestItem(Item):\n-            field = DateField()\n+            field = fields.DateField()\n \n         i = TestItem()\n \n@@ -121,7 +141,7 @@ class NewItemFieldsTest(unittest.TestCase):\n \n     def test_datetime_field(self):\n         class TestItem(Item):\n-            field = DateTimeField()\n+            field = fields.DateTimeField()\n \n         i = TestItem()\n \n@@ -163,7 +183,7 @@ class NewItemFieldsTest(unittest.TestCase):\n \n     def test_decimal_field(self):\n         class TestItem(Item):\n-            field = DecimalField()\n+            field = fields.DecimalField()\n \n         i = TestItem()\n \n@@ -176,11 +196,11 @@ class NewItemFieldsTest(unittest.TestCase):\n         def set_invalid_value():\n             i.field = 'text'\n \n-        self.assertRaises(ValueError, set_invalid_value)\n+        self.assertRaises(decimal.InvalidOperation, set_invalid_value)\n \n     def test_float_field(self):\n         class TestItem(Item):\n-            field = FloatField()\n+            field = fields.FloatField()\n \n         i = TestItem()\n \n@@ -197,7 +217,7 @@ class NewItemFieldsTest(unittest.TestCase):\n \n     def test_integer_field(self):\n         class TestItem(Item):\n-            field = IntegerField()\n+            field = fields.IntegerField()\n \n         i = TestItem()\n \n@@ -214,7 +234,7 @@ class NewItemFieldsTest(unittest.TestCase):\n \n     def test_string_field(self):\n         class TestItem(Item):\n-            field = StringField()\n+            field = fields.StringField()\n \n         i = TestItem()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
