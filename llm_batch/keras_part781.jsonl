{"custom_id": "keras#a44c05155d5f694876d2a1755faf30f54f921cca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 222 | Lines Deleted: 112 | Files Changed: 45 | Hunks: 136 | Methods Changed: 27 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 334 | Churn Cumulative: 29040 | Contributors (this commit): 39 | Commits (past 90d): 153 | Contributors (cumulative): 174 | DMM Complexity: 0.022222222222222223\n\nDIFF:\n@@ -12,7 +12,6 @@ python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n ```\n \"\"\"\n \n-\n from absl import app\n from absl import flags\n \n\n@@ -12,7 +12,6 @@ python3 -m benchmarks.layer_benchmark.pooling_benchmark \\\n ```\n \"\"\"\n \n-\n from absl import app\n from absl import flags\n \n\n@@ -4,6 +4,7 @@ In this file we use a convolution model. Training loop is written in the\n vanilla torch way, and we compare the performance between building model with\n Keras and torch.\n \"\"\"\n+\n import numpy as np\n import torch\n import torch.nn as nn\n\n@@ -4,6 +4,7 @@ In this file we use a model with 3 dense layers. Training loop is written in the\n vanilla torch way, and we compare the performance between building model with\n Keras and torch.\n \"\"\"\n+\n import numpy as np\n import torch\n import torch.nn as nn\n\n@@ -6,6 +6,7 @@ Last modified: 2023/07/11\n Description: Guide to multi-GPU/TPU training for Keras models with JAX.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Introduction\n \n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/29\n Description: Guide to multi-GPU training for Keras models with TensorFlow.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Introduction\n \n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/29\n Description: Guide to multi-GPU training for Keras models with PyTorch.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Introduction\n \n\n@@ -6,6 +6,7 @@ Last modified: 2020/04/12\n Description: Complete guide to the functional API.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Complete guide to writing `Layer` and `Model` objects from scratch.\n Accelerator: None\n \"\"\"\n+\n \"\"\"\n ## Introduction\n \n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Complete guide to the Sequential model.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Setup\n \n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Complete guide to transfer learning & fine-tuning in Keras.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Complete guide to using mask-aware sequence layers in Keras.\n Accelerator: None\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Writing low-level training & evaluation loops in JAX.\n Accelerator: None\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Writing low-level training & evaluation loops in TensorFlow.\n Accelerator: None\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Writing low-level training & evaluation loops in PyTorch.\n Accelerator: None\n \"\"\"\n+\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -6,6 +6,7 @@ Last modified: 2023/06/25\n Description: Complete guide to writing new Keras callbacks.\n Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Introduction\n \n\n@@ -5,6 +5,7 @@ Distribution related class for JAX backend.\n This is just a prototype and we might want to unify it\n with other backends in the future.\n \"\"\"\n+\n import jax\n import numpy as np\n \n\n@@ -16,9 +16,9 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     valid_segment_ids = segment_ids[valid_indices]\n \n     data_shape = list(valid_data.shape)\n-    data_shape[\n-        0\n-    ] = num_segments  # Replace first dimension (which corresponds to segments)\n+    data_shape[0] = (\n+        num_segments  # Replace first dimension (which corresponds to segments)\n+    )\n \n     if sorted:\n         result = np.zeros(data_shape, dtype=valid_data.dtype)\n@@ -43,9 +43,9 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n     valid_segment_ids = segment_ids[valid_indices]\n \n     data_shape = list(valid_data.shape)\n-    data_shape[\n-        0\n-    ] = num_segments  # Replace first dimension (which corresponds to segments)\n+    data_shape[0] = (\n+        num_segments  # Replace first dimension (which corresponds to segments)\n+    )\n \n     if sorted:\n         result = np.zeros(data_shape, dtype=valid_data.dtype)\n\n@@ -5,6 +5,7 @@ Distribution related class for Tensorflow backend.\n This is just a prototype and we might want to unify it\n with other backends in the future.\n \"\"\"\n+\n import tensorflow as tf\n from tensorflow.experimental import dtensor\n \n\n@@ -576,9 +576,11 @@ def concatenate(xs, axis=0):\n             return tf.sparse.concat(axis=axis, sp_inputs=xs)\n         else:\n             xs = [\n+                (\n                     convert_to_tensor(x, sparse=False)\n                     if isinstance(x, tf.SparseTensor)\n                     else x\n+                )\n                 for x in xs\n             ]\n     xs = tf.nest.map_structure(convert_to_tensor, xs)\n\n@@ -244,9 +244,11 @@ def rnn(\n             for i, inp in enumerate(flattened_inputs)\n         )\n         input_ta = tuple(\n+            (\n                 ta.unstack(input_)\n                 if not go_backwards\n                 else ta.unstack(tf.reverse(input_, [0]))\n+            )\n             for ta, input_ in zip(input_ta, flattened_inputs)\n         )\n \n\n@@ -167,9 +167,11 @@ def rnn(\n         # flattened tensor.\n \n         input_ta = tuple(\n+            (\n                 list(torch.unbind(input_))\n                 if not go_backwards\n                 else list(torch.unbind(torch.flip(input_, [0])))\n+            )\n             for input_ in flattened_inputs\n         )\n \n\n@@ -1,4 +1,5 @@\n \"\"\"Tests for inference-only model/layer exporting utilities.\"\"\"\n+\n import os\n \n import numpy as np\n\n@@ -1,6 +1,5 @@\n \"\"\"Keras abstract base layer for separable convolution.\"\"\"\n \n-\n from keras import activations\n from keras import constraints\n from keras import initializers\n\n@@ -206,9 +206,9 @@ def np_conv3d(\n             (*new_kenel_size_tuple, ch_in, ch_out),\n             dtype=kernel_weights.dtype,\n         )\n-        new_kernel_weights[\n-            ::h_dilation, ::w_dilation, ::d_dilation\n-        ] = kernel_weights\n+        new_kernel_weights[::h_dilation, ::w_dilation, ::d_dilation] = (\n+            kernel_weights\n+        )\n         kernel_weights = new_kernel_weights\n         h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n \n\n@@ -238,9 +238,9 @@ def np_conv3d_transpose(\n             (*new_kenel_size_tuple, ch_out, ch_in),\n             dtype=kernel_weights.dtype,\n         )\n-        new_kernel_weights[\n-            ::h_dilation, ::w_dilation, ::d_dilation\n-        ] = kernel_weights\n+        new_kernel_weights[::h_dilation, ::w_dilation, ::d_dilation] = (\n+            kernel_weights\n+        )\n         kernel_weights = new_kernel_weights\n         h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n \n\n@@ -254,9 +254,9 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             },\n             input_shape=input_shape,\n             expected_output_shape=expected_output_shape,\n-            expected_num_trainable_weights=2\n-            if expected_bias_shape is not None\n-            else 1,\n+            expected_num_trainable_weights=(\n+                2 if expected_bias_shape is not None else 1\n+            ),\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n\n@@ -217,11 +217,11 @@ class Lambda(Layer):\n                 )\n                 config[\"output_shape\"] = fn\n             else:\n-                config[\n-                    \"output_shape\"\n-                ] = serialization_lib.deserialize_keras_object(\n+                config[\"output_shape\"] = (\n+                    serialization_lib.deserialize_keras_object(\n                         fn_config, custom_objects=custom_objects\n                     )\n+                )\n         if \"arguments\" in config:\n             config[\"arguments\"] = serialization_lib.deserialize_keras_object(\n                 config[\"arguments\"], custom_objects=custom_objects\n\n@@ -15,6 +15,7 @@ And some more magic:\n - RNG seed tracking\n - activity regularization\n \"\"\"\n+\n import collections\n import inspect\n import warnings\n\n@@ -132,9 +132,9 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         self.assertNotAllClose(unmasked_out, masked_out)\n \n     @parameterized.product(\n-        synchronized=(False, True)\n-        if backend.backend == \"tensorflow\"\n-        else (False,),\n+        synchronized=(\n+            (False, True) if backend.backend == \"tensorflow\" else (False,)\n+        ),\n     )\n     def test_input_fully_masked(self, synchronized):\n         norm = layers.BatchNormalization(\n\n@@ -103,12 +103,16 @@ class Cropping2D(Layer):\n             return (\n                 input_shape[0],\n                 input_shape[1],\n+                (\n                     input_shape[2] - self.cropping[0][0] - self.cropping[0][1]\n                     if input_shape[2] is not None\n-                else None,\n+                    else None\n+                ),\n+                (\n                     input_shape[3] - self.cropping[1][0] - self.cropping[1][1]\n                     if input_shape[3] is not None\n-                else None,\n+                    else None\n+                ),\n             )\n         else:\n             if (\n@@ -125,12 +129,16 @@ class Cropping2D(Layer):\n                 )\n             return (\n                 input_shape[0],\n+                (\n                     input_shape[1] - self.cropping[0][0] - self.cropping[0][1]\n                     if input_shape[1] is not None\n-                else None,\n+                    else None\n+                ),\n+                (\n                     input_shape[2] - self.cropping[1][0] - self.cropping[1][1]\n                     if input_shape[2] is not None\n-                else None,\n+                    else None\n+                ),\n                 input_shape[3],\n             )\n \n\n@@ -46,9 +46,11 @@ class FlattenTest(testing.TestCase, parameterized.TestCase):\n             init_kwargs={},\n             input_data=inputs,\n             input_sparse=True,\n-            expected_output=expected_output_channels_last\n+            expected_output=(\n+                expected_output_channels_last\n                 if backend.config.image_data_format() == \"channels_last\"\n-            else expected_output_channels_first,\n+                else expected_output_channels_first\n+            ),\n             expected_output_sparse=sparse,\n             run_training_check=not sparse,\n         )\n\n@@ -44,9 +44,9 @@ class ConvLSTM1DTest(testing.TestCase):\n                 \"return_sequences\": True,\n             },\n             input_shape=(3, 2, 8, 3) if channels_last else (3, 2, 3, 8),\n-            expected_output_shape=(3, 2, 6, 5)\n-            if channels_last\n-            else (3, 2, 5, 6),\n+            expected_output_shape=(\n+                (3, 2, 6, 5) if channels_last else (3, 2, 5, 6)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n\n@@ -15,9 +15,9 @@ class ConvLSTM2DTest(testing.TestCase):\n             layers.ConvLSTM2D,\n             init_kwargs={\"filters\": 5, \"kernel_size\": 3, \"padding\": \"same\"},\n             input_shape=(3, 2, 4, 4, 3) if channels_last else (3, 2, 3, 4, 4),\n-            expected_output_shape=(3, 4, 4, 5)\n-            if channels_last\n-            else (3, 5, 4, 4),\n+            expected_output_shape=(\n+                (3, 4, 4, 5) if channels_last else (3, 5, 4, 4)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n@@ -32,9 +32,9 @@ class ConvLSTM2DTest(testing.TestCase):\n             },\n             input_shape=(3, 2, 8, 8, 3) if channels_last else (3, 2, 3, 8, 8),\n             call_kwargs={\"training\": True},\n-            expected_output_shape=(3, 6, 6, 5)\n-            if channels_last\n-            else (3, 5, 6, 6),\n+            expected_output_shape=(\n+                (3, 6, 6, 5) if channels_last else (3, 5, 6, 6)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n@@ -48,9 +48,9 @@ class ConvLSTM2DTest(testing.TestCase):\n                 \"return_sequences\": True,\n             },\n             input_shape=(3, 2, 8, 8, 3) if channels_last else (3, 2, 3, 8, 8),\n-            expected_output_shape=(3, 2, 6, 6, 5)\n-            if channels_last\n-            else (3, 2, 5, 6, 6),\n+            expected_output_shape=(\n+                (3, 2, 6, 6, 5) if channels_last else (3, 2, 5, 6, 6)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n\n@@ -14,12 +14,12 @@ class ConvLSTM1DTest(testing.TestCase):\n         self.run_layer_test(\n             layers.ConvLSTM3D,\n             init_kwargs={\"filters\": 5, \"kernel_size\": 3, \"padding\": \"same\"},\n-            input_shape=(3, 2, 4, 4, 4, 3)\n-            if channels_last\n-            else (3, 2, 3, 4, 4, 4),\n-            expected_output_shape=(3, 4, 4, 4, 5)\n-            if channels_last\n-            else (3, 5, 4, 4, 4),\n+            input_shape=(\n+                (3, 2, 4, 4, 4, 3) if channels_last else (3, 2, 3, 4, 4, 4)\n+            ),\n+            expected_output_shape=(\n+                (3, 4, 4, 4, 5) if channels_last else (3, 5, 4, 4, 4)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n@@ -32,13 +32,13 @@ class ConvLSTM1DTest(testing.TestCase):\n                 \"padding\": \"valid\",\n                 \"recurrent_dropout\": 0.5,\n             },\n-            input_shape=(3, 2, 8, 8, 8, 3)\n-            if channels_last\n-            else (3, 2, 3, 8, 8, 8),\n+            input_shape=(\n+                (3, 2, 8, 8, 8, 3) if channels_last else (3, 2, 3, 8, 8, 8)\n+            ),\n             call_kwargs={\"training\": True},\n-            expected_output_shape=(3, 6, 6, 6, 5)\n-            if channels_last\n-            else (3, 5, 6, 6, 6),\n+            expected_output_shape=(\n+                (3, 6, 6, 6, 5) if channels_last else (3, 5, 6, 6, 6)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n@@ -51,12 +51,12 @@ class ConvLSTM1DTest(testing.TestCase):\n                 \"padding\": \"valid\",\n                 \"return_sequences\": True,\n             },\n-            input_shape=(3, 2, 8, 8, 8, 3)\n-            if channels_last\n-            else (3, 2, 3, 8, 8, 8),\n-            expected_output_shape=(3, 2, 6, 6, 6, 5)\n-            if channels_last\n-            else (3, 2, 5, 6, 6, 6),\n+            input_shape=(\n+                (3, 2, 8, 8, 8, 3) if channels_last else (3, 2, 3, 8, 8, 8)\n+            ),\n+            expected_output_shape=(\n+                (3, 2, 6, 6, 6, 5) if channels_last else (3, 2, 5, 6, 6, 6)\n+            ),\n             expected_num_trainable_weights=3,\n             expected_num_non_trainable_weights=0,\n             supports_masking=True,\n\n@@ -1604,9 +1604,11 @@ def rnn(\n             for i, inp in enumerate(flatted_inputs)\n         )\n         input_ta = tuple(\n+            (\n                 ta.unstack(input_)\n                 if not go_backwards\n                 else ta.unstack(reverse(input_, 0))\n+            )\n             for ta, input_ in zip(input_ta, flatted_inputs)\n         )\n \n\n@@ -214,11 +214,11 @@ class LegacyH5WholeModelTest(testing.TestCase):\n \n             @classmethod\n             def from_config(cls, config):\n-                config[\n-                    \"sublayers\"\n-                ] = serialization_lib.deserialize_keras_object(\n+                config[\"sublayers\"] = (\n+                    serialization_lib.deserialize_keras_object(\n                         config[\"sublayers\"]\n                     )\n+                )\n                 return cls(**config)\n \n         @object_registration.register_keras_serializable(package=\"Foo\")\n@@ -455,11 +455,11 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n \n             @classmethod\n             def from_config(cls, config):\n-                config[\n-                    \"sublayers\"\n-                ] = serialization_lib.deserialize_keras_object(\n+                config[\"sublayers\"] = (\n+                    serialization_lib.deserialize_keras_object(\n                         config[\"sublayers\"]\n                     )\n+                )\n                 return cls(**config)\n \n         # Re-implement and re-register in Keras 3\n\n@@ -1,4 +1,5 @@\n \"\"\"Legacy serialization logic for Keras models.\"\"\"\n+\n import contextlib\n import inspect\n import json\n@@ -396,9 +397,9 @@ def class_and_config_for_serialized_keras_object(\n             # rare case.  This issue does not occur if a string field has a\n             # naming conflict with a custom object, since the config of an\n             # object will always be a dict.\n-            deserialized_objects[\n-                key\n-            ] = object_registration.get_registered_object(item, custom_objects)\n+            deserialized_objects[key] = (\n+                object_registration.get_registered_object(item, custom_objects)\n+            )\n     for key, item in deserialized_objects.items():\n         cls_config[key] = deserialized_objects[key]\n \n\n@@ -111,20 +111,26 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         else:\n             input_shape = (None, 3, 8, None)\n         x = KerasTensor(input_shape)\n-        self.assertEqual(\n-            knn.max_pool(x, 2, 1).shape, (None, 7, None, 3)\n-        ) if data_format == \"channels_last\" else (None, 3, 7, None)\n+        (\n+            self.assertEqual(knn.max_pool(x, 2, 1).shape, (None, 7, None, 3))\n+            if data_format == \"channels_last\"\n+            else (None, 3, 7, None)\n+        )\n         self.assertEqual(\n             knn.max_pool(x, 2, 2, padding=\"same\").shape,\n+            (\n                 (None, 4, None, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, None),\n+                else (None, 3, 4, None)\n+            ),\n         )\n         self.assertEqual(\n             knn.max_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n+            (\n                 (None, 4, None, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, None),\n+                else (None, 3, 4, None)\n+            ),\n         )\n \n     def test_average_pool(self):\n@@ -150,21 +156,27 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         x = KerasTensor(input_shape)\n         self.assertEqual(\n             knn.average_pool(x, 2, 1).shape,\n+            (\n                 (None, 7, None, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 7, None),\n+                else (None, 3, 7, None)\n+            ),\n         )\n         self.assertEqual(\n             knn.average_pool(x, 2, 2, padding=\"same\").shape,\n+            (\n                 (None, 4, None, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, None),\n+                else (None, 3, 4, None)\n+            ),\n         )\n         self.assertEqual(\n             knn.average_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n+            (\n                 (None, 4, None, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, None),\n+                else (None, 3, 4, None)\n+            ),\n         )\n \n     def test_multi_hot(self):\n@@ -192,16 +204,20 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.conv(inputs_1d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 17, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 17),\n+                    else (None, 2, 17)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.conv(inputs_1d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 20, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 20),\n+                    else (None, 2, 20)\n+                ),\n             )\n         self.assertEqual(\n             knn.conv(inputs_1d, kernel, (2,), dilation_rate=2).shape,\n@@ -218,22 +234,28 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 9, None, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 9, None),\n+                    else (None, 2, 9, None)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 10, None, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 10, None),\n+                    else (None, 2, 10, None)\n+                ),\n             )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n+            (\n                 (None, 4, None, 2)\n                 if data_format == \"channels_last\"\n-            else (None, 2, 4, None),\n+                else (None, 2, 4, None)\n+            ),\n         )\n \n         # Test 2D conv - H, W specified\n@@ -246,22 +268,28 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 9, 9, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 9, 9),\n+                    else (None, 2, 9, 9)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 10, 10, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 10, 10),\n+                    else (None, 2, 10, 10)\n+                ),\n             )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n+            (\n                 (None, 4, 9, 2)\n                 if data_format == \"channels_last\"\n-            else (None, 2, 4, 9),\n+                else (None, 2, 4, 9)\n+            ),\n         )\n \n         # Test 3D conv.\n@@ -274,24 +302,30 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.conv(inputs_3d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 6, None, 6, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 6, None, 6),\n+                    else (None, 2, 6, None, 6)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.conv(inputs_3d, kernel, (2, 1, 2), padding=padding).shape,\n+                (\n                     (None, 4, None, 4, 2)\n                     if data_format == \"channels_last\"\n-                else (None, 2, 4, None, 4),\n+                    else (None, 2, 4, None, 4)\n+                ),\n             )\n         self.assertEqual(\n             knn.conv(\n                 inputs_3d, kernel, 1, padding=\"valid\", dilation_rate=(1, 2, 2)\n             ).shape,\n+            (\n                 (None, 6, None, 4, 2)\n                 if data_format == \"channels_last\"\n-            else (None, 2, 6, None, 4),\n+                else (None, 2, 6, None, 4)\n+            ),\n         )\n \n     def test_depthwise_conv(self):\n@@ -306,18 +340,22 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.depthwise_conv(inputs_1d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 17, 3)\n                     if data_format == \"channels_last\"\n-                else (None, 3, 17),\n+                    else (None, 3, 17)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.depthwise_conv(\n                     inputs_1d, kernel, (1,), padding=padding\n                 ).shape,\n+                (\n                     (None, 20, 3)\n                     if data_format == \"channels_last\"\n-                else (None, 3, 20),\n+                    else (None, 3, 20)\n+                ),\n             )\n         self.assertEqual(\n             knn.depthwise_conv(inputs_1d, kernel, 2, dilation_rate=2).shape,\n@@ -334,32 +372,40 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         for padding in [\"valid\", \"VALID\"]:\n             self.assertEqual(\n                 knn.depthwise_conv(inputs_2d, kernel, 1, padding=padding).shape,\n+                (\n                     (None, 9, 9, 3)\n                     if data_format == \"channels_last\"\n-                else (None, 3, 9, 9),\n+                    else (None, 3, 9, 9)\n+                ),\n             )\n         for padding in [\"same\", \"SAME\"]:\n             self.assertEqual(\n                 knn.depthwise_conv(\n                     inputs_2d, kernel, (1, 2), padding=padding\n                 ).shape,\n+                (\n                     (None, 10, 5, 3)\n                     if data_format == \"channels_last\"\n-                else (None, 3, 10, 5),\n+                    else (None, 3, 10, 5)\n+                ),\n             )\n         self.assertEqual(\n             knn.depthwise_conv(inputs_2d, kernel, 2, dilation_rate=2).shape,\n+            (\n                 (None, 4, 4, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, 4),\n+                else (None, 3, 4, 4)\n+            ),\n         )\n         self.assertEqual(\n             knn.depthwise_conv(\n                 inputs_2d, kernel, 2, dilation_rate=(2, 1)\n             ).shape,\n+            (\n                 (None, 4, 5, 3)\n                 if data_format == \"channels_last\"\n-            else (None, 3, 4, 5),\n+                else (None, 3, 4, 5)\n+            ),\n         )\n \n     def test_separable_conv(self):\n@@ -403,25 +449,31 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n             knn.separable_conv(\n                 inputs_2d, kernel, pointwise_kernel, 1, padding=\"valid\"\n             ).shape,\n+            (\n                 (None, 9, 9, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 9, 9),\n+                else (None, 5, 9, 9)\n+            ),\n         )\n         self.assertEqual(\n             knn.separable_conv(\n                 inputs_2d, kernel, pointwise_kernel, (1, 2), padding=\"same\"\n             ).shape,\n+            (\n                 (None, 10, 5, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 10, 5),\n+                else (None, 5, 10, 5)\n+            ),\n         )\n         self.assertEqual(\n             knn.separable_conv(\n                 inputs_2d, kernel, pointwise_kernel, 2, dilation_rate=(2, 1)\n             ).shape,\n+            (\n                 (None, 4, 5, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 4, 5),\n+                else (None, 5, 4, 5)\n+            ),\n         )\n \n     def test_conv_transpose(self):\n@@ -455,23 +507,29 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         kernel = KerasTensor([2, 2, 5, 3])\n         self.assertEqual(\n             knn.conv_transpose(inputs_2d, kernel, 2).shape,\n+            (\n                 (None, 8, 8, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 8, 8),\n+                else (None, 5, 8, 8)\n+            ),\n         )\n         self.assertEqual(\n             knn.conv_transpose(inputs_2d, kernel, (2, 2), padding=\"same\").shape,\n+            (\n                 (None, 8, 8, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 8, 8),\n+                else (None, 5, 8, 8)\n+            ),\n         )\n         self.assertEqual(\n             knn.conv_transpose(\n                 inputs_2d, kernel, (5, 5), padding=\"valid\", output_padding=4\n             ).shape,\n+            (\n                 (None, 21, 21, 5)\n                 if data_format == \"channels_last\"\n-            else (None, 5, 21, 21),\n+                else (None, 5, 21, 21)\n+            ),\n         )\n \n     def test_one_hot(self):\n@@ -718,9 +776,11 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, 1, padding=\"same\").shape,\n+            (\n                 (2, 10, 10, 2)\n                 if data_format == \"channels_last\"\n-            else (2, 2, 10, 10),\n+                else (2, 2, 10, 10)\n+            ),\n         )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n@@ -736,23 +796,29 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         kernel = KerasTensor([3, 3, 3, 3, 2])\n         self.assertEqual(\n             knn.conv(inputs_3d, kernel, 1, padding=\"valid\").shape,\n+            (\n                 (2, 6, 6, 6, 2)\n                 if data_format == \"channels_last\"\n-            else (2, 2, 6, 6, 6),\n+                else (2, 2, 6, 6, 6)\n+            ),\n         )\n         self.assertEqual(\n             knn.conv(inputs_3d, kernel, (2, 1, 2), padding=\"same\").shape,\n+            (\n                 (2, 4, 8, 4, 2)\n                 if data_format == \"channels_last\"\n-            else (2, 2, 4, 8, 4),\n+                else (2, 2, 4, 8, 4)\n+            ),\n         )\n         self.assertEqual(\n             knn.conv(\n                 inputs_3d, kernel, 1, padding=\"valid\", dilation_rate=(1, 2, 2)\n             ).shape,\n+            (\n                 (2, 6, 4, 4, 2)\n                 if data_format == \"channels_last\"\n-            else (2, 2, 6, 4, 4),\n+                else (2, 2, 6, 4, 4)\n+            ),\n         )\n \n     def test_depthwise_conv(self):\n@@ -900,9 +966,11 @@ class NNOpsStaticShapeTest(testing.TestCase):\n             knn.conv_transpose(\n                 inputs_2d, kernel, (5, 5), padding=\"valid\", output_padding=4\n             ).shape,\n+            (\n                 (2, 21, 21, 5)\n                 if data_format == \"channels_last\"\n-            else (2, 5, 21, 21),\n+                else (2, 5, 21, 21)\n+            ),\n         )\n \n     def test_batched_and_unbatched_inputs_multi_hot(self):\n\n@@ -140,6 +140,7 @@ zeros_like\n \n \n \"\"\"\n+\n import builtins\n import re\n \n\n@@ -778,11 +778,11 @@ class BaseOptimizer:\n         \"\"\"\n         if \"learning_rate\" in config:\n             if isinstance(config[\"learning_rate\"], dict):\n-                config[\n-                    \"learning_rate\"\n-                ] = serialization_lib.deserialize_keras_object(\n+                config[\"learning_rate\"] = (\n+                    serialization_lib.deserialize_keras_object(\n                         config[\"learning_rate\"], custom_objects=custom_objects\n                     )\n+                )\n         return cls(**config)\n \n     def __setattr__(self, name, value):\n\n@@ -1,4 +1,5 @@\n \"\"\"Tests for Keras python-based idempotent saving functions.\"\"\"\n+\n import json\n import os\n import warnings\n\n@@ -220,9 +220,11 @@ def serialize_keras_object(obj):\n         # TensorShape and tf.DType conversion\n         ts_config = list(\n             map(\n-                lambda x: x.as_list()\n+                lambda x: (\n+                    x.as_list()\n                     if isinstance(x, tf.TensorShape)\n-                else (x.name if isinstance(x, tf.DType) else x),\n+                    else (x.name if isinstance(x, tf.DType) else x)\n+                ),\n                 ts_config,\n             )\n         )\n@@ -651,9 +653,11 @@ def deserialize_keras_object(\n         )\n         # Conversion to TensorShape and DType\n         inner_config = map(\n-            lambda x: tf.TensorShape(x)\n+            lambda x: (\n+                tf.TensorShape(x)\n                 if isinstance(x, list)\n-            else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x),\n+                else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x)\n+            ),\n             inner_config,\n         )\n         return obj._deserialize(tuple(inner_config))\n\n@@ -38,6 +38,7 @@ EpochIterator steps:\n or until there is no data\n \n \"\"\"\n+\n import warnings\n \n from keras.trainers import data_adapters\n\n@@ -14,6 +14,7 @@ python3 pip_build.py\n python3 pip_build.py --install\n ```\n \"\"\"\n+\n import argparse\n import datetime\n import glob\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47bf22d61fcf70255012802bb0772dbfdff002c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 12/4 | Churn Δ: 80 | Churn Cumulative: 1206 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -254,6 +254,10 @@ def erf(x):\n     return jax.lax.erf(x)\n \n \n+def erfinv(x):\n+    return jax.lax.erf_inv(x)\n+\n+\n def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n\n@@ -308,6 +308,10 @@ def erf(x):\n     return np.array(scipy.special.erf(x))\n \n \n+def erfinv(x):\n+    return np.array(scipy.special.erfinv(x))\n+\n+\n def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n\n@@ -246,6 +246,10 @@ def erf(x):\n     return tf.math.erf(x)\n \n \n+def erfinv(x):\n+    return tf.math.erfinv(x)\n+\n+\n def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n\n@@ -415,6 +415,11 @@ def erf(x):\n     return torch.erf(x)\n \n \n+def erfinv(x):\n+    x = convert_to_tensor(x)\n+    return torch.erfinv(x)\n+\n+\n def solve(a, b):\n     a = convert_to_tensor(a)\n     b = convert_to_tensor(b)\n\n@@ -968,6 +968,37 @@ def erf(x):\n     return backend.math.erf(x)\n \n \n+class Erfinv(Operation):\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape, dtype=x.dtype)\n+\n+    def call(self, x):\n+        return backend.math.erfinv(x)\n+\n+\n+@keras_export(\"keras.ops.erfinv\")\n+def erfinv(x):\n+    \"\"\"Computes the inverse error function of `x`, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same dtype as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-0.5, -0.2, -0.1, 0.0, 0.3])\n+    >>> keras.ops.erfinv(x)\n+    array([-0.47694, -0.17914, -0.08886,  0. ,  0.27246], dtype=float32)\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Erfinv().symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.math.erfinv(x)\n+\n+\n+\n class Solve(Operation):\n     def call(self, a, b):\n         a = backend.convert_to_tensor(a)\n\n@@ -887,6 +887,38 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         output_from_edge_erf_op = kmath.erf(edge_values)\n         self.assertAllClose(expected_output, output_from_edge_erf_op, atol=1e-4)\n \n+\n+    def test_erfinv_operation_basic(self):\n+        # Sample values for testing\n+        sample_values = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n+\n+        # Expected output using numpy's approximation of the error function\n+        expected_output = scipy.special.erfinv(sample_values)\n+\n+        # Output from the erf operation in keras_core\n+        output_from_erfinv_op = kmath.erfinv(sample_values)\n+\n+        # Assert that the outputs are close\n+        self.assertAllClose(expected_output, output_from_erfinv_op, atol=1e-4)\n+\n+    def test_erfinv_operation_dtype(self):\n+        # Test for float32 and float64 data types\n+        for dtype in (\"float32\", \"float64\"):\n+            sample_values = np.array(\n+                [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0], dtype=dtype\n+            )\n+            expected_output = scipy.special.erfinv(sample_values)\n+            output_from_erfinv_op = kmath.erfinv(sample_values)\n+            self.assertAllClose(expected_output, output_from_erfinv_op, atol=1e-4)\n+\n+    def test_erfinv_operation_edge_cases(self):\n+        # Test for edge cases\n+        edge_values = np.array([1e5, -1e5, 1e-5, -1e-5], dtype=np.float64)\n+        expected_output = scipy.special.erfinv(edge_values)\n+        output_from_edge_erfinv_op = kmath.erfinv(edge_values)\n+        self.assertAllClose(expected_output, output_from_edge_erfinv_op, atol=1e-4)\n+\n+\n     def test_solve(self):\n         x1 = np.array([[1, 2], [4, 5]], dtype=\"float32\")\n         x2 = np.array([[2, 4], [8, 10]], dtype=\"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acf73293f14314b249db002fda308300cc18cb6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 12 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -164,6 +164,7 @@ class Progbar:\n                             self._values[k][0] / max(1, self._values[k][1])\n                         )\n                     )\n+                    avg = float(avg)\n                     if abs(avg) > 1e-3:\n                         info += f\" {avg:.4f}\"\n                     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#892a5192b34a939e5b3ed46dada8a87afb36ef20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 965 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,3 +79,6 @@ class Initializer:\n             An `Initializer` instance.\n         \"\"\"\n         return cls(**config)\n+\n+    def clone(self):\n+        return self.__class__.from_config(self.get_config())\n\n@@ -998,7 +998,6 @@ def erfinv(x):\n     return backend.math.erfinv(x)\n \n \n-\n class Solve(Operation):\n     def call(self, a, b):\n         a = backend.convert_to_tensor(a)\n\n@@ -887,7 +887,6 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         output_from_edge_erf_op = kmath.erf(edge_values)\n         self.assertAllClose(expected_output, output_from_edge_erf_op, atol=1e-4)\n \n-\n     def test_erfinv_operation_basic(self):\n         # Sample values for testing\n         sample_values = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n@@ -909,15 +908,18 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             )\n             expected_output = scipy.special.erfinv(sample_values)\n             output_from_erfinv_op = kmath.erfinv(sample_values)\n-            self.assertAllClose(expected_output, output_from_erfinv_op, atol=1e-4)\n+            self.assertAllClose(\n+                expected_output, output_from_erfinv_op, atol=1e-4\n+            )\n \n     def test_erfinv_operation_edge_cases(self):\n         # Test for edge cases\n         edge_values = np.array([1e5, -1e5, 1e-5, -1e-5], dtype=np.float64)\n         expected_output = scipy.special.erfinv(edge_values)\n         output_from_edge_erfinv_op = kmath.erfinv(edge_values)\n-        self.assertAllClose(expected_output, output_from_edge_erfinv_op, atol=1e-4)\n-\n+        self.assertAllClose(\n+            expected_output, output_from_edge_erfinv_op, atol=1e-4\n+        )\n \n     def test_solve(self):\n         x1 = np.array([[1, 2], [4, 5]], dtype=\"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4d9567e28a8137c01511c69038b3c2e2040e264", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 38 | Files Changed: 5 | Hunks: 20 | Methods Changed: 4 | Complexity Δ (Sum/Max): 38/23 | Churn Δ: 75 | Churn Cumulative: 1248 | Contributors (this commit): 12 | Commits (past 90d): 15 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import keras\n from keras import backend\n from keras import testing\n from keras.applications import imagenet_utils as utils\n-from keras.mixed_precision import set_dtype_policy\n+from keras.layers.dtype_policy import set_dtype_policy\n \n \n class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n\n@@ -1,10 +1,12 @@\n from keras import backend\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n+from keras.saving import serialization_lib\n \n \n @keras_export(\n     [\n+        \"keras.DTypePolicy\",\n         \"keras.mixed_precision.DTypePolicy\",\n         \"keras.mixed_precision.Policy\",\n     ]\n@@ -15,7 +17,7 @@ class DTypePolicy:\n     A dtype policy determines a layer's computation and variable dtypes. Each\n     layer has a policy. Policies can be passed to the `dtype` argument of layer\n     constructors, or a global policy can be set with\n-    `keras.mixed_precision.set_dtype_policy`.\n+    `keras.config.set_dtype_policy`.\n \n     Args:\n         name: The policy name, which determines the compute and variable dtypes.\n@@ -32,7 +34,7 @@ class DTypePolicy:\n     API name. Mixed precision can be enabled by passing `\"mixed_float16\"` or\n     `\"mixed_bfloat16\"` to `keras.mixed_precision.set_dtype_policy()`.\n \n-    >>> keras.mixed_precision.set_dtype_policy(\"mixed_float16\")\n+    >>> keras.config.set_dtype_policy(\"mixed_float16\")\n     >>> layer1 = keras.layers.Dense(10)\n     >>> layer1.dtype_policy  # layer1 will automatically use mixed precision\n     <DTypePolicy \"mixed_float16\">\n@@ -42,11 +44,11 @@ class DTypePolicy:\n     >>> layer2.dtype_policy\n     <DTypePolicy \"float32\">\n     >>> # Set policy back to initial float32.\n-    >>> keras.mixed_precision.set_dtype_policy('float32')\n+    >>> keras.config.set_dtype_policy('float32')\n \n     In the example above, passing `dtype=\"float32\"` to the layer is\n     equivalent to passing\n-    `dtype=keras.mixed_precision.DTypePolicy(\"float32\")`.\n+    `dtype=keras.config.DTypePolicy(\"float32\")`.\n     In general, passing a dtype policy name to a layer is equivalent\n     to passing the corresponding policy, so it is never necessary\n     to explicitly construct a `DTypePolicy` object.\n@@ -142,6 +144,7 @@ class DTypePolicy:\n \n @keras_export(\n     [\n+        \"keras.config.set_dtype_policy\",\n         \"keras.mixed_precision.set_dtype_policy\",\n         \"keras.mixed_precision.set_global_policy\",\n     ]\n@@ -151,7 +154,7 @@ def set_dtype_policy(policy):\n \n     Example:\n \n-    >>> keras.mixed_precision.set_dtype_policy(\"mixed_float16\")\n+    >>> keras.config.set_dtype_policy(\"mixed_float16\")\n     \"\"\"\n     if not isinstance(policy, DTypePolicy):\n         if isinstance(policy, str):\n@@ -169,6 +172,7 @@ def set_dtype_policy(policy):\n \n @keras_export(\n     [\n+        \"keras.config.dtype_policy\",\n         \"keras.mixed_precision.dtype_policy\",\n         \"keras.mixed_precision.global_policy\",\n     ]\n@@ -180,3 +184,21 @@ def dtype_policy():\n         policy = DTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n     return policy\n+\n+\n+def get(identifier):\n+    if identifier is None:\n+        return dtype_policy()\n+    if isinstance(identifier, DTypePolicy):\n+        return identifier\n+    if isinstance(identifier, dict):\n+        return serialization_lib.deserialize_keras_object(identifier)\n+    if isinstance(identifier, str):\n+        return DTypePolicy(identifier)\n+    try:\n+        return DTypePolicy(backend.standardize_dtype(identifier))\n+    except:\n+        raise ValueError(\n+            \"Cannot interpret `dtype` argument. Expected a string \"\n+            f\"or an instance of DTypePolicy. Received: dtype={identifier}\"\n+        )\n\n@@ -1,6 +1,6 @@\n-from keras.mixed_precision import DTypePolicy\n-from keras.mixed_precision import dtype_policy\n-from keras.mixed_precision import set_dtype_policy\n+from keras.layers.dtype_policy import DTypePolicy\n+from keras.layers.dtype_policy import dtype_policy\n+from keras.layers.dtype_policy import set_dtype_policy\n from keras.testing import test_case\n \n \n\n@@ -26,7 +26,6 @@ import tree\n from keras import backend\n from keras import constraints\n from keras import initializers\n-from keras import mixed_precision\n from keras import ops\n from keras import regularizers\n from keras import utils\n@@ -35,6 +34,7 @@ from keras.backend import KerasTensor\n from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.distribution import distribution_lib\n+from keras.layers import dtype_policy\n from keras.layers import input_spec\n from keras.metrics.metric import Metric\n from keras.ops.operation import Operation\n@@ -83,12 +83,12 @@ class Layer(BackendLayer, Operation):\n         trainable: Boolean, whether the layer's variables should be trainable.\n         name: String name of the layer.\n         dtype: The dtype of the layer's computations and weights. Can also be a\n-            `keras.mixed_precision.DTypePolicy`,\n+            `keras.DTypePolicy`,\n             which allows the computation and\n             weight dtype to differ. Defaults to `None`. `None` means to use\n-            `keras.mixed_precision.dtype_policy()`,\n+            `keras.config.dtype_policy()`,\n             which is a `float32` policy unless set to different value\n-            (via `keras.mixed_precision.set_dtype_policy()`).\n+            (via `keras.config.set_dtype_policy()`).\n \n     Attributes:\n         name: The name of the layer (string).\n@@ -98,7 +98,7 @@ class Layer(BackendLayer, Operation):\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n             When mixed precision is used with a\n-            `keras.mixed_precision.DTypePolicy`, this will be different\n+            `keras.DTypePolicy`, this will be different\n             than `variable_dtype`.\n         trainable_weights: List of variables to be included in backprop.\n         non_trainable_weights: List of variables that should not be\n@@ -269,7 +269,7 @@ class Layer(BackendLayer, Operation):\n             )\n \n         self.built = False\n-        self.dtype_policy = mixed_precision.resolve_policy(dtype)\n+        self.dtype_policy = dtype_policy.get(dtype)\n         self.autocast = autocast\n         self._input_spec = None\n         self._called = False\n\n@@ -1,23 +0,0 @@\n-from keras import backend\n-from keras.mixed_precision.dtype_policy import DTypePolicy\n-from keras.mixed_precision.dtype_policy import dtype_policy\n-from keras.mixed_precision.dtype_policy import set_dtype_policy\n-from keras.saving import serialization_lib\n-\n-\n-def resolve_policy(identifier):\n-    if identifier is None:\n-        return dtype_policy()\n-    if isinstance(identifier, DTypePolicy):\n-        return identifier\n-    if isinstance(identifier, dict):\n-        return serialization_lib.deserialize_keras_object(identifier)\n-    if isinstance(identifier, str):\n-        return DTypePolicy(identifier)\n-    try:\n-        return DTypePolicy(backend.standardize_dtype(identifier))\n-    except:\n-        raise ValueError(\n-            \"Cannot interpret `dtype` argument. Expected a string \"\n-            f\"or an instance of DTypePolicy. Received: dtype={identifier}\"\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
