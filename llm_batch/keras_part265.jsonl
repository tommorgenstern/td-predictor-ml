{"custom_id": "keras#c2a6caae852adb0c832da86d5815c26b2e7d24c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 68 | Files Changed: 1 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 129 | Churn Cumulative: 11892 | Contributors (this commit): 64 | Commits (past 90d): 17 | Contributors (cumulative): 64 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1868,7 +1868,52 @@ class DepthwiseConv2D(Conv2D):\n         return config\n \n \n-class UpSampling1D(Layer):\n+class _UpSampling(Layer):\n+    \"\"\"Abstract nD UpSampling layer (private, used as implementation base).\n+\n+    # Arguments\n+        size: Tuple of ints.\n+        data_format: A string,\n+            one of `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n+            inputs with shape `(batch, channels, ...)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+    \"\"\"\n+    def __init__(self, size, data_format=None, **kwargs):\n+        # self.rank is 1 for UpSampling1D, 2 for UpSampling2D.\n+        self.rank = len(size)\n+        self.size = size\n+        self.data_format = K.normalize_data_format(data_format)\n+        self.input_spec = InputSpec(ndim=self.rank + 2)\n+        super(_UpSampling, self).__init__(**kwargs)\n+\n+    def call(self, inputs):\n+        raise NotImplementedError\n+\n+    def compute_output_shape(self, input_shape):\n+        size_all_dims = (1,) + self.size + (1,)\n+        spatial_axes = list(range(1, 1 + self.rank))\n+        size_all_dims = transpose_shape(size_all_dims,\n+                                        self.data_format,\n+                                        spatial_axes)\n+        output_shape = list(input_shape)\n+        for dim in range(len(output_shape)):\n+            if output_shape[dim] is not None:\n+                output_shape[dim] *= size_all_dims[dim]\n+        return tuple(output_shape)\n+\n+    def get_config(self):\n+        config = {'size': self.size,\n+                  'data_format': self.data_format}\n+        base_config = super(_UpSampling, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class UpSampling1D(_UpSampling):\n     \"\"\"Upsampling layer for 1D inputs.\n \n     Repeats each temporal step `size` times along the time axis.\n@@ -1885,25 +1930,20 @@ class UpSampling1D(Layer):\n \n     @interfaces.legacy_upsampling1d_support\n     def __init__(self, size=2, **kwargs):\n-        super(UpSampling1D, self).__init__(**kwargs)\n-        self.size = int(size)\n-        self.input_spec = InputSpec(ndim=3)\n-\n-    def compute_output_shape(self, input_shape):\n-        size = self.size * input_shape[1] if input_shape[1] is not None else None\n-        return (input_shape[0], size, input_shape[2])\n+        super(UpSampling1D, self).__init__((int(size),), 'channels_last', **kwargs)\n \n     def call(self, inputs):\n-        output = K.repeat_elements(inputs, self.size, axis=1)\n+        output = K.repeat_elements(inputs, self.size[0], axis=1)\n         return output\n \n     def get_config(self):\n-        config = {'size': self.size}\n-        base_config = super(UpSampling1D, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        config = super(UpSampling1D, self).get_config()\n+        config['size'] = self.size[0]\n+        config.pop('data_format')\n+        return config\n \n \n-class UpSampling2D(Layer):\n+class UpSampling2D(_UpSampling):\n     \"\"\"Upsampling layer for 2D inputs.\n \n     Repeats the rows and columns of the data\n@@ -1943,43 +1983,24 @@ class UpSampling2D(Layer):\n \n     @interfaces.legacy_upsampling2d_support\n     def __init__(self, size=(2, 2), data_format=None, interpolation='nearest', **kwargs):\n-        super(UpSampling2D, self).__init__(**kwargs)\n-        self.data_format = K.normalize_data_format(data_format)\n-        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n-        self.input_spec = InputSpec(ndim=4)\n+        normalized_size = conv_utils.normalize_tuple(size, 2, 'size')\n+        super(UpSampling2D, self).__init__(normalized_size, data_format, **kwargs)\n         if interpolation not in ['nearest', 'bilinear']:\n             raise ValueError('interpolation should be one '\n                              'of \"nearest\" or \"bilinear\".')\n         self.interpolation = interpolation\n \n-    def compute_output_shape(self, input_shape):\n-        if self.data_format == 'channels_first':\n-            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n-            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n-            return (input_shape[0],\n-                    input_shape[1],\n-                    height,\n-                    width)\n-        elif self.data_format == 'channels_last':\n-            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n-            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n-            return (input_shape[0],\n-                    height,\n-                    width,\n-                    input_shape[3])\n-\n     def call(self, inputs):\n         return K.resize_images(inputs, self.size[0], self.size[1],\n                                self.data_format, self.interpolation)\n \n     def get_config(self):\n-        config = {'size': self.size,\n-                  'data_format': self.data_format}\n-        base_config = super(UpSampling2D, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        config = super(UpSampling2D, self).get_config()\n+        config['interpolation'] = self.interpolation\n+        return config\n \n \n-class UpSampling3D(Layer):\n+class UpSampling3D(_UpSampling):\n     \"\"\"Upsampling layer for 3D inputs.\n \n     Repeats the 1st, 2nd and 3rd dimensions\n@@ -2016,42 +2037,14 @@ class UpSampling3D(Layer):\n \n     @interfaces.legacy_upsampling3d_support\n     def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n-        self.data_format = K.normalize_data_format(data_format)\n-        self.size = conv_utils.normalize_tuple(size, 3, 'size')\n-        self.input_spec = InputSpec(ndim=5)\n-        super(UpSampling3D, self).__init__(**kwargs)\n-\n-    def compute_output_shape(self, input_shape):\n-        if self.data_format == 'channels_first':\n-            dim1 = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n-            dim2 = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n-            dim3 = self.size[2] * input_shape[4] if input_shape[4] is not None else None\n-            return (input_shape[0],\n-                    input_shape[1],\n-                    dim1,\n-                    dim2,\n-                    dim3)\n-        elif self.data_format == 'channels_last':\n-            dim1 = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n-            dim2 = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n-            dim3 = self.size[2] * input_shape[3] if input_shape[3] is not None else None\n-            return (input_shape[0],\n-                    dim1,\n-                    dim2,\n-                    dim3,\n-                    input_shape[4])\n+        normalized_size = conv_utils.normalize_tuple(size, 3, 'size')\n+        super(UpSampling3D, self).__init__(normalized_size, data_format, **kwargs)\n \n     def call(self, inputs):\n         return K.resize_volumes(inputs,\n                                 self.size[0], self.size[1], self.size[2],\n                                 self.data_format)\n \n-    def get_config(self):\n-        config = {'size': self.size,\n-                  'data_format': self.data_format}\n-        base_config = super(UpSampling3D, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n \n class _ZeroPadding(Layer):\n     \"\"\"Abstract nD ZeroPadding layer (private, used as implementation base).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9400be98783135a1d42dd238f4e6c3aa048eceea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 855 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,12 +122,12 @@ class Embedding(Layer):\n             else:\n                 in_lens = [self.input_length]\n             if len(in_lens) != len(input_shape) - 1:\n-                ValueError('\"input_length\" is %s, but received input has shape %s' %\n+                raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n                                  (str(self.input_length), str(input_shape)))\n             else:\n                 for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n                     if s1 is not None and s2 is not None and s1 != s2:\n-                        ValueError('\"input_length\" is %s, but received input has shape %s' %\n+                        raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n                                          (str(self.input_length), str(input_shape)))\n                     elif s1 is None:\n                         in_lens[i] = s2\n\n@@ -1,6 +1,7 @@\n import pytest\n from keras.utils.test_utils import layer_test, keras_test\n from keras.layers.embeddings import Embedding\n+from keras.models import Sequential\n import keras.backend as K\n \n \n@@ -29,5 +30,25 @@ def test_embedding():\n                expected_output_dtype=K.floatx())\n \n \n+@keras_test\n+def test_embedding_invalid():\n+\n+    # len(input_length) should be equal to len(input_shape) - 1\n+    with pytest.raises(ValueError):\n+        model = Sequential([Embedding(\n+            input_dim=10,\n+            output_dim=4,\n+            input_length=2,\n+            input_shape=(3, 4, 5))])\n+\n+    # input_length should be equal to input_shape[1:]\n+    with pytest.raises(ValueError):\n+        model = Sequential([Embedding(\n+            input_dim=10,\n+            output_dim=4,\n+            input_length=2,\n+            input_shape=(3, 5))])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#244546c2fe5165b6770eb456afd5fac8878473c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 46 | Churn Cumulative: 15005 | Contributors (this commit): 124 | Commits (past 90d): 38 | Contributors (cumulative): 147 | DMM Complexity: 0.5757575757575758\n\nDIFF:\n@@ -2545,7 +2545,10 @@ class Function(object):\n         # (since the outputs of fetches are never returned).\n         # This requires us to wrap fetches in `identity` ops.\n         self.fetches = [tf.identity(x) for x in self.fetches]\n-        self.session_kwargs = session_kwargs\n+        # self.session_kwargs is used for _legacy_call\n+        self.session_kwargs = session_kwargs.copy()\n+        self.run_options = session_kwargs.pop('options', None)\n+        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '\n@@ -2593,6 +2596,9 @@ class Function(object):\n             callable_opts.fetch.append(x.name)\n         # Handle updates.\n         callable_opts.target.append(self.updates_op.name)\n+        # Handle run_options.\n+        if self.run_options:\n+            callable_opts.run_options.CopyFrom(self.run_options)\n         # Create callable.\n         callable_fn = session._make_callable_from_options(callable_opts)\n         # Cache parameters corresponding to the generated callable, so that\n@@ -2643,6 +2649,9 @@ class Function(object):\n                                 feed_symbols,\n                                 symbol_vals,\n                                 session)\n+        if self.run_metadata:\n+            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n+        else:\n             fetched = self._callable_fn(*array_vals)\n         return fetched[:len(self.outputs)]\n \n@@ -2673,6 +2682,16 @@ class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n \n+            # callable generated by Session._make_callable_from_options accepts\n+            # `run_metadata` keyword argument since TF 1.10\n+            if (self.run_metadata and\n+                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n+                if py_any(is_tensor(x) for x in inputs):\n+                    raise ValueError(\n+                        'In order to feed symbolic tensors to a Keras model and set '\n+                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n+                return self._legacy_call(inputs)\n+\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):\n\n@@ -528,6 +528,31 @@ class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='Uses the `options` and `run_metadata` arguments.')\n+    def test_function_tf_run_options_with_run_metadata(self):\n+        from tensorflow.core.protobuf import config_pb2\n+        x_placeholder = K.placeholder(shape=())\n+        y_placeholder = K.placeholder(shape=())\n+\n+        run_options = config_pb2.RunOptions(output_partition_graphs=True)\n+        run_metadata = config_pb2.RunMetadata()\n+        # enable run_options.\n+        f = K.function(inputs=[x_placeholder, y_placeholder],\n+                       outputs=[x_placeholder + y_placeholder],\n+                       options=run_options,\n+                       run_metadata=run_metadata)\n+        output = f([10., 20.])\n+        assert output == [30.]\n+        assert len(run_metadata.partition_graphs) > 0\n+        # disable run_options.\n+        f = K.function(inputs=[x_placeholder, y_placeholder],\n+                       outputs=[x_placeholder + y_placeholder],\n+                       run_metadata=run_metadata)\n+        output = f([10., 20.])\n+        assert output == [30.]\n+        assert len(run_metadata.partition_graphs) == 0\n+\n     @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Uses the `string` type for a tensor.')\n     def test_function_tf_string_input(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c913b6da92f6ab9a3f4c897caa4085e782a14680", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 34 | Files Changed: 3 | Hunks: 27 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 99 | Churn Cumulative: 15659 | Contributors (this commit): 125 | Commits (past 90d): 50 | Contributors (cumulative): 152 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1236,7 +1236,9 @@ def max(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to find maximum values.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to find maximum values. If `None` (default), finds the\n+            maximum over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1253,7 +1255,9 @@ def min(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to find minimum values.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to find minimum values. If `None` (default), finds the\n+            minimum over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1270,7 +1274,9 @@ def sum(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to sum over.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to sum over. If `None` (default), sums over all\n+            dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1287,7 +1293,9 @@ def prod(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to compute the product.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the product. If `None` (default), computes\n+            the product over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1330,7 +1338,9 @@ def var(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to compute the variance.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the variance. If `None` (default), computes\n+            the variance over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1353,7 +1363,9 @@ def std(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to compute the standard deviation.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the standard deviation. If `None` (default),\n+            computes the standard deviation over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`,\n@@ -1370,7 +1382,9 @@ def mean(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: A list of integer. Axes to compute the mean.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the mean. If `None` (default), computes\n+            the mean over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1 for each entry in `axis`. If `keepdims` is `True`,\n@@ -1389,7 +1403,9 @@ def any(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: Tensor or variable.\n-        axis: axis along which to perform the reduction.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the logical or. If `None` (default), computes\n+            the logical or over all dimensions.\n         keepdims: whether the drop or broadcast the reduction axes.\n \n     # Returns\n@@ -1404,7 +1420,9 @@ def all(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: Tensor or variable.\n-        axis: axis along which to perform the reduction.\n+        axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the logical and. If `None` (default), computes\n+            the logical and over all dimensions.\n         keepdims: whether the drop or broadcast the reduction axes.\n \n     # Returns\n@@ -1512,7 +1530,9 @@ def logsumexp(x, axis=None, keepdims=False):\n \n     # Arguments\n         x: A tensor or variable.\n-        axis: An integer, the axis to reduce over.\n+        axis: axis: An integer or list of integers in [-rank(x), rank(x)),\n+            the axes to compute the logsumexp. If `None` (default), computes\n+            the logsumexp over all dimensions.\n         keepdims: A boolean, whether to keep the dimensions or not.\n             If `keepdims` is `False`, the rank of the tensor is reduced\n             by 1. If `keepdims` is `True`, the reduced dimension is\n\n@@ -352,6 +352,7 @@ class TestBackend(object):\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)\n         check_single_tensor_operation('max', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('max', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('min', (4, 2), WITH_NP)\n         check_single_tensor_operation('min', (4, 2), WITH_NP, axis=1, keepdims=True)\n@@ -362,19 +363,30 @@ class TestBackend(object):\n         check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n         check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n+        check_single_tensor_operation('var', (4, 2), WITH_NP)\n+        check_single_tensor_operation('var', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('var', (4, 2, 3), WITH_NP, axis=[1, -1])\n+\n         check_single_tensor_operation('std', (4, 2), WITH_NP)\n         check_single_tensor_operation('std', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('std', (4, 2, 3), WITH_NP, axis=[1, -1])\n         # check_single_tensor_operation('std', (4, 2, 3), BACKENDS, axis=[1, -1])\n \n+        check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n+        check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n+\n         check_single_tensor_operation('prod', (4, 2), WITH_NP)\n         check_single_tensor_operation('prod', (4, 2), WITH_NP, axis=1, keepdims=True)\n         check_single_tensor_operation('prod', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('any', (4, 2), WITH_NP)\n         check_single_tensor_operation('any', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('any', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('all', (4, 2), WITH_NP)\n         check_single_tensor_operation('all', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('all', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('argmax', (4, 2), WITH_NP)\n         check_single_tensor_operation('argmax', (4, 2), WITH_NP, axis=1)\n\n@@ -5,6 +5,7 @@ from __future__ import print_function\n \n import numpy as np\n import scipy.signal as signal\n+import scipy as sp\n from keras.backend import floatx\n \n \n@@ -291,55 +292,49 @@ def categorical_crossentropy(target, output, from_logits=False):\n \n def max(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.max(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.max(x, axis=axis, keepdims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.min(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.min(x, axis=axis, keepdims=keepdims)\n \n \n def mean(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.mean(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.mean(x, axis=axis, keepdims=keepdims)\n \n \n+def var(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        axis = tuple(axis)\n+    return np.var(x, axis=axis, keepdims=keepdims)\n+\n+\n def std(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.std(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.std(x, axis=axis, keepdims=keepdims)\n \n \n+def logsumexp(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        axis = tuple(axis)\n+    return sp.misc.logsumexp(x, axis=axis, keepdims=keepdims)\n+\n+\n def sum(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.sum(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.sum(x, axis=axis, keepdims=keepdims)\n \n \n def prod(x, axis=None, keepdims=False):\n     if isinstance(axis, list):\n-        for a in axis:\n-            x = np.prod(x, axis=a, keepdims=keepdims)\n-        return x\n-    else:\n+        axis = tuple(axis)\n     return np.prod(x, axis=axis, keepdims=keepdims)\n \n \n@@ -352,10 +347,14 @@ def cumprod(x, axis=0):\n \n \n def any(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        axis = tuple(axis)\n     return np.any(x, axis=axis, keepdims=keepdims)\n \n \n def all(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        axis = tuple(axis)\n     return np.all(x, axis=axis, keepdims=keepdims)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a6af4bc6d44e9adbc2a21804bfcd18c4ce849ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 1092 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,7 +34,8 @@ def categorical_accuracy(y_true, y_pred):\n \n \n def sparse_categorical_accuracy(y_true, y_pred):\n-    return K.cast(K.equal(K.max(y_true, axis=-1),\n+    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\n+    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())\n \n\n@@ -47,6 +47,18 @@ def test_sparse_metrics():\n         assert K.eval(metric(y_a, y_b)).shape == (6,)\n \n \n+@keras_test\n+def test_sparse_categorical_accuracy_correctness():\n+    y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n+    y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n+    # use one_hot embedding to convert sparse labels to equivalent dense labels\n+    y_a_dense_labels = K.cast(K.one_hot(K.cast(y_a, dtype='int32'), num_classes=7),\n+                              dtype=K.floatx())\n+    sparse_categorical_acc = metrics.sparse_categorical_accuracy(y_a, y_b)\n+    categorical_acc = metrics.categorical_accuracy(y_a_dense_labels, y_b)\n+    assert np.allclose(K.eval(sparse_categorical_acc), K.eval(categorical_acc))\n+\n+\n def test_serialize():\n     '''This is a mock 'round trip' of serialize and deserialize.\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a4c5d8f10667e571c23f1b2f2e0397a85368bea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 719 | Lines Deleted: 156 | Files Changed: 6 | Hunks: 31 | Methods Changed: 32 | Complexity Δ (Sum/Max): 92/57 | Churn Δ: 875 | Churn Cumulative: 4769 | Contributors (this commit): 30 | Commits (past 90d): 21 | Contributors (cumulative): 49 | DMM Complexity: 0.5742296918767507\n\nDIFF:\n@@ -1255,6 +1255,13 @@ class Network(Layer):\n                                    positions=positions,\n                                    print_fn=print_fn)\n \n+    def __getstate__(self):\n+        return saving.pickle_model(self)\n+\n+    def __setstate__(self, state):\n+        model = saving.unpickle_model(state)\n+        self.__dict__.update(model.__dict__)\n+\n \n def _make_node_key(layer_name, node_index):\n     return layer_name + '_ib-' + str(node_index)\n\n@@ -14,6 +14,7 @@ from six.moves import zip\n from .. import backend as K\n from .. import optimizers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n+from ..utils.io_utils import h5dict\n from ..utils import conv_utils\n \n try:\n@@ -23,41 +24,20 @@ except ImportError:\n     h5py = None\n \n \n-def save_model(model, filepath, overwrite=True, include_optimizer=True):\n-    \"\"\"Save a model to a HDF5 file.\n+def _serialize_model(model, f, include_optimizer=True):\n+    \"\"\"Model serialization logic.\n \n-    Note: Please also see\n-    [How can I install HDF5 or h5py to save my models in Keras?](\n-        /getting-started/faq/\n-        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n-    in the FAQ for instructions on how to install `h5py`.\n-\n-    The saved model contains:\n-        - the model's configuration (topology)\n-        - the model's weights\n-        - the model's optimizer's state (if any)\n-\n-    Thus the saved model can be reinstantiated in\n-    the exact same state, without any of the code\n-    used for model definition or training.\n+    This method is used for both writing to HDF5 file/group,\n+    as well as pickling. This is achieved via a\n+    `keras.utils.hdf5_utls.H5Dict` object, which can wrap HDF5\n+    files, groups and dicts with a common API.\n \n     # Arguments\n-        model: Keras model instance to be saved.\n-        filepath: one of the following:\n-            - string, path where to save the model, or\n-            - h5py.File or h5py.Group object where to save the model\n-        overwrite: Whether we should overwrite any existing\n-            model at the target location, or instead\n-            ask the user with a manual prompt.\n-        include_optimizer: If True, save optimizer's state together.\n+        model: Keras model instance to be serialized.\n+        f: keras.utils.hdf5.HD5Dict instance.\n+        include_optimizer: If True, serialize optimizer's state together.\n \n-    # Raises\n-        ImportError: if h5py is not available.\n     \"\"\"\n-\n-    if h5py is None:\n-        raise ImportError('`save_model` requires h5py.')\n-\n     def get_json_type(obj):\n         \"\"\"Serialize any object to a JSON-serializable structure.\n \n@@ -95,31 +75,42 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n \n     from .. import __version__ as keras_version\n \n-    if not isinstance(filepath, h5py.Group):\n-        # If file exists and should not be overwritten.\n-        if not overwrite and os.path.isfile(filepath):\n-            proceed = ask_to_proceed_with_overwrite(filepath)\n-            if not proceed:\n-                return\n+    f['keras_version'] = str(keras_version).encode('utf8')\n+    f['backend'] = K.backend().encode('utf8')\n \n-        f = h5py.File(filepath, mode='w')\n-        opened_new_file = True\n-    else:\n-        f = filepath\n-        opened_new_file = False\n+    model_config = {}\n+    model_config['class_name'] = model.__class__.__name__\n+    model_config['config'] = model.get_config()\n+    model_config = json.dumps(model_config, default=get_json_type)\n+    model_config = model_config.encode('utf-8')\n+    f['model_config'] = model_config\n \n-    try:\n-        f.attrs['keras_version'] = str(keras_version).encode('utf8')\n-        f.attrs['backend'] = K.backend().encode('utf8')\n-        f.attrs['model_config'] = json.dumps({\n-            'class_name': model.__class__.__name__,\n-            'config': model.get_config()\n-        }, default=get_json_type).encode('utf8')\n-\n-        model_weights_group = f.create_group('model_weights')\n+    model_weights_group = f['model_weights']\n     model_layers = model.layers\n-        save_weights_to_hdf5_group(model_weights_group, model_layers)\n-\n+    model_weights_group['layer_names'] = [layer.name.encode('utf8') for layer in model_layers]\n+    model_weights_group['backend'] = K.backend().encode('utf8')\n+    model_weights_group['keras_version'] = str(keras_version).encode('utf8')\n+    for layer in model_layers:\n+        layer_group = model_weights_group[layer.name]\n+        symbolic_weights = layer.weights\n+        weight_values = K.batch_get_value(symbolic_weights)\n+        weight_names = []\n+        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n+            if hasattr(w, 'name') and w.name:\n+                name = str(w.name)\n+            else:\n+                name = 'param_' + str(i)\n+            if name in weight_names:\n+                idx = 2\n+                unique_name = name + '_1'\n+                while unique_name in weight_names:\n+                    unique_name = name + '_' + str(idx)\n+                    idx += 1\n+                name = unique_name\n+            weight_names.append(name.encode('utf8'))\n+        layer_group['weight_names'] = weight_names\n+        for name, val in zip(weight_names, weight_values):\n+            layer_group[name] = val\n     if include_optimizer and model.optimizer:\n         if isinstance(model.optimizer, optimizers.TFOptimizer):\n             warnings.warn(\n@@ -134,7 +125,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                 'Prefer using a Keras optimizer instead '\n                 '(see keras.io/optimizers).')\n         else:\n-                f.attrs['training_config'] = json.dumps({\n+            f['training_config'] = json.dumps({\n                 'optimizer_config': {\n                     'class_name': model.optimizer.__class__.__name__,\n                     'config': model.optimizer.get_config()\n@@ -144,12 +135,9 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                 'sample_weight_mode': model.sample_weight_mode,\n                 'loss_weights': model.loss_weights,\n             }, default=get_json_type).encode('utf8')\n-\n-                # Save optimizer weights.\n             symbolic_weights = getattr(model.optimizer, 'weights')\n             if symbolic_weights:\n-                    optimizer_weights_group = f.create_group(\n-                        'optimizer_weights')\n+                optimizer_weights_group = f['optimizer_weights']\n                 weight_values = K.batch_get_value(symbolic_weights)\n                 weight_names = []\n                 for i, (w, val) in enumerate(zip(symbolic_weights,\n@@ -169,20 +157,228 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                             name = str(w.name)\n                         else:\n                             name = 'param_' + str(i)\n+                    if name in weight_names:\n+                        idx = 2\n+                        unique_name = name + '_1'\n+                        while unique_name in weight_names:\n+                            unique_name = name + '_' + str(idx)\n+                            idx += 1\n+                        name = unique_name\n                     weight_names.append(name.encode('utf8'))\n-                    optimizer_weights_group.attrs[\n-                        'weight_names'] = weight_names\n+                optimizer_weights_group['weight_names'] = weight_names\n                 for name, val in zip(weight_names, weight_values):\n-                        param_dset = optimizer_weights_group.create_dataset(\n-                            name,\n-                            val.shape,\n-                            dtype=val.dtype)\n-                        if not val.shape:\n-                            # scalar\n-                            param_dset[()] = val\n+                    optimizer_weights_group[name] = val\n+\n+\n+def _deserialize_model(f, custom_objects=None, compile=True):\n+    \"\"\"De-serializes a model serialized via _serialize_model\n+\n+    # Arguments\n+        f: `keras.utils.hdf5_utils.HFDict` instance.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+        compile: Boolean, whether to compile the model\n+            after loading.\n+\n+    # Returns\n+        A Keras model instance. If an optimizer was found\n+        as part of the saved model, the model is already\n+        compiled. Otherwise, the model is uncompiled and\n+        a warning will be displayed. When `compile` is set\n+        to False, the compilation is omitted without any\n+        warning.\n+    \"\"\"\n+    if not custom_objects:\n+        custom_objects = {}\n+\n+    def convert_custom_objects(obj):\n+        \"\"\"Handles custom object lookup.\n+\n+        # Arguments\n+            obj: object, dict, or list.\n+\n+        # Returns\n+            The same structure, where occurrences\n+                of a custom object name have been replaced\n+                with the custom object.\n+        \"\"\"\n+        if isinstance(obj, list):\n+            deserialized = []\n+            for value in obj:\n+                deserialized.append(convert_custom_objects(value))\n+            return deserialized\n+        if isinstance(obj, dict):\n+            deserialized = {}\n+            for key, value in obj.items():\n+                deserialized[key] = convert_custom_objects(value)\n+            return deserialized\n+        if obj in custom_objects:\n+            return custom_objects[obj]\n+        return obj\n+\n+    model_config = f['model_config']\n+    if model_config is None:\n+        raise ValueError('No model found in config.')\n+    model_config = json.loads(model_config.decode('utf-8'))\n+    model = model_from_config(model_config, custom_objects=custom_objects)\n+    model_weights_group = f['model_weights']\n+\n+    if 'keras_version' in model_weights_group:\n+        original_keras_version = model_weights_group['keras_version'].decode('utf8')\n     else:\n-                            param_dset[:] = val\n-        f.file.flush()\n+        original_keras_version = '1'\n+    if 'backend' in model_weights_group:\n+        original_backend = model_weights_group['backend'].decode('utf8')\n+    else:\n+        original_backend = None\n+\n+    layer_names = model_weights_group['layer_names']\n+\n+    layers = model.layers\n+\n+    filtered_layers = []\n+    for layer in layers:\n+        weights = layer.weights\n+        if weights:\n+            filtered_layers.append(layer)\n+\n+    filtered_layer_names = []\n+    for name in layer_names:\n+        layer_weights = model_weights_group[name]\n+        weight_names = layer_weights['weight_names']\n+        if weight_names:\n+            filtered_layer_names.append(name)\n+\n+    layer_names = filtered_layer_names\n+    if len(layer_names) != len(filtered_layers):\n+        raise ValueError('You are trying to load a weight file'\n+                         ' containing {} layers into a model with {} layers'\n+                         .format(len(layer_names), len(filtered_layers))\n+                         )\n+\n+    # We batch weight value assignments in a single backend call\n+    # which provides a speedup in TensorFlow.\n+    weight_value_tuples = []\n+    for k, name in enumerate(layer_names):\n+        layer_weights = model_weights_group[name]\n+        weight_names = layer_weights['weight_names']\n+        weight_values = [layer_weights[weight_name] for weight_name in weight_names]\n+        layer = filtered_layers[k]\n+        symbolic_weights = layer.weights\n+        weight_values = preprocess_weights_for_loading(layer,\n+                                                       weight_values,\n+                                                       original_keras_version,\n+                                                       original_backend,\n+                                                       reshape=False)\n+        if len(weight_values) != len(symbolic_weights):\n+            raise ValueError('Layer #' + str(k) +\n+                             ' (named \"' + layer.name +\n+                             '\" in the current model) was found to '\n+                             'correspond to layer ' + name +\n+                             ' in the save file. '\n+                             'However the new layer ' + layer.name +\n+                             ' expects ' + str(len(symbolic_weights)) +\n+                             ' weights, but the saved weights have ' +\n+                             str(len(weight_values)) +\n+                             ' elements.')\n+        weight_value_tuples += zip(symbolic_weights, weight_values)\n+    K.batch_set_value(weight_value_tuples)\n+\n+    if compile:\n+        training_config = f.get('training_config')\n+        if training_config is None:\n+            warnings.warn('No training configuration found in save file: '\n+                          'the model was *not* compiled. '\n+                          'Compile it manually.')\n+            return model\n+        training_config = json.loads(training_config.decode('utf-8'))\n+        optimizer_config = training_config['optimizer_config']\n+        optimizer = optimizers.deserialize(optimizer_config,\n+                                           custom_objects=custom_objects)\n+\n+        # Recover loss functions and metrics.\n+        loss = convert_custom_objects(training_config['loss'])\n+        metrics = convert_custom_objects(training_config['metrics'])\n+        sample_weight_mode = training_config['sample_weight_mode']\n+        loss_weights = training_config['loss_weights']\n+\n+        # Compile model.\n+        model.compile(optimizer=optimizer,\n+                      loss=loss,\n+                      metrics=metrics,\n+                      loss_weights=loss_weights,\n+                      sample_weight_mode=sample_weight_mode)\n+\n+        # Set optimizer weights.\n+        if 'optimizer_weights' in f:\n+            # Build train function (to get weight updates).\n+            model._make_train_function()\n+            optimizer_weights_group = f['optimizer_weights']\n+            optimizer_weight_names = [\n+                n.decode('utf8') for n in\n+                optimizer_weights_group['weight_names']]\n+            optimizer_weight_values = [optimizer_weights_group[n] for n in\n+                                       optimizer_weight_names]\n+            try:\n+                model.optimizer.set_weights(optimizer_weight_values)\n+            except ValueError:\n+                warnings.warn('Error in loading the saved optimizer '\n+                              'state. As a result, your model is '\n+                              'starting with a freshly initialized '\n+                              'optimizer.')\n+\n+    return model\n+\n+\n+def save_model(model, filepath, overwrite=True, include_optimizer=True):\n+    \"\"\"Save a model to a HDF5 file.\n+\n+    Note: Please also see\n+    [How can I install HDF5 or h5py to save my models in Keras?](\n+        /getting-started/faq/\n+        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n+    in the FAQ for instructions on how to install `h5py`.\n+\n+    The saved model contains:\n+        - the model's configuration (topology)\n+        - the model's weights\n+        - the model's optimizer's state (if any)\n+\n+    Thus the saved model can be reinstantiated in\n+    the exact same state, without any of the code\n+    used for model definition or training.\n+\n+    # Arguments\n+        model: Keras model instance to be saved.\n+        filepath: one of the following:\n+            - string, path where to save the model, or\n+            - h5py.File or h5py.Group object where to save the model\n+        overwrite: Whether we should overwrite any existing\n+            model at the target location, or instead\n+            ask the user with a manual prompt.\n+        include_optimizer: If True, save optimizer's state together.\n+\n+    # Raises\n+        ImportError: if h5py is not available.\n+    \"\"\"\n+    if h5py is None:\n+        raise ImportError('`save_model` requires h5py.')\n+\n+    if not isinstance(filepath, h5py.Group):\n+        # If file exists and should not be overwritten.\n+        if not overwrite and os.path.isfile(filepath):\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        opened_new_file = True\n+    else:\n+        opened_new_file = False\n+\n+    f = h5dict(filepath, mode='w')\n+\n+    try:\n+        _serialize_model(model, f, include_optimizer)\n     finally:\n         if opened_new_file:\n             f.close()\n@@ -215,102 +411,29 @@ def load_model(filepath, custom_objects=None, compile=True):\n     \"\"\"\n     if h5py is None:\n         raise ImportError('`load_model` requires h5py.')\n-\n-    if not custom_objects:\n-        custom_objects = {}\n-\n-    def convert_custom_objects(obj):\n-        \"\"\"Handles custom object lookup.\n-\n-        # Arguments\n-            obj: object, dict, or list.\n-\n-        # Returns\n-            The same structure, where occurrences\n-                of a custom object name have been replaced\n-                with the custom object.\n-        \"\"\"\n-        if isinstance(obj, list):\n-            deserialized = []\n-            for value in obj:\n-                deserialized.append(convert_custom_objects(value))\n-            return deserialized\n-        if isinstance(obj, dict):\n-            deserialized = {}\n-            for key, value in obj.items():\n-                deserialized[key] = convert_custom_objects(value)\n-            return deserialized\n-        if obj in custom_objects:\n-            return custom_objects[obj]\n-        return obj\n-\n-    opened_new_file = not isinstance(filepath, h5py.Group)\n-    if opened_new_file:\n-        f = h5py.File(filepath, mode='r')\n-    else:\n-        f = filepath\n-\n     model = None\n+    opened_new_file = not isinstance(filepath, h5py.Group)\n+    f = h5dict(filepath, 'r')\n     try:\n-        # instantiate model\n-        model_config = f.attrs.get('model_config')\n-        if model_config is None:\n-            raise ValueError('No model found in config file.')\n-        model_config = json.loads(model_config.decode('utf-8'))\n-        model = model_from_config(model_config, custom_objects=custom_objects)\n-\n-        # set weights\n-        load_weights_from_hdf5_group(f['model_weights'], model.layers)\n-\n-        if compile:\n-            # instantiate optimizer\n-            training_config = f.attrs.get('training_config')\n-            if training_config is None:\n-                warnings.warn('No training configuration found in save file: '\n-                              'the model was *not* compiled. '\n-                              'Compile it manually.')\n-                return model\n-            training_config = json.loads(training_config.decode('utf-8'))\n-            optimizer_config = training_config['optimizer_config']\n-            optimizer = optimizers.deserialize(optimizer_config,\n-                                               custom_objects=custom_objects)\n-\n-            # Recover loss functions and metrics.\n-            loss = convert_custom_objects(training_config['loss'])\n-            metrics = convert_custom_objects(training_config['metrics'])\n-            sample_weight_mode = training_config['sample_weight_mode']\n-            loss_weights = training_config['loss_weights']\n-\n-            # Compile model.\n-            model.compile(optimizer=optimizer,\n-                          loss=loss,\n-                          metrics=metrics,\n-                          loss_weights=loss_weights,\n-                          sample_weight_mode=sample_weight_mode)\n-\n-            # Set optimizer weights.\n-            if 'optimizer_weights' in f:\n-                # Build train function (to get weight updates).\n-                model._make_train_function()\n-                optimizer_weights_group = f['optimizer_weights']\n-                optimizer_weight_names = [\n-                    n.decode('utf8') for n in\n-                    optimizer_weights_group.attrs['weight_names']]\n-                optimizer_weight_values = [optimizer_weights_group[n] for n in\n-                                           optimizer_weight_names]\n-                try:\n-                    model.optimizer.set_weights(optimizer_weight_values)\n-                except ValueError:\n-                    warnings.warn('Error in loading the saved optimizer '\n-                                  'state. As a result, your model is '\n-                                  'starting with a freshly initialized '\n-                                  'optimizer.')\n+        model = _deserialize_model(f, custom_objects, compile)\n     finally:\n         if opened_new_file:\n             f.close()\n     return model\n \n \n+def pickle_model(model):\n+    d = {}\n+    f = h5dict(d)\n+    _serialize_model(model, f)\n+    return d\n+\n+\n+def unpickle_model(state):\n+    f = h5dict(state, mode='r')\n+    return _deserialize_model(f)\n+\n+\n def model_from_config(config, custom_objects=None):\n     \"\"\"Instantiates a Keras model from its config.\n \n\n@@ -7,6 +7,7 @@ from . import conv_utils\n \n # Globally-importable utils.\n from .io_utils import HDF5Matrix\n+from .io_utils import h5dict\n from .data_utils import get_file\n from .data_utils import Sequence\n from .data_utils import GeneratorEnqueuer\n\n@@ -5,14 +5,23 @@ from __future__ import print_function\n \n import numpy as np\n from collections import defaultdict\n+import sys\n+\n \n import six\n try:\n     import h5py\n+    HDF5_OBJECT_HEADER_LIMIT = 64512\n except ImportError:\n     h5py = None\n \n \n+if sys.version_info[0] == 3:\n+    import pickle\n+else:\n+    import cPickle as pickle\n+\n+\n class HDF5Matrix(object):\n     \"\"\"Representation of HDF5 dataset to be used instead of a Numpy array.\n \n@@ -155,3 +164,186 @@ def ask_to_proceed_with_overwrite(filepath):\n         return False\n     print('[TIP] Next time specify overwrite=True!')\n     return True\n+\n+\n+class H5Dict(object):\n+    \"\"\" A dict-like wrapper around h5py groups (or dicts).\n+\n+    This allows us to have a single serialization logic\n+    for both pickling and saving to disk.\n+\n+    Note: This is not intended to be a generic wrapper.\n+    There are lot of edge cases which have been hardcoded,\n+    and makes sense only in the context of model serialization/\n+    deserialization.\n+    \"\"\"\n+\n+    def __init__(self, path, mode='a'):\n+        if isinstance(path, h5py.Group):\n+            self.data = path\n+            self._is_file = False\n+        elif isinstance(path, str):\n+            self.data = h5py.File(path,)\n+            self._is_file = True\n+        elif isinstance(path, dict):\n+            self.data = path\n+            self._is_file = False\n+            # Flag to check if a dict is user defined data or a sub group:\n+            self.data['_is_group'] = True\n+        else:\n+            raise TypeError('Required Group, str or dict.'\n+                            ' Received {}.'.format(type(path)))\n+        self.read_only = mode == 'r'\n+\n+    def __setitem__(self, attr, val):\n+        if self.read_only:\n+            raise ValueError('Can not set item in read only mode.')\n+        is_np = type(val).__module__ == np.__name__\n+        if isinstance(self.data, dict):\n+            if isinstance(attr, bytes):\n+                attr = attr.decode('utf-8')\n+            if is_np:\n+                self.data[attr] = pickle.dumps(val)\n+                # We have to remember to unpickle in __getitem__\n+                self.data['_{}_pickled'.format(attr)] = True\n+            else:\n+                self.data[attr] = val\n+            return\n+        if attr in self:\n+            raise KeyError('Can not set attribute.'\n+                           ' Group with name {} exists.'.format(attr))\n+        if is_np:\n+            dataset = self.data.create_dataset(attr, val.shape, dtype=val.dtype)\n+            if not val.shape:\n+                # scalar\n+                dataset[()] = val\n+            else:\n+                dataset[:] = val\n+        if isinstance(val, list):\n+            # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n+            # because in that case even chunking the array would not make the saving\n+            # possible.\n+            bad_attributes = [x for x in val if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n+\n+            # Expecting this to never be true.\n+            if len(bad_attributes) > 0:\n+                raise RuntimeError('The following attributes cannot be saved to '\n+                                   'HDF5 file because they are larger than '\n+                                   '%d bytes: %s' % (HDF5_OBJECT_HEADER_LIMIT,\n+                                                     ', '.join(bad_attributes)))\n+\n+            if val and sys.version_info[0] == 3 and isinstance(val[0], str):\n+                # convert to bytes\n+                val = [x.encode('utf-8') for x in val]\n+\n+            data_npy = np.asarray(val)\n+\n+            num_chunks = 1\n+            chunked_data = np.array_split(data_npy, num_chunks)\n+\n+            # This will never loop forever thanks to the test above.\n+            is_too_big = lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT\n+            while any(map(is_too_big, chunked_data)):\n+                num_chunks += 1\n+                chunked_data = np.array_split(data_npy, num_chunks)\n+\n+            if num_chunks > 1:\n+                for chunk_id, chunk_data in enumerate(chunked_data):\n+                    self.data.attrs['%s%d' % (attr, chunk_id)] = chunk_data\n+            else:\n+                self.data.attrs[attr] = val\n+        else:\n+            self.data.attrs[attr] = val\n+\n+    def __getitem__(self, attr):\n+        if isinstance(self.data, dict):\n+            if isinstance(attr, bytes):\n+                attr = attr.decode('utf-8')\n+            if attr in self.data:\n+                val = self.data[attr]\n+                if isinstance(val, dict) and val.get('_is_group'):\n+                    val = H5Dict(val)\n+                elif '_{}_pickled'.format(attr) in self.data:\n+                    val = pickle.loads(val)\n+                return val\n+            else:\n+                if self.read_only:\n+                    raise ValueError('Can not create group in read only mode.')\n+                val = {'_is_group': True}\n+                self.data[attr] = val\n+                return H5Dict(val)\n+        if attr in self.data.attrs:\n+            val = self.data.attrs[attr]\n+            if type(val).__module__ == np.__name__:\n+                if val.dtype.type == np.string_:\n+                    val = val.tolist()\n+        elif attr in self.data:\n+            val = self.data[attr]\n+            if isinstance(val, h5py.Dataset):\n+                val = np.asarray(val)\n+            else:\n+                val = H5Dict(val)\n+        else:\n+            # could be chunked\n+            chunk_attr = '%s%d' % (attr, 0)\n+            is_chunked = chunk_attr in self.data.attrs\n+            if is_chunked:\n+                val = []\n+                chunk_id = 0\n+                while chunk_attr in self.data.attrs:\n+                    chunk = self.data.attrs[chunk_attr]\n+                    val.extend([x.decode('utf8') for x in chunk])\n+                    chunk_id += 1\n+                    chunk_attr = '%s%d' % (attr, chunk_id)\n+            else:\n+                if self.read_only:\n+                    raise ValueError('Can not create group in read only mode.')\n+                val = H5Dict(self.data.create_group(attr))\n+        return val\n+\n+    def __len__(self):\n+        return len(self.data)\n+\n+    def __iter__(self):\n+        return iter(self.data)\n+\n+    def iter(self):\n+        return iter(self.data)\n+\n+    def __getattr__(self, attr):\n+\n+        def wrapper(f):\n+            def h5wrapper(*args, **kwargs):\n+                out = f(*args, **kwargs)\n+                if isinstance(self.data, type(out)):\n+                    return H5Dict(out)\n+                else:\n+                    return out\n+            return h5wrapper\n+\n+        return wrapper(getattr(self.data, attr))\n+\n+    def close(self):\n+        if isinstance(self.data, h5py.Group):\n+            self.data.file.flush()\n+            if self._is_file:\n+                self.data.close()\n+\n+    def update(self, *args):\n+        if isinstance(self.data, dict):\n+            self.data.update(*args)\n+        raise NotImplementedError\n+\n+    def __contains__(self, key):\n+        if isinstance(self.data, dict):\n+            return key in self.data\n+        else:\n+            return (key in self.data) or (key in self.data.attrs)\n+\n+    def get(self, key, default=None):\n+        if key in self:\n+            return self[key]\n+        return default\n+\n+\n+h5dict = H5Dict\n\n@@ -6,7 +6,9 @@ import pytest\n from keras.models import Sequential\n from keras.layers import Dense\n from keras.utils.io_utils import HDF5Matrix\n+from keras.utils.io_utils import h5dict\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n+from numpy.testing import assert_allclose\n import numpy as np\n import six\n import warnings\n@@ -133,5 +135,74 @@ def test_ask_to_proceed_with_overwrite():\n         assert not ask_to_proceed_with_overwrite('/tmp/not_exists')\n \n \n+def test_h5dict_attrs(h5_path='test.h5'):\n+    # test both HDF5 and dict implementations\n+    paths = [h5_path, dict()]\n+\n+    for path in paths:\n+        f = h5dict(path, mode='w')\n+\n+        # str\n+        f['x'] = 'abcd'\n+\n+        # list<bytes>\n+        f['y'] = [b'efg', b'hij', b'klmn']\n+\n+        # ndarray\n+        array = np.random.random((3, 4, 5))\n+        f['z'] = array\n+\n+        f.close()\n+\n+        f = h5dict(path, mode='r')\n+\n+        assert f['x'] == 'abcd'\n+        assert f['y'] == [b'efg', b'hij', b'klmn']\n+        assert_allclose(f['z'], array)\n+\n+        f.close()\n+\n+\n+def test_h5dict_groups(h5_path='test.h5'):\n+    # test both HDF5 and dict implementations\n+    paths = [h5_path, dict()]\n+\n+    for path in paths:\n+        f = h5dict(path, mode='w')\n+\n+        group1 = f['group1']\n+        group2 = group1['group2']\n+\n+        group2['x'] = 'abcd'\n+\n+        group3 = group2['group3']\n+        group3['y'] = [b'efg', b'hij', b'klmn']\n+\n+        group4 = group3['group4']\n+        array = np.random.random((3, 4, 5))\n+        group4['z'] = array\n+\n+        f.close()\n+\n+        f = h5dict(path, mode='r')\n+\n+        assert 'group1' in f\n+        group1 = f['group1']\n+\n+        assert 'group2' in group1\n+        group2 = group1['group2']\n+        assert group2['x'] == 'abcd'\n+\n+        assert 'group3' in group2\n+        group3 = group2['group3']\n+        assert group3['y'] == [b'efg', b'hij', b'klmn']\n+\n+        assert 'group4' in group3\n+        group4 = group3['group4']\n+        assert_allclose(group4['z'], array)\n+\n+        f.close()\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -0,0 +1,169 @@\n+import pytest\n+import os\n+import sys\n+import tempfile\n+import numpy as np\n+from numpy.testing import assert_allclose\n+from numpy.testing import assert_raises\n+\n+from keras import backend as K\n+from keras.models import Model, Sequential\n+from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed\n+from keras.layers import Input\n+from keras import optimizers\n+from keras import losses\n+from keras import metrics\n+from keras.utils.test_utils import keras_test\n+\n+if sys.version_info[0] == 3:\n+    import pickle\n+else:\n+    import cPickle as pickle\n+\n+\n+skipif_no_tf_gpu = pytest.mark.skipif(\n+    (K.backend() != 'tensorflow') or\n+    (not K.tensorflow_backend._get_available_gpus()),\n+    reason='Requires TensorFlow backend and a GPU')\n+\n+\n+@keras_test\n+def test_sequential_model_pickling():\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    model.add(RepeatVector(3))\n+    model.add(TimeDistributed(Dense(3)))\n+    model.compile(loss=losses.MSE,\n+                  optimizer=optimizers.RMSprop(lr=0.0001),\n+                  metrics=[metrics.categorical_accuracy],\n+                  sample_weight_mode='temporal')\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+\n+    state = pickle.dumps(model)\n+\n+    new_model = pickle.loads(state)\n+\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+    # test that new updates are the same with both models\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3, 3))\n+    model.train_on_batch(x, y)\n+    new_model.train_on_batch(x, y)\n+    out = model.predict(x)\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n+@keras_test\n+def test_sequential_model_pickling_2():\n+    # test with custom optimizer, loss\n+    custom_opt = optimizers.rmsprop\n+    custom_loss = losses.mse\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    model.add(Dense(3))\n+    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n+\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+\n+    state = pickle.dumps(model)\n+    model = pickle.loads(state)\n+\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n+@keras_test\n+def test_functional_model_pickling():\n+    inputs = Input(shape=(3,))\n+    x = Dense(2)(inputs)\n+    outputs = Dense(3)(x)\n+\n+    model = Model(inputs, outputs)\n+    model.compile(loss=losses.MSE,\n+                  optimizer=optimizers.Adam(),\n+                  metrics=[metrics.categorical_accuracy])\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    state = pickle.dumps(model)\n+\n+    model = pickle.loads(state)\n+\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n+@keras_test\n+def test_pickling_multiple_metrics_outputs():\n+    inputs = Input(shape=(5,))\n+    x = Dense(5)(inputs)\n+    output1 = Dense(1, name='output1')(x)\n+    output2 = Dense(1, name='output2')(x)\n+\n+    model = Model(inputs=inputs, outputs=[output1, output2])\n+\n+    metrics = {'output1': ['mse', 'binary_accuracy'],\n+               'output2': ['mse', 'binary_accuracy']\n+               }\n+    loss = {'output1': 'mse', 'output2': 'mse'}\n+\n+    model.compile(loss=loss, optimizer='sgd', metrics=metrics)\n+\n+    # assure that model is working\n+    x = np.array([[1, 1, 1, 1, 1]])\n+    out = model.predict(x)\n+\n+    model = pickle.loads(pickle.dumps(model))\n+\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n+@keras_test\n+def test_pickling_without_compilation():\n+    \"\"\"Test pickling model without compiling.\n+    \"\"\"\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    model.add(Dense(3))\n+\n+    model = pickle.loads(pickle.dumps(model))\n+\n+\n+@keras_test\n+def test_pickling_right_after_compilation():\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    model.add(Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+    model._make_train_function()\n+\n+    model = pickle.loads(pickle.dumps(model))\n+\n+\n+@keras_test\n+def test_pickling_unused_layers_is_ok():\n+    a = Input(shape=(256, 512, 6))\n+    b = Input(shape=(256, 512, 1))\n+    c = Lambda(lambda x: x[:, :, :, :1])(a)\n+\n+    model = Model(inputs=[a, b], outputs=c)\n+\n+    model = pickle.loads(pickle.dumps(model))\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
