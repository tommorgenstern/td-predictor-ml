{"custom_id": "scrapy#dcc90fc19698b4a18e1d163fac5c0aba98119df6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,4 +8,3 @@ class GoogledirItem(Item):\n \n     def __str__(self):\n         return \"Google Category: name=%s url=%s\" % (self['name'], self['url'])\n-\n\n@@ -1,6 +1,7 @@\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.contrib.spiders import CrawlSpider, Rule\n+from scrapy.contrib.loader import XPathItemLoader\n from googledir.items import GoogledirItem\n \n class GoogleDirectorySpider(CrawlSpider):\n@@ -16,18 +17,25 @@ class GoogleDirectorySpider(CrawlSpider):\n     )\n     \n     def parse_category(self, response):\n-        # The selector we're going to use in order to extract data from the page\n-        hxs = HtmlXPathSelector(response)\n+        # The main selector we're using to extract data from the page\n+        main_selector = HtmlXPathSelector(response)\n \n-        # The path to website links in directory page\n-        links = hxs.select('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n+        # The XPath to website links in the directory page\n+        xpath = '//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font'\n \n-        for link in links:\n+        # Get a list of (sub) selectors to each website node pointed by the XPath\n+        sub_selectors = main_selector.select(xpath)\n+\n+        # Iterate over the sub-selectors to extract data for each website\n+        for selector in sub_selectors:\n             item = GoogledirItem()\n \n-            item['name'] = link.select('a/text()').extract()\n-            item['url'] = link.select('a/@href').extract()\n-            item['description'] = link.select('font[2]/text()').extract()\n-            yield item\n+            l = XPathItemLoader(item=item, selector=selector)\n+            l.add_xpath('name', 'a/text()')\n+            l.add_xpath('url', 'a/@href')\n+            l.add_xpath('description', 'font[2]/text()')\n+\n+            # Here we populate the item and yield it\n+            yield l.load_item()\n \n SPIDER = GoogleDirectorySpider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33b53c59d5b3fbf78335766f608510aa4cb5eef0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 167 | Files Changed: 14 | Hunks: 16 | Methods Changed: 24 | Complexity Δ (Sum/Max): 6/38 | Churn Δ: 339 | Churn Cumulative: 1369 | Contributors (this commit): 3 | Commits (past 90d): 56 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ from twisted.internet import reactor, threads\n \n from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n-from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n\n@@ -8,7 +8,7 @@ import urlparse\n from scrapy.link import Link\n from scrapy.utils.url import canonicalize_url, urljoin_rfc\n from scrapy.utils.python import unicode_to_str, flatten\n-from scrapy.xpath.selector import XPathSelectorList, HtmlXPathSelector\n+from scrapy.selector import XPathSelectorList, HtmlXPathSelector\n \n class HTMLImageLinkExtractor(object):\n     '''HTMLImageLinkExtractor objects are intended to extract image links from HTML pages\n\n@@ -4,7 +4,7 @@ SGMLParser-based Link extractors\n \n import re\n \n-from scrapy.xpath import HtmlXPathSelector\n+from scrapy.selector import HtmlXPathSelector\n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/loaders.rst\n from collections import defaultdict\n \n from scrapy.item import Item\n-from scrapy.xpath import HtmlXPathSelector\n+from scrapy.selector import HtmlXPathSelector\n from scrapy.utils.misc import arg_to_iter\n from .common import wrap_loader_context\n from .processor import Identity\n\n@@ -9,7 +9,7 @@ from scrapy.contrib.spiders.init import InitSpider\n from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.core.exceptions import NotConfigured, NotSupported\n \n class XMLFeedSpider(InitSpider):\n\n@@ -0,0 +1,153 @@\n+\"\"\"\n+XPath selectors \n+\n+See documentation in docs/ref/selectors.rst\n+\"\"\"\n+\n+import libxml2\n+\n+from scrapy.http import TextResponse\n+from scrapy.utils.python import flatten, unicode_to_str\n+from scrapy.utils.misc import extract_regex\n+from scrapy.utils.trackref import object_ref\n+from scrapy.utils.decorator import deprecated\n+from .factories import xmlDoc_from_html, xmlDoc_from_xml\n+from .document import Libxml2Document\n+\n+__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n+    'XPathSelectorList']\n+\n+class XPathSelector(object_ref):\n+\n+    __slots__ = ['doc', 'xmlNode', 'expr', '__weakref__']\n+\n+    def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n+        if parent:\n+            self.doc = parent.doc\n+            self.xmlNode = node\n+        elif response:\n+            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n+            self.xmlNode = self.doc.xmlDoc\n+        elif text:\n+            response = TextResponse(url=None, body=unicode_to_str(text), \\\n+                encoding='utf-8')\n+            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n+            self.xmlNode = self.doc.xmlDoc\n+        self.expr = expr\n+\n+    def select(self, xpath):\n+        \"\"\"Perform the given XPath query on the current XPathSelector and\n+        return a XPathSelectorList of the result\"\"\"\n+        if hasattr(self.xmlNode, 'xpathEval'):\n+            self.doc.xpathContext.setContextNode(self.xmlNode)\n+            try:\n+                xpath_result = self.doc.xpathContext.xpathEval(xpath)\n+            except libxml2.xpathError:\n+                raise ValueError(\"Invalid XPath: %s\" % xpath)\n+            if hasattr(xpath_result, '__iter__'):\n+                return XPathSelectorList([self.__class__(node=node, parent=self, \\\n+                    expr=xpath) for node in xpath_result])\n+            else:\n+                return XPathSelectorList([self.__class__(node=xpath_result, \\\n+                    parent=self, expr=xpath)])\n+        else:\n+            return XPathSelectorList([])\n+\n+    def re(self, regex):\n+        \"\"\"Return a list of unicode strings by applying the regex over all\n+        current XPath selections, and flattening the results\"\"\"\n+        return extract_regex(regex, self.extract(), 'utf-8')\n+\n+    def extract(self):\n+        \"\"\"Return a unicode string of the content referenced by the XPathSelector\"\"\"\n+        if isinstance(self.xmlNode, basestring):\n+            text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n+        elif hasattr(self.xmlNode, 'serialize'):\n+            if isinstance(self.xmlNode, libxml2.xmlDoc):\n+                data = self.xmlNode.getRootElement().serialize('utf-8')\n+                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n+            elif isinstance(self.xmlNode, libxml2.xmlAttr): \n+                # serialization doesn't work sometimes for xmlAttr types\n+                text = unicode(self.xmlNode.content, 'utf-8', errors='ignore')\n+            else:\n+                data = self.xmlNode.serialize('utf-8')\n+                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n+        else:\n+            try:\n+                text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n+            except TypeError:  # catched when self.xmlNode is a float - see tests\n+                text = unicode(self.xmlNode)\n+        return text\n+\n+    def extract_unquoted(self):\n+        \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n+        if self.select('self::text()'):\n+            return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')\n+        else:\n+            return u''\n+\n+    def register_namespace(self, prefix, uri):\n+        \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n+        self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n+\n+    def _get_libxml2_doc(self, response):\n+        \"\"\"Return libxml2 document (xmlDoc) from response\"\"\"\n+        return xmlDoc_from_html(response)\n+\n+    def __nonzero__(self):\n+        return bool(self.extract())\n+\n+    def __str__(self):\n+        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, \\\n+            'name', type(self.xmlNode).__name__), self.expr)\n+\n+    __repr__ = __str__\n+\n+    @deprecated(use_instead='XPathSelector.select')\n+    def __call__(self, xpath):\n+        return self.select(xpath)\n+\n+    @deprecated(use_instead='XPathSelector.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n+\n+\n+class XPathSelectorList(list):\n+    \"\"\"List of XPathSelector objects\"\"\"\n+\n+    def __getslice__(self, i, j):\n+        return XPathSelectorList(list.__getslice__(self, i, j))\n+\n+    def select(self, xpath):\n+        \"\"\"Perform the given XPath query on each XPathSelector of the list and\n+        return a new (flattened) XPathSelectorList of the results\"\"\"\n+        return XPathSelectorList(flatten([x.select(xpath) for x in self]))\n+\n+    def re(self, regex):\n+        \"\"\"Perform the re() method on each XPathSelector of the list, and\n+        return the result as a flattened list of unicode strings\"\"\"\n+        return flatten([x.re(regex) for x in self])\n+\n+    def extract(self):\n+        \"\"\"Return a list of unicode strings with the content referenced by each\n+        XPathSelector of the list\"\"\"\n+        return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n+\n+    def extract_unquoted(self):\n+        return [x.extract_unquoted() if isinstance(x, XPathSelector) else x for x in self]\n+\n+    @deprecated(use_instead='XPathSelectorList.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n+\n+\n+class XmlXPathSelector(XPathSelector):\n+    \"\"\"XPathSelector for XML content\"\"\"\n+    __slots__ = ()\n+    _get_libxml2_doc = staticmethod(xmlDoc_from_xml)\n+\n+\n+class HtmlXPathSelector(XPathSelector):\n+    \"\"\"XPathSelector for HTML content\"\"\"\n+    __slots__ = ()\n+    _get_libxml2_doc = staticmethod(xmlDoc_from_html)\n\n@@ -5,7 +5,7 @@ garbage collection to libxml2 documents (xmlDoc).\n \n import weakref\n \n-from scrapy.xpath.factories import xmlDoc_from_html\n+from .factories import xmlDoc_from_html\n \n class Libxml2Document(object):\n \n\n\n@@ -4,7 +4,7 @@ from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n     Compose, MapCompose\n from scrapy.item import Item, Field\n-from scrapy.xpath import HtmlXPathSelector\n+from scrapy.selector import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n \n # test items\n\n@@ -5,9 +5,9 @@ import weakref\n import libxml2\n \n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, \\\n+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector, \\\n     XPathSelector\n-from scrapy.xpath.document import Libxml2Document\n+from scrapy.selector.document import Libxml2Document\n from scrapy.utils.test import libxml2debug\n \n class XPathSelectorTestCase(unittest.TestCase):\n\n@@ -1,7 +1,7 @@\n import re, csv\n \n from scrapy.http import Response\n-from scrapy.xpath import XmlXPathSelector\n+from scrapy.selector import XmlXPathSelector\n from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n from scrapy.utils.response import body_or_str\n\n@@ -6,7 +6,7 @@ import os\n \n import libxml2\n \n-from scrapy.xpath.document import Libxml2Document\n+from scrapy.selector.document import Libxml2Document\n \n def libxml2debug(testfunction):\n     \"\"\"Decorator for debugging libxml2 memory leaks inside a function.\n\n@@ -1,9 +1,5 @@\n-\"\"\"\n-The scrapy.xpath module provides useful classes for selecting HTML and XML\n-documents using XPath. It requires libxml2 and its python bindings.\n+from scrapy.selector import *\n \n-This parent module exports the classes most commonly used when building\n-spiders, for convenience.\n-\"\"\"\n-\n-from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n+import warnings\n+warnings.warn(\"scrapy.xpath module is deprecated, use scrapy.selector instead\",\n+    DeprecationWarning, stacklevel=2)\n\n@@ -1,149 +1,5 @@\n-\"\"\"\n-XPath selectors \n+from scrapy.selector import *\n \n-See documentation in docs/ref/selectors.rst\n-\"\"\"\n-\n-import libxml2\n-\n-from scrapy.http import TextResponse\n-from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n-from scrapy.xpath.document import Libxml2Document\n-from scrapy.utils.python import flatten, unicode_to_str\n-from scrapy.utils.misc import extract_regex\n-from scrapy.utils.trackref import object_ref\n-from scrapy.utils.decorator import deprecated\n-\n-class XPathSelector(object_ref):\n-\n-    __slots__ = ['doc', 'xmlNode', 'expr', '__weakref__']\n-\n-    def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n-        if parent:\n-            self.doc = parent.doc\n-            self.xmlNode = node\n-        elif response:\n-            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n-            self.xmlNode = self.doc.xmlDoc\n-        elif text:\n-            response = TextResponse(url=None, body=unicode_to_str(text), \\\n-                encoding='utf-8')\n-            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n-            self.xmlNode = self.doc.xmlDoc\n-        self.expr = expr\n-\n-    def select(self, xpath):\n-        \"\"\"Perform the given XPath query on the current XPathSelector and\n-        return a XPathSelectorList of the result\"\"\"\n-        if hasattr(self.xmlNode, 'xpathEval'):\n-            self.doc.xpathContext.setContextNode(self.xmlNode)\n-            try:\n-                xpath_result = self.doc.xpathContext.xpathEval(xpath)\n-            except libxml2.xpathError:\n-                raise ValueError(\"Invalid XPath: %s\" % xpath)\n-            if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([self.__class__(node=node, parent=self, \\\n-                    expr=xpath) for node in xpath_result])\n-            else:\n-                return XPathSelectorList([self.__class__(node=xpath_result, \\\n-                    parent=self, expr=xpath)])\n-        else:\n-            return XPathSelectorList([])\n-\n-    def re(self, regex):\n-        \"\"\"Return a list of unicode strings by applying the regex over all\n-        current XPath selections, and flattening the results\"\"\"\n-        return extract_regex(regex, self.extract(), 'utf-8')\n-\n-    def extract(self):\n-        \"\"\"Return a unicode string of the content referenced by the XPathSelector\"\"\"\n-        if isinstance(self.xmlNode, basestring):\n-            text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n-        elif hasattr(self.xmlNode, 'serialize'):\n-            if isinstance(self.xmlNode, libxml2.xmlDoc):\n-                data = self.xmlNode.getRootElement().serialize('utf-8')\n-                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n-            elif isinstance(self.xmlNode, libxml2.xmlAttr): \n-                # serialization doesn't work sometimes for xmlAttr types\n-                text = unicode(self.xmlNode.content, 'utf-8', errors='ignore')\n-            else:\n-                data = self.xmlNode.serialize('utf-8')\n-                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n-        else:\n-            try:\n-                text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n-            except TypeError:  # catched when self.xmlNode is a float - see tests\n-                text = unicode(self.xmlNode)\n-        return text\n-\n-    def extract_unquoted(self):\n-        \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n-        if self.select('self::text()'):\n-            return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')\n-        else:\n-            return u''\n-\n-    def register_namespace(self, prefix, uri):\n-        \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n-        self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n-\n-    def _get_libxml2_doc(self, response):\n-        \"\"\"Return libxml2 document (xmlDoc) from response\"\"\"\n-        return xmlDoc_from_html(response)\n-\n-    def __nonzero__(self):\n-        return bool(self.extract())\n-\n-    def __str__(self):\n-        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, \\\n-            'name', type(self.xmlNode).__name__), self.expr)\n-\n-    __repr__ = __str__\n-\n-    @deprecated(use_instead='XPathSelector.select')\n-    def __call__(self, xpath):\n-        return self.select(xpath)\n-\n-    @deprecated(use_instead='XPathSelector.select')\n-    def x(self, xpath):\n-        return self.select(xpath)\n-\n-\n-class XPathSelectorList(list):\n-    \"\"\"List of XPathSelector objects\"\"\"\n-\n-    def __getslice__(self, i, j):\n-        return XPathSelectorList(list.__getslice__(self, i, j))\n-\n-    def select(self, xpath):\n-        \"\"\"Perform the given XPath query on each XPathSelector of the list and\n-        return a new (flattened) XPathSelectorList of the results\"\"\"\n-        return XPathSelectorList(flatten([x.select(xpath) for x in self]))\n-\n-    def re(self, regex):\n-        \"\"\"Perform the re() method on each XPathSelector of the list, and\n-        return the result as a flattened list of unicode strings\"\"\"\n-        return flatten([x.re(regex) for x in self])\n-\n-    def extract(self):\n-        \"\"\"Return a list of unicode strings with the content referenced by each\n-        XPathSelector of the list\"\"\"\n-        return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n-\n-    def extract_unquoted(self):\n-        return [x.extract_unquoted() if isinstance(x, XPathSelector) else x for x in self]\n-\n-    @deprecated(use_instead='XPathSelectorList.select')\n-    def x(self, xpath):\n-        return self.select(xpath)\n-\n-\n-class XmlXPathSelector(XPathSelector):\n-    \"\"\"XPathSelector for XML content\"\"\"\n-    __slots__ = ()\n-    _get_libxml2_doc = staticmethod(xmlDoc_from_xml)\n-\n-class HtmlXPathSelector(XPathSelector):\n-    \"\"\"XPathSelector for HTML content\"\"\"\n-    __slots__ = ()\n-    _get_libxml2_doc = staticmethod(xmlDoc_from_html)\n+import warnings\n+warnings.warn(\"scrapy.xpath.selector module is deprecated, use scrapy.selector instead\",\n+    DeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06db0bc12428dc51f9faa08a5da76e8fc27cf215", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 33 | Files Changed: 32 | Hunks: 33 | Methods Changed: 0 | Complexity Δ (Sum/Max): 31/15 | Churn Δ: 66 | Churn Cumulative: 1176 | Contributors (this commit): 5 | Commits (past 90d): 138 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -2,14 +2,14 @@\n This module contains the default values for all settings used by Scrapy. \n \n For more information about these settings you can read the settings\n-documentation in docs/ref/settings.rst\n+documentation in docs/topics/settings.rst\n \n Scrapy developers, if you add a setting here remember to:\n \n * add it in alphabetical order\n * group similar settings without leaving blank lines\n * add its documentation to the available settings documentation\n-  (docs/ref/settings.rst)\n+  (docs/topics/settings.rst)\n \n \"\"\"\n \n\n@@ -1,7 +1,7 @@\n \"\"\"CloseDomain is an extension that forces spiders to be closed after certain\n conditions are met.\n \n-See documentation in docs/ref/extensions.rst\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n \n from collections import defaultdict\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Extensions for debugging Scrapy \n \n-See documentation in docs/ref/extensions.rst\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n import signal\n import traceback\n\n@@ -1,7 +1,7 @@\n \"\"\"\n DefaultHeaders downloader middleware\n \n-See documentation in docs/ref/downloader-middleware.rst\n+See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n from scrapy.conf import settings\n\n@@ -1,7 +1,7 @@\n \"\"\"\n HTTP basic auth downloader middleware\n \n-See documentation in docs/ref/downloader-middleware.rst\n+See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n from scrapy.utils.request import request_authenticate\n\n@@ -3,5 +3,5 @@ scrapy.contrib.linkextractors\n \n This package contains a collection of Link Extractors. \n \n-For more info see docs/ref/link-extractors.rst\n+For more info see docs/topics/link-extractors.rst\n \"\"\"\n\n@@ -1,7 +1,7 @@\n \"\"\"\n MemoryDebugger extension\n \n-See documentation in docs/ref/extensions.rst\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n \n import gc\n\n@@ -1,7 +1,7 @@\n \"\"\"\n MemoryUsage extension\n \n-See documentation in docs/ref/extensions.rst\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n \n import os\n\n@@ -1,7 +1,7 @@\n \"\"\"\n DuplicatesFilterMiddleware: Filter out already visited urls\n \n-See documentation in docs/ref/scheduler-middleware.rst\n+See documentation in docs/topics/scheduler-middleware.rst\n \"\"\"\n \n from scrapy.core.exceptions import IgnoreRequest, NotConfigured\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Depth Spider Middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n from scrapy import log\n\n@@ -1,7 +1,7 @@\n \"\"\"\n HttpError Spider Middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n class HttpErrorMiddleware(object):\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Offsite Spider Middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n import re\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Request Limit Spider middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from itertools import imap\n from scrapy.xlib.pydispatch import dispatcher\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Restrict Spider Middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n from itertools import ifilter\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Url Length Spider Middleware\n \n-See documentation in docs/ref/spider-middleware.rst\n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n from scrapy import log\n\n@@ -2,7 +2,7 @@\n This modules implements the CrawlSpider which is the recommended spider to use\n for scraping typical web sites that requires crawling pages.\n \n-See documentation in docs/ref/spiders.rst\n+See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n\n@@ -2,7 +2,7 @@\n This module implements the XMLFeedSpider which is the recommended spider to use\n for scraping from an XML feed.\n \n-See documentation in docs/ref/spiders.rst\n+See documentation in docs/topics/spiders.rst\n \"\"\"\n \n from scrapy.contrib.spiders.init import InitSpider\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Scheduler queue web console module \n \n-See documentation in docs/ref/extensions.rst\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n \n from scrapy.xlib.pydispatch import dispatcher\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Scrapy core exceptions\n \n-These exceptions are documented in docs/ref/exceptions.rst. Please don't add\n+These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n new exceptions here without documenting them there.\n \"\"\"\n \n\n@@ -1,7 +1,7 @@\n \"\"\"\n Scrapy core signals\n \n-These signals are documented in docs/ref/signals.rst. Please don't add new\n+These signals are documented in docs/topics/signals.rst. Please don't add new\n signals here without documenting them there.\n \"\"\"\n \n\n@@ -2,7 +2,7 @@\n This module implements the Request class which is used to represent HTTP\n requests in Scrapy.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import copy\n\n@@ -2,7 +2,7 @@\n This module implements the FormRequest class which is a more covenient class\n (than Request) to generate Requests based on form data.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import urllib\n\n@@ -2,7 +2,7 @@\n This module implements the XmlRpcRequest class which is a more convenient class\n (that Request) to generate xml-rpc requests.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import xmlrpclib\n\n@@ -2,7 +2,7 @@\n This module implements the Response class which is used to represent HTTP\n responses in Scrapy.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import copy\n\n@@ -2,7 +2,7 @@\n This module implements the HtmlResponse class which adds encoding\n discovering through HTML encoding declarations to the TextResponse class.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import re\n\n@@ -2,7 +2,7 @@\n This module implements the TextResponse class which adds encoding handling and\n discovering (through HTTP headers) to base Response class.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import re\n\n@@ -2,7 +2,7 @@\n This module implements the XmlResponse class which adds encoding\n discovering through XML encoding declarations to the TextResponse class.\n \n-See documentation in docs/ref/request-response.rst\n+See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import re\n\n@@ -2,7 +2,7 @@\n This module defines the Link object used in Link extractors.\n \n For actual link extractors implementation see scrapy.contrib.linkextractor, or\n-its documentation in: docs/ref/link-extractors.rst\n+its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n \n class Link(object):\n\n@@ -1,7 +1,7 @@\n \"\"\" \n Scrapy logging facility\n \n-See documentation in docs/ref/logging.rst\n+See documentation in docs/topics/logging.rst\n \"\"\"\n import sys\n from traceback import format_exc\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Mail sending helpers\n \n-See documentation in docs/ref/email.rst\n+See documentation in docs/topics/email.rst\n \"\"\"\n from cStringIO import StringIO\n from email.MIMEMultipart import MIMEMultipart\n\n@@ -1,7 +1,7 @@\n \"\"\"\n XPath selectors \n \n-See documentation in docs/ref/selectors.rst\n+See documentation in docs/topics/selectors.rst\n \"\"\"\n \n import libxml2\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Base class for Scrapy spiders\n \n-See documentation in docs/ref/spiders.rst\n+See documentation in docs/topics/spiders.rst\n \"\"\"\n from zope.interface import Interface, Attribute, invariant, implements\n from twisted.plugin import IPlugin\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5522560d360d1be732e6596b2d75776162d1b33c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from scrapy.xpath import HtmlXPathSelector\n+from scrapy.selector import HtmlXPathSelector\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.contrib.spiders import CrawlSpider, Rule\n from scrapy.contrib.loader import XPathItemLoader\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b7162f3caa6ca97eb150a83665466ed0e17e8fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 220 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,8 @@ class JsonLinesItemExporter(BaseItemExporter):\n         self.file = file\n         self.encoder = json.JSONEncoder(*args, **kwargs)\n \n-    def export(self, item):\n-        itemdict = dict(self._get_fields_to_export(item))\n+    def export_item(self, item):\n+        itemdict = {}\n+        for field, value in self._get_fields_to_export(item):\n+            itemdict[field] = self.serialize(item.fields[field], field, value)\n         self.file.write(self.encoder.encode(itemdict) + '\\n')\n\n@@ -134,7 +134,7 @@ class XmlItemExporterTest(BaseTest):\n         self.assertEqual(self.output.getvalue(), expected_value)\n \n \n-class JSONItemExporterTest(BaseTest):\n+class JsonLinesItemExporterTest(BaseTest):\n \n     def setUp(self):\n         try:\n@@ -145,18 +145,17 @@ class JSONItemExporterTest(BaseTest):\n             except ImportError:\n                 raise unittest.SkipTest(\"simplejson module not available\") \n \n-        from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n-\n-        super(JSONItemExporterTest, self).__init__()\n+        super(JsonLinesItemExporterTest, self).setUp()\n \n     def test_export(self):\n+        from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n \n-        ie = JsonLinesItemExporter(output)\n+        ie = JsonLinesItemExporter(self.output)\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n \n-        self.assertEqual(output.getvalue(), '{\"age\": \"22\", \"name\": \"John\"}\\n')\n+        self.assertEqual(self.output.getvalue(), '{\"age\": \"22\", \"name\": \"John\"}\\n')\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#20e82335e2127448809b7a30d0de484602cae23e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 84 | Files Changed: 3 | Hunks: 43 | Methods Changed: 21 | Complexity Δ (Sum/Max): 7/8 | Churn Δ: 161 | Churn Cumulative: 543 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,12 +21,14 @@ class BaseItemExporter(object):\n     def export_item(self, item):\n         raise NotImplementedError\n \n-    def serialize(self, field, name, value):\n+    def serialize_field(self, field, name, value):\n         serializer = field.get('serializer', identity)\n         return serializer(value)\n \n-    def _get_fields_to_export(self, item, default_value=None, include_empty=None):\n-        \"\"\"Return the fields to export as a list of tuples (name, value)\"\"\"\n+    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n+        \"\"\"Return the fields to export as an iterable of tuples (name,\n+        serialized_value)\n+        \"\"\"\n         if include_empty is None:\n             include_empty = self.export_empty_fields\n         if self.fields_to_export is None:\n@@ -41,7 +43,14 @@ class BaseItemExporter(object):\n                 nonempty_fields = set(item.keys())\n                 field_iter = (x for x in self.fields_to_export if x in \\\n                     nonempty_fields)\n-        return [(k, item.get(k, default_value)) for k in field_iter]\n+        for field_name in field_iter:\n+            if field_name in item:\n+                field = item.fields[field_name]\n+                value = self.serialize_field(field, field_name, item[field_name])\n+            else:\n+                value = default_value\n+\n+            yield field_name, value\n \n     def start_exporting(self):\n         pass\n@@ -66,18 +75,17 @@ class XmlItemExporter(BaseItemExporter):\n \n     def export_item(self, item):\n         self.xg.startElement(self.item_element, {})\n-        for field, value in self._get_fields_to_export(item, default_value=''):\n-            self._export_xml_field(item.fields[field], field, value)\n+        for name, value in self._get_serialized_fields(item, default_value=''):\n+            self._export_xml_field(name, value)\n         self.xg.endElement(self.item_element)\n \n     def finish_exporting(self):\n         self.xg.endElement(self.root_element)\n         self.xg.endDocument()\n \n-    def _export_xml_field(self, field, name, value):\n+    def _export_xml_field(self, name, serialized_value):\n         self.xg.startElement(name, {})\n-        if value is not None:\n-            self.xg.characters(self.serialize(field, name, value))\n+        self.xg.characters(serialized_value)\n         self.xg.endElement(name)\n \n \n@@ -97,7 +105,7 @@ class CsvItemExporter(BaseItemExporter):\n             self.csv_writer.writerow(self.fields_to_export)\n \n     def export_item(self, item):\n-        fields = self._get_fields_to_export(item, default_value='', \\\n+        fields = self._get_serialized_fields(item, default_value='', \\\n             include_empty=True)\n \n         values = [x[1] for x in fields]\n@@ -111,7 +119,7 @@ class PickleItemExporter(BaseItemExporter):\n         self.pickler = Pickler(*args, **kwargs)\n \n     def export_item(self, item):\n-        self.pickler.dump(dict(self._get_fields_to_export(item)))\n+        self.pickler.dump(dict(self._get_serialized_fields(item)))\n \n \n class PprintItemExporter(BaseItemExporter):\n@@ -121,5 +129,5 @@ class PprintItemExporter(BaseItemExporter):\n         self.file = file\n \n     def export_item(self, item):\n-        itemdict = dict(self._get_fields_to_export(item))\n+        itemdict = dict(self._get_serialized_fields(item))\n         self.file.write(pprint.pformat(itemdict) + '\\n')\n\n@@ -13,7 +13,5 @@ class JsonLinesItemExporter(BaseItemExporter):\n         self.encoder = json.JSONEncoder(*args, **kwargs)\n \n     def export_item(self, item):\n-        itemdict = {}\n-        for field, value in self._get_fields_to_export(item):\n-            itemdict[field] = self.serialize(item.fields[field], field, value)\n+        itemdict = dict(self._get_serialized_fields(item))\n         self.file.write(self.encoder.encode(itemdict) + '\\n')\n\n@@ -1,10 +1,9 @@\n-from cPickle import Pickler\n+import cPickle as pickle\n from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n from scrapy.item import Item, Field\n-\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter\n \n@@ -13,39 +12,50 @@ class TestItem(Item):\n     age = Field()\n \n \n-class BaseTest(unittest.TestCase):\n+class BaseItemExporterTest(unittest.TestCase):\n+\n     def setUp(self):\n         self.i = TestItem(name=u'John', age='22')\n-        self.ie = BaseItemExporter()\n-\n         self.output = StringIO()\n+        self.ie = self._get_exporter()\n \n-class BaseItemExporterTest(BaseTest):\n+    def _get_exporter(self):\n+        return BaseItemExporter()\n \n-    def test_export(self):\n-        self.assertRaises(NotImplementedError, self.ie.export_item, self.i)\n+    def _check_output(self):\n+        pass\n \n-    def test_serialize(self):\n-        self.assertEqual(self.ie.serialize( \\\n+    def test_export_item(self):\n+        self.ie.start_exporting()\n+        try:\n+            self.ie.export_item(self.i)\n+        except NotImplementedError:\n+            if self.ie.__class__ is not BaseItemExporter:\n+                raise\n+        self.ie.finish_exporting()\n+        self._check_output()\n+\n+    def test_serialize_field(self):\n+        self.assertEqual(self.ie.serialize_field( \\\n             self.i.fields['name'], 'name', self.i['name']), 'John')\n         self.assertEqual( \\\n-            self.ie.serialize(self.i.fields['age'], 'age', self.i['age']), '22')\n+            self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '22')\n \n     def test_exporter_custom_serializer(self):\n         class CustomItemExporter(BaseItemExporter):\n-            def serialize(self, field, name, value):\n+            def serialize_field(self, field, name, value):\n                 if name == 'age':\n                     return str(int(value) + 1)\n                 else:\n-                    return super(CustomItemExporter, self).serialize(field, \\\n+                    return super(CustomItemExporter, self).serialize_field(field, \\\n                         name, value)\n \n         ie = CustomItemExporter()\n \n         self.assertEqual( \\\n-            ie.serialize(self.i.fields['name'], 'name', self.i['name']), 'John')\n+            ie.serialize_field(self.i.fields['name'], 'name', self.i['name']), 'John')\n         self.assertEqual(\n-            ie.serialize(self.i.fields['age'], 'age', self.i['age']), '23')\n+            ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '23')\n \n     def test_field_custom_serializer(self):\n         def custom_serializer(value):\n@@ -58,57 +68,44 @@ class BaseItemExporterTest(BaseTest):\n         i = CustomFieldItem(name=u'John', age='22')\n \n         self.assertEqual( \\\n-            self.ie.serialize(i.fields['name'], 'name', i['name']), 'John')\n+            self.ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')\n         self.assertEqual( \\\n-            self.ie.serialize(i.fields['age'], 'age', i['age']), '24')\n+            self.ie.serialize_field(i.fields['age'], 'age', i['age']), '24')\n \n     def test_fields_to_export(self):\n         ie = BaseItemExporter()\n         ie.fields_to_export = ['name']\n \n-        self.assertEqual(ie._get_fields_to_export(self.i), [('name', 'John')])\n+        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John')])\n+\n+class PprintItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self):\n+        return PprintItemExporter(self.output)\n+\n+    def _check_output(self):\n+        self.assertEqual(dict(self.i), eval(self.output.getvalue()))\n \n \n-class PprintItemExporterTest(BaseTest):\n+class PickleItemExporterTest(BaseItemExporterTest):\n         \n-    def test_export(self):\n-        ie = PprintItemExporter(self.output)\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n+    def _get_exporter(self):\n+        return PickleItemExporter(self.output)\n \n-        self.assertEqual(self.output.getvalue(), \"{'age': '22', 'name': u'John'}\\n\")\n+    def _check_output(self):\n+        self.assertEqual(dict(self.i), pickle.loads(self.output.getvalue()))\n \n+class CsvItemExporterTest(BaseItemExporterTest):\n \n-class PickleItemExporterTest(BaseTest):\n-        \n-    def test_export(self):\n-        output = StringIO()\n-        ie = PickleItemExporter(output)\n-\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n-\n-        poutput = StringIO()\n-        p = Pickler(poutput)\n-        p.dump(dict(self.i))\n-        \n-        self.assertEqual(output.getvalue(), poutput.getvalue())\n-\n-\n-class CsvItemExporterTest(BaseTest):\n-\n-    def test_export(self):\n-        ie = CsvItemExporter(self.output)\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n+    def _get_exporter(self):\n+        return CsvItemExporter(self.output)\n \n+    def _check_output(self):\n         self.assertEqual(self.output.getvalue(), '22,John\\r\\n')\n \n     def test_header(self):\n-        ie = CsvItemExporter(self.output)\n+        output = StringIO()\n+        ie = CsvItemExporter(output)\n         ie.include_headers_line = True\n \n         self.assertRaises(RuntimeError, ie.start_exporting)\n@@ -118,25 +115,22 @@ class CsvItemExporterTest(BaseTest):\n         ie.export_item(self.i)\n         ie.finish_exporting()\n \n-        self.assertEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\r\\n')\n+        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\r\\n')\n \n \n-class XmlItemExporterTest(BaseTest):\n+class XmlItemExporterTest(BaseItemExporterTest):\n \n-    def test_export(self):\n-        ie = XmlItemExporter(self.output)\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n+    def _get_exporter(self):\n+        return XmlItemExporter(self.output)\n \n+    def _check_output(self):\n         expected_value = '<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\\n<items><item><age>22</age><name>John</name></item></items>'\n-\n         self.assertEqual(self.output.getvalue(), expected_value)\n \n \n-class JsonLinesItemExporterTest(BaseTest):\n+class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n-    def setUp(self):\n+    def _get_exporter(self):\n         try:\n             import json\n         except ImportError:\n@@ -144,17 +138,10 @@ class JsonLinesItemExporterTest(BaseTest):\n                 import simplejson\n             except ImportError:\n                 raise unittest.SkipTest(\"simplejson module not available\") \n-\n-        super(JsonLinesItemExporterTest, self).setUp()\n-\n-    def test_export(self):\n         from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n+        return JsonLinesItemExporter(self.output)\n \n-        ie = JsonLinesItemExporter(self.output)\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n-\n+    def _check_output(self):\n         self.assertEqual(self.output.getvalue(), '{\"age\": \"22\", \"name\": \"John\"}\\n')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5de409a6a06d4ee8bf9b7604f6ca353ea2911a7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 111 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 111 | Churn Cumulative: 126 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,33 +0,0 @@\n-from twisted.trial import unittest\n-\n-class SerializationTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        try:\n-            import json\n-        except ImportError:\n-            try:\n-                import simplejson\n-            except ImportError, e:\n-                raise unittest.SkipTest(e)\n-\n-    def test_string_serialization(self):\n-        \"\"\"Test simple string serialization\"\"\"\n-        from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n-\n-        s = \"string to serialize\"\n-        for format in serialize_funcs.iterkeys():\n-            self.assertEqual(s, unserialize(serialize(s)))\n-\n-    def test_dict_serialization(self):\n-        \"\"\"Test dict serialization\"\"\"\n-        from scrapy.utils.serialization import serialize, unserialize, serialize_funcs\n-\n-        # using only string keys/values since it's the only data type that\n-        # works with all serializers\n-        d = {'one': 'item one', 'two': 'item two'}\n-        for format in serialize_funcs.iterkeys():\n-            self.assertEqual(d, unserialize(serialize(d)))\n-        \n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,78 +0,0 @@\n-\"\"\"\n-This module provides a universal API for different serialization formats.\n-\n-Keep in mind that not all formats support all types for serialization, and some\n-formats (like pprint) may be unsafe for unserialization (like pprint) but it\n-may still very convenient for serialization.\n-\"\"\"\n-\n-import datetime\n-import decimal\n-import cPickle as pickle\n-import pprint\n-\n-try:\n-    import json\n-except ImportError:\n-    import simplejson as json\n-\n-\n-class ScrapyJSONEncoder(json.JSONEncoder):\n-    \"\"\"\n-    JSONEncoder subclass that knows how to encode date/time and decimal types.\n-    \"\"\"\n-\n-    DATE_FORMAT = \"%Y-%m-%d\"\n-    TIME_FORMAT = \"%H:%M:%S\"\n-\n-    def default(self, o):\n-        if isinstance(o, datetime.datetime):\n-            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n-        elif isinstance(o, datetime.date):\n-            return o.strftime(self.DATE_FORMAT)\n-        elif isinstance(o, datetime.time):\n-            return o.strftime(self.TIME_FORMAT)\n-        elif isinstance(o, decimal.Decimal):\n-            return str(o)\n-        else:\n-            return super(ScrapyJSONEncoder, self).default(o)\n-\n-serialize_funcs = {\n-    'json': lambda obj: json.dumps(obj, cls=ScrapyJSONEncoder),\n-    'pprint': lambda obj: pprint.pformat(obj),\n-    'pickle': lambda obj: pickle.dumps(obj),\n-}\n-\n-unserialize_funcs = {\n-    'json': lambda text: json.loads(text),\n-    'pprint': lambda text: eval(text),\n-    'pickle': lambda text: pickle.loads(text),\n-}\n-\n-def serialize(obj, format='pickle'):\n-    \"\"\"\n-    Main entrance point for serialization.\n-\n-    Supported formats: pickle, json, pprint\n-    \"\"\"\n-    try:\n-        func = serialize_funcs[format]\n-        return func(obj)\n-    except KeyError:\n-        raise TypeError(\"Unknown serialization format: %s\" % format)\n-\n-def unserialize(text, format='pickle'):\n-    \"\"\"\n-    Main entrance point for unserialization.\n-    \"\"\"\n-    try:\n-        func = unserialize_funcs[format]\n-        return func(text)\n-    except KeyError:\n-        raise TypeError(\"Unknown serialization format: %s\" % format)\n-\n-def parse_jsondatetime(text):\n-    if isinstance(text, basestring):\n-        return datetime.datetime.strptime(text, \"%s %s\" % \\\n-               (ScrapyJSONEncoder.DATE_FORMAT, ScrapyJSONEncoder.TIME_FORMAT))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b65c87cb67176bc4fad650aba1ee55b8580aa1da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 138 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 138 | Churn Cumulative: 138 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.20512820512820512\n\nDIFF:\n@@ -1,28 +0,0 @@\n-import unittest\n-from scrapy.utils.c14n import canonicalize\n-\n-class C14nTest(unittest.TestCase):\n-    \"\"\" c14n comparison functions \"\"\"\n-        \n-    def test_canonicalize(self):\n-        \"\"\"Test URL canonicalization function\"\"\"\n-        urls = [\n-            ('http://www.maddiebrown.co.uk//cgi-bin//html_parser.cgi?web_page_id=12&template_file=catalogue_list_items_template&category-id=1000070&category-type=1&page=3&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2&page=2',\n-            'http://www.maddiebrown.co.uk//cgi-bin//html_parser.cgi?web_page_id=12&template_file=catalogue_list_items_template&category-id=1000070&category-type=1&page=3&page=2'),\n-             \n-\n-            ('http://www.mfi.co.uk/mfi/productinfo.asp?CT=/1010/YourHomeOffice&CT=/1025/YourHomeOffice/Packages',\n-             'http://www.mfi.co.uk/mfi/productinfo.asp?CT=/1010/YourHomeOffice&CT=/1025/YourHomeOffice/Packages'),\n-\n-            ('http://www.test-nice-url.com/index.html',\n-             'http://www.test-nice-url.com/index.html'),\n-\n-            ('http://www.homebase.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=20001&langId=-1&catalogId=10701&productId=729482&Trail=C$cip=50704&categoryId=50704',\n-             'http://www.homebase.co.uk/webapp/wcs/stores/servlet/ProductDisplay?storeId=20001&langId=-1&catalogId=10701&productId=729482&Trail=C$cip=50704&categoryId=50704'),\n-        ]\n-        for origurl, c14nurl in urls:\n-            self.assertEqual(canonicalize(origurl), c14nurl)\n-        \n-if __name__ == \"__main__\":\n-    unittest.main()\n-        \n\n@@ -1,110 +0,0 @@\n-\"\"\"\n-Canonicalization routine for URLs\n-\"\"\"\n-import re\n-import urlparse\n-\n-_hexmatcher = re.compile(\"\\%[0-9A-Fa-f]{2}\")\n-\n-def get_canonized_port(port):\n-    if port and port != 80:\n-        cport = \":\" + str(port)\n-    else:\n-        cport = \"\"\n-    return cport\n-\n-def create_params_dict(params):\n-    param_dict = {}\n-    for param in params.split(\"&\"):\n-        nvpair = param.split(\"=\")\n-        if len(nvpair) >= 2:\n-            param_dict[nvpair[0]] = '='.join(nvpair[1:])\n-        else:\n-            param_dict[nvpair[0]] = None\n-    return param_dict\n-    \n-def create_unique_params_list(params):\n-    \"\"\"\n-    This function build a query string from unique params only.\n-    Unique params are params with unique key=>value pair. So param=qwe and param=123\n-    are diffrent and a query string will be param=qwe&param=123.\n-    If input params will contain pairs like this param=123 and param=123 a query\n-    string will be just param=123\n-    We have to do this because some ASP sites uses the same parameters names in the URLs,\n-    but with diffrent values. Such queries are interpreted as arrays in the ASP framework.\n-    \"\"\"\n-    param_list = []\n-    if params:\n-        for param in params.split(\"&\"):\n-            nvpair = param.split(\"=\", 1)\n-            if len(nvpair) >= 2:\n-                param_list.append(nvpair[0] + '=' + nvpair[1])\n-            else:\n-                param_list.append(nvpair[0] + '=')\n-        if param_list:\n-            p_list = []\n-            # select only unique pairs, it is possible to use set() for this but change order of pairs...\n-            for param in param_list:\n-                if param not in p_list:\n-                    p_list.append(param)\n-            return '&'.join(p_list)\n-    return ''\n-\n-def convert_escaped(string_to_convert):\n-    hexvalues = _hexmatcher.findall(string_to_convert)\n-    for hexvalue in hexvalues:\n-        string_to_convert = string_to_convert.replace(\n-                hexvalue, chr(int(hexvalue[1:len(hexvalue)],16)))\n-    return string_to_convert\n-\n-def remove_dirs(path):\n-    dirs = path.split(\"/\")\n-    dirs.reverse()\n-    stripped_dirs = []\n-    ellipsis = 0\n-    for dir in dirs:\n-        if dir == \"..\" :\n-            ellipsis += 1\n-        elif dir == \".\" :\n-            pass\n-        else:\n-            if ellipsis > 0 :\n-                ellipsis -= 1\n-            else:\n-                stripped_dirs.append(dir)\n-    stripped_dirs.reverse()\n-    return \"/\".join(stripped_dirs)\n-\n-def canonicalize(url):\n-    \"\"\"\n-    This method c14ns the url we are passed by doing the following\n-    1) lowercasing the hostname and scheme\n-    2) removing the port if it is 80\n-    3) making the query params unique and alphabetized\n-    4) removing url encoding\n-    5) Removing any ./\n-    6) Removing and ../ and their preceding directories\n-    \"\"\"\n-\n-    # Remove the fragment marker, even if it's url encoded\n-    defragmented_url = urlparse.urldefrag(url)[0]\n-\n-    # This automatically converts the scheme and hostname to lower case \n-    parsed = urlparse.urlparse(defragmented_url)\n-\n-    # Get the port in the correct format \n-    cport = get_canonized_port(parsed.port)\n-    \n-    query = create_unique_params_list(parsed.query)\n-\n-    # Remove the url encoded params from the path and query string\n-    query = convert_escaped(query)\n-    if query :\n-        query = \"?\" + query\n-    path = convert_escaped(parsed.path)\n-\n-    # Remove the ../ and the previous directories and remove the ./\n-    path = remove_dirs(path)\n-\n-    # Now put it all back together\n-    return \"%s://%s%s%s%s\" % (parsed.scheme, parsed.hostname, cport, path, query)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
