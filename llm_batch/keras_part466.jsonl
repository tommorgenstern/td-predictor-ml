{"custom_id": "keras#b0ffc0031e9c1964e7398ca47c6666bbfc0d5086", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 652 | Lines Deleted: 545 | Files Changed: 27 | Hunks: 250 | Methods Changed: 133 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1197 | Churn Cumulative: 23370 | Contributors (this commit): 25 | Commits (past 90d): 107 | Contributors (cumulative): 208 | DMM Complexity: 0.2608695652173913\n\nDIFF:\n@@ -47,8 +47,8 @@ def save(model, dirpath):\n \n     # TODO(rchao): Save the model's metadata (e.g. Keras version) in a separate\n     # file in the archive.\n-    # TODO(rchao): Save the model's state (e.g. layer weights/vocab) in a separate\n-    # set of files in the archive.\n+    # TODO(rchao): Save the model's state (e.g. layer weights/vocab) in a\n+    # separate set of files in the archive.\n     # TODO(rchao): Write the config into a file in an archive. In this prototype\n     # we're temporarily settled on a standalone json file.\n     serialized_model_dict = serialize_keras_object(model)\n@@ -84,8 +84,8 @@ def deserialize_keras_object(config_dict):\n       the format of '{package}>{name}', where `package` and `name` are the\n       arguments passed to `register_keras_serializable()`. If `name` is not\n       provided, it defaults to the class name. If `registered_name` successfully\n-      resolves to a class (that was registered), `class_name` and `config` values\n-      in the dict will not be used. `registered_name` is only used for\n+      resolves to a class (that was registered), `class_name` and `config`\n+      values in the dict will not be used. `registered_name` is only used for\n       non-built-in classes.\n \n     For example, the following dictionary represents the built-in Adam optimizer\n@@ -113,8 +113,8 @@ def deserialize_keras_object(config_dict):\n     deserialize_keras_object(dict_structure)\n     ```\n \n-    If the class does not have an exported Keras namespace, the library tracks it\n-    by its `module` and `class_name`. For example:\n+    If the class does not have an exported Keras namespace, the library tracks\n+    it by its `module` and `class_name`. For example:\n \n     ```\n     dict_structure = {\n@@ -153,14 +153,15 @@ def deserialize_keras_object(config_dict):\n     ```\n \n     Args:\n-      config_dict: the python dict structure to deserialize the Keras object from.\n+      config_dict: the python dict structure to deserialize the Keras object\n+        from.\n \n     Returns:\n       The Keras object that is deserialized from `config_dict`.\n \n     \"\"\"\n-    # TODO(rchao): Design a 'version' key for `config_dict` for defining versions\n-    # for classes.\n+    # TODO(rchao): Design a 'version' key for `config_dict` for defining\n+    # versions for classes.\n     class_name = config_dict[\"class_name\"]\n     config = config_dict[\"config\"]\n     module = config_dict[\"module\"]\n@@ -185,14 +186,15 @@ def deserialize_keras_object(config_dict):\n                 # `register_keras_serializable` API), that takes precedence.\n                 return custom_function\n \n-            # Otherwise, attempt to import the tracked module, and find the function.\n+            # Otherwise, attempt to import the tracked module, and find the\n+            # function.\n             function_module = config.get(\"module\", None)\n             try:\n                 function_module = importlib.import_module(function_module)\n             except ImportError as e:\n                 raise ImportError(\n-                    f\"The function module {function_module} is not available. The \"\n-                    f\"config dictionary provided is {config_dict}.\"\n+                    f\"The function module {function_module} is not available. \"\n+                    f\"The config dictionary provided is {config_dict}.\"\n                 ) from e\n             return vars(function_module).get(config[\"function_name\"])\n \n@@ -204,18 +206,18 @@ def deserialize_keras_object(config_dict):\n         # `register_keras_serializable` API). If so, that takes precedence.\n         return custom_class.from_config(config)\n     else:\n-        # Otherwise, attempt to retrieve the class object given the `module`, and\n-        # `class_name`.\n+        # Otherwise, attempt to retrieve the class object given the `module`,\n+        # and `class_name`.\n         if module is None:\n-            # In the case where `module` is not recorded, the `class_name` represents\n-            # the full exported Keras namespace (used by `keras_export`) such as\n-            # \"keras.optimizers.Adam\".\n+            # In the case where `module` is not recorded, the `class_name`\n+            # represents the full exported Keras namespace (used by\n+            # `keras_export`) such as \"keras.optimizers.Adam\".\n             cls = tf_export.get_symbol_from_name(class_name)\n         else:\n-            # In the case where `module` is available, the class does not have an\n-            # Keras namespace (which is the case when the symbol is not exported via\n-            # `keras_export`). Import the tracked module (that is used for the\n-            # internal path), find the class, and use its config.\n+            # In the case where `module` is available, the class does not have\n+            # an Keras namespace (which is the case when the symbol is not\n+            # exported via `keras_export`). Import the tracked module (that is\n+            # used for the internal path), find the class, and use its config.\n             mod = importlib.import_module(module)\n             cls = vars(mod).get(class_name, None)\n         if not hasattr(cls, \"from_config\"):\n@@ -239,9 +241,9 @@ def serialize_keras_object(obj):\n       deserialized via `deserialize_keras_object()`.\n     \"\"\"\n \n-    # Note that in the case of the `obj` being a function, the module used will be\n-    # \"builtins\", and the `class_name` used will be \"function\"; in the case of the\n-    # `obj` being a string, the module used will be \"builtins\", and the\n+    # Note that in the case of the `obj` being a function, the module used will\n+    # be \"builtins\", and the `class_name` used will be \"function\"; in the case\n+    # of the `obj` being a string, the module used will be \"builtins\", and the\n     # `class_name` used will be \"str\"\n     module = None\n \n\n@@ -67,7 +67,8 @@ class CustomModelX(keras.Model):\n     package=\"my_custom_package\"\n )\n def my_mean_squared_error(y_true, y_pred):\n-    \"\"\"Identical to built-in `mean_squared_error`, added here as a custom func.\"\"\"\n+    \"\"\"Identical to built-in `mean_squared_error`, added here as a custom\n+    func.\"\"\"\n     return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n \n \n@@ -145,8 +146,8 @@ class NewSavingTest(tf.test.TestCase):\n                 keras.metrics.base_metric.Mean,\n             )\n \n-        # Except for a custom function used because the loaded model is supposed to\n-        # be using the newly registered custom function.\n+        # Except for a custom function used because the loaded model is supposed\n+        # to be using the newly registered custom function.\n         self.assertIs(\n             subclassed_model.compiled_loss._losses[3],\n             module_my_mean_squared_error,\n@@ -167,8 +168,8 @@ class NewSavingTest(tf.test.TestCase):\n         loaded_model = saving_lib.load(temp_dir)\n \n         io_utils.enable_interactive_logging()\n-        # `tf.print` writes to stderr. This is to make sure the custom training step\n-        # is used.\n+        # `tf.print` writes to stderr. This is to make sure the custom training\n+        # step is used.\n         with self.captureWritesToStream(sys.stderr) as printed:\n             loaded_model.fit(x, y, epochs=1)\n             self.assertRegex(printed.contents(), train_step_message)\n\n@@ -241,9 +241,10 @@ def load_model_from_hdf5(\n                     )\n                 except (NotImplementedError, AttributeError):\n                     logging.warning(\n-                        \"Error when creating the weights of optimizer {}, making it \"\n-                        \"impossible to restore the saved optimizer state. As a result, \"\n-                        \"your model is starting with a freshly initialized optimizer.\"\n+                        \"Error when creating the weights of optimizer {}, \"\n+                        \"making it impossible to restore the saved optimizer \"\n+                        \"state. As a result, your model is starting with \"\n+                        \"a freshly initialized optimizer.\"\n                     )\n \n                 optimizer_weight_values = (\n@@ -313,8 +314,8 @@ def preprocess_weights_for_loading(\n     def convert_nested_time_distributed(weights):\n         \"\"\"Converts layers nested in `TimeDistributed` wrapper.\n \n-        This function uses `preprocess_weights_for_loading()` for converting nested\n-        layers.\n+        This function uses `preprocess_weights_for_loading()` for converting\n+        nested layers.\n \n         Args:\n             weights: List of weights values (Numpy arrays).\n@@ -329,8 +330,8 @@ def preprocess_weights_for_loading(\n     def convert_nested_model(weights):\n         \"\"\"Converts layers nested in `Model` or `Sequential`.\n \n-        This function uses `preprocess_weights_for_loading()` for converting nested\n-        layers.\n+        This function uses `preprocess_weights_for_loading()` for converting\n+        nested layers.\n \n         Args:\n             weights: List of weights values (Numpy arrays).\n@@ -506,8 +507,8 @@ def _convert_rnn_weights(layer, weights):\n \n     Args:\n         layer: Target layer instance.\n-        weights: List of source weights values (input kernels, recurrent kernels,\n-          [biases]) (Numpy arrays).\n+        weights: List of source weights values (input kernels, recurrent\n+          kernels, [biases]) (Numpy arrays).\n \n     Returns:\n         A list of converted weights values (Numpy arrays).\n@@ -580,7 +581,8 @@ def _convert_rnn_weights(layer, weights):\n \n             Args:\n               weights: Original weights.\n-              from_cudnn: Indicates whether original weights are from cuDNN layer.\n+              from_cudnn: Indicates whether original weights are from cuDNN\n+                layer.\n \n             Returns:\n               Updated weights compatible with LSTM.\n@@ -622,7 +624,8 @@ def _convert_rnn_weights(layer, weights):\n \n             Args:\n               weights: Original weights.\n-              from_cudnn: Indicates whether original weights are from cuDNN layer.\n+              from_cudnn: Indicates whether original weights are from cuDNN\n+                layer.\n \n             Returns:\n               Updated weights compatible with GRU.\n@@ -831,8 +834,8 @@ def load_weights_from_hdf5_group(f, model):\n         )\n         if len(weight_values) != len(symbolic_weights):\n             raise ValueError(\n-                f\"Weight count mismatch for layer #{k} (named {layer.name} in the \"\n-                f\"current model, {name} in the save file). \"\n+                f\"Weight count mismatch for layer #{k} (named {layer.name} in \"\n+                f\"the current model, {name} in the save file). \"\n                 f\"Layer expects {len(symbolic_weights)} weight(s). Received \"\n                 f\"{len(weight_values)} saved weight(s)\"\n             )\n@@ -847,8 +850,8 @@ def load_weights_from_hdf5_group(f, model):\n         )\n         if len(weight_values) != len(symbolic_weights):\n             raise ValueError(\n-                f\"Weight count mismatch for top-level weights when loading weights \"\n-                f\"from file. \"\n+                f\"Weight count mismatch for top-level weights when loading \"\n+                f\"weights from file. \"\n                 f\"Model expects {len(symbolic_weights)} top-level weight(s). \"\n                 f\"Received {len(weight_values)} saved top-level weight(s)\"\n             )\n@@ -914,14 +917,15 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                     logging.warning(\n                         f\"Skipping loading of weights for layer #{k} (named \"\n                         f\"{layer.name}) due to mismatch in number of weights. \"\n-                        f\"Layer expects {len(symbolic_weights)} weight(s). Received \"\n-                        f\"{len(weight_values)} saved weight(s)\"\n+                        f\"Layer expects {len(symbolic_weights)} weight(s). \"\n+                        f\"Received {len(weight_values)} saved weight(s)\"\n                     )\n                     continue\n                 raise ValueError(\n-                    f\"Weight count mismatch for layer #{k} (named {layer.name}). \"\n-                    f\"Layer expects {len(symbolic_weights)} weight(s). Received \"\n-                    f\"{len(weight_values)} saved weight(s)\"\n+                    f\"Weight count mismatch for layer #{k} \"\n+                    f\"(named {layer.name}). \"\n+                    f\"Layer expects {len(symbolic_weights)} weight(s). \"\n+                    f\"Received {len(weight_values)} saved weight(s)\"\n                 )\n             # Set values.\n             for i in range(len(weight_values)):\n@@ -931,16 +935,18 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                     if skip_mismatch:\n                         logging.warning(\n                             f\"Skipping loading weights for layer #{k} (named \"\n-                            f\"{layer.name}) due to mismatch in shape for weight \"\n-                            f\"{symbolic_weights[i].name}. \"\n-                            f\"Weight expects shape {expected_shape}. Received saved weight \"\n+                            f\"{layer.name}) due to mismatch in shape for \"\n+                            f\"weight {symbolic_weights[i].name}. \"\n+                            f\"Weight expects shape {expected_shape}. \"\n+                            f\"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                         continue\n                     raise ValueError(\n-                        f\"Shape mismatch in layer #{k} (named {layer.name}) for weight \"\n-                        f\"{symbolic_weights[i].name}. \"\n-                        f\"Weight expects shape {expected_shape}. Received saved weight \"\n+                        f\"Shape mismatch in layer #{k} (named {layer.name}) \"\n+                        f\"for weight {symbolic_weights[i].name}. \"\n+                        f\"Weight expects shape {expected_shape}. \"\n+                        f\"Received saved weight \"\n                         f\"with shape {received_shape}\"\n                     )\n                 else:\n@@ -959,15 +965,17 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n         if len(weight_values) != len(symbolic_weights):\n             if skip_mismatch:\n                 logging.warning(\n-                    f\"Skipping loading top-level weights for model due to mismatch \"\n-                    f\"in number of weights. \"\n-                    f\"Model expects {len(symbolic_weights)} top-level weight(s). \"\n+                    f\"Skipping loading top-level weights for model due to \"\n+                    f\"mismatch in number of weights. \"\n+                    f\"Model expects {len(symbolic_weights)} \"\n+                    f\"top-level weight(s). \"\n                     f\"Received {len(weight_values)} saved top-level weight(s)\"\n                 )\n             else:\n                 raise ValueError(\n                     f\"Weight count mismatch for top-level weights of model. \"\n-                    f\"Model expects {len(symbolic_weights)} top-level weight(s). \"\n+                    f\"Model expects {len(symbolic_weights)} \"\n+                    f\"top-level weight(s). \"\n                     f\"Received {len(weight_values)} saved top-level weight(s)\"\n                 )\n         else:\n@@ -977,16 +985,19 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                 if expected_shape != received_shape:\n                     if skip_mismatch:\n                         logging.warning(\n-                            f\"Skipping loading top-level weight for model due to \"\n-                            f\"mismatch in shape for weight {symbolic_weights[i].name}. \"\n-                            f\"Weight expects shape {expected_shape}. Received saved weight \"\n+                            f\"Skipping loading top-level weight for model due \"\n+                            f\"to mismatch in shape for \"\n+                            f\"weight {symbolic_weights[i].name}. \"\n+                            f\"Weight expects shape {expected_shape}. \"\n+                            f\"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                     else:\n                         raise ValueError(\n                             f\"Shape mismatch in model for top-level weight \"\n                             f\"{symbolic_weights[i].name}. \"\n-                            f\"Weight expects shape {expected_shape}. Received saved weight \"\n+                            f\"Weight expects shape {expected_shape}. \"\n+                            f\"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                 else:\n@@ -1023,8 +1034,9 @@ def save_attributes_to_hdf5_group(group, name, data):\n     # Expecting this to never be true.\n     if bad_attributes:\n         raise RuntimeError(\n-            \"The following attributes cannot be saved to HDF5 file because they \"\n-            f\"are larger than {HDF5_OBJECT_HEADER_LIMIT} bytes: {bad_attributes}\"\n+            \"The following attributes cannot be saved to HDF5 file because \"\n+            f\"they are larger than {HDF5_OBJECT_HEADER_LIMIT} \"\n+            f\"bytes: {bad_attributes}\"\n         )\n \n     data_npy = np.asarray(data)\n@@ -1098,8 +1110,9 @@ def _legacy_weights(layer):\n     weights = layer.trainable_weights + layer.non_trainable_weights\n     if any(not isinstance(w, tf.Variable) for w in weights):\n         raise NotImplementedError(\n-            f\"Save or restore weights that is not an instance of `tf.Variable` is \"\n-            f\"not supported in h5, use `save_format='tf'` instead. Received a \"\n-            f\"model or layer {layer.__class__.__name__} with weights {weights}\"\n+            f\"Save or restore weights that is not an instance of `tf.Variable` \"\n+            f\"is not supported in h5, use `save_format='tf'` instead. Received \"\n+            f\"a model or layer {layer.__class__.__name__} \"\n+            f\"with weights {weights}\"\n         )\n     return weights\n\n@@ -75,7 +75,8 @@ class TestPickleProtocol(test_combinations.TestCase):\n         (\"deepcopy\", copy.deepcopy),\n     )\n     def test_unbuilt_models(self, serializer):\n-        \"\"\"Unbuilt models should be copyable & deepcopyable for all model types.\"\"\"\n+        \"\"\"Unbuilt models should be copyable & deepcopyable for all model\n+        types.\"\"\"\n         if not tf.__internal__.tf2.enabled():\n             self.skipTest(\n                 \"pickle model only available in v2 when tf format is used.\"\n\n@@ -49,8 +49,8 @@ def save_model(\n     # pylint: disable=line-too-long\n     \"\"\"Saves a model as a TensorFlow SavedModel or HDF5 file.\n \n-    See the [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n-    for details.\n+    See the [Serialization and Saving\n+    guide](https://keras.io/guides/serialization_and_saving/) for details.\n \n     Usage:\n \n@@ -70,8 +70,8 @@ def save_model(\n     - the model's weights\n     - the model's optimizer's state (if any)\n \n-    Thus models can be reinstantiated in the exact same state, without any of the\n-    code used for model definition or training.\n+    Thus models can be reinstantiated in the exact same state, without any of\n+    the code used for model definition or training.\n \n     Note that the model weights may have different scoped names after being\n     loaded. Scoped names include the model/layer names, such as\n@@ -81,21 +81,24 @@ def save_model(\n     __SavedModel serialization format__\n \n     Keras SavedModel uses `tf.saved_model.save` to save the model and all\n-    trackable objects attached to the model (e.g. layers and variables). The model\n-    config, weights, and optimizer are saved in the SavedModel. Additionally, for\n-    every Keras layer attached to the model, the SavedModel stores:\n+    trackable objects attached to the model (e.g. layers and variables). The\n+    model config, weights, and optimizer are saved in the SavedModel.\n+    Additionally, for every Keras layer attached to the model, the SavedModel\n+    stores:\n \n       * the config and metadata -- e.g. name, dtype, trainable status\n-      * traced call and loss functions, which are stored as TensorFlow subgraphs.\n+      * traced call and loss functions, which are stored as TensorFlow\n+        subgraphs.\n \n     The traced functions allow the SavedModel format to save and load custom\n     layers without the original class definition.\n \n-    You can choose to not save the traced functions by disabling the `save_traces`\n-    option. This will decrease the time it takes to save the model and the\n-    amount of disk space occupied by the output SavedModel. If you enable this\n-    option, then you _must_ provide all custom class definitions when loading\n-    the model. See the `custom_objects` argument in `tf.keras.models.load_model`.\n+    You can choose to not save the traced functions by disabling the\n+    `save_traces` option. This will decrease the time it takes to save the model\n+    and the amount of disk space occupied by the output SavedModel. If you\n+    enable this option, then you _must_ provide all custom class definitions\n+    when loading the model. See the `custom_objects` argument in\n+    `tf.keras.models.load_model`.\n \n     Args:\n         model: Keras model instance to be saved.\n@@ -108,16 +111,17 @@ def save_model(\n         save_format: Either 'tf' or 'h5', indicating whether to save the model\n           to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5'\n           in TF 1.X.\n-        signatures: Signatures to save with the SavedModel. Applicable to the 'tf'\n-          format only. Please see the `signatures` argument in\n+        signatures: Signatures to save with the SavedModel. Applicable to the\n+          'tf' format only. Please see the `signatures` argument in\n           `tf.saved_model.save` for details.\n-        options: (only applies to SavedModel format) `tf.saved_model.SaveOptions`\n-          object that specifies options for saving to SavedModel.\n+        options: (only applies to SavedModel format)\n+          `tf.saved_model.SaveOptions` object that specifies options for saving\n+          to SavedModel.\n         save_traces: (only applies to SavedModel format) When enabled, the\n           SavedModel will store the function traces for each layer. This\n           can be disabled, so that only the configs of each layer are stored.\n-          Defaults to `True`. Disabling this will decrease serialization time and\n-          reduce file size, but it requires that all custom layers/models\n+          Defaults to `True`. Disabling this will decrease serialization time\n+          and reduce file size, but it requires that all custom layers/models\n           implement a `get_config()` method.\n \n     Raises:\n@@ -151,10 +155,10 @@ def save_model(\n             raise NotImplementedError(\n                 \"Saving the model to HDF5 format requires the model to be a \"\n                 \"Functional model or a Sequential model. It does not work for \"\n-                \"subclassed models, because such models are defined via the body of \"\n-                \"a Python method, which isn't safely serializable. Consider saving \"\n-                'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\n-                \"or using `save_weights`.\"\n+                \"subclassed models, because such models are defined via the \"\n+                \"body of a Python method, which isn't safely serializable. \"\n+                \"Consider saving to the Tensorflow SavedModel format (by \"\n+                'setting save_format=\"tf\") or using `save_weights`.'\n             )\n         hdf5_format.save_model_to_hdf5(\n             model, filepath, overwrite, include_optimizer\n@@ -207,10 +211,10 @@ def load_model(\n           options for loading from SavedModel.\n \n     Returns:\n-        A Keras model instance. If the original model was compiled, and saved with\n-        the optimizer, then the returned model will be compiled. Otherwise, the\n-        model will be left uncompiled. In the case that an uncompiled model is\n-        returned, a warning is displayed if the `compile` argument is set to\n+        A Keras model instance. If the original model was compiled, and saved\n+        with the optimizer, then the returned model will be compiled. Otherwise,\n+        the model will be left uncompiled. In the case that an uncompiled model\n+        is returned, a warning is displayed if the `compile` argument is set to\n         `True`.\n \n     Raises:\n@@ -234,7 +238,8 @@ def load_model(\n                     else:\n                         if h5py is None:\n                             raise ImportError(\n-                                \"Filepath looks like a hdf5 file but h5py is not available.\"\n+                                \"Filepath looks like a hdf5 file but h5py is \"\n+                                \"not available.\"\n                                 f\" filepath={filepath_str}\"\n                             )\n                         return hdf5_format.load_model_from_hdf5(\n\n@@ -88,7 +88,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n         self.assert_h5_format(path)\n         with self.assertRaisesRegex(\n             NotImplementedError,\n-            \"requires the model to be a Functional model or a Sequential model.\",\n+            \"requires the model to be a Functional model \"\n+            \"or a Sequential model.\",\n         ):\n             save.save_model(self.subclassed_model, path, save_format=\"h5\")\n \n@@ -476,8 +477,8 @@ class TestWholeModelSaving(test_combinations.TestCase):\n \n         if loaded_model.optimizer:\n             if test_utils.get_save_format() == \"tf\":\n-                # TODO(b/153110928): Keras TF format doesn't restore optimizer weights\n-                # currently.\n+                # TODO(b/153110928): Keras TF format doesn't restore optimizer\n+                # weights currently.\n                 return\n             self.assertAllClose(\n                 model.optimizer.weights, loaded_model.optimizer.weights\n@@ -505,8 +506,9 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             save_format == \"h5\" or not save_kwargs.get(\"save_traces\", True)\n         ) and test_utils.get_model_type() == \"subclass\":\n             # HDF5 format currently does not allow saving subclassed models.\n-            # When saving with `save_traces=False`, the subclassed model must have a\n-            # get_config/from_config, which the autogenerated model does not have.\n+            # When saving with `save_traces=False`, the subclassed model must\n+            # have a get_config/from_config, which the autogenerated model does\n+            # not have.\n             return\n \n         with self.cached_session():\n@@ -1037,8 +1039,8 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             model = keras.models.Sequential(\n                 [keras.layers.Dense(1, input_shape=(3,))]\n             )\n-            # Set the model's optimizer but don't compile. This can happen if the\n-            # model is trained with a custom training loop.\n+            # Set the model's optimizer but don't compile. This can happen if\n+            # the model is trained with a custom training loop.\n             model.optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop(\n                 lr=0.0001\n             )\n@@ -1294,8 +1296,8 @@ class TestWholeModelSaving(test_combinations.TestCase):\n             \"head_1\": np.random.randint(2, size=(2, 5)),\n         }\n \n-        # Make sure metrix prefixing works the same regardless of whether the user\n-        # has fit the model before saving.\n+        # Make sure metrix prefixing works the same regardless of whether the\n+        # user has fit the model before saving.\n         if fit:\n             model.fit(x, y, verbose=0)\n \n@@ -1305,8 +1307,8 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         keras.models.save_model(model, saved_model_dir, save_format=save_format)\n         loaded = keras.models.load_model(saved_model_dir)\n \n-        # Make sure the metrics names from the model before saving match the loaded\n-        # model.\n+        # Make sure the metrics names from the model before saving match the\n+        # loaded model.\n         self.assertSequenceEqual(model.metrics_names, loaded.metrics_names)\n \n     @test_combinations.generate(\n@@ -1344,8 +1346,9 @@ class TestWholeModelSaving(test_combinations.TestCase):\n                 return mask\n \n             # This get_config doesn't actually do anything because our mask is\n-            # static and doesn't need any external information to work. We do need a\n-            # dummy get_config method to prevent the warning from appearing, however.\n+            # static and doesn't need any external information to work. We do\n+            # need a dummy get_config method to prevent the warning from\n+            # appearing, however.\n             def get_config(self, *args, **kwargs):\n                 return {}\n \n@@ -1484,7 +1487,8 @@ class TestWholeModelSavingWithNesting(tf.test.TestCase, parameterized.TestCase):\n         ]\n     )\n     def test_functional(self, model_fn):\n-        \"\"\"Tests serializing a model that uses a nested model to share weights.\"\"\"\n+        \"\"\"Tests serializing a model that uses a nested model to share\n+        weights.\"\"\"\n         if h5py is None:\n             self.skipTest(\"h5py required to run this test\")\n \n\n@@ -388,8 +388,8 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n             )\n             with self.assertRaises(\n                 ValueError,\n-                msg=\"Shape mismatch in layer #0 (named d1) for weight d1_1/kernel:0. \"\n-                \"Weight expects shape (3, 10). \"\n+                msg=\"Shape mismatch in layer #0 (named d1) for weight \"\n+                \"d1_1/kernel:0. Weight expects shape (3, 10). \"\n                 \"Received saved weight with shape (3, 5)\",\n             ):\n                 hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n@@ -537,8 +537,8 @@ class TestWeightSavingAndLoadingTFFormat(\n             model.load_weights(prefix)\n             self.assertAllClose(ref_y_before_train, self.evaluate(model(x)))\n \n-            # Test restore-on-create if this is a subclassed Model (graph Networks\n-            # will have already created their variables).\n+            # Test restore-on-create if this is a subclassed Model (graph\n+            # Networks will have already created their variables).\n             load_model = make_model_fn()\n             load_model.load_weights(prefix)\n             self.assertAllClose(\n@@ -602,7 +602,8 @@ class TestWeightSavingAndLoadingTFFormat(\n             second_model(x)\n             status.run_restore_ops()\n             second_model.save_weights(prefix)\n-            # Check that the second model's checkpoint loads into the original model\n+            # Check that the second model's checkpoint loads into the original\n+            # model\n             status = model.load_weights(prefix)\n             status.run_restore_ops(session)\n             y = self.evaluate(model(x))\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Helper classes that list&validate all attributes to serialize to SavedModel.\"\"\"\n+\"\"\"Helper classes that list&validate all attributes to serialize to\n+SavedModel.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n@@ -25,7 +26,8 @@ from keras.saving.saved_model import utils\n \n \n class SavedModelSaver(object, metaclass=abc.ABCMeta):\n-    \"\"\"Saver defining the methods and properties used to serialize Keras objects.\"\"\"\n+    \"\"\"Saver defining the methods and properties used to serialize Keras\n+    objects.\"\"\"\n \n     def __init__(self, obj):\n         self.obj = obj\n@@ -44,7 +46,8 @@ class SavedModelSaver(object, metaclass=abc.ABCMeta):\n         \"\"\"String stored in metadata field in the SavedModel proto.\n \n         Returns:\n-          A serialized JSON storing information necessary for recreating this layer.\n+          A serialized JSON storing information necessary for recreating this\n+          layer.\n         \"\"\"\n         # TODO(kathywu): check that serialized JSON can be loaded (e.g., if an\n         # object is in the python property)\n@@ -65,8 +68,8 @@ class SavedModelSaver(object, metaclass=abc.ABCMeta):\n \n         This dictionary must be serializable and deserializable to/from JSON.\n \n-        When loading, the items in this dict are used to initialize the object and\n-        define attributes in the revived object.\n+        When loading, the items in this dict are used to initialize the object\n+        and define attributes in the revived object.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -78,8 +81,8 @@ class SavedModelSaver(object, metaclass=abc.ABCMeta):\n         effects.\n \n         Args:\n-          serialization_cache: Dictionary passed to all objects in the same object\n-            graph during serialization.\n+          serialization_cache: Dictionary passed to all objects in the same\n+            object graph during serialization.\n \n         Returns:\n             A dictionary mapping attribute names to checkpointable objects.\n@@ -112,23 +115,24 @@ class SavedModelSaver(object, metaclass=abc.ABCMeta):\n         loaded.w  # AttributeError\n         ```\n \n-        Assigning trackable objects to attributes creates a graph, which is used for\n-        both checkpointing and SavedModel serialization.\n+        Assigning trackable objects to attributes creates a graph, which is used\n+        for both checkpointing and SavedModel serialization.\n \n         When the graph generated from attribute tracking is insufficient, extra\n         objects and functions may be added at serialization time. For example,\n         most models do not have their call function wrapped with a @tf.function\n-        decorator. This results in `model.call` not being saved. Since Keras objects\n-        should be revivable from the SavedModel format, the call function is added\n-        as an extra function to serialize.\n+        decorator. This results in `model.call` not being saved. Since Keras\n+        objects should be revivable from the SavedModel format, the call\n+        function is added as an extra function to serialize.\n \n         This function and `objects_to_serialize` is called multiple times when\n         exporting to SavedModel. Please use the cache to avoid generating new\n-        functions and objects. A fresh cache is created for each SavedModel export.\n+        functions and objects. A fresh cache is created for each SavedModel\n+        export.\n \n         Args:\n-          serialization_cache: Dictionary passed to all objects in the same object\n-            graph during serialization.\n+          serialization_cache: Dictionary passed to all objects in the same\n+            object graph during serialization.\n \n         Returns:\n             A dictionary mapping attribute names to `Function` or\n\n@@ -40,7 +40,8 @@ class Encoder(json.JSONEncoder):\n     \"\"\"JSON encoder and decoder that handles TensorShapes and tuples.\"\"\"\n \n     def default(self, obj):  # pylint: disable=method-hidden\n-        \"\"\"Encodes objects for types that aren't handled by the default encoder.\"\"\"\n+        \"\"\"Encodes objects for types that aren't handled by the default\n+        encoder.\"\"\"\n         if isinstance(obj, tf.TensorShape):\n             items = obj.as_list() if obj.rank is not None else None\n             return {\"class_name\": \"TensorShape\", \"items\": items}\n@@ -93,7 +94,8 @@ def _decode_helper(\n       deserialize: Boolean, defaults to False. When True, deserializes any Keras\n         objects found in `obj`.\n       module_objects: A dictionary of built-in objects to look the name up in.\n-        Generally, `module_objects` is provided by midlevel library implementers.\n+        Generally, `module_objects` is provided by midlevel library\n+        implementers.\n       custom_objects: A dictionary of custom objects to look the name up in.\n         Generally, `custom_objects` is provided by the end user.\n \n@@ -124,8 +126,8 @@ def _decode_helper(\n         elif obj[\"class_name\"] == \"__ellipsis__\":\n             return Ellipsis\n         elif deserialize and \"__passive_serialization__\" in obj:\n-            # __passive_serialization__ is added by the JSON encoder when encoding\n-            # an object that has a `get_config()` method.\n+            # __passive_serialization__ is added by the JSON encoder when\n+            # encoding an object that has a `get_config()` method.\n             try:\n                 return generic_utils.deserialize_keras_object(\n                     obj,\n\n@@ -39,23 +39,24 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n     def _python_properties_internal(self):\n         \"\"\"Returns dictionary of all python properties.\"\"\"\n         # TODO(kathywu): Add support for metrics serialization.\n-        # TODO(kathywu): Synchronize with the keras spec (go/keras-json-spec) once\n-        # the python config serialization has caught up.\n+        # TODO(kathywu): Synchronize with the keras spec (go/keras-json-spec)\n+        # once the python config serialization has caught up.\n         metadata = dict(\n             name=self.obj.name,\n             trainable=self.obj.trainable,\n-            expects_training_arg=self.obj._expects_training_arg,  # pylint: disable=protected-access\n+            expects_training_arg=self.obj._expects_training_arg,\n             dtype=policy.serialize(\n                 self.obj._dtype_policy\n             ),  # pylint: disable=protected-access\n             batch_input_shape=getattr(self.obj, \"_batch_input_shape\", None),\n             stateful=self.obj.stateful,\n-            must_restore_from_config=self.obj._must_restore_from_config,  # pylint: disable=protected-access\n+            must_restore_from_config=self.obj._must_restore_from_config,\n         )\n \n         metadata.update(get_serialized(self.obj))\n         if self.obj.input_spec is not None:\n-            # Layer's input_spec has already been type-checked in the property setter.\n+            # Layer's input_spec has already been type-checked in the property\n+            # setter.\n             metadata[\"input_spec\"] = tf.nest.map_structure(\n                 lambda x: generic_utils.serialize_keras_object(x)\n                 if x\n@@ -120,8 +121,8 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n         functions = save_impl.wrap_layer_functions(\n             self.obj, serialization_cache\n         )\n-        # Attribute validator requires that the default save signature is added to\n-        # function dict, even if the value is None.\n+        # Attribute validator requires that the default save signature is added\n+        # to function dict, even if the value is None.\n         functions[\"_default_save_signature\"] = None\n         return objects, functions\n \n@@ -130,9 +131,10 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n # generic_utils.py) to a separate file.\n def get_serialized(obj):\n     with generic_utils.skip_failed_serialization():\n-        # Store the config dictionary, which may be used when reviving the object.\n-        # When loading, the program will attempt to revive the object from config,\n-        # and if that fails, the object will be revived from the SavedModel.\n+        # Store the config dictionary, which may be used when reviving the\n+        # object.  When loading, the program will attempt to revive the object\n+        # from config, and if that fails, the object will be revived from the\n+        # SavedModel.\n         return generic_utils.serialize_keras_object(obj)\n \n \n@@ -152,7 +154,7 @@ class InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n             dtype=self.obj.dtype,\n             sparse=self.obj.sparse,\n             ragged=self.obj.ragged,\n-            batch_input_shape=self.obj._batch_input_shape,  # pylint: disable=protected-access\n+            batch_input_shape=self.obj._batch_input_shape,\n             config=self.obj.get_config(),\n         )\n \n@@ -175,12 +177,12 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n             serialization_cache\n         )\n         states = tf.__internal__.tracking.wrap(self.obj.states)\n-        # SaveModel require all the objects to be Trackable when saving.\n-        # If the states is still a tuple after wrap_or_unwrap, it means it doesn't\n-        # contain any trackable item within it, eg empty tuple or (None, None) for\n-        # stateless ConvLSTM2D. We convert them to list so that wrap_or_unwrap can\n-        # make it a Trackable again for saving. When loaded, ConvLSTM2D is\n-        # able to handle the tuple/list conversion.\n+        # SaveModel require all the objects to be Trackable when saving.  If the\n+        # states is still a tuple after wrap_or_unwrap, it means it doesn't\n+        # contain any trackable item within it, eg empty tuple or (None, None)\n+        # for stateless ConvLSTM2D. We convert them to list so that\n+        # wrap_or_unwrap can make it a Trackable again for saving. When loaded,\n+        # ConvLSTM2D is able to handle the tuple/list conversion.\n         if isinstance(states, tuple):\n             states = tf.__internal__.tracking.wrap(list(states))\n         objects[\"states\"] = states\n@@ -193,9 +195,10 @@ class VocabularySavedModelSaver(LayerSavedModelSaver):\n     This class is needed for StringLookup, IntegerLookup, and TextVectorization,\n     which all have a vocabulary as part of the config. Currently, we keep this\n     vocab as part of the config until saving, when we need to clear it to avoid\n-    initializing a StaticHashTable twice (once when restoring the config and once\n-    when restoring restoring module resources). After clearing the vocab, we\n-    persist a property to the layer indicating it was constructed with a vocab.\n+    initializing a StaticHashTable twice (once when restoring the config and\n+    once when restoring restoring module resources). After clearing the vocab,\n+    we persist a property to the layer indicating it was constructed with a\n+    vocab.\n     \"\"\"\n \n     @property\n@@ -204,7 +207,8 @@ class VocabularySavedModelSaver(LayerSavedModelSaver):\n         metadata = self._python_properties_internal()\n         # Clear the vocabulary from the config during saving.\n         metadata[\"config\"][\"vocabulary\"] = None\n-        # Persist a property to track that a vocabulary was passed on construction.\n+        # Persist a property to track that a vocabulary was passed on\n+        # construction.\n         metadata[\"config\"][\n             \"has_input_vocabulary\"\n         ] = self.obj._has_input_vocabulary  # pylint: disable=protected-access\n\n@@ -70,26 +70,27 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n     \"\"\"Loads Keras objects from a SavedModel.\n \n     Any Keras layer or model saved to the SavedModel will be loaded back\n-    as Keras objects. Other objects are loaded as regular trackable objects (same\n-    as `tf.saved_model.load`).\n+    as Keras objects. Other objects are loaded as regular trackable objects\n+    (same as `tf.saved_model.load`).\n \n     Currently, Keras saving/loading only retains the Keras object's weights,\n     losses, and call function.\n \n-    The loaded model can be re-compiled, but the original optimizer, compiled loss\n-    functions, and metrics are not retained. This is temporary, and `model.save`\n-    will soon be able to serialize compiled models.\n+    The loaded model can be re-compiled, but the original optimizer, compiled\n+    loss functions, and metrics are not retained. This is temporary, and\n+    `model.save` will soon be able to serialize compiled models.\n \n     Args:\n       path: Path to SavedModel.\n       compile: If true, compile the model after loading it.\n-      options: Optional `tf.saved_model.LoadOptions` object that specifies options\n-        for loading from SavedModel.\n+      options: Optional `tf.saved_model.LoadOptions` object that specifies\n+        options for loading from SavedModel.\n \n     Returns:\n       Object loaded from SavedModel.\n     \"\"\"\n-    # TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\n+    # TODO(kathywu): Add saving/loading of optimizer, compiled losses and\n+    # metrics.\n     # TODO(kathywu): Add code to load from objects that contain all endpoints\n \n     # Look for metadata file or parse the SavedModel\n@@ -120,7 +121,8 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n         _read_legacy_metadata(object_graph_def, metadata, path)\n \n     if not metadata.nodes:\n-        # When there are no Keras objects, return the results from the core loader\n+        # When there are no Keras objects, return the results from the core\n+        # loader\n         return tf.saved_model.load(path, options=options)\n \n     metadata = _update_to_current_version(metadata)\n@@ -218,11 +220,13 @@ def _read_legacy_metadata(object_graph_def, metadata, path):\n         ):\n             if not proto.user_object.metadata:\n                 raise ValueError(\n-                    f\"Unable to create a Keras model from SavedModel at {path}. \"\n-                    \"This SavedModel was exported with `tf.saved_model.save`, and \"\n-                    \"lacks the Keras metadata file. Please save your Keras model by \"\n-                    \"calling `model.save`or `tf.keras.models.save_model`. Note that \"\n-                    \"you can still load this SavedModel with `tf.saved_model.load`.\"\n+                    \"Unable to create a Keras model from SavedModel at \"\n+                    f\"{path}. This SavedModel was exported with \"\n+                    \"`tf.saved_model.save`, and lacks the Keras metadata file. \"\n+                    \"Please save your Keras model by calling `model.save` \"\n+                    \"or `tf.keras.models.save_model`. Note that \"\n+                    \"you can still load this SavedModel with \"\n+                    \"`tf.saved_model.load`.\"\n                 )\n             metadata.nodes.add(\n                 node_id=node_id,\n@@ -236,7 +240,8 @@ def _read_legacy_metadata(object_graph_def, metadata, path):\n \n \n def _generate_object_paths(object_graph_def):\n-    \"\"\"Traverses through an ObjectGraphDef and builds a map of all node paths.\"\"\"\n+    \"\"\"Traverses through an ObjectGraphDef and builds a map of all node\n+    paths.\"\"\"\n     paths = {0: \"root\"}\n     nodes_to_visit = [0]\n \n@@ -274,13 +279,13 @@ class KerasObjectLoader:\n     Layers and models are revived from either the config or SavedModel following\n     these rules:\n     1. If object is a graph network (i.e. Sequential or Functional) then it will\n-       be initialized using the structure from the config only after the children\n-       layers have been created. Graph networks must be initialized with inputs\n-       and outputs, so all child layers must be created beforehand.\n+       be initialized using the structure from the config only after the\n+       children layers have been created. Graph networks must be initialized\n+       with inputs and outputs, so all child layers must be created beforehand.\n     2. If object's config exists and the class can be found, then revive from\n        config.\n-    3. Object may have already been created if its parent was revived from config.\n-       In this case, do nothing.\n+    3. Object may have already been created if its parent was revived from\n+       config. In this case, do nothing.\n     4. If nothing of the above applies, compose the various artifacts from the\n        SavedModel to create a subclassed layer or model. At this time, custom\n        metrics are not supported.\n@@ -297,23 +302,25 @@ class KerasObjectLoader:\n         }\n         self.loaded_nodes = {}  # Maps node path -> loaded node\n \n-        # Store all node ids that have already been traversed when tracking nodes\n-        # that were recreated from the config.\n+        # Store all node ids that have already been traversed when tracking\n+        # nodes that were recreated from the config.\n         self._traversed_nodes_from_config = set()\n \n-        # Maps model id -> (blank model obj, list of child layer or their node ids)\n-        # This tracks all layers in functional and sequential models. These models\n-        # are only reconstructed after all of their child layers have been created.\n+        # Maps model id -> (blank model obj, list of child layer or their node\n+        # ids) This tracks all layers in functional and sequential models. These\n+        # models are only reconstructed after all of their child layers have\n+        # been created.\n         self.model_layer_dependencies = {}\n         self._models_to_reconstruct = []\n \n     def del_tracking(self):\n-        \"\"\"Removes tracked references that are only used when loading the model.\"\"\"\n+        \"\"\"Removes tracked references that are only used when loading the\n+        model.\"\"\"\n         # Now that the node object has been fully loaded, and the checkpoint has\n         # been restored, the object no longer needs to track objects added from\n         # SerializedAttributes. (Note that saving a training checkpoint still\n-        # functions correctly, because layers and variables are tracked separately\n-        # by the Layer object.)\n+        # functions correctly, because layers and variables are tracked\n+        # separately by the Layer object.)\n         # TODO(kathywu): Instead of outright deleting these nodes (which would\n         # make restoring from a different checkpoint tricky), mark them as extra\n         # dependencies that are OK to overwrite.\n@@ -327,10 +334,10 @@ class KerasObjectLoader:\n                 node._delete_tracking(name)  # pylint: disable=protected-access\n \n             if isinstance(node, functional_lib.Functional):\n-                # Delete the temporary layer dependencies, which were used to restore\n-                # the checkpointed values. When the model is live, the user can delete\n-                # or add layers to the model at any time, so these layer dependencies\n-                # may be obsolete.\n+                # Delete the temporary layer dependencies, which were used to\n+                # restore the checkpointed values. When the model is live, the\n+                # user can delete or add layers to the model at any time, so\n+                # these layer dependencies may be obsolete.\n                 dependencies = list(\n                     node._self_unconditional_dependency_names\n                 )  # pylint: disable=protected-access\n@@ -406,13 +413,13 @@ class KerasObjectLoader:\n \n             if child_id in self.loaded_nodes:\n                 if self.loaded_nodes[child_id][0] is not obj_child:\n-                    # This means that the same trackable object is referenced by two\n-                    # different objects that were recreated from the config.\n+                    # This means that the same trackable object is referenced by\n+                    # two different objects that were recreated from the config.\n                     logging.warning(\n                         \"Looks like there is an object (perhaps variable or \"\n-                        \"layer) that is shared between different layers/models. \"\n-                        \"This may cause issues when restoring the variable \"\n-                        \"values. Object: {}\".format(obj_child)\n+                        \"layer) that is shared between different \"\n+                        \"layers/models. This may cause issues when restoring \"\n+                        \"the variable values. Object: {}\".format(obj_child)\n                     )\n                 continue\n \n@@ -440,8 +447,8 @@ class KerasObjectLoader:\n     def load_layers(self, compile=True):  # pylint: disable=redefined-builtin\n         \"\"\"Load all layer nodes from the metadata.\"\"\"\n         # Load metrics after models and layers, since it's likely that models\n-        # and layers will create the metric when initialized (this avoids wasting\n-        # time by creating objects multiple times).\n+        # and layers will create the metric when initialized (this avoids\n+        # wasting time by creating objects multiple times).\n         metric_list = []\n         for node_metadata in self._metadata.values():\n             if node_metadata.identifier == constants.METRIC_IDENTIFIER:\n@@ -462,9 +469,10 @@ class KerasObjectLoader:\n                     node_metadata.metadata,\n                 )\n             except ValueError as e:\n-                # Metrics are only needed when the model is compiled later. We ignore\n-                # errors when trying to load custom metrics when `compile=False` until\n-                # custom metrics are serialized properly (b/135550038).\n+                # Metrics are only needed when the model is compiled later. We\n+                # ignore errors when trying to load custom metrics when\n+                # `compile=False` until custom metrics are serialized properly\n+                # (b/135550038).\n                 if compile:\n                     raise e\n                 logging.warning(\n@@ -482,8 +490,8 @@ class KerasObjectLoader:\n         if node_id in self.loaded_nodes:\n             node, setter = self.loaded_nodes[node_id]\n \n-            # Revive setter requires the object to have a `_serialized_attributes`\n-            # property. Add it here.\n+            # Revive setter requires the object to have a\n+            # `_serialized_attributes` property. Add it here.\n             _maybe_add_serialized_attributes(node, metadata)\n \n             config = metadata.get(\"config\")\n@@ -496,8 +504,8 @@ class KerasObjectLoader:\n                     self._models_to_reconstruct.append(node_id)\n             return node, setter\n \n-        # Detect whether this object can be revived from the config. If not, then\n-        # revive from the SavedModel instead.\n+        # Detect whether this object can be revived from the config. If not,\n+        # then revive from the SavedModel instead.\n         obj, setter = self._revive_from_config(identifier, metadata, node_id)\n         if obj is None:\n             obj, setter = revive_custom_object(identifier, metadata)\n@@ -546,10 +554,10 @@ class KerasObjectLoader:\n         if not model_is_functional_or_sequential:\n             return None\n \n-        # Revive functional and sequential models as blank model objects for now (\n-        # must be initialized to enable setattr tracking and attribute caching).\n-        # Reconstruction of the network is deferred until all of the model's layers\n-        # have been revived.\n+        # Revive functional and sequential models as blank model objects for now\n+        # ( must be initialized to enable setattr tracking and attribute\n+        # caching).  Reconstruction of the network is deferred until all of the\n+        # model's layers have been revived.\n         if class_name == \"Sequential\":\n             model = models_lib.Sequential(name=config[\"name\"])\n         # The model is a custom Sequential model.\n@@ -561,8 +569,8 @@ class KerasObjectLoader:\n                 inputs=[], outputs=[], name=config[\"name\"]\n             )\n \n-        # Record this model and its layers. This will later be used to reconstruct\n-        # the model.\n+        # Record this model and its layers. This will later be used to\n+        # reconstruct the model.\n         layers = self._get_child_layer_node_ids(node_id)\n         self.model_layer_dependencies[node_id] = (model, layers)\n         if not layers:\n@@ -570,11 +578,13 @@ class KerasObjectLoader:\n         return model\n \n     def _revive_layer_or_model_from_config(self, metadata, node_id):\n-        \"\"\"Revives a layer/custom model from config; returns None if infeasible.\"\"\"\n-        # Check that the following requirements are met for reviving from config:\n+        \"\"\"Revives a layer/custom model from config; returns None if\n+        infeasible.\"\"\"\n+        # Check that the following requirements are met for reviving from\n+        # config:\n         #    1. Object can be deserialized from config.\n-        #    2. If the object needs to be built, then the build input shape can be\n-        #       found.\n+        #    2. If the object needs to be built, then the build input shape can\n+        #       be found.\n         class_name = metadata.get(\"class_name\")\n         config = metadata.get(\"config\")\n         shared_object_id = metadata.get(\"shared_object_id\")\n@@ -589,17 +599,19 @@ class KerasObjectLoader:\n                 )\n             )\n         except (TypeError, KeyError) as e:\n-            # A name conflict has occurred. The `class_name` is in the Keras native\n-            # framework; however, the value in the framework is different from the\n-            # user's class definition which confuses the KerasObjectLoader.\n+            # A name conflict has occurred. The `class_name` is in the Keras\n+            # native framework; however, the value in the framework is different\n+            # from the user's class definition which confuses the\n+            # KerasObjectLoader.\n             builtin_layer = layers_module.get_builtin_layer(class_name)\n             if builtin_layer:\n                 raise RuntimeError(\n-                    f\"Unable to restore object of class '{class_name}' likely due to \"\n-                    f\"name conflict with built-in Keras class '{builtin_layer}'. To \"\n-                    \"override the built-in Keras definition of the object, decorate \"\n-                    \"your class with `@keras.utils.register_keras_serializable` and \"\n-                    \"include that file in your program, or pass your class in a \"\n+                    f\"Unable to restore object of class '{class_name}' likely \"\n+                    f\"due to name conflict with built-in Keras class \"\n+                    f\"'{builtin_layer}'. To override the built-in Keras \"\n+                    \"definition of the object, decorate your class with \"\n+                    \"`@keras.utils.register_keras_serializable` and include \"\n+                    \"that file in your program, or pass your class in a \"\n                     \"`keras.utils.CustomObjectScope` that wraps this load call.\"\n                 ) from e\n             else:\n@@ -611,7 +623,8 @@ class KerasObjectLoader:\n                 return None\n \n         # Use the dtype, name, and trainable status. Often times these are not\n-        # specified in custom configs, so retrieve their values from the metadata.\n+        # specified in custom configs, so retrieve their values from the\n+        # metadata.\n         # pylint: disable=protected-access\n         obj._name = metadata[\"name\"]\n         if metadata.get(\"trainable\") is not None:\n@@ -685,15 +698,16 @@ class KerasObjectLoader:\n     def finalize_objects(self):\n         \"\"\"Finish setting up Keras objects.\n \n-        This function is executed after all objects and functions have been created.\n-        Call functions and losses are attached to each layer, and once all layers\n-        have been fully set up, graph networks are initialized.\n+        This function is executed after all objects and functions have been\n+        created.  Call functions and losses are attached to each layer, and once\n+        all layers have been fully set up, graph networks are initialized.\n \n         Subclassed models that are revived from the SavedModel are treated like\n         layers, and have their call/loss functions attached here.\n         \"\"\"\n-        # Finish setting up layers and subclassed models. This step attaches call\n-        # functions and losses to each object, and sets model inputs/outputs.\n+        # Finish setting up layers and subclassed models. This step attaches\n+        # call functions and losses to each object, and sets model\n+        # inputs/outputs.\n         layers_revived_from_config = []\n         layers_revived_from_saved_model = []\n         for node_id, (node, _) in self.loaded_nodes.items():\n@@ -720,7 +734,8 @@ class KerasObjectLoader:\n         _finalize_saved_model_layers(layers_revived_from_saved_model)\n         _finalize_config_layers(layers_revived_from_config)\n \n-        # Initialize graph networks, now that layer dependencies have been resolved.\n+        # Initialize graph networks, now that layer dependencies have been\n+        # resolved.\n         self._reconstruct_all_models()\n \n     def _unblock_model_reconstruction(self, layer_id, layer):\n@@ -765,8 +780,8 @@ class KerasObjectLoader:\n \n         # Set up model inputs\n         if model.inputs:\n-            # Inputs may already be created if the model is instantiated in another\n-            # object's __init__.\n+            # Inputs may already be created if the model is instantiated in\n+            # another object's __init__.\n             pass\n         elif isinstance(model, models_lib.Sequential):\n             if not layers or not isinstance(layers[0], input_layer.InputLayer):\n@@ -820,7 +835,8 @@ class KerasObjectLoader:\n         self._unblock_model_reconstruction(model_id, model)\n \n     def _get_child_layer_node_ids(self, node_id):\n-        \"\"\"Returns the node ids of each layer in a Sequential/Functional model.\"\"\"\n+        \"\"\"Returns the node ids of each layer in a Sequential/Functional\n+        model.\"\"\"\n         # Sequential and Functional track layers with names following the format\n         # \"layer-N\". Use this to generate the list of layers.\n         num_layers = 0\n@@ -848,8 +864,8 @@ class KerasObjectLoader:\n \n         A helper method for traversing the object graph proto.\n \n-        As an example, say that the object graph proto in the SavedModel contains an\n-        object with the following child and grandchild attributes:\n+        As an example, say that the object graph proto in the SavedModel\n+        contains an object with the following child and grandchild attributes:\n \n         `parent.child_a.child_b`\n \n@@ -933,8 +949,9 @@ def _finalize_saved_model_layers(layers):\n                 \"expects_training_arg\"\n             ]\n             if \"training\" in layer_call.function_spec.arg_names:\n-                # This could change the value of `expects_training_arg` if this layer\n-                # doesn't expect a training arg, but has a child layer that does.\n+                # This could change the value of `expects_training_arg` if this\n+                # layer doesn't expect a training arg, but has a child layer\n+                # that does.\n                 expects_training_arg = True\n             layer._init_call_fn_args(expects_training_arg)\n         else:\n@@ -970,9 +987,9 @@ def _finalize_saved_model_layers(layers):\n                     inputs, args, kwargs\n                 )  # pylint: disable=protected-access\n \n-                # V1 models require calling _set_inputs to set the `.inputs` attr.\n-                # Skip this step when there are multiple tensor inputs (this behavior\n-                # is not well supported in V1 models).\n+                # V1 models require calling _set_inputs to set the `.inputs`\n+                # attr.  Skip this step when there are multiple tensor inputs\n+                # (this behavior is not well supported in V1 models).\n                 if not any(\n                     isinstance(x, tf.TensorSpec)\n                     for x in tf.nest.flatten([args, kwargs])\n@@ -997,11 +1014,12 @@ def _unable_to_call_layer_due_to_serialization_issue(\n     Keras Model/Layer serialization is relatively relaxed because SavedModels\n     are not always loaded back as keras models. Thus, when there is an issue\n     tracing a non-signature function, a warning is logged instead of raising an\n-    error. This results in a SavedModel where the model's call function is saved,\n-    but the internal layer call functions are not.\n+    error. This results in a SavedModel where the model's call function is\n+    saved, but the internal layer call functions are not.\n \n     When deserialized with `tf.keras.models.load_model`, the internal layers\n-    which do not have serialized call functions should raise an error when called.\n+    which do not have serialized call functions should raise an error when\n+    called.\n \n     Args:\n       layer: Layer without the serialized call function.\n@@ -1029,19 +1047,19 @@ def _unable_to_call_layer_due_to_serialization_issue(\n def _finalize_config_layers(layers):\n     \"\"\"Runs the final steps of loading Keras Layers from config.\"\"\"\n     for layer in layers:\n-        # It is assumed that layers define their unconditional losses after being\n-        # recreated from the config and built. The exceptions to this\n-        # are Functional and Sequential models, which only store conditional losses\n-        # (losses dependent on the inputs) in the config. Unconditional losses like\n-        # weight regularization must be revived from the SavedModel.\n+        # It is assumed that layers define their unconditional losses after\n+        # being recreated from the config and built. The exceptions to this are\n+        # Functional and Sequential models, which only store conditional losses\n+        # (losses dependent on the inputs) in the config. Unconditional losses\n+        # like weight regularization must be revived from the SavedModel.\n         if _is_graph_network(layer):\n             _restore_layer_unconditional_losses(layer)\n \n         # Some layers, like Dense, record their activation loss function in the\n         # config. However, not all layers do this, so the activation loss may be\n         # missing when restored from the config/hdf5.\n-        # TODO(kathywu): Investigate ways to improve the config to ensure consistent\n-        # loading behavior between HDF5 and SavedModel.\n+        # TODO(kathywu): Investigate ways to improve the config to ensure\n+        # consistent loading behavior between HDF5 and SavedModel.\n         _restore_layer_activation_loss(layer)\n \n         # Restore metrics list.\n@@ -1098,7 +1116,8 @@ def _restore_layer_activation_loss(layer):\n             layer.activity_regularizer = activity_regularizer\n         except AttributeError:\n             # This may happen if a layer wrapper is saved with an activity\n-            # regularizer. The wrapper object's activity regularizer is unsettable.\n+            # regularizer. The wrapper object's activity regularizer is\n+            # unsettable.\n             pass\n \n \n@@ -1139,8 +1158,8 @@ def revive_custom_object(identifier, metadata):\n         raise ValueError(\n             f\"Unable to restore custom object of type {identifier}. \"\n             f\"Please make sure that any custom layers are included in the \"\n-            f\"`custom_objects` arg when calling `load_model()` and make sure that \"\n-            f\"all layers implement `get_config` and `from_config`.\"\n+            f\"`custom_objects` arg when calling `load_model()` and make sure \"\n+            f\"that all layers implement `get_config` and `from_config`.\"\n         )\n \n \n@@ -1151,7 +1170,8 @@ def _restore_layer_metrics(layer):\n     }  # pylint: disable=protected-access\n     for name, metric in metrics_list.items():\n         if name not in layer_metrics:\n-            # Metrics may be added during initialization/building of custom layers.\n+            # Metrics may be added during initialization/building of custom\n+            # layers.\n             layer._metrics.append(metric)  # pylint: disable=protected-access\n \n \n@@ -1222,15 +1242,15 @@ def _revive_setter(layer, name, value):\n         and re.match(r\"^layer(_with_weights)?-[\\d+]\", name) is not None\n     ):\n         # Edges named \"layer-n\" or \"layer_with_weights-n\", which are tracked in\n-        # network._track_layers, should not be added as an attribute. They should\n-        # be temporarily added as a dependency so that checkpointed values can be\n-        # restored. These dependencies are manually deleted in\n+        # network._track_layers, should not be added as an attribute. They\n+        # should be temporarily added as a dependency so that checkpointed\n+        # values can be restored. These dependencies are manually deleted in\n         # KerasObjectLoader.del_tracking.\n \n-        # Set `overwrite=True` in the case that `layer` already tracks a different\n-        # layer-n. This may cause variable values to not be loaded properly in the\n-        # original layer-n, but we already warn the users about this\n-        # (ctrl-f \"shared between different layers/models\").\n+        # Set `overwrite=True` in the case that `layer` already tracks a\n+        # different layer-n. This may cause variable values to not be loaded\n+        # properly in the original layer-n, but we already warn the users about\n+        # this (ctrl-f \"shared between different layers/models\").\n         layer._track_trackable(\n             value, name, overwrite=True\n         )  # pylint: disable=protected-access\n@@ -1287,8 +1307,8 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n         ]\n     else:\n         raise ValueError(\n-            f\"Unable to decode Keras layer config. Config should be a dictionary, \"\n-            f\"tuple or list. Received: config={config}\"\n+            f\"Unable to decode Keras layer config. Config should be a \"\n+            f\"dictionary, tuple or list. Received: config={config}\"\n         )\n \n \n@@ -1305,8 +1325,8 @@ def infer_inputs_from_restored_call_function(fn):\n \n     def common_spec(x, y):\n         if not isinstance(x, tf.TypeSpec):\n-            # Doesn't particularly matter what is returned in this case because the\n-            # result will be filtered out in _set_input_shape.\n+            # Doesn't particularly matter what is returned in this case because\n+            # the result will be filtered out in _set_input_shape.\n             return x\n         # pylint:disable=protected-access\n         result = x._without_tensor_names().most_specific_common_supertype(\n@@ -1329,7 +1349,8 @@ class RevivedNetwork(RevivedLayer):\n \n     @classmethod\n     def _init_from_metadata(cls, metadata):\n-        \"\"\"Create revived network from metadata stored in the SavedModel proto.\"\"\"\n+        \"\"\"Create revived network from metadata stored in the SavedModel\n+        proto.\"\"\"\n         revived_obj = cls(name=metadata[\"name\"])\n \n         # Store attributes revived from SerializedAttributes in a un-tracked\n\n@@ -54,8 +54,8 @@ class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n         if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\n             default_signature = save_impl.default_save_signature(self.obj)\n \n-        # Other than the default signature function, all other attributes match with\n-        # the ones serialized by Layer.\n+        # Other than the default signature function, all other attributes match\n+        # with the ones serialized by Layer.\n         objects, functions = super()._get_serialized_attributes_internal(\n             serialization_cache\n         )\n\n@@ -81,7 +81,8 @@ class OrderPreservingSet(collections.abc.MutableSet):\n         # ensure that other is ordered before performing __or__\n         if not isinstance(other, OrderPreservingSet):\n             raise TypeError(\n-                \"cannot union an 'OrderPreservingSet' with an unordered iterable.\"\n+                \"cannot union an 'OrderPreservingSet' with an \"\n+                \"unordered iterable.\"\n             )\n         result = self._from_iterable(value for value in self)\n         for value in other:\n\n@@ -51,8 +51,9 @@ class SubclassedModelNoConfig(keras.Model):\n                 CustomLayerNoConfig(self.a + 3, self.b + 4),\n                 keras.Sequential(\n                     [\n-                        # TODO(b/145029112): Bug with losses when there are shared layers.\n-                        # self.shared,  <-- Enable when bug is fixed.\n+                        # TODO(b/145029112): Bug with losses when there are\n+                        # shared layers.  self.shared,  <-- Enable when bug is\n+                        # fixed.\n                         CustomLayerNoConfig(self.a + 5, self.b + 6)\n                     ]\n                 ),\n@@ -237,8 +238,8 @@ class ReviveTestBase(test_combinations.TestCase):\n         self.assertAllClose(sum(model.losses), sum(revived.losses))\n         self.assertAllClose(len(model.losses), len(revived.losses))\n         self.assertEqual(len(model.metrics), len(revived.metrics))\n-        # TODO(b/150403085): Investigate why the metric order changes when running\n-        # this test in tf-nightly.\n+        # TODO(b/150403085): Investigate why the metric order changes when\n+        # running this test in tf-nightly.\n         self.assertAllClose(\n             sorted([m.result() for m in model.metrics]),\n             sorted([m.result() for m in revived.metrics]),\n@@ -256,8 +257,8 @@ class ReviveTestBase(test_combinations.TestCase):\n             if \"WithConfig\" in type(model_layer).__name__:\n                 self.assertEqual(type(model_layer), type(revived_layer))\n             else:\n-                # When loading layers from SavedModel, a new class is dynamically\n-                # created with the same name.\n+                # When loading layers from SavedModel, a new class is\n+                # dynamically created with the same name.\n                 self.assertEqual(\n                     type(model_layer).__name__, type(revived_layer).__name__\n                 )\n@@ -326,8 +327,8 @@ class TestBigModelRevive(ReviveTestBase):\n         # Run data through the Model to create save spec and weights.\n         model.predict(np.ones((10, 2, 3)), batch_size=10)\n \n-        # Test that the correct checkpointed values are loaded, whether the layer is\n-        # created from the config or SavedModel.\n+        # Test that the correct checkpointed values are loaded, whether the\n+        # layer is created from the config or SavedModel.\n         layer_with_config.c.assign(2 * layer_with_config.c)\n         layer_without_config.c.assign(3 * layer_without_config.c)\n \n\n@@ -55,8 +55,8 @@ def save(\n       overwrite: whether to overwrite the existing filepath.\n       include_optimizer: If True, save the model's optimizer state.\n       signatures: Signatures to save with the SavedModel. Applicable to the 'tf'\n-        format only. Please see the `signatures` argument in `tf.saved_model.save`\n-        for details.\n+        format only. Please see the `signatures` argument in\n+        `tf.saved_model.save` for details.\n       options: (only applies to SavedModel format) `tf.saved_model.SaveOptions`\n         object that specifies options for saving to SavedModel.\n       save_traces: (only applies to SavedModel format) When enabled, the\n@@ -109,7 +109,8 @@ def save(\n \n \n def generate_keras_metadata(saved_nodes, node_paths):\n-    \"\"\"Constructs a KerasMetadata proto with the metadata of each keras object.\"\"\"\n+    \"\"\"Constructs a KerasMetadata proto with the metadata of each keras\n+    object.\"\"\"\n     metadata = saved_metadata_pb2.SavedMetadata()\n     for node_id, node in enumerate(saved_nodes):\n         if isinstance(node, base_layer.Layer):\n@@ -127,12 +128,12 @@ def generate_keras_metadata(saved_nodes, node_paths):\n                 version=versions_pb2.VersionDef(\n                     producer=2, min_consumer=1, bad_consumers=[]\n                 ),\n-                identifier=node._object_identifier,  # pylint: disable=protected-access\n+                identifier=node._object_identifier,\n                 metadata=node._tracking_metadata,\n             )  # pylint: disable=protected-access\n \n-            # Log warning if the node's class name conflicts with a Keras built-in\n-            # object.\n+            # Log warning if the node's class name conflicts with a Keras\n+            # built-in object.\n             class_name = node.__class__.__name__\n             builtin_layer = serialization.get_builtin_layer(class_name)\n             if builtin_layer:\n@@ -141,8 +142,10 @@ def generate_keras_metadata(saved_nodes, node_paths):\n                         \"%s has the same name '%s' as a built-in Keras \"\n                         \"object. Consider renaming %s to avoid naming \"\n                         \"conflicts when loading with \"\n-                        \"`tf.keras.models.load_model`. If renaming is not possible, pass \"\n-                        \"the object in the `custom_objects` parameter of the load \"\n+                        \"`tf.keras.models.load_model`. \"\n+                        \"If renaming is not possible, pass \"\n+                        \"the object in the `custom_objects` \"\n+                        \"parameter of the load \"\n                         \"function.\",\n                         node,\n                         class_name,\n\n@@ -57,7 +57,8 @@ sequential_lib = LazyLoader(\n \n \n def should_skip_serialization(layer):\n-    \"\"\"Skip serializing extra objects and functions if layer inputs aren't set.\"\"\"\n+    \"\"\"Skip serializing extra objects and functions if layer inputs aren't\n+    set.\"\"\"\n     saved_model_input_spec_set = (\n         isinstance(layer, training_lib.Model)\n         and layer._saved_model_inputs_spec is not None\n@@ -250,8 +251,8 @@ def _replace_child_layer_functions(layer, serialization_cache):\n     This step allows functions from parent layers to reference the wrapped\n     functions from their children layers instead of retracing the ops.\n \n-    This function also resets all losses stored in the layer. These are stored in\n-    the returned dictionary. Use `_restore_child_layer_functions` to restore\n+    This function also resets all losses stored in the layer. These are stored\n+    in the returned dictionary. Use `_restore_child_layer_functions` to restore\n     the original attributes.\n \n     Args:\n@@ -272,7 +273,8 @@ def _replace_child_layer_functions(layer, serialization_cache):\n     original_fns = {}\n \n     def replace_layer_functions(child_layer, serialized_fns):\n-        \"\"\"Replaces layer call and activity regularizer with wrapped functions.\"\"\"\n+        \"\"\"Replaces layer call and activity regularizer with wrapped\n+        functions.\"\"\"\n         original_fns[child_layer] = {\n             \"call\": child_layer.call,\n             \"_activity_regularizer\": child_layer._activity_regularizer,\n@@ -320,9 +322,9 @@ def _replace_child_layer_functions(layer, serialization_cache):\n             # This indicates either:\n             #   - circular dependency, which means the current layer's functions\n             #     should be wrapped first.\n-            #   - Child layer's inputs are not defined, so its functions have not been\n-            #     wrapped. In this case, no replacement is necessary so move on to the\n-            #     next child.\n+            #   - Child layer's inputs are not defined, so its functions have\n+            #     not been wrapped. In this case, no replacement is necessary so\n+            #     move on to the next child.\n             continue\n \n         if isinstance(child_layer, metrics.Metric):\n@@ -344,8 +346,9 @@ def _restore_child_layer_functions(original_fns):\n                         child_layer, fn_name, fn\n                     )  # pylint: disable=protected-access\n                 except AttributeError:\n-                    pass  # In the case of _activity_regularizer, setting the attribute\n-                    # may be disallowed.\n+                    # In the case of _activity_regularizer, setting the\n+                    # attribute may be disallowed.\n+                    pass\n \n \n # pylint: disable=protected-access\n@@ -422,8 +425,8 @@ def tracing_enabled():\n class LayerCallCollection:\n     \"\"\"Groups wrapped layer call functions.\n \n-    This is used to ensure that all layer call functions are traced with the same\n-    inputs-\n+    This is used to ensure that all layer call functions are traced with the\n+    same inputs-\n       - call\n       - call_and_return_conditional_losses\n       - call_and_return_all_conditional_losses\n@@ -436,9 +439,10 @@ class LayerCallCollection:\n         self._expects_training_arg = utils.layer_uses_training_bool(layer)\n         self._call_spec = layer._call_spec  # pylint: disable=protected-access\n \n-        # Create new call spec if the layer itself does not accept a training arg,\n-        # but one of its child layers does. When this layer's call functions are\n-        # traced, they will be traced with an added `training` keyword argument.\n+        # Create new call spec if the layer itself does not accept a training\n+        # arg, but one of its child layers does. When this layer's call\n+        # functions are traced, they will be traced with an added `training`\n+        # keyword argument.\n         if (\n             not self.layer._expects_training_arg and self._expects_training_arg\n         ):  # pylint: disable=protected-access\n@@ -454,8 +458,8 @@ class LayerCallCollection:\n         if self._call_spec.arg_names:\n             self._input_arg_name = self._call_spec.arg_names[0]\n         else:\n-            # Layer could be defined with only varargs, in which case use a default\n-            # name.\n+            # Layer could be defined with only varargs, in which case use a\n+            # default name.\n             self._input_arg_name = \"inputs\"\n \n     def _get_layer_inputs(self, layer):\n@@ -465,8 +469,8 @@ class LayerCallCollection:\n           layer: Layer object.\n \n         Returns:\n-          List of possibly nested TensorSpecs of the layer call function inputs in\n-          the form of `(args, kwargs)`\n+          List of possibly nested TensorSpecs of the layer call function inputs\n+          in the form of `(args, kwargs)`\n         \"\"\"\n         if (\n             isinstance(layer.call, tf.__internal__.function.Function)\n@@ -484,10 +488,11 @@ class LayerCallCollection:\n                 spec = input_spec.to_tensor_spec(\n                     x, layer._compute_dtype\n                 )  # pylint: disable=protected-access\n-                # If the shape is too general (e.g. multiple dimensions are allowed),\n-                # return None so that separate functions can be generated for each\n-                # inferred input signature.\n-                # TODO(b/134962016): currently partial signatures are not supported.\n+                # If the shape is too general (e.g. multiple dimensions are\n+                # allowed), return None so that separate functions can be\n+                # generated for each inferred input signature.\n+                # TODO(b/134962016): currently partial signatures are not\n+                # supported.\n                 if spec.shape == tf.TensorShape(None):\n                     return None, None\n                 return spec\n@@ -511,16 +516,13 @@ class LayerCallCollection:\n         kwargs = kwargs.copy()\n \n         for fn in self._functions.values():\n-            # TODO(kathywu): Replace arguments with broader shapes defined in the\n-            # input signature.\n+            # TODO(kathywu): Replace arguments with broader shapes defined in\n+            # the input signature.\n             if self._expects_training_arg:\n \n                 def trace_with_training(value, fn=fn):\n                     nonlocal args, kwargs\n-                    (\n-                        args,\n-                        kwargs,\n-                    ) = self._call_spec.set_arg_value(  # pylint: disable=protected-access\n+                    (args, kwargs,) = self._call_spec.set_arg_value(\n                         \"training\", value, args, kwargs, inputs_in_args=True\n                     )\n                     add_trace_to_queue(fn, args, kwargs, value)\n@@ -539,7 +541,7 @@ class LayerCallCollection:\n \n     def get_training_arg_value(self, args, kwargs):\n         try:\n-            return self._call_spec.get_arg_value(  # pylint: disable=protected-access\n+            return self._call_spec.get_arg_value(\n                 \"training\", args, kwargs, inputs_in_args=True\n             )\n         except KeyError:  # Training is not in args or kwargs.\n@@ -557,18 +559,16 @@ class LayerCallCollection:\n         if (\n             not self.layer._expects_training_arg and self._expects_training_arg\n         ):  # pylint: disable=protected-access\n-            # Add training arg to wrapper function.  # pylint: disable=protected-access\n+            # Add training arg to wrapper function.\n             def wrap_with_training_arg(*args, **kwargs):\n                 if match_layer_training_arg:\n-                    # Remove the training value, since the original call_fn does not\n-                    # expect a training arg. Instead, the training value will be\n-                    # propagated using the call context created in LayerCall.\n+                    # Remove the training value, since the original call_fn does\n+                    # not expect a training arg. Instead, the training value\n+                    # will be propagated using the call context created in\n+                    # LayerCall.\n                     args = list(args)\n                     kwargs = kwargs.copy()\n-                    (\n-                        args,\n-                        kwargs,\n-                    ) = self._call_spec.set_arg_value(  # pylint: disable=protected-access\n+                    (args, kwargs,) = self._call_spec.set_arg_value(\n                         \"training\",\n                         None,\n                         args,\n@@ -619,8 +619,8 @@ class LayerCallCollection:\n                 \"training\", False, args, kwargs, inputs_in_args=True\n             )\n         if None not in tf.nest.flatten([args, kwargs]):\n-            # Manually add traces for layers that have keyword arguments and have\n-            # a fully defined input signature.\n+            # Manually add traces for layers that have keyword arguments and\n+            # have a fully defined input signature.\n             self.add_trace(*args, **kwargs)\n \n \n@@ -629,7 +629,8 @@ def _filtered_inputs(inputs):\n \n \n def layer_call_wrapper(call_collection, method, name):\n-    \"\"\"Ensures layer losses are kept the same, and runs method in call context.\"\"\"\n+    \"\"\"Ensures layer losses are kept the same, and runs method in call\n+    context.\"\"\"\n \n     # Create wrapper that deals with losses and call context.\n     def wrapper(*args, **kwargs):\n@@ -669,7 +670,8 @@ def layer_call_wrapper(call_collection, method, name):\n \n \n class LayerCall:\n-    \"\"\"Function that triggers traces of other functions in the same collection.\"\"\"\n+    \"\"\"Function that triggers traces of other functions in the same\n+    collection.\"\"\"\n \n     def __init__(self, call_collection, call_fn, name):\n         \"\"\"Initializes a LayerCall object.\n@@ -703,15 +705,16 @@ class LayerCall:\n def _wrap_call_and_conditional_losses(layer):\n     \"\"\"Wraps call function that returns a tuple of (outputs, losses).\n \n-    The losses returned are conditional on the inputs passed to the call function.\n-    Unconditional losses (e.g. weight regularizeration) are wrapped separately.\n+    The losses returned are conditional on the inputs passed to the call\n+    function.  Unconditional losses (e.g. weight regularizeration) are wrapped\n+    separately.\n \n     Args:\n       layer: a Keras layer object\n \n     Returns:\n-      python call function that returns outputs and conditional losses -- excludes\n-      activity regularizer\n+      python call function that returns outputs and conditional losses --\n+      excludes activity regularizer\n     \"\"\"\n     # Create function that generates both outputs and losses\n     layer_call = _get_layer_call_method(layer)\n@@ -746,7 +749,8 @@ def _extract_outputs_from_fn(layer, call_and_return_conditional_losses):\n def _append_activity_regularizer_loss(\n     layer, call_fn_with_losses, activity_regularizer_fn\n ):\n-    \"\"\"Appends activity regularizer loss to losses returned by the wrapped fn.\"\"\"\n+    \"\"\"Appends activity regularizer loss to losses returned by the wrapped\n+    fn.\"\"\"\n \n     def fn(inputs, *args, **kwargs):\n         outputs, losses = call_fn_with_losses(inputs, *args, **kwargs)\n\n@@ -143,7 +143,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         return loaded\n \n     def _test_evaluation(self, model, loaded):\n-        # Assert that original and loaded models have the same results when called.\n+        # Assert that original and loaded models have the same results when\n+        # called.\n         self.evaluate(tf.compat.v1.variables_initializer(loaded.variables))\n         self.assertAllClose(\n             self.evaluate(model.weights), self.evaluate(loaded.weights)\n@@ -153,8 +154,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         self.assertAllClose(\n             self.evaluate(model(input_arr)), self.evaluate(loaded(input_arr))\n         )\n-        # Validate losses. The order of conditional losses may change between the\n-        # model and loaded model, so sort the losses first.\n+        # Validate losses. The order of conditional losses may change between\n+        # the model and loaded model, so sort the losses first.\n         if tf.executing_eagerly():\n             self.assertAllClose(\n                 sorted(self.evaluate(model.losses)),\n@@ -255,8 +256,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         input_arr = np.random.random((1, 3))\n         target_arr = np.random.random((1, 3))\n \n-        # Test that symbolic losses are maintained (train_on_batch saves symbolic\n-        # losses.)\n+        # Test that symbolic losses are maintained (train_on_batch saves\n+        # symbolic losses.)\n         model.train_on_batch(input_arr, target_arr)\n         previous_losses = model.losses[:]\n \n@@ -264,8 +265,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         model.save(saved_model_dir, save_format=\"tf\")\n \n         with previous_losses[0].graph.as_default():\n-            # If we try to compare symbolic Tensors in eager mode assertAllEqual will\n-            # return False even if they are the same Tensor.\n+            # If we try to compare symbolic Tensors in eager mode assertAllEqual\n+            # will return False even if they are the same Tensor.\n             self.assertEqual(previous_losses, model.losses)\n \n         if tf.executing_eagerly():\n@@ -340,8 +341,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n \n     @test_combinations.run_with_all_model_types\n     def test_compiled_model(self):\n-        # TODO(b/134519980): Issue with model.fit if the model call function uses\n-        # a tf.function (Graph mode only).\n+        # TODO(b/134519980): Issue with model.fit if the model call function\n+        # uses a tf.function (Graph mode only).\n         if not tf.executing_eagerly():\n             return\n \n@@ -647,12 +648,12 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n             arg_spec = tf_inspect.getfullargspec(fn)\n             fn_defaults = arg_spec.defaults or []\n             defaults = dict()\n-            # The call arg defaults are an n-tuple of the last n elements of the args\n-            # list. (n = # of elements that have a default argument)\n+            # The call arg defaults are an n-tuple of the last n elements of the\n+            # args list. (n = # of elements that have a default argument)\n             for i in range(-1 * len(fn_defaults), 0):\n                 defaults[arg_spec.args[i]] = fn_defaults[i]\n-            # The default training arg will be any (non-None) default specified in the\n-            # method signature, or None if no value is specified.\n+            # The default training arg will be any (non-None) default specified\n+            # in the method signature, or None if no value is specified.\n             defaults.update(arg_spec.kwonlydefaults or {})\n             self.assertEqual(defaults[\"training\"], default_value)\n \n@@ -731,8 +732,8 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n                 loaded_without_scope.predict(np.ones([1, 3]).astype(\"float32\"))\n \n     def testFeatureColumns(self):\n-        # TODO(b/120099662): Error with table initialization with Keras models in\n-        # graph mode.\n+        # TODO(b/120099662): Error with table initialization with Keras models\n+        # in graph mode.\n         if tf.executing_eagerly():\n             numeric = tf.feature_column.numeric_column(\"a\")\n             bucketized = tf.feature_column.bucketized_column(\n@@ -929,8 +930,9 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n         self.assertAllClose(model(input_arr), loaded(input_arr))\n \n     def testSaveBidirectionalLSTM(self):\n-        # Make sure that the input spec of an unrolled RNN is not used when wrapped\n-        # in a Bidirectional layer. https://github.com/keras-team/keras/issues/15454\n+        # Make sure that the input spec of an unrolled RNN is not used when\n+        # wrapped in a Bidirectional layer.\n+        # https://github.com/keras-team/keras/issues/15454\n         input_layer = keras.Input(\n             batch_input_shape=(1, 15, 128), name=\"input\", dtype=tf.float32\n         )\n@@ -1432,9 +1434,9 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n             def update_state(\n                 self, *args\n             ):  # pylint: disable=useless-super-delegation\n-                # Sometimes built-in metrics return an op in update_state. Custom\n-                # metrics don't support returning ops, so wrap the update_state method\n-                # while returning nothing.\n+                # Sometimes built-in metrics return an op in update_state.\n+                # Custom metrics don't support returning ops, so wrap the\n+                # update_state method while returning nothing.\n                 super().update_state(*args)\n \n         with self.cached_session():\n@@ -1500,8 +1502,8 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n \n     @test_combinations.run_with_all_model_types\n     def test_custom_metric_model(self):\n-        # TODO(b/134519980): Issue with `model.fit` if the model call function uses\n-        # a `tf.function` in graph mode.\n+        # TODO(b/134519980): Issue with `model.fit` if the model call function\n+        # uses a `tf.function` in graph mode.\n         if not tf.executing_eagerly():\n             return\n \n\n@@ -37,19 +37,20 @@ class SerializedAttributes:\n \n     Keras models contain many Python-defined components. For example, the\n     trainable_variable property lists the model's trainable variables by\n-    recursively retrieving the trainable variables from each of the child layers.\n-    Another example is model.call, a python function that calls child layers and\n-    adds ops to the backend graph.\n+    recursively retrieving the trainable variables from each of the child\n+    layers.  Another example is model.call, a python function that calls child\n+    layers and adds ops to the backend graph.\n \n     Only Tensorflow checkpointable objects and functions can be serialized to\n-    SavedModel. Serializing a Keras model as-is results in a checkpointable object\n-    that does not resemble a Keras model at all. Thus, extra checkpointable\n-    objects and functions must be created during serialization.\n+    SavedModel. Serializing a Keras model as-is results in a checkpointable\n+    object that does not resemble a Keras model at all. Thus, extra\n+    checkpointable objects and functions must be created during serialization.\n \n     **Defining new serialized attributes**\n     Child classes should be defined using:\n       SerializedAttributes.with_attributes(\n-          'name', checkpointable_objects=[...], functions=[...], copy_from=[...])\n+          'name', checkpointable_objects=[...],\n+          functions=[...], copy_from=[...])\n     This class is used to cache generated checkpointable objects and functions,\n     ensuring that new objects and functions are generated a single time.\n \n@@ -76,27 +77,28 @@ class SerializedAttributes:\n        `set_and_validate_functions`.\n \n     **Common endpoints vs other attributes**\n-    Only common endpoints are attached directly to the root object. Keras-specific\n-    attributes are saved to a separate trackable object with the name \"keras_api\".\n-    The number of objects attached to the root is limited because any naming\n-    conflicts will cause user code to break.\n+    Only common endpoints are attached directly to the root object.\n+    Keras-specific attributes are saved to a separate trackable object with the\n+    name \"keras_api\".  The number of objects attached to the root is limited\n+    because any naming conflicts will cause user code to break.\n \n     Another reason is that this will only affect users who call\n     `tf.saved_model.load` instead of `tf.keras.models.load_model`. These are\n     advanced users who are likely to have defined their own tf.functions and\n-    trackable objects. The added Keras-specific attributes are kept out of the way\n-    in the \"keras_api\" namespace.\n+    trackable objects. The added Keras-specific attributes are kept out of the\n+    way in the \"keras_api\" namespace.\n \n     Properties defined in this class may be used to filter out keras-specific\n     attributes:\n     - `functions_to_serialize`: Returns dict of functions to attach to the root\n         object.\n-    - `checkpointable_objects_to_serialize`: Returns dict of objects to attach to\n-        the root object (including separate trackable object containing\n+    - `checkpointable_objects_to_serialize`: Returns dict of objects to attach\n+         to the root object (including separate trackable object containing\n          keras-specific attributes)\n \n     All changes to the serialized attributes must be backwards-compatible, so\n-    attributes should not be removed or modified without sufficient justification.\n+    attributes should not be removed or modified without sufficient\n+    justification.\n     \"\"\"\n \n     @staticmethod\n@@ -107,8 +109,8 @@ class SerializedAttributes:\n \n         Args:\n           name: Name of subclass\n-          checkpointable_objects: List of checkpointable objects to be serialized\n-            in the SavedModel.\n+          checkpointable_objects: List of checkpointable objects to be\n+            serialized in the SavedModel.\n           functions: List of functions to be serialized in the SavedModel.\n           copy_from: List of other SerializedAttributes subclasses. The returned\n             class will copy checkpoint objects/functions from each subclass.\n@@ -125,8 +127,8 @@ class SerializedAttributes:\n                 checkpointable_objects.extend(cls.all_checkpointable_objects)\n                 functions.extend(cls.all_functions)\n \n-        # OrderPreservingSets are used here to guarantee serialization determinism\n-        # of Keras objects.\n+        # OrderPreservingSets are used here to guarantee serialization\n+        # determinism of Keras objects.\n         classdict = {\n             \"all_checkpointable_objects\": ops.OrderPreservingSet(\n                 checkpointable_objects\n@@ -178,7 +180,8 @@ class SerializedAttributes:\n \n     @property\n     def functions_to_serialize(self):\n-        \"\"\"Returns functions to attach to the root object during serialization.\"\"\"\n+        \"\"\"Returns functions to attach to the root object during\n+        serialization.\"\"\"\n         functions = {}\n         for key, v in self.functions.items():\n             if key in CommonEndpoints.all_functions:\n@@ -204,7 +207,8 @@ class SerializedAttributes:\n             if key in function_dict:\n                 if function_dict[\n                     key\n-                ] is not None and not isinstance(  # Not all functions are required\n+                    # Not all functions are required\n+                ] is not None and not isinstance(\n                     function_dict[key],\n                     (\n                         tf.__internal__.function.Function,\n@@ -213,9 +217,10 @@ class SerializedAttributes:\n                     ),\n                 ):\n                     raise ValueError(\n-                        \"The tf.function dictionary contained a non-function object: \"\n-                        f\"{function_dict[key]} (for key {key}). Only tf.function \"\n-                        \"instances or ConcreteFunction instances should be passed.\"\n+                        \"The tf.function dictionary contained a non-function \"\n+                        f\"object: {function_dict[key]} (for key {key}). Only \"\n+                        \"tf.function instances or ConcreteFunction instances \"\n+                        \"should be passed.\"\n                     )\n                 fn = function_dict[key]\n                 self._function_dict[key] = fn\n@@ -229,7 +234,8 @@ class SerializedAttributes:\n                 setattr(self._keras_trackable, key, tf_fn)\n             else:\n                 raise ValueError(\n-                    f\"Function {key} missing from serialized tf.function dictionary.\"\n+                    f\"Function {key} missing from serialized \"\n+                    \"tf.function dictionary.\"\n                 )\n         return self.functions\n \n@@ -241,9 +247,11 @@ class SerializedAttributes:\n                     object_dict[key], tf.__internal__.tracking.Trackable\n                 ):\n                     raise ValueError(\n-                        \"The object dictionary contained a non-trackable object: \"\n-                        f\"{object_dict[key]} (for key {key}). Only trackable objects are \"\n-                        f\"allowed, such as Keras layers/models or tf.Module instances.\"\n+                        \"The object dictionary contained a non-trackable \"\n+                        f\"object: {object_dict[key]} (for key {key}). \"\n+                        \"Only trackable objects are \"\n+                        f\"allowed, such as Keras layers/models or \"\n+                        \"tf.Module instances.\"\n                     )\n                 self._object_dict[key] = object_dict[key]\n                 setattr(self._keras_trackable, key, object_dict[key])\n@@ -311,12 +319,13 @@ class LayerAttributes(\n         its sublayers.\n       layers: List of all sublayers.\n       metrics: List of all metrics in the layer and its sublayers.\n-      call_and_return_conditional_losses: Function that takes inputs and returns a\n-        tuple of (outputs of the call function, list of input-dependent losses).\n-        The list of losses excludes the activity regularizer function, which is\n-        separate to allow the deserialized Layer object to define a different\n-        activity regularizer.\n-      activity_regularizer_fn: Callable that returns the activity regularizer loss\n+      call_and_return_conditional_losses: Function that takes inputs and returns\n+        a tuple of (outputs of the call function, list of input-dependent\n+        losses).  The list of losses excludes the activity regularizer function,\n+        which is separate to allow the deserialized Layer object to define a\n+        different activity regularizer.\n+      activity_regularizer_fn: Callable that returns the activity regularizer\n+        loss\n       layer_regularization_losses: List of losses owned only by this layer.\n       layer_metrics: List of metrics owned by this layer.\n     \"\"\"\n@@ -333,8 +342,8 @@ class ModelAttributes(\n       All attributes from LayerAttributes (including CommonEndpoints)\n     \"\"\"\n \n-    # TODO(kathywu): Add attributes `compile_losses` and `compile_metrics`, which\n-    #  list all losses and metrics defined by `model.compile`.\n+    # TODO(kathywu): Add attributes `compile_losses` and `compile_metrics`,\n+    # which list all losses and metrics defined by `model.compile`.\n \n \n class MetricAttributes(\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Utility functions shared between SavedModel saving/loading implementations.\"\"\"\n+\"\"\"Utility functions shared between SavedModel saving/loading\n+implementations.\"\"\"\n \n import copy\n import inspect as _inspect\n@@ -41,11 +42,11 @@ def use_wrapped_call(\n \n     Args:\n       layer: A Keras layer object\n-      call_fn: tf.function that takes layer inputs (and possibly a training arg),\n-        and returns a tuple of (outputs, list of losses).\n+      call_fn: tf.function that takes layer inputs (and possibly a training\n+        arg), and returns a tuple of (outputs, list of losses).\n       call_spec: The `CallFunctionSpec` for the layer's call function.\n-      default_training_value: Default value of the training kwarg. If `None`, the\n-        default is `tf.keras.backend.learning_phase()`.\n+      default_training_value: Default value of the training kwarg. If `None`,\n+        the default is `tf.keras.backend.learning_phase()`.\n       return_method: Whether to return a method bound to the layer.\n \n     Returns:\n@@ -59,7 +60,8 @@ def use_wrapped_call(\n     )\n \n     def return_outputs_and_add_losses(*args, **kwargs):\n-        \"\"\"Returns the outputs from the layer call function, and adds the losses.\"\"\"\n+        \"\"\"Returns the outputs from the layer call function, and adds the\n+        losses.\"\"\"\n         if return_method:\n             args = args[1:]\n \n@@ -67,11 +69,12 @@ def use_wrapped_call(\n         layer.add_loss(losses)\n \n         # TODO(kathywu): This is a temporary hack. When a network of layers is\n-        # revived from SavedModel, only the top-level layer will have losses. This\n-        # causes issues in eager mode because the child layers may have graph losses\n-        # (thus model.losses returns a mix of Eager and graph tensors). To fix this,\n-        # whenever eager losses are added to one layer, add eager losses to all\n-        # child layers. This causes `.losses` to only return eager losses.\n+        # revived from SavedModel, only the top-level layer will have losses.\n+        # This causes issues in eager mode because the child layers may have\n+        # graph losses (thus model.losses returns a mix of Eager and graph\n+        # tensors). To fix this, whenever eager losses are added to one layer,\n+        # add eager losses to all child layers. This causes `.losses` to only\n+        # return eager losses.\n         # pylint: disable=protected-access\n         if tf.executing_eagerly():\n             for i in layer._flatten_layers():\n@@ -95,7 +98,8 @@ def use_wrapped_call(\n \n \n def layer_uses_training_bool(layer):\n-    \"\"\"Returns whether this layer or any of its children uses the training arg.\"\"\"\n+    \"\"\"Returns whether this layer or any of its children uses the training\n+    arg.\"\"\"\n     if layer._expects_training_arg:  # pylint: disable=protected-access\n         return True\n     visited = {layer}\n@@ -138,8 +142,9 @@ def maybe_add_training_arg(\n ):\n     \"\"\"Decorate call and optionally adds training argument.\n \n-    If a layer expects a training argument, this function ensures that 'training'\n-    is present in the layer args or kwonly args, with the default training value.\n+    If a layer expects a training argument, this function ensures that\n+    'training' is present in the layer args or kwonly args, with the default\n+    training value.\n \n     Args:\n       call_spec: CallFunctionSpec of the layer.\n@@ -199,7 +204,8 @@ def maybe_add_training_arg(\n def set_training_arg_spec(arg_spec, default_training_value):\n     \"\"\"Set `training=DEFAULT` argument in an ArgSpec.\"\"\"\n     if \"training\" in arg_spec.args:\n-        # If `training` is already in the args list, try to set the default value.\n+        # If `training` is already in the args list, try to set the default\n+        # value.\n         index = arg_spec.args.index(\"training\")\n         training_default_index = len(arg_spec.args) - index\n         defaults = (\n@@ -243,7 +249,8 @@ def keras_option_scope(save_traces):\n \n \n def should_save_traces():\n-    \"\"\"Whether to trace layer functions-can be disabled in the save_traces arg.\"\"\"\n+    \"\"\"Whether to trace layer functions-can be disabled in the save_traces\n+    arg.\"\"\"\n     return _save_options_context.save_traces\n \n \n@@ -252,9 +259,9 @@ def no_automatic_dependency_tracking_scope(obj):\n     \"\"\"A context that disables automatic dependency tracking when assigning attrs.\n \n     Objects that inherit from Autotrackable automatically creates dependencies\n-    to trackable objects through attribute assignments, and wraps data structures\n-    (lists or dicts) with trackable classes. This scope may be used to temporarily\n-    disable this behavior. This works similar to the decorator\n+    to trackable objects through attribute assignments, and wraps data\n+    structures (lists or dicts) with trackable classes. This scope may be used\n+    to temporarily disable this behavior. This works similar to the decorator\n     `no_automatic_dependency_tracking`.\n \n     Example usage:\n\n@@ -59,9 +59,9 @@ def export_saved_model(\n     Note that at this time, subclassed models can only be saved using\n     `serving_only=True`.\n \n-    The exported `SavedModel` is a standalone serialization of Tensorflow objects,\n-    and is supported by TF language APIs and the Tensorflow Serving system.\n-    To load the model, use the function\n+    The exported `SavedModel` is a standalone serialization of Tensorflow\n+    objects, and is supported by TF language APIs and the Tensorflow Serving\n+    system.  To load the model, use the function\n     `tf.keras.experimental.load_from_saved_model`.\n \n     The `SavedModel` contains:\n@@ -69,8 +69,8 @@ def export_saved_model(\n     1. a checkpoint containing the model weights.\n     2. a `SavedModel` proto containing the Tensorflow backend graph. Separate\n        graphs are saved for prediction (serving), train, and evaluation. If\n-       the model has not been compiled, then only the graph computing predictions\n-       will be exported.\n+       the model has not been compiled, then only the graph computing\n+       predictions will be exported.\n     3. the model's json config. If the model is subclassed, this will only be\n        included if the model's `get_config()` method is overwritten.\n \n@@ -94,23 +94,26 @@ def export_saved_model(\n     ```\n \n     Args:\n-      model: A `tf.keras.Model` to be saved. If the model is subclassed, the flag\n-        `serving_only` must be set to True.\n-      saved_model_path: a string specifying the path to the SavedModel directory.\n+      model: A `tf.keras.Model` to be saved. If the model is subclassed, the\n+        flag `serving_only` must be set to True.\n+      saved_model_path: a string specifying the path to the SavedModel\n+        directory.\n       custom_objects: Optional dictionary mapping string names to custom classes\n         or functions (e.g. custom loss functions).\n       as_text: bool, `False` by default. Whether to write the `SavedModel` proto\n         in text format. Currently unavailable in serving-only mode.\n-      input_signature: A possibly nested sequence of `tf.TensorSpec` objects, used\n-        to specify the expected model inputs. See `tf.function` for more details.\n+      input_signature: A possibly nested sequence of `tf.TensorSpec` objects,\n+        used to specify the expected model inputs. See `tf.function` for more\n+        details.\n       serving_only: bool, `False` by default. When this is true, only the\n         prediction graph is saved.\n \n     Raises:\n-      NotImplementedError: If the model is a subclassed model, and serving_only is\n-        False.\n+      NotImplementedError: If the model is a subclassed model, and serving_only\n+        is False.\n       ValueError: If the input signature cannot be inferred from the model.\n-      AssertionError: If the SavedModel directory already exists and isn't empty.\n+      AssertionError: If the SavedModel directory already exists and isn't\n+        empty.\n     \"\"\"\n     warnings.warn(\n         \"`tf.keras.experimental.export_saved_model` is deprecated\"\n@@ -162,8 +165,8 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n     \"\"\"Exports model to v1 SavedModel format.\"\"\"\n     if not model._is_graph_network:  # pylint: disable=protected-access\n         if isinstance(model, sequential.Sequential):\n-            # If input shape is not directly set in the model, the exported model\n-            # will infer the expected shapes of the input from the model.\n+            # If input shape is not directly set in the model, the exported\n+            # model will infer the expected shapes of the input from the model.\n             if not model.built:\n                 raise ValueError(\n                     \"Weights for sequential model have not yet been \"\n@@ -176,8 +179,8 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n             # weights before _export_model_variables().\n         else:\n             raise NotImplementedError(\n-                \"Subclassed models can only be exported for serving. Please set \"\n-                \"argument serving_only=True.\"\n+                \"Subclassed models can only be exported for serving. Please \"\n+                \"set argument serving_only=True.\"\n             )\n \n     builder = tf.__internal__.saved_model.SavedModelBuilder(\n@@ -192,11 +195,11 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n     # one save is needed once the weights can be copied from the model to clone.\n     checkpoint_path = _export_model_variables(model, path)\n \n-    # Export each mode. Use ModeKeys enums defined for `Estimator` to ensure that\n-    # Keras models and `Estimator`s are exported with the same format.\n-    # Every time a mode is exported, the code checks to see if new variables have\n-    # been created (e.g. optimizer slot variables). If that is the case, the\n-    # checkpoint is re-saved to include the new variables.\n+    # Export each mode. Use ModeKeys enums defined for `Estimator` to ensure\n+    # that Keras models and `Estimator`s are exported with the same format.\n+    # Every time a mode is exported, the code checks to see if new variables\n+    # have been created (e.g. optimizer slot variables). If that is the case,\n+    # the checkpoint is re-saved to include the new variables.\n     export_args = {\n         \"builder\": builder,\n         \"model\": model,\n@@ -218,10 +221,10 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n             _export_mode(mode_keys.ModeKeys.TEST, has_saved_vars, **export_args)\n         else:\n             logging.warning(\n-                \"Model was compiled with an optimizer, but the optimizer is not from \"\n-                \"`tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving \"\n-                \"graph was exported. The train and evaluate graphs were not added to \"\n-                \"the SavedModel.\"\n+                \"Model was compiled with an optimizer, but the optimizer is \"\n+                \"not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). \"\n+                \"Only the serving graph was exported. The train and evaluate \"\n+                \"graphs were not added to the SavedModel.\"\n             )\n     _export_mode(mode_keys.ModeKeys.PREDICT, has_saved_vars, **export_args)\n \n@@ -288,8 +291,8 @@ def _export_mode(\n                 create_placeholder, input_signature\n             )\n \n-        # Clone the model into blank graph. This will create placeholders for inputs\n-        # and targets.\n+        # Clone the model into blank graph. This will create placeholders for\n+        # inputs and targets.\n         clone = models_lib.clone_and_build_model(\n             model,\n             input_tensors=input_tensors,\n@@ -297,10 +300,11 @@ def _export_mode(\n             compile_clone=compile_clone,\n         )\n \n-        # Make sure that iterations variable is added to the global step collection,\n-        # to ensure that, when the SavedModel graph is loaded, the iterations\n-        # variable is returned by `tf.compat.v1.train.get_global_step()`. This is\n-        # required for compatibility with the SavedModelEstimator.\n+        # Make sure that iterations variable is added to the global step\n+        # collection, to ensure that, when the SavedModel graph is loaded, the\n+        # iterations variable is returned by\n+        # `tf.compat.v1.train.get_global_step()`. This is required for\n+        # compatibility with the SavedModelEstimator.\n         if compile_clone:\n             g.add_to_collection(\n                 tf.compat.v1.GraphKeys.GLOBAL_STEP, clone.optimizer.iterations\n@@ -322,14 +326,15 @@ def _export_mode(\n         with tf.compat.v1.Session().as_default():\n             clone_var_list = _get_var_list(clone)\n             if has_saved_vars:\n-                # Confirm all variables in the clone have an entry in the checkpoint.\n+                # Confirm all variables in the clone have an entry in the\n+                # checkpoint.\n                 status = clone.load_weights(checkpoint_path)\n                 status.assert_existing_objects_matched()\n             else:\n-                # Confirm that variables between the clone and model match up exactly,\n-                # not counting optimizer objects. Optimizer objects are ignored because\n-                # if the model has not trained, the slot variables will not have been\n-                # created yet.\n+                # Confirm that variables between the clone and model match up\n+                # exactly, not counting optimizer objects. Optimizer objects are\n+                # ignored because if the model has not trained, the slot\n+                # variables will not have been created yet.\n                 # TODO(b/113179535): Replace with trackable equivalence.\n                 _assert_same_non_optimizer_objects(model, model_graph, clone, g)\n \n@@ -351,8 +356,8 @@ def _export_mode(\n                 signature_def_map=_create_signature_def_map(clone, mode),\n                 saver=tf.compat.v1.train.Saver(\n                     clone_var_list,\n-                    # Allow saving Models with no variables. This is somewhat odd, but\n-                    # it's not necessarily a bug.\n+                    # Allow saving Models with no variables. This is somewhat\n+                    # odd, but it's not necessarily a bug.\n                     allow_empty=True,\n                 ),\n                 init_op=tf.compat.v1.local_variables_initializer(),\n\n@@ -388,8 +388,8 @@ class TestModelSavedModelExport(tf.test.TestCase, parameterized.TestCase):\n                     sess, saved_model_dir, mode_keys.ModeKeys.TEST\n                 )\n \n-                # First obtain the loss and predictions, and run the metric update op by\n-                # feeding in the inputs and targets.\n+                # First obtain the loss and predictions, and run the metric\n+                # update op by feeding in the inputs and targets.\n                 metrics_name = (\n                     \"mae\"\n                     if tf.__internal__.tf2.enabled()\n@@ -410,8 +410,8 @@ class TestModelSavedModelExport(tf.test.TestCase, parameterized.TestCase):\n                     },\n                 )\n \n-                # The metric value should be run after the update op, to ensure that it\n-                # reflects the correct value.\n+                # The metric value should be run after the update op, to ensure\n+                # that it reflects the correct value.\n                 metric_value = sess.run(outputs[metrics_value_op_key])\n \n                 self.assertEqual(\n@@ -422,7 +422,8 @@ class TestModelSavedModelExport(tf.test.TestCase, parameterized.TestCase):\n                 self.assertAllClose(ref_mae, metric_value, atol=1e-05)\n                 self.assertAllClose(ref_predict, predictions, atol=1e-05)\n \n-            # Load train graph, and check for the train op, and prediction values\n+            # Load train graph, and check for the train op, and prediction\n+            # values\n             with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n                 inputs, outputs, meta_graph_def = load_model(\n                     sess, saved_model_dir, mode_keys.ModeKeys.TRAIN\n\n@@ -48,8 +48,8 @@ def extract_model_metrics(model):\n     \"\"\"\n     if getattr(model, \"_compile_metrics\", None):\n         # TODO(psv/kathywu): use this implementation in model to estimator flow.\n-        # We are not using model.metrics here because we want to exclude the metrics\n-        # added using `add_metric` API.\n+        # We are not using model.metrics here because we want to exclude the\n+        # metrics added using `add_metric` API.\n         return {\n             m.name: m for m in model._compile_metric_functions\n         }  # pylint: disable=protected-access\n@@ -59,19 +59,20 @@ def extract_model_metrics(model):\n def model_call_inputs(model, keep_original_batch_size=False):\n     \"\"\"Inspect model to get its input signature.\n \n-    The model's input signature is a list with a single (possibly-nested) object.\n-    This is due to the Keras-enforced restriction that tensor inputs must be\n-    passed in as the first argument.\n+    The model's input signature is a list with a single (possibly-nested)\n+    object. This is due to the Keras-enforced restriction that tensor inputs\n+    must be passed in as the first argument.\n \n     For example, a model with input {'feature1': <Tensor>, 'feature2': <Tensor>}\n-    will have input signature: [{'feature1': TensorSpec, 'feature2': TensorSpec}]\n+    will have input signature:\n+    [{'feature1': TensorSpec, 'feature2': TensorSpec}]\n \n     Args:\n       model: Keras Model object.\n-      keep_original_batch_size: A boolean indicating whether we want to keep using\n-        the original batch size or set it to None. Default is `False`, which means\n-        that the batch dim of the returned input signature will always be set to\n-        `None`.\n+      keep_original_batch_size: A boolean indicating whether we want to keep\n+        using the original batch size or set it to None. Default is `False`,\n+        which means that the batch dim of the returned input signature will\n+        always be set to `None`.\n \n     Returns:\n       A tuple containing `(args, kwargs)` TensorSpecs of the model call function\n@@ -94,7 +95,8 @@ def raise_model_input_error(model):\n             \"data using `Model()`, `Model.fit()`, or `Model.predict()`.\"\n         )\n \n-    # If the model is not a `Sequential`, it is intended to be a subclassed model.\n+    # If the model is not a `Sequential`, it is intended to be a subclassed\n+    # model.\n     raise ValueError(\n         f\"Model {model} cannot be saved either because the input shape is not \"\n         \"available or because the forward pass of the model is not defined.\"\n@@ -116,7 +118,8 @@ def trace_model_call(model, input_signature=None):\n         inputs to the model.\n \n     Returns:\n-      A tf.function wrapping the model's call function with input signatures set.\n+      A tf.function wrapping the model's call function with input signatures\n+      set.\n \n     Raises:\n       ValueError: if input signature cannot be inferred from the model.\n@@ -202,9 +205,11 @@ def model_metadata(model, include_optimizer=True, require_config=True):\n             if isinstance(model.optimizer, optimizer_v2.RestoredOptimizer):\n                 raise NotImplementedError(\n                     \"Optimizers loaded from a SavedModel cannot be saved. \"\n-                    \"If you are calling `model.save` or `tf.keras.models.save_model`, \"\n+                    \"If you are calling `model.save` or \"\n+                    \"`tf.keras.models.save_model`, \"\n                     \"please set the `include_optimizer` option to `False`. For \"\n-                    \"`tf.saved_model.save`, delete the optimizer from the model.\"\n+                    \"`tf.saved_model.save`, \"\n+                    \"delete the optimizer from the model.\"\n                 )\n             else:\n                 optimizer_config = {\n@@ -319,8 +324,9 @@ def _deserialize_metric(metric_config):\n     from keras import metrics as metrics_module\n \n     if metric_config in [\"accuracy\", \"acc\", \"crossentropy\", \"ce\"]:\n-        # Do not deserialize accuracy and cross-entropy strings as we have special\n-        # case handling for these in compile, based on model output shape.\n+        # Do not deserialize accuracy and cross-entropy strings as we have\n+        # special case handling for these in compile, based on model output\n+        # shape.\n         return metric_config\n     return metrics_module.deserialize(metric_config)\n \n@@ -359,9 +365,9 @@ def try_build_compiled_arguments(model):\n                 model.compiled_metrics.build(model.outputs, model.outputs)\n         except:  # pylint: disable=bare-except\n             logging.warning(\n-                \"Compiled the loaded model, but the compiled metrics have yet to \"\n-                \"be built. `model.compile_metrics` will be empty until you train \"\n-                \"or evaluate the model.\"\n+                \"Compiled the loaded model, but the compiled metrics have \"\n+                \"yet to be built. `model.compile_metrics` will be empty \"\n+                \"until you train or evaluate the model.\"\n             )\n \n \n\n@@ -137,8 +137,8 @@ class TraceModelCallTest(test_combinations.TestCase):\n \n         fn = saving_utils.trace_model_call(model)\n         # tf.function requires that the input structures match when calling a\n-        # ConcreteFunction. For some reason V1 models defines the inputs as a list,\n-        # while V2 models sets the inputs as a tuple.\n+        # ConcreteFunction. For some reason V1 models defines the inputs as a\n+        # list, while V2 models sets the inputs as a tuple.\n         if (\n             not tf.executing_eagerly()\n             and test_utils.get_model_type() != \"functional\"\n@@ -453,7 +453,8 @@ class UnbuiltModelSavingErrorMessageTest(test_combinations.TestCase):\n         # Forward pass not called yet. Input shape not available and thus error.\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Model.*cannot be saved.*specify an input shape either by calling.*\",\n+            \"Model.*cannot be saved.\"\n+            \"*specify an input shape either by calling.*\",\n         ):\n             model.save(os.path.join(self.get_temp_dir(), \"my_saved_model\"))\n \n@@ -496,9 +497,9 @@ class UnbuiltModelSavingErrorMessageTest(test_combinations.TestCase):\n         subclassed_model.fit(x, y, epochs=1)\n \n         # Saving of this subclassed model is supposed to raise an error, even if\n-        # `fit` has been called. This is because the model does not have `call()`\n-        # overridden. Forward pass using `layer.__call__` works for training, but\n-        # saving requires that `call()` be used.\n+        # `fit` has been called. This is because the model does not have\n+        # `call()` overridden. Forward pass using `layer.__call__` works for\n+        # training, but saving requires that `call()` be used.\n         with self.assertRaisesRegex(\n             ValueError,\n             r\"Model.*cannot be saved.*as opposed to `model.call\\(\\).*\",\n@@ -536,10 +537,11 @@ class UnbuiltModelSavingErrorMessageTest(test_combinations.TestCase):\n         subclassed_model.fit(x, y, epochs=1)\n \n         # Saving of this subclassed model is supposed to raise an error, even if\n-        # `fit` has been called. This is because the model has `call()` overridden,\n-        # but the forward pass uses `Model.call` as opposed to `Model.__call__`, and\n-        # as a result the `Model` is not really built. The error message hints the\n-        # user to use `Model.__call__`, i.e., `Model(inputs)` instead.\n+        # `fit` has been called. This is because the model has `call()`\n+        # overridden, but the forward pass uses `Model.call` as opposed to\n+        # `Model.__call__`, and as a result the `Model` is not really built. The\n+        # error message hints the user to use `Model.__call__`, i.e.,\n+        # `Model(inputs)` instead.\n         with self.assertRaisesRegex(\n             ValueError,\n             r\"Model.*cannot be saved.*as opposed to `model.call\\(\\).*\",\n\n@@ -42,8 +42,8 @@ class ExportOutput:\n         and will use the provided receiver_tensors as inputs.\n \n         Args:\n-          receiver_tensors: a `Tensor`, or a dict of string to `Tensor`, specifying\n-            input nodes that will be fed.\n+          receiver_tensors: a `Tensor`, or a dict of string to `Tensor`,\n+            specifying input nodes that will be fed.\n         \"\"\"\n         pass\n \n@@ -79,8 +79,8 @@ class ExportOutput:\n           A dict of tensors\n \n         Raises:\n-          ValueError: if the outputs dict keys are not strings or tuples of strings\n-            or the values are not Tensors.\n+          ValueError: if the outputs dict keys are not strings or tuples of\n+            strings or the values are not Tensors.\n         \"\"\"\n         if not isinstance(outputs, dict):\n             outputs = {single_output_default_name: outputs}\n@@ -110,8 +110,8 @@ class ClassificationOutput(ExportOutput):\n     If only classes is set, it is interpreted as providing top-k results in\n     descending order.\n \n-    If only scores is set, it is interpreted as providing a score for every class\n-    in order of class ID.\n+    If only scores is set, it is interpreted as providing a score for every\n+    class in order of class ID.\n \n     If both classes and scores are set, they are interpreted as zipped, so each\n     score corresponds to the class at the same index.  Clients should not depend\n@@ -123,14 +123,16 @@ class ClassificationOutput(ExportOutput):\n \n         Args:\n           scores: A float `Tensor` giving scores (sometimes but not always\n-              interpretable as probabilities) for each class.  May be `None`, but\n-              only if `classes` is set.  Interpretation varies-- see class doc.\n-          classes: A string `Tensor` giving predicted class labels.  May be `None`,\n-              but only if `scores` is set.  Interpretation varies-- see class doc.\n+              interpretable as probabilities) for each class.  May be `None`,\n+              but only if `classes` is set.  Interpretation varies-- see class\n+              doc.\n+          classes: A string `Tensor` giving predicted class labels. May be\n+              `None`, but only if `scores` is set.  Interpretation varies-- see\n+              class doc.\n \n         Raises:\n-          ValueError: if neither classes nor scores is set, or one of them is not a\n-              `Tensor` with the correct dtype.\n+          ValueError: if neither classes nor scores is set, or one of them is\n+              not a `Tensor` with the correct dtype.\n         \"\"\"\n         if scores is not None and not (\n             isinstance(scores, tf.Tensor) and scores.dtype.is_floating\n@@ -167,21 +169,23 @@ class ClassificationOutput(ExportOutput):\n     def as_signature_def(self, receiver_tensors):\n         if len(receiver_tensors) != 1:\n             raise ValueError(\n-                \"Classification signatures can only accept a single tensor input of \"\n-                \"type tf.string. Please check to make sure that you have structured \"\n-                \"the serving_input_receiver_fn so that it creates a single string \"\n-                \"placeholder. If your model function expects multiple inputs, then \"\n-                \"use `tf.io.parse_example()` to parse the string into multiple \"\n+                \"Classification signatures can only accept a single tensor \"\n+                \"input of type tf.string. Please check to make sure that \"\n+                \"you have structured the serving_input_receiver_fn so that it \"\n+                \"creates a single string placeholder. If your model function \"\n+                \"expects multiple inputs, then use `tf.io.parse_example()` to \"\n+                \"parse the string into multiple \"\n                 f\"tensors.\\n Received: {receiver_tensors}\"\n             )\n         ((_, examples),) = receiver_tensors.items()\n         if tf.as_dtype(examples.dtype) != tf.string:\n             raise ValueError(\n-                \"Classification signatures can only accept a single tensor input of \"\n-                \"type tf.string. Please check to make sure that you have structured \"\n-                \"the serving_input_receiver_fn so that it creates a single string \"\n-                \"placeholder. If your model function expects multiple inputs, then \"\n-                \"use `tf.io.parse_example()` to parse the string into multiple \"\n+                \"Classification signatures can only accept a single tensor \"\n+                \"input of type tf.string. Please check to make sure that you \"\n+                \"have structured the serving_input_receiver_fn so that it \"\n+                \"creates a single string placeholder. If your model function \"\n+                \"expects multiple inputs, then use `tf.io.parse_example()` to \"\n+                \"parse the string into multiple \"\n                 f\"tensors.\\n Received: {receiver_tensors}\"\n             )\n         return tf.compat.v1.saved_model.classification_signature_def(\n@@ -215,21 +219,23 @@ class RegressionOutput(ExportOutput):\n     def as_signature_def(self, receiver_tensors):\n         if len(receiver_tensors) != 1:\n             raise ValueError(\n-                \"Regression signatures can only accept a single tensor input of \"\n-                \"type tf.string. Please check to make sure that you have structured \"\n-                \"the serving_input_receiver_fn so that it creates a single string \"\n-                \"placeholder. If your model function expects multiple inputs, then \"\n-                \"use `tf.io.parse_example()` to parse the string into multiple \"\n+                \"Regression signatures can only accept a single tensor input \"\n+                \"of type tf.string. Please check to make sure that you have \"\n+                \"structured the serving_input_receiver_fn so that it creates \"\n+                \"a single string placeholder. If your model function expects \"\n+                \"multiple inputs, then use `tf.io.parse_example()` to parse \"\n+                \"the string into multiple \"\n                 f\"tensors.\\n Received: {receiver_tensors}\"\n             )\n         ((_, examples),) = receiver_tensors.items()\n         if tf.as_dtype(examples.dtype) != tf.string:\n             raise ValueError(\n-                \"Regression signatures can only accept a single tensor input of \"\n-                \"type tf.string. Please check to make sure that you have structured \"\n-                \"the serving_input_receiver_fn so that it creates a single string \"\n-                \"placeholder. If your model function expects multiple inputs, then \"\n-                \"use `tf.io.parse_example()` to parse the string into multiple \"\n+                \"Regression signatures can only accept a single tensor input \"\n+                \"of type tf.string. Please check to make sure that you have \"\n+                \"structured the serving_input_receiver_fn so that it creates \"\n+                \"a single string placeholder. If your model function expects \"\n+                \"multiple inputs, then use `tf.io.parse_example()` to parse \"\n+                \"the string into multiple \"\n                 f\"tensors.\\n Received: {receiver_tensors}\"\n             )\n         return tf.compat.v1.saved_model.regression_signature_def(\n@@ -303,9 +309,9 @@ class _SupervisedOutput(ExportOutput):\n             metric_value must be a Tensor, and update_op must be a Tensor or Op.\n \n         Raises:\n-          ValueError: if any of the outputs' dict keys are not strings or tuples of\n-            strings or the values are not Tensors (or Operations in the case of\n-            update_op).\n+          ValueError: if any of the outputs' dict keys are not strings or tuples\n+            of strings or the values are not Tensors (or Operations in the case\n+            of update_op).\n         \"\"\"\n \n         if loss is not None:\n@@ -349,8 +355,9 @@ class _SupervisedOutput(ExportOutput):\n     def _wrap_and_check_metrics(self, metrics):\n         \"\"\"Handle the saving of metrics.\n \n-        Metrics is either a tuple of (value, update_op), or a dict of such tuples.\n-        Here, we separate out the tuples and create a dict with names to tensors.\n+        Metrics is either a tuple of (value, update_op), or a dict of such\n+        tuples.  Here, we separate out the tuples and create a dict with names\n+        to tensors.\n \n         Args:\n           metrics: Dict of metric results keyed by name.\n@@ -363,8 +370,8 @@ class _SupervisedOutput(ExportOutput):\n           dict of output_names to tensors\n \n         Raises:\n-          ValueError: if the dict key is not a string, or the metric values or ops\n-            are not tensors.\n+          ValueError: if the dict key is not a string, or the metric values or\n+            ops are not tensors.\n         \"\"\"\n         if not isinstance(metrics, dict):\n             metrics = {self.METRICS_NAME: metrics}\n@@ -392,13 +399,12 @@ class _SupervisedOutput(ExportOutput):\n                 tf.is_tensor(metric_op) or isinstance(metric_op, tf.Operation)\n             ):\n                 raise ValueError(\n-                    \"{} update_op must be a Tensor or Operation; got {}.\".format(\n-                        key, metric_op\n-                    )\n+                    f\"{key} update_op must be a \"\n+                    f\"Tensor or Operation; got {metric_op}.\"\n                 )\n \n-            # We must wrap any ops (or variables) in a Tensor before export, as the\n-            # SignatureDef proto expects tensors only. See b/109740581\n+            # We must wrap any ops (or variables) in a Tensor before export, as\n+            # the SignatureDef proto expects tensors only. See b/109740581\n             metric_op_tensor = metric_op\n             if not isinstance(metric_op, tf.Tensor):\n                 with tf.control_dependencies([metric_op]):\n@@ -423,7 +429,8 @@ class _SupervisedOutput(ExportOutput):\n \n     @abc.abstractmethod\n     def _get_signature_def_fn(self):\n-        \"\"\"Returns a function that produces a SignatureDef given desired outputs.\"\"\"\n+        \"\"\"Returns a function that produces a SignatureDef given desired\n+        outputs.\"\"\"\n         pass\n \n     def as_signature_def(self, receiver_tensors):\n\n@@ -281,34 +281,35 @@ def export_outputs_for_mode(\n ):\n     \"\"\"Util function for constructing a `ExportOutput` dict given a mode.\n \n-    The returned dict can be directly passed to `build_all_signature_defs` helper\n-    function as the `export_outputs` argument, used for generating a SignatureDef\n-    map.\n+    The returned dict can be directly passed to `build_all_signature_defs`\n+    helper function as the `export_outputs` argument, used for generating a\n+    SignatureDef map.\n \n     Args:\n       mode: A `ModeKeys` specifying the mode.\n       serving_export_outputs: Describes the output signatures to be exported to\n         `SavedModel` and used during serving. Should be a dict or None.\n       predictions: A dict of Tensors or single Tensor representing model\n-          predictions. This argument is only used if serving_export_outputs is not\n-          set.\n+        predictions. This argument is only used if serving_export_outputs is\n+        not set.\n       loss: A dict of Tensors or single Tensor representing calculated loss.\n       metrics: A dict of (metric_value, update_op) tuples, or a single tuple.\n         metric_value must be a Tensor, and update_op must be a Tensor or Op\n \n     Returns:\n-      Dictionary mapping the a key to an `tf.estimator.export.ExportOutput` object\n-      The key is the expected SignatureDef key for the mode.\n+      Dictionary mapping the a key to an `tf.estimator.export.ExportOutput`\n+      object The key is the expected SignatureDef key for the mode.\n \n     Raises:\n       ValueError: if an appropriate ExportOutput cannot be found for the mode.\n     \"\"\"\n     if mode not in SIGNATURE_KEY_MAP:\n         raise ValueError(\n-            f\"Export output type not found for `mode`: {mode}. Expected one of: \"\n-            f\"{list(SIGNATURE_KEY_MAP.keys())}.\\n\"\n-            \"One likely error is that V1 Estimator Modekeys were somehow passed to \"\n-            \"this function. Please ensure that you are using the new ModeKeys.\"\n+            f\"Export output type not found for `mode`: {mode}. Expected one \"\n+            f\"of: {list(SIGNATURE_KEY_MAP.keys())}.\\n\"\n+            \"One likely error is that V1 Estimator Modekeys were somehow \"\n+            \"passed to this function. Please ensure that you are using the new \"\n+            \"ModeKeys.\"\n         )\n     signature_key = SIGNATURE_KEY_MAP[mode]\n     if mode_keys.is_predict(mode):\n\n@@ -66,12 +66,12 @@ class ModeKeyMap(collections.abc.Mapping):\n     \"\"\"Map using ModeKeys as keys.\n \n     This class creates an immutable mapping from modes to values. For example,\n-    SavedModel export of Keras and Estimator models use this to map modes to their\n-    corresponding MetaGraph tags/SignatureDef keys.\n+    SavedModel export of Keras and Estimator models use this to map modes to\n+    their corresponding MetaGraph tags/SignatureDef keys.\n \n     Since this class uses modes, rather than strings, as keys, both \"predict\"\n-    (Keras's PREDICT ModeKey) and \"infer\" (Estimator's PREDICT ModeKey) map to the\n-    same value.\n+    (Keras's PREDICT ModeKey) and \"infer\" (Estimator's PREDICT ModeKey) map to\n+    the same value.\n     \"\"\"\n \n     def __init__(self, **kwargs):\n@@ -82,9 +82,8 @@ class ModeKeyMap(collections.abc.Mapping):\n             dict_key = self._get_internal_key(key)\n             if dict_key in self._internal_dict:\n                 raise ValueError(\n-                    \"Error creating ModeKeyMap. Multiple keys/values found for {} mode.\".format(\n-                        dict_key\n-                    )\n+                    \"Error creating ModeKeyMap. \"\n+                    f\"Multiple keys/values found for {dict_key} mode.\"\n                 )\n             self._internal_dict[dict_key] = kwargs[key]\n \n\n@@ -49,14 +49,15 @@ def _supervised_signature_def(\n \n     This function produces signatures that describe the inputs and outputs\n     of a supervised process, such as training or evaluation, that\n-    results in loss, metrics, and the like. Note that this function only requires\n-    inputs to be not None.\n+    results in loss, metrics, and the like. Note that this function only\n+    requires inputs to be not None.\n \n     Args:\n       method_name: Method name of the SignatureDef as a string.\n       inputs: dict of string to `Tensor`.\n       loss: dict of string to `Tensor` representing computed loss.\n-      predictions: dict of string to `Tensor` representing the output predictions.\n+      predictions: dict of string to `Tensor` representing the output\n+        predictions.\n       metrics: dict of string to `Tensor` representing metric ops.\n \n     Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
