{"custom_id": "keras#c0cc93f56368227eedb956dd7c038015c8ada4cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 142 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 13/10 | Churn Δ: 142 | Churn Cumulative: 248 | Contributors (this commit): 9 | Commits (past 90d): 50 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,6 +81,7 @@ from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n+from keras_core.layers.reshaping.cropping1d import Cropping1D\n from keras_core.layers.reshaping.flatten import Flatten\n from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n\n@@ -0,0 +1,79 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Cropping1D\")\n+class Cropping1D(Layer):\n+    \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n+\n+    It crops along the time dimension (axis 1).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 2)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> print(x)\n+    [[[ 0  1]\n+      [ 2  3]\n+      [ 4  5]]\n+     [[ 6  7]\n+      [ 8  9]\n+      [10 11]]]\n+    >>> y = keras_core.layers.Cropping1D(cropping=1)(x)\n+    >>> print(y)\n+    [[[2 3]]\n+     [[8 9]]]\n+\n+    Args:\n+        cropping: Integer or tuple of integers of length 2.\n+            How many units should be trimmed off at the beginning and end of\n+            the cropping dimension (axis 1).\n+            If a single int is provided, the same value will be used for both.\n+\n+    Input shape:\n+        3D tensor with shape `(batch_size, axis_to_crop, features)`\n+\n+    Output shape:\n+        3D tensor with shape `(batch_size, cropped_axis, features)`\n+    \"\"\"\n+\n+    def __init__(self, cropping=(1, 1), name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        if isinstance(cropping, int):\n+            cropping = (cropping, cropping)\n+        self.cropping = cropping\n+        self.input_spec = InputSpec(ndim=3)\n+\n+    def compute_output_shape(self, input_shape):\n+        if input_shape[1] is not None:\n+            length = input_shape[1] - self.cropping[0] - self.cropping[1]\n+            if length <= 0:\n+                raise ValueError(\n+                    \"`cropping` parameter of `Cropping1D` layer must be \"\n+                    \"greater than the input length. Received: input_shape=\"\n+                    f\"{input_shape}, cropping={self.cropping}\"\n+                )\n+        else:\n+            length = None\n+        return (input_shape[0], length, input_shape[2])\n+\n+    def call(self, inputs):\n+        if (\n+            inputs.shape[1] is not None\n+            and sum(self.cropping) >= inputs.shape[1]\n+        ):\n+            raise ValueError(\n+                \"`cropping` parameter of `Cropping1D` layer must be \"\n+                \"greater than the input length. Received: inputs.shape=\"\n+                f\"{inputs.shape}, cropping={self.cropping}\"\n+            )\n+        if self.cropping[1] == 0:\n+            return inputs[:, self.cropping[0] :, :]\n+        else:\n+            return inputs[:, self.cropping[0] : -self.cropping[1], :]\n+\n+    def get_config(self):\n+        config = {\"cropping\": self.cropping}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,62 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class CroppingTest(testing.TestCase):\n+    def test_cropping_1d(self):\n+        inputs = np.random.rand(3, 5, 7)\n+\n+        # Cropping with different values on the left and the right.\n+        self.run_layer_test(\n+            layers.Cropping1D,\n+            init_kwargs={\"cropping\": (1, 2)},\n+            input_data=inputs,\n+            expected_output=ops.convert_to_tensor(inputs[:, 1:3, :]),\n+        )\n+        # Same cropping on the left and the right.\n+        self.run_layer_test(\n+            layers.Cropping1D,\n+            init_kwargs={\"cropping\": (1, 1)},\n+            input_data=inputs,\n+            expected_output=ops.convert_to_tensor(inputs[:, 1:4, :]),\n+        )\n+        # Same cropping on the left and the right provided as an int.\n+        self.run_layer_test(\n+            layers.Cropping1D,\n+            init_kwargs={\"cropping\": 1},\n+            input_data=inputs,\n+            expected_output=ops.convert_to_tensor(inputs[:, 1:4, :]),\n+        )\n+        # Cropping on the right only.\n+        self.run_layer_test(\n+            layers.Cropping1D,\n+            init_kwargs={\"cropping\": (0, 1)},\n+            input_data=inputs,\n+            expected_output=ops.convert_to_tensor(inputs[:, 0:4, :]),\n+        )\n+        # Cropping on the left only.\n+        self.run_layer_test(\n+            layers.Cropping1D,\n+            init_kwargs={\"cropping\": (1, 0)},\n+            input_data=inputs,\n+            expected_output=ops.convert_to_tensor(inputs[:, 1:5, :]),\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_cropping_1d_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 5, 7))\n+        permuted = layers.Cropping1D((1, 2))(input_layer)\n+        self.assertEqual(permuted.shape, (None, 2, 7))\n+\n+    def test_cropping_1d_errors_if_cropping_more_than_available(self):\n+        with self.assertRaises(ValueError):\n+            input_layer = layers.Input(shape=(3, 4, 7))\n+            layers.Cropping1D(cropping=(2, 3))(input_layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63573e72f008a359e1a9f730cd2e8d0fc375690b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1165 | Lines Deleted: 1308 | Files Changed: 19 | Hunks: 43 | Methods Changed: 72 | Complexity Δ (Sum/Max): 94/66 | Churn Δ: 2473 | Churn Cumulative: 10176 | Contributors (this commit): 14 | Commits (past 90d): 142 | Contributors (cumulative): 52 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,12 +1,10 @@\n \"\"\"Tests for tf.distribute related functionality under tf implementation.\"\"\"\n \n-import numpy as np\n import pytest\n import tensorflow as tf\n from tensorflow.python.eager import context\n \n from keras_core import backend\n-from keras_core.backend.tensorflow import trainer as tf_trainer\n from keras_core import layers\n from keras_core import models\n from keras_core import testing\n@@ -81,41 +79,3 @@ class DistributeTest(testing.TestCase):\n         self.assertEqual(result.values[1].shape, [8, 2])\n         self.assertNotAllClose(result.values[0], result.values[1])\n         self.assertAllClose(result.values[0], tf.zeros([8, 2]))\n-\n-    def test_epoch_iterator(self):\n-        x = np.random.random((100, 16))\n-        y = np.random.random((100, 4))\n-        sample_weight = np.random.random((100,))\n-        batch_size = 16\n-        shuffle = True\n-\n-        strategy = tf.distribute.MirroredStrategy(['CPU:0', 'CPU:1'])\n-\n-        epoch_iterator = tf_trainer.TFEpochIterator(\n-            x=x,\n-            y=y,\n-            sample_weight=sample_weight,\n-            batch_size=batch_size,\n-            shuffle=shuffle,\n-            distribute_strategy=strategy\n-        )\n-        steps_seen = []\n-        for step, data_iterator in epoch_iterator.enumerate_epoch():\n-            steps_seen.append(step)\n-            batch = next(data_iterator)\n-            self.assertEqual(len(batch), 3)\n-            x, y, sample_weight = batch\n-            self.assertTrue(\n-                isinstance(x,\n-                           tf.types.experimental.distributed.PerReplica))\n-            # Make sure the local batch size is 8\n-            if step < 6:\n-                self.assertEqual(x.values[0].shape, [8, 16])\n-                self.assertEqual(y.values[0].shape, [8, 4])\n-                self.assertEqual(sample_weight.values[0].shape, [8])\n-            else:\n-                # Last partial batch\n-                self.assertEqual(x.values[0].shape, [2, 16])\n-                self.assertEqual(y.values[0].shape, [2, 4])\n-                self.assertEqual(sample_weight.values[0].shape, [2])\n-        self.assertEqual(steps_seen, [0, 1, 2, 3, 4, 5, 6])\n\n@@ -33,17 +33,16 @@ def _compute_conv_transpose_output_length(\n \n \n def compute_conv_transpose_output_shape(\n-    input_shape,\n-    kernel_size,\n-    filters,\n+    inputs,\n+    kernel,\n     strides,\n     padding,\n     output_padding=None,\n     data_format=\"channels_last\",\n     dilation_rate=1,\n ):\n-    num_spatial_dims = len(input_shape) - 2\n-    kernel_spatial_shape = kernel_size\n+    num_spatial_dims = len(inputs.shape) - 2\n+    kernel_spatial_shape = kernel.shape[:-2]\n \n     if isinstance(output_padding, int):\n         output_padding = (output_padding,) * len(kernel_spatial_shape)\n@@ -53,9 +52,9 @@ def compute_conv_transpose_output_shape(\n         dilation_rate = (dilation_rate,) * num_spatial_dims\n \n     if data_format == \"channels_last\":\n-        input_spatial_shape = input_shape[1:-1]\n+        inputs_spatial_shape = inputs.shape[1:-1]\n     else:\n-        input_spatial_shape = input_shape[2:]\n+        inputs_spatial_shape = inputs.shape[2:]\n \n     output_shape = []\n     for i in range(num_spatial_dims):\n@@ -64,7 +63,7 @@ def compute_conv_transpose_output_shape(\n         )\n         output_shape.append(\n             _compute_conv_transpose_output_length(\n-                input_spatial_shape[i],\n+                inputs_spatial_shape[i],\n                 kernel_spatial_shape[i],\n                 padding=padding,\n                 output_padding=current_output_padding,\n@@ -74,7 +73,7 @@ def compute_conv_transpose_output_shape(\n         )\n \n     if data_format == \"channels_last\":\n-        output_shape = [input_shape[0]] + output_shape + [filters]\n+        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n     else:\n-        output_shape = [input_shape[0], filters] + output_shape\n+        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n     return output_shape\n\n@@ -360,12 +360,9 @@ def conv_transpose(\n     dilation_rate=1,\n ):\n     tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    kernel_size = kernel.shape[:-2]\n-    filters = kernel.shape[-2]\n     output_shape = compute_conv_transpose_output_shape(\n-        inputs.shape,\n-        kernel_size,\n-        filters,\n+        inputs,\n+        kernel,\n         strides,\n         padding,\n         output_padding,\n@@ -586,3 +583,458 @@ def binary_crossentropy(target, output, from_logits=False):\n     bce = target * tf.math.log(output)\n     bce += (1 - target) * tf.math.log(1 - output)\n     return -bce\n+\n+\n+def rnn(\n+    step_function,\n+    inputs,\n+    initial_states,\n+    go_backwards=False,\n+    mask=None,\n+    constants=None,\n+    unroll=False,\n+    input_length=None,\n+    time_major=False,\n+    zero_output_for_mask=False,\n+    return_all_outputs=True,\n+):\n+    \"\"\"Iterates over the time dimension of a tensor.\n+\n+    Args:\n+        step_function: RNN step function.\n+            Args;\n+                `input`; Tensor with shape `(samples, ...)` (no time dimension),\n+                    representing input for the batch of samples at a certain\n+                    time step.\n+                `states`; List of tensors.\n+            Returns;\n+                `output`; Tensor with shape `(samples, output_dim)`\n+                    (no time dimension).\n+                `new_states`; List of tensors, same length and shapes\n+                    as 'states'. The first state in the list must be the\n+                    output tensor at the previous timestep.\n+        inputs: Tensor of temporal data of shape `(samples, time, ...)`\n+            (at least 3D), or nested tensors, and each of which has shape\n+            `(samples, time, ...)`.\n+        initial_states: Tensor with shape `(samples, state_size)`\n+            (no time dimension), containing the initial values for the states\n+            used in the step function. In the case that state_size is in a\n+            nested shape, the shape of initial_states will also follow the\n+            nested structure.\n+        go_backwards: Boolean. If `True`, do the iteration over the time\n+            dimension in reverse order and return the reversed sequence.\n+        mask: Binary tensor with shape `(samples, time, 1)`,\n+            with a zero for every element that is masked.\n+        constants: List of constant values passed at each step.\n+        unroll: Whether to unroll the RNN or to use a symbolic `while_loop`.\n+        input_length: An integer or a 1-D Tensor, depending on whether\n+            the time dimension is fixed-length or not. In case of variable\n+            length input, it is used for masking in case there's no mask\n+            specified.\n+        time_major: Boolean. If `True`, the inputs and outputs will be in shape\n+            `(timesteps, batch, ...)`, whereas in the False case, it will be\n+            `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n+            efficient because it avoids transposes at the beginning and end of\n+            the RNN calculation. However, most TensorFlow data is batch-major,\n+            so by default this function accepts input and emits output in\n+            batch-major form.\n+        zero_output_for_mask: Boolean. If `True`, the output for masked timestep\n+            will be zeros, whereas in the `False` case, output from previous\n+            timestep is returned.\n+        return_all_outputs: Boolean. If `True`, return the recurrent outputs for\n+            all timesteps in the sequence. If `False`, only return the output\n+            for the last timestep (which consumes less memory).\n+\n+    Returns:\n+        A tuple, `(last_output, outputs, new_states)`.\n+            - `last_output`: the latest output of the rnn,\n+                with shape `(samples, ...)`.\n+            - `outputs`:\n+                - If `return_all_outputs=True`: a tensor with shape\n+                  `(samples, time, ...)` where each entry `outputs[s, t]` is the\n+                  output of the step function at time `t` for sample `s`\n+                - Else, a tensor equal to `last_output` with shape\n+                  `(samples, 1, ...)`\n+            - `new_states`: list of tensors, latest states returned by\n+                the step function, of shape `(samples, ...)`.\n+    \"\"\"\n+\n+    def swap_batch_timestep(input_t):\n+        # Swap the batch and timestep dim for the incoming tensor.\n+        axes = list(range(len(input_t.shape)))\n+        axes[0], axes[1] = 1, 0\n+        return tf.transpose(input_t, axes)\n+\n+    if not time_major:\n+        inputs = tf.nest.map_structure(swap_batch_timestep, inputs)\n+\n+    flatted_inputs = tf.nest.flatten(inputs)\n+    time_steps = flatted_inputs[0].shape[0]\n+    batch = flatted_inputs[0].shape[1]\n+    time_steps_t = tf.shape(flatted_inputs[0])[0]\n+\n+    for input_ in flatted_inputs:\n+        input_.shape.with_rank_at_least(3)\n+\n+    if mask is not None:\n+        if mask.dtype != tf.bool:\n+            mask = tf.cast(mask, tf.bool)\n+        if len(mask.shape) == 2:\n+            mask = tf.expand_dims(mask, axis=-1)\n+        if not time_major:\n+            mask = swap_batch_timestep(mask)\n+\n+    if constants is None:\n+        constants = []\n+\n+    # tf.where needs its condition tensor to be the same shape as its two\n+    # result tensors, but in our case the condition (mask) tensor is\n+    # (nsamples, 1), and inputs are (nsamples, ndimensions) or even more.\n+    # So we need to broadcast the mask to match the shape of inputs.\n+    # That's what the tile call does, it just repeats the mask along its\n+    # second dimension n times.\n+    def _expand_mask(mask_t, input_t, fixed_dim=1):\n+        if tf.nest.is_nested(mask_t):\n+            raise ValueError(\n+                f\"mask_t is expected to be tensor, but got {mask_t}\"\n+            )\n+        if tf.nest.is_nested(input_t):\n+            raise ValueError(\n+                f\"input_t is expected to be tensor, but got {input_t}\"\n+            )\n+        rank_diff = len(input_t.shape) - len(mask_t.shape)\n+        for _ in range(rank_diff):\n+            mask_t = tf.expand_dims(mask_t, -1)\n+        multiples = [1] * fixed_dim + input_t.shape.as_list()[fixed_dim:]\n+        return tf.tile(mask_t, multiples)\n+\n+    if unroll:\n+        if not time_steps:\n+            raise ValueError(\"Unrolling requires a fixed number of timesteps.\")\n+        states = tuple(initial_states)\n+        successive_states = []\n+        successive_outputs = []\n+\n+        # Process the input tensors. The input tensor need to be split on the\n+        # time_step dim, and reverse if go_backwards is True. In the case of\n+        # nested input, the input is flattened and then transformed\n+        # individually.  The result of this will be a tuple of lists, each of\n+        # the item in tuple is list of the tensor with shape (batch, feature)\n+        def _process_single_input_t(input_t):\n+            input_t = tf.unstack(input_t)  # unstack for time_step dim\n+            if go_backwards:\n+                input_t.reverse()\n+            return input_t\n+\n+        if tf.nest.is_nested(inputs):\n+            processed_input = tf.nest.map_structure(\n+                _process_single_input_t, inputs\n+            )\n+        else:\n+            processed_input = (_process_single_input_t(inputs),)\n+\n+        def _get_input_tensor(time):\n+            inp = [t_[time] for t_ in processed_input]\n+            return tf.nest.pack_sequence_as(inputs, inp)\n+\n+        if mask is not None:\n+            mask_list = tf.unstack(mask)\n+            if go_backwards:\n+                mask_list.reverse()\n+\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                mask_t = mask_list[i]\n+                output, new_states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                tiled_mask_t = _expand_mask(mask_t, output)\n+\n+                if not successive_outputs:\n+                    prev_output = tf.zeros_like(output)\n+                else:\n+                    prev_output = successive_outputs[-1]\n+\n+                output = tf.where(tiled_mask_t, output, prev_output)\n+\n+                flat_states = tf.nest.flatten(states)\n+                flat_new_states = tf.nest.flatten(new_states)\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, s) for s in flat_states\n+                )\n+                flat_final_states = tuple(\n+                    tf.where(m, s, ps)\n+                    for m, s, ps in zip(\n+                        tiled_mask_t, flat_new_states, flat_states\n+                    )\n+                )\n+                states = tf.nest.pack_sequence_as(states, flat_final_states)\n+\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+\n+            if zero_output_for_mask:\n+                last_output = tf.where(\n+                    _expand_mask(mask_list[-1], last_output),\n+                    last_output,\n+                    tf.zeros_like(last_output),\n+                )\n+                outputs = tf.where(\n+                    _expand_mask(mask, outputs, fixed_dim=2),\n+                    outputs,\n+                    tf.zeros_like(outputs),\n+                )\n+\n+        else:  # mask is None\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                output, states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+\n+    else:  # Unroll == False\n+        states = tuple(initial_states)\n+\n+        # Create input tensor array, if the inputs is nested tensors, then it\n+        # will be flattened first, and tensor array will be created one per\n+        # flattened tensor.\n+        input_ta = tuple(\n+            tf.TensorArray(\n+                dtype=inp.dtype,\n+                size=time_steps_t,\n+                tensor_array_name=f\"input_ta_{i}\",\n+            )\n+            for i, inp in enumerate(flatted_inputs)\n+        )\n+        input_ta = tuple(\n+            ta.unstack(input_)\n+            if not go_backwards\n+            else ta.unstack(tf.reverse(input_, [0]))\n+            for ta, input_ in zip(input_ta, flatted_inputs)\n+        )\n+\n+        # Get the time(0) input and compute the output for that, the output will\n+        # be used to determine the dtype of output tensor array. Don't read from\n+        # input_ta due to TensorArray clear_after_read default to True.\n+        input_time_zero = tf.nest.pack_sequence_as(\n+            inputs, [inp[0] for inp in flatted_inputs]\n+        )\n+        # output_time_zero is used to determine the cell output shape and its\n+        # dtype.  the value is discarded.\n+        output_time_zero, _ = step_function(\n+            input_time_zero, tuple(initial_states) + tuple(constants)\n+        )\n+\n+        output_ta_size = time_steps_t if return_all_outputs else 1\n+        output_ta = tuple(\n+            tf.TensorArray(\n+                dtype=out.dtype,\n+                size=output_ta_size,\n+                element_shape=out.shape,\n+                tensor_array_name=f\"output_ta_{i}\",\n+            )\n+            for i, out in enumerate(tf.nest.flatten(output_time_zero))\n+        )\n+\n+        time = tf.constant(0, dtype=\"int32\", name=\"time\")\n+\n+        if input_length is None:\n+            max_iterations = time_steps_t\n+        else:\n+            max_iterations = tf.reduce_max(input_length)\n+\n+        while_loop_kwargs = {\n+            \"cond\": lambda time, *_: time < time_steps_t,\n+            \"maximum_iterations\": max_iterations,\n+            \"parallel_iterations\": 32,\n+            \"swap_memory\": True,\n+        }\n+        if mask is not None:\n+            if go_backwards:\n+                mask = tf.reverse(mask, [0])\n+\n+            mask_ta = tf.TensorArray(\n+                dtype=tf.bool, size=time_steps_t, tensor_array_name=\"mask_ta\"\n+            )\n+            mask_ta = mask_ta.unstack(mask)\n+\n+            def masking_fn(time):\n+                return mask_ta.read(time)\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, o, fixed_dim=len(mask_t.shape))\n+                    for o in flat_out\n+                )\n+                return tuple(\n+                    tf.where(m, o, fm)\n+                    for m, o, fm in zip(tiled_mask_t, flat_out, flat_mask)\n+                )\n+\n+        elif isinstance(input_length, tf.Tensor):\n+            if go_backwards:\n+                max_len = tf.reduce_max(input_length, axis=0)\n+                rev_input_length = tf.subtract(max_len - 1, input_length)\n+\n+                def masking_fn(time):\n+                    return tf.less(rev_input_length, time)\n+\n+            else:\n+\n+                def masking_fn(time):\n+                    return tf.greater(input_length, time)\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                return tuple(\n+                    tf.where(mask_t, o, zo)\n+                    for (o, zo) in zip(flat_out, flat_mask)\n+                )\n+\n+        else:\n+            masking_fn = None\n+\n+        if masking_fn is not None:\n+            # Mask for the T output will be base on the output of T - 1. In the\n+            # case T = 0, a zero filled tensor will be used.\n+            flat_zero_output = tuple(\n+                tf.zeros_like(o) for o in tf.nest.flatten(output_time_zero)\n+            )\n+\n+            def _step(time, output_ta_t, prev_output, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    prev_output: tuple of outputs from time - 1.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1, output_ta_t, output) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta.read(time) for ta in input_ta)\n+                # maybe set shape.\n+                current_input = tf.nest.pack_sequence_as(inputs, current_input)\n+                mask_t = masking_fn(time)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                # mask output\n+                flat_output = tf.nest.flatten(output)\n+                flat_mask_output = (\n+                    flat_zero_output\n+                    if zero_output_for_mask\n+                    else tf.nest.flatten(prev_output)\n+                )\n+                flat_new_output = compute_masked_output(\n+                    mask_t, flat_output, flat_mask_output\n+                )\n+\n+                # mask states\n+                flat_state = tf.nest.flatten(states)\n+                flat_new_state = tf.nest.flatten(new_states)\n+                flat_final_state = compute_masked_output(\n+                    mask_t, flat_new_state, flat_state\n+                )\n+                new_states = tf.nest.pack_sequence_as(\n+                    new_states, flat_final_state\n+                )\n+\n+                ta_index_to_write = time if return_all_outputs else 0\n+                output_ta_t = tuple(\n+                    ta.write(ta_index_to_write, out)\n+                    for ta, out in zip(output_ta_t, flat_new_output)\n+                )\n+\n+                return (time + 1, output_ta_t, tuple(flat_new_output)) + tuple(\n+                    new_states\n+                )\n+\n+            final_outputs = tf.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta, flat_zero_output) + states,\n+                **while_loop_kwargs,\n+            )\n+            # Skip final_outputs[2] which is the output for final timestep.\n+            new_states = final_outputs[3:]\n+        else:\n+\n+            def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta.read(time) for ta in input_ta)\n+                current_input = tf.nest.pack_sequence_as(inputs, current_input)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                flat_new_state = tf.nest.flatten(new_states)\n+\n+                flat_output = tf.nest.flatten(output)\n+                ta_index_to_write = time if return_all_outputs else 0\n+                output_ta_t = tuple(\n+                    ta.write(ta_index_to_write, out)\n+                    for ta, out in zip(output_ta_t, flat_output)\n+                )\n+\n+                new_states = tf.nest.pack_sequence_as(\n+                    initial_states, flat_new_state\n+                )\n+                return (time + 1, output_ta_t) + tuple(new_states)\n+\n+            final_outputs = tf.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta) + states,\n+                **while_loop_kwargs,\n+            )\n+            new_states = final_outputs[2:]\n+\n+        output_ta = final_outputs[1]\n+\n+        outputs = tuple(o.stack() for o in output_ta)\n+        last_output = tuple(o[-1] for o in outputs)\n+\n+        outputs = tf.nest.pack_sequence_as(output_time_zero, outputs)\n+        last_output = tf.nest.pack_sequence_as(output_time_zero, last_output)\n+\n+    # static shape inference\n+    def set_shape(output_):\n+        if isinstance(output_, tf.Tensor):\n+            shape = output_.shape.as_list()\n+            if return_all_outputs:\n+                shape[0] = time_steps\n+            else:\n+                shape[0] = 1\n+            shape[1] = batch\n+            output_.set_shape(shape)\n+        return output_\n+\n+    outputs = tf.nest.map_structure(set_shape, outputs)\n+\n+    if not time_major:\n+        outputs = tf.nest.map_structure(swap_batch_timestep, outputs)\n+\n+    return last_output, outputs, new_states\n\n@@ -243,7 +243,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             steps_per_epoch=steps_per_epoch,\n             shuffle=shuffle,\n             class_weight=class_weight,\n-            distribute_strategy=self.distribute_strategy,\n         )\n \n         # Container that configures and calls callbacks.\n@@ -286,7 +285,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        distribute_strategy=self.distribute_strategy,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n@@ -348,7 +346,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 batch_size=batch_size,\n                 steps_per_epoch=steps,\n                 shuffle=False,\n-                distribute_strategy=self.distribute_strategy,\n             )\n \n         # Container that configures and calls callbacks.\n@@ -388,7 +385,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             batch_size=batch_size,\n             steps_per_epoch=steps,\n             shuffle=False,\n-            distribute_strategy=self.distribute_strategy,\n         )\n \n         # Container that configures and calls callbacks.\n@@ -432,23 +428,20 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n \n class TFEpochIterator(EpochIterator):\n-    def __init__(self, distribute_strategy=None, *args, **kwargs):\n+    def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n-        self._distribute_strategy = distribute_strategy\n         self._steps_seen = 0\n \n     def enumerate_epoch(self):\n         if self.steps_per_epoch:\n             if not self._current_iterator:\n                 self._current_iterator = iter(\n-                    self._distribute_strategy.experimental_distribute_dataset(\n-                        self.data_adapter.get_tf_dataset()))\n+                    self.data_adapter.get_tf_dataset()\n+                )\n             for step in range(self.steps_per_epoch):\n                 yield step, self._current_iterator\n         else:\n-            iterator = iter(\n-                self._distribute_strategy.experimental_distribute_dataset(\n-                    self.data_adapter.get_tf_dataset()))\n+            iterator = iter(self.data_adapter.get_tf_dataset())\n             if self.num_batches:\n                 for step in range(self.num_batches):\n                     yield step, iterator\n\n@@ -34,7 +34,7 @@ class ReduceLROnPlateauTest(testing.TestCase):\n \n     def test_reduces_lr_with_model_fit(self):\n         reduce_lr = callbacks.ReduceLROnPlateau(\n-            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=100\n+            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=10\n         )\n \n         self.model.fit(\n@@ -49,7 +49,7 @@ class ReduceLROnPlateauTest(testing.TestCase):\n \n     def test_throws_when_optimizer_has_schedule(self):\n         reduce_lr = callbacks.ReduceLROnPlateau(\n-            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=100\n+            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=10\n         )\n \n         self.model.compile(\n@@ -75,7 +75,7 @@ class ReduceLROnPlateauTest(testing.TestCase):\n \n     def test_verbose_logging(self):\n         reduce_lr = callbacks.ReduceLROnPlateau(\n-            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=100, verbose=1\n+            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=10, verbose=1\n         )\n         io_utils.disable_interactive_logging()\n \n@@ -111,11 +111,7 @@ class ReduceLROnPlateauTest(testing.TestCase):\n \n     def test_cooldown(self):\n         reduce_lr = callbacks.ReduceLROnPlateau(\n-            patience=1,\n-            factor=0.1,\n-            monitor=\"val_loss\",\n-            min_delta=100,\n-            cooldown=2,\n+            patience=1, factor=0.1, monitor=\"val_loss\", min_delta=10, cooldown=2\n         )\n \n         self.model.fit(\n\n@@ -1,11 +1,8 @@\n from keras_core.layers.activations.activation import Activation\n from keras_core.layers.attention.attention import Attention\n from keras_core.layers.convolutional.conv1d import Conv1D\n-from keras_core.layers.convolutional.conv1d_transpose import Conv1DTranspose\n from keras_core.layers.convolutional.conv2d import Conv2D\n-from keras_core.layers.convolutional.conv2d_transpose import Conv2DTranspose\n from keras_core.layers.convolutional.conv3d import Conv3D\n-from keras_core.layers.convolutional.conv3d_transpose import Conv3DTranspose\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.einsum_dense import EinsumDense\n from keras_core.layers.core.embedding import Embedding\n@@ -81,9 +78,9 @@ from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n-from keras_core.layers.reshaping.cropping1d import Cropping1D\n from keras_core.layers.reshaping.flatten import Flatten\n from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.rnn.rnn import RNN\n\n@@ -1,255 +0,0 @@\n-\"\"\"Keras base class for transpose convolution layers.\"\"\"\n-\n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import operations as ops\n-from keras_core import regularizers\n-from keras_core.backend import standardize_data_format\n-from keras_core.backend.common.backend_utils import (\n-    compute_conv_transpose_output_shape,\n-)\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-\n-\n-class BaseConvTranspose(Layer):\n-    \"\"\"Abstract N-D transpose convolution layer.\n-\n-    The need for transposed convolutions generally arises\n-    from the desire to use a transformation going in the opposite direction\n-    of a normal convolution, i.e., from something that has the shape of the\n-    output of some convolution to something that has the shape of its input\n-    while maintaining a connectivity pattern that is compatible with\n-    said convolution.\n-\n-    Args:\n-        rank: int, the rank of the transposed convolution, e.g. 2 for 2D\n-            transposed convolution.\n-        filters: int, the dimension of the output space (the number of filters\n-            in the transposed convolution).\n-        kernel_size: int or tuple/list of N integers (N=`rank`), specifying the\n-            size of the transposed convolution window.\n-        strides: int or tuple/list of N integers, specifying the stride length\n-            of the transposed convolution. If only one int is specified, the\n-            same stride size will be used for all dimensions.\n-            `stride value != 1` is incompatible with `dilation_rate != 1`.\n-        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n-            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, steps, features)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, features, steps)`. It defaults to the `image_data_format`\n-            value found in your Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of N integers, specifying the dilation\n-            rate to use for dilated convolution. If only one int is specified,\n-            the same dilation rate will be used for all dimensions.\n-        activation: Activation function. If `None`, no activation is applied.\n-        use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n-        bias_initializer: Initializer for the bias vector. If `None`, the\n-            default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n-        bias_regularizer: Optional regularizer for the bias vector.\n-        activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n-            kernel after being updated by an `Optimizer` (e.g. used to implement\n-            norm constraints or value constraints for layer weights). The\n-            function must take as input the unprojected variable and must return\n-            the projected variable (which must have the same shape). Constraints\n-            are not safe to use when doing asynchronous distributed training.\n-        bias_constraint: Optional projection function to be applied to the\n-            bias after being updated by an `Optimizer`.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        rank,\n-        filters,\n-        kernel_size,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=None,\n-        dilation_rate=1,\n-        activation=None,\n-        use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n-        bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n-        bias_regularizer=None,\n-        activity_regularizer=None,\n-        kernel_constraint=None,\n-        bias_constraint=None,\n-        trainable=True,\n-        name=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            trainable=trainable,\n-            name=name,\n-            activity_regularizer=activity_regularizer,\n-            **kwargs,\n-        )\n-        self.rank = rank\n-        self.filters = filters\n-\n-        if isinstance(kernel_size, int):\n-            kernel_size = (kernel_size,) * self.rank\n-        self.kernel_size = kernel_size\n-\n-        if isinstance(strides, int):\n-            strides = (strides,) * self.rank\n-        self.strides = strides\n-\n-        if isinstance(dilation_rate, int):\n-            dilation_rate = (dilation_rate,) * self.rank\n-        self.dilation_rate = dilation_rate\n-\n-        self.padding = padding\n-        self.data_format = standardize_data_format(data_format)\n-        self.activation = activations.get(activation)\n-        self.use_bias = use_bias\n-        self.kernel_initializer = initializers.get(kernel_initializer)\n-        self.bias_initializer = initializers.get(bias_initializer)\n-        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-        self.bias_regularizer = regularizers.get(bias_regularizer)\n-        self.kernel_constraint = constraints.get(kernel_constraint)\n-        self.bias_constraint = constraints.get(bias_constraint)\n-        self.input_spec = InputSpec(min_ndim=self.rank + 2)\n-        self.data_format = self.data_format\n-\n-        if self.filters is not None and self.filters <= 0:\n-            raise ValueError(\n-                \"Invalid value for argument `filters`. Expected a strictly \"\n-                f\"positive value. Received filters={self.filters}.\"\n-            )\n-\n-        if not all(self.kernel_size):\n-            raise ValueError(\n-                \"The argument `kernel_size` cannot contain 0. Received: \"\n-                f\"{self.kernel_size}\"\n-            )\n-\n-        if not all(self.strides):\n-            raise ValueError(\n-                \"The argument `strides` cannot contains 0. Received: \"\n-                f\"{self.strides}\"\n-            )\n-\n-        if max(self.strides) > 1 and max(self.dilation_rate) > 1:\n-            raise ValueError(\n-                \"`strides > 1` not supported in conjunction with \"\n-                f\"`dilation_rate > 1`. Received: strides={self.strides} and \"\n-                f\"dilation_rate={self.dilation_rate}\"\n-            )\n-\n-    def build(self, input_shape):\n-        if self.data_format == \"channels_last\":\n-            channel_axis = -1\n-            input_channel = input_shape[-1]\n-        else:\n-            channel_axis = 1\n-            input_channel = input_shape[1]\n-        self.input_spec = InputSpec(\n-            min_ndim=self.rank + 2, axes={channel_axis: input_channel}\n-        )\n-        kernel_shape = self.kernel_size + (\n-            self.filters,\n-            input_channel,\n-        )\n-\n-        self.kernel = self.add_weight(\n-            name=\"kernel\",\n-            shape=kernel_shape,\n-            initializer=self.kernel_initializer,\n-            regularizer=self.kernel_regularizer,\n-            constraint=self.kernel_constraint,\n-            trainable=True,\n-            dtype=self.dtype,\n-        )\n-        if self.use_bias:\n-            self.bias = self.add_weight(\n-                name=\"bias\",\n-                shape=(self.filters,),\n-                initializer=self.bias_initializer,\n-                regularizer=self.bias_regularizer,\n-                constraint=self.bias_constraint,\n-                trainable=True,\n-                dtype=self.dtype,\n-            )\n-        else:\n-            self.bias = None\n-        self.built = True\n-\n-    def call(self, inputs):\n-        outputs = ops.conv_transpose(\n-            inputs,\n-            self.kernel,\n-            strides=list(self.strides),\n-            padding=self.padding,\n-            dilation_rate=self.dilation_rate,\n-            data_format=self.data_format,\n-        )\n-\n-        if self.use_bias:\n-            if self.data_format == \"channels_last\":\n-                bias_shape = (1,) * (self.rank + 1) + (self.filters,)\n-            else:\n-                bias_shape = (1, self.filters) + (1,) * self.rank\n-            bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n-\n-        if self.activation is not None:\n-            return self.activation(outputs)\n-        return outputs\n-\n-    def compute_output_shape(self, input_shape):\n-        return compute_conv_transpose_output_shape(\n-            input_shape,\n-            self.kernel_size,\n-            self.filters,\n-            strides=self.strides,\n-            padding=self.padding,\n-            data_format=self.data_format,\n-            dilation_rate=self.dilation_rate,\n-        )\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"filters\": self.filters,\n-                \"kernel_size\": self.kernel_size,\n-                \"strides\": self.strides,\n-                \"padding\": self.padding,\n-                \"data_format\": self.data_format,\n-                \"dilation_rate\": self.dilation_rate,\n-                \"activation\": activations.serialize(self.activation),\n-                \"use_bias\": self.use_bias,\n-                \"kernel_initializer\": initializers.serialize(\n-                    self.kernel_initializer\n-                ),\n-                \"bias_initializer\": initializers.serialize(\n-                    self.bias_initializer\n-                ),\n-                \"kernel_regularizer\": regularizers.serialize(\n-                    self.kernel_regularizer\n-                ),\n-                \"bias_regularizer\": regularizers.serialize(\n-                    self.bias_regularizer\n-                ),\n-                \"activity_regularizer\": regularizers.serialize(\n-                    self.activity_regularizer\n-                ),\n-                \"kernel_constraint\": constraints.serialize(\n-                    self.kernel_constraint\n-                ),\n-                \"bias_constraint\": constraints.serialize(self.bias_constraint),\n-            }\n-        )\n-        return config\n\n@@ -80,7 +80,8 @@ class Conv1D(BaseConv):\n \n     >>> # The inputs are 128-length vectors with 10 timesteps, and the\n     >>> # batch size is 4.\n-    >>> x = np.random.rand(4, 10, 128)\n+    >>> input_shape = (4, 10, 128)\n+    >>> x = np.random.normal(4, 10, 128)\n     >>> y = keras_core.layers.Conv1D(32, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 32)\n\n@@ -1,136 +0,0 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.layers.Conv1DTranspose\",\n-        \"keras_core.layers.Convolution1DTranspose\",\n-    ]\n-)\n-class Conv1DTranspose(BaseConvTranspose):\n-    \"\"\"1D transposed convolution layer.\n-\n-    The need for transposed convolutions generally arise from the desire to use\n-    a transformation going in the opposite direction of a normal convolution,\n-    i.e., from something that has the shape of the output of some convolution\n-    to something that has the shape of its input while maintaining a\n-    connectivity pattern that is compatible with said convolution.\n-\n-    Args:\n-        filters: int, the dimension of the output space (the number of filters\n-            in the transpose convolution).\n-        kernel_size: int or tuple/list of 1 integer, specifying the size of the\n-            transposed convolution window.\n-        strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n-        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n-            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, steps, features)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, features, steps)`. It defaults to the `image_data_format`\n-            value found in your Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n-            rate to use for dilated transposed convolution.\n-        groups: A positive int specifying the number of groups in which the\n-            input is split along the channel axis. Each group is convolved\n-            separately with `filters // groups` filters. The output is the\n-            concatenation of all the `groups` results along the channel axis.\n-            Input channels and `filters` must both be divisible by `groups`.\n-        activation: Activation function. If `None`, no activation is applied.\n-        use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n-        bias_initializer: Initializer for the bias vector. If `None`, the\n-            default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n-        bias_regularizer: Optional regularizer for the bias vector.\n-        activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n-            kernel after being updated by an `Optimizer` (e.g. used to implement\n-            norm constraints or value constraints for layer weights). The\n-            function must take as input the unprojected variable and must return\n-            the projected variable (which must have the same shape). Constraints\n-            are not safe to use when doing asynchronous distributed training.\n-        bias_constraint: Optional projection function to be applied to the\n-            bias after being updated by an `Optimizer`.\n-\n-    Input shape:\n-    - If `data_format=\"channels_last\"`:\n-        A 3D tensor with shape: `(batch_shape, steps, channels)`\n-    - If `data_format=\"channels_first\"`:\n-        A 3D tensor with shape: `(batch_shape, channels, steps)`\n-\n-    Output shape:\n-    - If `data_format=\"channels_last\"`:\n-        A 3D tensor with shape: `(batch_shape, new_steps, channels)`\n-    - If `data_format=\"channels_first\"`:\n-        A 3D tensor with shape: `(batch_shape, channels, new_steps)`\n-\n-    Returns:\n-        A 3D tensor representing\n-        `activation(conv1d_transpose(inputs, kernel) + bias)`.\n-\n-    Raises:\n-        ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-\n-    References:\n-    - [A guide to convolution arithmetic for deep learning](\n-        https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional Networks](\n-        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-\n-    Examples:\n-\n-    >>> x = np.random.rand(4, 10, 128)\n-    >>> y = keras_core.layers.Conv1DTranspose(32, 3, 2, activation='relu')(x)\n-    >>> print(y.shape)\n-    (4, 21, 32)\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-        activation=None,\n-        use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n-        bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n-        bias_regularizer=None,\n-        activity_regularizer=None,\n-        kernel_constraint=None,\n-        bias_constraint=None,\n-        **kwargs\n-    ):\n-        super().__init__(\n-            rank=1,\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-            activation=activation,\n-            use_bias=use_bias,\n-            kernel_initializer=kernel_initializer,\n-            bias_initializer=bias_initializer,\n-            kernel_regularizer=kernel_regularizer,\n-            bias_regularizer=bias_regularizer,\n-            activity_regularizer=activity_regularizer,\n-            kernel_constraint=kernel_constraint,\n-            bias_constraint=bias_constraint,\n-            **kwargs\n-        )\n\n@@ -80,7 +80,7 @@ class Conv2D(BaseConv):\n \n     Examples:\n \n-    >>> x = np.random.rand(4, 10, 10, 128)\n+    >>> x = np.random.normal(4, 10, 10, 128)\n     >>> y = keras_core.layers.Conv2D(32, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 8, 32)\n\n@@ -1,133 +0,0 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.layers.Conv2DTranspose\",\n-        \"keras_core.layers.Convolution2DTranspose\",\n-    ]\n-)\n-class Conv2DTranspose(BaseConvTranspose):\n-    \"\"\"2D transposed convolution layer.\n-\n-    The need for transposed convolutions generally arise from the desire to use\n-    a transformation going in the opposite direction of a normal convolution,\n-    i.e., from something that has the shape of the output of some convolution\n-    to something that has the shape of its input while maintaining a\n-    connectivity pattern that is compatible with said convolution.\n-\n-    Args:\n-        filters: int, the dimension of the output space (the number of filters\n-            in the transposed convolution).\n-        kernel_size: int or tuple/list of 1 integer, specifying the size of the\n-            transposed convolution window.\n-        strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n-        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n-            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape\n-            `(batch_size, channels, height, width)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch_size, channels, height, width)`. It defaults to the\n-            `image_data_format` value found in your Keras config file at\n-            `~/.keras/keras.json`. If you never set it, then it will be\n-            `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n-            rate to use for dilated transposed convolution.\n-        activation: Activation function. If `None`, no activation is applied.\n-        use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n-        bias_initializer: Initializer for the bias vector. If `None`, the\n-            default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n-        bias_regularizer: Optional regularizer for the bias vector.\n-        activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n-            kernel after being updated by an `Optimizer` (e.g. used to implement\n-            norm constraints or value constraints for layer weights). The\n-            function must take as input the unprojected variable and must return\n-            the projected variable (which must have the same shape). Constraints\n-            are not safe to use when doing asynchronous distributed training.\n-        bias_constraint: Optional projection function to be applied to the\n-            bias after being updated by an `Optimizer`.\n-\n-    Input shape:\n-    - If `data_format=\"channels_last\"`:\n-        A 4D tensor with shape: `(batch_size, height, width, channels)`\n-    - If `data_format=\"channels_first\"`:\n-        A 4D tensor with shape: `(batch_size, channels, height, width)`\n-\n-    Output shape:\n-    - If `data_format=\"channels_last\"`:\n-        A 4D tensor with shape: `(batch_size, new_height, new_width filters)`\n-    - If `data_format=\"channels_first\"`:\n-        A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n-\n-    Returns:\n-        A 4D tensor representing\n-        `activation(conv2d_transpose(inputs, kernel) + bias)`.\n-\n-    Raises:\n-        ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-\n-    References:\n-    - [A guide to convolution arithmetic for deep learning](\n-        https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional Networks](\n-        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-\n-    Examples:\n-\n-    >>> x = np.random.rand(4, 10, 8, 128)\n-    >>> y = keras_core.layers.Conv2DTranspose(32, 2, 2, activation='relu')(x)\n-    >>> print(y.shape)\n-    (4, 20, 16, 32)\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-        activation=None,\n-        use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n-        bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n-        bias_regularizer=None,\n-        activity_regularizer=None,\n-        kernel_constraint=None,\n-        bias_constraint=None,\n-        **kwargs\n-    ):\n-        super().__init__(\n-            rank=2,\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-            activation=activation,\n-            use_bias=use_bias,\n-            kernel_initializer=kernel_initializer,\n-            bias_initializer=bias_initializer,\n-            kernel_regularizer=kernel_regularizer,\n-            bias_regularizer=bias_regularizer,\n-            activity_regularizer=activity_regularizer,\n-            kernel_constraint=kernel_constraint,\n-            bias_constraint=bias_constraint,\n-            **kwargs\n-        )\n\n@@ -1,138 +0,0 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.layers.Conv3DTranspose\",\n-        \"keras_core.layers.Convolution3DTranspose\",\n-    ]\n-)\n-class Conv3DTranspose(BaseConvTranspose):\n-    \"\"\"3D transposed convolution layer.\n-\n-    The need for transposed convolutions generally arise from the desire to use\n-    a transformation going in the opposite direction of a normal convolution,\n-    i.e., from something that has the shape of the output of some convolution\n-    to something that has the shape of its input while maintaining a\n-    connectivity pattern that is compatible with said convolution.\n-\n-    Args:\n-        filters: int, the dimension of the output space (the number of filters\n-            in the transposed convolution).\n-        kernel_size: int or tuple/list of 1 integer, specifying the size of the\n-            transposed convolution window.\n-        strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n-        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n-            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape\n-            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`.\n-            It defaults to the `image_data_format` value found in your Keras\n-            config file at `~/.keras/keras.json`. If you never set it, then it\n-            will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n-            rate to use for dilated transposed convolution.\n-        activation: Activation function. If `None`, no activation is applied.\n-        use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n-        bias_initializer: Initializer for the bias vector. If `None`, the\n-            default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n-        bias_regularizer: Optional regularizer for the bias vector.\n-        activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n-            kernel after being updated by an `Optimizer` (e.g. used to implement\n-            norm constraints or value constraints for layer weights). The\n-            function must take as input the unprojected variable and must return\n-            the projected variable (which must have the same shape). Constraints\n-            are not safe to use when doing asynchronous distributed training.\n-        bias_constraint: Optional projection function to be applied to the\n-            bias after being updated by an `Optimizer`.\n-\n-    Input shape:\n-    - If `data_format=\"channels_last\"`:\n-        5D tensor with shape:\n-        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-    - If `data_format=\"channels_first\"`:\n-        5D tensor with shape:\n-        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n-\n-    Output shape:\n-    - If `data_format=\"channels_last\"`:\n-        5D tensor with shape:\n-        `(batch_size, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3,\n-        channels)`\n-    - If `data_format=\"channels_first\"`:\n-        5D tensor with shape:\n-        `(batch_size, channels, new_spatial_dim1, new_spatial_dim2,\n-        new_spatial_dim3)`\n-\n-    Returns:\n-        A 5D tensor representing `activation(conv3d(inputs, kernel) + bias)`.\n-\n-    Raises:\n-        ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-\n-    References:\n-    - [A guide to convolution arithmetic for deep learning](\n-        https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional Networks](\n-        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-\n-    Examples:\n-\n-    >>> x = np.random.rand(4, 10, 8, 12, 128)\n-    >>> y = keras_core.layers.Conv3DTranspose(32, 2, 2, activation='relu')(x)\n-    >>> print(y.shape)\n-    (4, 20, 16, 24, 32)\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-        activation=None,\n-        use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n-        bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n-        bias_regularizer=None,\n-        activity_regularizer=None,\n-        kernel_constraint=None,\n-        bias_constraint=None,\n-        **kwargs\n-    ):\n-        super().__init__(\n-            rank=3,\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-            activation=activation,\n-            use_bias=use_bias,\n-            kernel_initializer=kernel_initializer,\n-            bias_initializer=bias_initializer,\n-            kernel_regularizer=kernel_regularizer,\n-            bias_regularizer=bias_regularizer,\n-            activity_regularizer=activity_regularizer,\n-            kernel_constraint=kernel_constraint,\n-            bias_constraint=bias_constraint,\n-            **kwargs\n-        )\n\n@@ -1,420 +0,0 @@\n-import numpy as np\n-import tensorflow as tf\n-from absl.testing import parameterized\n-\n-from keras_core import layers\n-from keras_core import testing\n-\n-\n-class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-            \"input_shape\": (2, 8, 4),\n-            \"output_shape\": (2, 16, 5),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1,),\n-            \"input_shape\": (2, 8, 4),\n-            \"output_shape\": (2, 24, 6),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2,),\n-            \"strides\": (2,),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-            \"input_shape\": (2, 8, 4),\n-            \"output_shape\": (2, 16, 6),\n-        },\n-    )\n-    def test_conv1d_transpose_basic(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-        input_shape,\n-        output_shape,\n-    ):\n-        self.run_layer_test(\n-            layers.Conv1DTranspose,\n-            init_kwargs={\n-                \"filters\": filters,\n-                \"kernel_size\": kernel_size,\n-                \"strides\": strides,\n-                \"padding\": padding,\n-                \"data_format\": data_format,\n-                \"dilation_rate\": dilation_rate,\n-            },\n-            input_shape=input_shape,\n-            expected_output_shape=output_shape,\n-            expected_num_trainable_weights=2,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-        )\n-\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-            \"input_shape\": (2, 8, 8, 4),\n-            \"output_shape\": (2, 16, 16, 5),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1),\n-            \"input_shape\": (2, 8, 8, 4),\n-            \"output_shape\": (2, 24, 24, 6),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2, 3),\n-            \"strides\": (2, 1),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1),\n-            \"input_shape\": (2, 8, 8, 4),\n-            \"output_shape\": (2, 16, 10, 6),\n-        },\n-    )\n-    def test_conv2d_transpose_basic(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-        input_shape,\n-        output_shape,\n-    ):\n-        self.run_layer_test(\n-            layers.Conv2DTranspose,\n-            init_kwargs={\n-                \"filters\": filters,\n-                \"kernel_size\": kernel_size,\n-                \"strides\": strides,\n-                \"padding\": padding,\n-                \"data_format\": data_format,\n-                \"dilation_rate\": dilation_rate,\n-            },\n-            input_shape=input_shape,\n-            expected_output_shape=output_shape,\n-            expected_num_trainable_weights=2,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-        )\n-\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-            \"input_shape\": (2, 8, 8, 8, 4),\n-            \"output_shape\": (2, 16, 16, 16, 5),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1, 1),\n-            \"input_shape\": (2, 8, 8, 8, 4),\n-            \"output_shape\": (2, 24, 24, 24, 6),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2, 2, 3),\n-            \"strides\": (2, 1, 2),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1, 1),\n-            \"input_shape\": (2, 8, 8, 8, 4),\n-            \"output_shape\": (2, 16, 9, 17, 6),\n-        },\n-    )\n-    def test_conv3d_transpose_basic(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-        input_shape,\n-        output_shape,\n-    ):\n-        self.run_layer_test(\n-            layers.Conv3DTranspose,\n-            init_kwargs={\n-                \"filters\": filters,\n-                \"kernel_size\": kernel_size,\n-                \"strides\": strides,\n-                \"padding\": padding,\n-                \"data_format\": data_format,\n-                \"dilation_rate\": dilation_rate,\n-            },\n-            input_shape=input_shape,\n-            expected_output_shape=output_shape,\n-            expected_num_trainable_weights=2,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-        )\n-\n-    def test_bad_init_args(self):\n-        # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n-            layers.Conv1DTranspose(filters=0, kernel_size=1)\n-\n-        # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n-            layers.Conv2DTranspose(filters=2, kernel_size=(1, 0))\n-\n-        # `strides` has 0.\n-        with self.assertRaises(ValueError):\n-            layers.Conv2DTranspose(\n-                filters=2, kernel_size=(2, 2), strides=(1, 0)\n-            )\n-\n-        # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n-            layers.Conv2DTranspose(\n-                filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n-            )\n-\n-\n-class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1,),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2,),\n-            \"strides\": (2,),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-        },\n-    )\n-    def test_conv1d_transpose(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        layer = layers.Conv1DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-        tf_keras_layer = tf.keras.layers.Conv1DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-\n-        inputs = np.random.normal(size=[2, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n-\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2, 3),\n-            \"strides\": (2, 1),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1),\n-        },\n-    )\n-    def test_conv2d_transpose(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        layer = layers.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-        tf_keras_layer = tf.keras.layers.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-\n-        inputs = np.random.normal(size=[2, 8, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n-\n-    @parameterized.parameters(\n-        {\n-            \"filters\": 5,\n-            \"kernel_size\": 2,\n-            \"strides\": 2,\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": 1,\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n-            \"padding\": \"same\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1, 1),\n-        },\n-        {\n-            \"filters\": 6,\n-            \"kernel_size\": (2, 2, 3),\n-            \"strides\": (2, 1, 2),\n-            \"padding\": \"valid\",\n-            \"data_format\": \"channels_last\",\n-            \"dilation_rate\": (1, 1, 1),\n-        },\n-    )\n-    def test_conv3d_transpose(\n-        self,\n-        filters,\n-        kernel_size,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        layer = layers.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-        tf_keras_layer = tf.keras.layers.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n-\n-        inputs = np.random.normal(size=[2, 8, 8, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n\n@@ -1,79 +0,0 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-\n-\n-@keras_core_export(\"keras_core.layers.Cropping1D\")\n-class Cropping1D(Layer):\n-    \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n-\n-    It crops along the time dimension (axis 1).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 2)\n-    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> print(x)\n-    [[[ 0  1]\n-      [ 2  3]\n-      [ 4  5]]\n-     [[ 6  7]\n-      [ 8  9]\n-      [10 11]]]\n-    >>> y = keras_core.layers.Cropping1D(cropping=1)(x)\n-    >>> print(y)\n-    [[[2 3]]\n-     [[8 9]]]\n-\n-    Args:\n-        cropping: Integer or tuple of integers of length 2.\n-            How many units should be trimmed off at the beginning and end of\n-            the cropping dimension (axis 1).\n-            If a single int is provided, the same value will be used for both.\n-\n-    Input shape:\n-        3D tensor with shape `(batch_size, axis_to_crop, features)`\n-\n-    Output shape:\n-        3D tensor with shape `(batch_size, cropped_axis, features)`\n-    \"\"\"\n-\n-    def __init__(self, cropping=(1, 1), name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n-        if isinstance(cropping, int):\n-            cropping = (cropping, cropping)\n-        self.cropping = cropping\n-        self.input_spec = InputSpec(ndim=3)\n-\n-    def compute_output_shape(self, input_shape):\n-        if input_shape[1] is not None:\n-            length = input_shape[1] - self.cropping[0] - self.cropping[1]\n-            if length <= 0:\n-                raise ValueError(\n-                    \"`cropping` parameter of `Cropping1D` layer must be \"\n-                    \"greater than the input length. Received: input_shape=\"\n-                    f\"{input_shape}, cropping={self.cropping}\"\n-                )\n-        else:\n-            length = None\n-        return (input_shape[0], length, input_shape[2])\n-\n-    def call(self, inputs):\n-        if (\n-            inputs.shape[1] is not None\n-            and sum(self.cropping) >= inputs.shape[1]\n-        ):\n-            raise ValueError(\n-                \"`cropping` parameter of `Cropping1D` layer must be \"\n-                \"greater than the input length. Received: inputs.shape=\"\n-                f\"{inputs.shape}, cropping={self.cropping}\"\n-            )\n-        if self.cropping[1] == 0:\n-            return inputs[:, self.cropping[0] :, :]\n-        else:\n-            return inputs[:, self.cropping[0] : -self.cropping[1], :]\n-\n-    def get_config(self):\n-        config = {\"cropping\": self.cropping}\n-        base_config = super().get_config()\n-        return {**base_config, **config}\n\n@@ -1,62 +0,0 @@\n-import numpy as np\n-import pytest\n-\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import operations as ops\n-from keras_core import testing\n-\n-\n-class CroppingTest(testing.TestCase):\n-    def test_cropping_1d(self):\n-        inputs = np.random.rand(3, 5, 7)\n-\n-        # Cropping with different values on the left and the right.\n-        self.run_layer_test(\n-            layers.Cropping1D,\n-            init_kwargs={\"cropping\": (1, 2)},\n-            input_data=inputs,\n-            expected_output=ops.convert_to_tensor(inputs[:, 1:3, :]),\n-        )\n-        # Same cropping on the left and the right.\n-        self.run_layer_test(\n-            layers.Cropping1D,\n-            init_kwargs={\"cropping\": (1, 1)},\n-            input_data=inputs,\n-            expected_output=ops.convert_to_tensor(inputs[:, 1:4, :]),\n-        )\n-        # Same cropping on the left and the right provided as an int.\n-        self.run_layer_test(\n-            layers.Cropping1D,\n-            init_kwargs={\"cropping\": 1},\n-            input_data=inputs,\n-            expected_output=ops.convert_to_tensor(inputs[:, 1:4, :]),\n-        )\n-        # Cropping on the right only.\n-        self.run_layer_test(\n-            layers.Cropping1D,\n-            init_kwargs={\"cropping\": (0, 1)},\n-            input_data=inputs,\n-            expected_output=ops.convert_to_tensor(inputs[:, 0:4, :]),\n-        )\n-        # Cropping on the left only.\n-        self.run_layer_test(\n-            layers.Cropping1D,\n-            init_kwargs={\"cropping\": (1, 0)},\n-            input_data=inputs,\n-            expected_output=ops.convert_to_tensor(inputs[:, 1:5, :]),\n-        )\n-\n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n-    def test_cropping_1d_with_dynamic_batch_size(self):\n-        input_layer = layers.Input(batch_shape=(None, 5, 7))\n-        permuted = layers.Cropping1D((1, 2))(input_layer)\n-        self.assertEqual(permuted.shape, (None, 2, 7))\n-\n-    def test_cropping_1d_errors_if_cropping_more_than_available(self):\n-        with self.assertRaises(ValueError):\n-            input_layer = layers.Input(shape=(3, 4, 7))\n-            layers.Cropping1D(cropping=(2, 3))(input_layer)\n\n\n@@ -0,0 +1,446 @@\n+from tensorflow import nest\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import tracking\n+\n+\n+class StackedRNNCells:\n+    pass\n+\n+\n+class DropoutRNNCellMixin:\n+    pass\n+\n+\n+@keras_core_export(\"keras_core.layers.RNN\")\n+class RNN(Layer):\n+    \"\"\"Base class for recurrent layers.\n+\n+    Args:\n+        cell: A RNN cell instance or a list of RNN cell instances.\n+            A RNN cell is a class that has:\n+            - A `call(input_at_t, states_at_t)` method, returning\n+            `(output_at_t, states_at_t_plus_1)`. The call method of the\n+            cell can also take the optional argument `constants`, see\n+            section \"Note on passing external constants\" below.\n+            - A `state_size` attribute. This can be a single integer\n+            (single state) in which case it is the size of the recurrent\n+            state. This can also be a list of integers\n+            (one size per state).\n+            - A `output_size` attribute, a single integer.\n+            - A `get_initial_state(batch_size=None, dtype=None)`\n+            method that creates a tensor meant to be fed to `call()` as the\n+            initial state, if the user didn't specify any initial state\n+            via other means. The returned initial state should have\n+            shape `(batch_size, cell.state_size)`.\n+            The cell might choose to create a tensor full of zeros,\n+            or other values based on the cell's implementation.\n+            `inputs` is the input tensor to the RNN layer, with shape\n+            `(batch_size, timesteps, features)`.\n+            If this method is not implemented\n+            by the cell, the RNN layer will create a zero filled tensor\n+            with shape `(batch_size, cell.state_size)`.\n+            In the case that `cell` is a list of RNN cell instances, the cells\n+            will be stacked on top of each other in the RNN, resulting in an\n+            efficient stacked RNN.\n+        return_sequences: Boolean (default `False`). Whether to return the last\n+            output in the output sequence, or the full sequence.\n+        return_state: Boolean (default `False`).\n+            Whether to return the last state in addition to the output.\n+        go_backwards: Boolean (default `False`).\n+            If `True`, process the input sequence backwards and return the\n+            reversed sequence.\n+        stateful: Boolean (default `False`). If True, the last state\n+            for each sample at index `i` in a batch will be used as initial\n+            state for the sample of index `i` in the following batch.\n+        unroll: Boolean (default `False`).\n+            If True, the network will be unrolled, else a symbolic loop will be\n+            used. Unrolling can speed-up a RNN, although it tends to be more\n+            memory-intensive. Unrolling is only suitable for short sequences.\n+        zero_output_for_mask: Boolean (default `False`).\n+            Whether the output should use zeros for the masked timesteps.\n+            Note that this field is only used when `return_sequences`\n+            is `True` and `mask` is provided.\n+            It can useful if you want to reuse the raw output sequence of\n+            the RNN without interference from the masked timesteps, e.g.,\n+            merging bidirectional RNNs.\n+\n+    Call arguments:\n+        inputs: Input tensor.\n+        initial_state: List of initial state tensors to be passed to the first\n+            call of the cell.\n+        mask: Binary tensor of shape `[batch_size, timesteps]`\n+            indicating whether a given timestep should be masked.\n+            An individual `True` entry indicates that the corresponding\n+            timestep should be utilized, while a `False` entry indicates\n+            that the corresponding timestep should be ignored.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode or in inference mode. This argument is passed\n+            to the cell when calling it.\n+            This is for use with cells that use dropout.\n+\n+    Input shape:\n+        N-D tensor with shape `[batch_size, timesteps, ...]` or\n+        `[timesteps, batch_size, ...]` when time_major is True.\n+\n+    Output shape:\n+\n+    - If `return_state`: a list of tensors. The first tensor is\n+    the output. The remaining tensors are the last states,\n+    each with shape `[batch_size, state_size]`, where `state_size` could\n+    be a high dimension tensor shape.\n+    - If `return_sequences`: N-D tensor with shape\n+    `[batch_size, timesteps, output_size]`, where `output_size` could\n+    be a high dimension tensor shape, or\n+    `[timesteps, batch_size, output_size]` when `time_major` is True.\n+    - Else, N-D tensor with shape `[batch_size, output_size]`, where\n+    `output_size` could be a high dimension tensor shape.\n+\n+    Masking:\n+\n+    This layer supports masking for input data with a variable number\n+    of timesteps. To introduce masks to your data,\n+    use a `keras_core.layers.Embedding` layer with the `mask_zero` parameter\n+    set to `True`.\n+\n+    Note on using statefulness in RNNs:\n+\n+    You can set RNN layers to be 'stateful', which means that the states\n+    computed for the samples in one batch will be reused as initial states\n+    for the samples in the next batch. This assumes a one-to-one mapping\n+    between samples in different successive batches.\n+\n+    To enable statefulness:\n+\n+    - Specify `stateful=True` in the layer constructor.\n+    - Specify a fixed batch size for your model, by passing\n+    If sequential model:\n+        `batch_input_shape=(...)` to the first layer in your model.\n+    Else for functional model with 1 or more Input layers:\n+        `batch_shape=(...)` to all the first layers in your model.\n+    This is the expected shape of your inputs\n+    *including the batch size*.\n+    It should be a tuple of integers, e.g. `(32, 10, 100)`.\n+    - Specify `shuffle=False` when calling `fit()`.\n+\n+    To reset the states of your model, call `.reset_states()` on either\n+    a specific layer, or on your entire model.\n+\n+    Note on specifying the initial state of RNNs:\n+\n+    You can specify the initial state of RNN layers symbolically by\n+    calling them with the keyword argument `initial_state`. The value of\n+    `initial_state` should be a tensor or list of tensors representing\n+    the initial state of the RNN layer.\n+\n+    You can specify the initial state of RNN layers numerically by\n+    calling `reset_states` with the keyword argument `states`. The value of\n+    `states` should be a numpy array or list of numpy arrays representing\n+    the initial state of the RNN layer.\n+\n+    Examples:\n+\n+    ```python\n+    from keras_core.layers import RNN\n+    from keras_core import operations as ops\n+\n+    # First, let's define a RNN Cell, as a layer subclass.\n+    class MinimalRNNCell(keras_core.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(MinimalRNNCell, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n+                                          initializer='uniform',\n+                                          name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states):\n+            prev_output = states[0]\n+            h = ops.matmul(inputs, self.kernel)\n+            output = h + ops.matmul(prev_output, self.recurrent_kernel)\n+            return output, [output]\n+\n+    # Let's use this cell in a RNN layer:\n+\n+    cell = MinimalRNNCell(32)\n+    x = keras_core.Input((None, 5))\n+    layer = RNN(cell)\n+    y = layer(x)\n+\n+    # Here's how to use the cell to build a stacked RNN:\n+\n+    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n+    x = keras_core.Input((None, 5))\n+    layer = RNN(cells)\n+    y = layer(x)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        cell,\n+        return_sequences=False,\n+        return_state=False,\n+        go_backwards=False,\n+        stateful=False,\n+        unroll=False,\n+        **kwargs,\n+    ):\n+        if isinstance(cell, (list, tuple)):\n+            cell = StackedRNNCells(cell)\n+        if \"call\" not in dir(cell):\n+            raise ValueError(\n+                \"Argument `cell` should have a `call` method. \"\n+                f\"Received: cell={cell}\"\n+            )\n+        if \"state_size\" not in dir(cell):\n+            raise ValueError(\n+                \"The RNN cell should have a `state_size` attribute \"\n+                \"(single integer or list of integers, \"\n+                \"one integer per RNN state). \"\n+                f\"Received: cell={cell}\"\n+            )\n+        # If True, the output for masked timestep will be zeros, whereas in the\n+        # False case, output from previous timestep is returned for masked\n+        # timestep.\n+        self.zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n+\n+        super().__init__(**kwargs)\n+        self.cell = cell\n+        self.return_sequences = return_sequences\n+        self.return_state = return_state\n+        self.go_backwards = go_backwards\n+        self.stateful = stateful\n+        self.unroll = unroll\n+\n+        self.supports_masking = True\n+        self.input_spec = None\n+        self.states = None\n+\n+        state_size = getattr(self.cell, \"state_size\", None)\n+        if state_size is None:\n+            raise ValueError(\n+                \"state_size must be specified as property on the RNN cell.\"\n+            )\n+        if not isinstance(state_size, (list, int)):\n+            raise ValueError(\n+                \"state_size must be an integer, or a list of integers \"\n+                \"(one for each state tensor).\"\n+            )\n+        if isinstance(state_size, int):\n+            self.state_size = [state_size]\n+            self.single_state = True\n+        else:\n+            self.state_size = state_size\n+            self.single_state = False\n+\n+    def compute_output_shape(self, sequence_shape, initial_state_shape=None):\n+        if self.single_state:\n+            # Single state case.\n+            state_shape = (sequence_shape[0], self.state_size[0])\n+        else:\n+            # Multiple states case.\n+            state_shape = [(sequence_shape[0], d) for d in self.state_size]\n+\n+        # There can only be one output.\n+        output_size = getattr(self.cell, \"output_size\", None)\n+        if output_size is None:\n+            output_size = self.state_size[0]\n+        if not isinstance(output_size, int):\n+            raise ValueError(\"output_size must be an integer.\")\n+        if self.return_sequences:\n+            output_shape = (sequence_shape[0], sequence_shape[1], output_size)\n+        else:\n+            output_shape = (sequence_shape[0], output_size)\n+        if self.return_state:\n+            return output_shape, state_shape\n+        return output_shape\n+\n+    def compute_mask(self, _, mask):\n+        # Time step masks must be the same for each input.\n+        # This is because the mask for an RNN is of size [batch, time_steps, 1],\n+        # and specifies which time steps should be skipped, and a time step\n+        # must be skipped for all inputs.\n+        mask = nest.flatten(mask)[0]\n+        output_mask = mask if self.return_sequences else None\n+        if self.return_state:\n+            state_mask = [None for _ in self.state_size]\n+            return [output_mask] + state_mask\n+        else:\n+            return output_mask\n+\n+    def build(self, sequence_shape, initial_state_shape=None):\n+        # Build cell (if layer).\n+        step_input_shape = (sequence_shape[0],) + sequence_shape[2:]\n+        if isinstance(self.cell, Layer) and not self.cell.built:\n+            self.cell.build(step_input_shape)\n+            self.cell.built = True\n+        if self.stateful:\n+            if self.states is not None:\n+                self.reset_state()\n+            else:\n+                if sequence_shape[0] is None:\n+                    raise ValueError(\n+                        \"When using `stateful=True` in a RNN, the \"\n+                        \"batch size must be static. Found dynamic \"\n+                        f\"batch size: sequence.shape={sequence_shape}\"\n+                    )\n+                self._create_state_variables(sequence_shape[0])\n+        self.built = True\n+\n+    @tracking.no_automatic_dependency_tracking\n+    def _create_state_variables(self, batch_size):\n+        self.states = [\n+            self.add_variable(\n+                initializer=\"zeros\", shape=(batch_size, d), trainable=False\n+            )\n+            for d in self.state_size\n+        ]\n+\n+    def get_initial_state(self, sequence):\n+        input_shape = ops.shape(sequence)\n+        batch_size = input_shape[0]\n+        dtype = sequence.dtype\n+\n+        get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n+        if get_initial_state_fn:\n+            init_state = get_initial_state_fn(\n+                batch_size=batch_size, dtype=dtype\n+            )\n+        else:\n+            return [\n+                ops.zeros((batch_size, d), dtype=dtype) for d in self.state_size\n+            ]\n+\n+        # RNN expect the states in a list, even if single state.\n+        if not nest.is_nested(init_state):\n+            init_state = [init_state]\n+        # Force the state to be a list in case it is a namedtuple eg\n+        # LSTMStateTuple.\n+        return list(init_state)\n+\n+    def reset_states(self):\n+        # Compatibility alias.\n+        self.reset_state()\n+\n+    def reset_state(self):\n+        if self.states is not None:\n+            for v in self.states:\n+                v.assign(ops.zeros_like(v))\n+\n+    def call(\n+        self,\n+        sequence,\n+        initial_state=None,\n+        mask=None,\n+        training=False,\n+    ):\n+        self._maybe_reset_cell_dropout_mask(self.cell)\n+\n+        timesteps = sequence.shape[1]\n+        if self.unroll and timesteps is None:\n+            raise ValueError(\n+                \"Cannot unroll a RNN if the \"\n+                \"time dimension is undefined. \\n\"\n+                \"- If using a Sequential model, \"\n+                \"specify the time dimension by passing \"\n+                \"an `input_shape` or `batch_input_shape` \"\n+                \"argument to your first layer. If your \"\n+                \"first layer is an Embedding, you can \"\n+                \"also use the `input_length` argument.\\n\"\n+                \"- If using the functional API, specify \"\n+                \"the time dimension by passing a `shape` \"\n+                \"or `batch_shape` argument to your Input layer.\"\n+            )\n+\n+        cell_kwargs = {}\n+        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg():\n+            cell_kwargs[\"training\"] = training\n+        if initial_state is None:\n+            if self.stateful:\n+                initial_state = self.states\n+            else:\n+                initial_state = self.get_initial_state(sequence)\n+        else:\n+            # RNN expect the states in a list, even if single state.\n+            if not nest.is_nested(initial_state):\n+                initial_state = [initial_state]\n+            initial_state = list(initial_state)\n+\n+        def step(inputs, states):\n+            output, new_states = self.cell(inputs, states, **cell_kwargs)\n+            if not nest.is_nested(new_states):\n+                new_states = [new_states]\n+            return output, new_states\n+\n+        last_output, outputs, states = backend.nn.rnn(\n+            step,\n+            sequence,\n+            initial_state,\n+            go_backwards=self.go_backwards,\n+            mask=mask,\n+            unroll=self.unroll,\n+            input_length=timesteps,\n+            zero_output_for_mask=self.zero_output_for_mask,\n+            return_all_outputs=self.return_sequences,\n+        )\n+        assert isinstance(states, (list, tuple))\n+\n+        if self.stateful:\n+            for self_state, state in zip(\n+                nest.flatten(self.states), nest.flatten(states)\n+            ):\n+                self_state.assign(state)\n+\n+        if self.return_sequences:\n+            output = outputs\n+        else:\n+            output = last_output\n+\n+        if self.return_state:\n+            if len(states) == 1:\n+                state = states[0]\n+                return output, state\n+            return output, *states\n+        return output\n+\n+    def _maybe_reset_cell_dropout_mask(self, cell):\n+        if isinstance(cell, DropoutRNNCellMixin):\n+            cell.reset_dropout_mask()\n+            cell.reset_recurrent_dropout_mask()\n+        if isinstance(self.cell, StackedRNNCells):\n+            for cell in self.cell.cells:\n+                self._maybe_reset_cell_dropout_mask(cell)\n+\n+    def get_config(self):\n+        config = {\n+            \"return_sequences\": self.return_sequences,\n+            \"return_state\": self.return_state,\n+            \"go_backwards\": self.go_backwards,\n+            \"stateful\": self.stateful,\n+            \"unroll\": self.unroll,\n+            \"zero_output_for_mask\": self.zero_output_for_mask,\n+        }\n+        config[\"cell\"] = serialization_lib.serialize_keras_object(self.cell)\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        cell = serialization_lib.deserialize_keras_object(\n+            config.pop(\"cell\"), custom_objects=custom_objects\n+        )\n+        layer = cls(cell, **config)\n+        return layer\n\n@@ -0,0 +1,239 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class OneStateRNNCell(layers.Layer):\n+    def __init__(self, units, **kwargs):\n+        self.units = units\n+        self.state_size = units\n+        super().__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        self.kernel = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"ones\",\n+            name=\"kernel\",\n+        )\n+        self.recurrent_kernel = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel\",\n+        )\n+        self.built = True\n+\n+    def call(self, inputs, states):\n+        prev_output = states[0]\n+        h = ops.matmul(inputs, self.kernel)\n+        output = h + ops.matmul(prev_output, self.recurrent_kernel)\n+        return output, [output]\n+\n+\n+class TwoStatesRNNCell(layers.Layer):\n+    def __init__(self, units, **kwargs):\n+        self.units = units\n+        self.state_size = [units, units]\n+        self.output_size = units\n+        super().__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        self.kernel = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"ones\",\n+            name=\"kernel\",\n+        )\n+        self.recurrent_kernel_1 = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel_1\",\n+        )\n+        self.recurrent_kernel_2 = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel_2\",\n+        )\n+        self.built = True\n+\n+    def call(self, inputs, states):\n+        prev_1 = states[0]\n+        prev_2 = states[0]\n+        h = ops.matmul(inputs, self.kernel)\n+        output_1 = h + ops.matmul(prev_1, self.recurrent_kernel_1)\n+        output_2 = h + ops.matmul(prev_2, self.recurrent_kernel_2)\n+        output = output_1 + output_2\n+        return output, [output_1, output_2]\n+\n+\n+class RNNTest(testing.TestCase):\n+    def test_compute_output_shape_single_state(self):\n+        sequence = np.ones((3, 4, 5))\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 8))\n+\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 4, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 4, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+\n+    def test_compute_output_shape_two_states(self):\n+        sequence = np.ones((3, 4, 5))\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 8))\n+\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 4, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 8))\n+        self.assertEqual(output_shape[1][0], (3, 8))\n+        self.assertEqual(output_shape[1][1], (3, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 4, 8))\n+        self.assertEqual(output_shape[1][0], (3, 8))\n+        self.assertEqual(output_shape[1][1], (3, 8))\n+\n+    def test_forward_pass_single_state(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), output)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=True)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[[3.0, 3.0], [9.0, 9.0]]]), output)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=True, return_state=True\n+        )\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[[3.0, 3.0], [9.0, 9.0]]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state)\n+\n+    def test_forward_pass_two_states(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[18.0, 18.0]]), output)\n+\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=True)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[[6.0, 6.0], [18.0, 18.0]]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state1, state2 = layer(sequence)\n+        self.assertAllClose(np.array([[18.0, 18.0]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state1)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state2)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=True, return_state=True\n+        )\n+        output, state1, state2 = layer(sequence)\n+        self.assertAllClose(np.array([[[6.0, 6.0], [18.0, 18.0]]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state1)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state2)\n+\n+    def test_passing_initial_state_single_state(self):\n+        sequence = np.ones((2, 3, 2))\n+        state = np.ones((2, 2))\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        output = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), output)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), output)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state)\n+\n+    def test_passing_initial_state_two_states(self):\n+        sequence = np.ones((2, 3, 2))\n+        state = [np.ones((2, 2)), np.ones((2, 2))]\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        output = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[44.0, 44.0], [44.0, 44.0]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state_1, state_2 = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[44.0, 44.0], [44.0, 44.0]]), output)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state_1)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state_2)\n+\n+    def test_statefulness_single_state(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(OneStateRNNCell(2), stateful=True)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), output)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), stateful=True, return_state=True)\n+        layer(sequence)\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), output)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state)\n+\n+    def test_statefulness_two_states(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(TwoStatesRNNCell(2), stateful=True)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[90.0, 90.0]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), stateful=True, return_state=True\n+        )\n+        layer(sequence)\n+        output, state_1, state_2 = layer(sequence)\n+        self.assertAllClose(np.array([[90.0, 90.0]]), output)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state_1)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state_2)\n+\n+    def test_serialization(self):\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        self.run_class_serialization_test(layer)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        self.run_class_serialization_test(layer)\n+\n+    # def test_dropout(self):\n+    #     pass\n+\n+    # def test_cell_stacking(self):\n+    #     pass\n\n@@ -831,12 +831,9 @@ class ConvTranspose(Operation):\n         )\n \n     def compute_output_spec(self, inputs, kernel):\n-        kernel_size = kernel.shape[:-2]\n-        filters = kernel.shape[-2]\n         output_shape = compute_conv_transpose_output_shape(\n-            inputs.shape,\n-            kernel_size,\n-            filters,\n+            inputs,\n+            kernel,\n             self.strides,\n             self.padding,\n             self.output_padding,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
