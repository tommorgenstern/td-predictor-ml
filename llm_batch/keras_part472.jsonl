{"custom_id": "keras#3c5ee5224f035773b8b79e7f744631cdfa6f8863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 891 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,7 @@ def export_saved_model(\n     The exported `SavedModel` is a standalone serialization of Tensorflow\n     objects, and is supported by TF language APIs and the Tensorflow Serving\n     system.  To load the model, use the function\n-    `tf.keras.experimental.load_from_saved_model`.\n+    `tf.compat.v1.keras.experimental.load_from_saved_model`.\n \n     The `SavedModel` contains:\n \n@@ -87,10 +87,10 @@ def export_saved_model(\n \n     # Save the tf.keras model in the SavedModel format.\n     path = '/tmp/simple_keras_model'\n-    tf.keras.experimental.export_saved_model(model, path)\n+    tf.compat.v1.keras.experimental.export_saved_model(model, path)\n \n     # Load the saved keras model back.\n-    new_model = tf.keras.experimental.load_from_saved_model(path)\n+    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)\n     new_model.summary()\n     ```\n \n@@ -437,10 +437,10 @@ def load_from_saved_model(saved_model_path, custom_objects=None):\n \n     # Save the tf.keras model in the SavedModel format.\n     path = '/tmp/simple_keras_model'\n-    tf.keras.experimental.export_saved_model(model, path)\n+    tf.compat.v1.keras.experimental.export_saved_model(model, path)\n \n     # Load the saved keras model back.\n-    new_model = tf.keras.experimental.load_from_saved_model(path)\n+    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)\n     new_model.summary()\n     ```\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50700de3df4dd8add26857024e5587afcb6c37c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 756 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras import layers\n+from keras import models\n from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import layout_map as layout_map_lib\n from keras.utils import tf_utils\n@@ -137,7 +138,7 @@ class LayoutMapTest(test_util.DTensorBaseTest):\n \n \n # Class used for testing.\n-class SubclassModel(tf.keras.Model):\n+class SubclassModel(models.Model):\n     def __init__(self, name=None):\n         super().__init__(name=name)\n         self.d1 = layers.Dense(1000)\n@@ -217,12 +218,12 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         layout_map[\"d2.bias\"] = self.layout_1d\n \n         with layout_map_lib.layout_map_scope(layout_map):\n-            inputs = tf.keras.Input((10,), batch_size=10)\n+            inputs = layers.Input((10,), batch_size=10)\n             x = layers.Dense(20, name=\"d1\")(inputs)\n             x = layers.Dropout(0.1)(x)\n             output = layers.Dense(30, name=\"d2\")(x)\n \n-            model = tf.keras.Model(inputs, output)\n+            model = models.Model(inputs, output)\n \n         # It includes input layer as well.\n         self.assertLen(model.layers, 4)\n@@ -262,7 +263,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         layout_map[\"d2.bias\"] = self.layout_1d\n \n         with layout_map_lib.layout_map_scope(layout_map):\n-            model = tf.keras.Sequential(\n+            model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n                     layers.Dropout(0.1),\n@@ -300,7 +301,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         # all replicated.\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n         with layout_map_lib.layout_map_scope(layout_map):\n-            model = tf.keras.Sequential(\n+            model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n                     layers.Dropout(0.1),\n@@ -320,7 +321,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n     def test_weight_regularization(self):\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n         with layout_map_lib.layout_map_scope(layout_map):\n-            model = tf.keras.Sequential(\n+            model = models.Sequential(\n                 [\n                     layers.Dense(\n                         20,\n@@ -348,7 +349,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n     def test_dvariable_name(self):\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n         with layout_map_lib.layout_map_scope(layout_map):\n-            model = tf.keras.Sequential(\n+            model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n                     layers.Dropout(0.1),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11d062107ee8c4f56600c7493468e204f343abc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 29 | Files Changed: 11 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 52 | Churn Cumulative: 14001 | Contributors (this commit): 13 | Commits (past 90d): 86 | Contributors (cumulative): 80 | DMM Complexity: None\n\nDIFF:\n@@ -23,9 +23,7 @@ from tensorflow.python.framework import ops\n from tensorflow.python.ops import gen_resource_variable_ops\n from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.ops import variable_scope\n-from tensorflow.python.training.tracking import (\n-    base as trackable,\n-)\n+from tensorflow.python.trackable import base as trackable\n from tensorflow.python.util import compat\n from tensorflow.python.util import tf_contextlib\n \n\n@@ -34,10 +34,10 @@ from keras.utils import layer_utils\n from keras.utils import tf_utils\n \n # isort: off\n-from tensorflow.python.framework import extension_type\n-from tensorflow.python.training.tracking.util import (\n+from tensorflow.python.checkpoint.checkpoint import (\n     Checkpoint,\n )\n+from tensorflow.python.framework import extension_type\n \n \n class NetworkConstructionTest(test_combinations.TestCase):\n\n@@ -35,8 +35,8 @@ from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n # isort: off\n-from tensorflow.python.training.tracking import (\n-    util as trackable_util,\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_util,\n )\n \n # Used for nested input/output/state RNN test.\n\n@@ -30,12 +30,12 @@ from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n # isort: off\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_util,\n+)\n from tensorflow.python.framework import (\n     test_util as tf_test_util,\n )\n-from tensorflow.python.training.tracking import (\n-    util as trackable_util,\n-)\n \n \n class _RNNCellWithConstants(keras.layers.Layer):\n\n@@ -24,8 +24,8 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n # isort: off\n-from tensorflow.python.training.tracking import (\n-    util as trackable_util,\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_util,\n )\n \n \n\n@@ -30,9 +30,7 @@ from keras.tests import model_subclassing_test_util as model_util\n from tensorflow.python.framework import (\n     test_util as tf_test_utils,\n )\n-from tensorflow.python.training.tracking import (\n-    data_structures,\n-)\n+from tensorflow.python.trackable import data_structures\n \n try:\n     import h5py\n\n@@ -23,8 +23,8 @@ from keras.engine import training\n from keras.layers import core\n \n # isort: off\n-from tensorflow.python.training.tracking import (\n-    util as trackable_utils,\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_utils,\n )\n \n \n\n@@ -26,10 +26,8 @@ from keras.layers.normalization import batch_normalization_v1\n from keras.testing_infra import test_combinations\n \n # isort: off\n-from tensorflow.python.training.tracking import (\n-    data_structures,\n-)\n-from tensorflow.python.training.tracking import util\n+from tensorflow.python.trackable import data_structures\n+from tensorflow.python.checkpoint import checkpoint as util\n \n \n class HasList(training.Model):\n\n@@ -29,14 +29,14 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n # isort: off\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_utils,\n+)\n from tensorflow.python.eager import context\n from tensorflow.python.framework import (\n     test_util as tf_test_utils,\n )\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import (\n-    util as trackable_utils,\n-)\n \n \n class MyModel(training.Model):\n\n@@ -25,13 +25,13 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n # isort: off\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_utils,\n+)\n from tensorflow.python.eager import context\n from tensorflow.python.framework import (\n     test_util as tf_test_utils,\n )\n-from tensorflow.python.training.tracking import (\n-    util as trackable_utils,\n-)\n \n \n class NonLayerTrackable(tf.Module):\n\n@@ -21,8 +21,8 @@ from keras.optimizers.optimizer_v2 import adam\n \n # isort: off\n from tensorflow.compiler.tests import xla_test\n-from tensorflow.python.training.tracking import (\n-    util as trackable_utils,\n+from tensorflow.python.checkpoint import (\n+    checkpoint as trackable_utils,\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#846719986bd3e54198f5b4c172f11e19ff5251b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 66 | Churn Cumulative: 22703 | Contributors (this commit): 135 | Commits (past 90d): 42 | Contributors (cumulative): 150 | DMM Complexity: 1.0\n\nDIFF:\n@@ -776,7 +776,9 @@ class Functional(training_lib.Model):\n         return tensor\n \n     def get_config(self):\n-        return copy.deepcopy(get_network_config(self))\n+        # Continue adding configs into what the super class has added.\n+        config = super().get_config()\n+        return copy.deepcopy(get_network_config(self, config=config))\n \n     def _validate_graph_inputs_and_outputs(self):\n         \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"\n@@ -1500,12 +1502,14 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n     return input_tensors, output_tensors, created_layers\n \n \n-def get_network_config(network, serialize_layer_fn=None):\n+def get_network_config(network, serialize_layer_fn=None, config=None):\n     \"\"\"Builds the config, which consists of the node graph and serialized layers.\n \n     Args:\n       network: A Network object.\n       serialize_layer_fn: Function used to serialize layers.\n+      config: A dict to append more config entries into. If None, start with a\n+          new dict for the config.\n \n     Returns:\n       Config dictionary.\n@@ -1513,9 +1517,8 @@ def get_network_config(network, serialize_layer_fn=None):\n     serialize_layer_fn = (\n         serialize_layer_fn or generic_utils.serialize_keras_object\n     )\n-    config = {\n-        \"name\": network.name,\n-    }\n+    config = config or {}\n+    config[\"name\"] = network.name\n     node_conversion_map = {}\n     for layer in network.layers:\n         kept_nodes = 1 if _should_skip_first_node(layer) else 0\n\n@@ -3005,6 +3005,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n+\n+        # Grab the optimizer and loss from the `config` for `compile()` and\n+        # `build()`.\n+        optimizer, loss = None, None\n+        optimizer_dict = config.pop(\"optimizer\", {})\n+        if optimizer_dict:\n+            optimizer = saving_lib.deserialize_keras_object(optimizer_dict)\n+        loss_dict = config.pop(\"loss\", {})\n+        if loss_dict:\n+            loss = saving_lib.deserialize_keras_object(loss_dict)\n+        input_shape = config.pop(\"input_shape\", {})\n+\n         # `from_config` assumes `cls` is either `Functional` or a child class of\n         # `Functional`. In the case that `cls` is meant to behave like a child\n         # class of `Functional` but only inherits from the `Model` class, we\n@@ -3026,25 +3038,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     inputs=inputs, outputs=outputs, name=config.get(\"name\")\n                 )\n                 functional.connect_ancillary_layers(model, layers)\n-                return model\n \n+            else:\n                 # The config does not contain all the information necessary to\n                 # revive a Functional model. This happens when the user creates\n-            # subclassed models where `get_config()` is returning insufficient\n-            # information to be considered a Functional model. In this case, we\n-            # fall back to provide all config into the constructor of the class.\n-            optimizer, loss = None, None\n-\n-            optimizer_dict = config.pop(\"optimizer\", {})\n-            if optimizer_dict:\n-                optimizer = saving_lib.deserialize_keras_object(optimizer_dict)\n-\n-            loss_dict = config.pop(\"loss\", {})\n-            if loss_dict:\n-                loss = saving_lib.deserialize_keras_object(loss_dict)\n-\n-            input_shape = config.pop(\"input_shape\", {})\n-\n+                # subclassed models where `get_config()` is returning\n+                # insufficient information to be considered a Functional model.\n+                # In this case, we fall back to provide all config into the\n+                # constructor of the class.\n                 try:\n                     model = cls(**config)\n                 except TypeError as e:\n\n@@ -18,6 +18,7 @@ import sys\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n \n import keras\n from keras import backend\n@@ -75,7 +76,7 @@ def my_mean_squared_error(y_true, y_pred):\n module_my_mean_squared_error = my_mean_squared_error\n \n \n-class NewSavingTest(tf.test.TestCase):\n+class NewSavingTest(tf.test.TestCase, parameterized.TestCase):\n     def setUp(self):\n         super().setUp()\n         saving_lib._ENABLED = True\n@@ -264,7 +265,12 @@ class NewSavingTest(tf.test.TestCase):\n             config_dict[\"config\"][\"loss\"][\"class_name\"], \"LossesContainer\"\n         )\n \n-    def test_functional_model_with_tf_op_lambda_layer(self):\n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            layer=[\"tf_op_lambda\", \"lambda\"],\n+        )\n+    )\n+    def test_functional_model_with_tf_op_lambda_layer(self, layer):\n         class ToString:\n             def __init__(self):\n                 self.contents = \"\"\n@@ -274,9 +280,17 @@ class NewSavingTest(tf.test.TestCase):\n \n         temp_dir = os.path.join(self.get_temp_dir(), \"my_model\")\n \n+        if layer == \"lambda\":\n+            func = tf.function(lambda x: tf.math.cos(x) + tf.math.sin(x))\n+            inputs = keras.layers.Input(shape=(32,))\n+            outputs = keras.layers.Dense(1)(inputs)\n+            outputs = keras.layers.Lambda(func._python_function)(outputs)\n+\n+        elif layer == \"tf_op_lambda\":\n             inputs = keras.layers.Input(shape=(32,))\n             outputs = keras.layers.Dense(1)(inputs)\n             outputs = outputs + inputs\n+\n         functional_model = keras.Model(inputs, outputs)\n         functional_to_string = ToString()\n         functional_model.summary(print_fn=functional_to_string)\n@@ -287,9 +301,11 @@ class NewSavingTest(tf.test.TestCase):\n         functional_model.fit(x, y, epochs=3)\n         functional_model._save_new(temp_dir)\n         loaded_model = saving_lib.load(temp_dir)\n+        loaded_model.fit(x, y, epochs=3)\n         loaded_to_string = ToString()\n         loaded_model.summary(print_fn=loaded_to_string)\n \n+        # Confirming the original and saved/loaded model have same structure.\n         self.assertEqual(\n             functional_to_string.contents, loaded_to_string.contents\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57e923ed974720b4c104d4e4089fdb81276b2f6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 28 | Churn Cumulative: 7739 | Contributors (this commit): 19 | Commits (past 90d): 25 | Contributors (cumulative): 35 | DMM Complexity: 0.75\n\nDIFF:\n@@ -28,6 +28,7 @@ from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.engine import sequential\n from keras.engine import training as training_lib\n+from keras.saving import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import layer_utils\n@@ -1835,6 +1836,28 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         self.assertLen(config[\"input_layers\"], 1)\n         self.assertLen(config[\"output_layers\"], 1)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    @test_utils.run_v2_only\n+    def test_save_load_with_single_elem_list_inputs(self):\n+        class MyLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self._preserve_input_structure_in_config = True\n+\n+            def call(self, inputs):\n+                return inputs[0]\n+\n+        inputs = input_layer_lib.Input(shape=(3,))\n+        layer = MyLayer()\n+        outputs = layer([inputs])\n+\n+        model = training_lib.Model(inputs=inputs, outputs=outputs)\n+        model.save(\"/tmp/km2\")\n+\n+        save.load_model(\"/tmp/km2\")\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n\n@@ -49,6 +49,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n             batch_input_shape=getattr(self.obj, \"_batch_input_shape\", None),\n             stateful=self.obj.stateful,\n             must_restore_from_config=self.obj._must_restore_from_config,\n+            preserve_input_structure_in_config=self.obj._preserve_input_structure_in_config,  # noqa: E501\n         )\n \n         metadata.update(get_serialized(self.obj))\n\n@@ -1187,6 +1187,10 @@ class RevivedLayer:\n                 revived_obj._is_feature_layer = metadata[\"_is_feature_layer\"]\n             if metadata.get(\"stateful\") is not None:\n                 revived_obj.stateful = metadata[\"stateful\"]\n+            if metadata.get(\"preserve_input_structure_in_config\") is not None:\n+                revived_obj._preserve_input_structure_in_config = metadata[\n+                    \"preserve_input_structure_in_config\"\n+                ]\n \n         return revived_obj, _revive_setter\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8024bd3bd6da31adcbcfda28dacfc66f74e1fb20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 47 | Churn Cumulative: 5834 | Contributors (this commit): 45 | Commits (past 90d): 13 | Contributors (cumulative): 52 | DMM Complexity: 0.7368421052631579\n\nDIFF:\n@@ -47,6 +47,8 @@ _SKIP_FAILED_SERIALIZATION = False\n # If a layer does not have a defined config, then the returned config will be a\n # dictionary with the below key.\n _LAYER_UNDEFINED_CONFIG_KEY = \"layer was saved without config\"\n+# Thread-local custom objects set by custom_object_scope.\n+_THREAD_LOCAL_CUSTOM_OBJECTS = threading.local()\n \n \n @keras_export(\n@@ -84,23 +86,23 @@ class CustomObjectScope:\n         self.backup = None\n \n     def __enter__(self):\n-        self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()\n+        self.backup = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.copy()\n         for objects in self.custom_objects:\n-            _GLOBAL_CUSTOM_OBJECTS.update(objects)\n+            _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(objects)\n         return self\n \n     def __exit__(self, *args, **kwargs):\n-        _GLOBAL_CUSTOM_OBJECTS.clear()\n-        _GLOBAL_CUSTOM_OBJECTS.update(self.backup)\n+        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.clear()\n+        _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.update(self.backup)\n \n \n @keras_export(\"keras.utils.get_custom_objects\")\n def get_custom_objects():\n     \"\"\"Retrieves a live reference to the global dictionary of custom objects.\n \n-    Updating and clearing custom objects using `custom_object_scope`\n-    is preferred, but `get_custom_objects` can\n-    be used to directly access the current collection of custom objects.\n+    Custom objects set using using `custom_object_scope` are not added to the\n+    global dictionary of custom objects, and will not appear in the returned\n+    dictionary.\n \n     Example:\n \n@@ -479,7 +481,9 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n       An instantiable class associated with 'name', or None if no such class\n         exists.\n     \"\"\"\n-    if name in _GLOBAL_CUSTOM_OBJECTS:\n+    if name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n+        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[name]\n+    elif name in _GLOBAL_CUSTOM_OBJECTS:\n         return _GLOBAL_CUSTOM_OBJECTS[name]\n     elif custom_objects and name in custom_objects:\n         return custom_objects[name]\n@@ -569,7 +573,9 @@ def serialize_keras_object(instance):\n \n def get_custom_objects_by_name(item, custom_objects=None):\n     \"\"\"Returns the item if it is in either local or global custom objects.\"\"\"\n-    if item in _GLOBAL_CUSTOM_OBJECTS:\n+    if item in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n+        return _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[item]\n+    elif item in _GLOBAL_CUSTOM_OBJECTS:\n         return _GLOBAL_CUSTOM_OBJECTS[item]\n     elif custom_objects and item in custom_objects:\n         return custom_objects[item]\n@@ -729,6 +735,7 @@ def deserialize_keras_object(\n                     cls_config,\n                     custom_objects=dict(\n                         list(_GLOBAL_CUSTOM_OBJECTS.items())\n+                        + list(_THREAD_LOCAL_CUSTOM_OBJECTS.__dict__.items())\n                         + list(custom_objects.items())\n                     ),\n                 )\n@@ -752,6 +759,8 @@ def deserialize_keras_object(\n         object_name = identifier\n         if custom_objects and object_name in custom_objects:\n             obj = custom_objects.get(object_name)\n+        elif object_name in _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__:\n+            obj = _THREAD_LOCAL_CUSTOM_OBJECTS.__dict__[object_name]\n         elif object_name in _GLOBAL_CUSTOM_OBJECTS:\n             obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\n         else:\n\n@@ -88,13 +88,25 @@ class TestCustomObjectScope(tf.test.TestCase):\n         class CustomClass:\n             pass\n \n+        def check_get_in_thread():\n             with keras.utils.generic_utils.custom_object_scope(\n                 {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n             ):\n-            act = keras.activations.get(\"custom_fn\")\n-            self.assertEqual(act, custom_fn)\n-            cl = keras.regularizers.get(\"CustomClass\")\n-            self.assertEqual(cl.__class__, CustomClass)\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+\n+            with keras.utils.generic_utils.custom_object_scope(\n+                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n+            ):\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+                checked_thread = self.checkedThread(check_get_in_thread)\n+                checked_thread.start()\n+                checked_thread.join()\n \n \n class SerializeKerasObjectTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75811c3dc99b54be9144aed75bb1d46b7ef6d36f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10497 | Contributors (this commit): 31 | Commits (past 90d): 17 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -763,7 +763,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         Returns:\n             Python dictionary.\n         \"\"\"\n-        all_args = tf_inspect.getfullargspec(self.__init__).args\n+        all_args = tf_inspect.getfullargspec(self.__init__).args[1:]\n         config = {\n             \"name\": self.name,\n             \"trainable\": self.trainable,\n@@ -782,7 +782,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         extra_args = [arg for arg in all_args if arg not in expected_args]\n         # Check that either the only argument in the `__init__` is  `self`,\n         # or that `get_config` has been overridden:\n-        if len(extra_args) > 1 and hasattr(self.get_config, \"_is_default\"):\n+        if extra_args and hasattr(self.get_config, \"_is_default\"):\n             raise NotImplementedError(\n                 textwrap.dedent(\n                     f\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3d781553b8015d2cae023d584eeba2f314abaeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 831 | Contributors (this commit): 21 | Commits (past 90d): 7 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,7 @@ def load_data(\n     common words, but eliminate the top 20 most common words\".\n \n     As a convention, \"0\" does not stand for a specific word, but instead is used\n-    to encode any unknown word.\n+    to encode the pad token.\n \n     Args:\n       path: where to cache the data (relative to `~/.keras/dataset`).\n@@ -181,12 +181,24 @@ def get_word_index(path=\"imdb_word_index.json\"):\n     Example:\n \n     ```python\n+    # Use the default parameters to keras.datasets.imdb.load_data\n+    start_char = 1\n+    oov_char = 2\n+    index_from = 3\n     # Retrieve the training sequences.\n-    (x_train, _), _ = keras.datasets.imdb.load_data()\n+    (x_train, _), _ = keras.datasets.imdb.load_data(\n+        start_char=start_char, oov_char=oov_char, index_from=index_from\n+    )\n     # Retrieve the word index file mapping words to indices\n     word_index = keras.datasets.imdb.get_word_index()\n     # Reverse the word index to obtain a dict mapping indices to words\n-    inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n+    # And add `index_from` to indices to sync with `x_train`\n+    inverted_word_index = dict(\n+        (i + index_from, word) for (word, i) in word_index.items()\n+    )\n+    # Update `inverted_word_index` to include `start_char` and `oov_char`\n+    inverted_word_index[start_char] = \"[START]\"\n+    inverted_word_index[oov_char] = \"[OOV]\"\n     # Decode the first sequence in the dataset\n     decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n     ```\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c380d05e2675efca8b2bbb01a395ef2f5b4f0a6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 490 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -499,8 +499,8 @@ class MultiHeadAttention(Layer):\n \n         Args:\n           query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n-          key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n-          value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n+          key: Projected key `Tensor` of shape `(B, S, N, key_dim)`.\n+          value: Projected value `Tensor` of shape `(B, S, N, value_dim)`.\n           attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n             attention to certain positions.\n           training: Python boolean indicating whether the layer should behave in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80a394698ae159347cae7c57871504fcc004e754", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 103 | Churn Cumulative: 1947 | Contributors (this commit): 7 | Commits (past 90d): 24 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,7 @@ from keras.dtensor import utils\n from keras.engine import base_layer\n \n # isort: off\n+from tensorflow.python.util.deprecation import deprecated\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -144,11 +145,101 @@ class LayoutMap(collections.abc.MutableMapping):\n         \"\"\"\n         return self._default_mesh\n \n+    def scope(self):\n+        \"\"\"Apply layout to all `tf.Variable` instances created under the scope.\n+\n+        All `tf.Variable` instances created under this scope\n+        will be lazily initialized first. Once they are attached as the model\n+        or layer attributes, and there is a stable layout mapping for it, the\n+        variables will be reinitialized into a\n+        `tf.experimental.dtensor.DVariable` with corresponding layout.\n+\n+        Note that the layout mapping will use object/attribute names as the\n+        keys to map the variable to the layout.\n+\n+        For subclassed models, the full object/attribute name is used as the\n+        key. For Functional/Sequential models, we use `layer.name` as\n+        the key for the layer, followed by the attribute name. Keras ensures\n+        name uniqueness among the layers within a Functional/Sequential model.\n+\n+        See the following examples that show variable object names\n+        for different Keras model types:\n+\n+        ```python\n+        layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n+        layout_map['d1.kernel'] = layout_1\n+        layout_map['d1.bias'] = layout_2\n+        layout_map['d2.kernel'] = layout_3\n+        layout_map['d2.bias'] = layout_4\n+\n+        ## Subclassed model\n+        class SubclassModel(tf.keras.Model):\n+\n+          def __init__(self, name=None):\n+            super().__init__(name=name)\n+            self.d1 = tf.keras.layers.Dense(1000)\n+            self.d2 = tf.keras.layers.Dense(1000)\n+\n+          def call(self, inputs):\n+            x = self.d1(inputs)\n+            return self.d2(x)\n+\n+        with layout_map.scope():\n+          model = SubclassModel()\n+        inputs = tf.zeros((10, 10))\n+        results = model(inputs)\n+\n+        model.d1.kernel.layout == layout_1\n+        model.d1.bias.layout == layout_2\n+        model.d2.kernel.layout == layout_3\n+        model.d2.bias.layout == layout_4\n+\n+        ## Functional model\n+        with layout_map.scope():\n+          inputs = tf.keras.Input((10,), batch_size=10)\n+          x = tf.keras.layers.Dense(20, name='d1')(inputs)\n+          output = tf.keras.layers.Dense(30, name='d2')(x)\n+\n+          model = tf.keras.Model(inputs, output)\n+\n+        d1 = model.layers[1]\n+        d2 = model.layers[2]\n+\n+        d1.kernel.layout == layout_1\n+        d1.bias.layout == layout_2\n+        d1.kernel.layout == layout_3\n+        d1.bias.layout == layout_4\n+\n+        ## Sequential model\n+        with layout_map.scope():\n+          model = tf.keras.Sequential([\n+              tf.keras.layers.Dense(20, name='d1', input_shape=(10,)),\n+              tf.keras.layers.Dense(30, name='d2')\n+          ])\n+\n+        d1 = model.layers[0]\n+        d2 = model.layers[1]\n+\n+        d1.kernel.layout == layout_1\n+        d1.bias.layout == layout_2\n+        d1.kernel.layout == layout_3\n+        d1.bias.layout == layout_4\n+        ```\n+\n+        Returns:\n+          A context that will lazily initialize all `tf.Variable` objects\n+          within the model, with their attributed layouts.\n+        \"\"\"\n+        return layout_map_scope(self)\n+\n \n LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n \n \n @keras_export(\"keras.dtensor.experimental.layout_map_scope\", v1=[])\n+@deprecated(\n+    None, \"use tf.keras.dtensor.experimental.LayoutMap.scope() instead.\"\n+)\n @contextlib.contextmanager\n def layout_map_scope(layout_map):\n     \"\"\"Apply the layout to all the tf.Variables created under the scope.\n\n@@ -195,7 +195,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         layout_map[\"d2.kernel\"] = self.layout_2d\n         layout_map[\"d2.bias\"] = self.layout_1d\n \n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = SubclassModel(name=\"model\")\n \n         # Init the model with eager tensor, make sure the model weights have\n@@ -248,7 +248,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         layout_map[\"d2.kernel\"] = self.layout_2d\n         layout_map[\"d2.bias\"] = self.layout_1d\n \n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             inputs = layers.Input((10,), batch_size=10)\n             x = layers.Dense(20, name=\"d1\")(inputs)\n             x = layers.Dropout(0.1)(x)\n@@ -304,7 +304,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         layout_map[\"d2.kernel\"] = self.layout_2d\n         layout_map[\"d2.bias\"] = self.layout_1d\n \n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n@@ -353,7 +353,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n         # Create empty layout map, which means all the weights just default to\n         # all replicated.\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n@@ -401,7 +401,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n     def test_dvariable_name(self):\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n@@ -416,7 +416,7 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n     def test_checkpoint(self):\n         layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n-        with layout_map_lib.layout_map_scope(layout_map):\n+        with layout_map.scope():\n             model = models.Sequential(\n                 [\n                     layers.Dense(20, name=\"d1\", input_shape=(10,)),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e9376b5b94b6fb445dd52dbfafbc4e95bff5e35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 5291 | Contributors (this commit): 11 | Commits (past 90d): 19 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,7 @@ class InputLayer(base_layer.Layer):\n     model = tf.keras.Sequential([\n       tf.keras.layers.InputLayer(input_shape=(4,)),\n       tf.keras.layers.Dense(8)])\n-    model.compile(tf.optimizers.RMSprop(0.001), loss='mse')\n+    model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')\n     model.fit(np.zeros((10, 4)),\n               np.ones((10, 8)))\n \n@@ -72,7 +72,7 @@ class InputLayer(base_layer.Layer):\n     # Keras will add a input for the model behind the scene.\n     model = tf.keras.Sequential([\n       tf.keras.layers.Dense(8, input_shape=(4,))])\n-    model.compile(tf.optimizers.RMSprop(0.001), loss='mse')\n+    model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')\n     model.fit(np.zeros((10, 4)),\n               np.ones((10, 8)))\n     ```\n\n@@ -1662,7 +1662,8 @@ def prepare_loss_functions(loss, output_names):\n \n     Args:\n         loss: String (name of objective function), objective function or\n-          `tf.losses.Loss` instance. See `tf.losses`. If the model has multiple\n+          `tf.keras.losses.Loss` instance. See `tf.keras.losses`.\n+          If the model has multiple\n           outputs, you can use a different loss on each output by passing a\n           dictionary or a list of losses. The loss value that will be minimized\n           by the model will then be the sum of all individual losses.\n\n@@ -72,7 +72,7 @@ class LegacyRNNTest(tf.test.TestCase):\n                 outputs.shape.as_list(), [None, timestep, output_shape]\n             )\n             self.assertEqual(state.shape.as_list(), [None, output_shape])\n-            loss = tf.losses.softmax_cross_entropy(predict, state)\n+            loss = tf.keras.losses.categorical_crossentropy(predict, state)\n             train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n             sess.run([tf.global_variables_initializer()])\n@@ -108,7 +108,7 @@ class LegacyRNNTest(tf.test.TestCase):\n                 outputs.shape.as_list(), [None, timestep, output_shape]\n             )\n             self.assertEqual(state.shape.as_list(), [None, output_shape])\n-            loss = tf.losses.softmax_cross_entropy(predict, state)\n+            loss = tf.keras.losses.categorical_crossentropy(predict, state)\n             train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n             sess.run([tf.global_variables_initializer()])\n@@ -146,7 +146,7 @@ class LegacyRNNTest(tf.test.TestCase):\n             self.assertEqual(len(state), 2)\n             self.assertEqual(state[0].shape.as_list(), [None, output_shape])\n             self.assertEqual(state[1].shape.as_list(), [None, output_shape])\n-            loss = tf.losses.softmax_cross_entropy(predict, state[0])\n+            loss = tf.keras.losses.categorical_crossentropy(predict, state[0])\n             train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n             sess.run([tf.global_variables_initializer()])\n@@ -195,7 +195,7 @@ class LegacyRNNTest(tf.test.TestCase):\n             self.assertEqual(state[1].shape.as_list(), [None, 2 * output_shape])\n             self.assertEqual(state[2].shape.as_list(), [None, output_shape])\n             self.assertEqual(state[3].shape.as_list(), [None, output_shape])\n-            loss = tf.losses.softmax_cross_entropy(predict, state[2])\n+            loss = tf.keras.losses.categorical_crossentropy(predict, state[2])\n             train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n             sess.run([tf.global_variables_initializer()])\n@@ -233,7 +233,7 @@ class LegacyRNNTest(tf.test.TestCase):\n             self.assertEqual(len(outputs), timestep)\n             self.assertEqual(outputs[0].shape.as_list(), [None, output_shape])\n             self.assertEqual(state.shape.as_list(), [None, output_shape])\n-            loss = tf.losses.softmax_cross_entropy(predict, state)\n+            loss = tf.keras.losses.categorical_crossentropy(predict, state)\n             train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n             sess.run([tf.global_variables_initializer()])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acfb52d1f3cde59e1d299d1fdad3d2118a62e2a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 1673 | Contributors (this commit): 13 | Commits (past 90d): 13 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -665,7 +665,7 @@ def keras_tensor_from_tensor(tensor):\n \n     out = keras_tensor_cls.from_tensor(tensor)\n \n-    if hasattr(tensor, \"_keras_mask\"):\n+    if getattr(tensor, \"_keras_mask\", None) is not None:\n         out._keras_mask = keras_tensor_from_tensor(tensor._keras_mask)\n     return out\n \n\n@@ -247,6 +247,17 @@ class KerasTensorTest(test_combinations.TestCase):\n         ):\n             kt.dtype\n \n+    def test_from_tensor_mask_tensor_is_none(self):\n+        tensor = tf.constant([1.0])\n+        kt = keras_tensor.keras_tensor_from_tensor(tensor)\n+        self.assertIsNone(getattr(kt, \"_keras_mask\", None))\n+\n+    def test_from_tensor_mask_tensor_is_not_none(self):\n+        tensor = tf.constant([1.0])\n+        tensor._keras_mask = tf.constant([1.0])\n+        kt = keras_tensor.keras_tensor_from_tensor(tensor)\n+        self.assertIsInstance(kt._keras_mask, keras_tensor.KerasTensor)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#091fc05c5653f826b8c179e897d95db507551e40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 9133 | Contributors (this commit): 107 | Commits (past 90d): 16 | Contributors (cumulative): 107 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -24,6 +24,9 @@ import os\n import re\n import sys\n import time\n+from typing import Iterable\n+from typing import Optional\n+from typing import Union\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -934,7 +937,7 @@ class BaseLogger(Callback):\n             All others will be averaged in `on_epoch_end`.\n     \"\"\"\n \n-    def __init__(self, stateful_metrics=None):\n+    def __init__(self, stateful_metrics: Optional[Iterable[str]] = None):\n         super().__init__()\n         self.stateful_metrics = set(stateful_metrics or [])\n \n@@ -1009,7 +1012,13 @@ class ProgbarLogger(Callback):\n         ValueError: In case of invalid `count_mode`.\n     \"\"\"\n \n-    def __init__(self, count_mode=\"samples\", stateful_metrics=None):\n+    def __init__(\n+        self,\n+        count_mode: str = \"samples\",\n+        stateful_metrics: Optional[Iterable[str]] = None,\n+    ):\n+        # when we drop support for python 3.7, replace 'count_mode: str'\n+        # with 'count_mode: Literal[\"samples\", \"steps\"]'\n         super().__init__()\n         self._supports_tf_logs = True\n         if count_mode == \"samples\":\n@@ -1318,15 +1327,17 @@ class ModelCheckpoint(Callback):\n \n     def __init__(\n         self,\n-        filepath,\n-        monitor=\"val_loss\",\n-        verbose=0,\n-        save_best_only=False,\n-        save_weights_only=False,\n-        mode=\"auto\",\n-        save_freq=\"epoch\",\n-        options=None,\n-        initial_value_threshold=None,\n+        filepath: Union[str, os.PathLike],\n+        monitor: str = \"val_loss\",\n+        verbose: int = 0,\n+        save_best_only: bool = False,\n+        save_weights_only: bool = False,\n+        mode: str = \"auto\",\n+        save_freq: Union[int, str] = \"epoch\",\n+        options: Union[\n+            tf.train.CheckpointOptions, tf.saved_model.SaveOptions, None\n+        ] = None,\n+        initial_value_threshold: Optional[float] = None,\n         **kwargs,\n     ):\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4f84d8da2de2938c1d6f74194c157ac979e5e0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 278 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -120,7 +120,7 @@ def image_dataset_from_directory(\n       interpolation: String, the interpolation method used when resizing images.\n         Defaults to `bilinear`. Supports `bilinear`, `nearest`, `bicubic`,\n         `area`, `lanczos3`, `lanczos5`, `gaussian`, `mitchellcubic`.\n-      follow_links: Whether to visits subdirectories pointed to by symlinks.\n+      follow_links: Whether to visit subdirectories pointed to by symlinks.\n           Defaults to False.\n       crop_to_aspect_ratio: If True, resize the images without aspect\n         ratio distortion. When the original aspect ratio differs from the target\n@@ -152,9 +152,9 @@ def image_dataset_from_directory(\n       - if `color_mode` is `grayscale`,\n         there's 1 channel in the image tensors.\n       - if `color_mode` is `rgb`,\n-        there are 3 channel in the image tensors.\n+        there are 3 channels in the image tensors.\n       - if `color_mode` is `rgba`,\n-        there are 4 channel in the image tensors.\n+        there are 4 channels in the image tensors.\n     \"\"\"\n     if \"smart_resize\" in kwargs:\n         crop_to_aspect_ratio = kwargs.pop(\"smart_resize\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2c41a7f9ca27bd23a82a7fc6d26189fac6f868d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 27131 | Contributors (this commit): 141 | Commits (past 90d): 52 | Contributors (cumulative): 163 | DMM Complexity: None\n\nDIFF:\n@@ -104,7 +104,7 @@ class Functional(training_lib.Model):\n \n     Note that the `backbone` and `activations` models are not\n     created with `keras.Input` objects, but with the tensors that are originated\n-    from `keras.Inputs` objects. Under the hood, the layers and weights will\n+    from `keras.Input` objects. Under the hood, the layers and weights will\n     be shared across these models, so that user can train the `full_model`, and\n     use `backbone` or `activations` to do feature extraction.\n     The inputs and outputs of the model can be nested structures of tensors as\n\n@@ -114,7 +114,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     Note that the `backbone` and `activations` models are not\n     created with `keras.Input` objects, but with the tensors that are originated\n-    from `keras.Inputs` objects. Under the hood, the layers and weights will\n+    from `keras.Input` objects. Under the hood, the layers and weights will\n     be shared across these models, so that user can train the `full_model`, and\n     use `backbone` or `activations` to do feature extraction.\n     The inputs and outputs of the model can be nested structures of tensors as\n\n@@ -769,7 +769,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n     def _lookup_dense(self, inputs):\n         \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n-        # When executing eagerly and tracing keras.Inputs, do not call lookup.\n+        # When executing eagerly and tracing keras.Input objects,\n+        # do not call lookup.\n         # This is critical for restoring SavedModel, which will first trace\n         # layer.call and then attempt to restore the table. We need the table to\n         # be uninitialized for the restore to work, but calling the table\n\n@@ -2091,5 +2091,5 @@ class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertAllClose(v1, v2)\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
