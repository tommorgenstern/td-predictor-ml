{"custom_id": "scrapy#18b6fecc474b11f4670d7140b5cc63c82cdbaf12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 29 | Churn Cumulative: 527 | Contributors (this commit): 3 | Commits (past 90d): 37 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -141,6 +141,7 @@ NEWSPIDER_MODULE = ''\n \n REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n+REDIRECT_PRIORITY_ADJUST = -2\n \n REQUESTS_QUEUE_SIZE = 0\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n@@ -148,6 +149,7 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n+RETRY_PRIORITY_ADJUST = +1\n \n ROBOTSTXT_OBEY = False\n \n\n@@ -10,6 +10,7 @@ class RedirectMiddleware(object):\n     def __init__(self):\n         self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n         self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n+        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n         domain = spider.domain_name\n@@ -41,6 +42,7 @@ class RedirectMiddleware(object):\n             redirected.meta['redirect_times'] = redirects\n             redirected.meta['redirect_ttl'] = ttl - 1\n             redirected.dont_filter = request.dont_filter\n+            redirected.priority = request.priority + self.priority_adjust\n             log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request),\n                     domain=spider.domain_name, level=log.DEBUG)\n             return redirected\n\n@@ -41,6 +41,7 @@ class RetryMiddleware(object):\n     def __init__(self):\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n         self.retry_http_codes = map(int, settings.getlist('RETRY_HTTP_CODES'))\n+        self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n         if response.status in self.retry_http_codes:\n@@ -61,6 +62,7 @@ class RetryMiddleware(object):\n             retryreq = request.copy()\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n+            retryreq.priority = request.priority + self.priority_adjust\n             return retryreq\n         else:\n             log.msg(\"Discarding %s (failed %d times): %s\" % (request, retries, reason),\n\n@@ -44,10 +44,6 @@ class ExecutionEngine(object):\n     process for that domain.\n     \"\"\"\n \n-    # Scheduler priority of redirected requests. Negative means high priority.\n-    # We use high priority to avoid hogging memory with pending redirected requests\n-    REDIRECTION_PRIORITY = -10\n-\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n@@ -323,10 +319,9 @@ class ExecutionEngine(object):\n                 log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n-                redirected = response # proper alias\n-                redirected.priority = self.REDIRECTION_PRIORITY\n-                schd = self.schedule(redirected, spider)\n-                chain_deferred(schd, redirected.deferred)\n+                newrequest = response # proper alias\n+                schd = self.schedule(newrequest, spider)\n+                chain_deferred(schd, newrequest.deferred)\n                 return schd\n \n         def _on_error(_failure):\n\n@@ -12,6 +12,12 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.spider = spiders.fromdomain('scrapytest.org')\n         self.mw = RedirectMiddleware()\n \n+    def test_priority_adjust(self):\n+        req = Request('http://a.com')\n+        rsp = Response('http://a.com', headers={'Location': 'http://a.com/redirected'}, status=301)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert req2.priority < req.priority\n+\n     def test_redirect_301(self):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n\n@@ -16,6 +16,12 @@ class RetryTest(unittest.TestCase):\n         self.mw = RetryMiddleware()\n         self.mw.max_retry_times = 2\n \n+    def test_priority_adjust(self):\n+        req = Request('http://www.scrapytest.org/503')\n+        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert req2.priority > req.priority\n+\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n         rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47970e91bc5a370ca58f1a1ee951ed100e0b864f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 363 | Contributors (this commit): 3 | Commits (past 90d): 28 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -141,7 +141,7 @@ NEWSPIDER_MODULE = ''\n \n REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n-REDIRECT_PRIORITY_ADJUST = -2\n+REDIRECT_PRIORITY_ADJUST = +2\n \n REQUESTS_QUEUE_SIZE = 0\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n@@ -149,7 +149,7 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n-RETRY_PRIORITY_ADJUST = +1\n+RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n \n\n@@ -54,7 +54,7 @@ class Scheduler(object):\n \n     def _enqueue_request(self, domain, request):\n         dfd = defer.Deferred()\n-        self.pending_requests[domain].push((request, dfd), request.priority)\n+        self.pending_requests[domain].push((request, dfd), -request.priority)\n         return dfd\n \n     def next_request(self, domain):\n\n@@ -16,7 +16,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         req = Request('http://a.com')\n         rsp = Response('http://a.com', headers={'Location': 'http://a.com/redirected'}, status=301)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert req2.priority < req.priority\n+        assert req2.priority > req.priority\n \n     def test_redirect_301(self):\n         url = 'http://www.example.com/301'\n\n@@ -20,7 +20,7 @@ class RetryTest(unittest.TestCase):\n         req = Request('http://www.scrapytest.org/503')\n         rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert req2.priority > req.priority\n+        assert req2.priority < req.priority\n \n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#adccd9a04ea3e1bfccbd42bce5b92002ecb95551", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 92 | Files Changed: 5 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): -2/9 | Churn Δ: 162 | Churn Cumulative: 304 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,7 +80,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n \n DOWNLOADER_STATS = True\n \n-DUPEFILTER_FILTERCLASS = 'scrapy.dupefilter.SimplePerDomainFilter'\n+DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n ENABLED_SPIDERS_FILE = ''\n \n\n@@ -0,0 +1,49 @@\n+\"\"\"\n+Dupe Filter classes implement a mechanism for filtering duplicate requests.\n+They must implement the following methods:\n+\n+* open_domain(domain)\n+  open a domain for tracking duplicates (typically used to reserve resources)\n+\n+* close_domain(domain)\n+  close a domain (typically used for freeing resources)\n+\n+* request_seen(domain, request, dont_record=False)\n+  return ``True`` if the request was seen before, or ``False`` otherwise. If\n+  ``dont_record`` is ``True`` the request must not be recorded as seen.\n+\n+\"\"\"\n+\n+from scrapy.utils.request import request_fingerprint\n+\n+\n+class NullDupeFilter(dict):\n+    def open_domain(self, domain):\n+        pass\n+\n+    def close_domain(self, domain):\n+        pass\n+\n+    def request_seen(self, domain, request, dont_record=False):\n+        return False\n+\n+\n+class RequestFingerprintDupeFilter(object):\n+    \"\"\"Duplicate filter using scrapy.utils.request.request_fingerprint\"\"\"\n+\n+    def __init__(self):\n+        self.fingerprints = {}\n+\n+    def open_domain(self, domain):\n+        self.fingerprints[domain] = set()\n+\n+    def close_domain(self, domain):\n+        del self.fingerprints[domain]\n+\n+    def request_seen(self, domain, request, dont_record=False):\n+        fp = request_fingerprint(request)\n+        if fp in self.fingerprints[domain]:\n+            return True\n+        if not dont_record:\n+            self.fingerprints[domain].add(fp)\n+        return False\n\n@@ -9,19 +9,19 @@ from scrapy.conf import settings\n class DuplicatesFilterMiddleware(object):\n     \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n     def __init__(self):\n-        clspath = settings.get('DUPEFILTER_FILTERCLASS')\n+        clspath = settings.get('DUPEFILTER_CLASS')\n         if not clspath:\n             raise NotConfigured\n \n         self.dupefilter = load_object(clspath)()\n \n     def enqueue_request(self, domain, request):\n-        added = self.dupefilter.add(domain, request)\n-        if not (added or request.dont_filter):\n-            raise IgnoreRequest('Skipped (already seen request)')\n+        seen = self.dupefilter.request_seen(domain, request)\n+        if seen and not request.dont_filter:\n+            raise IgnoreRequest('Skipped (request already seen)')\n \n     def open_domain(self, domain):\n-        self.dupefilter.open(domain)\n+        self.dupefilter.open_domain(domain)\n \n     def close_domain(self, domain):\n-        self.dupefilter.close(domain)\n+        self.dupefilter.close_domain(domain)\n\n@@ -1,66 +0,0 @@\n-from scrapy.utils.request import request_fingerprint\n-\n-\n-class BaseFilter(dict):\n-    \"\"\"Base class defining the duplicates requests filtering api\"\"\"\n-\n-    def open(self, domain):\n-        \"\"\"Called when a domain starts\"\"\"\n-        raise NotImplementedError()\n-\n-    def close(self, domain):\n-        \"\"\"Called when a domain is closed\"\"\"\n-        raise NotImplementedError()\n-\n-    def add(self, domain, request):\n-        \"\"\"Called to check if a request was already seen, and adds it to seen set.\n-\n-        returns True if not seen before, or False otherwise.\n-        \"\"\"\n-        raise NotImplementedError()\n-\n-    def has(self, domain, request):\n-        \"\"\"Called to check if a request was seen but doesnt add request to seen set.\"\"\"\n-        raise NotImplementedError()\n-\n-\n-class SimplePerDomainFilter(BaseFilter):\n-    \"\"\"Filter out a request if already seen for same domain\"\"\"\n-\n-    def open(self, domain):\n-        \"\"\"Initialize the resources needed for filtering for this domain\"\"\"\n-        self[domain] = set()\n-\n-    def close(self, domain):\n-        \"\"\"Remove the resources reserved for filtering for this domain\"\"\"\n-        del self[domain]\n-\n-    def add(self, domain, request):\n-        \"\"\"Add the fingerprint of a request to the domain set if a equivalent fingerprint has not been added.\n-        This method will return true if the fingerprint was added and false otherwise.\n-        \"\"\"\n-        fp = request_fingerprint(request)\n-        if fp not in self[domain]:\n-            self[domain].add(fp)\n-            return True\n-        return False\n-\n-    def has(self, domain, request):\n-        fp = request_fingerprint(request)\n-        return fp in self[domain]\n-\n-\n-class NullFilter(dict):\n-    def open(self, domain):\n-        pass\n-\n-    def close(self, domain):\n-        pass\n-\n-    def add(self, domain, request):\n-        return True\n-\n-    def has(self, domain, request):\n-        return None\n-\n-\n\n@@ -1,41 +1,36 @@\n import unittest\n \n from scrapy.http import Request\n-from scrapy.dupefilter import SimplePerDomainFilter, NullFilter\n+from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilter\n \n \n-class SimplePerDomainFilterTest(unittest.TestCase):\n+class RequestFingerprintDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n         domain = 'scrapytest.org'\n-        filter = SimplePerDomainFilter()\n-        filter.open(domain)\n-        assert domain in filter\n+        filter = RequestFingerprintDupeFilter()\n+        filter.open_domain(domain)\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        assert not filter.has(domain, r1)\n-        assert filter.add(domain, r1)\n-        assert filter.has(domain, r1)\n+        assert not filter.request_seen(domain, r1)\n+        assert filter.request_seen(domain, r1)\n \n-        assert filter.add(domain, r2)\n-        assert not filter.add(domain, r3)\n+        assert not filter.request_seen(domain, r2)\n+        assert filter.request_seen(domain, r3)\n \n-        filter.close(domain)\n-        assert domain not in filter\n+        filter.close_domain(domain)\n \n \n-class NullFilterTest(unittest.TestCase):\n+class NullDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n         domain = 'scrapytest.org'\n-        filter = NullFilter()\n-        filter.open(domain)\n+        filter = NullDupeFilter()\n+        filter.open_domain(domain)\n \n         r1 = Request('http://scrapytest.org/1')\n-        assert not filter.has(domain, r1)\n-        assert filter.add(domain, r1)\n-        assert not filter.has(domain, r1)\n-        filter.close(domain)\n+        assert not filter.request_seen(domain, r1)\n+        filter.close_domain(domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61b1e67cd39a94e79391e1c45c5acd48a91cb08f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 282 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -416,7 +416,7 @@ class ExecutionEngine(object):\n             \"datetime.now()-self.start_time\", \n             \"self.is_idle()\", \n             \"self.scheduler.is_idle()\",\n-            \"len(self.scheduler.pending_domains)\",\n+            \"len(self.scheduler.pending_requests)\",\n             \"self.downloader.is_idle()\",\n             \"len(self.downloader.sites)\",\n             \"self.downloader.has_capacity()\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd59748de604b272dd459602df3621df93f08c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,21 +36,7 @@ def mustbe_deferred(f, *args, **kw):\n         return defer_result(result)\n \n def chain_deferred(d1, d2):\n-    if callable(d2):\n-        d2 = lambda_deferred(d2)\n-\n-    def _pause(_):\n-        d2.pause()\n-        reactor.callLater(0, d2.unpause)\n-        return _\n-\n-    def _reclaim(_):\n-        return d2\n-\n-    #d1.addBoth(_pause) ## needs more debugging before reenable it\n-    d1.chainDeferred(d2)\n-    d1.addBoth(_reclaim)\n-    return d1\n+    return d1.chainDeferred(d2).addBoth(lambda _:d2)\n \n def lambda_deferred(func):\n     deferred = defer.Deferred()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31d5d519fcfa8e1734e0a7652252849f3ef086a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 12 | Churn Cumulative: 28 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,18 +38,6 @@ def mustbe_deferred(f, *args, **kw):\n def chain_deferred(d1, d2):\n     return d1.chainDeferred(d2).addBoth(lambda _:d2)\n \n-def lambda_deferred(func):\n-    deferred = defer.Deferred()\n-    def _success(res):\n-        d = func()\n-        d.callback(res)\n-        return d\n-    def _fail(res):\n-        d = func()\n-        d.errback(res)\n-        return d\n-    return deferred.addCallbacks(_success, _fail)\n-\n def deferred_imap(function, *sequences, **kwargs):\n     \"\"\"Analog to itertools.imap python function but friendly iterable evaluation\n     taking in count cooperative multitasking.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd1ad337b173632f0f42534930b893787936c8d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 69 | Churn Cumulative: 77 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,12 +8,9 @@ from twisted.internet import reactor, defer\n \n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.spider import spiders\n-from scrapy.core.downloader.handlers import download_any\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.defer import chain_deferred, mustbe_deferred\n-from scrapy.utils.request import request_info\n \n \n class SiteDetails(object):\n@@ -58,10 +55,7 @@ class Downloader(object):\n         self.engine = engine\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n-        self.middleware.download_function = self.enqueue\n-        self.download_function = download_any\n         self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n-        self.debug_mode = settings.getbool('DOWNLOADER_DEBUG')\n \n     def fetch(self, request, spider):\n         \"\"\" Main method to use to request a download\n@@ -72,29 +66,20 @@ class Downloader(object):\n         \"\"\"\n         domain = spider.domain_name\n         site = self.sites[domain]\n-        if not site or site.closed:\n-            if self.debug_mode:\n-                raise IgnoreRequest('Unable to fetch (domain already closed): %s' % request)\n-            else:\n-                raise IgnoreRequest\n+        if site.closed:\n+            raise IgnoreRequest('Can\\'t fetch on a closed domain: %s' + request)\n \n         site.active.add(request)\n         def _deactivate(_):\n             site.active.remove(request)\n             return _\n-        dwld = self.middleware.download(request, spider)\n-        dwld.addBoth(_deactivate)\n-        return dwld\n+\n+        return self.middleware.download(request, spider).addBoth(_deactivate)\n \n     def enqueue(self, request, spider):\n-        \"\"\" Enqueue a Request for a effective download from site\n-        \"\"\"\n-        domain = spider.domain_name\n-        site = self.sites.get(domain)\n-        if not site or site.closed:\n-            raise IgnoreRequest('Trying to enqueue %s from closed site %s' % (request, domain))\n-\n+        \"\"\"Enqueue a Request for a effective download from site\"\"\"\n         deferred = defer.Deferred()\n+        site = self.sites[spider.domain_name]\n         site.queue.append((request, deferred))\n         self.process_queue(spider)\n         return deferred\n@@ -119,41 +104,29 @@ class Downloader(object):\n \n         while site.queue and site.capacity()>0:\n             request, deferred = site.queue.pop(0)\n-            self._download(request, spider, deferred)\n+            self._download(site, request, spider, deferred)\n \n         if site.closed and site.is_idle():\n             del self.sites[domain]\n             self.engine.closed_domain(domain)\n \n-    def _download(self, request, spider, deferred):\n-        if self.debug_mode:\n-            log.msg('Activating %s' % request_info(request), log.DEBUG)\n-        domain = spider.domain_name\n-        site = self.sites.get(domain)\n+    def _download(self, site, request, spider, deferred):\n         site.downloading.add(request)\n-\n-        def _remove(result):\n-            if self.debug_mode:\n-                log.msg('Deactivating %s' % request_info(request), log.DEBUG)\n+        def _finish(_):\n             site.downloading.remove(request)\n-            return result\n-\n-        def _finish(result):\n             self.process_queue(spider)\n \n-        dwld = mustbe_deferred(self.download_function, request, spider)\n-        dwld.addBoth(_remove)\n+        dwld = mustbe_deferred(request, spider)\n         chain_deferred(dwld, deferred)\n-        dwld.addBoth(_finish)\n+        return dwld.addBoth(_finish)\n \n     def open_domain(self, domain):\n         \"\"\"Allocate resources to begin processing a domain\"\"\"\n-        spider = spiders.fromdomain(domain)\n-        if domain in self.sites: # reopen\n-            self.sites[domain].closed = False\n-            return\n+        if domain in self.sites:\n+            raise RuntimeError('Downloader domain already opened: %s' % domain)\n \n         # Instanciate site specific handling based on info provided by spider\n+        spider = spiders.fromdomain(domain)\n         delay = getattr(spider, 'download_delay', None) or settings.getint(\"DOWNLOAD_DELAY\", 0)\n         maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n         site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n@@ -161,16 +134,13 @@ class Downloader(object):\n \n     def close_domain(self, domain):\n         \"\"\"Free any resources associated with the given domain\"\"\"\n-        if self.debug_mode:\n-            log.msg(\"Downloader closing domain %s\" % domain, log.DEBUG, domain=domain)\n-        site = self.sites.get(domain)\n-        if site:\n+        if domain not in self.sites:\n+            raise RuntimeError('Downloader domain already closed: %s' % domain)\n+\n+        site = self.sites[domain]\n         site.closed = True\n         spider = spiders.fromdomain(domain)\n         self.process_queue(spider)\n-        else:\n-            if self.debug_mode:\n-                log.msg('Domain %s already closed' % domain, log.DEBUG, domain=domain)\n \n     def needs_backout(self, domain):\n         site = self.sites.get(domain)\n\n@@ -24,7 +24,6 @@ class DownloaderMiddlewareManager(object):\n         self.response_middleware = []\n         self.exception_middleware = []\n         self.load()\n-        self.download_function = download_any\n \n     def _add_middleware(self, mw):\n         if hasattr(mw, 'process_request'):\n@@ -61,7 +60,7 @@ class DownloaderMiddlewareManager(object):\n                         (method.im_self.__class__.__name__, response.__class__.__name__)\n                 if response:\n                     return response\n-            return self.download_function(request=request, spider=spider)\n+            return download_any(request=request, spider=spider)\n \n         def process_response(response):\n             assert response is not None, 'Received None in process_response'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5944f6590ab2080d02be646f637a3a2ab0737c88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 87 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.internet import reactor, defer\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.spider import spiders\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n from scrapy.utils.defer import chain_deferred, mustbe_deferred\n \n@@ -74,7 +75,7 @@ class Downloader(object):\n             site.active.remove(request)\n             return _\n \n-        return self.middleware.download(request, spider).addBoth(_deactivate)\n+        return self.middleware.download(self.enqueue, request, spider).addBoth(_deactivate)\n \n     def enqueue(self, request, spider):\n         \"\"\"Enqueue a Request for a effective download from site\"\"\"\n@@ -116,7 +117,7 @@ class Downloader(object):\n             site.downloading.remove(request)\n             self.process_queue(spider)\n \n-        dwld = mustbe_deferred(request, spider)\n+        dwld = mustbe_deferred(download_any, request, spider)\n         chain_deferred(dwld, deferred)\n         return dwld.addBoth(_finish)\n \n\n@@ -13,7 +13,6 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.middleware import build_middleware_list\n-from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n \n class DownloaderMiddlewareManager(object):\n@@ -51,7 +50,7 @@ class DownloaderMiddlewareManager(object):\n         log.msg(\"Enabled downloader middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n         self.loaded = True\n \n-    def download(self, request, spider):\n+    def download(self, download_func, request, spider):\n         def process_request(request):\n             for method in self.request_middleware:\n                 response = method(request=request, spider=spider)\n@@ -60,7 +59,7 @@ class DownloaderMiddlewareManager(object):\n                         (method.im_self.__class__.__name__, response.__class__.__name__)\n                 if response:\n                     return response\n-            return download_any(request=request, spider=spider)\n+            return download_func(request=request, spider=spider)\n \n         def process_response(response):\n             assert response is not None, 'Received None in process_response'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abe4f812e9f0cf7ea8cb574bc15201487b63ff0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 78 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -135,10 +135,10 @@ class Downloader(object):\n \n     def close_domain(self, domain):\n         \"\"\"Free any resources associated with the given domain\"\"\"\n-        if domain not in self.sites:\n+        site = self.sites.get(domain)\n+        if not site or site.closed:\n             raise RuntimeError('Downloader domain already closed: %s' % domain)\n \n-        site = self.sites[domain]\n         site.closed = True\n         spider = spiders.fromdomain(domain)\n         self.process_queue(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89712a4adf4a82be2662f4d897eb16129f46e3e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 88 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -24,15 +24,15 @@ class SiteDetails(object):\n \n         self.queue = []\n         self.active = set()\n-        self.downloading = set()\n+        self.transferring = set()\n         self.closed = False\n         self.lastseen = None\n \n     def is_idle(self):\n-        return not (self.active or self.downloading)\n+        return not (self.active or self.transferring)\n \n     def capacity(self):\n-        return self.max_concurrent_requests - len(self.downloading)\n+        return self.max_concurrent_requests - len(self.transferring)\n \n     def outstanding(self):\n         return len(self.active) + len(self.queue)\n@@ -112,9 +112,9 @@ class Downloader(object):\n             self.engine.closed_domain(domain)\n \n     def _download(self, site, request, spider, deferred):\n-        site.downloading.add(request)\n+        site.transferring.add(request)\n         def _finish(_):\n-            site.downloading.remove(request)\n+            site.transferring.remove(request)\n             self.process_queue(spider)\n \n         dwld = mustbe_deferred(download_any, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fda1fe0eb827c2db8a6d82e7728dae82d9c0a76d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 201 | Contributors (this commit): 3 | Commits (past 90d): 26 | Contributors (cumulative): 12 | DMM Complexity: 0.8\n\nDIFF:\n@@ -47,7 +47,8 @@ class DownloaderMiddlewareManager(object):\n                     mws.append(mw)\n                 except NotConfigured:\n                     pass\n-        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n+        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]),\n+            level=log.DEBUG)\n         self.loaded = True\n \n     def download(self, download_func, request, spider):\n\n@@ -36,7 +36,8 @@ class ExecutionManager(object):\n             extensions.load()\n         if not spiders.loaded:\n             spiders.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()))\n+        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n+            level=log.DEBUG)\n \n         scheduler = load_object(settings['SCHEDULER'])()\n         scrapyengine.configure(scheduler=scheduler)\n\n@@ -38,7 +38,8 @@ class SchedulerMiddlewareManager(object):\n                     self._add_middleware(mw)\n                     self.mws.append(mw)\n \n-        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in self.mws]))\n+        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in self.mws]),\n+            level=log.DEBUG)\n \n     def _add_middleware(self, mw):\n         for name in ('enqueue_request', 'open_domain', 'close_domain'):\n\n@@ -25,7 +25,8 @@ class ItemPipelineManager(object):\n                     self.pipeline.append(stageinstance)\n                 except NotConfigured:\n                     pass\n-        log.msg(\"Enabled item pipelines: %s\" % \", \".join([type(p).__name__ for p in self.pipeline]))\n+        log.msg(\"Enabled item pipelines: %s\" % \", \".join([type(p).__name__ for p in self.pipeline]),\n+            level=log.DEBUG)\n         self.loaded = True\n \n     def open_domain(self, domain):\n\n@@ -47,7 +47,8 @@ class SpiderMiddlewareManager(object):\n                     mws.append(mw)\n                 except NotConfigured:\n                     pass\n-        log.msg(\"Enabled spider middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n+        log.msg(\"Enabled spider middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]),\n+            level=log.DEBUG)\n         self.loaded = True\n \n     def scrape(self, request, response, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7bf610d0b7dc5e96712b206ea8ece4aed9c5bada", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 21 | Files Changed: 4 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 15/14 | Churn Δ: 34 | Churn Cumulative: 404 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,7 +25,10 @@ class DelayedCloseDomain(object):\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n \n     def domain_idle(self, domain):\n-        lastseen = scrapyengine.downloader.lastseen(domain)\n+        try:\n+            lastseen = scrapyengine.downloader.sites[domain].lastseen\n+        except KeyError:\n+            lastseen = None\n         if not lastseen:\n             lastseen = self.opened_at[domain]\n \n\n@@ -55,7 +55,8 @@ class LiveStats(object):\n         s += \"<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Pending</th><th>Downloader<br/>Active</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n         for d in sorted(self.domains.keys()):\n             scheduled = len(sch.pending_requests[d]) if d in sch.pending_requests else 0\n-            active, pending = len(dwl.active_requests(d)), len(dwl.request_queue(d))\n+            active = len(dwl.sites[d].active) if d in dwl.sites else 0\n+            pending = len(dwl.sites[d].queue) if d in dwl.queue else 0\n             stats = self.domains[d]\n             runtime = stats.finished - stats.started if stats.finished else datetime.now() - stats.started\n \n\n@@ -151,10 +151,6 @@ class Downloader(object):\n     def domain_is_open(self, domain):\n         return domain in self.sites\n \n-    def lastseen(self, domain):\n-        if domain in self.sites:\n-            return self.sites[domain].lastseen\n-\n     def outstanding(self, domain):\n         \"\"\"The number of outstanding requests for a domain\n         This includes both active requests and pending requests.\n@@ -163,17 +159,6 @@ class Downloader(object):\n         if site:\n             return site.outstanding()\n \n-    def domain_is_idle(self, domain):\n-        return not self.outstanding(domain)\n-\n-    def request_queue(self, domain):\n-        site = self.sites.get(domain)\n-        return site.queue if site else []\n-\n-    def active_requests(self, domain):\n-        site = self.sites.get(domain)\n-        return site.active if site else []\n-\n     def has_capacity(self):\n         \"\"\"Does the downloader have capacity to handle more domains\"\"\"\n         return len(self.sites) < self.concurrent_domains\n\n@@ -208,7 +208,7 @@ class ExecutionEngine(object):\n     def domain_is_idle(self, domain):\n         scraping = self._scraping.get(domain)\n         pending = self.scheduler.domain_has_pending_requests(domain)\n-        downloading = not self.downloader.domain_is_idle(domain)\n+        downloading = domain in self.downloader.sites and self.downloader.sites[domain].outstanding() > 0\n         haspipe = not self.pipeline.domain_is_idle(domain)\n         return not (pending or downloading or haspipe or scraping)\n \n@@ -426,11 +426,14 @@ class ExecutionEngine(object):\n             ]\n         domain_tests = [\n             \"self.domain_is_idle(domain)\",\n+            \"domain in self.cancelled\",\n             \"self.scheduler.domain_has_pending_requests(domain)\",\n             \"len(self.scheduler.pending_requests[domain])\",\n-            \"self.downloader.outstanding(domain)\",\n-            \"len(self.downloader.request_queue(domain))\",\n-            \"len(self.downloader.active_requests(domain))\",\n+            \"len(self.downloader.sites[domain].queue)\",\n+            \"len(self.downloader.sites[domain].active)\",\n+            \"len(self.downloader.sites[domain].transferring)\",\n+            \"self.downloader.sites[domain].closed\",\n+            \"self.downloader.sites[domain].lastseen\",\n             \"self.pipeline.domain_is_idle(domain)\",\n             \"len(self.pipeline.domaininfo[domain])\",\n             \"len(self._scraping[domain])\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab2dd764e0a09f4934b250c45320091f810f0501", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 126 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,13 +14,21 @@ from scrapy.conf import settings\n from scrapy.utils.defer import chain_deferred, mustbe_deferred\n \n \n-class SiteDetails(object):\n+class SiteInfo(object):\n     \"\"\"This is a simple data record that encapsulates the details we hold on\n     each domain which we are scraping.\n     \"\"\"\n-    def __init__(self, download_delay=None, max_concurrent_requests=2):\n+    def __init__(self, download_delay=None, max_concurrent_requests=None):\n+        if download_delay is None:\n+            self.download_delay = settings.getint('DOWNLOAD_DELAY')\n+        else:\n             self.download_delay = download_delay\n-        self.max_concurrent_requests = max_concurrent_requests if not download_delay else 1\n+        if download_delay:\n+            self.max_concurrent_requests = 1\n+        elif max_concurrent_requests is None:\n+            self.max_concurrent_requests = settings.getint('REQUESTS_PER_DOMAIN')\n+        else:\n+            self.max_concurrent_requests =  max_concurrent_requests\n \n         self.queue = []\n         self.active = set()\n@@ -126,12 +134,11 @@ class Downloader(object):\n         if domain in self.sites:\n             raise RuntimeError('Downloader domain already opened: %s' % domain)\n \n-        # Instanciate site specific handling based on info provided by spider\n         spider = spiders.fromdomain(domain)\n-        delay = getattr(spider, 'download_delay', None) or settings.getint(\"DOWNLOAD_DELAY\", 0)\n-        maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n-        site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n-        self.sites[domain] = site\n+        self.sites[domain] = SiteInfo(\n+            download_delay=getattr(spider, 'download_delay', None),\n+            max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n+        )\n \n     def close_domain(self, domain):\n         \"\"\"Free any resources associated with the given domain\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#400a54bf7c765f6f443358ca647b5b439b746e70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 26 | Files Changed: 7 | Hunks: 17 | Methods Changed: 21 | Complexity Δ (Sum/Max): 54/21 | Churn Δ: 51 | Churn Cumulative: 348 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -68,8 +68,8 @@ class ItemSamplerPipeline(object):\n         if self.empty_domains:\n             log.msg(\"No products sampled for: %s\" % \" \".join(self.empty_domains), level=log.WARNING)\n \n-    def domain_closed(self, domain, spider, status):\n-        if status == 'finished' and not stats.getpath(\"%s/items_sampled\" % domain):\n+    def domain_closed(self, domain, spider, reason):\n+        if reason == 'finished' and not stats.getpath(\"%s/items_sampled\" % domain):\n             self.empty_domains.add(domain)\n         self.domains_count += 1\n         log.msg(\"Sampled %d domains so far (%d empty)\" % (self.domains_count, len(self.empty_domains)), level=log.INFO)\n\n@@ -47,6 +47,6 @@ class DNSCache(object):\n             self._cache[host] = ips\n         return ips[0]\n \n-    def domain_closed(self, domain, spider, status):\n+    def domain_closed(self, domain, spider, reason):\n         if domain in self._cache:\n             del self._cache[domain]\n\n@@ -47,7 +47,7 @@ class ExecutionEngine(object):\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n-        self.cancelled = set() # domains in cancelation state\n+        self.closing = {} # domains being closed\n         self.debug_mode = settings.getbool('ENGINE_DEBUG')\n         self.tasks = []\n         self.ports = []\n@@ -135,7 +135,7 @@ class ExecutionEngine(object):\n             self.running = False\n             for domain in self.open_domains:\n                 spider = spiders.fromdomain(domain)\n-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status='cancelled')\n+                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason='shutdown')\n             for tsk, _, _ in self.tasks: # stop looping calls\n                 if tsk.running:\n                     tsk.stop()\n@@ -186,7 +186,7 @@ class ExecutionEngine(object):\n             return\n \n         # backout enqueing downloads if domain needs it\n-        if domain in self.cancelled or self.downloader.needs_backout(domain):\n+        if domain in self.closing or self.downloader.needs_backout(domain):\n             return\n \n         # Next pending request from scheduler\n@@ -235,7 +235,7 @@ class ExecutionEngine(object):\n                             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n                         self.next_request(spider)\n \n-                    if domain in self.cancelled:\n+                    if domain in self.closing:\n                         return\n                     elif isinstance(output, ScrapedItem):\n                         log.msg(\"Scraped %s in <%s>\" % (output, request.url), log.INFO, domain=domain)\n@@ -346,7 +346,7 @@ class ExecutionEngine(object):\n         spider = spider or spiders.fromdomain(domain)\n         self.next_request(spider)\n \n-        self.cancelled.discard(domain)\n+        self.closing.pop(domain, None)\n         self.downloader.open_domain(domain)\n         self.pipeline.open_domain(domain)\n         self._scraping[domain] = set()\n@@ -379,11 +379,11 @@ class ExecutionEngine(object):\n         if self.is_idle() and not self.keep_alive:\n             self.stop()\n \n-    def close_domain(self, domain):\n+    def close_domain(self, domain, reason='cancelled'):\n         \"\"\"Close (cancel) domain and clear all its outstanding requests\"\"\"\n-        if domain not in self.cancelled:\n-            log.msg(\"Closing domain\", domain=domain)\n-            self.cancelled.add(domain)\n+        if domain not in self.closing:\n+            log.msg(\"Closing domain (%s)\" % reason, domain=domain)\n+            self.closing[domain] = reason\n             self._close_domain(domain)\n \n     def _close_domain(self, domain):\n@@ -400,10 +400,10 @@ class ExecutionEngine(object):\n         self.scheduler.close_domain(domain)\n         self.pipeline.close_domain(domain)\n         del self._scraping[domain]\n-        status = 'cancelled' if domain in self.cancelled else 'finished'\n-        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n-        log.msg(\"Domain closed (%s)\" % status, domain=domain) \n-        self.cancelled.discard(domain)\n+        reason = self.closing.get(domain, 'finished')\n+        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n+        log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n+        self.closing.pop(domain, None)\n         self._mainloop()\n \n     def getstatus(self):\n@@ -426,7 +426,7 @@ class ExecutionEngine(object):\n             ]\n         domain_tests = [\n             \"self.domain_is_idle(domain)\",\n-            \"domain in self.cancelled\",\n+            \"self.closing.get(domain)\",\n             \"self.scheduler.domain_has_pending_requests(domain)\",\n             \"len(self.scheduler.pending_requests[domain])\",\n             \"len(self.downloader.sites[domain].queue)\",\n\n@@ -30,8 +30,7 @@ domain_opened = object()\n domain_idle = object()\n \n # After a domain has been closed\n-# args: domain, spider, status\n-# status is a string and its possible values are: \"finished\" or \"cancelled\"\n+# args: domain, spider, reason\n domain_closed = object()\n \n # New request received from spiders\n\n@@ -32,10 +32,10 @@ class CoreStats(object):\n         stats.setpath('%s/envinfo' % domain, stats.getpath('_envinfo'))\n         stats.incpath('_global/domain_count/opened')\n \n-    def stats_domain_closing(self, domain, spider, status):\n+    def stats_domain_closing(self, domain, spider, reason):\n         stats.setpath('%s/finish_time' % domain, datetime.datetime.now())\n-        stats.setpath('%s/finish_status' % domain, 'OK' if status == 'finished' else status)\n-        stats.incpath('_global/domain_count/%s' % status)\n+        stats.setpath('%s/finish_status' % domain, 'OK' if reason == 'finished' else reason)\n+        stats.incpath('_global/domain_count/%s' % reason)\n \n     def item_scraped(self, item, spider):\n         stats.incpath('%s/item_scraped_count' % spider.domain_name)\n\n@@ -66,12 +66,12 @@ class StatsCollector(dict):\n     def _domain_open(self, domain, spider):\n         dispatcher.send(signal=self.domain_open, sender=self.__class__, domain=domain, spider=spider)\n \n-    def _domain_closed(self, domain, spider, status):\n-        dispatcher.send(signal=self.domain_closing, sender=self.__class__, domain=domain, spider=spider, status=status)\n+    def _domain_closed(self, domain, spider, reason):\n+        dispatcher.send(signal=self.domain_closing, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n         if self.debug:\n             log.msg(pprint.pformat(self[domain]), domain=domain, level=log.DEBUG)\n         if self.db:\n             self.db.put(domain, self[domain])\n         if self.cleanup:\n             del self[domain]\n-        dispatcher.send(signal=self.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n+        dispatcher.send(signal=self.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n\n@@ -186,7 +186,7 @@ class EngineTest(unittest.TestCase):\n                          session.signals_catched[signals.domain_opened])\n         self.assertEqual({'domain': session.domain, 'spider': session.spider},\n                          session.signals_catched[signals.domain_idle])\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'status': 'finished'},\n+        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'reason': 'finished'},\n                          session.signals_catched[signals.domain_closed])\n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
