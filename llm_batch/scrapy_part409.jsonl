{"custom_id": "scrapy#fc566a7ff9913be5fce2b961a6599f861e1ef59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 56 | Churn Cumulative: 3324 | Contributors (this commit): 33 | Commits (past 90d): 14 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,7 +175,20 @@ def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n     )\n \n \n+def is_reactor_installed() -> bool:\n+    return \"twisted.internet.reactor\" in sys.modules\n+\n+\n def is_asyncio_reactor_installed() -> bool:\n+    \"\"\"Check whether the installed reactor is :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n+\n+    Raise a :exc:`RuntimeError` if no reactor is installed.\n+    \"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\n+            \"is_asyncio_reactor_installed() called without an installed reactor.\"\n+        )\n+\n     from twisted.internet import reactor\n \n     return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n@@ -1,5 +1,12 @@\n import scrapy\n from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.reactor import is_asyncio_reactor_installed\n+\n+\n+class ReactorCheckExtension:\n+    def __init__(self):\n+        if not is_asyncio_reactor_installed():\n+            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n \n \n class NoRequestsSpider(scrapy.Spider):\n@@ -12,6 +19,7 @@ class NoRequestsSpider(scrapy.Spider):\n process = CrawlerProcess(\n     settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n     }\n )\n process.crawl(NoRequestsSpider)\n\n@@ -1,9 +1,39 @@\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from scrapy.utils.reactor import install_reactor\n+from scrapy.utils.reactor import (\n+    install_reactor,\n+    is_asyncio_reactor_installed,\n+    is_reactor_installed,\n+)\n+\n+if is_reactor_installed():\n+    raise RuntimeError(\n+        \"Reactor already installed before is_asyncio_reactor_installed().\"\n+    )\n+\n+try:\n+    is_asyncio_reactor_installed()\n+except RuntimeError:\n+    pass\n+else:\n+    raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n+\n+if is_reactor_installed():\n+    raise RuntimeError(\n+        \"Reactor already installed after is_asyncio_reactor_installed().\"\n+    )\n \n install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n \n+if not is_asyncio_reactor_installed():\n+    raise RuntimeError(\"Wrong reactor installed after install_reactor().\")\n+\n+\n+class ReactorCheckExtension:\n+    def __init__(self):\n+        if not is_asyncio_reactor_installed():\n+            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n+\n \n class NoRequestsSpider(scrapy.Spider):\n     name = \"no_request\"\n@@ -15,6 +45,7 @@ class NoRequestsSpider(scrapy.Spider):\n process = CrawlerProcess(\n     settings={\n         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n     }\n )\n process.crawl(NoRequestsSpider)\n\n@@ -747,6 +747,7 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n             in log\n         )\n+        assert \"RuntimeError\" not in log\n \n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script(\"asyncio_enabled_reactor.py\")\n@@ -755,6 +756,7 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n             in log\n         )\n+        assert \"RuntimeError\" not in log\n \n     @pytest.mark.skipif(\n         parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9057bf4e1e08dccac4fa6d9f0f191d1f4708a43a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1266 | Contributors (this commit): 24 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -362,7 +362,8 @@ def deferred_from_coro(o: _T) -> _T: ...\n \n \n def deferred_from_coro(o: _T) -> Deferred | _T:\n-    \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n+    \"\"\"Converts a coroutine or other awaitable object into a Deferred,\n+    or returns the object as is if it isn't a coroutine.\"\"\"\n     if isinstance(o, Deferred):\n         return o\n     if asyncio.isfuture(o) or inspect.isawaitable(o):\n@@ -442,12 +443,12 @@ def maybe_deferred_to_future(d: Deferred[_T]) -> Deferred[_T] | Future[_T]:\n     What you can await in Scrapy callables defined as coroutines depends on the\n     value of :setting:`TWISTED_REACTOR`:\n \n-    -   When not using the asyncio reactor, you can only await on\n-        :class:`~twisted.internet.defer.Deferred` objects.\n-\n     -   When :ref:`using the asyncio reactor <install-asyncio>`, you can only\n         await on :class:`asyncio.Future` objects.\n \n+    -   When not using the asyncio reactor, you can only await on\n+        :class:`~twisted.internet.defer.Deferred` objects.\n+\n     If you want to write code that uses ``Deferred`` objects but works with any\n     reactor, use this function on all ``Deferred`` objects::\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#872924721344502cca92a72ce68d5f78a9b9e5bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 348 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,6 +119,9 @@ def requires_boto3(request):\n def pytest_configure(config):\n     if config.getoption(\"--reactor\") != \"default\":\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+    else:\n+        # install the reactor explicitly\n+        from twisted.internet import reactor  # noqa: F401\n \n \n # Generate localhost certificate files, needed by some tests\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ee01efe496db9ff8506ca37fe2571f5f4ac2849", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2061 | Contributors (this commit): 34 | Commits (past 90d): 5 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -246,6 +246,7 @@ class Scraper:\n             spider=spider,\n         )\n         assert self.crawler.stats\n+        self.crawler.stats.inc_value(\"spider_exceptions/count\", spider=spider)\n         self.crawler.stats.inc_value(\n             f\"spider_exceptions/{_failure.value.__class__.__name__}\", spider=spider\n         )\n\n@@ -88,6 +88,7 @@ class TestCloseSpider(TestCase):\n         assert reason == \"closespider_errorcount\"\n         key = f\"spider_exceptions/{crawler.spider.exception_cls.__name__}\"\n         errorcount = crawler.stats.get_value(key)\n+        assert crawler.stats.get_value(\"spider_exceptions/count\") >= close_on\n         assert errorcount >= close_on\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ca882fba86750199c2f41ef24b5495a4afa7988", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 31 | Files Changed: 3 | Hunks: 35 | Methods Changed: 18 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 114 | Churn Cumulative: 2493 | Contributors (this commit): 91 | Commits (past 90d): 8 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -351,3 +351,5 @@ SPIDER_CONTRACTS_BASE = {\n     \"scrapy.contracts.default.ReturnsContract\": 2,\n     \"scrapy.contracts.default.ScrapesContract\": 3,\n }\n+\n+WARN_ON_GENERATOR_RETURN_VALUE = True\n\n@@ -286,6 +286,8 @@ def warn_on_generator_with_return_value(\n     Logs a warning if a callable is a generator function and includes\n     a 'return' statement with a value different than None\n     \"\"\"\n+    if not spider.settings.getbool(\"WARN_ON_GENERATOR_RETURN_VALUE\"):\n+        return\n     try:\n         if is_generator_with_return_value(callable):\n             warnings.warn(\n\n@@ -2,6 +2,8 @@ import warnings\n from functools import partial\n from unittest import mock\n \n+import pytest\n+\n from scrapy.utils.misc import (\n     is_generator_with_return_value,\n     warn_on_generator_with_return_value,\n@@ -40,7 +42,24 @@ def generator_that_returns_stuff():\n \n \n class TestUtilsMisc:\n-    def test_generators_return_something(self):\n+    @pytest.fixture\n+    def mock_spider(self):\n+        class MockSettings:\n+            def __init__(self, settings_dict=None):\n+                self.settings_dict = settings_dict or {\n+                    \"WARN_ON_GENERATOR_RETURN_VALUE\": True\n+                }\n+\n+            def getbool(self, name, default=False):\n+                return self.settings_dict.get(name, default)\n+\n+        class MockSpider:\n+            def __init__(self):\n+                self.settings = MockSettings()\n+\n+        return MockSpider()\n+\n+    def test_generators_return_something(self, mock_spider):\n         def f1():\n             yield 1\n             return 2\n@@ -75,30 +94,30 @@ https://example.org\n         assert is_generator_with_return_value(i1)\n \n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, top_level_return_something)\n+            warn_on_generator_with_return_value(mock_spider, top_level_return_something)\n             assert len(w) == 1\n             assert (\n-                'The \"NoneType.top_level_return_something\" method is a generator'\n+                'The \"MockSpider.top_level_return_something\" method is a generator'\n                 in str(w[0].message)\n             )\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, f1)\n+            warn_on_generator_with_return_value(mock_spider, f1)\n             assert len(w) == 1\n-            assert 'The \"NoneType.f1\" method is a generator' in str(w[0].message)\n+            assert 'The \"MockSpider.f1\" method is a generator' in str(w[0].message)\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, g1)\n+            warn_on_generator_with_return_value(mock_spider, g1)\n             assert len(w) == 1\n-            assert 'The \"NoneType.g1\" method is a generator' in str(w[0].message)\n+            assert 'The \"MockSpider.g1\" method is a generator' in str(w[0].message)\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, h1)\n+            warn_on_generator_with_return_value(mock_spider, h1)\n             assert len(w) == 1\n-            assert 'The \"NoneType.h1\" method is a generator' in str(w[0].message)\n+            assert 'The \"MockSpider.h1\" method is a generator' in str(w[0].message)\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, i1)\n+            warn_on_generator_with_return_value(mock_spider, i1)\n             assert len(w) == 1\n-            assert 'The \"NoneType.i1\" method is a generator' in str(w[0].message)\n+            assert 'The \"MockSpider.i1\" method is a generator' in str(w[0].message)\n \n-    def test_generators_return_none(self):\n+    def test_generators_return_none(self, mock_spider):\n         def f2():\n             yield 1\n \n@@ -142,31 +161,31 @@ https://example.org\n         assert not is_generator_with_return_value(l2)\n \n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, top_level_return_none)\n+            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, f2)\n+            warn_on_generator_with_return_value(mock_spider, f2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, g2)\n+            warn_on_generator_with_return_value(mock_spider, g2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, h2)\n+            warn_on_generator_with_return_value(mock_spider, h2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, i2)\n+            warn_on_generator_with_return_value(mock_spider, i2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, j2)\n+            warn_on_generator_with_return_value(mock_spider, j2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, k2)\n+            warn_on_generator_with_return_value(mock_spider, k2)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, l2)\n+            warn_on_generator_with_return_value(mock_spider, l2)\n             assert len(w) == 0\n \n-    def test_generators_return_none_with_decorator(self):\n+    def test_generators_return_none_with_decorator(self, mock_spider):\n         def decorator(func):\n             def inner_func():\n                 func()\n@@ -223,36 +242,36 @@ https://example.org\n         assert not is_generator_with_return_value(l3)\n \n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, top_level_return_none)\n+            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, f3)\n+            warn_on_generator_with_return_value(mock_spider, f3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, g3)\n+            warn_on_generator_with_return_value(mock_spider, g3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, h3)\n+            warn_on_generator_with_return_value(mock_spider, h3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, i3)\n+            warn_on_generator_with_return_value(mock_spider, i3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, j3)\n+            warn_on_generator_with_return_value(mock_spider, j3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, k3)\n+            warn_on_generator_with_return_value(mock_spider, k3)\n             assert len(w) == 0\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, l3)\n+            warn_on_generator_with_return_value(mock_spider, l3)\n             assert len(w) == 0\n \n     @mock.patch(\n         \"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error\n     )\n-    def test_indentation_error(self):\n+    def test_indentation_error(self, mock_spider):\n         with warnings.catch_warnings(record=True) as w:\n-            warn_on_generator_with_return_value(None, top_level_return_none)\n+            warn_on_generator_with_return_value(mock_spider, top_level_return_none)\n             assert len(w) == 1\n             assert \"Unable to determine\" in str(w[0].message)\n \n@@ -262,3 +281,32 @@ https://example.org\n \n         partial_cb = partial(cb, arg1=42)\n         assert not is_generator_with_return_value(partial_cb)\n+\n+    def test_warn_on_generator_with_return_value_settings_disabled(self):\n+        class MockSettings:\n+            def __init__(self, settings_dict=None):\n+                self.settings_dict = settings_dict or {}\n+\n+            def getbool(self, name, default=False):\n+                return self.settings_dict.get(name, default)\n+\n+        class MockSpider:\n+            def __init__(self):\n+                self.settings = MockSettings({\"WARN_ON_GENERATOR_RETURN_VALUE\": False})\n+\n+        spider = MockSpider()\n+\n+        def gen_with_return():\n+            yield 1\n+            return \"value\"\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(spider, gen_with_return)\n+            assert len(w) == 0\n+\n+        spider.settings.settings_dict[\"WARN_ON_GENERATOR_RETURN_VALUE\"] = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(spider, gen_with_return)\n+            assert len(w) == 1\n+            assert \"is a generator\" in str(w[0].message)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f99da8f865efff11f6c8736567b8fbd4413091c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 86 | Files Changed: 1 | Hunks: 33 | Methods Changed: 19 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 156 | Churn Cumulative: 790 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n-from typing import Any\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n from unittest import mock\n \n import pytest\n from twisted.internet import error, reactor\n-from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n+from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.python import failure\n from twisted.trial import unittest\n \n@@ -13,8 +15,12 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n from tests.test_robotstxt_interface import rerp_available\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n \n class TestRobotsTxtMiddleware(unittest.TestCase):\n     def setUp(self):\n@@ -31,7 +37,7 @@ class TestRobotsTxtMiddleware(unittest.TestCase):\n         with pytest.raises(NotConfigured):\n             RobotsTxtMiddleware(self.crawler)\n \n-    def _get_successful_crawler(self):\n+    def _get_successful_crawler(self) -> Crawler:\n         crawler = self.crawler\n         crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         ROBOTS = \"\"\"\n@@ -54,54 +60,41 @@ Disallow: /some/randome/page.html\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n-    def test_robotstxt(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n-        return DeferredList(\n-            [\n-                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n-                maybeDeferred(self.assertRobotsTxtRequested, \"http://site.local\"),\n-                self.assertIgnored(Request(\"http://site.local/admin/main\"), middleware),\n-                self.assertIgnored(Request(\"http://site.local/static/\"), middleware),\n-                self.assertIgnored(\n-                    Request(\"http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\"),\n-                    middleware,\n-                ),\n-                self.assertIgnored(\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n+        self.assertRobotsTxtRequested(\"http://site.local\")\n+        await self.assertIgnored(Request(\"http://site.local/admin/main\"), middleware)\n+        await self.assertIgnored(Request(\"http://site.local/static/\"), middleware)\n+        await self.assertIgnored(\n+            Request(\"http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\"), middleware\n+        )\n+        await self.assertIgnored(\n             Request(\"http://site.local/wiki/Käyttäjä:\"), middleware\n-                ),\n-            ],\n-            fireOnOneErrback=True,\n         )\n \n-    def test_robotstxt_ready_parser(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_ready_parser(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n-        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n-        d.addCallback(\n-            lambda _: self.assertNotIgnored(\n-                Request(\"http://site.local/allowed\"), middleware\n-            )\n-        )\n-        return d\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n \n-    def test_robotstxt_meta(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_meta(self):\n         middleware = RobotsTxtMiddleware(self._get_successful_crawler())\n         meta = {\"dont_obey_robotstxt\": True}\n-        return DeferredList(\n-            [\n-                self.assertNotIgnored(\n+        await self.assertNotIgnored(\n             Request(\"http://site.local/allowed\", meta=meta), middleware\n-                ),\n-                self.assertNotIgnored(\n+        )\n+        await self.assertNotIgnored(\n             Request(\"http://site.local/admin/main\", meta=meta), middleware\n-                ),\n-                self.assertNotIgnored(\n+        )\n+        await self.assertNotIgnored(\n             Request(\"http://site.local/static/\", meta=meta), middleware\n-                ),\n-            ],\n-            fireOnOneErrback=True,\n         )\n \n-    def _get_garbage_crawler(self):\n+    def _get_garbage_crawler(self) -> Crawler:\n         crawler = self.crawler\n         crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         response = Response(\n@@ -116,22 +109,16 @@ Disallow: /some/randome/page.html\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n-    def test_robotstxt_garbage(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_garbage(self):\n         # garbage response should be discarded, equal 'allow all'\n         middleware = RobotsTxtMiddleware(self._get_garbage_crawler())\n-        return DeferredList(\n-            [\n-                self.assertNotIgnored(Request(\"http://site.local\"), middleware),\n-                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n-                self.assertNotIgnored(\n-                    Request(\"http://site.local/admin/main\"), middleware\n-                ),\n-                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n-            ],\n-            fireOnOneErrback=True,\n-        )\n+        await self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/admin/main\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware)\n \n-    def _get_emptybody_crawler(self):\n+    def _get_emptybody_crawler(self) -> Crawler:\n         crawler = self.crawler\n         crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         response = Response(\"http://site.local/robots.txt\")\n@@ -144,21 +131,16 @@ Disallow: /some/randome/page.html\n         crawler.engine.download.side_effect = return_response\n         return crawler\n \n-    def test_robotstxt_empty_response(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_empty_response(self):\n         # empty response should equal 'allow all'\n         middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())\n-        return DeferredList(\n-            [\n-                self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware),\n-                self.assertNotIgnored(\n-                    Request(\"http://site.local/admin/main\"), middleware\n-                ),\n-                self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware),\n-            ],\n-            fireOnOneErrback=True,\n-        )\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/admin/main\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local/static/\"), middleware)\n \n-    def test_robotstxt_error(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_error(self):\n         self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         err = error.DNSLookupError(\"Robotstxt address not found\")\n \n@@ -171,15 +153,13 @@ Disallow: /some/randome/page.html\n \n         middleware = RobotsTxtMiddleware(self.crawler)\n         middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)\n-        deferred = middleware.process_request(Request(\"http://site.local\"), None)\n-\n-        def check_called(_: Any) -> None:\n+        await maybe_deferred_to_future(\n+            middleware.process_request(Request(\"http://site.local\"), None)\n+        )\n         assert middleware._logerror.called\n \n-        deferred.addCallback(check_called)\n-        return deferred\n-\n-    def test_robotstxt_immediate_error(self):\n+    @deferred_f_from_coro_f\n+    async def test_robotstxt_immediate_error(self):\n         self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n         err = error.DNSLookupError(\"Robotstxt address not found\")\n \n@@ -191,9 +171,10 @@ Disallow: /some/randome/page.html\n         self.crawler.engine.download.side_effect = immediate_failure\n \n         middleware = RobotsTxtMiddleware(self.crawler)\n-        return self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n+        await self.assertNotIgnored(Request(\"http://site.local\"), middleware)\n \n-    def test_ignore_robotstxt_request(self):\n+    @deferred_f_from_coro_f\n+    async def test_ignore_robotstxt_request(self):\n         self.crawler.settings.set(\"ROBOTSTXT_OBEY\", True)\n \n         def ignore_request(request):\n@@ -206,14 +187,9 @@ Disallow: /some/randome/page.html\n         middleware = RobotsTxtMiddleware(self.crawler)\n         mw_module_logger.error = mock.MagicMock()\n \n-        d = self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n-\n-        def check_not_called(_: Any) -> None:\n+        await self.assertNotIgnored(Request(\"http://site.local/allowed\"), middleware)\n         assert not mw_module_logger.error.called  # type: ignore[attr-defined]\n \n-        d.addCallback(check_not_called)\n-        return d\n-\n     def test_robotstxt_user_agent_setting(self):\n         crawler = self._get_successful_crawler()\n         crawler.settings.set(\"ROBOTSTXT_USER_AGENT\", \"Examplebot\")\n@@ -236,19 +212,27 @@ Disallow: /some/randome/page.html\n             Deferred,\n         )\n \n-    def assertNotIgnored(self, request, middleware):\n+    async def assertNotIgnored(\n+        self, request: Request, middleware: RobotsTxtMiddleware\n+    ) -> None:\n         spider = None  # not actually used\n-        dfd = maybeDeferred(middleware.process_request, request, spider)\n-        dfd.addCallback(self.assertIsNone)\n-        return dfd\n+        result = await maybe_deferred_to_future(\n+            maybeDeferred(middleware.process_request, request, spider)  # type: ignore[call-overload]\n+        )\n+        assert result is None\n \n-    def assertIgnored(self, request, middleware):\n+    async def assertIgnored(\n+        self, request: Request, middleware: RobotsTxtMiddleware\n+    ) -> None:\n         spider = None  # not actually used\n-        return self.assertFailure(\n-            maybeDeferred(middleware.process_request, request, spider), IgnoreRequest\n+        await maybe_deferred_to_future(\n+            self.assertFailure(\n+                middleware.process_request(request, spider),  # type: ignore[arg-type]\n+                IgnoreRequest,\n+            )\n         )\n \n-    def assertRobotsTxtRequested(self, base_url):\n+    def assertRobotsTxtRequested(self, base_url: str) -> None:\n         calls = self.crawler.engine.download.call_args_list\n         request = calls[0][0][0]\n         assert request.url == f\"{base_url}/robots.txt\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daf9db72b2dc4ff8a6344fb9a0a4817fb953fdd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 293 | Lines Deleted: 114 | Files Changed: 8 | Hunks: 47 | Methods Changed: 42 | Complexity Δ (Sum/Max): 4/17 | Churn Δ: 407 | Churn Cumulative: 2510 | Contributors (this commit): 27 | Commits (past 90d): 11 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,97 @@\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+from scrapy import Request, Spider\n+\n+if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, Iterable\n+\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+    from scrapy.http import Response\n+\n+\n+class BaseSpiderMiddleware:\n+    \"\"\"Optional base class for spider middlewares.\n+\n+    This class provides helper methods for asynchronous ``process_spider_output``\n+    methods. Middlewares that don't have a ``process_spider_output`` method don't need\n+    to use it.\n+\n+    You can override the\n+    :meth:`~scrapy.spidermiddlewares.base.BaseSpiderMiddleware.get_processed_request`\n+    method to add processing code for requests and the\n+    :meth:`~scrapy.spidermiddlewares.base.BaseSpiderMiddleware.get_processed_item`\n+    method to add processing code for items. These methods take a single\n+    request or item from the spider output iterable and return a request or\n+    item (the same or a new one), or ``None`` to remove this request or item\n+    from the processing.\n+    \"\"\"\n+\n+    def __init__(self, crawler: Crawler):\n+        self.crawler: Crawler = crawler\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls(crawler)\n+\n+    def process_spider_output(\n+        self, response: Response, result: Iterable[Any], spider: Spider\n+    ) -> Iterable[Any]:\n+        for o in result:\n+            if isinstance(o, Request):\n+                o = self.get_processed_request(o, response)\n+            else:\n+                o = self.get_processed_item(o, response)\n+            if o is not None:\n+                yield o\n+\n+    async def process_spider_output_async(\n+        self, response: Response, result: AsyncIterable[Any], spider: Spider\n+    ) -> AsyncIterable[Any]:\n+        async for o in result:\n+            if isinstance(o, Request):\n+                o = self.get_processed_request(o, response)\n+            else:\n+                o = self.get_processed_item(o, response)\n+            if o is not None:\n+                yield o\n+\n+    def get_processed_request(\n+        self, request: Request, response: Response\n+    ) -> Request | None:\n+        \"\"\"Return a processed request from the spider output.\n+\n+        This method is called with a single request from the spider output.\n+        It should return the same or a different request, or ``None`` to\n+        ignore it.\n+\n+        :param request: the input request\n+        :type request: :class:`~scrapy.Request` object\n+\n+        :param response: the response being processed\n+        :type response: :class:`~scrapy.http.Response` object\n+\n+        :return: the processed request or ``None``\n+        \"\"\"\n+        return request\n+\n+    def get_processed_item(self, item: Any, response: Response) -> Any:\n+        \"\"\"Return a processed item from the spider output.\n+\n+        This method is called with a single item from the spider output.\n+        It should return the same or a different item, or ``None`` to\n+        ignore it.\n+\n+        :param item: the input item\n+        :type item: item object\n+\n+        :param response: the response being processed\n+        :type response: :class:`~scrapy.http.Response` object\n+\n+        :return: the processed item or ``None``\n+        \"\"\"\n+        return item\n\n@@ -9,7 +9,7 @@ from __future__ import annotations\n import logging\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.http import Request, Response\n+from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n \n if TYPE_CHECKING:\n     from collections.abc import AsyncIterable, Iterable\n@@ -19,14 +19,17 @@ if TYPE_CHECKING:\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n+    from scrapy.http import Request, Response\n     from scrapy.statscollectors import StatsCollector\n \n \n logger = logging.getLogger(__name__)\n \n \n-class DepthMiddleware:\n-    def __init__(\n+class DepthMiddleware(BaseSpiderMiddleware):\n+    crawler: Crawler\n+\n+    def __init__(  # pylint: disable=super-init-not-called\n         self,\n         maxdepth: int,\n         stats: StatsCollector,\n@@ -45,21 +48,22 @@ class DepthMiddleware:\n         verbose = settings.getbool(\"DEPTH_STATS_VERBOSE\")\n         prio = settings.getint(\"DEPTH_PRIORITY\")\n         assert crawler.stats\n-        return cls(maxdepth, crawler.stats, verbose, prio)\n+        o = cls(maxdepth, crawler.stats, verbose, prio)\n+        o.crawler = crawler\n+        return o\n \n     def process_spider_output(\n         self, response: Response, result: Iterable[Any], spider: Spider\n     ) -> Iterable[Any]:\n         self._init_depth(response, spider)\n-        return (r for r in result if self._filter(r, response, spider))\n+        yield from super().process_spider_output(response, result, spider)\n \n     async def process_spider_output_async(\n         self, response: Response, result: AsyncIterable[Any], spider: Spider\n     ) -> AsyncIterable[Any]:\n         self._init_depth(response, spider)\n-        async for r in result:\n-            if self._filter(r, response, spider):\n-                yield r\n+        async for o in super().process_spider_output_async(response, result, spider):\n+            yield o\n \n     def _init_depth(self, response: Response, spider: Spider) -> None:\n         # base case (depth=0)\n@@ -68,9 +72,9 @@ class DepthMiddleware:\n             if self.verbose_stats:\n                 self.stats.inc_value(\"request_depth_count/0\", spider=spider)\n \n-    def _filter(self, request: Any, response: Response, spider: Spider) -> bool:\n-        if not isinstance(request, Request):\n-            return True\n+    def get_processed_request(\n+        self, request: Request, response: Response\n+    ) -> Request | None:\n         depth = response.meta[\"depth\"] + 1\n         request.meta[\"depth\"] = depth\n         if self.prio:\n@@ -79,10 +83,12 @@ class DepthMiddleware:\n             logger.debug(\n                 \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                 {\"maxdepth\": self.maxdepth, \"requrl\": request.url},\n-                extra={\"spider\": spider},\n+                extra={\"spider\": self.crawler.spider},\n             )\n-            return False\n+            return None\n         if self.verbose_stats:\n-            self.stats.inc_value(f\"request_depth_count/{depth}\", spider=spider)\n-        self.stats.max_value(\"request_depth_max\", depth, spider=spider)\n-        return True\n+            self.stats.inc_value(\n+                f\"request_depth_count/{depth}\", spider=self.crawler.spider\n+            )\n+        self.stats.max_value(\"request_depth_max\", depth, spider=self.crawler.spider)\n+        return request\n\n@@ -9,11 +9,11 @@ from __future__ import annotations\n import logging\n import re\n import warnings\n-from typing import TYPE_CHECKING, Any\n+from typing import TYPE_CHECKING\n \n from scrapy import Spider, signals\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.http import Request, Response\n+from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n from scrapy.utils.httpobj import urlparse_cached\n \n warnings.warn(\n@@ -23,61 +23,52 @@ warnings.warn(\n )\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, Iterable\n-\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n+    from scrapy.http import Request, Response\n     from scrapy.statscollectors import StatsCollector\n \n \n logger = logging.getLogger(__name__)\n \n \n-class OffsiteMiddleware:\n-    def __init__(self, stats: StatsCollector):\n+class OffsiteMiddleware(BaseSpiderMiddleware):\n+    crawler: Crawler\n+\n+    def __init__(self, stats: StatsCollector):  # pylint: disable=super-init-not-called\n         self.stats: StatsCollector = stats\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.stats\n         o = cls(crawler.stats)\n+        o.crawler = crawler\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n-    def process_spider_output(\n-        self, response: Response, result: Iterable[Any], spider: Spider\n-    ) -> Iterable[Any]:\n-        return (r for r in result if self._filter(r, spider))\n-\n-    async def process_spider_output_async(\n-        self, response: Response, result: AsyncIterable[Any], spider: Spider\n-    ) -> AsyncIterable[Any]:\n-        async for r in result:\n-            if self._filter(r, spider):\n-                yield r\n-\n-    def _filter(self, request: Any, spider: Spider) -> bool:\n-        if not isinstance(request, Request):\n-            return True\n+    def get_processed_request(\n+        self, request: Request, response: Response\n+    ) -> Request | None:\n+        assert self.crawler.spider\n         if (\n             request.dont_filter\n             or request.meta.get(\"allow_offsite\")\n-            or self.should_follow(request, spider)\n+            or self.should_follow(request, self.crawler.spider)\n         ):\n-            return True\n+            return request\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n             self.domains_seen.add(domain)\n             logger.debug(\n                 \"Filtered offsite request to %(domain)r: %(request)s\",\n                 {\"domain\": domain, \"request\": request},\n-                extra={\"spider\": spider},\n+                extra={\"spider\": self.crawler.spider},\n             )\n-            self.stats.inc_value(\"offsite/domains\", spider=spider)\n-        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n-        return False\n+            self.stats.inc_value(\"offsite/domains\", spider=self.crawler.spider)\n+        self.stats.inc_value(\"offsite/filtered\", spider=self.crawler.spider)\n+        return None\n \n     def should_follow(self, request: Request, spider: Spider) -> bool:\n         regex = self.host_regex\n\n@@ -6,7 +6,7 @@ originated it.\n from __future__ import annotations\n \n import warnings\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, cast\n from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n@@ -14,13 +14,12 @@ from w3lib.url import safe_url_string\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n+from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, Iterable\n-\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -327,8 +326,8 @@ def _load_policy_class(\n         return None\n \n \n-class RefererMiddleware:\n-    def __init__(self, settings: BaseSettings | None = None):\n+class RefererMiddleware(BaseSpiderMiddleware):\n+    def __init__(self, settings: BaseSettings | None = None):  # pylint: disable=super-init-not-called\n         self.default_policy: type[ReferrerPolicy] = DefaultReferrerPolicy\n         if settings is not None:\n             settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n@@ -370,23 +369,13 @@ class RefererMiddleware:\n         cls = _load_policy_class(policy_name, warning_only=True)\n         return cls() if cls else self.default_policy()\n \n-    def process_spider_output(\n-        self, response: Response, result: Iterable[Any], spider: Spider\n-    ) -> Iterable[Any]:\n-        return (self._set_referer(r, response) for r in result)\n-\n-    async def process_spider_output_async(\n-        self, response: Response, result: AsyncIterable[Any], spider: Spider\n-    ) -> AsyncIterable[Any]:\n-        async for r in result:\n-            yield self._set_referer(r, response)\n-\n-    def _set_referer(self, r: Any, response: Response) -> Any:\n-        if isinstance(r, Request):\n-            referrer = self.policy(response, r).referrer(response.url, r.url)\n+    def get_processed_request(\n+        self, request: Request, response: Response\n+    ) -> Request | None:\n+        referrer = self.policy(response, request).referrer(response.url, request.url)\n         if referrer is not None:\n-                r.headers.setdefault(\"Referer\", referrer)\n-        return r\n+            request.headers.setdefault(\"Referer\", referrer)\n+        return request\n \n     def request_scheduled(self, request: Request, spider: Spider) -> None:\n         # check redirected request to patch \"Referer\" header if necessary\n\n@@ -7,72 +7,49 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-import warnings\n-from typing import TYPE_CHECKING, Any\n+from typing import TYPE_CHECKING\n \n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.http import Request, Response\n+from scrapy.exceptions import NotConfigured\n+from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n \n if TYPE_CHECKING:\n-    from collections.abc import AsyncIterable, Iterable\n-\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n-    from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n+    from scrapy.http import Request, Response\n \n \n logger = logging.getLogger(__name__)\n \n \n-class UrlLengthMiddleware:\n-    def __init__(self, maxlength: int):\n+class UrlLengthMiddleware(BaseSpiderMiddleware):\n+    crawler: Crawler\n+\n+    def __init__(self, maxlength: int):  # pylint: disable=super-init-not-called\n         self.maxlength: int = maxlength\n \n-    @classmethod\n-    def from_settings(cls, settings: BaseSettings) -> Self:\n-        warnings.warn(\n-            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        return cls._from_settings(settings)\n-\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls._from_settings(crawler.settings)\n-\n-    @classmethod\n-    def _from_settings(cls, settings: BaseSettings) -> Self:\n-        maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n+        maxlength = crawler.settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\n-        return cls(maxlength)\n+        o = cls(maxlength)\n+        o.crawler = crawler\n+        return o\n \n-    def process_spider_output(\n-        self, response: Response, result: Iterable[Any], spider: Spider\n-    ) -> Iterable[Any]:\n-        return (r for r in result if self._filter(r, spider))\n-\n-    async def process_spider_output_async(\n-        self, response: Response, result: AsyncIterable[Any], spider: Spider\n-    ) -> AsyncIterable[Any]:\n-        async for r in result:\n-            if self._filter(r, spider):\n-                yield r\n-\n-    def _filter(self, request: Any, spider: Spider) -> bool:\n-        if isinstance(request, Request) and len(request.url) > self.maxlength:\n+    def get_processed_request(\n+        self, request: Request, response: Response\n+    ) -> Request | None:\n+        if len(request.url) <= self.maxlength:\n+            return request\n         logger.info(\n             \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n             {\"maxlength\": self.maxlength, \"url\": request.url},\n-                extra={\"spider\": spider},\n+            extra={\"spider\": self.crawler.spider},\n         )\n-            assert spider.crawler.stats\n-            spider.crawler.stats.inc_value(\n-                \"urllength/request_ignored_count\", spider=spider\n+        assert self.crawler.stats\n+        self.crawler.stats.inc_value(\n+            \"urllength/request_ignored_count\", spider=self.crawler.spider\n         )\n-            return False\n-        return True\n+        return None\n\n@@ -0,0 +1,120 @@\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+import pytest\n+\n+from scrapy import Request, Spider\n+from scrapy.http import Response\n+from scrapy.spidermiddlewares.base import BaseSpiderMiddleware\n+from scrapy.utils.test import get_crawler\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+\n+@pytest.fixture\n+def crawler() -> Crawler:\n+    return get_crawler(Spider)\n+\n+\n+def test_trivial(crawler):\n+    class TrivialSpiderMiddleware(BaseSpiderMiddleware):\n+        pass\n+\n+    mw = TrivialSpiderMiddleware.from_crawler(crawler)\n+    assert hasattr(mw, \"crawler\")\n+    assert mw.crawler is crawler\n+    test_req = Request(\"data:,\")\n+    spider_output = [test_req, {\"foo\": \"bar\"}]\n+    processed = list(\n+        mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n+    )\n+    assert processed == [test_req, {\"foo\": \"bar\"}]\n+\n+\n+def test_processed_request(crawler):\n+    class ProcessReqSpiderMiddleware(BaseSpiderMiddleware):\n+        def get_processed_request(\n+            self, request: Request, response: Response\n+        ) -> Request | None:\n+            if request.url == \"data:2,\":\n+                return None\n+            if request.url == \"data:3,\":\n+                return Request(\"data:30,\")\n+            return request\n+\n+    mw = ProcessReqSpiderMiddleware.from_crawler(crawler)\n+    test_req1 = Request(\"data:1,\")\n+    test_req2 = Request(\"data:2,\")\n+    test_req3 = Request(\"data:3,\")\n+    spider_output = [test_req1, {\"foo\": \"bar\"}, test_req2, test_req3]\n+    processed = list(\n+        mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n+    )\n+    assert len(processed) == 3\n+    assert isinstance(processed[0], Request)\n+    assert processed[0].url == \"data:1,\"\n+    assert processed[1] == {\"foo\": \"bar\"}\n+    assert isinstance(processed[2], Request)\n+    assert processed[2].url == \"data:30,\"\n+\n+\n+def test_processed_item(crawler):\n+    class ProcessItemSpiderMiddleware(BaseSpiderMiddleware):\n+        def get_processed_item(self, item: Any, response: Response) -> Any:\n+            if item[\"foo\"] == 2:\n+                return None\n+            if item[\"foo\"] == 3:\n+                item[\"foo\"] = 30\n+            return item\n+\n+    mw = ProcessItemSpiderMiddleware.from_crawler(crawler)\n+    test_req = Request(\"data:,\")\n+    spider_output = [{\"foo\": 1}, {\"foo\": 2}, test_req, {\"foo\": 3}]\n+    processed = list(\n+        mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n+    )\n+    assert processed == [{\"foo\": 1}, test_req, {\"foo\": 30}]\n+\n+\n+def test_processed_both(crawler):\n+    class ProcessBothSpiderMiddleware(BaseSpiderMiddleware):\n+        def get_processed_request(\n+            self, request: Request, response: Response\n+        ) -> Request | None:\n+            if request.url == \"data:2,\":\n+                return None\n+            if request.url == \"data:3,\":\n+                return Request(\"data:30,\")\n+            return request\n+\n+        def get_processed_item(self, item: Any, response: Response) -> Any:\n+            if item[\"foo\"] == 2:\n+                return None\n+            if item[\"foo\"] == 3:\n+                item[\"foo\"] = 30\n+            return item\n+\n+    mw = ProcessBothSpiderMiddleware.from_crawler(crawler)\n+    test_req1 = Request(\"data:1,\")\n+    test_req2 = Request(\"data:2,\")\n+    test_req3 = Request(\"data:3,\")\n+    spider_output = [\n+        test_req1,\n+        {\"foo\": 1},\n+        {\"foo\": 2},\n+        test_req2,\n+        {\"foo\": 3},\n+        test_req3,\n+    ]\n+    processed = list(\n+        mw.process_spider_output(Response(\"data:,\"), spider_output, crawler.spider)\n+    )\n+    assert len(processed) == 4\n+    assert isinstance(processed[0], Request)\n+    assert processed[0].url == \"data:1,\"\n+    assert processed[1] == {\"foo\": 1}\n+    assert processed[2] == {\"foo\": 30}\n+    assert isinstance(processed[3], Request)\n+    assert processed[3].url == \"data:30,\"\n\n@@ -1,19 +1,18 @@\n from scrapy.http import Request, Response\n from scrapy.spidermiddlewares.depth import DepthMiddleware\n from scrapy.spiders import Spider\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.test import get_crawler\n \n \n class TestDepthMiddleware:\n     def setup_method(self):\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"DEPTH_LIMIT\": 1, \"DEPTH_STATS_VERBOSE\": True})\n         self.spider = crawler._create_spider(\"scrapytest.org\")\n \n-        self.stats = StatsCollector(crawler)\n+        self.stats = crawler.stats\n         self.stats.open_spider(self.spider)\n \n-        self.mw = DepthMiddleware(1, self.stats, True)\n+        self.mw = DepthMiddleware.from_crawler(crawler)\n \n     def test_process_spider_output(self):\n         req = Request(\"http://scrapytest.org\")\n\n@@ -10,7 +10,7 @@ from scrapy.utils.test import get_crawler\n class TestOffsiteMiddleware:\n     def setup_method(self):\n         crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider(**self._get_spiderargs())\n+        self.spider = crawler.spider = crawler._create_spider(**self._get_spiderargs())\n         self.mw = OffsiteMiddleware.from_crawler(crawler)\n         self.mw.spider_opened(self.spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
