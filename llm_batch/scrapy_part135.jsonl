{"custom_id": "scrapy#1dfbe5d7a847ca7b9c0e9afaf51b71c82604031f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 221 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -66,6 +66,17 @@ class ListSpiders(WsResource):\n         spiders = get_spider_list(project, runner=self.root.runner)\n         return {\"status\": \"ok\", \"spiders\": spiders}\n \n+class ListJobs(WsResource):\n+\n+    def render_POST(self, txrequest):\n+        project = txrequest.args['project'][0]\n+        spiders = self.root.launcher.processes.values()\n+        jlist = list()\n+        for s in spiders:\n+            if project == s.project:\n+                jlist.append({\"job\": {\"id\":s.job, \"spider\": s.spider}})\n+        return {\"status\":\"ok\", \"jobs\": jlist}\n+\n class DeleteProject(WsResource):\n \n     def render_POST(self, txrequest):\n@@ -84,13 +95,3 @@ class DeleteVersion(DeleteProject):\n         version = txrequest.args['version'][0]\n         self._delete_version(project, version)\n         return {\"status\": \"ok\"}\n-\n-class ListJobs(WsResource):\n-    def render_POST(self, txrequest):\n-        project = txrequest.args['project'][0]\n-        spiders = self.root.launcher.processes.values()\n-        jlist = list()\n-        for s in spiders:\n-            if project == s.project:\n-                jlist.append({\"job\": {\"id\":s.job, \"spider\": s.spider}})\n-        return {\"status\":\"ok\", \"jobs\": jlist}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#150f82e6006e80909084399c57303c1391db94ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 435 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,6 +16,8 @@ class Launcher(Service):\n \n     def __init__(self, config, app):\n         self.processes = {}\n+        self.finished = []\n+        self.finished_to_keep = config.getint('finished_to_keep', 100)\n         self.max_proc = config.getint('max_proc', 0)\n         if not self.max_proc:\n             self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)\n@@ -47,7 +49,9 @@ class Launcher(Service):\n         self.processes[slot] = pp\n \n     def _process_finished(self, _, slot):\n-        self.processes.pop(slot)\n+        process = self.processes.pop(slot)\n+        self.finished.append(process)\n+        del self.finished[:-self.finished_to_keep] # keep last 100 finished jobs\n         self._wait_for_project(slot)\n \n \n\n@@ -68,14 +68,15 @@ class ListSpiders(WsResource):\n \n class ListJobs(WsResource):\n \n-    def render_POST(self, txrequest):\n+    def render_GET(self, txrequest):\n         project = txrequest.args['project'][0]\n         spiders = self.root.launcher.processes.values()\n-        jlist = list()\n-        for s in spiders:\n-            if project == s.project:\n-                jlist.append({\"job\": {\"id\":s.job, \"spider\": s.spider}})\n-        return {\"status\":\"ok\", \"jobs\": jlist}\n+        running = [{\"id\": s.job, \"spider\": s.spider} for s in spiders if s.project == project]\n+        queue = self.root.poller.queues[project]\n+        pending = [{\"id\": x[\"_job\"], \"spider\": x[\"name\"]} for x in queue.list()]\n+        finished = [{\"id\": s.job, \"spider\": s.spider} for s in self.root.launcher.finished\n+            if s.project == project]\n+        return {\"status\":\"ok\", \"pending\": pending, \"running\": running, \"finished\": finished}\n \n class DeleteProject(WsResource):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#906418803503dac01ffa67ecb1744e365628f13d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 203 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import sys, os\n+import sys\n from datetime import datetime\n \n from twisted.internet import reactor, defer, protocol, error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2966eebc7ecafa9562bf6707992c006aa6ff40b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 482 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n+from scrapy.stats import stats\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n@@ -43,13 +44,17 @@ class HttpCacheMiddleware(object):\n         response = self.storage.retrieve_response(spider, request)\n         if response and self.is_cacheable_response(response):\n             response.flags.append('cached')\n+            stats.inc_value('httpcache/hit', spider=spider)\n             return response\n-        elif self.ignore_missing:\n+\n+        stats.inc_value('httpcache/miss', spider=spider)\n+        if self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n         if self.is_cacheable(request) and self.is_cacheable_response(response):\n             self.storage.store_response(spider, request, response)\n+            stats.inc_value('httpcache/store', spider=spider)\n         return response\n \n     def is_cacheable_response(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#503fdf39fe6417436a675de09cfac9f2c4114194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 486 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,7 +52,9 @@ class HttpCacheMiddleware(object):\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if self.is_cacheable(request) and self.is_cacheable_response(response):\n+        if (self.is_cacheable(request)\n+            and self.is_cacheable_response(response)\n+            and 'cached' not in response.flags):\n             self.storage.store_response(spider, request, response)\n             stats.inc_value('httpcache/store', spider=spider)\n         return response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93eb5b32ddd82ca161f5a5eed054c82de69145d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 170 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import unittest, tempfile, shutil, time\n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spider import BaseSpider\n from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware\n+from scrapy.stats import stats\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n \n@@ -16,8 +17,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com', headers={'User-Agent': 'test'})\n         self.response = Response('http://www.example.com', headers={'Content-Type': 'text/html'}, body='test body', status=202)\n+        stats.open_spider(self.spider)\n \n     def tearDown(self):\n+        stats.close_spider(self.spider, '')\n         shutil.rmtree(self.tmpdir)\n \n     def _get_settings(self, **new_settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6220b8e9503efcc00557b4988bee87049c5c9bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 181 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -82,14 +82,10 @@ scripts = ['bin/scrapy']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n-if os.environ.get('SCRAPY_VERSION_FROM_HG'):\n-    rev = Popen([\"hg\", \"tip\", \"--template\", \"{rev}\"], stdout=PIPE).communicate()[0]\n+if os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n+    v = Popen(\"git describe\", shell=True, stdout=PIPE).communicate()[0]\n     with open('scrapy/__init__.py', 'a') as f:\n-        f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev)\n-elif os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n-    rev = Popen(\"git log --oneline | wc -l\", shell=True, stdout=PIPE).communicate()[0]\n-    with open('scrapy/__init__.py', 'a') as f:\n-        f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev.strip())\n+        f.write(\"\\n__version__ = '%s'\" % v.strip())\n version = __import__('scrapy').__version__\n \n setup_args = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db92bc8c40c5f56e965c9c7f165be82f9adee2e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 15, 0)\n-__version__ = \"0.15.0\"\n+version_info = (0, 15, 1)\n+__version__ = \"0.15.1\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb44f303a9c55b728379eed453e337d562f7b817", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,9 +16,8 @@ def build(suffix):\n         with open(ifn, 'w') as of:\n             of.write(s)\n \n-    env={'SCRAPY_VERSION_FROM_GIT': '1'}\n     check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)+$(date +%s) \"Automatic build\"', \\\n-        shell=True, env=env)\n+        shell=True)\n     check_call('debuild -us -uc -b', shell=True)\n \n def clean(suffix):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10ed28b9d02ddc0b216889c7d37d452ac4b11324", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 62 | Churn Cumulative: 358 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ import re\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n-from scrapy.utils.gz import gunzip\n+from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy import log\n \n class SitemapSpider(BaseSpider):\n@@ -29,12 +29,9 @@ class SitemapSpider(BaseSpider):\n             for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n-            if isinstance(response, XmlResponse):\n-                body = response.body\n-            elif is_gzipped(response):\n-                body = gunzip(response.body)\n-            else:\n-                log.msg(\"Ignoring non-XML sitemap: %s\" % response, log.WARNING)\n+            body = self._get_sitemap_body(response)\n+            if body is None:\n+                log.msg(\"Ignoring invalid sitemap: %s\" % response, log.WARNING)\n                 return\n \n             s = Sitemap(body)\n@@ -49,9 +46,18 @@ class SitemapSpider(BaseSpider):\n                             yield Request(loc, callback=c)\n                             break\n \n-def is_gzipped(response):\n-    ctype = response.headers.get('Content-Type', '')\n-    return ctype in ('application/x-gzip', 'application/gzip')\n+    def _get_sitemap_body(self, response):\n+        \"\"\"Return the sitemap body contained in the given response, or None if the\n+        response is not a sitemap.\n+        \"\"\"\n+        if isinstance(response, XmlResponse):\n+            return response.body\n+        elif is_gzipped(response):\n+            return gunzip(response.body)\n+        elif response.url.endswith('.xml'):\n+            return response.body\n+        elif response.url.endswith('.xml.gz'):\n+            return gunzip(response.body)\n \n def regex(x):\n     if isinstance(x, basestring):\n\n@@ -1,10 +1,12 @@\n from __future__ import with_statement\n \n-import warnings\n+import gzip, warnings\n+from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n from scrapy.spider import BaseSpider\n+from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider\n \n@@ -55,6 +57,33 @@ class SitemapSpiderTest(BaseSpiderTest):\n \n     spider_class = SitemapSpider\n \n+    BODY = \"SITEMAP\"\n+    f = StringIO()\n+    g = gzip.GzipFile(fileobj=f, mode='w+b')\n+    g.write(BODY)\n+    g.close()\n+    GZBODY = f.getvalue()\n+\n+    def test_get_sitemap_body(self):\n+        spider = self.spider_class(\"example.com\")\n+\n+        r = XmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n+        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+\n+        r = HtmlResponse(url=\"http://www.example.com/\", body=self.BODY)\n+        self.assertEqual(spider._get_sitemap_body(r), None)\n+\n+        r = Response(url=\"http://www.example.com/favicon.ico\", body=self.BODY)\n+        self.assertEqual(spider._get_sitemap_body(r), None)\n+\n+        r = Response(url=\"http://www.example.com/sitemap\", body=self.GZBODY, headers={\"content-type\": \"application/gzip\"})\n+        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+\n+        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=self.BODY)\n+        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n+\n+        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n+        self.assertEqual(spider._get_sitemap_body(r), self.BODY)\n \n if __name__ == '__main__':\n     unittest.main()\n\n@@ -21,3 +21,8 @@ def gunzip(data):\n             else:\n                 raise\n     return output\n+\n+def is_gzipped(response):\n+    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n+    ctype = response.headers.get('Content-Type', '')\n+    return ctype in ('application/x-gzip', 'application/gzip')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f07e968a936df4f99b2f3278acbf90cbf380dd3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 49/41 | Churn Δ: 56 | Churn Cumulative: 569 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,11 @@ class ISpiderQueue(Interface):\n \n         This method can return a deferred. \"\"\"\n \n+    def remove(func):\n+        \"\"\"Remove all elements from the queue for which func(element) is true,\n+        and return the number of removed elements.\n+        \"\"\"\n+\n     def clear():\n         \"\"\"Clear the queue.\n \n\n@@ -26,5 +26,8 @@ class SqliteSpiderQueue(object):\n     def list(self):\n         return [x[0] for x in self.q]\n \n+    def remove(self, func):\n+        return self.q.remove(func)\n+\n     def clear(self):\n         self.q.clear()\n\n@@ -119,6 +119,20 @@ class SqlitePriorityQueue(object):\n         self.conn.commit()\n         return self.decode(msg)\n \n+    def remove(self, func):\n+        q = \"select id, message from %s\" % self.table\n+        n = 0\n+        for id, msg in self.conn.execute(q):\n+            if func(self.decode(msg)):\n+                q = \"delete from %s where id=?\" % self.table\n+                c = self.conn.execute(q, (id,))\n+                if not c.rowcount: # record vanished, so let's try again\n+                    self.conn.rollback()\n+                    return self.remove(func)\n+                n += 1\n+        self.conn.commit()\n+        return n\n+\n     def clear(self):\n         self.conn.execute(\"delete from %s\" % self.table)\n         self.conn.commit()\n\n@@ -125,6 +125,20 @@ class SqlitePriorityQueueTest(unittest.TestCase):\n         self.failUnlessEqual(len(self.q), 0)\n         self.failUnlessEqual(list(self.q), [])\n \n+    def test_remove(self):\n+        self.failUnlessEqual(len(self.q), 0)\n+        self.failUnlessEqual(list(self.q), [])\n+        msg1 = \"good message 1\"\n+        msg2 = \"bad message 2\"\n+        msg3 = \"good message 3\"\n+        msg4 = \"bad message 4\"\n+        self.q.put(msg1)\n+        self.q.put(msg2)\n+        self.q.put(msg3)\n+        self.q.put(msg4)\n+        self.q.remove(lambda x: x.startswith(\"bad\"))\n+        self.failUnlessEqual(list(self.q), [(msg1, 0.0), (msg3, 0.0)])\n+\n     def test_types(self):\n         for x in self.supported_values:\n             self.q.put(x)\n\n@@ -34,6 +34,25 @@ class Schedule(WsResource):\n         self.root.scheduler.schedule(project, spider, **args)\n         return {\"status\": \"ok\", \"jobid\": jobid}\n \n+class Cancel(WsResource):\n+\n+    def render_POST(self, txrequest):\n+        args = dict((k, v[0]) for k, v in txrequest.args.items())\n+        project = args['project']\n+        jobid = args['job']\n+        signal = args.get('signal', 'TERM')\n+        prevstate = None\n+        queue = self.root.poller.queues[project]\n+        c = queue.remove(lambda x: x[\"_job\"] == jobid)\n+        if c:\n+            prevstate = \"pending\"\n+        spiders = self.root.launcher.processes.values()\n+        for s in spiders:\n+            if s.job == jobid:\n+                s.transport.signalProcess(signal)\n+                prevstate = \"running\"\n+        return {\"status\": \"ok\", \"prevstate\": prevstate}\n+\n class AddVersion(WsResource):\n \n     def render_POST(self, txrequest):\n\n@@ -16,6 +16,7 @@ class Root(resource.Resource):\n         self.app = app\n         self.putChild('', Home(self))\n         self.putChild('schedule.json', webservice.Schedule(self))\n+        self.putChild('cancel.json', webservice.Cancel(self))\n         self.putChild('addversion.json', webservice.AddVersion(self))\n         self.putChild('listprojects.json', webservice.ListProjects(self))\n         self.putChild('listversions.json', webservice.ListVersions(self))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#485bc180dfd1c824de00ff689ca4d44a8f449770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 29 | Churn Cumulative: 359 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -50,6 +50,7 @@ class Launcher(Service):\n \n     def _process_finished(self, _, slot):\n         process = self.processes.pop(slot)\n+        process.end_time = datetime.now()\n         self.finished.append(process)\n         del self.finished[:-self.finished_to_keep] # keep last 100 finished jobs\n         self._wait_for_project(slot)\n@@ -64,6 +65,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.spider = spider\n         self.job = job\n         self.start_time = datetime.now()\n+        self.end_time = None\n         self.env = env\n         self.logfile = env['SCRAPY_LOG_FILE']\n         self.deferred = defer.Deferred()\n\n@@ -25,7 +25,7 @@ class Root(resource.Resource):\n         self.putChild('delversion.json', webservice.DeleteVersion(self))\n         self.putChild('listjobs.json', webservice.ListJobs(self))\n         self.putChild('logs', static.File(logsdir, 'text/plain'))\n-        self.putChild('procmon', ProcessMonitor(self))\n+        self.putChild('jobs', Jobs(self))\n         self.update_projects()\n \n     def update_projects(self):\n@@ -67,7 +67,7 @@ class Home(resource.Resource):\n <h1>Scrapyd</h1>\n <p>Available projects: <b>%(projects)s</b></p>\n <ul>\n-<li><a href=\"/procmon\">Process monitor</a></li>\n+<li><a href=\"/jobs\">Jobs</a></li>\n <li><a href=\"/logs/\">Logs</li>\n <li><a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Documentation</a></li>\n </ul>\n@@ -86,7 +86,7 @@ monitoring)</p>\n \"\"\" % vars\n \n \n-class ProcessMonitor(resource.Resource):\n+class Jobs(resource.Resource):\n \n     def __init__(self, root):\n         resource.Resource.__init__(self)\n@@ -95,12 +95,19 @@ class ProcessMonitor(resource.Resource):\n     def render(self, txrequest):\n         s = \"<html><head><title>Scrapyd</title></title>\"\n         s += \"<body>\"\n-        s += \"<h1>Process monitor</h1>\"\n+        s += \"<h1>Jobs</h1>\"\n         s += \"<p><a href='..'>Go back</a></p>\"\n         s += \"<table border='1'>\"\n-        s += \"<tr>\"\n         s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th>\"\n+        s += \"<tr><th colspan='6' style='background-color: #ddd'>Pending</th></tr>\"\n+        for project, queue in self.root.poller.queues.items():\n+            for m in queue.list():\n+                s += \"<tr>\"\n+                s += \"<td>%s</td>\" % project\n+                s += \"<td>%s</td>\" % str(m['name'])\n+                s += \"<td>%s</td>\" % str(m['_job'])\n                 s += \"</tr>\"\n+        s += \"<tr><th colspan='6' style='background-color: #ddd'>Running</th></tr>\"\n         for p in self.root.launcher.processes.values():\n             s += \"<tr>\"\n             for a in ['project', 'spider', 'job', 'pid']:\n@@ -108,8 +115,16 @@ class ProcessMonitor(resource.Resource):\n             s += \"<td>%s</td>\" % (datetime.now() - p.start_time)\n             s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n             s += \"</tr>\"\n+        s += \"<tr><th colspan='6' style='background-color: #ddd'>Finished</th></tr>\"\n+        for p in self.root.launcher.finished:\n+            s += \"<tr>\"\n+            for a in ['project', 'spider', 'job']:\n+                s += \"<td>%s</td>\" % getattr(p, a)\n+            s += \"<td></td>\"\n+            s += \"<td>%s</td>\" % (p.end_time - p.start_time)\n+            s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n+            s += \"</tr>\"\n         s += \"</table>\"\n         s += \"</body>\"\n         s += \"</html>\"\n         return s\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0693694bcf89e3e5a48074454ab6e3602a3d98e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 156 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ class Home(resource.Resource):\n <ul>\n <li><a href=\"/jobs\">Jobs</a></li>\n <li><a href=\"/logs/\">Logs</li>\n-<li><a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Documentation</a></li>\n+<li><a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\">Documentation</a></li>\n </ul>\n \n <h2>How to schedule a spider?</h2>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbda33efa6ddfde96421724622f0ff0c15b02b57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 18 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 557 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -11,7 +11,8 @@ class Environment(object):\n     def __init__(self, config, initenv=os.environ):\n         self.dbs_dir = config.get('dbs_dir', 'dbs')\n         self.logs_dir = config.get('logs_dir', 'logs')\n-        self.logs_to_keep = config.getint('logs_to_keep', 5)\n+        self.items_dir = config.get('items_dir', 'items')\n+        self.jobs_to_keep = config.getint('jobs_to_keep', 5)\n         if config.cp.has_section('settings'):\n             self.settings = dict(config.cp.items('settings'))\n         else:\n@@ -29,15 +30,17 @@ class Environment(object):\n             env['SCRAPY_SETTINGS_MODULE'] = self.settings[project]\n         dbpath = os.path.join(self.dbs_dir, '%s.db' % project)\n         env['SCRAPY_SQLITE_DB'] = dbpath\n-        env['SCRAPY_LOG_FILE'] = self._get_log_file(message)\n+        env['SCRAPY_LOG_FILE'] = self._get_file(message, self.logs_dir, 'log')\n+        env['SCRAPY_FEED_URI'] = self._get_file(message, self.items_dir, 'jl')\n         return env\n \n-    def _get_log_file(self, message):\n-        logsdir = os.path.join(self.logs_dir, message['_project'], \\\n+    def _get_file(self, message, dir, ext):\n+        logsdir = os.path.join(dir, message['_project'], \\\n             message['_spider'])\n         if not os.path.exists(logsdir):\n             os.makedirs(logsdir)\n-        to_delete = sorted((os.path.join(logsdir, x) for x in os.listdir(logsdir)), key=os.path.getmtime)[:-self.logs_to_keep]\n+        to_delete = sorted((os.path.join(logsdir, x) for x in \\\n+            os.listdir(logsdir)), key=os.path.getmtime)[:-self.jobs_to_keep]\n         for x in to_delete:\n             os.remove(x)\n-        return os.path.join(logsdir, \"%s.log\" % message['_job'])\n+        return os.path.join(logsdir, \"%s.%s\" % (message['_job'], ext))\n\n@@ -68,6 +68,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.end_time = None\n         self.env = env\n         self.logfile = env['SCRAPY_LOG_FILE']\n+        self.itemsfile = env['SCRAPY_FEED_URI']\n         self.deferred = defer.Deferred()\n \n     def outReceived(self, data):\n@@ -88,6 +89,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"project=%r spider=%r job=%r pid=%r log=%r\" % (self.project, \\\n-            self.spider, self.job, self.pid, self.logfile)\n+        msg += \"project=%r spider=%r job=%r pid=%r log=%r items=%r\" % (self.project, \\\n+            self.spider, self.job, self.pid, self.logfile, self.itemsfile)\n         log.msg(msg, system=\"Launcher\")\n\n@@ -18,9 +18,10 @@ def _get_config():\n     conf = {\n         'eggs_dir': os.path.join(datadir, 'eggs'),\n         'logs_dir': os.path.join(datadir, 'logs'),\n+        'items_dir': os.path.join(datadir, 'items'),\n         'dbs_dir': os.path.join(datadir, 'dbs'),\n     }\n-    for k in ['eggs_dir', 'logs_dir', 'dbs_dir']: # create dirs\n+    for k in ['eggs_dir', 'logs_dir', 'items_dir', 'dbs_dir']: # create dirs\n         d = conf[k]\n         if not os.path.exists(d):\n             os.makedirs(d)\n@@ -28,6 +29,7 @@ def _get_config():\n [scrapyd]\n eggs_dir = %(eggs_dir)s\n logs_dir = %(logs_dir)s\n+items_dir = %(items_dir)s\n dbs_dir  = %(dbs_dir)s\n     \"\"\" % conf\n     return Config(extra_sources=[StringIO(scrapyd_conf)])\n\n@@ -31,4 +31,5 @@ class EnvironmentTest(unittest.TestCase):\n         self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID.log')))\n+        self.assert_(env['SCRAPY_FEED_URI'].endswith(os.path.join('mybot', 'myspider', 'ID.jl')))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n\n@@ -13,6 +13,7 @@ class Root(resource.Resource):\n         self.debug = config.getboolean('debug', False)\n         self.runner = config.get('runner')\n         logsdir = config.get('logs_dir')\n+        itemsdir = config.get('items_dir')\n         self.app = app\n         self.putChild('', Home(self))\n         self.putChild('schedule.json', webservice.Schedule(self))\n@@ -25,6 +26,7 @@ class Root(resource.Resource):\n         self.putChild('delversion.json', webservice.DeleteVersion(self))\n         self.putChild('listjobs.json', webservice.ListJobs(self))\n         self.putChild('logs', static.File(logsdir, 'text/plain'))\n+        self.putChild('items', static.File(itemsdir, 'text/plain'))\n         self.putChild('jobs', Jobs(self))\n         self.update_projects()\n \n@@ -68,6 +70,7 @@ class Home(resource.Resource):\n <p>Available projects: <b>%(projects)s</b></p>\n <ul>\n <li><a href=\"/jobs\">Jobs</a></li>\n+<li><a href=\"/items/\">Items</li>\n <li><a href=\"/logs/\">Logs</li>\n <li><a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\">Documentation</a></li>\n </ul>\n@@ -98,8 +101,8 @@ class Jobs(resource.Resource):\n         s += \"<h1>Jobs</h1>\"\n         s += \"<p><a href='..'>Go back</a></p>\"\n         s += \"<table border='1'>\"\n-        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th>\"\n-        s += \"<tr><th colspan='6' style='background-color: #ddd'>Pending</th></tr>\"\n+        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th><th>Items</th>\"\n+        s += \"<tr><th colspan='7' style='background-color: #ddd'>Pending</th></tr>\"\n         for project, queue in self.root.poller.queues.items():\n             for m in queue.list():\n                 s += \"<tr>\"\n@@ -107,15 +110,16 @@ class Jobs(resource.Resource):\n                 s += \"<td>%s</td>\" % str(m['name'])\n                 s += \"<td>%s</td>\" % str(m['_job'])\n                 s += \"</tr>\"\n-        s += \"<tr><th colspan='6' style='background-color: #ddd'>Running</th></tr>\"\n+        s += \"<tr><th colspan='7' style='background-color: #ddd'>Running</th></tr>\"\n         for p in self.root.launcher.processes.values():\n             s += \"<tr>\"\n             for a in ['project', 'spider', 'job', 'pid']:\n                 s += \"<td>%s</td>\" % getattr(p, a)\n             s += \"<td>%s</td>\" % (datetime.now() - p.start_time)\n             s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n+            s += \"<td><a href='/items/%s/%s/%s.jl'>Items</a></td>\" % (p.project, p.spider, p.job)\n             s += \"</tr>\"\n-        s += \"<tr><th colspan='6' style='background-color: #ddd'>Finished</th></tr>\"\n+        s += \"<tr><th colspan='7' style='background-color: #ddd'>Finished</th></tr>\"\n         for p in self.root.launcher.finished:\n             s += \"<tr>\"\n             for a in ['project', 'spider', 'job']:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#531fa95f987c6ccd38be1833ccada51f410534f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from scrapyd import get_application\n from scrapyd.config import Config\n \n def _get_config():\n-    datadir = os.path.join(project_data_dir(), '.scrapy', 'scrapyd')\n+    datadir = os.path.join(project_data_dir(), 'scrapyd')\n     conf = {\n         'eggs_dir': os.path.join(datadir, 'eggs'),\n         'logs_dir': os.path.join(datadir, 'logs'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5bf6bf880d94c81bba6432cb329d1ab84b39dcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 64 | Churn Cumulative: 73 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.2894736842105263\n\nDIFF:\n@@ -1,11 +1,15 @@\n-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy\n+import time\n+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n \n from scrapy.utils.httpobj import urlparse_cached\n \n class CookieJar(object):\n-    def __init__(self, policy=None):\n-        self.jar = _CookieJar(policy or DefaultCookiePolicy())\n+    def __init__(self, policy=None, check_expired_frequency=10000):\n+        self.policy = policy or DefaultCookiePolicy()\n+        self.jar = _CookieJar(self.policy)\n         self.jar._cookies_lock = _DummyLock()\n+        self.check_expired_frequency = check_expired_frequency\n+        self.processed = 0\n \n     def extract_cookies(self, response, request):\n         wreq = WrappedRequest(request)\n@@ -14,7 +18,41 @@ class CookieJar(object):\n \n     def add_cookie_header(self, request):\n         wreq = WrappedRequest(request)\n-        self.jar.add_cookie_header(wreq)\n+        self.policy._now = self.jar._now = int(time.time())\n+        \n+        # the cookiejar implementation iterates through all domains\n+        # instead we restrict to potential matches on the domain\n+        \n+        req_host = urlparse_cached(request).netloc\n+        if not IPV4_RE.search(req_host):\n+            hosts = potential_domain_matches(req_host)\n+            if req_host.find(\".\") == -1:\n+                hosts += req_host + \".local\"\n+        else:\n+            hosts = [req_host]\n+\n+        cookies = []\n+        for host in hosts:\n+            if host in self.jar._cookies:\n+                cookies += self.jar._cookies_for_domain(host, wreq)\n+\n+        attrs = self.jar._cookie_attrs(cookies)\n+        if attrs:\n+            if not wreq.has_header(\"Cookie\"):\n+                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n+\n+        # if necessary, advertise that we know RFC 2965\n+        if (self.policy.rfc2965 and not self.policy.hide_cookie2 and\n+            not request.has_header(\"Cookie2\")):\n+            for cookie in cookies:\n+                if cookie.version != 1:\n+                    request.add_unredirected_header(\"Cookie2\", '$Version=\"1\"')\n+                    break\n+        \n+        self.processed += 1\n+        if self.processed % self.check_expired_frequency == 0:\n+            # This is still quite inefficient for large number of cookies\n+            self.jar.clear_expired_cookies()\n     \n     @property\n     def _cookies(self):\n@@ -47,6 +85,24 @@ class CookieJar(object):\n         self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))\n \n \n+def potential_domain_matches(domain):\n+    \"\"\"Potential domain matches for a cookie\n+    \n+    >>> potential_domain_matches('www.example.com')\n+    ['www.example.com', 'example.com']\n+\n+    \"\"\"\n+    matches = [domain]\n+    try:\n+        start = domain.index('.') + 1\n+        end = domain.rindex('.')\n+        while start < end:\n+            matches.append(domain[start:])\n+            start = domain.index('.', start) + 1\n+    except ValueError:\n+        pass\n+    return matches\n+\n class _DummyLock(object):\n     def acquire(self):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ee523b14aaf613c053008ac5a389aaadabbfd4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 169 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -127,6 +127,7 @@ class Jobs(resource.Resource):\n             s += \"<td></td>\"\n             s += \"<td>%s</td>\" % (p.end_time - p.start_time)\n             s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n+            s += \"<td><a href='/items/%s/%s/%s.jl'>Items</a></td>\" % (p.project, p.spider, p.job)\n             s += \"</tr>\"\n         s += \"</table>\"\n         s += \"</body>\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6a2120715ec97c11e0fa1d3d9448f99ad1302c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 201 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ class XPathSelector(object_ref):\n     __slots__ = ['doc', 'xmlNode', 'expr', '__weakref__']\n \n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n-        if parent:\n+        if parent is not None:\n             self.doc = parent.doc\n             self.xmlNode = node\n         elif response:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59cf9d9b1afef4a317b8d3a2933f330c9b4cf0cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 251 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,12 +69,15 @@ class AutoThrottle(object):\n         self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n         self.MAX_CONCURRENCY = settings.getint(\"AUTOTHROTTLE_MAX_CONCURRENCY\", 8)\n         self.DEBUG = settings.getint(\"AUTOTHROTTLE_DEBUG\", False)\n+        self.MIN_DOWNLOAD_DELAY = settings.getint(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n     def spider_opened(self, spider):\n+        if hasattr(spider, \"download_delay\"):\n+            self.MIN_DOWNLOAD_DELAY = spider.download_delay\n         spider.download_delay = self.START_DELAY\n         if hasattr(spider, \"max_concurrent_requests\"):\n             self.MAX_CONCURRENCY = spider.max_concurrent_requests\n@@ -124,6 +127,9 @@ class AutoThrottle(object):\n         # if latency is bigger than old delay, then use latency instead of mean. Works better with problematic sites\n         new_delay = (slot.delay + latency) / 2.0 if latency < slot.delay else latency\n \n+        if new_delay < self.MIN_DOWNLOAD_DELAY:\n+            new_delay = self.MIN_DOWNLOAD_DELAY\n+\n         # dont adjust delay if response status != 200 and new delay is smaller than old one,\n         # as error pages (and redirections) are usually small and so tend to reduce latency, thus provoking a positive feedback\n         # by reducing delay instead of increase.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c6a5a9374306f6d2fa374146b65418e01261938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 257 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -68,8 +68,10 @@ class AutoThrottle(object):\n         self.START_DELAY = settings.getfloat(\"AUTOTHROTTLE_START_DELAY\", 5.0)\n         self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n         self.MAX_CONCURRENCY = settings.getint(\"AUTOTHROTTLE_MAX_CONCURRENCY\", 8)\n-        self.DEBUG = settings.getint(\"AUTOTHROTTLE_DEBUG\", False)\n+        self.DEBUG = settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n         self.MIN_DOWNLOAD_DELAY = settings.getint(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\")\n+        self.last_latencies = [self.START_DELAY]\n+        self.last_lat = self.START_DELAY, 0.0\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -83,8 +85,6 @@ class AutoThrottle(object):\n             self.MAX_CONCURRENCY = spider.max_concurrent_requests\n         # override in order to avoid to initialize slot with concurrency > 1\n         spider.max_concurrent_requests = 1\n-        self.last_latencies = [self.START_DELAY]\n-        self.last_lat = self.START_DELAY, 0.0\n \n     def response_received(self, response, spider):\n         slot = self._get_slot(response.request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2840865746a207afe7e98c3139032df0242c2dda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 408 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,10 +8,9 @@ from scrapy.conf import settings\n from scrapy import optional_features\n \n ssl_supported = 'ssl' in optional_features\n-if ssl_supported:\n-    from twisted.internet.ssl import ClientContextFactory\n \n HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n \n \n class HttpDownloadHandler(object):\n\n@@ -9,6 +9,7 @@ from twisted.internet import defer\n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.responsetypes import responsetypes\n+from scrapy import optional_features\n \n \n def _parsed_url_args(parsed):\n@@ -135,3 +136,19 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     def gotHeaders(self, headers):\n         self.headers_time = time()\n         self.response_headers = headers\n+\n+\n+\n+if 'ssl' in optional_features:\n+    from twisted.internet.ssl import ClientContextFactory\n+    from OpenSSL import SSL\n+else:\n+    ClientContextFactory = object\n+\n+\n+class ScrapyClientContextFactory(ClientContextFactory):\n+\n+    def getContext(self):\n+        ctx = ClientContextFactory.getContext(self)\n+        ctx.set_options(SSL.OP_ALL)\n+        return ctx\n\n@@ -64,6 +64,7 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n DOWNLOADER_DEBUG = False\n \n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n+DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.webclient.ScrapyClientContextFactory'\n \n DOWNLOADER_MIDDLEWARES = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb8e98461d0c5806bfc0fe939675fc06099855ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 194 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -147,8 +147,13 @@ else:\n \n \n class ScrapyClientContextFactory(ClientContextFactory):\n+    \"A SSL context factory which is more permissive against SSL bugs.\"\n+    # see https://github.com/scrapy/scrapy/issues/82\n+    # and https://github.com/scrapy/scrapy/issues/26\n \n     def getContext(self):\n         ctx = ClientContextFactory.getContext(self)\n+        # Enable all workarounds to SSL bugs as documented by\n+        # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n         ctx.set_options(SSL.OP_ALL)\n         return ctx\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e18f0db3316accdbb232edd1aab7f097e32f82d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1935 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -106,9 +106,13 @@ class ExecutionEngine(object):\n         if slot.start_requests and not self._needs_backout(spider):\n             try:\n                 request = slot.start_requests.next()\n-                self.crawl(request, spider)\n             except StopIteration:\n                 slot.start_requests = None\n+            except Exception, exc:\n+                log.err(None, 'Obtaining request from start requests', \\\n+                        spider=spider)\n+            else:\n+                self.crawl(request, spider)\n \n         if self.spider_is_idle(spider) and slot.close_if_idle:\n             self._spider_idle(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
