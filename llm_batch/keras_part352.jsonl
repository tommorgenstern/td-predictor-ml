{"custom_id": "keras#b04fc0d0b392368fcc4819fd38a6a09e9feb0da4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 643 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -195,8 +195,8 @@ class MultiHeadAttention(Layer):\n       indicates no attention. Broadcasting can happen for the missing batch\n       dimensions and the head dimension.\n     return_attention_scores: A boolean to indicate whether the output should\n-      be attention output if True, or (attention_output, attention_scores) if\n-      False. Defaults to False.\n+      be `(attention_output, attention_scores)` if `True`, or `attention_output`\n+      if `False`. Defaults to `False`.\n     training: Python boolean indicating whether the layer should behave in\n       training mode (adding dropout) or in inference mode (no dropout).\n       Defaults to either using the training mode of the parent layer/model,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8671ed55bb299fb43838079af07982b51768a39a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 7328 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -65,6 +65,7 @@ _CURRENT_SCRATCH_GRAPH = threading.local()\n # This is a thread local object that will hold the default internal TF session\n # used by Keras. It can be set manually via `set_session(sess)`.\n _SESSION = threading.local()\n+_SESSION.session = None\n \n \n # A global dictionary mapping graph objects to an index of counters used\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06b04aeb745c90c5368be02928b08cc7fd592659", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 27 | Churn Cumulative: 5571 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -701,19 +701,14 @@ class Functional(training_lib.Model):\n                         'The tensor that caused the issue was: ' + str(x.name))\n \n     # Check compatibility of batch sizes of Input Layers.\n-    input_batch_sizes = [\n+    input_batch_sizes = set([\n         training_utils.get_static_batch_size(x._keras_history.layer)\n-        for x in self.inputs\n-    ]\n-    consistent_batch_size = None\n-    for batch_size in input_batch_sizes:\n-      if batch_size is not None:\n-        if (consistent_batch_size is not None and\n-            batch_size != consistent_batch_size):\n-          raise ValueError('The specified batch sizes of the Input Layers'\n-                           ' are incompatible. Found batch sizes: {}'.format(\n-                               input_batch_sizes))\n-        consistent_batch_size = batch_size\n+        for x in self.inputs])\n+    input_batch_sizes.discard(None)\n+    if len(input_batch_sizes) > 1:\n+      logging.warning('Found incompatiable static batch sizes among all the '\n+                      'inputs. Batch sizes: {}'.format(\n+                          sorted(input_batch_sizes)))\n \n     for x in self.outputs:\n       if not hasattr(x, '_keras_history'):\n\n@@ -1093,9 +1093,13 @@ class TrainingTest(keras_parameterized.TestCase):\n     input1 = input_layer.Input(batch_size=2, shape=(10,))\n     input2 = input_layer.Input(batch_size=3, shape=(10,))\n     outputs = MyLayer()([input1, input2])\n-    with self.assertRaisesRegex(ValueError,\n-                                'specified batch sizes of the Input Layers'):\n+    with tf.compat.v1.test.mock.patch.object(\n+        logging, 'warning') as mock_warn:\n       training_module.Model([input1, input2], outputs)\n+      self.assertEqual(\n+          mock_warn.call_args_list[0][0][0],\n+          'Found incompatiable static batch sizes among all the inputs. '\n+          'Batch sizes: [2, 3]')\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_calling_subclass_model_on_different_datasets(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8571475cebd6749fecc543a625aa78b0c0fc24af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 49 | Churn Cumulative: 139 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,9 +14,13 @@\n # ==============================================================================\n \"\"\"Utilities related to distributed training.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+from absl import flags\n from keras import backend\n \n+import tensorflow.compat.v2 as tf\n+\n+FLAGS = flags.FLAGS\n+\n \n # TODO(b/118776054): Currently we support global batch size for TPUStrategy and\n # core MirroredStrategy only. Remove this check when contrib MirroredStrategy is\n@@ -60,3 +64,46 @@ def is_distributed_variable(v):\n   \"\"\"Returns whether `v` is a distributed variable.\"\"\"\n   return (isinstance(v, tf.distribute.DistributedValues) and\n           isinstance(v, tf.Variable))\n+\n+\n+def get_strategy():\n+  \"\"\"Creates a `tf.distribute.Strategy` object from flags.\n+\n+  Example usage:\n+\n+  ```python\n+  strategy = utils.get_strategy()\n+  with strategy.scope():\n+    model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n+\n+  model.compile(...)\n+  train_ds, test_ds = ...\n+  model.fit(train_ds, validation_data=test_ds, epochs=10)\n+  ```\n+\n+  Returns:\n+    `tf.distribute.Strategy` instance.\n+  \"\"\"\n+  cls = FLAGS.keras_distribute_strategy_class\n+  if cls == 'tpu':\n+    tpu_addr = FLAGS.keras_distribute_strategy_tpu_addr\n+    if not tpu_addr:\n+      raise ValueError('Must set flag: keras_distribute_strategy_tpu_addr')\n+    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n+        tpu=tpu_addr)\n+    tf.config.experimental_connect_to_cluster(cluster_resolver)\n+    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n+    strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n+  elif cls == 'multi_worker_mirrored':\n+    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n+  elif cls == 'mirrored':\n+    strategy = tf.distribute.MirroredStrategy()\n+  elif cls == 'parameter_server':\n+    cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n+    strategy = tf.distribute.experimental.ParameterServerStrategy(\n+        cluster_resolver)\n+  elif cls == 'one_device':\n+    strategy = tf.distribute.OneDeviceStrategy('/gpu:0')\n+  else:\n+    raise ValueError('Unknown strategy: {}'.format(cls))\n+  return strategy\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc08c25fe535ec9f18bf92f80406f531bfb30906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 7121 | Contributors (this commit): 93 | Commits (past 90d): 10 | Contributors (cumulative): 93 | DMM Complexity: None\n\nDIFF:\n@@ -1811,7 +1811,8 @@ class EarlyStopping(Callback):\n       if self.baseline is None or self._is_improvement(current, self.baseline):\n         self.wait = 0\n \n-    if self.wait >= self.patience:\n+    #Only stop after first epoch\n+    if self.wait >= self.patience and epoch > 0:\n       self.stopped_epoch = epoch\n       self.model.stop_training = True\n       if self.restore_best_weights and self.best_weights is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1584c68791cb82b18eaa7ffd80a23153a0297b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 76 | Churn Cumulative: 186 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,16 +14,17 @@\n # ==============================================================================\n \"\"\"Tests for keras premade models using tf.distribute.Strategy.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n+\n from keras.engine import sequential\n from keras.layers import core\n from keras.optimizer_v2 import adagrad\n from keras.optimizer_v2 import gradient_descent\n from keras.premade import linear\n from keras.premade import wide_deep\n+from keras.utils import dataset_creator\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n def strategy_combinations_eager_data_fn():\n@@ -32,40 +33,74 @@ def strategy_combinations_eager_data_fn():\n           tf.__internal__.distribute.combinations.default_strategy,\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n-          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n+          tf.__internal__.distribute.combinations\n+          .mirrored_strategy_with_gpu_and_cpu,\n+          tf.__internal__.distribute.combinations\n+          .mirrored_strategy_with_two_gpus,\n+          tf.__internal__.distribute.combinations\n+          .mirrored_strategy_with_two_gpus_no_merge_call,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n+          tf.__internal__.distribute.combinations\n+          .parameter_server_strategy_1worker_2ps_1gpu,\n           # NOTE: TPUStrategy not tested because the models in this test are\n           # sparse and do not work with TPUs.\n       ],\n+      use_dataset_creator=[True, False],\n       mode=['eager'],\n       data_fn=['numpy', 'dataset'])\n \n \n+INPUT_SIZE = 64\n+BATCH_SIZE = 10\n+\n+\n def get_numpy():\n-  inputs = np.random.uniform(low=-5., high=5., size=(64, 2)).astype(np.float32)\n+  inputs = np.random.uniform(\n+      low=-5., high=5., size=(INPUT_SIZE, 2)).astype(np.float32)\n   output = .3 * inputs[:, 0] + .2 * inputs[:, 1]\n   return inputs, output\n \n \n-def get_dataset():\n+def get_dataset(input_context=None, batch_size=None):\n   inputs, output = get_numpy()\n   dataset = tf.data.Dataset.from_tensor_slices((inputs, output))\n-  dataset = dataset.batch(10).repeat(10)\n+  if input_context:\n+    dataset = dataset.shard(input_context.num_input_pipelines,\n+                            input_context.input_pipeline_id)\n+  if batch_size is None:\n+    batch_size = BATCH_SIZE\n+\n+  dataset = dataset.batch(batch_size).repeat(200)\n   return dataset\n \n \n+# A `dataset_fn` is required for `Model.fit` to work across all strategies.\n+def dataset_fn(input_context):\n+  batch_size = input_context.get_per_replica_batch_size(\n+      global_batch_size=BATCH_SIZE)\n+  return get_dataset(input_context, batch_size)\n+\n+\n class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @tf.__internal__.distribute.combinations.generate(strategy_combinations_eager_data_fn())\n-  def test_linear_model(self, distribution, data_fn):\n+  @tf.__internal__.distribute.combinations.generate(\n+      strategy_combinations_eager_data_fn())\n+  def test_linear_model(self, distribution, use_dataset_creator, data_fn):\n+    if ((not use_dataset_creator) and isinstance(\n+        distribution, tf.distribute.experimental.ParameterServerStrategy)):\n+      self.skipTest(\n+          'Parameter Server strategy requires dataset creator to be used in '\n+          'model.fit.')\n     with distribution.scope():\n       model = linear.LinearModel()\n       opt = gradient_descent.SGD(learning_rate=0.1)\n       model.compile(opt, 'mse')\n+      if use_dataset_creator:\n+        x = dataset_creator.DatasetCreator(dataset_fn)\n+        hist = model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)\n+      else:\n         if data_fn == 'numpy':\n           inputs, output = get_numpy()\n           hist = model.fit(inputs, output, epochs=5)\n@@ -73,17 +108,26 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n           hist = model.fit(get_dataset(), epochs=5)\n         self.assertLess(hist.history['loss'][4], 0.2)\n \n-  @tf.__internal__.distribute.combinations.generate(strategy_combinations_eager_data_fn())\n-  def test_wide_deep_model(self, distribution, data_fn):\n+  @tf.__internal__.distribute.combinations.generate(\n+      strategy_combinations_eager_data_fn())\n+  def test_wide_deep_model(self, distribution, use_dataset_creator, data_fn):\n+    if ((not use_dataset_creator) and isinstance(\n+        distribution, tf.distribute.experimental.ParameterServerStrategy)):\n+      self.skipTest(\n+          'Parameter Server strategy requires dataset creator to be used in '\n+          'model.fit.')\n     with distribution.scope():\n       linear_model = linear.LinearModel(units=1)\n       dnn_model = sequential.Sequential([core.Dense(units=1)])\n       wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)\n       linear_opt = gradient_descent.SGD(learning_rate=0.05)\n       dnn_opt = adagrad.Adagrad(learning_rate=0.1)\n-      wide_deep_model.compile(\n-          optimizer=[linear_opt, dnn_opt],\n-          loss='mse')\n+      wide_deep_model.compile(optimizer=[linear_opt, dnn_opt], loss='mse')\n+\n+      if use_dataset_creator:\n+        x = dataset_creator.DatasetCreator(dataset_fn)\n+        hist = wide_deep_model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)\n+      else:\n         if data_fn == 'numpy':\n           inputs, output = get_numpy()\n           hist = wide_deep_model.fit(inputs, output, epochs=5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70e2f1239b87d953e5ac84ed6286e15dbd254123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 342 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -31,18 +31,33 @@ _CONSTANT_VALUE = '_CONSTANT_VALUE'\n \n \n class Node:\n-  \"\"\"A `Node` describes the connectivity between two layers.\n+  \"\"\"A `Node` describes a layer `__call__()` event.\n \n-  Each time a layer is connected to some new input,\n-  a node is added to `layer._inbound_nodes`.\n-  Each time the output of a layer is used by another layer,\n-  a node is added to `layer._outbound_nodes`.\n+  A Functional model is a DAG with `Node` instances as nodes, and `KerasTensor`\n+  instances as edges. Nodes aren't `Layer` instances, because a single layer\n+  could be called multiple times, which would result in graph cycles.\n+\n+  A `__call__()` event involves input tensors (and other input arguments),\n+  the layer that was called, and the resulting output tensors.\n+  A `Node` will include all this information.\n+\n+  Since a single `Layer` could be called multiple times, the `Node` instances\n+  are stored on layers as a list. Each time a layer is called\n+  a node is added to `layer._inbound_nodes`. Each time the output of a layer is\n+  used by another layer, a node is added to `layer._outbound_nodes`.\n+\n+  Every `KerasTensor` instance has a `KerasHistory` object attached,\n+  which tracks the `Node` that records the `__call__()` event that created\n+  the tensor. By recursively walking through `Node` instances\n+  via the `KerasHistory` metadata of `KerasTensor` instances, once can\n+  retrieve the entire DAG of a Functional model.\n \n   Args:\n-      layer: The Layer for the Layer.__call__ this node represents.\n-      call_args: The positional arguments the Layer was called with.\n-      call_kwargs: The keyword arguments the Layer was called with.\n-      outputs: The outputs of the Layer.__call__\n+      layer: The layer that was called in the `Layer.__call__()`\n+        event that this node represents.\n+      call_args: The positional arguments the layer was called with.\n+      call_kwargs: The keyword arguments the layer was called with.\n+      outputs: The output tensors of the `Layer.__call__()`\n   \"\"\"\n \n   def __init__(self,\n@@ -273,7 +288,9 @@ class KerasHistory(\n     layer: The Layer that produced the Tensor.\n     node_index: The specific call to the Layer that produced this Tensor. Layers\n       can be called multiple times in order to share weights. A new node is\n-      created every time a Layer is called.\n+      created every time a Layer is called. The corresponding node that\n+      represents the call event that produced the Tensor can be found at\n+      `layer._inbound_nodes[node_index]`.\n     tensor_index: The output index for this Tensor. Always zero if the Layer\n       that produced this Tensor only has one output. Nested structures of\n       Tensors are deterministically assigned an index via `nest.flatten`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#351cac9d9d68f77cd603585e5af502115b39d0a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 357 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 18 | Methods Changed: 32 | Complexity Δ (Sum/Max): 35/32 | Churn Δ: 381 | Churn Cumulative: 2241 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -549,23 +549,18 @@ class _EagerVariableStore(object):\n     return initializer, initializing_from_value\n \n \n+# pylint: disable=not-context-manager\n+# The linter doesn't pick up that `scope` is a context manager\n class VariableAndLossTracker(tf.Module):\n   \"\"\"Module that has a scope to capture vars/losses made by `get_variable`.\"\"\"\n \n   def __init__(self):\n     self._var_store = _EagerVariableStore()  # pylint: disable=protected-access\n-    self._variables = {}\n-\n-  def _variable_creator(self, next_creator, **kwargs):\n-    var = next_creator(**kwargs)\n-    self._variables[var.name] = var\n-\n-    return var\n+    self._variables = self._var_store._vars  # pylint: disable=protected-access\n \n   @tf_contextlib.contextmanager\n   def scope(self):\n-    with tf.variable_creator_scope(\n-        self._variable_creator), vs.with_variable_store(self._var_store):\n+    with vs.with_variable_store(self._var_store):\n       yield\n \n   def get_regularization_losses(self):\n@@ -578,7 +573,48 @@ class VariableAndLossTracker(tf.Module):\n     return losses\n \n \n-class VariableScopeWrapperLayer(base_layer.Layer):\n+class VariableScopeModule(tf.Module):\n+  \"\"\"Wrapper Module to capture `compat.v1.get_variable` and `compat.v1.layers`.\n+\n+  See go/tf2-migration-model-bookkeeping for background.\n+\n+  This shim layer allows using large sets of TF1 model-forward-pass code as a\n+  tf.Module that works in TF2 with TF2 behaviors enabled. To use it,\n+  override this class and put your TF1 model's forward pass inside your\n+  implementation for `forward_pass`.\n+\n+  Limitations:\n+    * TF2 will not prune unused variable updates (or unused outputs). You may\n+      need to adjust your forward pass code to avoid computations or variable\n+      updates that you don't intend to use. (E.g. by adding a flag to the\n+      `forward_pass` call signature and branching on it).\n+    * Avoid Nesting variable creation in `tf.function` inside of `forward_pass`\n+      While the layer may safely be used from inside a `tf.function`, using\n+      a function inside of `forward_pass` will break the variable scoping.\n+    * TBD: Nesting keras layers/models or other `VariableScopeModule`s\n+      directly in `forward_pass` may not work correctly just yet.\n+      Support for this/instructions for how to do this is still being worked on.\n+\n+  Coming soon: A better guide, testing/verification guide.\n+  \"\"\"\n+\n+  def __init__(self, name=None):\n+    super().__init__(name=name)\n+    self._tracker = VariableAndLossTracker()\n+\n+  def forward_pass(self, *args, **kwargs):\n+    raise NotImplementedError\n+\n+  def __call__(self, *args, **kwargs):\n+    with self.name_scope:\n+      with self._tracker.scope():\n+        return self.forward_pass(*args, **kwargs)\n+\n+  def get_compat_v1_regularization_losses(self):\n+    return self._tracker.get_regularization_losses()\n+\n+\n+class VariableScopeLayer(base_layer.Layer):\n   \"\"\"Wrapper Layer to capture `compat.v1.get_variable` and `compat.v1.layers`.\n \n   See go/tf2-migration-model-bookkeeping for background.\n@@ -589,12 +625,12 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n   implementation for `forward_pass`.\n \n   Below are some examples, and then more details on the functionality of this\n-  shhim layer to wrap TF1 model forward passes.\n+  shim layer to wrap TF1 model forward passes.\n \n   Example of capturing tf.compat.v1.layer-based modeling code as a Keras layer:\n \n   ```python\n-  class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+  class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):\n \n     def __init__(self, units, *args, **kwargs):\n       super().__init__(*args, **kwargs)\n@@ -631,7 +667,7 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n   scope:\n \n   ```python\n-  class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+  class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):\n \n     def __init__(self, units, *args, **kwargs):\n       super().__init__(*args, **kwargs)\n@@ -723,7 +759,7 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n     * Avoid Nesting variable creation in tf.function inside of `forward_pass`\n       While the layer may safetely be used from inside a `tf.function`, using\n       a function inside of `forward_pass` will break the variable scoping.\n-    * TBD: Nesting keras layers/models or other `VariableScopeWrapperLayer`s\n+    * TBD: Nesting keras layers/models or other `VariableScopeLayer`s\n       directly in `forward_pass` may not work correctly just yet.\n       Support for this/instructions for how to do this is sill being worked on.\n \n@@ -733,7 +769,7 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n   def __init__(self, **kwargs):\n     super().__init__(**kwargs)\n     # Relies on keras layers tracking Modules\n-    self.tracker = VariableAndLossTracker()\n+    self._tracker = VariableAndLossTracker()\n     # May need to inspect func to see if it should pass a `training` arg or not\n \n   @property\n@@ -747,13 +783,13 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n     raise NotImplementedError\n \n   def call(self, *args, **kwargs):\n-    with self.tracker.scope():\n+    with self._tracker.scope():\n       out = self.forward_pass(*args, **kwargs)\n     if not self._eager_losses:\n       # We have to record regularization losses in the call as if they\n       # are activity losses.\n       # So, don't double-count regularization losses if the layer is used\n       # multiple times in a model\n-      for loss in self.tracker.get_regularization_losses().values():\n+      for loss in self._tracker.get_regularization_losses().values():\n         self.add_loss(loss)\n     return out\n\n@@ -28,6 +28,9 @@ import numpy\n from tensorflow.python.framework import test_util\n from keras import combinations\n from keras import regularizers\n+from keras.engine import input_layer as input_layer_module\n+from keras.engine import training as training_module\n+from keras.layers import core\n from keras.legacy_tf_layers import core as core_layers\n from keras.legacy_tf_layers import variable_scope_shim\n from tensorflow.python.ops import variable_scope\n@@ -799,12 +802,12 @@ class VariableScopeMultithreadedTest(tf.test.TestCase):\n \n \n @combinations.generate(combinations.combine(mode=[\"eager\"]))\n-class TF1VariableScopeWrapperLayerTest(tf.test.TestCase, parameterized.TestCase):\n+class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_get_variable(self):\n     # Test the shim when using `get_variable` (and regularizers) directly\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -864,14 +867,15 @@ class TF1VariableScopeWrapperLayerTest(tf.test.TestCase, parameterized.TestCase)\n   def test_compat_v1_layer(self):\n     # Test the shim when using `compat.v1` layers\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.units = units\n \n       def forward_pass(self, inputs, training=None):\n-        out = core_layers.dense(inputs, self.units, name=\"dense_one\",\n+        out = core_layers.dense(\n+            inputs, self.units, name=\"dense_one\",\n             kernel_initializer=tf.compat.v1.ones_initializer(),\n             kernel_regularizer=\"l2\")\n         with tf.compat.v1.variable_scope(\"nested_scope\"):\n@@ -901,10 +905,303 @@ class TF1VariableScopeWrapperLayerTest(tf.test.TestCase, parameterized.TestCase)\n     self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n     self.assertAllEqual(tf.add_n(layer.losses), 6)\n \n-  def test_training_arg(self):\n-    # Test the shim when using `compat.v1` layers\n+  def test_module_get_variable(self):\n+    # Test the module shim when using `get_variable` (and regularizers) directly\n \n-    class TrainingCheckLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def forward_pass(self, inputs, training=None):\n+        out = inputs\n+        with tf.compat.v1.variable_scope(\"dense_one\"):\n+          # The weights are created with a `regularizer`,\n+          # so the layer should track their regularization losses\n+          kernel = tf.compat.v1.get_variable(\n+              shape=[out.shape[-1], self.units],\n+              regularizer=regularizers.L2(),\n+              initializer=tf.compat.v1.ones_initializer(),\n+              name=\"kernel\")\n+          bias = tf.compat.v1.get_variable(\n+              shape=[self.units,],\n+              initializer=tf.compat.v1.zeros_initializer(),\n+              name=\"bias\")\n+          out = tf.matmul(out, kernel)\n+          out = tf.nn.bias_add(out, bias)\n+        with tf.compat.v1.variable_scope(\"nested_scope\"):\n+          with tf.compat.v1.variable_scope(\"dense_two\"):\n+            kernel = tf.compat.v1.get_variable(\n+                shape=[out.shape[-1], self.units],\n+                regularizer=regularizers.L2(),\n+                initializer=tf.compat.v1.ones_initializer(),\n+                name=\"kernel\")\n+            bias = tf.compat.v1.get_variable(\n+                shape=[self.units,],\n+                initializer=tf.compat.v1.zeros_initializer(),\n+                name=\"bias\")\n+            out = tf.matmul(out, kernel)\n+            out = tf.nn.bias_add(out, bias)\n+        return out\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, regularization losses, + variables were made\n+    self.assertEqual(weights.keys(), {\"dense_one/bias:0\",\n+                                      \"dense_one/kernel:0\",\n+                                      \"nested_scope/dense_two/bias:0\",\n+                                      \"nested_scope/dense_two/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 50)\n+    self.assertAllEqual(\n+        tf.add_n(layer.get_compat_v1_regularization_losses().values()), 1.5)\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense_one/kernel:0\"].assign(tf.ones(shape=(5, 10)) * 2)\n+    weights[\"nested_scope/dense_two/kernel:0\"].assign(\n+        tf.ones(shape=(10, 10)) * 2)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n+    self.assertAllEqual(\n+        tf.add_n(layer.get_compat_v1_regularization_losses().values()), 6)\n+\n+  def test_module_compat_v1_layer(self):\n+    # Test the module shim when using `compat.v1` layers\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def forward_pass(self, inputs, training=None):\n+        out = core_layers.dense(\n+            inputs, self.units, name=\"dense_one\",\n+            kernel_initializer=tf.compat.v1.ones_initializer(),\n+            kernel_regularizer=\"l2\")\n+        with tf.compat.v1.variable_scope(\"nested_scope\"):\n+          out = core_layers.dense(\n+              out, self.units, name=\"dense_two\",\n+              kernel_initializer=tf.compat.v1.ones_initializer(),\n+              kernel_regularizer=\"l2\")\n+        return out\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, losses, + variables were made\n+    self.assertEqual(weights.keys(), {\"dense_one/bias:0\",\n+                                      \"dense_one/kernel:0\",\n+                                      \"nested_scope/dense_two/bias:0\",\n+                                      \"nested_scope/dense_two/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 50)\n+    self.assertAllEqual(tf.add_n(\n+        layer.get_compat_v1_regularization_losses().values()), 1.5)\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense_one/kernel:0\"].assign(tf.ones(shape=(5, 10)) * 2)\n+    weights[\"nested_scope/dense_two/kernel:0\"].assign(\n+        tf.ones(shape=(10, 10)) * 2)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n+    self.assertAllEqual(tf.add_n(\n+        layer.get_compat_v1_regularization_losses().values()), 6)\n+\n+  def test_shim_nesting(self):\n+    # Test that nesting the shim in itself works\n+\n+    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, name, *args, **kwargs):\n+        super().__init__(*args, name=name, **kwargs)\n+        self.units = units\n+\n+      def forward_pass(self, inputs):\n+        out = inputs\n+        with tf.compat.v1.variable_scope(self.name):\n+          # The weights are created with a `regularizer`,\n+          # so the layer should track their regularization losses\n+          kernel = tf.compat.v1.get_variable(\n+              shape=[out.shape[-1], self.units],\n+              regularizer=regularizers.L2(1.0),\n+              initializer=tf.compat.v1.ones_initializer(),\n+              name=\"kernel\")\n+          bias = tf.compat.v1.get_variable(\n+              shape=[self.units,],\n+              initializer=tf.compat.v1.initializers.zeros,\n+              name=\"bias\")\n+          out = tf.linalg.matmul(out, kernel)\n+          out = tf.compat.v1.nn.bias_add(out, bias)\n+        return out\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n+        self.units = units\n+        self.dense_layer_a = None\n+        self.dense_layer_b = None\n+\n+      def forward_pass(self, inputs):\n+        # Only create the nested tf.variable/module/layer/model if it has not\n+        # already been created!\n+        if not self.dense_layer_a:\n+          self.dense_layer_a = NestedLayer(self.units * 2, \"dense_one\")\n+        out = self.dense_layer_a(inputs)\n+        if not self.dense_layer_b:\n+          self.dense_layer_b = NestedLayer(self.units, \"dense_two\")\n+        out = self.dense_layer_b(out)\n+        return out\n+\n+    layer = WrappedDenseLayer(5)\n+    out = layer(tf.ones(shape=(1, 3)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, losses, + variables were made\n+    # (Specifically: no double-counting of any weights or reg. losses\n+    # between nested components!)\n+    self.assertEqual({var.name for var in layer.trainable_weights},\n+                     {\"dense_one/bias:0\",\n+                      \"dense_one/kernel:0\",\n+                      \"dense_two/bias:0\",\n+                      \"dense_two/kernel:0\"})\n+    self.assertEqual({var.name for var in layer.dense_layer_a.weights},\n+                     {\"dense_one/bias:0\",\n+                      \"dense_one/kernel:0\"})\n+    self.assertEqual({var.name for var in layer.dense_layer_b.weights},\n+                     {\"dense_two/bias:0\",\n+                      \"dense_two/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(1, 5)) * 30)\n+    self.assertAllEqual(tf.add_n(layer.dense_layer_a.losses), 30)\n+    self.assertAllEqual(tf.add_n(layer.dense_layer_b.losses), 50)\n+    self.assertAllEqual(tf.add_n(layer.losses), 80)\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense_one/kernel:0\"].assign(tf.ones(shape=(3, 10)) * 2)\n+    weights[\"dense_two/kernel:0\"].assign(\n+        tf.ones(shape=(10, 5)) * 2)\n+    out = layer(tf.ones(shape=(1, 3)))\n+    self.assertAllEqual(out, tf.ones(shape=(1, 5)) * 120)\n+    self.assertAllEqual(tf.add_n(layer.losses), 320)\n+\n+  def test_only_track_get_variable(self):\n+    # Test the shim does not try tracking or reusing variables\n+    # that were not created by get_variable. These variables/modules/layers\n+    # need to be tracked separately\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n+        self.units = units\n+        self._dense_model = None\n+\n+      def forward_pass(self, inputs):\n+        dense_layer = core.Dense(\n+            self.units, name=\"dense\",\n+            kernel_initializer=tf.compat.v1.ones_initializer(),\n+            kernel_regularizer=\"l2\")\n+        return dense_layer(inputs)\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 5)\n+\n+    self.assertEmpty(layer.weights)\n+\n+  def test_embedded_keras_model(self):\n+    # Test the shim when embedding a Keras model inside of it\n+    # And assigning the model to an attribute\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n+        self.units = units\n+        self._dense_model = None\n+\n+      def forward_pass(self, inputs):\n+        if not self._dense_model:\n+          inp = input_layer_module.Input(shape=inputs.shape)\n+          dense_layer = core.Dense(\n+              self.units, name=\"dense\",\n+              kernel_initializer=tf.compat.v1.ones_initializer(),\n+              kernel_regularizer=\"l2\")\n+          self._dense_model = training_module.Model(\n+              inputs=inp, outputs=dense_layer(inp))\n+        return self._dense_model(inputs)\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, losses, + variables were made\n+    self.assertEqual(weights.keys(), {\"dense/bias:0\",\n+                                      \"dense/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 5)\n+    self.assertAllEqual(tf.add_n(layer.losses), 0.5)\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense/kernel:0\"].assign(\n+        tf.ones(shape=(5, 10)) * 2)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 10)\n+    self.assertAllEqual(tf.add_n(layer.losses), 2)\n+\n+  def test_embedded_keras_model_in_module(self):\n+    # Test the module shim when embedding a Keras model inside of it\n+    # And assigning the model to an attribute\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeModule):\n+\n+      def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n+        self.units = units\n+        self._dense_model = None\n+\n+      def forward_pass(self, inputs):\n+        if not self._dense_model:\n+          inp = input_layer_module.Input(shape=inputs.shape)\n+          dense_layer = core.Dense(\n+              self.units, name=\"dense\",\n+              kernel_initializer=tf.compat.v1.ones_initializer(),\n+              kernel_regularizer=\"l2\")\n+          self._dense_model = training_module.Model(\n+              inputs=inp, outputs=dense_layer(inp))\n+        return self._dense_model(inputs)\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, losses, + variables were made\n+    self.assertEqual(weights.keys(), {\"dense/bias:0\",\n+                                      \"dense/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 5)\n+\n+    # The module shim will only track regularization losses made by\n+    # compat.v1.layers and compat.v1.get_variable. Other regularization\n+    # losses must be tracked by separate user-created mechanisms.\n+    self.assertEmpty(layer.get_compat_v1_regularization_losses())\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense/kernel:0\"].assign(\n+        tf.ones(shape=(5, 10)) * 2)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 10)\n+\n+    # The module shim will only track regularization losses made by\n+    # compat.v1.layers and compat.v1.get_variable. Other regularization\n+    # losses must be tracked by separate user-created mechanisms.\n+    self.assertEmpty(layer.get_compat_v1_regularization_losses())\n+\n+  def test_training_arg(self):\n+    # Test the shim when passing in a Keras `training` arg\n+\n+    class TrainingCheckLayer(variable_scope_shim.VariableScopeLayer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aaa022c622f583482e7502725272de201495fe52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 41 | Files Changed: 3 | Hunks: 29 | Methods Changed: 24 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 98 | Churn Cumulative: 2434 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,47 +15,44 @@\n # ==============================================================================\n \"\"\"Tests for `DatasetCreator` with `Model.fit` across usages and strategies.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n from keras import callbacks as callbacks_lib\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n from keras.distribute import strategy_combinations\n+import tensorflow.compat.v2 as tf\n \n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.parameter_server_strategies_multi_worker,\n+        use_dataset_creator=[True, False],\n         mode=\"eager\"))\n class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     test_base.DatasetCreatorModelFitTestBase):\n \n-  def testModelFitWithRunEagerly(self, strategy):\n+  def testModelFitWithRunEagerly(self, strategy, use_dataset_creator):\n     with self.assertRaisesRegex(\n         ValueError, \"When using `Model` with `ParameterServerStrategy`, \"\n         \"`run_eagerly` is not supported.\"):\n-      self._model_fit(strategy, run_eagerly=True)\n-\n-  def testModelFitWithDatasetInstance(self, strategy):\n-    with self.assertRaisesRegex(\n-        NotImplementedError,\n-        \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n-        \"numpy arrays and pandas dataframes are supported types at this \"\n-        \"time.\"):\n       self._model_fit(\n-          strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+          strategy, run_eagerly=True, use_dataset_creator=use_dataset_creator)\n \n-  def testModelPredict(self, strategy):\n+  def testModelPredict(self, strategy, use_dataset_creator):\n+    if use_dataset_creator:\n+      self.skipTest(\"Unused option.\")\n     model, _ = self._model_compile(strategy)\n     test_data = tf.data.Dataset.from_tensor_slices(\n         [1., 2., 3., 1., 5., 1.]).repeat().batch(2)\n     model.predict(x=test_data, steps=3)\n \n-  def testClusterCoordinatorSingleInstance(self, strategy):\n-    model = self._model_fit(strategy)\n+  def testClusterCoordinatorSingleInstance(self, strategy, use_dataset_creator):\n+    model = self._model_fit(strategy, use_dataset_creator=use_dataset_creator)\n     strategy = model.distribute_strategy\n-    self.assertIs(strategy._cluster_coordinator,\n+    self.assertIs(\n+        strategy._cluster_coordinator,\n         tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n \n-  def testModelFitErrorOnBatchLevelCallbacks(self, strategy):\n+  def testModelFitErrorOnBatchLevelCallbacks(self, strategy,\n+                                             use_dataset_creator):\n \n     class BatchLevelCallback(callbacks_lib.Callback):\n \n@@ -65,9 +62,12 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     with self.assertRaisesRegex(ValueError,\n                                 \"Batch-level `Callback`s are not supported\"):\n       callbacks = [BatchLevelCallback()]\n-      self._model_fit(strategy, callbacks=callbacks)\n+      self._model_fit(\n+          strategy,\n+          callbacks=callbacks,\n+          use_dataset_creator=use_dataset_creator)\n \n-  def testModelFitCallbackSupportsTFLogs(self, strategy):\n+  def testModelFitCallbackSupportsTFLogs(self, strategy, use_dataset_creator):\n \n     class MyCallback(callbacks_lib.Callback):\n \n@@ -81,37 +81,32 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n \n     my_callback = MyCallback()\n     callbacks = [my_callback]\n-    self._model_fit(strategy, callbacks=callbacks)\n+    self._model_fit(\n+        strategy, callbacks=callbacks, use_dataset_creator=use_dataset_creator)\n \n-  def testModelFitVerbosity(self, strategy):\n+  def testModelFitVerbosity(self, strategy, use_dataset_creator):\n \n     class MyCallback(callbacks_lib.Callback):\n       pass\n \n     my_callback = MyCallback()\n     callbacks = [my_callback]\n-    self._model_fit(strategy, callbacks=callbacks)\n+    self._model_fit(\n+        strategy, callbacks=callbacks, use_dataset_creator=use_dataset_creator)\n     # PSStrategy should default to epoch-level logging.\n     self.assertEqual(my_callback.params[\"verbose\"], 2)\n \n-  def testModelFitTensorBoardEpochLevel(self, strategy):\n+  def testModelFitTensorBoardEpochLevel(self, strategy, use_dataset_creator):\n     log_dir = self.get_temp_dir()\n     callbacks = [callbacks_lib.TensorBoard(log_dir)]\n-    self._model_fit(strategy, callbacks=callbacks)\n+    self._model_fit(\n+        strategy, callbacks=callbacks, use_dataset_creator=use_dataset_creator)\n     self.assertTrue(tf.compat.v1.gfile.Exists(log_dir))\n     files = tf.compat.v1.gfile.ListDirectory(log_dir)\n     self.assertGreaterEqual(len(files), 1)\n \n-  def testModelEvaluateWithDatasetInstance(self, strategy):\n-    with self.assertRaisesRegex(\n-        NotImplementedError,\n-        \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n-        \"numpy arrays and pandas dataframes are supported types at this \"\n-        \"time.\"):\n-      self._model_evaluate(\n-          strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy):\n+  def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy,\n+                                                  use_dataset_creator):\n \n     class BatchLevelCallback(callbacks_lib.Callback):\n \n@@ -121,7 +116,10 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     with self.assertRaisesRegex(ValueError,\n                                 \"Batch-level `Callback`s are not supported\"):\n       callbacks = [BatchLevelCallback()]\n-      self._model_evaluate(strategy, callbacks=callbacks)\n+      self._model_evaluate(\n+          strategy,\n+          callbacks=callbacks,\n+          use_dataset_creator=use_dataset_creator)\n \n \n if __name__ == \"__main__\":\n\n@@ -118,7 +118,8 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                  run_eagerly=False,\n                  with_normalization_layer=False,\n                  callbacks=None,\n-                 use_lookup_layer=False):\n+                 use_lookup_layer=False,\n+                 use_dataset_creator=True):\n     if callbacks is None:\n       callbacks = []\n \n@@ -130,11 +131,18 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n     callbacks += default_callbacks\n \n     if x is None:\n-      x = dataset_creator.DatasetCreator(self._get_dataset_fn(use_lookup_layer))\n+      if use_dataset_creator:\n+        x = dataset_creator.DatasetCreator(\n+            self._get_dataset_fn(use_lookup_layer))\n+      else:\n+        x = self._get_dataset_fn(use_lookup_layer)(None)\n \n     if validation_data is None:\n+      if use_dataset_creator:\n         validation_data = dataset_creator.DatasetCreator(\n             self._get_dataset_fn(use_lookup_layer))\n+      else:\n+        validation_data = self._get_dataset_fn(use_lookup_layer)(None)\n \n     model.fit(\n         x,\n@@ -157,7 +165,8 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                       steps=10,\n                       run_eagerly=False,\n                       with_normalization_layer=False,\n-                      callbacks=None):\n+                      callbacks=None,\n+                      use_dataset_creator=True):\n     if callbacks is None:\n       callbacks = []\n \n@@ -177,7 +186,10 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n           (x, y)).shuffle(10).repeat().batch(8)\n \n     if x is None:\n+      if use_dataset_creator:\n         x = dataset_creator.DatasetCreator(dataset_fn)\n+      else:\n+        x = dataset_fn(None)\n \n     model.evaluate(\n         x=x, y=y, steps=steps, callbacks=callbacks, batch_size=batch_size)\n\n@@ -1329,7 +1329,8 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n   \"\"\"A `DataHandler` that is compatible with `ClusterCoordinator`.\"\"\"\n \n   def __init__(self, x, y=None, **kwargs):\n-    if not isinstance(x, dataset_creator.DatasetCreator):\n+    if (not _is_distributed_dataset(x) and\n+        not isinstance(x, (dataset_creator.DatasetCreator, tf.data.Dataset))):\n       x = self._convert_to_dataset_creator(x, y, **kwargs)\n \n     super().__init__(x=x, **kwargs)\n@@ -1355,9 +1356,7 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n \n   def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n                                             class_weight, distribute):\n-    if not isinstance(x, dataset_creator.DatasetCreator):\n-      raise TypeError(\"When using `ParameterServerStrategy`, `x` must be a \"\n-                      \"`DatasetCreator`.\")\n+    if isinstance(x, dataset_creator.DatasetCreator):\n \n       def per_worker_dataset_fn():\n \n@@ -1366,6 +1365,13 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n \n       self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n           per_worker_dataset_fn)\n+    else:\n+      assert distribute\n+      if not _is_distributed_dataset(x):\n+        x = strategy.experimental_distribute_dataset(x)\n+\n+      self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n+          x)\n \n     if steps_per_epoch == -1:\n       self._inferred_steps = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
