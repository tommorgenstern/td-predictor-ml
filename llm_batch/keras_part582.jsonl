{"custom_id": "keras#44739c76abf0e2928e04b59ab4dd53088d60ed1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 287 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,15 +7,17 @@ from keras_core.layers.layer import Layer\n class LeakyReLU(Layer):\n     \"\"\"Leaky version of a Rectified Linear Unit activation layer.\n \n-    The layer allows a small gradient when the unit is not active.\n+    This layer allows a small gradient when the unit is not active.\n \n     Formula:\n+\n     ``` python\n     f(x) = alpha * x if x < 0\n     f(x) = x if x >= 0\n     ```\n \n     Example:\n+\n     ``` python\n     leaky_relu_layer = LeakyReLU(negative_slope=0.5)\n     input = np.array([-10, -5, 0.0, 5, 10])\n@@ -36,8 +38,8 @@ class LeakyReLU(Layer):\n         if negative_slope is None:\n             raise ValueError(\n                 \"The negative_slope value of a Leaky ReLU layer \"\n-                \"cannot be None, Expecting a float. Received \"\n-                f\"negative_slope: {negative_slope}\"\n+                \"cannot be None, Expecting a float. Received: \"\n+                f\"negative_slope={negative_slope}\"\n             )\n         self.supports_masking = True\n         self.negative_slope = negative_slope\n\n@@ -25,8 +25,7 @@ class LeakyReLUTest(testing.TestCase):\n     def test_invalid_usage(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"The negative_slope value of a Leaky ReLU layer cannot be None, \"\n-            \"Expecting a float. Received negative_slope: None\",\n+            \"The negative_slope value of a Leaky ReLU layer cannot be None\",\n         ):\n             self.run_layer_test(\n                 leaky_relu.LeakyReLU,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d3c29bc78632457b1407abe2a867efce45f23ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 36 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 1466 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,16 @@\n+from keras_core.optimizers.schedules.learning_rate_schedule import CosineDecay\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    CosineDecayRestarts,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    ExponentialDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    InverseTimeDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    PiecewiseConstantDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    PolynomialDecay,\n+)\n\n@@ -6,13 +6,13 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.optimizers.schedules import learning_rate_schedule\n+from keras_core.optimizers import schedules\n \n \n class ExponentialDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.ExponentialDecay(\n+            schedules.ExponentialDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -23,15 +23,13 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_continuous(self):\n         step = 5\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n+        decayed_lr = schedules.ExponentialDecay(0.05, 10, 0.96)\n         expected = 0.05 * 0.96 ** (5.0 / 10.0)\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n     def test_staircase(self):\n         step = backend.Variable(1)\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(\n-            0.1, 3, 0.96, staircase=True\n-        )\n+        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n \n         # No change to learning rate due to staircase\n         expected = 0.1\n@@ -48,9 +46,7 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_variables(self):\n         step = backend.Variable(1)\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(\n-            0.1, 3, 0.96, staircase=True\n-        )\n+        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n \n         # No change to learning rate\n         step.assign(1)\n@@ -66,14 +62,14 @@ class ExponentialDecayTest(testing.TestCase):\n class PiecewiseConstantDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.PiecewiseConstantDecay(\n+            schedules.PiecewiseConstantDecay(\n                 boundaries=[10, 20], values=[1, 2, 3], name=\"my_pcd\"\n             )\n         )\n \n     def test_piecewise_values(self):\n         x = backend.Variable(-999)\n-        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+        decayed_lr = schedules.PiecewiseConstantDecay(\n             [100, 110, 120], [1.0, 0.1, 0.01, 0.001]\n         )\n \n@@ -93,9 +89,7 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n         # Test casting boundaries from int32 to int64.\n         x_int64 = backend.Variable(0, dtype=\"int64\")\n         boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]\n-        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n-            boundaries, values\n-        )\n+        decayed_lr = schedules.PiecewiseConstantDecay(boundaries, values)\n \n         self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n         x_int64.assign(1)\n@@ -111,7 +105,7 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n class LinearDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.PolynomialDecay(\n+            schedules.PolynomialDecay(\n                 initial_learning_rate=0.1,\n                 decay_steps=100,\n                 end_learning_rate=0.005,\n@@ -125,7 +119,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.0\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = lr * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -133,7 +127,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 10\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -141,7 +135,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = (lr + end_lr) * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -149,7 +143,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -157,9 +151,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, cycle=True\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, cycle=True)\n         expected = (lr - end_lr) * 0.25 + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -170,9 +162,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.0\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = lr * 0.5**power\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -181,9 +171,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -192,9 +180,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = (lr - end_lr) * 0.5**power + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -203,9 +189,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -214,7 +198,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+        decayed_lr = schedules.PolynomialDecay(\n             lr, 10, end_lr, power=power, cycle=True\n         )\n         expected = (lr - end_lr) * 0.25**power + end_lr\n@@ -224,9 +208,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.001\n         decay_steps = 10\n         step = 0\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, decay_steps, cycle=True\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, decay_steps, cycle=True)\n         expected = lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -234,7 +216,7 @@ class SqrtDecayTest(testing.TestCase):\n class InverseTimeDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.InverseTimeDecay(\n+            schedules.InverseTimeDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -248,9 +230,7 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n-            initial_lr, k, decay_rate\n-        )\n+        decayed_lr = schedules.InverseTimeDecay(initial_lr, k, decay_rate)\n \n         for i in range(k + 1):\n             expected = initial_lr / (1 + i / k * decay_rate)\n@@ -262,7 +242,7 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+        decayed_lr = schedules.InverseTimeDecay(\n             initial_lr, k, decay_rate, staircase=True\n         )\n \n@@ -275,7 +255,7 @@ class InverseTimeDecayTest(testing.TestCase):\n class CosineDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.CosineDecay(\n+            schedules.CosineDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 alpha=0.1,\n@@ -295,9 +275,7 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n-                initial_lr, num_training_steps\n-            )\n+            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -311,7 +289,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 1500, 250):\n-            lr = learning_rate_schedule.CosineDecay(\n+            lr = schedules.CosineDecay(\n                 initial_lr,\n                 0,\n                 warmup_target=target_lr,\n@@ -327,7 +305,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n+            decayed_lr = schedules.CosineDecay(\n                 initial_lr, num_training_steps, alpha\n             )\n             expected = self.np_cosine_decay(step, num_training_steps, alpha)\n@@ -337,9 +315,7 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n-                initial_lr, num_training_steps\n-            )\n+            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -349,7 +325,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 3000, 250):\n-            lr = learning_rate_schedule.CosineDecay(\n+            lr = schedules.CosineDecay(\n                 initial_lr,\n                 decay_steps,\n                 warmup_target=target_lr,\n@@ -369,7 +345,7 @@ class CosineDecayTest(testing.TestCase):\n class CosineDecayRestartsTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.CosineDecayRestarts(\n+            schedules.CosineDecayRestarts(\n                 initial_learning_rate=0.05,\n                 first_decay_steps=10,\n                 alpha=0.1,\n@@ -396,7 +372,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -406,7 +382,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -417,7 +393,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, alpha=alpha\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -430,7 +406,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         m_mul = 0.9\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, m_mul=m_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -443,7 +419,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         t_mul = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, t_mul=t_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f3428568e849c578f7495351fe20a38567b695c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 282 | Lines Deleted: 51 | Files Changed: 6 | Hunks: 39 | Methods Changed: 31 | Complexity Δ (Sum/Max): 17/14 | Churn Δ: 333 | Churn Cumulative: 1885 | Contributors (this commit): 7 | Commits (past 90d): 47 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,6 +53,7 @@ from keras_core.layers.pooling.max_pooling1d import MaxPooling1D\n from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n from keras_core.layers.preprocessing.center_crop import CenterCrop\n+from keras_core.layers.preprocessing.discretization import Discretization\n from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -0,0 +1,185 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.layers.layer import Layer\n+\n+\n+class Discretization(Layer):\n+    \"\"\"A preprocessing layer which buckets continuous features by ranges.\n+\n+    This layer will place each element of its input data into one of several\n+    contiguous ranges and output an integer index indicating which range each\n+    element was placed in.\n+\n+    **Note:** This layer wraps `tf.keras.layers.Discretization`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n+    Input shape:\n+        Any array of dimension 2 or higher.\n+\n+    Output shape:\n+        Same as input shape.\n+\n+    Arguments:\n+        bin_boundaries: A list of bin boundaries.\n+            The leftmost and rightmost bins\n+            will always extend to `-inf` and `inf`,\n+            so `bin_boundaries=[0., 1., 2.]`\n+            generates bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`,\n+            and `[2., +inf)`.\n+            If this option is set, `adapt()` should not be called.\n+        num_bins: The integer number of bins to compute.\n+            If this option is set,\n+            `adapt()` should be called to learn the bin boundaries.\n+        epsilon: Error tolerance, typically a small fraction\n+            close to zero (e.g. 0.01). Higher values of epsilon increase\n+            the quantile approximation, and hence result in more\n+            unequal buckets, but could improve performance\n+            and resource consumption.\n+        output_mode: Specification for the output of the layer.\n+            Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+            `\"count\"` configuring the layer as follows:\n+            - `\"int\"`: Return the discretized bin indices directly.\n+            - `\"one_hot\"`: Encodes each individual element in the\n+                input into an array the same size as `num_bins`,\n+                containing a 1 at the input's bin\n+                index. If the last dimension is size 1, will encode on that\n+                dimension.  If the last dimension is not size 1,\n+                will append a new dimension for the encoded output.\n+            - `\"multi_hot\"`: Encodes each sample in the input into a\n+                single array the same size as `num_bins`,\n+                containing a 1 for each bin index\n+                index present in the sample.\n+                Treats the last dimension as the sample\n+                dimension, if input shape is `(..., sample_length)`,\n+                output shape will be `(..., num_tokens)`.\n+            - `\"count\"`: As `\"multi_hot\"`, but the int array contains\n+                a count of the number of times the bin index appeared\n+                in the sample.\n+            Defaults to `\"int\"`.\n+        sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n+            and `\"count\"` output modes. Only supported with TensorFlow\n+            backend. If `True`, returns a `SparseTensor` instead of\n+            a dense `Tensor`. Defaults to `False`.\n+\n+    Examples:\n+\n+    Bucketize float values based on provided buckets.\n+    >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n+    >>> layer = Discretization(bin_boundaries=[0., 1., 2.])\n+    >>> layer(input)\n+    array([[0, 2, 3, 1],\n+           [1, 3, 2, 1]])\n+\n+    Bucketize float values based on a number of buckets to compute.\n+    >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n+    >>> layer = Discretization(num_bins=4, epsilon=0.01)\n+    >>> layer.adapt(input)\n+    >>> layer(input)\n+    array([[0, 2, 3, 2],\n+           [1, 3, 3, 1]])\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        bin_boundaries=None,\n+        num_bins=None,\n+        epsilon=0.01,\n+        output_mode=\"int\",\n+        sparse=False,\n+        name=None,\n+    ):\n+        super().__init__(name=name)\n+        if sparse and backend.backend() != \"tensorflow\":\n+            raise ValueError()\n+        self.layer = tf.keras.layers.Discretization(\n+            bin_boundaries=bin_boundaries,\n+            num_bins=num_bins,\n+            epsilon=epsilon,\n+            output_mode=output_mode,\n+            sparse=sparse,\n+            name=name,\n+        )\n+        self.bin_boundaries = (\n+            bin_boundaries if bin_boundaries is not None else []\n+        )\n+        self.num_bins = num_bins\n+        self.epsilon = epsilon\n+        self.output_mode = output_mode\n+        self.sparse = sparse\n+\n+    def build(self, input_shape):\n+        self.layer.build(input_shape)\n+        self.built = True\n+\n+    # We override this method solely to generate a docstring.\n+    def adapt(self, data, batch_size=None, steps=None):\n+        \"\"\"Computes bin boundaries from quantiles in a input dataset.\n+\n+        Calling `adapt()` on a `Discretization` layer is an alternative to\n+        passing in a `bin_boundaries` argument during construction. A\n+        `Discretization` layer should always be either adapted over a dataset or\n+        passed `bin_boundaries`.\n+\n+        During `adapt()`, the layer will estimate the quantile boundaries of the\n+        input dataset. The number of quantiles can be controlled via the\n+        `num_bins` argument, and the error tolerance for quantile boundaries can\n+        be controlled via the `epsilon` argument.\n+\n+        Arguments:\n+          data: The data to train on. It can be passed either as a\n+              `tf.data.Dataset`, or as a numpy array.\n+          batch_size: Integer or `None`.\n+              Number of samples per state update.\n+              If unspecified, `batch_size` will default to 32.\n+              Do not specify the `batch_size` if your data is in the\n+              form of datasets, generators, or `keras.utils.Sequence` instances\n+              (since they generate batches).\n+          steps: Integer or `None`.\n+              Total number of steps (batches of samples)\n+              When training with input tensors such as\n+              TensorFlow data tensors, the default `None` is equal to\n+              the number of samples in your dataset divided by\n+              the batch size, or 1 if that cannot be determined. If x is a\n+              `tf.data.Dataset`, and `steps` is `None`, the epoch will run until\n+              the input dataset is exhausted. When passing an infinitely\n+              repeating dataset, you must specify the `steps` argument. This\n+              argument is not supported with array inputs.\n+        \"\"\"\n+        self.layer.adapt(data, batch_size=batch_size, steps=steps)\n+\n+    def update_state(self, data):\n+        self.layer.update_state(data)\n+\n+    def finalize_state(self):\n+        self.layer.finalize_state()\n+\n+    def reset_state(self):\n+        self.layer.reset_state()\n+\n+    def get_config(self):\n+        return {\n+            \"bin_boundaries\": self.bin_boundaries,\n+            \"num_bins\": self.num_bins,\n+            \"epsilon\": self.epsilon,\n+            \"output_mode\": self.output_mode,\n+            \"sparse\": self.sparse,\n+            \"name\": self.name,\n+        }\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def call(self, inputs):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n\n@@ -0,0 +1,37 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class DicretizationTest(testing.TestCase):\n+    def test_discretization_basics(self):\n+        self.run_layer_test(\n+            layers.Discretization,\n+            init_kwargs={\n+                \"bin_boundaries\": [0.0, 0.5, 1.0],\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    def test_adapt_flow(self):\n+        layer = layers.Discretization(num_bins=4)\n+        layer.adapt(\n+            np.random.random((32, 3)),\n+            batch_size=8,\n+        )\n+        output = layer(np.array([[0.0, 0.1, 0.3]]))\n+        self.assertTrue(output.dtype, \"int32\")\n+\n+    def test_correctness(self):\n+        layer = layers.Discretization(bin_boundaries=[0.0, 0.5, 1.0])\n+        output = layer(np.array([[0.0, 0.1, 0.8]]))\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertAllClose(output, np.array([[1, 1, 2]]))\n\n\n@@ -1,16 +0,0 @@\n-from keras_core.optimizers.schedules.learning_rate_schedule import CosineDecay\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    CosineDecayRestarts,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    ExponentialDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    InverseTimeDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    PiecewiseConstantDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    PolynomialDecay,\n-)\n\n@@ -6,13 +6,13 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.optimizers import schedules\n+from keras_core.optimizers.schedules import learning_rate_schedule\n \n \n class ExponentialDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.ExponentialDecay(\n+            learning_rate_schedule.ExponentialDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -23,13 +23,15 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_continuous(self):\n         step = 5\n-        decayed_lr = schedules.ExponentialDecay(0.05, 10, 0.96)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n         expected = 0.05 * 0.96 ** (5.0 / 10.0)\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n     def test_staircase(self):\n         step = backend.Variable(1)\n-        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n \n         # No change to learning rate due to staircase\n         expected = 0.1\n@@ -46,7 +48,9 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_variables(self):\n         step = backend.Variable(1)\n-        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n \n         # No change to learning rate\n         step.assign(1)\n@@ -62,14 +66,14 @@ class ExponentialDecayTest(testing.TestCase):\n class PiecewiseConstantDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.PiecewiseConstantDecay(\n+            learning_rate_schedule.PiecewiseConstantDecay(\n                 boundaries=[10, 20], values=[1, 2, 3], name=\"my_pcd\"\n             )\n         )\n \n     def test_piecewise_values(self):\n         x = backend.Variable(-999)\n-        decayed_lr = schedules.PiecewiseConstantDecay(\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n             [100, 110, 120], [1.0, 0.1, 0.01, 0.001]\n         )\n \n@@ -89,7 +93,9 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n         # Test casting boundaries from int32 to int64.\n         x_int64 = backend.Variable(0, dtype=\"int64\")\n         boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]\n-        decayed_lr = schedules.PiecewiseConstantDecay(boundaries, values)\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+            boundaries, values\n+        )\n \n         self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n         x_int64.assign(1)\n@@ -105,7 +111,7 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n class LinearDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.PolynomialDecay(\n+            learning_rate_schedule.PolynomialDecay(\n                 initial_learning_rate=0.1,\n                 decay_steps=100,\n                 end_learning_rate=0.005,\n@@ -119,7 +125,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.0\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n         expected = lr * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -127,7 +133,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 10\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -135,7 +141,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n         expected = (lr + end_lr) * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -143,7 +149,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -151,7 +157,9 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, cycle=True)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, cycle=True\n+        )\n         expected = (lr - end_lr) * 0.25 + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -162,7 +170,9 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.0\n         power = 0.5\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n         expected = lr * 0.5**power\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -171,7 +181,9 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -180,7 +192,9 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n         expected = (lr - end_lr) * 0.5**power + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -189,7 +203,9 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -198,7 +214,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = schedules.PolynomialDecay(\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n             lr, 10, end_lr, power=power, cycle=True\n         )\n         expected = (lr - end_lr) * 0.25**power + end_lr\n@@ -208,7 +224,9 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.001\n         decay_steps = 10\n         step = 0\n-        decayed_lr = schedules.PolynomialDecay(lr, decay_steps, cycle=True)\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, decay_steps, cycle=True\n+        )\n         expected = lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -216,7 +234,7 @@ class SqrtDecayTest(testing.TestCase):\n class InverseTimeDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.InverseTimeDecay(\n+            learning_rate_schedule.InverseTimeDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -230,7 +248,9 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = schedules.InverseTimeDecay(initial_lr, k, decay_rate)\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+            initial_lr, k, decay_rate\n+        )\n \n         for i in range(k + 1):\n             expected = initial_lr / (1 + i / k * decay_rate)\n@@ -242,7 +262,7 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = schedules.InverseTimeDecay(\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n             initial_lr, k, decay_rate, staircase=True\n         )\n \n@@ -255,7 +275,7 @@ class InverseTimeDecayTest(testing.TestCase):\n class CosineDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.CosineDecay(\n+            learning_rate_schedule.CosineDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 alpha=0.1,\n@@ -275,7 +295,9 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -289,7 +311,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 1500, 250):\n-            lr = schedules.CosineDecay(\n+            lr = learning_rate_schedule.CosineDecay(\n                 initial_lr,\n                 0,\n                 warmup_target=target_lr,\n@@ -305,7 +327,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecay(\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n                 initial_lr, num_training_steps, alpha\n             )\n             expected = self.np_cosine_decay(step, num_training_steps, alpha)\n@@ -315,7 +337,9 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -325,7 +349,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 3000, 250):\n-            lr = schedules.CosineDecay(\n+            lr = learning_rate_schedule.CosineDecay(\n                 initial_lr,\n                 decay_steps,\n                 warmup_target=target_lr,\n@@ -345,7 +369,7 @@ class CosineDecayTest(testing.TestCase):\n class CosineDecayRestartsTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            schedules.CosineDecayRestarts(\n+            learning_rate_schedule.CosineDecayRestarts(\n                 initial_learning_rate=0.05,\n                 first_decay_steps=10,\n                 alpha=0.1,\n@@ -372,7 +396,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecayRestarts(\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -382,7 +406,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecayRestarts(\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -393,7 +417,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecayRestarts(\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n                 initial_lr, num_training_steps, alpha=alpha\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -406,7 +430,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         m_mul = 0.9\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecayRestarts(\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n                 initial_lr, num_training_steps, m_mul=m_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -419,7 +443,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         t_mul = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = schedules.CosineDecayRestarts(\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n                 initial_lr, num_training_steps, t_mul=t_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2e5850f8d87ee4790ca808a4d0004550485d16e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 36 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 1686 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,16 @@\n+from keras_core.optimizers.schedules.learning_rate_schedule import CosineDecay\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    CosineDecayRestarts,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    ExponentialDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    InverseTimeDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    PiecewiseConstantDecay,\n+)\n+from keras_core.optimizers.schedules.learning_rate_schedule import (\n+    PolynomialDecay,\n+)\n\n@@ -6,13 +6,13 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.optimizers.schedules import learning_rate_schedule\n+from keras_core.optimizers import schedules\n \n \n class ExponentialDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.ExponentialDecay(\n+            schedules.ExponentialDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -23,15 +23,13 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_continuous(self):\n         step = 5\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n+        decayed_lr = schedules.ExponentialDecay(0.05, 10, 0.96)\n         expected = 0.05 * 0.96 ** (5.0 / 10.0)\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n     def test_staircase(self):\n         step = backend.Variable(1)\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(\n-            0.1, 3, 0.96, staircase=True\n-        )\n+        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n \n         # No change to learning rate due to staircase\n         expected = 0.1\n@@ -48,9 +46,7 @@ class ExponentialDecayTest(testing.TestCase):\n \n     def test_variables(self):\n         step = backend.Variable(1)\n-        decayed_lr = learning_rate_schedule.ExponentialDecay(\n-            0.1, 3, 0.96, staircase=True\n-        )\n+        decayed_lr = schedules.ExponentialDecay(0.1, 3, 0.96, staircase=True)\n \n         # No change to learning rate\n         step.assign(1)\n@@ -66,14 +62,14 @@ class ExponentialDecayTest(testing.TestCase):\n class PiecewiseConstantDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.PiecewiseConstantDecay(\n+            schedules.PiecewiseConstantDecay(\n                 boundaries=[10, 20], values=[1, 2, 3], name=\"my_pcd\"\n             )\n         )\n \n     def test_piecewise_values(self):\n         x = backend.Variable(-999)\n-        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+        decayed_lr = schedules.PiecewiseConstantDecay(\n             [100, 110, 120], [1.0, 0.1, 0.01, 0.001]\n         )\n \n@@ -93,9 +89,7 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n         # Test casting boundaries from int32 to int64.\n         x_int64 = backend.Variable(0, dtype=\"int64\")\n         boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]\n-        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n-            boundaries, values\n-        )\n+        decayed_lr = schedules.PiecewiseConstantDecay(boundaries, values)\n \n         self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n         x_int64.assign(1)\n@@ -111,7 +105,7 @@ class PiecewiseConstantDecayTest(testing.TestCase):\n class LinearDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.PolynomialDecay(\n+            schedules.PolynomialDecay(\n                 initial_learning_rate=0.1,\n                 decay_steps=100,\n                 end_learning_rate=0.005,\n@@ -125,7 +119,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.0\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = lr * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -133,7 +127,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 10\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -141,7 +135,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 5\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = (lr + end_lr) * 0.5\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -149,7 +143,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -157,9 +151,7 @@ class LinearDecayTest(testing.TestCase):\n         step = 15\n         lr = 0.05\n         end_lr = 0.001\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, cycle=True\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, cycle=True)\n         expected = (lr - end_lr) * 0.25 + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -170,9 +162,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.0\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = lr * 0.5**power\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -181,9 +171,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -192,9 +180,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = (lr - end_lr) * 0.5**power + end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -203,9 +189,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, 10, end_lr, power=power\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, 10, end_lr, power=power)\n         expected = end_lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -214,7 +198,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.05\n         end_lr = 0.001\n         power = 0.5\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+        decayed_lr = schedules.PolynomialDecay(\n             lr, 10, end_lr, power=power, cycle=True\n         )\n         expected = (lr - end_lr) * 0.25**power + end_lr\n@@ -224,9 +208,7 @@ class SqrtDecayTest(testing.TestCase):\n         lr = 0.001\n         decay_steps = 10\n         step = 0\n-        decayed_lr = learning_rate_schedule.PolynomialDecay(\n-            lr, decay_steps, cycle=True\n-        )\n+        decayed_lr = schedules.PolynomialDecay(lr, decay_steps, cycle=True)\n         expected = lr\n         self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -234,7 +216,7 @@ class SqrtDecayTest(testing.TestCase):\n class InverseTimeDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.InverseTimeDecay(\n+            schedules.InverseTimeDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 decay_rate=0.96,\n@@ -248,9 +230,7 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n-            initial_lr, k, decay_rate\n-        )\n+        decayed_lr = schedules.InverseTimeDecay(initial_lr, k, decay_rate)\n \n         for i in range(k + 1):\n             expected = initial_lr / (1 + i / k * decay_rate)\n@@ -262,7 +242,7 @@ class InverseTimeDecayTest(testing.TestCase):\n         k = 10\n         decay_rate = 0.96\n         step = backend.Variable(0)\n-        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+        decayed_lr = schedules.InverseTimeDecay(\n             initial_lr, k, decay_rate, staircase=True\n         )\n \n@@ -275,7 +255,7 @@ class InverseTimeDecayTest(testing.TestCase):\n class CosineDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.CosineDecay(\n+            schedules.CosineDecay(\n                 initial_learning_rate=0.05,\n                 decay_steps=10,\n                 alpha=0.1,\n@@ -295,9 +275,7 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n-                initial_lr, num_training_steps\n-            )\n+            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -311,7 +289,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 1500, 250):\n-            lr = learning_rate_schedule.CosineDecay(\n+            lr = schedules.CosineDecay(\n                 initial_lr,\n                 0,\n                 warmup_target=target_lr,\n@@ -327,7 +305,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n+            decayed_lr = schedules.CosineDecay(\n                 initial_lr, num_training_steps, alpha\n             )\n             expected = self.np_cosine_decay(step, num_training_steps, alpha)\n@@ -337,9 +315,7 @@ class CosineDecayTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecay(\n-                initial_lr, num_training_steps\n-            )\n+            decayed_lr = schedules.CosineDecay(initial_lr, num_training_steps)\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(decayed_lr(step), expected, 1e-6)\n \n@@ -349,7 +325,7 @@ class CosineDecayTest(testing.TestCase):\n         initial_lr = 0.0\n         target_lr = 10.0\n         for step in range(0, 3000, 250):\n-            lr = learning_rate_schedule.CosineDecay(\n+            lr = schedules.CosineDecay(\n                 initial_lr,\n                 decay_steps,\n                 warmup_target=target_lr,\n@@ -369,7 +345,7 @@ class CosineDecayTest(testing.TestCase):\n class CosineDecayRestartsTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            learning_rate_schedule.CosineDecayRestarts(\n+            schedules.CosineDecayRestarts(\n                 initial_learning_rate=0.05,\n                 first_decay_steps=10,\n                 alpha=0.1,\n@@ -396,7 +372,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -406,7 +382,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         num_training_steps = 1000\n         initial_lr = np.float64(1.0)\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps\n             )\n             expected = self.np_cosine_decay_restarts(step, num_training_steps)\n@@ -417,7 +393,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         alpha = 0.1\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, alpha=alpha\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -430,7 +406,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         m_mul = 0.9\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, m_mul=m_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n@@ -443,7 +419,7 @@ class CosineDecayRestartsTest(testing.TestCase):\n         initial_lr = 1.0\n         t_mul = 1.0\n         for step in range(0, 1500, 250):\n-            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+            decayed_lr = schedules.CosineDecayRestarts(\n                 initial_lr, num_training_steps, t_mul=t_mul\n             )\n             expected = self.np_cosine_decay_restarts(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
