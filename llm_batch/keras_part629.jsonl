{"custom_id": "keras#9bc903bf1b44a5212d308d467d96b00cd2ae522c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2659 | Contributors (this commit): 16 | Commits (past 90d): 12 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -540,7 +540,7 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)) and not subdir.startswith(\".\"):\n+            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))and subdir[0]!='.':\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e337f3a4de716cc97ec2aebf320b322761e12383", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2662 | Contributors (this commit): 16 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -540,7 +540,8 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))and subdir[0]!='.':\n+            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)):\n+                if not subdir.startswith(\".\"):\n                     if subdir.endswith(\"/\"):\n                         subdir = subdir[:-1]\n                     subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d636049753146a39ed59f9d2640fdf0af122163", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 56 | Files Changed: 7 | Hunks: 31 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 111 | Churn Cumulative: 4128 | Contributors (this commit): 9 | Commits (past 90d): 128 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,7 +80,7 @@ class KerasTensor:\n             \"class MyLayer(Layer):\\n\"\n             \"    def call(self, x):\\n\"\n             \"        return jax_fn(x)\\n\\n\"\n-            \"x = MyLayer()(x)\"\n+            \"x = MyLayer()(x)\\n\"\n             \"```\\n\"\n         )\n \n\n@@ -10,6 +10,7 @@ from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n+from keras_core.utils import traceback_utils\n \n \n class TensorFlowTrainer(base_trainer.Trainer):\n@@ -100,6 +101,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n \n+        @tf.autograph.experimental.do_not_convert\n         def one_step_on_iterator(iterator):\n             \"\"\"Runs a single training step given a Dataset iterator.\"\"\"\n             data = next(iterator)\n@@ -113,8 +115,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n+        @tf.autograph.experimental.do_not_convert\n         def multi_step_on_iterator(iterator):\n-            for _ in tf.range(self.steps_per_execution):\n+            for _ in range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n@@ -142,6 +145,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n \n+        @tf.autograph.experimental.do_not_convert\n         def one_step_on_iterator(iterator):\n             \"\"\"Runs a single test step given a Dataset iterator.\"\"\"\n             data = next(iterator)\n@@ -155,8 +159,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n+        @tf.autograph.experimental.do_not_convert\n         def multi_step_on_iterator(iterator):\n-            for _ in tf.range(self.steps_per_execution):\n+            for _ in range(self.steps_per_execution):\n                 outputs = one_step_on_iterator(iterator)\n             return outputs\n \n@@ -184,6 +189,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n \n+        @tf.autograph.experimental.do_not_convert\n         def one_step_on_data_distributed(data):\n             data = data[0]\n             outputs = self.distribute_strategy.run(\n@@ -196,6 +202,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n             return outputs\n \n+        @tf.autograph.experimental.do_not_convert\n         def multi_step_on_data(data):\n             outputs = one_step_on_data_distributed(data[:1])\n             for single_step_data in data[1:]:\n@@ -217,6 +224,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         self.predict_function = predict_function\n \n+    @traceback_utils.filter_traceback\n     def fit(\n         self,\n         x=None,\n@@ -346,6 +354,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n+    @traceback_utils.filter_traceback\n     def evaluate(\n         self,\n         x=None,\n@@ -405,6 +414,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return logs\n         return self._flatten_metrics_in_order(logs)\n \n+    @traceback_utils.filter_traceback\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n\n@@ -9,6 +9,7 @@ from keras_core.saving import saving_api\n from keras_core.saving import saving_lib\n from keras_core.utils import io_utils\n from keras_core.utils import summary_utils\n+from keras_core.utils import traceback_utils\n \n if backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.trainer import (\n@@ -164,6 +165,7 @@ class Model(Trainer, Layer):\n             \"Please use another name.\"\n         )\n \n+    @traceback_utils.filter_traceback\n     def get_layer(self, name=None, index=None):\n         \"\"\"Retrieves a layer based on either its name (unique) or index.\n \n@@ -204,6 +206,7 @@ class Model(Trainer, Layer):\n             \"Provide either a layer name or layer index at `get_layer`.\"\n         )\n \n+    @traceback_utils.filter_traceback\n     def summary(\n         self,\n         line_length=None,\n@@ -253,6 +256,7 @@ class Model(Trainer, Layer):\n             layer_range=layer_range,\n         )\n \n+    @traceback_utils.filter_traceback\n     def save(self, filepath, overwrite=True):\n         if not str(filepath).endswith(\".keras\"):\n             raise ValueError(\n@@ -269,6 +273,7 @@ class Model(Trainer, Layer):\n                 return\n         saving_lib.save_model(self, filepath)\n \n+    @traceback_utils.filter_traceback\n     def save_weights(self, filepath, overwrite=True):\n         if not str(filepath).endswith(\".weights.h5\"):\n             raise ValueError(\n@@ -285,6 +290,7 @@ class Model(Trainer, Layer):\n                 return\n         saving_lib.save_weights_only(self, filepath)\n \n+    @traceback_utils.filter_traceback\n     def load_weights(self, filepath, skip_mismatch=False, **kwargs):\n         saving_api.load_weights(\n             self, filepath, skip_mismatch=skip_mismatch, **kwargs\n@@ -335,6 +341,7 @@ class Model(Trainer, Layer):\n                 stacklevel=2,\n             )\n \n+    @traceback_utils.filter_traceback\n     def export(self, filepath):\n         raise NotImplementedError\n \n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-import keras_core\n from keras_core import testing\n from keras_core.operations import numpy as knp\n from keras_core.random import random\n@@ -63,23 +61,3 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         x_res = random.dropout(x, rate=0.8, seed=0)\n         self.assertGreater(knp.max(x_res), knp.max(x))\n         self.assertGreater(knp.sum(x_res == 0), 2)\n-\n-    @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"jax\",\n-        reason=\"This test requires `jax` as the backend.\",\n-    )\n-    def test_dropout_jax_jit_stateless(self):\n-        import jax\n-        import jax.numpy as jnp\n-\n-        x = knp.ones(3)\n-\n-        @jax.jit\n-        def train_step(x):\n-            with keras_core.backend.StatelessScope():\n-                x = keras_core.layers.Dropout(rate=0.1)(x, training=True)\n-            return x\n-\n-        keras_core.utils.traceback_utils.disable_traceback_filtering()\n-        x = train_step(x)\n-        assert isinstance(x, jnp.ndarray)\n\n@@ -16,10 +16,7 @@ class SeedGenerator:\n             )\n \n         def seed_initializer(*args, **kwargs):\n-            from keras_core.backend import convert_to_tensor\n-\n-            dtype = kwargs.get(\"dtype\", None)\n-            return convert_to_tensor([seed, 0], dtype=dtype)\n+            return [seed, 0]\n \n         self.state = Variable(\n             seed_initializer,\n\n@@ -176,25 +176,9 @@ class PyDatasetAdapter(DataAdapter):\n         self.shuffle = shuffle\n \n         # Grab the first example\n-        data = self.py_dataset[0]\n-        if not isinstance(data, tuple):\n-            raise ValueError(\n-                \"PyDataset.__getitem__() must return a tuple, either \"\n-                \"(input,) or (inputs, targets) or \"\n-                \"(inputs, targets, sample_weights). \"\n-                f\"Received: {data}\"\n-            )\n-        if self.class_weight is not None:\n-            if len(data) == 3:\n-                raise ValueError(\n-                    \"You cannot `class_weight` and `sample_weight` \"\n-                    \"at the same time.\"\n-                )\n-            if len(data) == 2:\n-                sw = data_adapter_utils.class_weight_to_sample_weights(\n-                    data[1], class_weight\n-                )\n-                data = data + (sw,)\n+        batch = self.py_dataset[0]\n+        # Run checks on it and format it\n+        batch = self._standardize_batch(batch)\n \n         def get_tensor_spec(x):\n             shape = x.shape\n@@ -208,7 +192,32 @@ class PyDatasetAdapter(DataAdapter):\n             shape[0] = None  # The batch size is not guaranteed to be static.\n             return tf.TensorSpec(shape=shape, dtype=x.dtype.name)\n \n-        self._output_signature = tf.nest.map_structure(get_tensor_spec, data)\n+        self._output_signature = tf.nest.map_structure(get_tensor_spec, batch)\n+\n+    def _standardize_batch(self, batch):\n+        if isinstance(batch, np.ndarray):\n+            batch = (batch,)\n+        if isinstance(batch, list):\n+            batch = tuple(batch)\n+        if not isinstance(batch, tuple) or len(batch) not in {1, 2, 3}:\n+            raise ValueError(\n+                \"PyDataset.__getitem__() must return a tuple, either \"\n+                \"(input,) or (inputs, targets) or \"\n+                \"(inputs, targets, sample_weights). \"\n+                f\"Received: {str(batch)[:100]}... of type {type(batch)}\"\n+            )\n+        if self.class_weight is not None:\n+            if len(batch) == 3:\n+                raise ValueError(\n+                    \"You cannot specify `class_weight` \"\n+                    \"and `sample_weight` at the same time.\"\n+                )\n+            if len(batch) == 2:\n+                sw = data_adapter_utils.class_weight_to_sample_weights(\n+                    batch[1], class_weight\n+                )\n+                batch = batch + (sw,)\n+        return batch\n \n     def _make_multiprocessed_generator_fn(self):\n         workers = self.py_dataset.workers\n@@ -244,11 +253,7 @@ class PyDatasetAdapter(DataAdapter):\n     def get_numpy_iterator(self):\n         gen_fn = self._make_multiprocessed_generator_fn()\n         for i, batch in enumerate(gen_fn()):\n-            if len(batch) == 2 and self.class_weight is not None:\n-                sw = data_adapter_utils.class_weight_to_sample_weights(\n-                    batch[1], self.class_weight\n-                )\n-                batch = batch + (sw,)\n+            batch = self._standardize_batch(batch)\n             yield batch\n             if i >= len(self.py_dataset) - 1 and self.enqueuer:\n                 self.enqueuer.stop()\n@@ -266,11 +271,11 @@ class PyDatasetAdapter(DataAdapter):\n     def on_epoch_end(self):\n         if self.enqueuer:\n             self.enqueuer.stop()\n-        self._py_dataset.on_epoch_end()\n+        self.py_dataset.on_epoch_end()\n \n     @property\n     def num_batches(self):\n-        return len(self._py_dataset)\n+        return len(self.py_dataset)\n \n     @property\n     def batch_size(self):\n\n@@ -7,6 +7,7 @@ from keras_core import optimizers\n from keras_core.saving import serialization_lib\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n+from keras_core.utils import traceback_utils\n from keras_core.utils import tracking\n \n \n@@ -18,6 +19,7 @@ class Trainer:\n         self.compiled = False\n         self.steps_per_execution = 1\n \n+    @traceback_utils.filter_traceback\n     @tracking.no_automatic_dependency_tracking\n     def compile(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff037a7ff8c3e9a1b25eb4d8ae4a8eee1724f655", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 165 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n+import keras_core\n from keras_core import testing\n from keras_core.operations import numpy as knp\n from keras_core.random import random\n@@ -61,3 +63,23 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         x_res = random.dropout(x, rate=0.8, seed=0)\n         self.assertGreater(knp.max(x_res), knp.max(x))\n         self.assertGreater(knp.sum(x_res == 0), 2)\n+\n+    @pytest.mark.skipif(\n+        keras_core.backend.backend() != \"jax\",\n+        reason=\"This test requires `jax` as the backend.\",\n+    )\n+    def test_dropout_jax_jit_stateless(self):\n+        import jax\n+        import jax.numpy as jnp\n+\n+        x = knp.ones(3)\n+\n+        @jax.jit\n+        def train_step(x):\n+            with keras_core.backend.StatelessScope():\n+                x = keras_core.layers.Dropout(rate=0.1)(x, training=True)\n+            return x\n+\n+        keras_core.utils.traceback_utils.disable_traceback_filtering()\n+        x = train_step(x)\n+        assert isinstance(x, jnp.ndarray)\n\n@@ -16,7 +16,10 @@ class SeedGenerator:\n             )\n \n         def seed_initializer(*args, **kwargs):\n-            return [seed, 0]\n+            from keras_core.backend import convert_to_tensor\n+\n+            dtype = kwargs.get(\"dtype\", None)\n+            return convert_to_tensor([seed, 0], dtype=dtype)\n \n         self.state = Variable(\n             seed_initializer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72720cb2902c1d867dba80f06fe12be489f62110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 69 | Churn Cumulative: 2281 | Contributors (this commit): 7 | Commits (past 90d): 46 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import tensorflow as tf\n from tensorflow.python.eager import context as tf_context\n \n from keras_core import callbacks as callbacks_module\n+from keras_core import metrics as metrics_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n@@ -487,6 +488,45 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             batch_outputs, np.concatenate, outputs\n         )\n \n+    # Backwards compatibility shims.\n+    @property\n+    def compiled_metrics(self):\n+        class DeprecatedCompiledMetric:\n+            def update_state(_, y, y_pred, sample_weight=None):\n+                return self._compiled_metrics_update_state(\n+                    y, y_pred, sample_weight=sample_weight\n+                )\n+\n+        return DeprecatedCompiledMetric()\n+\n+    def _compiled_metrics_update_state(self, y, y_pred, sample_weight=None):\n+        warnings.warn(\n+            \"`model.compiled_metrics()` is deprecated. \"\n+            \"Instead, use e.g.:\\n\"\n+            \"```\\n\"\n+            \"for metric in self.metrics:\\n\"\n+            \"    metric.update_state(y, y_pred)\\n\"\n+            \"```\\n\",\n+            stacklevel=2,\n+        )\n+        for metric in self.metrics:\n+            if isinstance(metric, metrics_module.Mean):\n+                metric.update_state(y_pred, sample_weight=sample_weight)\n+            else:\n+                metric.update_state(y, y_pred, sample_weight=sample_weight)\n+\n+    def compiled_loss(\n+        self, y, y_pred, sample_weight=None, regularization_losses=None\n+    ):\n+        warnings.warn(\n+            \"`model.compiled_loss()` is deprecated. \"\n+            \"Instead, use `model.compute_loss(x, y, y_pred, sample_weight)`.\",\n+            stacklevel=2,\n+        )\n+        return self.compute_loss(\n+            x=None, y=y, y_pred=y_pred, sample_weight=sample_weight\n+        )\n+\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n\n@@ -3,6 +3,7 @@ import queue\n import random\n import threading\n import time\n+import warnings\n import weakref\n from contextlib import closing\n \n@@ -94,18 +95,30 @@ class PyDataset:\n         self._use_multiprocessing = use_multiprocessing\n         self._max_queue_size = max_queue_size\n \n-    def _raise_if_super_not_called(self):\n-        raise RuntimeError(\n+    def _warn_if_super_not_called(self):\n+        warn = False\n+        if not hasattr(self, \"_workers\"):\n+            self._workers = 1\n+            warn = True\n+        if not hasattr(self, \"_use_multiprocessing\"):\n+            self._use_multiprocessing = False\n+            warn = True\n+        if not hasattr(self, \"_max_queue_size\"):\n+            self._max_queue_size = 10\n+            warn = True\n+        if warn:\n+            warnings.warn(\n                 \"Your `PyDataset` class should call \"\n                 \"`super().__init__(**kwargs)` in its constructor. \"\n                 \"`**kwargs` can include `workers`, \"\n-            \"`use_multiprocessing`, `max_queue_size`.\"\n+                \"`use_multiprocessing`, `max_queue_size`. Do not pass \"\n+                \"these arguments to `fit()`, as they will be ignored.\",\n+                stacklevel=2,\n             )\n \n     @property\n     def workers(self):\n-        if not hasattr(self, \"_workers\"):\n-            self._raise_if_super_not_called()\n+        self._warn_if_super_not_called()\n         return self._workers\n \n     @workers.setter\n@@ -114,8 +127,7 @@ class PyDataset:\n \n     @property\n     def use_multiprocessing(self):\n-        if not hasattr(self, \"_use_multiprocessing\"):\n-            self._raise_if_super_not_called()\n+        self._warn_if_super_not_called()\n         return self._use_multiprocessing\n \n     @use_multiprocessing.setter\n@@ -124,8 +136,7 @@ class PyDataset:\n \n     @property\n     def max_queue_size(self):\n-        if not hasattr(self, \"_max_queue_size\"):\n-            self._raise_if_super_not_called()\n+        self._warn_if_super_not_called()\n         return self._max_queue_size\n \n     @max_queue_size.setter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6aac2389d533b365fb28656092aeaf6848d3d285", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 1327 | Contributors (this commit): 4 | Commits (past 90d): 33 | Contributors (cumulative): 7 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -14,6 +14,7 @@ class KerasVariable:\n         self.name = name or auto_name(self.__class__.__name__)\n         dtype = standardize_dtype(dtype)\n         self._dtype = dtype\n+        self._shape = None\n         self._initializer = None\n         self.trainable = trainable\n         if callable(initializer):\n\n@@ -7,7 +7,13 @@ class TorchLayer(torch.nn.Module):\n     def forward(self, *args, **kwargs):\n         # TODO: find a good place to add the params. It should be added right\n         # after the variables are initialized.\n-        self.params = torch.nn.ParameterList(\n-            [variable.value for variable in self.variables]\n+        if not hasattr(self, \"torch_params\"):\n+            self.torch_params = torch.nn.ParameterList(\n+                [\n+                    torch.nn.Parameter(\n+                        variable.value, requires_grad=variable.trainable\n+                    )\n+                    for variable in self.variables\n+                ]\n             )\n         return Operation.__call__(self, *args, **kwargs)\n\n@@ -225,7 +225,7 @@ class PyDatasetAdapter(DataAdapter):\n                 )\n             if len(batch) == 2:\n                 sw = data_adapter_utils.class_weight_to_sample_weights(\n-                    batch[1], class_weight\n+                    batch[1], self.class_weight\n                 )\n                 batch = batch + (sw,)\n         return batch\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a53630c6dfe79822f15d9039fbf099af002aa37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,9 +4,7 @@ from keras_core.operations.operation import Operation\n \n \n class TorchLayer(torch.nn.Module):\n-    def forward(self, *args, **kwargs):\n-        # TODO: find a good place to add the params. It should be added right\n-        # after the variables are initialized.\n+    def parameters(self, recurse=True):\n         if not hasattr(self, \"torch_params\"):\n             self.torch_params = torch.nn.ParameterList(\n                 [\n@@ -16,4 +14,7 @@ class TorchLayer(torch.nn.Module):\n                     for variable in self.variables\n                 ]\n             )\n+        return torch.nn.Module.parameters(self, recurse=recurse)\n+\n+    def forward(self, *args, **kwargs):\n         return Operation.__call__(self, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6473add8f5b3c6f12801bb311e184846d2483c17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 51 | Churn Cumulative: 961 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,7 +72,9 @@ img_size = (299, 299, 3)\n model = xception.Xception(weights=\"imagenet\")\n \n # The local path to our target image\n-img_path = keras.utils.get_file(\"elephant.jpg\", \"https://i.imgur.com/Bvro0YD.png\")\n+img_path = keras.utils.get_file(\n+    \"elephant.jpg\", \"https://i.imgur.com/Bvro0YD.png\"\n+)\n display(Image(img_path))\n \n \"\"\"\n@@ -112,7 +114,9 @@ def get_gradients(img_input, top_pred_idx):\n     return grads\n \n \n-def get_integrated_gradients(img_input, top_pred_idx, baseline=None, num_steps=50):\n+def get_integrated_gradients(\n+    img_input, top_pred_idx, baseline=None, num_steps=50\n+):\n     \"\"\"Computes Integrated Gradients for a predicted label.\n \n     Args:\n@@ -240,9 +244,9 @@ class GradVisualizer:\n \n         # 2. Transform the attributions by a linear function f(x) = a*x + b such that\n         # f(m) = 1.0 and f(e) = lower_end\n-        transformed_attributions = (1 - lower_end) * (np.abs(attributions) - e) / (\n-            m - e\n-        ) + lower_end\n+        transformed_attributions = (1 - lower_end) * (\n+            np.abs(attributions) - e\n+        ) / (m - e) + lower_end\n \n         # 3. Make sure that the sign of transformed attributions is the same as original attributions\n         transformed_attributions *= np.sign(attributions)\n@@ -287,7 +291,10 @@ class GradVisualizer:\n         return opened\n \n     def draw_outlines(\n-        self, attributions, percentage=90, connected_component_structure=np.ones((3, 3))\n+        self,\n+        attributions,\n+        percentage=90,\n+        connected_component_structure=np.ones((3, 3)),\n     ):\n         # 1. Binarize the attributions.\n         attributions = self.binarize(attributions)\n@@ -309,7 +316,9 @@ class GradVisualizer:\n             component_sums.append((component_sum, mask))\n \n         # 5. Compute the percentage of top components to keep\n-        sorted_sums_and_masks = sorted(component_sums, key=lambda x: x[0], reverse=True)\n+        sorted_sums_and_masks = sorted(\n+            component_sums, key=lambda x: x[0], reverse=True\n+        )\n         sorted_sums = list(zip(*sorted_sums_and_masks))[0]\n         cumulative_sorted_sums = np.cumsum(sorted_sums)\n         cutoff_threshold = percentage * total / 100\n\n@@ -162,7 +162,9 @@ x_test_2 = pairs_test[:, 1]\n \"\"\"\n \n \n-def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False):\n+def visualize(\n+    pairs, labels, to_show=6, num_col=3, predictions=None, test=False\n+):\n     \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n \n     Arguments:\n@@ -212,10 +214,14 @@ def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False)\n         else:\n             ax = axes[i // num_col, i % num_col]\n \n-        ax.imshow(ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\")\n+        ax.imshow(\n+            ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\"\n+        )\n         ax.set_axis_off()\n         if test:\n-            ax.set_title(\"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0]))\n+            ax.set_title(\n+                \"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0])\n+            )\n         else:\n             ax.set_title(\"Label: {}\".format(labels[i]))\n     if test:\n@@ -293,7 +299,9 @@ input_2 = keras.layers.Input((28, 28, 1))\n tower_1 = embedding_network(input_1)\n tower_2 = embedding_network(input_2)\n \n-merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))([tower_1, tower_2])\n+merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))(\n+    [tower_1, tower_2]\n+)\n normal_layer = keras.layers.BatchNormalization()(merge_layer)\n output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n@@ -331,9 +339,7 @@ def loss(margin=1):\n \n         square_pred = ops.square(y_pred)\n         margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n-        return ops.mean(\n-            (1 - y_true) * square_pred + (y_true) * margin_square\n-        )\n+        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n \n     return contrastive_loss\n \n@@ -342,7 +348,9 @@ def loss(margin=1):\n ## Compile the model with the contrastive loss\n \"\"\"\n \n-siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n+siamese.compile(\n+    loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"]\n+)\n siamese.summary()\n \n \n@@ -403,7 +411,9 @@ print(\"test loss, test acc:\", results)\n \"\"\"\n \n predictions = siamese.predict([x_test_1, x_test_2])\n-visualize(pairs_test, labels_test, to_show=3, predictions=predictions, test=True)\n+visualize(\n+    pairs_test, labels_test, to_show=3, predictions=predictions, test=True\n+)\n \n \"\"\"\n **Example available on HuggingFace**\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1d0cf90f89c6ef753208905cf7b3d30cb7657a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 83 | Churn Cumulative: 83 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,83 @@\n+import torch\n+import torch.nn as nn\n+import torch.optim as optim\n+from keras_core import layers\n+import keras_core\n+import numpy as np\n+\n+# Model / data parameters\n+num_classes = 10\n+input_shape = (28, 28, 1)\n+learning_rate = 0.01\n+batch_size = 64\n+num_epochs = 1\n+\n+# Load the data and split it between train and test sets\n+(x_train, y_train), (x_test, y_test) = keras_core.datasets.mnist.load_data()\n+\n+# Scale images to the [0, 1] range\n+x_train = x_train.astype(\"float32\") / 255\n+x_test = x_test.astype(\"float32\") / 255\n+# Make sure images have shape (28, 28, 1)\n+x_train = np.expand_dims(x_train, -1)\n+x_test = np.expand_dims(x_test, -1)\n+print(\"x_train shape:\", x_train.shape)\n+print(x_train.shape[0], \"train samples\")\n+print(x_test.shape[0], \"test samples\")\n+\n+# Create the Keras model\n+model = keras_core.Sequential(\n+    [\n+        layers.Input(shape=(28, 28, 1)),\n+        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(pool_size=(2, 2)),\n+        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(pool_size=(2, 2)),\n+        layers.Flatten(),\n+        layers.Dropout(0.5),\n+        layers.Dense(num_classes),\n+    ]\n+)\n+\n+# Instantiate the torch optimizer\n+print(\"Num params:\", len(list(model.parameters())))\n+optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n+\n+# Instantiate the torch loss function\n+loss_fn = nn.CrossEntropyLoss()\n+\n+\n+def train(model, train_loader, num_epochs):\n+    for epoch in range(num_epochs):\n+        running_loss = 0.0\n+        for batch_idx, (inputs, targets) in enumerate(train_loader):\n+            # Forward pass\n+            outputs = model(inputs)\n+            loss = loss_fn(outputs, targets)\n+\n+            # Backward and optimize\n+            optimizer.zero_grad()\n+            loss.backward()\n+            optimizer.step()\n+\n+            running_loss += loss.item()\n+\n+            # Print loss statistics\n+            if (batch_idx + 1) % 10 == 0:\n+                print(\n+                    f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss / 10}\"\n+                )\n+                running_loss = 0.0\n+\n+\n+# Create a TensorDataset\n+dataset = torch.utils.data.TensorDataset(\n+    torch.from_numpy(x_train), torch.from_numpy(y_train)\n+)\n+\n+# Create a DataLoader\n+train_loader = torch.utils.data.DataLoader(\n+    dataset, batch_size=batch_size, shuffle=False\n+)\n+\n+train(model, train_loader, num_epochs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06d7bc5994f7e17f34f8f92ec66614f4f70973c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 239 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,8 +35,10 @@ def to_torch_dtype(dtype):\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n-        self._value = convert_to_tensor(value, dtype=self._dtype)\n-        self._value.requires_grad_(self.trainable)\n+        self._value = torch.nn.Parameter(\n+            convert_to_tensor(value, dtype=self._dtype),\n+            requires_grad=self.trainable,\n+        )\n \n     def _direct_assign(self, value):\n         with torch.no_grad():\n\n@@ -7,12 +7,7 @@ class TorchLayer(torch.nn.Module):\n     def parameters(self, recurse=True):\n         if not hasattr(self, \"torch_params\"):\n             self.torch_params = torch.nn.ParameterList(\n-                [\n-                    torch.nn.Parameter(\n-                        variable.value, requires_grad=variable.trainable\n-                    )\n-                    for variable in self.variables\n-                ]\n+                [variable.value for variable in self.variables]\n             )\n         return torch.nn.Module.parameters(self, recurse=recurse)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c63de3adca525e73fa6a25898b2969e9e61d8264", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 142 | Lines Deleted: 197 | Files Changed: 2 | Hunks: 92 | Methods Changed: 20 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 339 | Churn Cumulative: 1793 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import warnings\n import logging\n import os\n import sys\n import time\n+import warnings\n \n import tensorflow.summary as summary\n from tensorflow import nest\n@@ -16,12 +16,10 @@ from keras_core.callbacks.callback import Callback\n from keras_core.layers import Embedding\n from keras_core.optimizers import Optimizer\n from keras_core.optimizers.schedules import learning_rate_schedule\n-from keras_core.utils import file_utils\n \n \n @keras_core_export(\"keras_core.callbacks.TensorBoard\")\n class TensorBoard(Callback):\n-\n     \"\"\"Enable visualizations for TensorBoard.\n \n     TensorBoard is a visualization tool provided with TensorFlow. A TensorFlow\n@@ -34,10 +32,10 @@ class TensorBoard(Callback):\n     * Weight histograms\n     * Sampled profiling\n \n-    When used in `Model.evaluate` or regular validation\n+    When used in `model.evaluate()` or regular validation\n     in addition to epoch summaries, there will be a summary that records\n-    evaluation metrics vs `Model.optimizer.iterations` written. The metric names\n-    will be prepended with `evaluation`, with `Model.optimizer.iterations` being\n+    evaluation metrics vs `model.optimizer.iterations` written. The metric names\n+    will be prepended with `evaluation`, with `model.optimizer.iterations` being\n     the step in the visualized TensorBoard.\n \n     If you have installed TensorFlow with pip, you should be able\n@@ -52,34 +50,37 @@ class TensorBoard(Callback):\n \n     Args:\n         log_dir: the path of the directory where to save the log files to be\n-            parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir,\n-            'logs') This directory should not be reused by any other callbacks.\n+            parsed by TensorBoard. e.g.,\n+            `log_dir = os.path.join(working_dir, 'logs')`.\n+            This directory should not be reused by any other callbacks.\n         histogram_freq: frequency (in epochs) at which to compute\n             weight histograms for the layers of the model. If set to 0,\n             histograms won't be computed. Validation data (or split) must be\n             specified for histogram visualizations.\n-        write_graph: TODO: still not supported.\n-            whether to visualize the graph in TensorBoard. The log file\n-            can become quite large when write_graph is set to True.\n+        write_graph:  (Not supported at this time)\n+            Whether to visualize the graph in TensorBoard.\n+            Note that the log file can become quite large\n+            when `write_graph` is set to `True`.\n         write_images: whether to write model weights to visualize as image in\n             TensorBoard.\n         write_steps_per_second: whether to log the training steps per second\n             into TensorBoard. This supports both epoch and batch frequency\n             logging.\n-        update_freq: `'batch'` or `'epoch'` or integer. When using `'epoch'`,\n+        update_freq: `\"batch\"` or `\"epoch\"` or integer. When using `\"epoch\"`,\n             writes the losses and metrics to TensorBoard after every epoch.\n             If using an integer, let's say `1000`, all metrics and losses\n             (including custom ones added by `Model.compile`) will be logged to\n-            TensorBoard every 1000 batches. `'batch'` is a synonym for `1`,\n+            TensorBoard every 1000 batches. `\"batch\"` is a synonym for 1,\n             meaning that they will be written every batch.\n             Note however that writing too frequently to TensorBoard can slow\n             down your training, especially when used with distribution\n             strategies as it will incur additional synchronization overhead.\n             Batch-level summary writing is also available via `train_step`\n             override. Please see\n-            [TensorBoard Scalars tutorial](https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging)  # noqa: E501\n+            [TensorBoard Scalars tutorial](\n+                https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging)  # noqa: E501\n             for more details.\n-        profile_batch: TODO: still not supported.\n+        profile_batch: (Not supported at this time)\n             Profile the batch(es) to sample compute characteristics.\n             profile_batch must be a non-negative integer or a tuple of integers.\n             A pair of positive integers signify a range of batches to profile.\n@@ -118,7 +119,7 @@ class TensorBoard(Callback):\n     model.compile('sgd', 'mse')\n \n     # Make sure to set `update_freq=N` to log a batch-level summary every N\n-    # batches.  In addition to any `tf.summary` contained in `Model.call`,\n+    # batches.  In addition to any `tf.summary` contained in `model.call()`,\n     # metrics added in `Model.compile` will be logged every N batches.\n     tb_callback = keras_core.callbacks.TensorBoard('./logs', update_freq=1)\n     model.fit(x_train, y_train, callbacks=[tb_callback])\n@@ -628,9 +629,9 @@ def keras_model_summary(name, data, step=None):\n         name: A name for this summary. The summary tag used for TensorBoard will\n             be this name prefixed by any active name scopes.\n         data: A Keras Model to write.\n-        step: Explicit `int64`-castable monotonic step value for this summary. If\n-            omitted, this defaults to `tf.summary.experimental.get_step()`, which\n-            must not be None.\n+        step: Explicit `int64`-castable monotonic step value for this summary.\n+            If omitted, this defaults to `tf.summary.experimental.get_step()`,\n+            which must not be `None`.\n \n     Returns:\n         True on success, or False if no summary was written because no default\n@@ -638,7 +639,7 @@ def keras_model_summary(name, data, step=None):\n \n     Raises:\n         ValueError: if a default writer exists, but no step was provided and\n-            `tf.summary.experimental.get_step()` is None.\n+            `tf.summary.experimental.get_step()` is `None`.\n     \"\"\"\n     summary_metadata = SummaryMetadata()\n     # Hard coding a plugin name. Please refer to go/tb-plugin-name-hardcode for\n\n@@ -1,5 +1,6 @@\n import collections\n import os\n+import random\n \n import numpy as np\n import tensorflow.summary as summary\n@@ -7,19 +8,13 @@ from tensorflow.compat.v1 import SummaryMetadata\n from tensorflow.core.util import event_pb2\n from tensorflow.python.lib.io import tf_record\n \n-from keras_core import backend\n from keras_core import callbacks\n+from keras_core import layers\n from keras_core import losses\n from keras_core import models\n from keras_core import operations as ops\n from keras_core import optimizers\n from keras_core import testing\n-from keras_core.layers import Conv2D\n-from keras_core.layers import Dense\n-from keras_core.layers import Embedding\n-from keras_core.layers import Flatten\n-from keras_core.layers import Input\n-from keras_core.layers import Layer\n from keras_core.optimizers import schedules\n \n # Note: this file and tensorboard in general has a dependency on tensorflow\n@@ -31,7 +26,7 @@ from keras_core.optimizers import schedules\n _ObservedSummary = collections.namedtuple(\"_ObservedSummary\", (\"logdir\", \"tag\"))\n \n \n-class _SummaryIterator(object):\n+class _SummaryIterator:\n     \"\"\"Yields `Event` protocol buffers from a given path.\"\"\"\n \n     def __init__(self, path):\n@@ -63,19 +58,6 @@ class _SummaryFile:\n         self.convert_from_v2_summary_proto = False\n \n \n-def get_model_from_layers(model_layers, input_shape, name=None):\n-    model = models.Sequential(name=name)\n-    model.add(\n-        Input(\n-            input_shape,\n-            dtype=\"float32\",\n-        )\n-    )\n-    for layer in model_layers:\n-        model.add(layer)\n-    return model\n-\n-\n def list_summaries(logdir):\n     \"\"\"Read all summaries under the logdir into a `_SummaryFile`.\n \n@@ -139,32 +121,31 @@ def list_summaries(logdir):\n \n \n class TestTensorBoardV2(testing.TestCase):\n-    def setUp(self):\n-        super(TestTensorBoardV2, self).setUp()\n-        self.logdir = os.path.join(self.get_temp_dir(), \"tb\")\n-        self.train_dir = os.path.join(self.logdir, \"train\")\n-        self.validation_dir = os.path.join(self.logdir, \"validation\")\n+    def _get_log_dirs(self):\n+        logdir = os.path.join(\n+            self.get_temp_dir(), str(random.randint(1, 1e7)), \"tb\"\n+        )\n+        train_dir = os.path.join(logdir, \"train\")\n+        validation_dir = os.path.join(logdir, \"validation\")\n+        return logdir, train_dir, validation_dir\n \n     def _get_model(self, compile_model=True):\n-        layers = [\n-            Conv2D(8, (3, 3)),\n-            Flatten(),\n-            Dense(1),\n+        model = models.Sequential(\n+            [\n+                layers.Input((10, 10, 1)),\n+                layers.Flatten(),\n+                layers.Dense(1),\n             ]\n-        model = get_model_from_layers(layers, input_shape=(10, 10, 1))\n-\n+        )\n         if compile_model:\n-            opt = optimizers.SGD(learning_rate=0.001)\n-            model.compile(opt, \"mse\")\n+            model.compile(\"sgd\", \"mse\")\n         return model\n \n-    def test_TensorBoard_default_logdir(self):\n-        \"\"\"Regression test for cross-platform pathsep in default logdir.\"\"\"\n-        os.chdir(self.get_temp_dir())\n-\n+    def test_TensorBoard_basic(self):\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard()  # no logdir specified\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir)\n \n         model.fit(\n             x,\n@@ -175,55 +156,25 @@ class TestTensorBoardV2(testing.TestCase):\n             callbacks=[tb_cbk],\n         )\n \n-        summary_file = list_summaries(logdir=\".\")\n-        train_dir = os.path.join(\".\", \"logs\", \"train\")\n-        validation_dir = os.path.join(\".\", \"logs\", \"validation\")\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.scalars,\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=validation_dir, tag=\"evaluation_loss_vs_iterations\"\n-                ),\n-            },\n-        )\n-\n-    def test_TensorBoard_basic(self):\n-        model = self._get_model()\n-        x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard(self.logdir)\n-\n-        model.fit(\n-            x,\n-            y,\n-            batch_size=2,\n-            epochs=2,\n-            validation_data=(x, y),\n-            callbacks=[tb_cbk],\n-        )\n-\n-        summary_file = list_summaries(self.logdir)\n-        self.assertEqual(\n-            summary_file.scalars,\n-            {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n             },\n         )\n \n     def test_TensorBoard_across_invocations(self):\n-        \"\"\"Regression test for summary writer resource use-after-free.\n-\n-        See: <https://github.com/tensorflow/tensorflow/issues/25707>\n-        \"\"\"\n+        \"\"\"Regression test for summary writer resource use-after-free.\"\"\"\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard(self.logdir)\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir)\n \n         for _ in (1, 2):\n             model.fit(\n@@ -235,14 +186,14 @@ class TestTensorBoardV2(testing.TestCase):\n                 callbacks=[tb_cbk],\n             )\n \n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n             },\n@@ -251,12 +202,12 @@ class TestTensorBoardV2(testing.TestCase):\n     def test_TensorBoard_no_spurious_event_files(self):\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard(self.logdir)\n-\n+        logdir, train_dir, _ = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir)\n         model.fit(x, y, batch_size=2, epochs=2, callbacks=[tb_cbk])\n \n         events_file_run_basenames = set()\n-        for dirpath, _, filenames in os.walk(self.train_dir):\n+        for dirpath, _, filenames in os.walk(train_dir):\n             if any(fn.startswith(\"events.out.\") for fn in filenames):\n                 events_file_run_basenames.add(os.path.basename(dirpath))\n         self.assertEqual(events_file_run_basenames, {\"train\"})\n@@ -264,7 +215,8 @@ class TestTensorBoardV2(testing.TestCase):\n     def test_TensorBoard_batch_metrics(self):\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard(self.logdir, update_freq=1)\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir, update_freq=1)\n \n         model.fit(\n             x,\n@@ -275,15 +227,15 @@ class TestTensorBoardV2(testing.TestCase):\n             callbacks=[tb_cbk],\n         )\n \n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n             },\n@@ -293,7 +245,7 @@ class TestTensorBoardV2(testing.TestCase):\n         model = self._get_model(compile_model=False)\n         opt = optimizers.SGD(schedules.CosineDecay(0.01, 1))\n         model.compile(opt, \"mse\")\n-\n+        logdir, train_dir, _ = self._get_log_dirs()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n \n         model.fit(\n@@ -301,17 +253,15 @@ class TestTensorBoardV2(testing.TestCase):\n             y,\n             batch_size=2,\n             epochs=2,\n-            callbacks=[callbacks.TensorBoard(self.logdir)],\n+            callbacks=[callbacks.TensorBoard(logdir)],\n         )\n \n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(\n-                    logdir=self.train_dir, tag=\"epoch_learning_rate\"\n-                ),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n             },\n         )\n \n@@ -319,7 +269,7 @@ class TestTensorBoardV2(testing.TestCase):\n         model = self._get_model(compile_model=False)\n         opt = optimizers.SGD(schedules.CosineDecay(0.01, 1))\n         model.compile(opt, \"mse\")\n-\n+        logdir, train_dir, _ = self._get_log_dirs()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n \n         model.fit(\n@@ -330,7 +280,7 @@ class TestTensorBoardV2(testing.TestCase):\n             verbose=0,\n             callbacks=[\n                 callbacks.TensorBoard(\n-                    self.logdir,\n+                    logdir,\n                     update_freq=1,\n                     profile_batch=0,\n                     write_steps_per_second=True,\n@@ -338,20 +288,18 @@ class TestTensorBoardV2(testing.TestCase):\n             ],\n         )\n \n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n-                    logdir=self.train_dir, tag=\"epoch_learning_rate\"\n+                    logdir=train_dir, tag=\"epoch_steps_per_second\"\n                 ),\n                 _ObservedSummary(\n-                    logdir=self.train_dir, tag=\"epoch_steps_per_second\"\n-                ),\n-                _ObservedSummary(\n-                    logdir=self.train_dir, tag=\"batch_steps_per_second\"\n+                    logdir=train_dir, tag=\"batch_steps_per_second\"\n                 ),\n             },\n         )\n@@ -359,7 +307,8 @@ class TestTensorBoardV2(testing.TestCase):\n     def test_TensorBoard_weight_histograms(self):\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n-        tb_cbk = callbacks.TensorBoard(self.logdir, histogram_freq=1)\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir, histogram_freq=1)\n \n         model.fit(\n             x,\n@@ -369,32 +318,41 @@ class TestTensorBoardV2(testing.TestCase):\n             validation_data=(x, y),\n             callbacks=[tb_cbk],\n         )\n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n \n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n             },\n         )\n         self.assertEqual(\n             self._strip_layer_names(summary_file.histograms, \"sequential\"),\n-            {_ObservedSummary(logdir=self.train_dir, tag=\"histogram\")},\n+            {_ObservedSummary(logdir=train_dir, tag=\"histogram\")},\n         )\n \n     def test_TensorBoard_weight_images(self):\n         model = self._get_model()\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n         tb_cbk = callbacks.TensorBoard(\n-            self.logdir, histogram_freq=1, write_images=True\n+            logdir, histogram_freq=1, write_images=True\n         )\n         model_type = \"sequential\"\n-\n+        model = models.Sequential(\n+            [\n+                layers.Input((10, 10, 1)),\n+                layers.Conv2D(3, 10),\n+                layers.GlobalAveragePooling2D(),\n+                layers.Dense(1),\n+            ]\n+        )\n+        model.compile(\"sgd\", \"mse\")\n         model.fit(\n             x,\n             y,\n@@ -403,15 +361,15 @@ class TestTensorBoardV2(testing.TestCase):\n             validation_data=(x, y),\n             callbacks=[tb_cbk],\n         )\n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n \n         self.assertEqual(\n             summary_file.scalars,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n             },\n@@ -419,13 +377,13 @@ class TestTensorBoardV2(testing.TestCase):\n         self.assertEqual(\n             self._strip_layer_names(summary_file.histograms, model_type),\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"histogram\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"histogram\"),\n             },\n         )\n         expected_image_summaries = {\n-            _ObservedSummary(logdir=self.train_dir, tag=\"image\"),\n-            _ObservedSummary(logdir=self.train_dir, tag=\"bias/image\"),\n-            _ObservedSummary(logdir=self.train_dir, tag=\"kernel/image\"),\n+            _ObservedSummary(logdir=train_dir, tag=\"image\"),\n+            _ObservedSummary(logdir=train_dir, tag=\"bias/image\"),\n+            _ObservedSummary(logdir=train_dir, tag=\"kernel/image\"),\n         }\n         self.assertEqual(\n             self._strip_variable_names(summary_file.images),\n@@ -433,18 +391,20 @@ class TestTensorBoardV2(testing.TestCase):\n         )\n \n     def test_TensorBoard_projector_callback(self):\n-        layers = [\n-            Embedding(10, 10, name=\"test_embedding\"),\n-            Dense(10, activation=\"relu\"),\n-            Dense(1, activation=\"sigmoid\"),\n+        model = models.Sequential(\n+            [\n+                layers.Input((10,)),\n+                layers.Embedding(10, 10, name=\"test_embedding\"),\n+                layers.Dense(1, activation=\"sigmoid\"),\n             ]\n-        model = get_model_from_layers(layers, input_shape=(10,))\n+        )\n         model.compile(\n             optimizer=\"adam\", loss=losses.BinaryCrossentropy(from_logits=True)\n         )\n         x, y = np.ones((10, 10)), np.ones((10, 10))\n+        logdir, _, _ = self._get_log_dirs()\n         tb_cbk = callbacks.TensorBoard(\n-            self.logdir,\n+            logdir,\n             embeddings_freq=1,\n             embeddings_metadata={\"test_embedding\": \"metadata.tsv\"},\n         )\n@@ -458,7 +418,7 @@ class TestTensorBoardV2(testing.TestCase):\n             callbacks=[tb_cbk],\n         )\n \n-        with open(os.path.join(self.logdir, \"projector_config.pbtxt\")) as f:\n+        with open(os.path.join(logdir, \"projector_config.pbtxt\")) as f:\n             self.assertEqual(\n                 f.readlines(),\n                 [\n@@ -489,40 +449,45 @@ class TestTensorBoardV2(testing.TestCase):\n                     metadata=metadata,\n                 )\n \n-        class LayerWithSummary(Layer):\n+        class LayerWithSummary(layers.Layer):\n             def call(self, x):\n                 scalar_v2_mock(\"custom_summary\", ops.sum(x))\n                 return x\n \n-        model = get_model_from_layers(\n-            [LayerWithSummary()], input_shape=(5,), name=\"model\"\n+        model = models.Sequential(\n+            [\n+                layers.Input((5,)),\n+                LayerWithSummary(),\n+            ]\n         )\n \n-        model.compile(\"sgd\", \"mse\", jit_compile=False)  # summary ops can't xla\n-        tb_cbk = callbacks.TensorBoard(self.logdir, update_freq=1)\n+        # summary ops not compatible with XLA\n+        model.compile(\"sgd\", \"mse\", jit_compile=False)\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n+        tb_cbk = callbacks.TensorBoard(logdir, update_freq=1)\n         x, y = np.ones((10, 5)), np.ones((10, 5))\n         model.fit(\n             x, y, batch_size=2, validation_data=(x, y), callbacks=[tb_cbk]\n         )\n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         # TODO: tensorflow will tag with model/layer_with_summary/custom_summary\n         # Jax will only use custom_summary tag\n         self.assertEqual(\n             self._strip_to_only_final_name(summary_file.scalars),\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n-                _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-                _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n                 ),\n                 _ObservedSummary(\n-                    logdir=self.train_dir,\n+                    logdir=train_dir,\n                     tag=\"custom_summary\",\n                 ),\n                 _ObservedSummary(\n-                    logdir=self.validation_dir,\n+                    logdir=validation_dir,\n                     tag=\"custom_summary\",\n                 ),\n             },\n@@ -530,20 +495,20 @@ class TestTensorBoardV2(testing.TestCase):\n         # self.assertEqual(\n         #     summary_file.scalars,\n         #     {\n-        #         _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n-        #         _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n-        #         _ObservedSummary(logdir=self.validation_dir,\n+        #         _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n+        #         _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+        #         _ObservedSummary(logdir=validation_dir,\n         #               tag=\"epoch_loss\"),\n         #         _ObservedSummary(\n-        #             logdir=self.validation_dir,\n+        #             logdir=validation_dir,\n         #             tag=\"evaluation_loss_vs_iterations\",\n         #         ),\n         #         _ObservedSummary(\n-        #             logdir=self.train_dir,\n+        #             logdir=train_dir,\n         #             tag=\"model/layer_with_summary/custom_summary\",\n         #         ),\n         #         _ObservedSummary(\n-        #             logdir=self.validation_dir,\n+        #             logdir=validation_dir,\n         #             tag=\"model/layer_with_summary/custom_summary\",\n         #         ),\n         #     },\n@@ -618,13 +583,13 @@ class TestTensorBoardV2(testing.TestCase):\n         return result\n \n     def test_TensorBoard_non_blocking(self):\n-        model = models.Sequential([Dense(1)])\n+        logdir, _, _ = self._get_log_dirs()\n+        model = models.Sequential([layers.Dense(1)])\n         model.optimizer = optimizers.Adam()\n-        tb = callbacks.TensorBoard(self.logdir)\n+        tb = callbacks.TensorBoard(logdir)\n         cb_list = callbacks.CallbackList(\n             [tb], model=model, epochs=1, steps=100, verbose=0\n         )\n-\n         tensor = ops.convert_to_tensor(1.0)\n \n         def mock_numpy():\n@@ -654,28 +619,6 @@ class TestTensorBoardV2(testing.TestCase):\n         cb_list.on_predict_batch_end(logs)\n         cb_list.on_predict_end(logs)\n \n-\n-# Note that this test specifies model_type explicitly.\n-class TestTensorBoardV2NonParameterizedTest(testing.TestCase):\n-    def setUp(self):\n-        super(TestTensorBoardV2NonParameterizedTest, self).setUp()\n-        self.logdir = os.path.join(self.get_temp_dir(), \"tb\")\n-        self.train_dir = os.path.join(self.logdir, \"train\")\n-        self.validation_dir = os.path.join(self.logdir, \"validation\")\n-\n-    def _get_seq_model(self):\n-        model = models.Sequential(\n-            [\n-                Input((10, 10, 1)),\n-                Conv2D(8, (3, 3)),\n-                Flatten(),\n-                Dense(1),\n-            ]\n-        )\n-        opt = optimizers.SGD(learning_rate=0.001)\n-        model.compile(opt, \"mse\")\n-        return model\n-\n     def _count_xplane_file(self, logdir):\n         profile_dir = os.path.join(logdir, \"plugins\", \"profile\")\n         count = 0\n@@ -689,8 +632,9 @@ class TestTensorBoardV2NonParameterizedTest(testing.TestCase):\n \n     def fitModelAndAssertKerasModelWritten(self, model):\n         x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n+        logdir, train_dir, validation_dir = self._get_log_dirs()\n         tb_cbk = callbacks.TensorBoard(\n-            self.logdir, write_graph=True, profile_batch=0\n+            logdir, write_graph=True, profile_batch=0\n         )\n         model.fit(\n             x,\n@@ -700,11 +644,11 @@ class TestTensorBoardV2NonParameterizedTest(testing.TestCase):\n             validation_data=(x, y),\n             callbacks=[tb_cbk],\n         )\n-        summary_file = list_summaries(self.logdir)\n+        summary_file = list_summaries(logdir)\n         self.assertEqual(\n             summary_file.tensors,\n             {\n-                _ObservedSummary(logdir=self.train_dir, tag=\"keras\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"keras\"),\n             },\n         )\n         if not model.run_eagerly:\n@@ -768,7 +712,7 @@ class TestTensorBoardV2NonParameterizedTest(testing.TestCase):\n         # model = self._get_seq_model()\n         # x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n         # tb_cbk = callbacks.TensorBoard(\n-        #     self.logdir, histogram_freq=1, profile_batch=1, write_graph=False\n+        #     logdir, histogram_freq=1, profile_batch=1, write_graph=False\n         # )\n \n         # model.fit(\n@@ -779,13 +723,13 @@ class TestTensorBoardV2NonParameterizedTest(testing.TestCase):\n         #     validation_data=(x, y),\n         #     callbacks=[tb_cbk],\n         # )\n-        # summary_file = list_summaries(self.logdir)\n+        # summary_file = list_summaries(logdir)\n \n         # self.assertEqual(\n         #     summary_file.tensors,\n         #     {\n-        #         _ObservedSummary(logdir=self.train_dir, tag=\"batch_1\"),\n+        #         _ObservedSummary(logdir=train_dir, tag=\"batch_1\"),\n         #     },\n         # )\n-        # self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n+        # self.assertEqual(1, self._count_xplane_file(logdir=logdir))\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
