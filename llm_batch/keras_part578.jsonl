{"custom_id": "keras#9b55a1ca4ad7612dc1adbf3461347bcc15e2e803", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 12/6 | Churn Δ: 53 | Churn Cumulative: 1507 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: 0.18181818181818182\n\nDIFF:\n@@ -1,5 +1,6 @@\n \"\"\"Test Model inference and save/load with an ExtensionType.\"\"\"\n \n+import os\n import typing\n \n import tensorflow.compat.v2 as tf\n@@ -89,6 +90,13 @@ class ExtensionTypeTest(tf.test.TestCase):\n                 serving_fn(args_0=mt.values, args_0_1=mt.mask)[\"lambda\"], mt\n             )\n \n+        with self.subTest(\"keras v3\"):\n+            path = os.path.join(self.create_tempdir().full_path, \"model.keras\")\n+            model.save(path)\n+            loaded_model = load_model(path, safe_mode=False)\n+            self.assertEqual(loaded_model.input.type_spec, mt_spec)\n+            self.assertEqual(loaded_model(mt), mt)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -220,12 +220,21 @@ def serialize_keras_object(obj):\n                 ts_config,\n             )\n         )\n+        spec_name = obj.__class__.__name__\n+        registered_name = None\n+        if hasattr(obj, \"_tf_extension_type_fields\"):\n+            # Special casing for ExtensionType\n+            ts_config = tf.experimental.extension_type.as_dict(obj)\n+            ts_config = serialize_dict(ts_config)\n+            registered_name = object_registration.get_registered_name(\n+                obj.__class__\n+            )\n         return {\n             \"class_name\": \"__typespec__\",\n-            \"spec_name\": obj.__class__.__name__,\n+            \"spec_name\": spec_name,\n             \"module\": obj.__class__.__module__,\n             \"config\": ts_config,\n-            \"registered_name\": None,\n+            \"registered_name\": registered_name,\n         }\n \n     inner_config = _get_class_or_fn_config(obj)\n@@ -638,8 +647,9 @@ def deserialize_keras_object(\n                 \"the loading function in order to allow `lambda` loading.\"\n             )\n         return generic_utils.func_load(inner_config[\"value\"])\n+\n     if config[\"class_name\"] == \"__typespec__\":\n-        obj = _retrieve_class_or_fn(\n+        cls = _retrieve_class_or_fn(\n             config[\"spec_name\"],\n             config[\"registered_name\"],\n             config[\"module\"],\n@@ -647,6 +657,20 @@ def deserialize_keras_object(\n             full_config=config,\n             custom_objects=custom_objects,\n         )\n+\n+        # Special casing for ExtensionType.Spec\n+        if hasattr(cls, \"_tf_extension_type_fields\"):\n+            inner_config = {\n+                key: deserialize_keras_object(\n+                    value, custom_objects=custom_objects, safe_mode=safe_mode\n+                )\n+                for key, value in inner_config.items()\n+            }  # Deserialization of dict created by ExtensionType.as_dict()\n+            return cls(**inner_config)  # Instantiate ExtensionType.Spec\n+\n+        if config[\"registered_name\"] is not None:\n+            return cls.from_config(inner_config)\n+\n         # Conversion to TensorShape and tf.DType\n         inner_config = map(\n             lambda x: tf.TensorShape(x)\n@@ -654,7 +678,7 @@ def deserialize_keras_object(\n             else (getattr(tf, x) if hasattr(tf.dtypes, str(x)) else x),\n             inner_config,\n         )\n-        return obj._deserialize(tuple(inner_config))\n+        return cls._deserialize(tuple(inner_config))\n \n     # Below: classes and functions.\n     module = config.get(\"module\", None)\n@@ -782,10 +806,21 @@ def _retrieve_class_or_fn(\n             )\n         obj = vars(mod).get(name, None)\n \n+        if obj is None:\n             # Special case for keras.metrics.metrics\n-        if obj is None and registered_name is not None:\n+            if registered_name is not None:\n                 obj = vars(mod).get(registered_name, None)\n \n+            # Support for `__qualname__`\n+            if name.count(\".\") == 1:\n+                outer_name, inner_name = name.split(\".\")\n+                outer_obj = vars(mod).get(outer_name, None)\n+                obj = (\n+                    getattr(outer_obj, inner_name, None)\n+                    if outer_obj is not None\n+                    else None\n+                )\n+\n         if obj is not None:\n             return obj\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc89199f1e0f45818f2b76e19bdf884f96dfa805", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1496 | Lines Deleted: 252 | Files Changed: 15 | Hunks: 31 | Methods Changed: 57 | Complexity Δ (Sum/Max): 99/42 | Churn Δ: 1748 | Churn Cumulative: 7637 | Contributors (this commit): 9 | Commits (past 90d): 105 | Contributors (cumulative): 38 | DMM Complexity: 0.7857142857142857\n\nDIFF:\n@@ -22,7 +22,6 @@ def resize(\n             \"Argument `size` must be a tuple of two elements \"\n             f\"(height, width). Received: size={size}\"\n         )\n-    size = tuple(size)\n     if len(image.shape) == 4:\n         if data_format == \"channels_last\":\n             size = (image.shape[0],) + size + (image.shape[-1],)\n\n@@ -22,7 +22,7 @@ def resize(\n             \"Argument `size` must be a tuple of two elements \"\n             f\"(height, width). Received: size={size}\"\n         )\n-    size = tuple(size)\n+\n     if data_format == \"channels_first\":\n         if len(image.shape) == 4:\n             image = tf.transpose(image, (0, 2, 3, 1))\n\n@@ -1,6 +1,8 @@\n from keras_core.callbacks.callback import Callback\n from keras_core.callbacks.callback_list import CallbackList\n+from keras_core.callbacks.csv_logger import CSVLogger\n from keras_core.callbacks.early_stopping import EarlyStopping\n from keras_core.callbacks.history import History\n from keras_core.callbacks.lambda_callback import LambdaCallback\n from keras_core.callbacks.progbar_logger import ProgbarLogger\n+from keras_core.callbacks.terminate_on_nan import TerminateOnNaN\n\n@@ -0,0 +1,101 @@\n+import collections\n+import csv\n+\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import file_utils\n+\n+\n+@keras_core_export(\"keras_core.callbacks.CSVLogger\")\n+class CSVLogger(Callback):\n+    \"\"\"Callback that streams epoch results to a CSV file.\n+\n+    Supports all values that can be represented as a string,\n+    including 1D iterables such as `np.ndarray`.\n+\n+    Args:\n+        filename: Filename of the CSV file, e.g. `'run/log.csv'`.\n+        separator: String used to separate elements in the CSV file.\n+        append: Boolean. True: append if file exists (useful for continuing\n+            training). False: overwrite existing file.\n+\n+    Example:\n+\n+    ```python\n+    csv_logger = CSVLogger('training.log')\n+    model.fit(X_train, Y_train, callbacks=[csv_logger])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, filename, separator=\",\", append=False):\n+        super().__init__()\n+        self.sep = separator\n+        self.filename = file_utils.path_to_string(filename)\n+        self.append = append\n+        self.writer = None\n+        self.keys = None\n+        self.append_header = True\n+\n+    def on_train_begin(self, logs=None):\n+        if self.append:\n+            if tf.io.gfile.exists(self.filename):\n+                with tf.io.gfile.GFile(self.filename, \"r\") as f:\n+                    self.append_header = not bool(len(f.readline()))\n+            mode = \"a\"\n+        else:\n+            mode = \"w\"\n+        self.csv_file = tf.io.gfile.GFile(self.filename, mode)\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+\n+        def handle_value(k):\n+            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n+            if isinstance(k, str):\n+                return k\n+            elif (\n+                isinstance(k, collections.abc.Iterable)\n+                and not is_zero_dim_ndarray\n+            ):\n+                return f\"\\\"[{', '.join(map(str, k))}]\\\"\"\n+            else:\n+                return k\n+\n+        if self.keys is None:\n+            self.keys = sorted(logs.keys())\n+            # When validation_freq > 1, `val_` keys are not in first epoch logs\n+            # Add the `val_` keys so that its part of the fieldnames of writer.\n+            val_keys_found = False\n+            for key in self.keys:\n+                if key.startswith(\"val_\"):\n+                    val_keys_found = True\n+                    break\n+            if not val_keys_found:\n+                self.keys.extend([\"val_\" + k for k in self.keys])\n+\n+        if not self.writer:\n+\n+            class CustomDialect(csv.excel):\n+                delimiter = self.sep\n+\n+            fieldnames = [\"epoch\"] + self.keys\n+\n+            self.writer = csv.DictWriter(\n+                self.csv_file, fieldnames=fieldnames, dialect=CustomDialect\n+            )\n+            if self.append_header:\n+                self.writer.writeheader()\n+\n+        row_dict = collections.OrderedDict({\"epoch\": epoch})\n+        row_dict.update(\n+            (key, handle_value(logs.get(key, \"NA\"))) for key in self.keys\n+        )\n+        self.writer.writerow(row_dict)\n+        self.csv_file.flush()\n+\n+    def on_train_end(self, logs=None):\n+        self.csv_file.close()\n+        self.writer = None\n\n@@ -0,0 +1,176 @@\n+import csv\n+import os\n+import re\n+import tempfile\n+\n+import numpy as np\n+\n+from keras_core import callbacks\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.utils import numerical_utils\n+\n+TRAIN_SAMPLES = 10\n+TEST_SAMPLES = 10\n+INPUT_DIM = 3\n+BATCH_SIZE = 4\n+\n+\n+class CSVLoggerTest(testing.TestCase):\n+    def test_CSVLogger(self):\n+        OUTPUT_DIM = 1\n+        np.random.seed(1337)\n+        temp_dir = tempfile.TemporaryDirectory()\n+        filepath = os.path.join(temp_dir.name, \"log.tsv\")\n+\n+        sep = \"\\t\"\n+        x_train = np.random.random((TRAIN_SAMPLES, INPUT_DIM))\n+        y_train = np.random.random((TRAIN_SAMPLES, OUTPUT_DIM))\n+        x_test = np.random.random((TEST_SAMPLES, INPUT_DIM))\n+        y_test = np.random.random((TEST_SAMPLES, OUTPUT_DIM))\n+\n+        def make_model():\n+            np.random.seed(1337)\n+            model = Sequential(\n+                [\n+                    layers.Dense(2, activation=\"relu\"),\n+                    layers.Dense(OUTPUT_DIM),\n+                ]\n+            )\n+            model.compile(\n+                loss=\"mse\",\n+                optimizer=\"sgd\",\n+                metrics=[\"mse\"],\n+            )\n+            return model\n+\n+        # case 1, create new file with defined separator\n+        model = make_model()\n+        cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+\n+        assert os.path.exists(filepath)\n+        with open(filepath) as csvfile:\n+            dialect = csv.Sniffer().sniff(csvfile.read())\n+        assert dialect.delimiter == sep\n+        del model\n+        del cbks\n+\n+        # case 2, append data to existing file, skip header\n+        model = make_model()\n+        cbks = [callbacks.CSVLogger(filepath, separator=sep, append=True)]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+\n+        # case 3, reuse of CSVLogger object\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=2,\n+            verbose=0,\n+        )\n+\n+        with open(filepath) as csvfile:\n+            list_lines = csvfile.readlines()\n+            for line in list_lines:\n+                assert line.count(sep) == 4\n+            assert len(list_lines) == 5\n+            output = \" \".join(list_lines)\n+            assert len(re.findall(\"epoch\", output)) == 1\n+\n+        os.remove(filepath)\n+\n+        # case 3, Verify Val. loss also registered when Validation Freq > 1\n+        model = make_model()\n+        cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n+        hist = model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            validation_freq=3,\n+            callbacks=cbks,\n+            epochs=5,\n+            verbose=0,\n+        )\n+        assert os.path.exists(filepath)\n+        # Verify that validation loss is registered at val. freq\n+        with open(filepath) as csvfile:\n+            rows = csv.DictReader(csvfile, delimiter=sep)\n+            for idx, row in enumerate(rows, 1):\n+                self.assertIn(\"val_loss\", row)\n+                if idx == 3:\n+                    self.assertEqual(\n+                        row[\"val_loss\"], str(hist.history[\"val_loss\"][0])\n+                    )\n+                else:\n+                    self.assertEqual(row[\"val_loss\"], \"NA\")\n+\n+    def test_stop_training_csv(self):\n+        # Test that using the CSVLogger callback with the TerminateOnNaN\n+        # callback does not result in invalid CSVs.\n+        tmpdir = tempfile.TemporaryDirectory()\n+        csv_logfile = os.path.join(tmpdir.name, \"csv_logger.csv\")\n+        NUM_CLASSES = 2\n+        np.random.seed(1337)\n+        x_train = np.random.random((TRAIN_SAMPLES, INPUT_DIM))\n+        y_train = np.random.choice(np.arange(NUM_CLASSES), size=TRAIN_SAMPLES)\n+        x_test = np.random.random((TEST_SAMPLES, INPUT_DIM))\n+        y_test = np.random.choice(np.arange(NUM_CLASSES), size=TEST_SAMPLES)\n+\n+        y_test = numerical_utils.to_categorical(y_test)\n+        y_train = numerical_utils.to_categorical(y_train)\n+        model = Sequential()\n+        initializer = initializers.Constant(value=1e5)\n+        for _ in range(5):\n+            model.add(\n+                layers.Dense(\n+                    2,\n+                    activation=\"relu\",\n+                    kernel_initializer=initializer,\n+                )\n+            )\n+        model.add(layers.Dense(NUM_CLASSES))\n+        model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n+\n+        history = model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=[\n+                callbacks.TerminateOnNaN(),\n+                callbacks.CSVLogger(csv_logfile),\n+            ],\n+            epochs=20,\n+        )\n+        loss = history.history[\"loss\"]\n+        self.assertEqual(len(loss), 1)\n+        self.assertTrue(np.isnan(loss[0]) or np.isinf(loss[0]))\n+\n+        values = []\n+        with open(csv_logfile) as f:\n+            # On Windows, due to \\r\\n line ends, we may end up reading empty\n+            # lines after each line. Skip empty lines.\n+            values = [x for x in csv.reader(f) if x]\n+        self.assertIn(\"nan\", values[-1], \"NaN not logged in CSV Logger.\")\n\n@@ -0,0 +1,20 @@\n+import numpy as np\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import io_utils\n+\n+\n+@keras_core_export(\"keras_core.callbacks.TerminateOnNaN\")\n+class TerminateOnNaN(Callback):\n+    \"\"\"Callback that terminates training when a NaN loss is encountered.\"\"\"\n+\n+    def on_batch_end(self, batch, logs=None):\n+        logs = logs or {}\n+        loss = logs.get(\"loss\")\n+        if loss is not None:\n+            if np.isnan(loss) or np.isinf(loss):\n+                io_utils.print_msg(\n+                    f\"Batch {batch}: Invalid loss, terminating training\"\n+                )\n+                self.model.stop_training = True\n\n@@ -0,0 +1,50 @@\n+import numpy as np\n+\n+from keras_core import callbacks\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.utils import numerical_utils\n+\n+\n+class TerminateOnNaNTest(testing.TestCase):\n+    def test_TerminateOnNaN(self):\n+        TRAIN_SAMPLES = 10\n+        TEST_SAMPLES = 10\n+        INPUT_DIM = 3\n+        NUM_CLASSES = 2\n+        BATCH_SIZE = 4\n+\n+        np.random.seed(1337)\n+        x_train = np.random.random((TRAIN_SAMPLES, INPUT_DIM))\n+        y_train = np.random.choice(np.arange(NUM_CLASSES), size=TRAIN_SAMPLES)\n+        x_test = np.random.random((TEST_SAMPLES, INPUT_DIM))\n+        y_test = np.random.choice(np.arange(NUM_CLASSES), size=TEST_SAMPLES)\n+\n+        y_test = numerical_utils.to_categorical(y_test)\n+        y_train = numerical_utils.to_categorical(y_train)\n+        model = Sequential()\n+        initializer = initializers.Constant(value=1e5)\n+        for _ in range(5):\n+            model.add(\n+                layers.Dense(\n+                    2,\n+                    activation=\"relu\",\n+                    kernel_initializer=initializer,\n+                )\n+            )\n+        model.add(layers.Dense(NUM_CLASSES))\n+        model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n+\n+        history = model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=[callbacks.TerminateOnNaN()],\n+            epochs=20,\n+        )\n+        loss = history.history[\"loss\"]\n+        self.assertEqual(len(loss), 1)\n+        self.assertTrue(np.isnan(loss[0]) or np.isinf(loss[0]))\n\n@@ -48,7 +48,6 @@ from keras_core.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n from keras_core.layers.pooling.max_pooling1d import MaxPooling1D\n from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n-from keras_core.layers.preprocessing.center_crop import CenterCrop\n from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -1,130 +0,0 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import image_utils\n-\n-\n-@keras_core_export(\"keras_core.layers.CenterCrop\")\n-class CenterCrop(Layer):\n-    \"\"\"A preprocessing layer which crops images.\n-\n-    This layers crops the central portion of the images to a target size. If an\n-    image is smaller than the target size, it will be resized and cropped\n-    so as to return the largest possible window in the image that matches\n-    the target aspect ratio.\n-\n-    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`).\n-\n-    Input shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format,\n-        or `(..., channels, height, width)`, in `\"channels_first\"` format.\n-\n-    Output shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., target_height, target_width, channels)`,\n-        or `(..., channels, target_height, target_width)`,\n-        in `\"channels_first\"` format.\n-\n-    If the input height/width is even and the target height/width is odd (or\n-    inversely), the input image is left-padded by 1 pixel.\n-\n-    Args:\n-        height: Integer, the height of the output shape.\n-        width: Integer, the width of the output shape.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, height, width, channels)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, channels, height, width)`. It defaults to the\n-            `image_data_format` value found in your Keras config file at\n-            `~/.keras/keras.json`. If you never set it, then it will be\n-            `\"channels_last\"`.\n-    \"\"\"\n-\n-    def __init__(self, height, width, data_format=None, **kwargs):\n-        super().__init__(**kwargs)\n-        self.height = height\n-        self.width = width\n-        self.data_format = data_format or backend.image_data_format()\n-\n-    def call(self, inputs):\n-        if self.data_format == \"channels_first\":\n-            init_height = inputs.shape[-2]\n-            init_width = inputs.shape[-1]\n-        else:\n-            init_height = inputs.shape[-3]\n-            init_width = inputs.shape[-2]\n-\n-        if init_height is None or init_width is None:\n-            # Dynamic size case. TODO.\n-            raise ValueError(\n-                \"At this time, CenterCrop can only \"\n-                \"process images with a static spatial \"\n-                f\"shape. Received: inputs.shape={inputs.shape}\"\n-            )\n-\n-        h_diff = init_height - self.height\n-        w_diff = init_width - self.width\n-\n-        h_start = int(h_diff / 2)\n-        w_start = int(w_diff / 2)\n-\n-        if h_diff >= 0 and w_diff >= 0:\n-            if len(inputs.shape) == 4:\n-                if self.data_format == \"channels_first\":\n-                    return inputs[\n-                        :,\n-                        :,\n-                        h_start : h_start + self.height,\n-                        w_start : w_start + self.width,\n-                    ]\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                    :,\n-                ]\n-            elif len(inputs.shape) == 3:\n-                if self.data_format == \"channels_first\":\n-                    return inputs[\n-                        :,\n-                        h_start : h_start + self.height,\n-                        w_start : w_start + self.width,\n-                    ]\n-                return inputs[\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                    :,\n-                ]\n-\n-        return image_utils.smart_resize(\n-            inputs, [self.height, self.width], data_format=self.data_format\n-        )\n-\n-    def compute_output_shape(self, input_shape):\n-        input_shape = list(input_shape)\n-        if len(input_shape) == 4:\n-            if self.data_format == \"channels_last\":\n-                input_shape[1] = self.height\n-                input_shape[2] = self.width\n-            else:\n-                input_shape[2] = self.height\n-                input_shape[3] = self.width\n-        else:\n-            if self.data_format == \"channels_last\":\n-                input_shape[0] = self.height\n-                input_shape[1] = self.width\n-            else:\n-                input_shape[1] = self.height\n-                input_shape[2] = self.width\n-        return tuple(input_shape)\n-\n-    def get_config(self):\n-        base_config = super().get_config()\n-        config = {\n-            \"height\": self.height,\n-            \"width\": self.width,\n-            \"data_format\": self.data_format,\n-        }\n-        return {**base_config, **config}\n\n@@ -1,96 +0,0 @@\n-import numpy as np\n-import tensorflow as tf\n-from absl.testing import parameterized\n-\n-from keras_core import layers\n-from keras_core import testing\n-\n-\n-class CenterCropTest(testing.TestCase, parameterized.TestCase):\n-    def test_center_crop_basics(self):\n-        self.run_layer_test(\n-            layers.CenterCrop,\n-            init_kwargs={\n-                \"height\": 6,\n-                \"width\": 6,\n-                \"data_format\": \"channels_last\",\n-            },\n-            input_shape=(2, 12, 12, 3),\n-            expected_output_shape=(2, 6, 6, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-        )\n-        self.run_layer_test(\n-            layers.CenterCrop,\n-            init_kwargs={\n-                \"height\": 7,\n-                \"width\": 7,\n-                \"data_format\": \"channels_first\",\n-            },\n-            input_shape=(2, 3, 13, 13),\n-            expected_output_shape=(2, 3, 7, 7),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=False,\n-        )\n-\n-    @parameterized.parameters(\n-        [\n-            ((5, 7), \"channels_first\"),\n-            ((5, 7), \"channels_last\"),\n-            ((15, 10), \"channels_first\"),\n-            ((10, 17), \"channels_last\"),\n-        ]\n-    )\n-    def test_center_crop_correctness(self, size, data_format):\n-        # batched case\n-        if data_format == \"channels_first\":\n-            img = np.random.random((2, 3, 9, 11))\n-        else:\n-            img = np.random.random((2, 9, 11, 3))\n-        out = layers.CenterCrop(\n-            size[0],\n-            size[1],\n-            data_format=data_format,\n-        )(img)\n-        if data_format == \"channels_first\":\n-            img_transpose = np.transpose(img, (0, 2, 3, 1))\n-\n-            ref_out = tf.transpose(\n-                tf.keras.layers.CenterCrop(size[0], size[1])(img_transpose),\n-                (0, 3, 1, 2),\n-            )\n-        else:\n-            ref_out = tf.keras.layers.CenterCrop(size[0], size[1])(img)\n-        self.assertAllClose(ref_out, out)\n-\n-        # unbatched case\n-        if data_format == \"channels_first\":\n-            img = np.random.random((3, 9, 11))\n-        else:\n-            img = np.random.random((9, 11, 3))\n-        out = layers.CenterCrop(\n-            size[0],\n-            size[1],\n-            data_format=data_format,\n-        )(img)\n-        if data_format == \"channels_first\":\n-            img_transpose = np.transpose(img, (1, 2, 0))\n-            ref_out = tf.transpose(\n-                tf.keras.layers.CenterCrop(\n-                    size[0],\n-                    size[1],\n-                )(img_transpose),\n-                (2, 0, 1),\n-            )\n-        else:\n-            ref_out = tf.keras.layers.CenterCrop(\n-                size[0],\n-                size[1],\n-            )(img)\n-        self.assertAllClose(ref_out, out)\n\n@@ -14,17 +14,6 @@ class Resizing(Layer):\n     format. Input pixel values can be of any range\n     (e.g. `[0., 1.)` or `[0, 255]`).\n \n-    Input shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format,\n-        or `(..., channels, height, width)`, in `\"channels_first\"` format.\n-\n-    Output shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., target_height, target_width, channels)`,\n-        or `(..., channels, target_height, target_width)`,\n-        in `\"channels_first\"` format.\n-\n     Args:\n         height: Integer, the height of the output shape.\n         width: Integer, the width of the output shape.\n\n@@ -5,6 +5,7 @@ from keras_core.metrics.accuracy_metrics import CategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import SparseCategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n from keras_core.metrics.accuracy_metrics import TopKCategoricalAccuracy\n+from keras_core.metrics.confusion_metrics import AUC\n from keras_core.metrics.confusion_metrics import FalseNegatives\n from keras_core.metrics.confusion_metrics import FalsePositives\n from keras_core.metrics.confusion_metrics import Precision\n@@ -41,6 +42,7 @@ ALL_OBJECTS = {\n     # Regression\n     MeanSquaredError,\n     # Classification\n+    AUC,\n     FalseNegatives,\n     FalsePositives,\n     Precision,\n\n@@ -1,3 +1,6 @@\n+import numpy as np\n+\n+from keras_core import activations\n from keras_core import backend\n from keras_core import initializers\n from keras_core import operations as ops\n@@ -366,7 +369,7 @@ class Precision(Metric):\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -507,7 +510,7 @@ class Recall(Metric):\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -605,7 +608,7 @@ class SensitivitySpecificityBase(Metric):\n             y_true: The ground truth values.\n             y_pred: The predicted values.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n-                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n         metrics_utils.update_confusion_matrix_variables(\n@@ -1052,3 +1055,518 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n         }\n         base_config = super().get_config()\n         return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.metrics.AUC\")\n+class AUC(Metric):\n+    \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n+\n+    The AUC (Area under the curve) of the ROC (Receiver operating\n+    characteristic; default) or PR (Precision Recall) curves are quality\n+    measures of binary classifiers. Unlike the accuracy, and like cross-entropy\n+    losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n+\n+    This class approximates AUCs using a Riemann sum. During the metric\n+    accumulation phrase, predictions are accumulated within predefined buckets\n+    by value. The AUC is then computed by interpolating per-bucket averages.\n+    These buckets define the evaluated operational points.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the AUC.  To discretize the AUC curve, a linearly spaced set of\n+    thresholds is used to compute pairs of recall and precision values. The area\n+    under the ROC-curve is therefore computed using the height of the recall\n+    values by the false positive rate, while the area under the PR-curve is the\n+    computed using the height of the precision values by the recall.\n+\n+    This value is ultimately returned as `auc`, an idempotent operation that\n+    computes the area under a discretized curve of precision versus recall\n+    values (computed using the aforementioned variables). The `num_thresholds`\n+    variable controls the degree of discretization with larger numbers of\n+    thresholds more closely approximating the true AUC. The quality of the\n+    approximation may vary dramatically depending on `num_thresholds`. The\n+    `thresholds` parameter can be used to manually specify thresholds which\n+    split the predictions more evenly.\n+\n+    For a best approximation of the real AUC, `predictions` should be\n+    distributed approximately uniformly in the range `[0, 1]` (if\n+    `from_logits=False`). The quality of the AUC approximation may be poor if\n+    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'\n+    can help quantify the error in the approximation by providing lower or upper\n+    bound estimate of the AUC.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        num_thresholds: (Optional) The number of thresholds to\n+            use when discretizing the roc curve. Values must be > 1.\n+            Defaults to `200`.\n+        curve: (Optional) Specifies the name of the curve to be computed,\n+            `'ROC'` (default) or `'PR'` for the Precision-Recall-curve.\n+        summation_method: (Optional) Specifies the [Riemann summation method](\n+              https://en.wikipedia.org/wiki/Riemann_sum) used.\n+              'interpolation' (default) applies mid-point summation scheme for\n+              `ROC`.  For PR-AUC, interpolates (true/false) positives but not\n+              the ratio that is precision (see Davis & Goadrich 2006 for\n+              details); 'minoring' applies left summation for increasing\n+              intervals and right summation for decreasing intervals; 'majoring'\n+              does the opposite.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        thresholds: (Optional) A list of floating point values to use as the\n+            thresholds for discretizing the curve. If set, the `num_thresholds`\n+            parameter is ignored. Values should be in `[0, 1]`. Endpoint\n+            thresholds equal to {`-epsilon`, `1+epsilon`} for a small positive\n+            epsilon value will be automatically included with these to correctly\n+            handle predictions equal to exactly 0 or 1.\n+        multi_label: boolean indicating whether multilabel data should be\n+            treated as such, wherein AUC is computed separately for each label\n+            and then averaged across labels, or (when `False`) if the data\n+            should be flattened into a single label before AUC computation. In\n+            the latter case, when multilabel data is passed to AUC, each\n+            label-prediction pair is treated as an individual data point. Should\n+            be set to False for multi-class data.\n+        num_labels: (Optional) The number of labels, used when `multi_label` is\n+            True. If `num_labels` is not specified, then state variables get\n+            created on the first call to `update_state`.\n+        label_weights: (Optional) list, array, or tensor of non-negative weights\n+            used to compute AUCs for multilabel data. When `multi_label` is\n+            True, the weights are applied to the individual label AUCs when they\n+            are averaged to produce the multi-label AUC. When it's False, they\n+            are used to weight the individual label predictions in computing the\n+            confusion matrix on the flattened data. Note that this is unlike\n+            `class_weights` in that `class_weights` weights the example\n+            depending on the value of its label, whereas `label_weights` depends\n+            only on the index of that label before flattening; therefore\n+            `label_weights` should not be used for multi-class data.\n+        from_logits: boolean indicating whether the predictions (`y_pred` in\n+        `update_state`) are probabilities or sigmoid logits. As a rule of thumb,\n+        when using a keras loss, the `from_logits` constructor argument of the\n+        loss should match the AUC `from_logits` constructor argument.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.AUC(num_thresholds=3)\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n+    >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+    >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+    >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n+    >>> # auc = ((((1 + 0.5) / 2) * (1 - 0)) + (((0.5 + 0) / 2) * (0 - 0)))\n+    >>> #     = 0.75\n+    >>> m.result()\n+    0.75\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    # Reports the AUC of a model outputting a probability.\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.BinaryCrossentropy(),\n+                  metrics=[keras_core.metrics.AUC()])\n+\n+    # Reports the AUC of a model outputting a logit.\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras_core.metrics.AUC(from_logits=True)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_thresholds=200,\n+        curve=\"ROC\",\n+        summation_method=\"interpolation\",\n+        name=None,\n+        dtype=None,\n+        thresholds=None,\n+        multi_label=False,\n+        num_labels=None,\n+        label_weights=None,\n+        from_logits=False,\n+    ):\n+        # Validate configurations.\n+        if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n+            metrics_utils.AUCCurve\n+        ):\n+            raise ValueError(\n+                f'Invalid `curve` argument value \"{curve}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCCurve)}\"\n+            )\n+        if isinstance(\n+            summation_method, metrics_utils.AUCSummationMethod\n+        ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n+            raise ValueError(\n+                \"Invalid `summation_method` \"\n+                f'argument value \"{summation_method}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n+            )\n+\n+        # Update properties.\n+        self._init_from_thresholds = thresholds is not None\n+        if thresholds is not None:\n+            # If specified, use the supplied thresholds.\n+            self.num_thresholds = len(thresholds) + 2\n+            thresholds = sorted(thresholds)\n+            self._thresholds_distributed_evenly = (\n+                metrics_utils.is_evenly_distributed_thresholds(\n+                    np.array([0.0] + thresholds + [1.0])\n+                )\n+            )\n+        else:\n+            if num_thresholds <= 1:\n+                raise ValueError(\n+                    \"Argument `num_thresholds` must be an integer > 1. \"\n+                    f\"Received: num_thresholds={num_thresholds}\"\n+                )\n+\n+            # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n+            # (0, 1).\n+            self.num_thresholds = num_thresholds\n+            thresholds = [\n+                (i + 1) * 1.0 / (num_thresholds - 1)\n+                for i in range(num_thresholds - 2)\n+            ]\n+            self._thresholds_distributed_evenly = True\n+\n+        # Add an endpoint \"threshold\" below zero and above one for either\n+        # threshold method to account for floating point imprecisions.\n+        self._thresholds = np.array(\n+            [0.0 - backend.epsilon()] + thresholds + [1.0 + backend.epsilon()]\n+        )\n+\n+        if isinstance(curve, metrics_utils.AUCCurve):\n+            self.curve = curve\n+        else:\n+            self.curve = metrics_utils.AUCCurve.from_str(curve)\n+        if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n+            self.summation_method = summation_method\n+        else:\n+            self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n+                summation_method\n+            )\n+        super().__init__(name=name, dtype=dtype)\n+\n+        # Handle multilabel arguments.\n+        self.multi_label = multi_label\n+        self.num_labels = num_labels\n+        if label_weights is not None:\n+            label_weights = ops.array(label_weights, dtype=self.dtype)\n+            self.label_weights = label_weights\n+\n+        else:\n+            self.label_weights = None\n+\n+        self._from_logits = from_logits\n+\n+        self._built = False\n+        if self.multi_label:\n+            if num_labels:\n+                shape = [None, num_labels]\n+                self._build(shape)\n+        else:\n+            if num_labels:\n+                raise ValueError(\n+                    \"`num_labels` is needed only when `multi_label` is True.\"\n+                )\n+            self._build(None)\n+\n+    @property\n+    def thresholds(self):\n+        \"\"\"The thresholds used for evaluating AUC.\"\"\"\n+        return list(self._thresholds)\n+\n+    def _build(self, shape):\n+        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the\n+        data.\"\"\"\n+        if self.multi_label:\n+            if len(shape) != 2:\n+                raise ValueError(\n+                    \"`y_pred` must have rank 2 when `multi_label=True`. \"\n+                    f\"Found rank {len(shape)}. \"\n+                    f\"Full shape received for `y_pred`: {shape}\"\n+                )\n+            self._num_labels = shape[1]\n+            variable_shape = [self.num_thresholds, self._num_labels]\n+        else:\n+            variable_shape = [self.num_thresholds]\n+\n+        self._build_input_shape = shape\n+        # Create metric variables\n+        self.true_positives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"true_positives\",\n+        )\n+        self.false_positives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"false_positives\",\n+        )\n+        self.true_negatives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"true_negatives\",\n+        )\n+        self.false_negatives = self.add_variable(\n+            shape=variable_shape,\n+            initializer=initializers.Zeros(),\n+            name=\"false_negatives\",\n+        )\n+\n+        self._built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates confusion matrix statistics.\n+\n+        Args:\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Can\n+                be a tensor whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`. Defaults to\n+                `1`.\n+        \"\"\"\n+        if not self._built:\n+            self._build(y_pred.shape)\n+\n+        if self.multi_label or (self.label_weights is not None):\n+            # y_true should have shape (number of examples, number of labels).\n+            shapes = [(y_true, (\"N\", \"L\"))]\n+            if self.multi_label:\n+                # TP, TN, FP, and FN should all have shape\n+                # (number of thresholds, number of labels).\n+                shapes.extend(\n+                    [\n+                        (self.true_positives, (\"T\", \"L\")),\n+                        (self.true_negatives, (\"T\", \"L\")),\n+                        (self.false_positives, (\"T\", \"L\")),\n+                        (self.false_negatives, (\"T\", \"L\")),\n+                    ]\n+                )\n+            if self.label_weights is not None:\n+                # label_weights should be of length equal to the number of\n+                # labels.\n+                shapes.append((self.label_weights, (\"L\",)))\n+\n+        # Only forward label_weights to update_confusion_matrix_variables when\n+        # multi_label is False. Otherwise the averaging of individual label AUCs\n+        # is handled in AUC.result\n+        label_weights = None if self.multi_label else self.label_weights\n+\n+        if self._from_logits:\n+            y_pred = activations.sigmoid(y_pred)\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            self._thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            sample_weight=sample_weight,\n+            multi_label=self.multi_label,\n+            label_weights=label_weights,\n+        )\n+\n+    def interpolate_pr_auc(self):\n+        \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n+\n+        https://www.biostat.wisc.edu/~page/rocpr.pdf\n+\n+        Note here we derive & use a closed formula not present in the paper\n+        as follows:\n+\n+            Precision = TP / (TP + FP) = TP / P\n+\n+        Modeling all of TP (true positive), FP (false positive) and their sum\n+        P = TP + FP (predicted positive) as varying linearly within each\n+        interval [A, B] between successive thresholds, we get\n+\n+            Precision slope = dTP / dP\n+                            = (TP_B - TP_A) / (P_B - P_A)\n+                            = (TP - TP_A) / (P - P_A)\n+            Precision = (TP_A + slope * (P - P_A)) / P\n+\n+        The area within the interval is (slope / total_pos_weight) times\n+\n+            int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n+            int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n+\n+        where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n+\n+            int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n+\n+        Bringing back the factor (slope / total_pos_weight) we'd put aside, we\n+        get\n+\n+            slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n+\n+        where dTP == TP_B - TP_A.\n+\n+        Note that when P_A == 0 the above calculation simplifies into\n+\n+            int_A^B{Precision.dTP} = int_A^B{slope * dTP}\n+                                   = slope * (TP_B - TP_A)\n+\n+        which is really equivalent to imputing constant precision throughout the\n+        first bucket having >0 true positives.\n+\n+        Returns:\n+            pr_auc: an approximation of the area under the P-R curve.\n+        \"\"\"\n+\n+        dtp = (\n+            self.true_positives[: self.num_thresholds - 1]\n+            - self.true_positives[1:]\n+        )\n+        p = ops.add(self.true_positives, self.false_positives)\n+        dp = p[: self.num_thresholds - 1] - p[1:]\n+        prec_slope = ops.divide(dtp, ops.maximum(dp, backend.epsilon()))\n+        intercept = self.true_positives[1:] - ops.multiply(prec_slope, p[1:])\n+\n+        safe_p_ratio = ops.where(\n+            ops.logical_and(p[: self.num_thresholds - 1] > 0, p[1:] > 0),\n+            ops.divide(\n+                p[: self.num_thresholds - 1],\n+                ops.maximum(p[1:], backend.epsilon()),\n+            ),\n+            ops.ones_like(p[1:]),\n+        )\n+\n+        pr_auc_increment = ops.divide(\n+            prec_slope * (dtp + intercept * ops.log(safe_p_ratio)),\n+            ops.maximum(\n+                self.true_positives[1:] + self.false_negatives[1:],\n+                backend.epsilon(),\n+            ),\n+        )\n+\n+        if self.multi_label:\n+            by_label_auc = ops.sum(pr_auc_increment, axis=0)\n+            if self.label_weights is None:\n+                # Evenly weighted average of the label AUCs.\n+                return ops.mean(by_label_auc)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return ops.divide(\n+                    ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n+                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                )\n+        else:\n+            return ops.sum(pr_auc_increment)\n+\n+    def result(self):\n+        if (\n+            self.curve == metrics_utils.AUCCurve.PR\n+            and self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # This use case is different and is handled separately.\n+            return self.interpolate_pr_auc()\n+\n+        # Set `x` and `y` values for the curves based on `curve` config.\n+        recall = ops.divide(\n+            self.true_positives,\n+            ops.maximum(\n+                ops.add(self.true_positives, self.false_negatives),\n+                backend.epsilon(),\n+            ),\n+        )\n+        if self.curve == metrics_utils.AUCCurve.ROC:\n+            fp_rate = ops.divide(\n+                self.false_positives,\n+                ops.maximum(\n+                    ops.add(self.false_positives, self.true_negatives),\n+                    backend.epsilon(),\n+                ),\n+            )\n+            x = fp_rate\n+            y = recall\n+        else:  # curve == 'PR'.\n+            precision = ops.divide(\n+                self.true_positives,\n+                ops.maximum(\n+                    ops.add(self.true_positives, self.false_positives),\n+                    backend.epsilon(),\n+                ),\n+            )\n+            x = recall\n+            y = precision\n+\n+        # Find the rectangle heights based on `summation_method`.\n+        if (\n+            self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # Note: the case ('PR', 'interpolation') has been handled above.\n+            heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n+        elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n+            heights = ops.minimum(y[: self.num_thresholds - 1], y[1:])\n+        # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+        else:\n+            heights = ops.maximum(y[: self.num_thresholds - 1], y[1:])\n+\n+        # Sum up the areas of all the rectangles.\n+        if self.multi_label:\n+            riemann_terms = ops.multiply(\n+                x[: self.num_thresholds - 1] - x[1:], heights\n+            )\n+            by_label_auc = ops.sum(riemann_terms, axis=0)\n+\n+            if self.label_weights is None:\n+                # Unweighted average of the label AUCs.\n+                return ops.mean(by_label_auc)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return ops.divide(\n+                    ops.sum(ops.multiply(by_label_auc, self.label_weights)),\n+                    ops.maximum(ops.sum(self.label_weights), backend.epsilon()),\n+                )\n+        else:\n+            return ops.sum(\n+                ops.multiply(x[: self.num_thresholds - 1] - x[1:], heights)\n+            )\n+\n+    def reset_state(self):\n+        if self._built:\n+            if self.multi_label:\n+                variable_shape = (self.num_thresholds, self._num_labels)\n+            else:\n+                variable_shape = (self.num_thresholds,)\n+\n+            self.true_positives.assign(ops.zeros(variable_shape))\n+            self.false_positives.assign(ops.zeros(variable_shape))\n+            self.true_negatives.assign(ops.zeros(variable_shape))\n+            self.false_negatives.assign(ops.zeros(variable_shape))\n+\n+    def get_config(self):\n+        label_weights = self.label_weights\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"curve\": self.curve.value,\n+            \"summation_method\": self.summation_method.value,\n+            \"multi_label\": self.multi_label,\n+            \"num_labels\": self.num_labels,\n+            \"label_weights\": label_weights,\n+            \"from_logits\": self._from_logits,\n+        }\n+        # optimization to avoid serializing a large number of generated\n+        # thresholds\n+        if self._init_from_thresholds:\n+            # We remove the endpoint thresholds as an inverse of how the\n+            # thresholds were initialized. This ensures that a metric\n+            # initialized from this config has the same thresholds.\n+            config[\"thresholds\"] = self.thresholds[1:-1]\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -1,10 +1,16 @@\n+import json\n+\n import numpy as np\n+from absl import logging\n from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n+from keras_core import layers\n from keras_core import metrics\n+from keras_core import models\n from keras_core import operations as ops\n from keras_core import testing\n+from keras_core.metrics import metrics_utils\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n # This is no longer needed with tf-nightly.\n@@ -1101,3 +1107,558 @@ class RecallAtPrecisionTest(testing.TestCase, parameterized.TestCase):\n             ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n         ):\n             metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n+\n+\n+class AUCTest(testing.TestCase):\n+    def setUp(self):\n+        self.num_thresholds = 3\n+        self.y_pred = np.array([0, 0.5, 0.3, 0.9], dtype=\"float32\")\n+        self.y_pred_multi_label = np.array(\n+            [[0.0, 0.4], [0.5, 0.7], [0.3, 0.2], [0.9, 0.3]], dtype=\"float32\"\n+        )\n+        epsilon = 1e-12\n+        self.y_pred_logits = -ops.log(1.0 / (self.y_pred + epsilon) - 1.0)\n+        self.y_true = np.array([0, 0, 1, 1])\n+        self.y_true_multi_label = np.array([[0, 0], [1, 1], [1, 1], [1, 0]])\n+        self.sample_weight = [1, 2, 3, 4]\n+\n+        # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+        # y_pred when threshold = 0 - 1e-7  : [1, 1, 1, 1]\n+        # y_pred when threshold = 0.5       : [0, 0, 0, 1]\n+        # y_pred when threshold = 1 + 1e-7  : [0, 0, 0, 0]\n+\n+        # without sample_weight:\n+        # tp = np.sum([[0, 0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]], axis=1)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+\n+        # with sample_weight:\n+        # tp = np.sum([[0, 0, 3, 4], [0, 0, 0, 4], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 3, 0], [0, 0, 3, 4]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 2, 0, 0], [1, 2, 0, 0]], axis=1)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+\n+    def test_config(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=100,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+            name=\"auc_1\",\n+            dtype=\"float64\",\n+            multi_label=True,\n+            num_labels=2,\n+            from_logits=True,\n+        )\n+        auc_obj.update_state(self.y_true_multi_label, self.y_pred_multi_label)\n+        self.assertEqual(auc_obj.name, \"auc_1\")\n+        self.assertEqual(auc_obj._dtype, \"float64\")\n+        self.assertLen(auc_obj.variables, 4)\n+        self.assertEqual(auc_obj.num_thresholds, 100)\n+        self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        self.assertTrue(auc_obj.multi_label)\n+        self.assertEqual(auc_obj.num_labels, 2)\n+        self.assertTrue(auc_obj._from_logits)\n+        old_config = auc_obj.get_config()\n+        self.assertNotIn(\"thresholds\", old_config)\n+        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        auc_obj2.update_state(self.y_true_multi_label, self.y_pred_multi_label)\n+        self.assertEqual(auc_obj2.name, \"auc_1\")\n+        self.assertLen(auc_obj2.variables, 4)\n+        self.assertEqual(auc_obj2.num_thresholds, 100)\n+        self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj2.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        self.assertTrue(auc_obj2.multi_label)\n+        self.assertEqual(auc_obj2.num_labels, 2)\n+        self.assertTrue(auc_obj2._from_logits)\n+        new_config = auc_obj2.get_config()\n+        self.assertNotIn(\"thresholds\", new_config)\n+        self.assertDictEqual(old_config, new_config)\n+        self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)\n+\n+    def test_config_manual_thresholds(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=None,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+            name=\"auc_1\",\n+            thresholds=[0.3, 0.5],\n+        )\n+        auc_obj.update_state(self.y_true, self.y_pred)\n+        self.assertEqual(auc_obj.name, \"auc_1\")\n+        self.assertLen(auc_obj.variables, 4)\n+        self.assertEqual(auc_obj.num_thresholds, 4)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0])\n+        self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        old_config = auc_obj.get_config()\n+        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        auc_obj2.update_state(self.y_true, self.y_pred)\n+        self.assertEqual(auc_obj2.name, \"auc_1\")\n+        self.assertLen(auc_obj2.variables, 4)\n+        self.assertEqual(auc_obj2.num_thresholds, 4)\n+        self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)\n+        self.assertEqual(\n+            auc_obj2.summation_method, metrics_utils.AUCSummationMethod.MAJORING\n+        )\n+        new_config = auc_obj2.get_config()\n+        self.assertDictEqual(old_config, new_config)\n+        self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)\n+\n+    def test_unweighted_all_correct(self):\n+        auc_obj = metrics.AUC()\n+        self.assertEqual(auc_obj(self.y_true, self.y_true), 1)\n+\n+    def test_unweighted(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, from_logits=True\n+        )\n+        result = auc_obj(self.y_true, self.y_pred_logits)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_manual_thresholds(self):\n+        # Verify that when specified, thresholds are used instead of\n+        # num_thresholds.\n+        auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5])\n+        self.assertEqual(auc_obj.num_thresholds, 3)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.75 * 1 + 0.25 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_interpolation(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.571)/2, (0.571 + 0)/2] = [0.7855, 0.2855]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.7855 * 1 + 0.2855 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_majoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method=\"majoring\"\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [max(1, 0.571), max(0.571, 0)] = [1, 0.571]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 1 * 1 + 0.571 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_minoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method=\"minoring\"\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [min(1, 0.571), min(0.571, 0)] = [0.571, 0]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 0.571 * 1 + 0 * 0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_majoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PR\",\n+            summation_method=\"majoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [max(0.7, 1), max(1, 0)] = [1, 1]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = 1 * 0.429 + 1 * 0.571\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_minoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PR\",\n+            summation_method=\"minoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = 0.7 * 0.429 + 0 * 0.571\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_pr_interpolation(self):\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, curve=\"PR\")\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # auc = (slope / Total Pos) * [dTP - intercept * log(Pb/Pa)]\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # P = tp + fp = [10, 4, 0]\n+        # dTP = [7-4, 4-0] = [3, 4]\n+        # dP = [10-4, 4-0] = [6, 4]\n+        # slope = dTP/dP = [0.5, 1]\n+        # intercept = (TPa+(slope*Pa) = [(4 - 0.5*4), (0 - 1*0)] = [2, 0]\n+        # (Pb/Pa) = (Pb/Pa) if Pb > 0 AND Pa > 0 else 1 = [10/4, 4/0] = [2.5, 1]\n+        # auc * TotalPos = [(0.5 * (3 + 2 * log(2.5))), (1 * (4 + 0))]\n+        #                = [2.416, 4]\n+        # auc = [2.416, 4]/(tp[1:]+fn[1:])\n+        expected_result = 2.416 / 7 + 4 / 7\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 1\"\n+        ):\n+            metrics.AUC(num_thresholds=-1)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 1.\"\n+        ):\n+            metrics.AUC(num_thresholds=1)\n+\n+    def test_invalid_curve(self):\n+        with self.assertRaisesRegex(\n+            ValueError, 'Invalid AUC curve value: \"Invalid\".'\n+        ):\n+            metrics.AUC(curve=\"Invalid\")\n+\n+    def test_invalid_summation_method(self):\n+        with self.assertRaisesRegex(\n+            ValueError, 'Invalid AUC summation method value: \"Invalid\".'\n+        ):\n+            metrics.AUC(summation_method=\"Invalid\")\n+\n+    def test_extra_dims(self):\n+        try:\n+            from scipy import special\n+\n+            logits = special.expit(\n+                -np.array(\n+                    [\n+                        [[-10.0, 10.0, -10.0], [10.0, -10.0, 10.0]],\n+                        [[-12.0, 12.0, -12.0], [12.0, -12.0, 12.0]],\n+                    ],\n+                    dtype=np.float32,\n+                )\n+            )\n+            labels = np.array(\n+                [[[1, 0, 0], [1, 0, 0]], [[0, 1, 1], [0, 1, 1]]], dtype=np.int64\n+            )\n+            auc_obj = metrics.AUC()\n+            result = auc_obj(labels, logits)\n+            self.assertEqual(result, 0.5)\n+        except ImportError as e:\n+            logging.warning(f\"Cannot test special functions: {str(e)}\")\n+\n+\n+class MultiAUCTest(testing.TestCase):\n+    def setUp(self):\n+        self.num_thresholds = 5\n+        self.y_pred = np.array(\n+            [[0, 0.5, 0.3, 0.9], [0.1, 0.2, 0.3, 0.4]], dtype=\"float32\"\n+        ).T\n+\n+        epsilon = 1e-12\n+        self.y_pred_logits = -ops.log(1.0 / (self.y_pred + epsilon) - 1.0)\n+\n+        self.y_true_good = np.array([[0, 0, 1, 1], [0, 0, 1, 1]]).T\n+        self.y_true_bad = np.array([[0, 0, 1, 1], [1, 1, 0, 0]]).T\n+        self.sample_weight = [1, 2, 3, 4]\n+\n+        # threshold values are [0 - 1e-7, 0.25, 0.5, 0.75, 1 + 1e-7]\n+        # y_pred when threshold = 0 - 1e-7   : [[1, 1, 1, 1], [1, 1, 1, 1]]\n+        # y_pred when threshold = 0.25       : [[0, 1, 1, 1], [0, 0, 1, 1]]\n+        # y_pred when threshold = 0.5        : [[0, 0, 0, 1], [0, 0, 0, 0]]\n+        # y_pred when threshold = 0.75       : [[0, 0, 0, 1], [0, 0, 0, 0]]\n+        # y_pred when threshold = 1 + 1e-7   : [[0, 0, 0, 0], [0, 0, 0, 0]]\n+\n+        # for y_true_good, over thresholds:\n+        # tp = [[2, 2, 1, 1, 0], [2, 2, 0, 0, 0]]\n+        # fp = [[2, 1, 0, 0 , 0], [2, 0, 0 ,0, 0]]\n+        # fn = [[0, 0, 1, 1, 2], [0, 0, 2, 2, 2]]\n+        # tn = [[0, 1, 2, 2, 2], [0, 2, 2, 2, 2]]\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+\n+        # for y_true_bad:\n+        # tp = [[2, 2, 1, 1, 0], [2, 0, 0, 0, 0]]\n+        # fp = [[2, 1, 0, 0 , 0], [2, 2, 0 ,0, 0]]\n+        # fn = [[0, 0, 1, 1, 2], [0, 2, 2, 2, 2]]\n+        # tn = [[0, 1, 2, 2, 2], [0, 0, 2, 2, 2]]\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 0, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 1, 0, 0, 0]]\n+\n+        # for y_true_good with sample_weights:\n+\n+        # tp = [[7, 7, 4, 4, 0], [7, 7, 0, 0, 0]]\n+        # fp = [[3, 2, 0, 0, 0], [3, 0, 0, 0, 0]]\n+        # fn = [[0, 0, 3, 3, 7], [0, 0, 7, 7, 7]]\n+        # tn = [[0, 1, 3, 3, 3], [0, 3, 3, 3, 3]]\n+\n+        # tpr = [[1, 1,    0.57, 0.57, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.67, 0,    0,    0], [1, 0, 0, 0, 0]]\n+\n+    def test_unweighted_all_correct(self):\n+        auc_obj = metrics.AUC(multi_label=True)\n+        result = auc_obj(self.y_true_good, self.y_true_good)\n+        self.assertEqual(result, 1)\n+\n+    def test_unweighted_all_correct_flat(self):\n+        auc_obj = metrics.AUC(multi_label=False)\n+        result = auc_obj(self.y_true_good, self.y_true_good)\n+        self.assertEqual(result, 1)\n+\n+    def test_unweighted(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 + 1.0) / 2.0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=True,\n+            from_logits=True,\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 + 1.0) / 2.0\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_sample_weight_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=[1, 2, 3, 4]\n+        )\n+\n+        # tpr = [1, 1, 0.2857, 0.2857, 0]\n+        # fpr = [1, 0.3333, 0, 0, 0]\n+        expected_result = 1.0 - (0.3333 * (1.0 - 0.2857) / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_full_sample_weight_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        sw = np.arange(4 * 2)\n+        sw = sw.reshape(4, 2)\n+        result = auc_obj(self.y_true_good, self.y_pred, sample_weight=sw)\n+\n+        # tpr = [1, 1, 0.2727, 0.2727, 0]\n+        # fpr = [1, 0.3333, 0, 0, 0]\n+        expected_result = 1.0 - (0.3333 * (1.0 - 0.2727) / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_label_weights(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=True,\n+            label_weights=[0.75, 0.25],\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+        expected_result = (0.875 * 0.75 + 1.0 * 0.25) / (0.75 + 0.25)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_label_weights_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=False,\n+            label_weights=[0.75, 0.25],\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tpr = [1, 1, 0.375, 0.375, 0]\n+        # fpr = [1, 0.375, 0, 0, 0]\n+        expected_result = 1.0 - ((1.0 - 0.375) * 0.375 / 2.0)\n+        self.assertAllClose(result, expected_result, 1e-2)\n+\n+    def test_unweighted_flat(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=False\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tp = [4, 4, 1, 1, 0]\n+        # fp = [4, 1, 0, 0, 0]\n+        # fn = [0, 0, 3, 3, 4]\n+        # tn = [0, 3, 4, 4, 4]\n+\n+        # tpr = [1, 1, 0.25, 0.25, 0]\n+        # fpr = [1, 0.25, 0, 0, 0]\n+        expected_result = 1.0 - (3.0 / 32.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_unweighted_flat_from_logits(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            multi_label=False,\n+            from_logits=True,\n+        )\n+        result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+        # tp = [4, 4, 1, 1, 0]\n+        # fp = [4, 1, 0, 0, 0]\n+        # fn = [0, 0, 3, 3, 4]\n+        # tn = [0, 3, 4, 4, 4]\n+\n+        # tpr = [1, 1, 0.25, 0.25, 0]\n+        # fpr = [1, 0.25, 0, 0, 0]\n+        expected_result = 1.0 - (3.0 / 32.0)\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_manual_thresholds(self):\n+        # Verify that when specified, thresholds are used instead of\n+        # num_thresholds.\n+        auc_obj = metrics.AUC(\n+            num_thresholds=2, thresholds=[0.5], multi_label=True\n+        )\n+        self.assertEqual(auc_obj.num_thresholds, 3)\n+        self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])\n+        result = auc_obj(self.y_true_good, self.y_pred)\n+\n+        # tp = [[2, 1, 0], [2, 0, 0]]\n+        # fp = [2, 0, 0], [2, 0, 0]]\n+        # fn = [[0, 1, 2], [0, 2, 2]]\n+        # tn = [[0, 2, 2], [0, 2, 2]]\n+\n+        # tpr = [[1, 0.5, 0], [1, 0, 0]]\n+        # fpr = [[1, 0, 0], [1, 0, 0]]\n+\n+        # auc by slice = [0.75, 0.5]\n+        expected_result = (0.75 + 0.5) / 2.0\n+\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_roc_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tpr = [[1, 1,    0.57, 0.57, 0], [1, 1, 0, 0, 0]]\n+        # fpr = [[1, 0.67, 0,    0,    0], [1, 0, 0, 0, 0]]\n+        expected_result = 1.0 - 0.5 * 0.43 * 0.67\n+        self.assertAllClose(result, expected_result, 1e-1)\n+\n+    def test_pr_interpolation_unweighted(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PR\", multi_label=True\n+        )\n+        good_result = auc_obj(self.y_true_good, self.y_pred)\n+        with self.subTest(name=\"good\"):\n+            # PR AUCs are 0.917 and 1.0 respectively\n+            self.assertAllClose(good_result, (0.91667 + 1.0) / 2.0, 1e-1)\n+        bad_result = auc_obj(self.y_true_bad, self.y_pred)\n+        with self.subTest(name=\"bad\"):\n+            # PR AUCs are 0.917 and 0.5 respectively\n+            self.assertAllClose(bad_result, (0.91667 + 0.5) / 2.0, 1e-1)\n+\n+    def test_pr_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PR\", multi_label=True\n+        )\n+        good_result = auc_obj(\n+            self.y_true_good, self.y_pred, sample_weight=self.sample_weight\n+        )\n+        # PR AUCs are 0.939 and 1.0 respectively\n+        self.assertAllClose(good_result, (0.939 + 1.0) / 2.0, 1e-1)\n+\n+    def test_keras_model_compiles(self):\n+        inputs = layers.Input(shape=(10,), batch_size=1)\n+        output = layers.Dense(3, activation=\"sigmoid\")(inputs)\n+        model = models.Model(inputs=inputs, outputs=output)\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=\"binary_crossentropy\",\n+            metrics=[metrics.AUC(multi_label=True)],\n+        )\n+\n+    def test_reset_state(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, multi_label=True\n+        )\n+        auc_obj(self.y_true_good, self.y_pred)\n+        auc_obj.reset_state()\n+        self.assertAllClose(auc_obj.true_positives, np.zeros((5, 2)))\n\n@@ -38,6 +38,59 @@ class ConfusionMatrix(Enum):\n     FALSE_NEGATIVES = \"fn\"\n \n \n+class AUCCurve(Enum):\n+    \"\"\"Type of AUC Curve (ROC or PR).\"\"\"\n+\n+    ROC = \"ROC\"\n+    PR = \"PR\"\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in (\"pr\", \"PR\"):\n+            return AUCCurve.PR\n+        elif key in (\"roc\", \"ROC\"):\n+            return AUCCurve.ROC\n+        else:\n+            raise ValueError(\n+                f'Invalid AUC curve value: \"{key}\". '\n+                'Expected values are [\"PR\", \"ROC\"]'\n+            )\n+\n+\n+class AUCSummationMethod(Enum):\n+    \"\"\"Type of AUC summation method.\n+\n+    https://en.wikipedia.org/wiki/Riemann_sum)\n+\n+    Contains the following values:\n+    * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For\n+      `PR` curve, interpolates (true/false) positives but not the ratio that is\n+      precision (see Davis & Goadrich 2006 for details).\n+    * 'minoring': Applies left summation for increasing intervals and right\n+      summation for decreasing intervals.\n+    * 'majoring': Applies right summation for increasing intervals and left\n+      summation for decreasing intervals.\n+    \"\"\"\n+\n+    INTERPOLATION = \"interpolation\"\n+    MAJORING = \"majoring\"\n+    MINORING = \"minoring\"\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in (\"interpolation\", \"Interpolation\"):\n+            return AUCSummationMethod.INTERPOLATION\n+        elif key in (\"majoring\", \"Majoring\"):\n+            return AUCSummationMethod.MAJORING\n+        elif key in (\"minoring\", \"Minoring\"):\n+            return AUCSummationMethod.MINORING\n+        else:\n+            raise ValueError(\n+                f'Invalid AUC summation method value: \"{key}\". '\n+                'Expected values are [\"interpolation\", \"majoring\", \"minoring\"]'\n+            )\n+\n+\n def _update_confusion_matrix_variables_optimized(\n     variables_to_update,\n     y_true,\n@@ -203,14 +256,15 @@ def _update_confusion_matrix_variables_optimized(\n                 num_segments=num_thresholds,\n             )\n \n-        tp_bucket_v = ops.vectorized_map(\n-            gather_bucket, (true_labels, bucket_indices), warn=False\n+        tp_bucket_v = backend.vectorized_map(\n+            gather_bucket,\n+            (true_labels, bucket_indices),\n         )\n-        fp_bucket_v = ops.vectorized_map(\n-            gather_bucket, (false_labels, bucket_indices), warn=False\n+        fp_bucket_v = backend.vectorized_map(\n+            gather_bucket, (false_labels, bucket_indices)\n         )\n-        tp = ops.transpose(ops.cumsum(ops.flip(tp_bucket_v), axis=1))\n-        fp = ops.transpose(ops.cumsum(ops.flip(fp_bucket_v), axis=1))\n+        tp = ops.transpose(ops.flip(ops.cumsum(ops.flip(tp_bucket_v), axis=1)))\n+        fp = ops.transpose(ops.flip(ops.cumsum(ops.flip(fp_bucket_v), axis=1)))\n     else:\n         tp_bucket_v = ops.segment_sum(\n             data=true_labels,\n@@ -222,8 +276,8 @@ def _update_confusion_matrix_variables_optimized(\n             segment_ids=bucket_indices,\n             num_segments=num_thresholds,\n         )\n-        tp = ops.cumsum(ops.flip(tp_bucket_v))\n-        fp = ops.cumsum(ops.flip(fp_bucket_v))\n+        tp = ops.flip(ops.cumsum(ops.flip(tp_bucket_v)))\n+        fp = ops.flip(ops.cumsum(ops.flip(fp_bucket_v)))\n \n     # fn = sum(true_labels) - tp\n     # tn = sum(false_labels) - fp\n@@ -383,7 +437,6 @@ def update_confusion_matrix_variables(\n         one_thresh = ops.equal(\n             ops.cast(1, dtype=\"int32\"),\n             thresholds.ndim,\n-            name=\"one_set_of_thresholds_cond\",\n         )\n     else:\n         one_thresh = ops.cast(True, dtype=\"bool\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
