{"custom_id": "keras#0cfacc35990a71b6ed24c246c154ca2e19c1a131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 17 | Churn Cumulative: 4728 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Integration tests for Keras applications.\"\"\"\n \n+import os\n+\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n@@ -36,6 +38,7 @@ from keras.applications import resnet_v2\n from keras.applications import vgg16\n from keras.applications import vgg19\n from keras.applications import xception\n+from keras.testing_infra import test_utils\n \n MODEL_LIST_NO_NASNET = [\n     (resnet.ResNet50, 2048),\n@@ -239,6 +242,19 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n             include_top=include_top,\n         )\n \n+    @parameterized.parameters(MODEL_LIST)\n+    @test_utils.run_v2_only\n+    def test_model_checkpoint(self, app, _):\n+        model = app(weights=None)\n+\n+        checkpoint = tf.train.Checkpoint(model=model)\n+        checkpoint_manager = tf.train.CheckpointManager(\n+            checkpoint,\n+            directory=os.path.join(self.get_temp_dir(), model.name),\n+            max_to_keep=1,\n+        )\n+        checkpoint_manager.save(checkpoint_number=1)\n+\n \n def _get_output_shape(model_fn):\n     model = model_fn()\n\n@@ -219,6 +219,7 @@ class LayerScale(layers.Layer):\n \n     def build(self, input_shape):\n         self.gamma = self.add_weight(\n+            name=\"gamma\",\n             shape=(self.projection_dim,),\n             initializer=initializers.Constant(self.init_values),\n             trainable=True,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95c4a33d1bdeca513b3770a4d1584835470e3058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 583 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -146,8 +146,21 @@ from keras_core.ops.operation_utils import reduce_shape\n \n \n def broadcast_shapes(shape1, shape2):\n-    # Broadcast input shapes to a unified shape.\n-    # Convert to list for mutability.\n+    \"\"\"Broadcast input shapes to a unified shape.\n+    \n+    Convert to list for mutability.\n+\n+    Args:\n+        shape1: A tuple or list of integers.\n+        shape2: A tuple or list of integers.\n+\n+    Returns:\n+        output_shape (list of int or None): The broadcasted shape.\n+\n+    Example:\n+    >>> broadcast_shapes((5, 3), (1, 3))\n+    [5, 3]\n+    \"\"\"\n     shape1 = list(shape1)\n     shape2 = list(shape2)\n     origin_shape1 = shape1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f6c9287661d7cc635b2b0910621ed43a8eec550", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 586 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -155,9 +155,10 @@ def broadcast_shapes(shape1, shape2):\n         shape2: A tuple or list of integers.\n \n     Returns:\n-        output_shape (list of int or None): The broadcasted shape.\n+        output_shape (list of integers or `None`): The broadcasted shape.\n \n     Example:\n+\n     >>> broadcast_shapes((5, 3), (1, 3))\n     [5, 3]\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d11ba72c7f508bfad84307dc9906c24895271aa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 16 | Churn Cumulative: 2107 | Contributors (this commit): 8 | Commits (past 90d): 25 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -467,15 +467,13 @@ def conv_transpose(\n     if isinstance(dilation_rate, int):\n         dilation_rate = [dilation_rate] * len(kernel_spatial_shape)\n     for i, value in enumerate(padding_values):\n-        total_padding = value[0] + value[1]\n+        both_side_padding = value[0]\n+        longer_side_padding = value[1] - value[0]\n         padding_arg.append(\n             dilation_rate[i] * (kernel_spatial_shape[i] - 1)\n-            - total_padding // 2\n+            - both_side_padding,\n         )\n-        if total_padding % 2 == 0:\n-            output_padding_arg.append(0)\n-        else:\n-            output_padding_arg.append(1)\n+        output_padding_arg.append(longer_side_padding)\n \n     if num_spatial_dims == 1:\n         outputs = tnn.conv_transpose1d(\n\n@@ -315,8 +315,8 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         },\n         {\n             \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n+            \"kernel_size\": 7,\n+            \"strides\": 16,\n             \"padding\": \"same\",\n             \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n@@ -361,7 +361,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n         )\n \n-        inputs = np.random.normal(size=[2, 8, 8, 4])\n+        inputs = np.random.normal(size=[2, 14, 14, 4])\n         layer.build(input_shape=inputs.shape)\n         tf_keras_layer.build(input_shape=inputs.shape)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19440d220dd2adf77b642dbf9d790b25d7559c1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 508 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 21 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 508 | Churn Cumulative: 508 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,508 @@\n+\"\"\"\n+Title: Low-light image enhancement using MIRNet\n+Author: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Date created: 2021/09/11\n+Last modified: 2023/07/15\n+Description: Implementing the MIRNet architecture for low-light image enhancement.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+With the goal of recovering high-quality image content from its degraded version, image\n+restoration enjoys numerous applications, such as in\n+photography, security, medical imaging, and remote sensing. In this example, we implement the\n+**MIRNet** model for low-light image enhancement, a fully-convolutional architecture that\n+learns an enriched set of\n+features that combines contextual information from multiple scales, while\n+simultaneously preserving the high-resolution spatial details.\n+\n+### References:\n+\n+- [Learning Enriched Features for Real Image Restoration and Enhancement](https://arxiv.org/abs/2003.06792)\n+- [The Retinex Theory of Color Vision](http://www.cnbc.cmu.edu/~tai/cp_papers/E.Land_Retinex_Theory_ScientifcAmerican.pdf)\n+- [Two deterministic half-quadratic regularization algorithms for computed imaging](https://ieeexplore.ieee.org/document/413553)\n+\"\"\"\n+\n+\"\"\"\n+## Downloading LOLDataset\n+\n+The **LoL Dataset** has been created for low-light image enhancement.\n+It provides 485 images for training and 15 for testing. Each image pair in the dataset\n+consists of a low-light input image and its corresponding well-exposed reference image.\n+\"\"\"\n+\n+\"\"\"shell\n+pip install -q git+https://github.com/keras-team/keras-core\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import random\n+import numpy as np\n+from glob import glob\n+from PIL import Image, ImageOps\n+import matplotlib.pyplot as plt\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+import tensorflow as tf\n+\n+\"\"\"shell\n+wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n+unzip -q lol_dataset.zip && rm lol_dataset.zip\n+\"\"\"\n+\n+\"\"\"\n+## Creating a TensorFlow Dataset\n+\n+We use 300 image pairs from the LoL Dataset's training set for training,\n+and we use the remaining 185 image pairs for validation.\n+We generate random crops of size `128 x 128` from the image pairs to be\n+used for both training and validation.\n+\"\"\"\n+\n+random.seed(10)\n+\n+IMAGE_SIZE = 128\n+BATCH_SIZE = 4\n+MAX_TRAIN_IMAGES = 300\n+\n+\n+def read_image(image_path):\n+    image = tf.io.read_file(image_path)\n+    image = tf.image.decode_png(image, channels=3)\n+    image.set_shape([None, None, 3])\n+    image = tf.cast(image, dtype=tf.float32) / 255.0\n+    return image\n+\n+\n+def random_crop(low_image, enhanced_image):\n+    low_image_shape = tf.shape(low_image)[:2]\n+    low_w = tf.random.uniform(\n+        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n+    )\n+    low_h = tf.random.uniform(\n+        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n+    )\n+    low_image_cropped = low_image[\n+        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n+    ]\n+    enhanced_image_cropped = enhanced_image[\n+        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n+    ]\n+    # in order to avoid `NONE` during shape inference\n+    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n+    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n+    return low_image_cropped, enhanced_image_cropped\n+\n+\n+def load_data(low_light_image_path, enhanced_image_path):\n+    low_light_image = read_image(low_light_image_path)\n+    enhanced_image = read_image(enhanced_image_path)\n+    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n+    return low_light_image, enhanced_image\n+\n+\n+def get_dataset(low_light_images, enhanced_images):\n+    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n+    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n+    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n+    return dataset\n+\n+\n+train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n+train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n+\n+val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n+val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n+\n+test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n+test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n+\n+\n+train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n+val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n+\n+\n+print(\"Train Dataset:\", train_dataset.element_spec)\n+print(\"Val Dataset:\", val_dataset.element_spec)\n+\n+\"\"\"\n+## MIRNet Model\n+\n+Here are the main features of the MIRNet model:\n+\n+- A feature extraction model that computes a complementary set of features across multiple\n+spatial scales, while maintaining the original high-resolution features to preserve\n+precise spatial details.\n+- A regularly repeated mechanism for information exchange, where the features across\n+multi-resolution branches are progressively fused together for improved representation\n+learning.\n+- A new approach to fuse multi-scale features using a selective kernel network\n+that dynamically combines variable receptive fields and faithfully preserves\n+the original feature information at each spatial resolution.\n+- A recursive residual design that progressively breaks down the input signal\n+in order to simplify the overall learning process, and allows the construction\n+of very deep networks.\n+\n+\n+![](https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png)\n+\"\"\"\n+\n+\"\"\"\n+### Selective Kernel Feature Fusion\n+\n+The Selective Kernel Feature Fusion or SKFF module performs dynamic adjustment of\n+receptive fields via two operations: **Fuse** and **Select**. The Fuse operator generates\n+global feature descriptors by combining the information from multi-resolution streams.\n+The Select operator uses these descriptors to recalibrate the feature maps (of different\n+streams) followed by their aggregation.\n+\n+**Fuse**: The SKFF receives inputs from three parallel convolution streams carrying\n+different scales of information. We first combine these multi-scale features using an\n+element-wise sum, on which we apply Global Average Pooling (GAP) across the spatial\n+dimension. Next, we apply a channel- downscaling convolution layer to generate a compact\n+feature representation which passes through three parallel channel-upscaling convolution\n+layers (one for each resolution stream) and provides us with three feature descriptors.\n+\n+**Select**: This operator applies the softmax function to the feature descriptors to\n+obtain the corresponding activations that are used to adaptively recalibrate multi-scale\n+feature maps. The aggregated features are defined as the sum of product of the corresponding\n+multi-scale feature and the feature descriptor.\n+\n+![](https://i.imgur.com/7U6ixF6.png)\n+\"\"\"\n+\n+\n+def selective_kernel_feature_fusion(\n+    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n+):\n+    channels = list(multi_scale_feature_1.shape)[-1]\n+    combined_feature = layers.Add()(\n+        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n+    )\n+    gap = layers.GlobalAveragePooling2D()(combined_feature)\n+    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n+    compact_feature_representation = layers.Conv2D(\n+        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n+    )(channel_wise_statistics)\n+    feature_descriptor_1 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_descriptor_2 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_descriptor_3 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n+    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n+    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n+    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n+    return aggregated_feature\n+\n+\n+\"\"\"\n+### Dual Attention Unit\n+\n+The Dual Attention Unit or DAU is used to extract features in the convolutional streams.\n+While the SKFF block fuses information across multi-resolution branches, we also need a\n+mechanism to share information within a feature tensor, both along the spatial and the\n+channel dimensions which is done by the DAU block. The DAU suppresses less useful\n+features and only allows more informative ones to pass further. This feature\n+recalibration is achieved by using **Channel Attention** and **Spatial Attention**\n+mechanisms.\n+\n+The **Channel Attention** branch exploits the inter-channel relationships of the\n+convolutional feature maps by applying squeeze and excitation operations. Given a feature\n+map, the squeeze operation applies Global Average Pooling across spatial dimensions to\n+encode global context, thus yielding a feature descriptor. The excitation operator passes\n+this feature descriptor through two convolutional layers followed by the sigmoid gating\n+and generates activations. Finally, the output of Channel Attention branch is obtained by\n+rescaling the input feature map with the output activations.\n+\n+The **Spatial Attention** branch is designed to exploit the inter-spatial dependencies of\n+convolutional features. The goal of Spatial Attention is to generate a spatial attention\n+map and use it to recalibrate the incoming features. To generate the spatial attention\n+map, the Spatial Attention branch first independently applies Global Average Pooling and\n+Max Pooling operations on input features along the channel dimensions and concatenates\n+the outputs to form a resultant feature map which is then passed through a convolution\n+and sigmoid activation to obtain the spatial attention map. This spatial attention map is\n+then used to rescale the input feature map.\n+\n+![](https://i.imgur.com/Dl0IwQs.png)\n+\"\"\"\n+\n+class ChannelPooling(layers.Layer):\n+    def __init__(self, axis=-1, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.axis = axis\n+        self.concat = layers.Concatenate(axis=self.axis)\n+    \n+    def call(self, inputs):\n+        average_pooling = tf.expand_dims(\n+            tf.reduce_mean(inputs, axis=-1), axis=-1\n+        )\n+        max_pooling = tf.expand_dims(\n+            tf.reduce_max(inputs, axis=-1), axis=-1\n+        )\n+        return self.concat([average_pooling, max_pooling])\n+    \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"axis\": self.axis})\n+\n+\n+def spatial_attention_block(input_tensor):\n+    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n+    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n+    feature_map = keras.activations.sigmoid(feature_map)\n+    return input_tensor * feature_map\n+\n+\n+def channel_attention_block(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n+    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n+    feature_activations = layers.Conv2D(\n+        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n+    )(feature_descriptor)\n+    feature_activations = layers.Conv2D(\n+        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n+    )(feature_activations)\n+    return input_tensor * feature_activations\n+\n+\n+def dual_attention_unit_block(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    feature_map = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(input_tensor)\n+    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n+        feature_map\n+    )\n+    channel_attention = channel_attention_block(feature_map)\n+    spatial_attention = spatial_attention_block(feature_map)\n+    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n+    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n+    return layers.Add()([input_tensor, concatenation])\n+\n+\n+\"\"\"\n+### Multi-Scale Residual Block\n+\n+The Multi-Scale Residual Block is capable of generating a spatially-precise output by\n+maintaining high-resolution representations, while receiving rich contextual information\n+from low-resolutions. The MRB consists of multiple (three in this paper)\n+fully-convolutional streams connected in parallel. It allows information exchange across\n+parallel streams in order to consolidate the high-resolution features with the help of\n+low-resolution features, and vice versa. The MIRNet employs a recursive residual design\n+(with skip connections) to ease the flow of information during the learning process. In\n+order to maintain the residual nature of our architecture, residual resizing modules are\n+used to perform downsampling and upsampling operations that are used in the Multi-scale\n+Residual Block.\n+\n+![](https://i.imgur.com/wzZKV57.png)\n+\"\"\"\n+\n+# Recursive Residual Modules\n+\n+\n+def down_sampling_module(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n+        input_tensor\n+    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(main_branch)\n+    main_branch = layers.MaxPooling2D()(main_branch)\n+    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n+    skip_branch = layers.MaxPooling2D()(input_tensor)\n+    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n+    return layers.Add()([skip_branch, main_branch])\n+\n+\n+def up_sampling_module(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n+        input_tensor\n+    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(main_branch)\n+    main_branch = layers.UpSampling2D()(main_branch)\n+    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n+    skip_branch = layers.UpSampling2D()(input_tensor)\n+    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n+    return layers.Add()([skip_branch, main_branch])\n+\n+\n+# MRB Block\n+def multi_scale_residual_block(input_tensor, channels):\n+    # features\n+    level1 = input_tensor\n+    level2 = down_sampling_module(input_tensor)\n+    level3 = down_sampling_module(level2)\n+    # DAU\n+    level1_dau = dual_attention_unit_block(level1)\n+    level2_dau = dual_attention_unit_block(level2)\n+    level3_dau = dual_attention_unit_block(level3)\n+    # SKFF\n+    level1_skff = selective_kernel_feature_fusion(\n+        level1_dau,\n+        up_sampling_module(level2_dau),\n+        up_sampling_module(up_sampling_module(level3_dau)),\n+    )\n+    level2_skff = selective_kernel_feature_fusion(\n+        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n+    )\n+    level3_skff = selective_kernel_feature_fusion(\n+        down_sampling_module(down_sampling_module(level1_dau)),\n+        down_sampling_module(level2_dau),\n+        level3_dau,\n+    )\n+    # DAU 2\n+    level1_dau_2 = dual_attention_unit_block(level1_skff)\n+    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n+    level3_dau_2 = up_sampling_module(\n+        up_sampling_module(dual_attention_unit_block(level3_skff))\n+    )\n+    # SKFF 2\n+    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n+    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n+    return layers.Add()([input_tensor, conv])\n+\n+\n+\"\"\"\n+### MIRNet Model\n+\"\"\"\n+\n+\n+def recursive_residual_group(input_tensor, num_mrb, channels):\n+    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    for _ in range(num_mrb):\n+        conv1 = multi_scale_residual_block(conv1, channels)\n+    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n+    return layers.Add()([conv2, input_tensor])\n+\n+\n+def mirnet_model(num_rrg, num_mrb, channels):\n+    input_tensor = keras.Input(shape=[None, None, 3])\n+    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    for _ in range(num_rrg):\n+        x1 = recursive_residual_group(x1, num_mrb, channels)\n+    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n+    output_tensor = layers.Add()([input_tensor, conv])\n+    return keras.Model(input_tensor, output_tensor)\n+\n+\n+model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n+\n+\"\"\"\n+## Training\n+\n+- We train MIRNet using **Charbonnier Loss** as the loss function and **Adam\n+Optimizer** with a learning rate of `1e-4`.\n+- We use **Peak Signal Noise Ratio** or PSNR as a metric which is an expression for the\n+ratio between the maximum possible value (power) of a signal and the power of distorting\n+noise that affects the quality of its representation.\n+\"\"\"\n+\n+\n+def charbonnier_loss(y_true, y_pred):\n+    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n+\n+\n+def peak_signal_noise_ratio(y_true, y_pred):\n+    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n+\n+\n+optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n+model.compile(\n+    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n+)\n+\n+history = model.fit(\n+    train_dataset,\n+    validation_data=val_dataset,\n+    epochs=50,\n+    callbacks=[\n+        keras.callbacks.ReduceLROnPlateau(\n+            monitor=\"val_peak_signal_noise_ratio\",\n+            factor=0.5,\n+            patience=5,\n+            verbose=1,\n+            min_delta=1e-7,\n+            mode=\"max\",\n+        )\n+    ],\n+)\n+\n+def plot_history(value):\n+    plt.plot(history.history[value], label=f\"train_{value}\")\n+    plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(value)\n+    plt.title(f\"Train and Validation {value} Over Epochs\", fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_history(\"loss\")\n+plot_history(\"peak_signal_noise_ratio\")\n+\n+\"\"\"\n+## Inference\n+\"\"\"\n+\n+\n+def plot_results(images, titles, figure_size=(12, 12)):\n+    fig = plt.figure(figsize=figure_size)\n+    for i in range(len(images)):\n+        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n+        _ = plt.imshow(images[i])\n+        plt.axis(\"off\")\n+    plt.show()\n+\n+\n+def infer(original_image):\n+    image = keras.utils.img_to_array(original_image)\n+    image = image.astype(\"float32\") / 255.0\n+    image = np.expand_dims(image, axis=0)\n+    output = model.predict(image, verbose=0)\n+    output_image = output[0] * 255.0\n+    output_image = output_image.clip(0, 255)\n+    output_image = output_image.reshape(\n+        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n+    )\n+    output_image = Image.fromarray(np.uint8(output_image))\n+    original_image = Image.fromarray(np.uint8(original_image))\n+    return output_image\n+\n+\n+\"\"\"\n+### Inference on Test Images\n+\n+We compare the test images from LOLDataset enhanced by MIRNet with images\n+enhanced via the `PIL.ImageOps.autocontrast()` function.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/lowlight-enhance-mirnet)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Enhance_Low_Light_Image).\n+\"\"\"\n+\n+\n+for low_light_image in random.sample(test_low_light_images, 6):\n+    original_image = Image.open(low_light_image)\n+    enhanced_image = infer(original_image)\n+    plot_results(\n+        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n+        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n+        (20, 12),\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82206929b50c51a20fbd4a635f10d93ef6b63914", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95309adb6b6579b5cf450144f851254bf2b7cb4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 516 | Files Changed: 5 | Hunks: 7 | Methods Changed: 24 | Complexity Δ (Sum/Max): -25/1 | Churn Δ: 534 | Churn Cumulative: 5497 | Contributors (this commit): 13 | Commits (past 90d): 116 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,508 +0,0 @@\n-\"\"\"\n-Title: Low-light image enhancement using MIRNet\n-Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n-Date created: 2021/09/11\n-Last modified: 2023/07/15\n-Description: Implementing the MIRNet architecture for low-light image enhancement.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-With the goal of recovering high-quality image content from its degraded version, image\n-restoration enjoys numerous applications, such as in\n-photography, security, medical imaging, and remote sensing. In this example, we implement the\n-**MIRNet** model for low-light image enhancement, a fully-convolutional architecture that\n-learns an enriched set of\n-features that combines contextual information from multiple scales, while\n-simultaneously preserving the high-resolution spatial details.\n-\n-### References:\n-\n-- [Learning Enriched Features for Real Image Restoration and Enhancement](https://arxiv.org/abs/2003.06792)\n-- [The Retinex Theory of Color Vision](http://www.cnbc.cmu.edu/~tai/cp_papers/E.Land_Retinex_Theory_ScientifcAmerican.pdf)\n-- [Two deterministic half-quadratic regularization algorithms for computed imaging](https://ieeexplore.ieee.org/document/413553)\n-\"\"\"\n-\n-\"\"\"\n-## Downloading LOLDataset\n-\n-The **LoL Dataset** has been created for low-light image enhancement.\n-It provides 485 images for training and 15 for testing. Each image pair in the dataset\n-consists of a low-light input image and its corresponding well-exposed reference image.\n-\"\"\"\n-\n-\"\"\"shell\n-pip install -q git+https://github.com/keras-team/keras-core\n-\"\"\"\n-\n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import random\n-import numpy as np\n-from glob import glob\n-from PIL import Image, ImageOps\n-import matplotlib.pyplot as plt\n-\n-import keras_core as keras\n-from keras_core import layers\n-\n-import tensorflow as tf\n-\n-\"\"\"shell\n-wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n-unzip -q lol_dataset.zip && rm lol_dataset.zip\n-\"\"\"\n-\n-\"\"\"\n-## Creating a TensorFlow Dataset\n-\n-We use 300 image pairs from the LoL Dataset's training set for training,\n-and we use the remaining 185 image pairs for validation.\n-We generate random crops of size `128 x 128` from the image pairs to be\n-used for both training and validation.\n-\"\"\"\n-\n-random.seed(10)\n-\n-IMAGE_SIZE = 128\n-BATCH_SIZE = 4\n-MAX_TRAIN_IMAGES = 300\n-\n-\n-def read_image(image_path):\n-    image = tf.io.read_file(image_path)\n-    image = tf.image.decode_png(image, channels=3)\n-    image.set_shape([None, None, 3])\n-    image = tf.cast(image, dtype=tf.float32) / 255.0\n-    return image\n-\n-\n-def random_crop(low_image, enhanced_image):\n-    low_image_shape = tf.shape(low_image)[:2]\n-    low_w = tf.random.uniform(\n-        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n-    )\n-    low_h = tf.random.uniform(\n-        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n-    )\n-    low_image_cropped = low_image[\n-        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n-    ]\n-    enhanced_image_cropped = enhanced_image[\n-        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n-    ]\n-    # in order to avoid `NONE` during shape inference\n-    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n-    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n-    return low_image_cropped, enhanced_image_cropped\n-\n-\n-def load_data(low_light_image_path, enhanced_image_path):\n-    low_light_image = read_image(low_light_image_path)\n-    enhanced_image = read_image(enhanced_image_path)\n-    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n-    return low_light_image, enhanced_image\n-\n-\n-def get_dataset(low_light_images, enhanced_images):\n-    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n-    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n-    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n-    return dataset\n-\n-\n-train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n-train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n-\n-val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n-val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n-\n-test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n-test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n-\n-\n-train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n-val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n-\n-\n-print(\"Train Dataset:\", train_dataset.element_spec)\n-print(\"Val Dataset:\", val_dataset.element_spec)\n-\n-\"\"\"\n-## MIRNet Model\n-\n-Here are the main features of the MIRNet model:\n-\n-- A feature extraction model that computes a complementary set of features across multiple\n-spatial scales, while maintaining the original high-resolution features to preserve\n-precise spatial details.\n-- A regularly repeated mechanism for information exchange, where the features across\n-multi-resolution branches are progressively fused together for improved representation\n-learning.\n-- A new approach to fuse multi-scale features using a selective kernel network\n-that dynamically combines variable receptive fields and faithfully preserves\n-the original feature information at each spatial resolution.\n-- A recursive residual design that progressively breaks down the input signal\n-in order to simplify the overall learning process, and allows the construction\n-of very deep networks.\n-\n-\n-![](https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png)\n-\"\"\"\n-\n-\"\"\"\n-### Selective Kernel Feature Fusion\n-\n-The Selective Kernel Feature Fusion or SKFF module performs dynamic adjustment of\n-receptive fields via two operations: **Fuse** and **Select**. The Fuse operator generates\n-global feature descriptors by combining the information from multi-resolution streams.\n-The Select operator uses these descriptors to recalibrate the feature maps (of different\n-streams) followed by their aggregation.\n-\n-**Fuse**: The SKFF receives inputs from three parallel convolution streams carrying\n-different scales of information. We first combine these multi-scale features using an\n-element-wise sum, on which we apply Global Average Pooling (GAP) across the spatial\n-dimension. Next, we apply a channel- downscaling convolution layer to generate a compact\n-feature representation which passes through three parallel channel-upscaling convolution\n-layers (one for each resolution stream) and provides us with three feature descriptors.\n-\n-**Select**: This operator applies the softmax function to the feature descriptors to\n-obtain the corresponding activations that are used to adaptively recalibrate multi-scale\n-feature maps. The aggregated features are defined as the sum of product of the corresponding\n-multi-scale feature and the feature descriptor.\n-\n-![](https://i.imgur.com/7U6ixF6.png)\n-\"\"\"\n-\n-\n-def selective_kernel_feature_fusion(\n-    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n-):\n-    channels = list(multi_scale_feature_1.shape)[-1]\n-    combined_feature = layers.Add()(\n-        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n-    )\n-    gap = layers.GlobalAveragePooling2D()(combined_feature)\n-    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n-    compact_feature_representation = layers.Conv2D(\n-        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n-    )(channel_wise_statistics)\n-    feature_descriptor_1 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_descriptor_2 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_descriptor_3 = layers.Conv2D(\n-        channels, kernel_size=(1, 1), activation=\"softmax\"\n-    )(compact_feature_representation)\n-    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n-    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n-    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n-    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n-    return aggregated_feature\n-\n-\n-\"\"\"\n-### Dual Attention Unit\n-\n-The Dual Attention Unit or DAU is used to extract features in the convolutional streams.\n-While the SKFF block fuses information across multi-resolution branches, we also need a\n-mechanism to share information within a feature tensor, both along the spatial and the\n-channel dimensions which is done by the DAU block. The DAU suppresses less useful\n-features and only allows more informative ones to pass further. This feature\n-recalibration is achieved by using **Channel Attention** and **Spatial Attention**\n-mechanisms.\n-\n-The **Channel Attention** branch exploits the inter-channel relationships of the\n-convolutional feature maps by applying squeeze and excitation operations. Given a feature\n-map, the squeeze operation applies Global Average Pooling across spatial dimensions to\n-encode global context, thus yielding a feature descriptor. The excitation operator passes\n-this feature descriptor through two convolutional layers followed by the sigmoid gating\n-and generates activations. Finally, the output of Channel Attention branch is obtained by\n-rescaling the input feature map with the output activations.\n-\n-The **Spatial Attention** branch is designed to exploit the inter-spatial dependencies of\n-convolutional features. The goal of Spatial Attention is to generate a spatial attention\n-map and use it to recalibrate the incoming features. To generate the spatial attention\n-map, the Spatial Attention branch first independently applies Global Average Pooling and\n-Max Pooling operations on input features along the channel dimensions and concatenates\n-the outputs to form a resultant feature map which is then passed through a convolution\n-and sigmoid activation to obtain the spatial attention map. This spatial attention map is\n-then used to rescale the input feature map.\n-\n-![](https://i.imgur.com/Dl0IwQs.png)\n-\"\"\"\n-\n-class ChannelPooling(layers.Layer):\n-    def __init__(self, axis=-1, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self.axis = axis\n-        self.concat = layers.Concatenate(axis=self.axis)\n-    \n-    def call(self, inputs):\n-        average_pooling = tf.expand_dims(\n-            tf.reduce_mean(inputs, axis=-1), axis=-1\n-        )\n-        max_pooling = tf.expand_dims(\n-            tf.reduce_max(inputs, axis=-1), axis=-1\n-        )\n-        return self.concat([average_pooling, max_pooling])\n-    \n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"axis\": self.axis})\n-\n-\n-def spatial_attention_block(input_tensor):\n-    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n-    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n-    feature_map = keras.activations.sigmoid(feature_map)\n-    return input_tensor * feature_map\n-\n-\n-def channel_attention_block(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n-    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n-    feature_activations = layers.Conv2D(\n-        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n-    )(feature_descriptor)\n-    feature_activations = layers.Conv2D(\n-        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n-    )(feature_activations)\n-    return input_tensor * feature_activations\n-\n-\n-def dual_attention_unit_block(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    feature_map = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(input_tensor)\n-    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n-        feature_map\n-    )\n-    channel_attention = channel_attention_block(feature_map)\n-    spatial_attention = spatial_attention_block(feature_map)\n-    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n-    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n-    return layers.Add()([input_tensor, concatenation])\n-\n-\n-\"\"\"\n-### Multi-Scale Residual Block\n-\n-The Multi-Scale Residual Block is capable of generating a spatially-precise output by\n-maintaining high-resolution representations, while receiving rich contextual information\n-from low-resolutions. The MRB consists of multiple (three in this paper)\n-fully-convolutional streams connected in parallel. It allows information exchange across\n-parallel streams in order to consolidate the high-resolution features with the help of\n-low-resolution features, and vice versa. The MIRNet employs a recursive residual design\n-(with skip connections) to ease the flow of information during the learning process. In\n-order to maintain the residual nature of our architecture, residual resizing modules are\n-used to perform downsampling and upsampling operations that are used in the Multi-scale\n-Residual Block.\n-\n-![](https://i.imgur.com/wzZKV57.png)\n-\"\"\"\n-\n-# Recursive Residual Modules\n-\n-\n-def down_sampling_module(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n-        input_tensor\n-    )\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(main_branch)\n-    main_branch = layers.MaxPooling2D()(main_branch)\n-    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n-    skip_branch = layers.MaxPooling2D()(input_tensor)\n-    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n-    return layers.Add()([skip_branch, main_branch])\n-\n-\n-def up_sampling_module(input_tensor):\n-    channels = list(input_tensor.shape)[-1]\n-    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n-        input_tensor\n-    )\n-    main_branch = layers.Conv2D(\n-        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n-    )(main_branch)\n-    main_branch = layers.UpSampling2D()(main_branch)\n-    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n-    skip_branch = layers.UpSampling2D()(input_tensor)\n-    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n-    return layers.Add()([skip_branch, main_branch])\n-\n-\n-# MRB Block\n-def multi_scale_residual_block(input_tensor, channels):\n-    # features\n-    level1 = input_tensor\n-    level2 = down_sampling_module(input_tensor)\n-    level3 = down_sampling_module(level2)\n-    # DAU\n-    level1_dau = dual_attention_unit_block(level1)\n-    level2_dau = dual_attention_unit_block(level2)\n-    level3_dau = dual_attention_unit_block(level3)\n-    # SKFF\n-    level1_skff = selective_kernel_feature_fusion(\n-        level1_dau,\n-        up_sampling_module(level2_dau),\n-        up_sampling_module(up_sampling_module(level3_dau)),\n-    )\n-    level2_skff = selective_kernel_feature_fusion(\n-        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n-    )\n-    level3_skff = selective_kernel_feature_fusion(\n-        down_sampling_module(down_sampling_module(level1_dau)),\n-        down_sampling_module(level2_dau),\n-        level3_dau,\n-    )\n-    # DAU 2\n-    level1_dau_2 = dual_attention_unit_block(level1_skff)\n-    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n-    level3_dau_2 = up_sampling_module(\n-        up_sampling_module(dual_attention_unit_block(level3_skff))\n-    )\n-    # SKFF 2\n-    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n-    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n-    return layers.Add()([input_tensor, conv])\n-\n-\n-\"\"\"\n-### MIRNet Model\n-\"\"\"\n-\n-\n-def recursive_residual_group(input_tensor, num_mrb, channels):\n-    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n-    for _ in range(num_mrb):\n-        conv1 = multi_scale_residual_block(conv1, channels)\n-    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n-    return layers.Add()([conv2, input_tensor])\n-\n-\n-def mirnet_model(num_rrg, num_mrb, channels):\n-    input_tensor = keras.Input(shape=[None, None, 3])\n-    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n-    for _ in range(num_rrg):\n-        x1 = recursive_residual_group(x1, num_mrb, channels)\n-    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n-    output_tensor = layers.Add()([input_tensor, conv])\n-    return keras.Model(input_tensor, output_tensor)\n-\n-\n-model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n-\n-\"\"\"\n-## Training\n-\n-- We train MIRNet using **Charbonnier Loss** as the loss function and **Adam\n-Optimizer** with a learning rate of `1e-4`.\n-- We use **Peak Signal Noise Ratio** or PSNR as a metric which is an expression for the\n-ratio between the maximum possible value (power) of a signal and the power of distorting\n-noise that affects the quality of its representation.\n-\"\"\"\n-\n-\n-def charbonnier_loss(y_true, y_pred):\n-    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n-\n-\n-def peak_signal_noise_ratio(y_true, y_pred):\n-    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n-\n-\n-optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n-model.compile(\n-    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n-)\n-\n-history = model.fit(\n-    train_dataset,\n-    validation_data=val_dataset,\n-    epochs=50,\n-    callbacks=[\n-        keras.callbacks.ReduceLROnPlateau(\n-            monitor=\"val_peak_signal_noise_ratio\",\n-            factor=0.5,\n-            patience=5,\n-            verbose=1,\n-            min_delta=1e-7,\n-            mode=\"max\",\n-        )\n-    ],\n-)\n-\n-def plot_history(value):\n-    plt.plot(history.history[value], label=f\"train_{value}\")\n-    plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")\n-    plt.xlabel(\"Epochs\")\n-    plt.ylabel(value)\n-    plt.title(f\"Train and Validation {value} Over Epochs\", fontsize=14)\n-    plt.legend()\n-    plt.grid()\n-    plt.show()\n-\n-\n-plot_history(\"loss\")\n-plot_history(\"peak_signal_noise_ratio\")\n-\n-\"\"\"\n-## Inference\n-\"\"\"\n-\n-\n-def plot_results(images, titles, figure_size=(12, 12)):\n-    fig = plt.figure(figsize=figure_size)\n-    for i in range(len(images)):\n-        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n-        _ = plt.imshow(images[i])\n-        plt.axis(\"off\")\n-    plt.show()\n-\n-\n-def infer(original_image):\n-    image = keras.utils.img_to_array(original_image)\n-    image = image.astype(\"float32\") / 255.0\n-    image = np.expand_dims(image, axis=0)\n-    output = model.predict(image, verbose=0)\n-    output_image = output[0] * 255.0\n-    output_image = output_image.clip(0, 255)\n-    output_image = output_image.reshape(\n-        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n-    )\n-    output_image = Image.fromarray(np.uint8(output_image))\n-    original_image = Image.fromarray(np.uint8(original_image))\n-    return output_image\n-\n-\n-\"\"\"\n-### Inference on Test Images\n-\n-We compare the test images from LOLDataset enhanced by MIRNet with images\n-enhanced via the `PIL.ImageOps.autocontrast()` function.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/lowlight-enhance-mirnet)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Enhance_Low_Light_Image).\n-\"\"\"\n-\n-\n-for low_light_image in random.sample(test_low_light_images, 6):\n-    original_image = Image.open(low_light_image)\n-    enhanced_image = infer(original_image)\n-    plot_results(\n-        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n-        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n-        (20, 12),\n-    )\n\n@@ -467,13 +467,15 @@ def conv_transpose(\n     if isinstance(dilation_rate, int):\n         dilation_rate = [dilation_rate] * len(kernel_spatial_shape)\n     for i, value in enumerate(padding_values):\n-        both_side_padding = value[0]\n-        longer_side_padding = value[1] - value[0]\n+        total_padding = value[0] + value[1]\n         padding_arg.append(\n             dilation_rate[i] * (kernel_spatial_shape[i] - 1)\n-            - both_side_padding,\n+            - total_padding // 2\n         )\n-        output_padding_arg.append(longer_side_padding)\n+        if total_padding % 2 == 0:\n+            output_padding_arg.append(0)\n+        else:\n+            output_padding_arg.append(1)\n \n     if num_spatial_dims == 1:\n         outputs = tnn.conv_transpose1d(\n\n@@ -315,8 +315,8 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         },\n         {\n             \"filters\": 6,\n-            \"kernel_size\": 7,\n-            \"strides\": 16,\n+            \"kernel_size\": 2,\n+            \"strides\": 3,\n             \"padding\": \"same\",\n             \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n@@ -361,7 +361,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n         )\n \n-        inputs = np.random.normal(size=[2, 14, 14, 4])\n+        inputs = np.random.normal(size=[2, 8, 8, 4])\n         layer.build(input_shape=inputs.shape)\n         tf_keras_layer.build(input_shape=inputs.shape)\n \n\n@@ -1064,7 +1064,15 @@ class Layer(BackendLayer, Operation):\n                             f\"Layer '{self.name}' looks like it has \"\n                             \"unbuilt state, but Keras is not able to \"\n                             \"trace the layer `call()` in order to \"\n-                            \"build it automatically. You must implement \"\n+                            \"build it automatically. Possible causes:\\n\"\n+                            \"1. The `call()` method of your layer may be \"\n+                            \"crashing. Try to `__call__()` the layer \"\n+                            \"eagerly on some test input \"\n+                            \"first to see if it works. \"\n+                            \"E.g. `x = np.random.random((3, 4)); \"\n+                            \"y = layer(x)`\\n\"\n+                            \"2. If the `call()` method is correct, \"\n+                            \"then you may need to implement \"\n                             \"the `def build(self, input_shape)` method on your \"\n                             \"layer. It should create all variables used by the \"\n                             \"layer (e.g. by calling `layer.build()` on all its \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
