{"custom_id": "keras#1239ec8b0419763a5dc925823a23ea66037f1eac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1592 | Lines Deleted: 4 | Files Changed: 11 | Hunks: 18 | Methods Changed: 61 | Complexity Δ (Sum/Max): 74/16 | Churn Δ: 1596 | Churn Cumulative: 6340 | Contributors (this commit): 6 | Commits (past 90d): 55 | Contributors (cumulative): 26 | DMM Complexity: 0.9609507640067911\n\nDIFF:\n@@ -60,6 +60,46 @@ def benchmark_prelu(\n     )\n \n \n+def benchmark_relu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ReLU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_leaky_relu(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"LeakyReLU\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n def benchmark_softmax(\n     num_samples,\n     batch_size,\n@@ -82,6 +122,8 @@ def benchmark_softmax(\n \n BENCHMARK_NAMES = {\n     \"benchmark_elu\": benchmark_elu,\n+    \"benchmark_relu\": benchmark_relu,\n+    \"benchmark_leaky_relu\": benchmark_leaky_relu,\n     \"benchmark_prelu\": benchmark_prelu,\n     \"benchmark_softmax\": benchmark_softmax,\n }\n\n@@ -163,7 +163,9 @@ class LayerBenchmark:\n             outputs = self._tf_keras_layer(inputs)\n         return tf.keras.Model(inputs=inputs, outputs=outputs)\n \n-    def benchmark_predict(self, num_samples, batch_size):\n+    def benchmark_predict(self, num_samples, batch_size, data=None):\n+        if data is None:\n+            # Generate default data if not provided.\n             if isinstance(self.input_shape[0], (tuple, list)):\n                 # The layer has multiple inputs.\n                 data = []\n@@ -207,7 +209,9 @@ class LayerBenchmark:\n             f\"{tf_keras_throughput:.2f} samples/sec.\"\n         )\n \n-    def benchmark_train(self, num_samples, batch_size):\n+    def benchmark_train(self, num_samples, batch_size, data=None, label=None):\n+        if data is None:\n+            # Generate default data if not provided.\n             if isinstance(self.input_shape[0], (tuple, list)):\n                 # The layer has multiple inputs.\n                 data = []\n@@ -217,6 +221,9 @@ class LayerBenchmark:\n             else:\n                 data_shape = [num_samples] + list(self.input_shape)\n                 data = [np.random.normal(size=data_shape)]\n+\n+        if label is None:\n+            # Generate default label if not provided.\n             if self.flat_call_inputs:\n                 # Scale by a small factor to avoid zero gradients.\n                 label = np.array(self._keras_core_layer(*data)) * 1.001\n\n@@ -161,6 +161,64 @@ def benchmark_depthwise_conv2D(\n     )\n \n \n+def benchmark_separable_conv1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SeparableConv1D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+        \"filters\": 3,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_separable_conv2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SeparableConv2D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+        \"filters\": 3,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[128, 128, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n def benchmark_conv1D_transpose(\n     num_samples,\n     batch_size,\n@@ -251,6 +309,8 @@ BENCHMARK_NAMES = {\n     \"benchmark_conv3D\": benchmark_conv3D,\n     \"benchmark_depthwise_conv1D\": benchmark_depthwise_conv1D,\n     \"benchmark_depthwise_conv2D\": benchmark_depthwise_conv2D,\n+    \"benchmark_separable_conv1D\": benchmark_separable_conv1D,\n+    \"benchmark_separable_conv2D\": benchmark_separable_conv2D,\n     \"benchmark_conv1D_transpose\": benchmark_conv1D_transpose,\n     \"benchmark_conv2D_transpose\": benchmark_conv2D_transpose,\n     \"benchmark_conv3D_transpose\": benchmark_conv3D_transpose,\n\n@@ -12,6 +12,7 @@ python3 -m benchmarks.layer_benchmark.core_benchmark \\\n ```\n \"\"\"\n \n+import numpy as np\n from absl import app\n from absl import flags\n \n@@ -73,6 +74,37 @@ def benchmark_einsum_dense(\n     )\n \n \n+def benchmark_embedding(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Embedding\"\n+    init_args = {\n+        \"input_dim\": 30,\n+        \"output_shape\": 128,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 32],\n+        jit_compile=jit_compile,\n+    )\n+\n+    data = np.random.randint(30, size=(num_samples, 32))\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+        data=data,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+        data=data,\n+    )\n+\n+\n BENCHMARK_NAMES = {\n     \"benchmark_dense\": benchmark_dense,\n     \"benchmark_einsum_dense\": benchmark_einsum_dense,\n\n@@ -0,0 +1,224 @@\n+\"\"\" Benchmark merge layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.merge_benchmark \\\n+    --benchmark_name=benchmark_add \\\n+    --num_samples=8192 \\\n+    --batch_size=1024 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_add(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Add\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Average\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_concatenate(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Concatenate\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_dot(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Dot\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 32], [32, 64]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_maximum(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Maximum\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_minimum(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Minimum\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_multiply(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Multiply\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 32]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_subtract(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Subtract\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[[256, 256], [256, 256]],\n+        flat_call_inputs=False,\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_add\": benchmark_add,\n+    \"benchmark_average\": benchmark_average,\n+    \"benchmark_concatenate\": benchmark_concatenate,\n+    \"benchmark_dot\": benchmark_dot,\n+    \"benchmark_maximum\": benchmark_maximum,\n+    \"benchmark_minimum\": benchmark_minimum,\n+    \"benchmark_multiply\": benchmark_multiply,\n+    \"benchmark_subtract\": benchmark_subtract,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -97,10 +97,31 @@ def benchmark_layer_normalization(\n     )\n \n \n+def benchmark_unit_normalization(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UnitNormalization\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n BENCHMARK_NAMES = {\n     \"benchmark_batch_normalization\": benchmark_batch_normalization,\n     \"benchmark_group_normalization\": benchmark_group_normalization,\n     \"benchmark_layer_normalization\": benchmark_layer_normalization,\n+    \"benchmark_unit_normalization\": benchmark_unit_normalization,\n }\n \n \n\n@@ -0,0 +1,373 @@\n+\"\"\" Benchmark pooling layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.pooling_benchmark \\\n+    --benchmark_name=benchmark_max_pooling1d \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_average_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling1D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling2D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_average_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"AveragePooling3D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling1D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling2D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_max_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"MaxPooling3D\"\n+    init_args = {\n+        \"pool_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_average_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalAveragePooling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_global_max_pooling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GlobalMaxPooling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[64, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_average_pooling1d\": benchmark_average_pooling1d,\n+    \"benchmark_average_pooling2d\": benchmark_average_pooling2d,\n+    \"benchmark_average_pooling3d\": benchmark_average_pooling3d,\n+    \"benchmark_max_pooling1d\": benchmark_max_pooling1d,\n+    \"benchmark_max_pooling2d\": benchmark_max_pooling2d,\n+    \"benchmark_max_pooling3d\": benchmark_max_pooling3d,\n+    \"benchmark_global_average_pooling1d\": benchmark_global_average_pooling1d,\n+    \"benchmark_global_average_pooling2d\": benchmark_global_average_pooling2d,\n+    \"benchmark_global_average_pooling3d\": benchmark_global_average_pooling3d,\n+    \"benchmark_global_max_pooling1d\": benchmark_global_max_pooling1d,\n+    \"benchmark_global_max_pooling2d\": benchmark_global_max_pooling2d,\n+    \"benchmark_global_max_pooling3d\": benchmark_global_max_pooling3d,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,216 @@\n+\"\"\" Benchmark regularization layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.regularization_benchmark \\\n+    --benchmark_name=benchmark_dropout\\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_dropout(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Dropout\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gaussian_dropout(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GaussionDropout\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gaussian_noise(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GaussionNoise\"\n+    init_args = {\n+        \"stddev\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout1D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout2D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_spatial_dropout3D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SpatialDropout3D\"\n+    init_args = {\n+        \"rate\": 0.5,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_dropout\": benchmark_dropout,\n+    \"benchmark_gaussian_dropout\": benchmark_gaussian_dropout,\n+    \"benchmark_gaussian_noise\": benchmark_gaussian_noise,\n+    \"benchmark_spatial_dropout1D\": benchmark_spatial_dropout1D,\n+    \"benchmark_spatial_dropout2D\": benchmark_spatial_dropout2D,\n+    \"benchmark_spatial_dropout3D\": benchmark_spatial_dropout3D,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,336 @@\n+\"\"\" Benchmark reshaping layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.reshaping_benchmark \\\n+    --benchmark_name=benchmark_cropping2d \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_cropping1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_cropping2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_cropping3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Cropping3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_flatten(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Flatten\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_permute(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Permute\"\n+    init_args = {\n+        \"dim\": (2, 1),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_up_sampling3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"UpSampling3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding1D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding2D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[256, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_zero_padding3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ZeroPadding3D\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_cropping1d\": benchmark_cropping1d,\n+    \"benchmark_cropping2d\": benchmark_cropping2d,\n+    \"benchmark_cropping3d\": benchmark_cropping3d,\n+    \"benchmark_flatten\": benchmark_flatten,\n+    \"benchmark_permute\": benchmark_permute,\n+    \"benchmark_up_sampling1d\": benchmark_up_sampling1d,\n+    \"benchmark_up_sampling2d\": benchmark_up_sampling2d,\n+    \"benchmark_up_sampling3d\": benchmark_up_sampling3d,\n+    \"benchmark_zero_padding1d\": benchmark_zero_padding1d,\n+    \"benchmark_zero_padding2d\": benchmark_zero_padding2d,\n+    \"benchmark_zero_padding3d\": benchmark_zero_padding3d,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,276 @@\n+\"\"\" Benchmark rnn layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.rnn_benchmark \\\n+    --benchmark_name=benchmark_lstm \\\n+    --num_samples=2048 \\\n+    --batch_size=256 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+import keras_core\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_conv_lstm1d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM1D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv_lstm2d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM2D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 64, 64, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv_lstm3d(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"ConvLSTM3D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[16, 32, 32, 16, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_gru(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GRU\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_lstm(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"LSTM\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_simple_rnn(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"SimpleRNN\"\n+    init_args = {\n+        \"units\": 32,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_bidirectional(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Bidirectional\"\n+    init_args = {\n+        \"layer\": keras_core.layers.LSTM(32),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 256],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_time_distributed(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"TimeDistributed\"\n+    init_args = {\n+        \"layer\": keras_core.layers.Conv2D(64, (3, 3)),\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[10, 128, 128, 3],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_conv_lstm1d\": benchmark_conv_lstm1d,\n+    \"benchmark_conv_lstm2d\": benchmark_conv_lstm2d,\n+    \"benchmark_conv_lstm3d\": benchmark_conv_lstm3d,\n+    \"benchmark_gru\": benchmark_gru,\n+    \"benchmark_lstm\": benchmark_lstm,\n+    \"benchmark_simple_rnn\": benchmark_simple_rnn,\n+    \"benchmark_bidirectional\": benchmark_bidirectional,\n+    \"benchmark_time_distributed\": benchmark_time_distributed,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name is None:\n+        for name, benchmark_fn in BENCHMARK_NAMES:\n+            benchmark_fn(num_samples, batch_size, jit_compile)\n+        return\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -362,9 +362,10 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n+    size_x = size(x)\n     tf.assert_greater(\n-        size(x),\n-        tf.constant(0, dtype=tf.int64),\n+        size_x,\n+        tf.constant(0, dtype=size_x.dtype),\n         message=\"Cannot compute the min of an empty tensor.\",\n     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61eaf98d35839d80c4ba28e6dc9ae42b473d79c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 33 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,33 +0,0 @@\n-import numpy as np\n-\n-from keras_core import Model\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import optimizers\n-\n-inputs = layers.Input((100,), batch_size=32)\n-x = layers.Dense(256, activation=\"relu\")(inputs)\n-x = layers.Dense(256, activation=\"relu\")(x)\n-x = layers.Dense(256, activation=\"relu\")(x)\n-outputs = layers.Dense(16)(x)\n-model = Model(inputs, outputs)\n-\n-model.summary()\n-\n-x = np.random.random((50000, 100))\n-y = np.random.random((50000, 16))\n-batch_size = 32\n-epochs = 5\n-\n-model.compile(\n-    optimizer=optimizers.SGD(learning_rate=0.001),\n-    loss=losses.MeanSquaredError(),\n-    metrics=[metrics.MeanSquaredError()],\n-)\n-history = model.fit(\n-    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n-)\n-\n-print(\"History:\")\n-print(history.history)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4c5e416e2e243b51e0eef5e2e9d6290ca9032c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1173 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -22,6 +22,9 @@ import shutil\n \n import namex\n \n+# Needed because importing torch after TF causes the runtime to crash\n+import torch  # noqa: F401\n+\n package = \"keras_core\"\n build_directory = \"tmp_build_dir\"\n dist_directory = \"dist\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de1255b1e5117e50aca8a15441c261323c0b12e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 127/127 | Churn Δ: 3 | Churn Cumulative: 1611 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,6 +60,9 @@ class AutoCastVariableSpec(tf.types.experimental.TraceType):\n         \"\"\"Use the AutoCastVariable value itself as a placeholder.\"\"\"\n         return self._value\n \n+    def _cast(self, value, _):\n+        return value\n+\n     def _to_tensors(self, value):\n         return []\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2abf535e1b470cd836dbf3f47d2b035e52bd157", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 513 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -212,7 +212,7 @@ class L1L2(Regularizer):\n         return {\"l1\": float(self.l1), \"l2\": float(self.l2)}\n \n \n-@keras_core_export(\"keras_core.regularizers.L1\")\n+@keras_core_export([\"keras_core.regularizers.L1\", \"keras_core.regularizers.l1\"])\n class L1(Regularizer):\n     \"\"\"A regularizer that applies a L1 regularization penalty.\n \n@@ -241,7 +241,7 @@ class L1(Regularizer):\n         return {\"l1\": float(self.l1)}\n \n \n-@keras_core_export(\"keras_core.regularizers.L2\")\n+@keras_core_export([\"keras_core.regularizers.L2\", \"keras_core.regularizers.l2\"])\n class L2(Regularizer):\n     \"\"\"A regularizer that applies a L2 regularization penalty.\n \n@@ -270,7 +270,12 @@ class L2(Regularizer):\n         return {\"l2\": float(self.l2)}\n \n \n-@keras_core_export(\"keras_core.regularizers.OrthogonalRegularizer\")\n+@keras_core_export(\n+    [\n+        \"keras_core.regularizers.OrthogonalRegularizer\",\n+        \"keras_core.regularizers.orthogonal_regularizer\",\n+    ]\n+)\n class OrthogonalRegularizer(Regularizer):\n     \"\"\"Regularizer that encourages input vectors to be orthogonal to each other.\n \n\n@@ -35,7 +35,16 @@ class RegularizersTest(testing.TestCase):\n \n     def test_get_method(self):\n         obj = regularizers.get(\"l1l2\")\n-        self.assertTrue(obj, regularizers.L1L2)\n+        self.assertIsInstance(obj, regularizers.L1L2)\n+\n+        obj = regularizers.get(\"l1\")\n+        self.assertIsInstance(obj, regularizers.L1)\n+\n+        obj = regularizers.get(\"l2\")\n+        self.assertIsInstance(obj, regularizers.L2)\n+\n+        obj = regularizers.get(\"orthogonal_regularizer\")\n+        self.assertIsInstance(obj, regularizers.OrthogonalRegularizer)\n \n         obj = regularizers.get(None)\n         self.assertEqual(obj, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb80a1adbfc2a4e279aaec52e7fd4bb7f70e6b57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 5204 | Contributors (this commit): 9 | Commits (past 90d): 125 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -391,7 +391,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 }\n \n                 # Callbacks\n-                callbacks.on_train_batch_end(step, logs)\n+                callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n                 if self.stop_training:\n                     break\n \n@@ -401,7 +401,7 @@ class JAXTrainer(base_trainer.Trainer):\n             self.jax_state_sync()\n \n             # Override with model metrics instead of last step logs\n-            epoch_logs = self._pythonify_logs(self.get_metrics_result())\n+            epoch_logs = self.get_metrics_result()\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n@@ -427,7 +427,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 val_logs = {\n                     \"val_\" + name: val for name, val in val_logs.items()\n                 }\n-                epoch_logs.update(self._pythonify_logs(val_logs))\n+                epoch_logs.update(val_logs)\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n             training_logs = epoch_logs\n@@ -528,12 +528,12 @@ class JAXTrainer(base_trainer.Trainer):\n                 \"non_trainable_variables\": non_trainable_variables,\n                 \"metrics_variables\": metrics_variables,\n             }\n-            callbacks.on_test_batch_end(step, logs)\n+            callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n \n         # Reattach state back to model.\n         self.jax_state_sync()\n \n-        logs = self._pythonify_logs(self.get_metrics_result())\n+        logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n         self._jax_state = None\n         if return_dict:\n\n@@ -304,12 +304,14 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 for step, iterator in epoch_iterator.enumerate_epoch():\n                     callbacks.on_train_batch_begin(step)\n                     logs = self.train_function(iterator)\n-                    callbacks.on_train_batch_end(step, logs)\n+                    callbacks.on_train_batch_end(\n+                        step, self._pythonify_logs(logs)\n+                    )\n                     if self.stop_training:\n                         break\n \n             # Override with model metrics instead of last step logs\n-            epoch_logs = self._pythonify_logs(self.get_metrics_result())\n+            epoch_logs = self.get_metrics_result()\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n@@ -335,7 +337,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 val_logs = {\n                     \"val_\" + name: val for name, val in val_logs.items()\n                 }\n-                epoch_logs.update(self._pythonify_logs(val_logs))\n+                epoch_logs.update(val_logs)\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n             training_logs = epoch_logs\n@@ -407,8 +409,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             for step, iterator in epoch_iterator.enumerate_epoch():\n                 callbacks.on_test_batch_begin(step)\n                 logs = self.test_function(iterator)\n-                callbacks.on_test_batch_end(step, logs)\n-        logs = self._pythonify_logs(self.get_metrics_result())\n+                callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+        logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n \n         if return_dict:\n\n@@ -144,12 +144,12 @@ class TorchTrainer(base_trainer.Trainer):\n                 logs = self.train_step(data)\n \n                 # Callbacks\n-                callbacks.on_train_batch_end(step, logs)\n+                callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n                 if self.stop_training:\n                     break\n \n             # Override with model metrics instead of last step logs\n-            epoch_logs = self._pythonify_logs(self.get_metrics_result())\n+            epoch_logs = self.get_metrics_result()\n \n             # Switch the torch Module back to testing mode.\n             self.eval()\n@@ -178,7 +178,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 val_logs = {\n                     \"val_\" + name: val for name, val in val_logs.items()\n                 }\n-                epoch_logs.update(self._pythonify_logs(val_logs))\n+                epoch_logs.update(val_logs)\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n             training_logs = epoch_logs\n@@ -263,8 +263,8 @@ class TorchTrainer(base_trainer.Trainer):\n             callbacks.on_test_batch_begin(step)\n             with torch.no_grad():\n                 logs = self.test_step(data)\n-            callbacks.on_test_batch_end(step, logs)\n-        logs = self._pythonify_logs(self.get_metrics_result())\n+            callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+        logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n \n         if return_dict:\n\n@@ -230,7 +230,7 @@ class Trainer:\n                 return_metrics.update(result)\n             else:\n                 return_metrics[metric.name] = result\n-        return return_metrics\n+        return self._pythonify_logs(return_metrics)\n \n     def fit(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
