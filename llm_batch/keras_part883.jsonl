{"custom_id": "keras#6b4a4dfaa26c14d3071a489e43453917f7b42e30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 47 | Files Changed: 2 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 125 | Churn Cumulative: 2640 | Contributors (this commit): 16 | Commits (past 90d): 24 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,7 +24,17 @@ from keras.src.backend.tensorflow.core import shape as shape_op\n \n \n def rot90(array, k=1, axes=(0, 1)):\n-    \"\"\"Rotate an array by 90 degrees in the specified plane.\"\"\"\n+    \"\"\"Rotate an array by 90 degrees in the specified plane.\n+\n+    Args:\n+        array: Input tensor\n+        k: Number of 90-degree rotations (default=1)\n+        axes: Tuple of two axes that define the plane of rotation.\n+        Defaults to (0, 1).\n+\n+    Returns:\n+        Rotated tensor with correct shape transformation\n+    \"\"\"\n     array = convert_to_tensor(array)\n \n     if array.shape.rank < 2:\n@@ -53,14 +63,26 @@ def rot90(array, k=1, axes=(0, 1)):\n \n     shape = tf.shape(array)\n     non_rot_shape = shape[:-2]\n-    rot_shape = shape[-2:]\n+    h, w = shape[-2], shape[-1]\n \n-    array = tf.reshape(array, tf.concat([[-1], rot_shape], axis=0))\n+    array = tf.reshape(array, tf.concat([[-1], [h, w]], axis=0))\n \n-    for _ in range(k):\n+    array = tf.reverse(array, axis=[2])\n     array = tf.transpose(array, [0, 2, 1])\n-        array = tf.reverse(array, axis=[1])\n-    array = tf.reshape(array, tf.concat([non_rot_shape, rot_shape], axis=0))\n+\n+    if k % 2 == 1:\n+        final_h, final_w = w, h\n+    else:\n+        final_h, final_w = h, w\n+\n+    if k > 1:\n+        array = tf.reshape(array, tf.concat([[-1], [final_h, final_w]], axis=0))\n+        for _ in range(k - 1):\n+            array = tf.reverse(array, axis=[2])\n+            array = tf.transpose(array, [0, 2, 1])\n+\n+    final_shape = tf.concat([non_rot_shape, [final_h, final_w]], axis=0)\n+    array = tf.reshape(array, final_shape)\n \n     inv_perm = [0] * len(perm)\n     for i, p in enumerate(perm):\n\n@@ -20,59 +20,68 @@ from keras.src.testing.test_utils import named_product\n \n \n class NumPyTestRot90(testing.TestCase):\n-    def test_basic(self):\n-        array = np.array([[1, 2], [3, 4]])\n+    def test_basic_rotation(self):\n+        array = np.array([[1, 2, 3], [4, 5, 6]])\n         rotated = knp.rot90(array)\n-        expected = np.array([[2, 4], [1, 3]])\n+        expected = np.rot90(array)\n         self.assertAllClose(rotated, expected)\n \n-    def test_multiple_k(self):\n+    @parameterized.named_parameters(\n+        (\"k_0\", 0, [[1, 2], [3, 4]]),\n+        (\"k_1\", 1, [[2, 4], [1, 3]]),\n+        (\"k_2\", 2, [[4, 3], [2, 1]]),\n+        (\"k_neg1\", -1, [[3, 1], [4, 2]]),\n+        (\"k_5\", 5, [[2, 4], [1, 3]]),  # k=5 ≡ k=1 (mod 4)\n+        (\"k_6\", 6, [[4, 3], [2, 1]]),  # k=6 ≡ k=2 (mod 4)\n+    )\n+    def test_k_parameter_variations(self, k, expected):\n         array = np.array([[1, 2], [3, 4]])\n+        rotated = knp.rot90(array, k=k)\n+        expected = np.array(expected)\n+        self.assertAllClose(rotated, expected)\n+        print(k)\n \n-        # k=2 (180 degrees rotation)\n-        rotated = knp.rot90(array, k=2)\n-        expected = np.array([[4, 3], [2, 1]])\n+    @parameterized.named_parameters(\n+        (\"axes_0_1\", (0, 1)), (\"axes_1_2\", (1, 2)), (\"axes_0_2\", (0, 2))\n+    )\n+    def test_3d_operations(self, axes):\n+        array_3d = np.arange(12).reshape(3, 2, 2)\n+        rotated = knp.rot90(array_3d, axes=axes)\n+        expected = np.rot90(array_3d, axes=axes)\n         self.assertAllClose(rotated, expected)\n \n-        # k=3 (270 degrees rotation)\n-        rotated = knp.rot90(array, k=3)\n-        expected = np.array([[3, 1], [4, 2]])\n+    @parameterized.named_parameters(\n+        (\"single_image\", np.random.random((4, 4, 3))),\n+        (\"batch_images\", np.random.random((2, 4, 4, 3))),\n+    )\n+    def test_image_processing(self, array):\n+        np.random.seed(0)\n+        rotated = knp.rot90(array, axes=(0, 1))\n+        expected = np.rot90(array, axes=(0, 1))\n         self.assertAllClose(rotated, expected)\n \n-        # k=4 (full rotation)\n-        rotated = knp.rot90(array, k=4)\n-        expected = array\n+    @parameterized.named_parameters(\n+        (\"single_row\", [[1, 2, 3]]),\n+        (\"single_column\", [[1], [2], [3]]),\n+        (\"negative_values\", [[-1, 0], [1, -2]]),\n+    )\n+    def test_edge_conditions(self, array):\n+        numpy_array = np.array(array)\n+        rotated = knp.rot90(numpy_array)\n+        expected = np.rot90(numpy_array)\n         self.assertAllClose(rotated, expected)\n \n-    def test_axes(self):\n-        array = np.arange(8).reshape((2, 2, 2))\n-        rotated = knp.rot90(array, k=1, axes=(1, 2))\n-        expected = np.array([[[1, 3], [0, 2]], [[5, 7], [4, 6]]])\n-        self.assertAllClose(rotated, expected)\n-\n-    def test_single_image(self):\n-        array = np.random.random((4, 4, 3))\n-        rotated = knp.rot90(array, k=1, axes=(0, 1))\n-        expected = np.rot90(array, k=1, axes=(0, 1))\n-        self.assertAllClose(rotated, expected)\n-\n-    def test_batch_images(self):\n-        array = np.random.random((2, 4, 4, 3))\n-        rotated = knp.rot90(array, k=1, axes=(1, 2))\n-        expected = np.rot90(array, k=1, axes=(1, 2))\n-        self.assertAllClose(rotated, expected)\n-\n-    def test_invalid_axes(self):\n-        array = np.array([[1, 2], [3, 4]])\n-        with self.assertRaisesRegex(ValueError, \"Invalid axes\"):\n-            knp.rot90(array, axes=(0, 0))\n-\n-    def test_invalid_rank(self):\n-        array = np.array([1, 2, 3])  # 1D array\n-        with self.assertRaisesRegex(\n-            ValueError, \"Input array must have at least 2 dimensions\"\n-        ):\n+    @parameterized.named_parameters(\n+        (\"1D_array\", np.array([1, 2, 3]), None),\n+        (\"duplicate_axes\", np.array([[1, 2], [3, 4]]), (0, 0)),\n+    )\n+    def test_error_conditions(self, array, axes):\n+        if axes is None:\n+            with self.assertRaises(ValueError):\n                 knp.rot90(array)\n+        else:\n+            with self.assertRaises(ValueError):\n+                knp.rot90(array, axes=axes)\n \n \n class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7bb2f37056f6ddfffe8da6e29db8842b706b0c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 103 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -115,8 +115,8 @@ def fft2(x):\n \n def ifft2(x):\n     real, imag = x\n-    h = cast(tf.shape(real)[-2], \"float32\")\n-    w = cast(tf.shape(real)[-1], \"float32\")\n+    h = cast(tf.shape(real)[-2], real.dtype)\n+    w = cast(tf.shape(real)[-1], real.dtype)\n     real_conj, imag_conj = real, -imag\n     fft_real, fft_imag = fft2((real_conj, imag_conj))\n     return fft_real / (h * w), -fft_imag / (h * w)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ad808d315f7eda8230914ddc88131ad5667c802", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 83 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,15 @@ class Sequential(Model):\n             self._functional = None\n \n     def pop(self, rebuild=True):\n-        \"\"\"Removes the last layer in the model.\"\"\"\n+        \"\"\"Removes the last layer in the model.\n+\n+        Args:\n+            rebuild: `bool`. Whether to rebuild the model after removing\n+            the layer. Defaults to `True`.\n+\n+        Returns:\n+            layer: layer instance.\n+        \"\"\"\n         layer = self._layers.pop()\n         self.built = False\n         self._functional = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e739db38db99ff5fa37d3d118d9342508fa5546b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 56 | Files Changed: 5 | Hunks: 21 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 152 | Churn Cumulative: 1092 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 0.11764705882352941\n\nDIFF:\n@@ -4,7 +4,6 @@ import itertools\n import string\n import warnings\n \n-from keras.src import layers\n from keras.src import tree\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.utils.module_utils import tensorflow as tf\n@@ -16,20 +15,11 @@ class JaxExportArchive:\n         self._backend_trainable_variables = []\n         self._backend_non_trainable_variables = []\n \n-    def track(self, resource):\n-        if not isinstance(resource, layers.Layer):\n-            raise ValueError(\n-                \"Invalid resource type. Expected an instance of a \"\n-                \"JAX-based Keras `Layer` or `Model`. \"\n-                f\"Received instead an object of type '{type(resource)}'. \"\n-                f\"Object received: {resource}\"\n-            )\n-\n-        if isinstance(resource, layers.Layer):\n+    def _track_layer(self, layer):\n         # Variables in the lists below are actually part of the trackables\n         # that get saved, because the lists are created in __init__.\n-            trainable_variables = resource.trainable_variables\n-            non_trainable_variables = resource.non_trainable_variables\n+        trainable_variables = layer.trainable_variables\n+        non_trainable_variables = layer.non_trainable_variables\n \n         self._tf_trackable.trainable_variables += tree.map_structure(\n             self._convert_to_tf_variable, trainable_variables\n\n@@ -1,29 +1,16 @@\n import tensorflow as tf\n \n-from keras.src import layers\n-\n \n class TFExportArchive:\n-    def track(self, resource):\n-        if not isinstance(resource, tf.__internal__.tracking.Trackable):\n-            raise ValueError(\n-                \"Invalid resource type. Expected an instance of a \"\n-                \"TensorFlow `Trackable` (such as a Keras `Layer` or `Model`). \"\n-                f\"Received instead an object of type '{type(resource)}'. \"\n-                f\"Object received: {resource}\"\n-            )\n-\n-        if isinstance(resource, layers.Layer):\n+    def _track_layer(self, layer):\n         # Variables in the lists below are actually part of the trackables\n         # that get saved, because the lists are created in __init__.\n-            variables = resource.variables\n-            trainable_variables = resource.trainable_variables\n-            non_trainable_variables = resource.non_trainable_variables\n+        variables = layer.variables\n+        trainable_variables = layer.trainable_variables\n+        non_trainable_variables = layer.non_trainable_variables\n         self._tf_trackable.variables += variables\n         self._tf_trackable.trainable_variables += trainable_variables\n-            self._tf_trackable.non_trainable_variables += (\n-                non_trainable_variables\n-            )\n+        self._tf_trackable.non_trainable_variables += non_trainable_variables\n \n     def add_endpoint(self, name, fn, input_signature=None, **kwargs):\n         decorated_fn = tf.function(\n\n@@ -10,16 +10,16 @@ from keras.src.utils.module_utils import torch_xla\n \n \n class TorchExportArchive:\n-    def track(self, resource):\n+    def _track_layer(self, layer):\n         raise NotImplementedError(\n-            \"`track` is not implemented in the torch backend. Use\"\n-            \"`track_and_add_endpoint` instead.\"\n+            \"`track` is not supported for `Layer`s and `Model`s in the torch \"\n+            \"backend. Use `track_and_add_endpoint` instead.\"\n         )\n \n     def add_endpoint(self, name, fn, input_signature, **kwargs):\n         raise NotImplementedError(\n-            \"`add_endpoint` is not implemented in the torch backend. Use\"\n-            \"`track_and_add_endpoint` instead.\"\n+            \"`add_endpoint` is not supported for `Layer`s and `Model`s in the \"\n+            \"torch backend. Use `track_and_add_endpoint` instead.\"\n         )\n \n     def track_and_add_endpoint(self, name, resource, input_signature, **kwargs):\n\n@@ -31,7 +31,7 @@ elif backend.backend() == \"openvino\":\n     )\n else:\n     raise RuntimeError(\n-        f\"Backend '{backend.backend()}' must implement a layer mixin class.\"\n+        f\"Backend '{backend.backend()}' must implement ExportArchive.\"\n     )\n \n \n@@ -135,18 +135,17 @@ class ExportArchive(BackendExportArchive):\n         return self._tf_trackable.non_trainable_variables\n \n     def track(self, resource):\n-        \"\"\"Track the variables (and other assets) of a layer or model.\n+        \"\"\"Track the variables (of a layer or model) and other assets.\n \n-        By default, all variables used by an endpoint function\n-        are automatically tracked when you call `add_endpoint()`.\n-        However, non-variables assets such as lookup tables\n-        need to be tracked manually. Note that lookup tables\n-        used by built-in Keras layers\n-        (`TextVectorization`, `IntegerLookup`, `StringLookup`)\n-        are automatically tracked in `add_endpoint()`.\n+        By default, all variables used by an endpoint function are automatically\n+        tracked when you call `add_endpoint()`. However, non-variables assets\n+        such as lookup tables need to be tracked manually. Note that lookup\n+        tables used by built-in Keras layers (`TextVectorization`,\n+        `IntegerLookup`, `StringLookup`) are automatically tracked by\n+        `add_endpoint()`.\n \n         Args:\n-            resource: A trackable Keras resource, such as a layer or model.\n+            resource: A layer, model or a TensorFlow trackable resource.\n         \"\"\"\n         if isinstance(resource, layers.Layer) and not resource.built:\n             raise ValueError(\n@@ -154,14 +153,22 @@ class ExportArchive(BackendExportArchive):\n                 \"It must be built before export.\"\n             )\n \n-        # Layers in `_tracked` are not part of the trackables that get saved,\n-        # because we're creating the attribute in a\n-        # no_automatic_dependency_tracking scope.\n+        # Note: with the TensorFlow backend, Layers and Models fall into both\n+        # the Layer case and the Trackable case. The Trackable case is needed\n+        # for preprocessing layers in order to track lookup tables.\n+        if isinstance(resource, tf.__internal__.tracking.Trackable):\n             if not hasattr(self, \"_tracked\"):\n                 self._tracked = []\n             self._tracked.append(resource)\n \n-        BackendExportArchive.track(self, resource)\n+        if isinstance(resource, layers.Layer):\n+            self._track_layer(resource)\n+        elif not isinstance(resource, tf.__internal__.tracking.Trackable):\n+            raise ValueError(\n+                \"Invalid resource type. Expected a Keras `Layer` or `Model` \"\n+                \"or a TensorFlow `Trackable` object. \"\n+                f\"Received object {resource} of type '{type(resource)}'. \"\n+            )\n \n     def add_endpoint(self, name, fn, input_signature=None, **kwargs):\n         \"\"\"Register a new serving endpoint.\n@@ -283,6 +290,28 @@ class ExportArchive(BackendExportArchive):\n         export_archive.track(model)\n         export_archive.add_endpoint(name=\"serve\", fn=serving_fn)\n         ```\n+\n+        Combining a model with some TensorFlow preprocessing, which can use\n+        TensorFlow resources:\n+\n+        ```python\n+        lookup_table = tf.lookup.StaticHashTable(initializer, default_value=0.0)\n+\n+        export_archive = ExportArchive()\n+        model_fn = export_archive.track_and_add_endpoint(\n+            \"model_fn\",\n+            model,\n+            input_signature=[tf.TensorSpec(shape=(None, 5), dtype=tf.float32)],\n+        )\n+        export_archive.track(lookup_table)\n+\n+        @tf.function()\n+        def serving_fn(x):\n+            x = lookup_table.lookup(x)\n+            return model_fn(x)\n+\n+        export_archive.add_endpoint(name=\"serve\", fn=serving_fn)\n+        ```\n         \"\"\"\n         if name in self._endpoint_names:\n             raise ValueError(f\"Endpoint name '{name}' is already taken.\")\n@@ -332,9 +361,7 @@ class ExportArchive(BackendExportArchive):\n         input_signature = tree.map_structure(\n             make_tf_tensor_spec, input_signature\n         )\n-        decorated_fn = BackendExportArchive.add_endpoint(\n-            self, name, fn, input_signature, **kwargs\n-        )\n+        decorated_fn = super().add_endpoint(name, fn, input_signature, **kwargs)\n         self._endpoint_signatures[name] = input_signature\n         setattr(self._tf_trackable, name, decorated_fn)\n         self._endpoint_names.append(name)\n@@ -400,8 +427,8 @@ class ExportArchive(BackendExportArchive):\n             )\n         else:\n             # Special case for the torch backend.\n-            decorated_fn = BackendExportArchive.track_and_add_endpoint(\n-                self, name, resource, input_signature, **kwargs\n+            decorated_fn = super().track_and_add_endpoint(\n+                name, resource, input_signature, **kwargs\n             )\n             self._endpoint_signatures[name] = input_signature\n             setattr(self._tf_trackable, name, decorated_fn)\n@@ -480,7 +507,6 @@ class ExportArchive(BackendExportArchive):\n             raise ValueError(\n                 \"No endpoints have been set yet. Call add_endpoint().\"\n             )\n-        if backend.backend() == \"tensorflow\":\n         self._filter_and_track_resources()\n \n         signatures = {}\n\n@@ -1002,3 +1002,40 @@ class ExportArchiveTest(testing.TestCase):\n         self.assertAllClose(ref_output, revived_model.serve(ref_input))\n         # Test with a different batch size\n         revived_model.serve(tf.random.normal((6, 10)))\n+\n+    def test_model_combined_with_tf_preprocessing(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+\n+        lookup_table = tf.lookup.StaticHashTable(\n+            tf.lookup.KeyValueTensorInitializer(\n+                tf.constant([\"a\", \"b\", \"c\"]), tf.constant([1.0, 2.0, 3.0])\n+            ),\n+            default_value=-1.0,\n+        )\n+        ref_input = tf.constant([[\"c\", \"b\", \"c\", \"a\", \"d\"]])\n+        ref_intermediate = lookup_table.lookup(ref_input)\n+\n+        model = models.Sequential([layers.Dense(1)])\n+        ref_output = model(ref_intermediate)\n+\n+        export_archive = saved_model.ExportArchive()\n+        model_fn = export_archive.track_and_add_endpoint(\n+            \"model\",\n+            model,\n+            input_signature=[tf.TensorSpec(shape=(None, 5), dtype=tf.float32)],\n+        )\n+        export_archive.track(lookup_table)\n+\n+        @tf.function()\n+        def combined_fn(x):\n+            x = lookup_table.lookup(x)\n+            x = model_fn(x)\n+            return x\n+\n+        self.assertAllClose(combined_fn(ref_input), ref_output)\n+\n+        export_archive.add_endpoint(\"combined_fn\", combined_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(revived_model.combined_fn(ref_input), ref_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ecf5760103fcf7069f97b2e4f40ad5d758b62591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 26 | Churn Cumulative: 60 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -262,6 +262,10 @@ class LossScaleOptimizer(optimizer.Optimizer):\n     def learning_rate(self, learning_rate):\n         self.inner_optimizer.learning_rate = learning_rate\n \n+    @property\n+    def iterations(self):\n+        return self.inner_optimizer.iterations\n+\n     def scale_loss(self, loss):\n         scale = self.dynamic_scale if self.built else self.initial_scale\n         return loss * scale\n\n@@ -106,3 +106,25 @@ class LossScaleOptimizerTest(testing.TestCase):\n             else:\n                 optimizer.apply(grads, vars)\n         self.assertAllClose(optimizer.scale_loss(1.0), 32.0)\n+\n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_iterations_update(self, stateless):\n+        self._skip_test_for_stateless(stateless)\n+\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        optimizer.build(vars)\n+        opt_vars = optimizer.variables\n+        grads = [ops.array([1.0, 6.0, 7.0, 2.0])]\n+\n+        self.assertEqual(optimizer.iterations.value, 0)\n+\n+        for i in range(3):\n+            if stateless:\n+                _, opt_vars = optimizer.stateless_apply(opt_vars, grads, vars)\n+                for ref_v, v in zip(optimizer.variables, opt_vars):\n+                    ref_v.assign(v)\n+            else:\n+                optimizer.apply(grads, vars)\n+            self.assertEqual(optimizer.iterations.value, i + 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e045b6abe42ed9335a85b553cc9c609b76a8a54c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 23 | Churn Cumulative: 371 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 0.47058823529411764\n\nDIFF:\n@@ -323,7 +323,15 @@ def _clone_sequential_model(model, clone_function, input_tensors=None):\n                 name=input_name,\n             )\n             new_layers = [inputs] + new_layers\n-    return Sequential(new_layers, name=model.name, trainable=model.trainable)\n+    cloned_model = Sequential(\n+        new_layers, name=model.name, trainable=model.trainable\n+    )\n+\n+    # If model compiled already then set same to cloned model\n+    if model.compiled:\n+        compiled_config = model.get_compile_config()\n+        cloned_model.compile_from_config(compiled_config)\n+    return cloned_model\n \n \n def _clone_functional_model(\n@@ -405,5 +413,7 @@ def _clone_functional_model(\n         # class than the original. However various existing models rely\n         # on this behavior, so we keep it.\n         new_model = Functional(input_tensors, output_tensors, name=model.name)\n-\n+    if model.compiled:\n+        compiled_config = model.get_compile_config()\n+        new_model.compile_from_config(compiled_config)\n     return new_model\n\n@@ -242,3 +242,12 @@ class CloneModelTest(testing.TestCase):\n             if isinstance(l2, layers.Dense):\n                 self.assertFalse(hasattr(l1, \"flag\"))\n                 self.assertTrue(hasattr(l2, \"flag\"))\n+\n+    def test_compiled_model_cloning(self):\n+        model = models.Sequential()\n+        model.add(layers.Input((3,)))\n+        model.add(layers.Dense(5, activation=\"relu\"))\n+        model.add(layers.Dense(1, activation=\"sigmoid\"))\n+        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n+        cloned_model = clone_model(model)\n+        self.assertEqual(model.compiled, cloned_model.compiled)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86873b5f9555bb5b98a66e51fc8584c39f723dad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1379 | Lines Deleted: 0 | Files Changed: 8 | Hunks: 12 | Methods Changed: 21 | Complexity Δ (Sum/Max): 104/21 | Churn Δ: 1379 | Churn Cumulative: 6099 | Contributors (this commit): 13 | Commits (past 90d): 17 | Contributors (cumulative): 47 | DMM Complexity: 0.6637630662020906\n\nDIFF:\n@@ -10,6 +10,7 @@ from keras.src.ops.image import extract_patches\n from keras.src.ops.image import hsv_to_rgb\n from keras.src.ops.image import map_coordinates\n from keras.src.ops.image import pad_images\n+from keras.src.ops.image import perspective_transform\n from keras.src.ops.image import resize\n from keras.src.ops.image import rgb_to_grayscale\n from keras.src.ops.image import rgb_to_hsv\n\n@@ -10,6 +10,7 @@ from keras.src.ops.image import extract_patches\n from keras.src.ops.image import hsv_to_rgb\n from keras.src.ops.image import map_coordinates\n from keras.src.ops.image import pad_images\n+from keras.src.ops.image import perspective_transform\n from keras.src.ops.image import resize\n from keras.src.ops.image import rgb_to_grayscale\n from keras.src.ops.image import rgb_to_hsv\n\n@@ -491,6 +491,151 @@ MAP_COORDINATES_FILL_MODES = {\n }\n \n \n+def perspective_transform(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    if start_points.shape[-2:] != (4, 2) or start_points.ndim not in (2, 3):\n+        raise ValueError(\n+            \"Invalid start_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {start_points.shape}\"\n+        )\n+    if end_points.shape[-2:] != (4, 2) or end_points.ndim not in (2, 3):\n+        raise ValueError(\n+            \"Invalid end_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {end_points.shape}\"\n+        )\n+    if start_points.shape != end_points.shape:\n+        raise ValueError(\n+            \"start_points and end_points must have the same shape.\"\n+            f\" Received start_points.shape={start_points.shape}, \"\n+            f\"end_points.shape={end_points.shape}\"\n+        )\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = jnp.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if len(start_points.shape) == 2:\n+        start_points = jnp.expand_dims(start_points, axis=0)\n+    if len(end_points.shape) == 2:\n+        end_points = jnp.expand_dims(end_points, axis=0)\n+\n+    if data_format == \"channels_first\":\n+        images = jnp.transpose(images, (0, 2, 3, 1))\n+\n+    batch_size, height, width, channels = images.shape\n+    transforms = compute_homography_matrix(\n+        jnp.asarray(start_points), jnp.asarray(end_points)\n+    )\n+\n+    x, y = jnp.meshgrid(jnp.arange(width), jnp.arange(height), indexing=\"xy\")\n+    grid = jnp.stack([x.ravel(), y.ravel(), jnp.ones_like(x).ravel()], axis=0)\n+\n+    def transform_coordinates(transform):\n+        denom = transform[6] * grid[0] + transform[7] * grid[1] + 1.0\n+        x_in = (\n+            transform[0] * grid[0] + transform[1] * grid[1] + transform[2]\n+        ) / denom\n+        y_in = (\n+            transform[3] * grid[0] + transform[4] * grid[1] + transform[5]\n+        ) / denom\n+        return jnp.stack([y_in, x_in], axis=0)\n+\n+    transformed_coords = jax.vmap(transform_coordinates)(transforms)\n+\n+    def interpolate_image(image, coords):\n+        def interpolate_channel(channel_img):\n+            return jax.scipy.ndimage.map_coordinates(\n+                channel_img,\n+                coords,\n+                order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                mode=\"constant\",\n+                cval=fill_value,\n+            ).reshape(height, width)\n+\n+        return jax.vmap(interpolate_channel, in_axes=0)(\n+            jnp.moveaxis(image, -1, 0)\n+        )\n+\n+    output = jax.vmap(interpolate_image, in_axes=(0, 0))(\n+        images, transformed_coords\n+    )\n+    output = jnp.moveaxis(output, 1, -1)\n+\n+    if data_format == \"channels_first\":\n+        output = jnp.transpose(output, (0, 3, 1, 2))\n+    if need_squeeze:\n+        output = jnp.squeeze(output, axis=0)\n+\n+    return output\n+\n+\n+def compute_homography_matrix(start_points, end_points):\n+    start_x, start_y = start_points[..., 0], start_points[..., 1]\n+    end_x, end_y = end_points[..., 0], end_points[..., 1]\n+\n+    zeros = jnp.zeros_like(end_x)\n+    ones = jnp.ones_like(end_x)\n+\n+    x_rows = jnp.stack(\n+        [\n+            end_x,\n+            end_y,\n+            ones,\n+            zeros,\n+            zeros,\n+            zeros,\n+            -start_x * end_x,\n+            -start_x * end_y,\n+        ],\n+        axis=-1,\n+    )\n+    y_rows = jnp.stack(\n+        [\n+            zeros,\n+            zeros,\n+            zeros,\n+            end_x,\n+            end_y,\n+            ones,\n+            -start_y * end_x,\n+            -start_y * end_y,\n+        ],\n+        axis=-1,\n+    )\n+\n+    coefficient_matrix = jnp.concatenate([x_rows, y_rows], axis=1)\n+\n+    target_vector = jnp.expand_dims(\n+        jnp.concatenate([start_x, start_y], axis=-1), axis=-1\n+    )\n+\n+    homography_matrix = jnp.linalg.solve(coefficient_matrix, target_vector)\n+\n+    return homography_matrix.squeeze(-1)\n+\n+\n def map_coordinates(\n     inputs, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n ):\n\n@@ -493,6 +493,257 @@ def affine_transform(\n     return affined\n \n \n+def perspective_transform(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    start_points = convert_to_tensor(start_points)\n+    end_points = convert_to_tensor(end_points)\n+\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS:\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_INTERPOLATIONS}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    if start_points.ndim not in (2, 3) or start_points.shape[-2:] != (4, 2):\n+        raise ValueError(\n+            \"Invalid start_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {start_points.shape}\"\n+        )\n+    if end_points.ndim not in (2, 3) or end_points.shape[-2:] != (4, 2):\n+        raise ValueError(\n+            \"Invalid end_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {end_points.shape}\"\n+        )\n+    if start_points.shape != end_points.shape:\n+        raise ValueError(\n+            \"start_points and end_points must have the same shape.\"\n+            f\" Received start_points.shape={start_points.shape}, \"\n+            f\"end_points.shape={end_points.shape}\"\n+        )\n+\n+    input_dtype = images.dtype\n+    if input_dtype == \"float16\":\n+        images = images.astype(\"float32\")\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = np.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if len(start_points.shape) == 2:\n+        start_points = np.expand_dims(start_points, axis=0)\n+    if len(end_points.shape) == 2:\n+        end_points = np.expand_dims(end_points, axis=0)\n+\n+    if data_format == \"channels_first\":\n+        images = np.transpose(images, (0, 2, 3, 1))\n+\n+    batch_size, height, width, channels = images.shape\n+\n+    transforms = compute_homography_matrix(start_points, end_points)\n+\n+    if len(transforms.shape) == 1:\n+        transforms = np.expand_dims(transforms, axis=0)\n+    if transforms.shape[0] == 1 and batch_size > 1:\n+        transforms = np.tile(transforms, (batch_size, 1))\n+\n+    x, y = np.meshgrid(\n+        np.arange(width, dtype=np.float32),\n+        np.arange(height, dtype=np.float32),\n+        indexing=\"xy\",\n+    )\n+\n+    output = np.empty((batch_size, height, width, channels))\n+\n+    for i in range(batch_size):\n+        a0, a1, a2, a3, a4, a5, a6, a7 = transforms[i]\n+        denom = a6 * x + a7 * y + 1.0\n+        x_in = (a0 * x + a1 * y + a2) / denom\n+        y_in = (a3 * x + a4 * y + a5) / denom\n+\n+        coords = np.stack([y_in.ravel(), x_in.ravel()], axis=0)\n+\n+        mapped_channels = []\n+        for channel in range(channels):\n+            channel_img = images[i, :, :, channel]\n+\n+            mapped_channel = map_coordinates(\n+                channel_img,\n+                coords,\n+                order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                fill_mode=\"constant\",\n+                fill_value=fill_value,\n+            )\n+            mapped_channels.append(mapped_channel.reshape(height, width))\n+\n+        output[i] = np.stack(mapped_channels, axis=-1)\n+\n+    if data_format == \"channels_first\":\n+        output = np.transpose(output, (0, 3, 1, 2))\n+    if need_squeeze:\n+        output = np.squeeze(output, axis=0)\n+    output = output.astype(input_dtype)\n+\n+    return output\n+\n+\n+def compute_homography_matrix(start_points, end_points):\n+    start_x1, start_y1 = start_points[:, 0, 0], start_points[:, 0, 1]\n+    start_x2, start_y2 = start_points[:, 1, 0], start_points[:, 1, 1]\n+    start_x3, start_y3 = start_points[:, 2, 0], start_points[:, 2, 1]\n+    start_x4, start_y4 = start_points[:, 3, 0], start_points[:, 3, 1]\n+\n+    end_x1, end_y1 = end_points[:, 0, 0], end_points[:, 0, 1]\n+    end_x2, end_y2 = end_points[:, 1, 0], end_points[:, 1, 1]\n+    end_x3, end_y3 = end_points[:, 2, 0], end_points[:, 2, 1]\n+    end_x4, end_y4 = end_points[:, 3, 0], end_points[:, 3, 1]\n+\n+    coefficient_matrix = np.stack(\n+        [\n+            np.stack(\n+                [\n+                    end_x1,\n+                    end_y1,\n+                    np.ones_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    -start_x1 * end_x1,\n+                    -start_x1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    end_x1,\n+                    end_y1,\n+                    np.ones_like(end_x1),\n+                    -start_y1 * end_x1,\n+                    -start_y1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x2,\n+                    end_y2,\n+                    np.ones_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    -start_x2 * end_x2,\n+                    -start_x2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    end_x2,\n+                    end_y2,\n+                    np.ones_like(end_x2),\n+                    -start_y2 * end_x2,\n+                    -start_y2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x3,\n+                    end_y3,\n+                    np.ones_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    -start_x3 * end_x3,\n+                    -start_x3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    end_x3,\n+                    end_y3,\n+                    np.ones_like(end_x3),\n+                    -start_y3 * end_x3,\n+                    -start_y3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x4,\n+                    end_y4,\n+                    np.ones_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    -start_x4 * end_x4,\n+                    -start_x4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    end_x4,\n+                    end_y4,\n+                    np.ones_like(end_x4),\n+                    -start_y4 * end_x4,\n+                    -start_y4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+        ],\n+        axis=1,\n+    )\n+\n+    target_vector = np.stack(\n+        [\n+            start_x1,\n+            start_y1,\n+            start_x2,\n+            start_y2,\n+            start_x3,\n+            start_y3,\n+            start_x4,\n+            start_y4,\n+        ],\n+        axis=-1,\n+    )\n+    target_vector = np.expand_dims(target_vector, axis=-1)\n+\n+    homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n+    homography_matrix = np.reshape(homography_matrix, [-1, 8])\n+\n+    return homography_matrix\n+\n+\n MAP_COORDINATES_FILL_MODES = {\n     \"constant\",\n     \"nearest\",\n\n@@ -374,6 +374,226 @@ def affine_transform(\n     return affined\n \n \n+def perspective_transform(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+    start_points = convert_to_tensor(start_points, dtype=tf.float32)\n+    end_points = convert_to_tensor(end_points, dtype=tf.float32)\n+\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS:\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{AFFINE_TRANSFORM_INTERPOLATIONS}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+    if len(images.shape) not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    if start_points.shape.rank not in (2, 3) or start_points.shape[-2:] != (\n+        4,\n+        2,\n+    ):\n+        raise ValueError(\n+            \"Invalid start_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {start_points.shape}\"\n+        )\n+    if end_points.shape.rank not in (2, 3) or end_points.shape[-2:] != (4, 2):\n+        raise ValueError(\n+            \"Invalid end_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {end_points.shape}\"\n+        )\n+    if start_points.shape != end_points.shape:\n+        raise ValueError(\n+            \"start_points and end_points must have the same shape.\"\n+            f\" Received start_points.shape={start_points.shape}, \"\n+            f\"end_points.shape={end_points.shape}\"\n+        )\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = tf.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if len(start_points.shape) == 2:\n+        start_points = tf.expand_dims(start_points, axis=0)\n+    if len(end_points.shape) == 2:\n+        end_points = tf.expand_dims(end_points, axis=0)\n+\n+    if data_format == \"channels_first\":\n+        images = tf.transpose(images, (0, 2, 3, 1))\n+\n+    transform = compute_homography_matrix(start_points, end_points)\n+    if len(transform.shape) == 1:\n+        transform = tf.expand_dims(transform, axis=0)\n+\n+    output = tf.raw_ops.ImageProjectiveTransformV3(\n+        images=images,\n+        transforms=tf.cast(transform, dtype=tf.float32),\n+        output_shape=tf.shape(images)[1:-1],\n+        fill_value=fill_value,\n+        interpolation=interpolation.upper(),\n+    )\n+    output = tf.ensure_shape(output, images.shape)\n+\n+    if data_format == \"channels_first\":\n+        output = tf.transpose(output, (0, 3, 1, 2))\n+    if need_squeeze:\n+        output = tf.squeeze(output, axis=0)\n+    return output\n+\n+\n+def compute_homography_matrix(start_points, end_points):\n+    start_x1, start_y1 = start_points[:, 0, 0], start_points[:, 0, 1]\n+    start_x2, start_y2 = start_points[:, 1, 0], start_points[:, 1, 1]\n+    start_x3, start_y3 = start_points[:, 2, 0], start_points[:, 2, 1]\n+    start_x4, start_y4 = start_points[:, 3, 0], start_points[:, 3, 1]\n+\n+    end_x1, end_y1 = end_points[:, 0, 0], end_points[:, 0, 1]\n+    end_x2, end_y2 = end_points[:, 1, 0], end_points[:, 1, 1]\n+    end_x3, end_y3 = end_points[:, 2, 0], end_points[:, 2, 1]\n+    end_x4, end_y4 = end_points[:, 3, 0], end_points[:, 3, 1]\n+\n+    coefficient_matrix = tf.stack(\n+        [\n+            tf.stack(\n+                [\n+                    end_x1,\n+                    end_y1,\n+                    tf.ones_like(end_x1),\n+                    tf.zeros_like(end_x1),\n+                    tf.zeros_like(end_x1),\n+                    tf.zeros_like(end_x1),\n+                    -start_x1 * end_x1,\n+                    -start_x1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    tf.zeros_like(end_x1),\n+                    tf.zeros_like(end_x1),\n+                    tf.zeros_like(end_x1),\n+                    end_x1,\n+                    end_y1,\n+                    tf.ones_like(end_x1),\n+                    -start_y1 * end_x1,\n+                    -start_y1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    end_x2,\n+                    end_y2,\n+                    tf.ones_like(end_x2),\n+                    tf.zeros_like(end_x2),\n+                    tf.zeros_like(end_x2),\n+                    tf.zeros_like(end_x2),\n+                    -start_x2 * end_x2,\n+                    -start_x2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    tf.zeros_like(end_x2),\n+                    tf.zeros_like(end_x2),\n+                    tf.zeros_like(end_x2),\n+                    end_x2,\n+                    end_y2,\n+                    tf.ones_like(end_x2),\n+                    -start_y2 * end_x2,\n+                    -start_y2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    end_x3,\n+                    end_y3,\n+                    tf.ones_like(end_x3),\n+                    tf.zeros_like(end_x3),\n+                    tf.zeros_like(end_x3),\n+                    tf.zeros_like(end_x3),\n+                    -start_x3 * end_x3,\n+                    -start_x3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    tf.zeros_like(end_x3),\n+                    tf.zeros_like(end_x3),\n+                    tf.zeros_like(end_x3),\n+                    end_x3,\n+                    end_y3,\n+                    tf.ones_like(end_x3),\n+                    -start_y3 * end_x3,\n+                    -start_y3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    end_x4,\n+                    end_y4,\n+                    tf.ones_like(end_x4),\n+                    tf.zeros_like(end_x4),\n+                    tf.zeros_like(end_x4),\n+                    tf.zeros_like(end_x4),\n+                    -start_x4 * end_x4,\n+                    -start_x4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+            tf.stack(\n+                [\n+                    tf.zeros_like(end_x4),\n+                    tf.zeros_like(end_x4),\n+                    tf.zeros_like(end_x4),\n+                    end_x4,\n+                    end_y4,\n+                    tf.ones_like(end_x4),\n+                    -start_y4 * end_x4,\n+                    -start_y4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+        ],\n+        axis=1,\n+    )\n+\n+    target_vector = tf.stack(\n+        [\n+            start_x1,\n+            start_y1,\n+            start_x2,\n+            start_y2,\n+            start_x3,\n+            start_y3,\n+            start_x4,\n+            start_y4,\n+        ],\n+        axis=-1,\n+    )\n+    target_vector = tf.expand_dims(target_vector, axis=-1)\n+\n+    homography_matrix = tf.linalg.solve(coefficient_matrix, target_vector)\n+    homography_matrix = tf.reshape(homography_matrix, [-1, 8])\n+\n+    return homography_matrix\n+\n+\n def _mirror_index_fixer(index, size):\n     s = size - 1  # Half-wavelength of triangular wave\n     # Scaled, integer-valued version of the triangular wave |x - round(x)|\n\n@@ -444,6 +444,255 @@ def affine_transform(\n     return affined\n \n \n+def perspective_transform(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+\n+    images = convert_to_tensor(images)\n+    start_points = torch.tensor(start_points, dtype=torch.float32)\n+    end_points = torch.tensor(end_points, dtype=torch.float32)\n+\n+    if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n+        raise ValueError(\n+            \"Invalid value for argument `interpolation`. Expected of one \"\n+            f\"{set(AFFINE_TRANSFORM_INTERPOLATIONS.keys())}. Received: \"\n+            f\"interpolation={interpolation}\"\n+        )\n+\n+    if images.ndim not in (3, 4):\n+        raise ValueError(\n+            \"Invalid images rank: expected rank 3 (single image) \"\n+            \"or rank 4 (batch of images). Received input with shape: \"\n+            f\"images.shape={images.shape}\"\n+        )\n+\n+    if start_points.shape[-2:] != (4, 2) or start_points.dim() not in (2, 3):\n+        raise ValueError(\n+            \"Invalid start_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {start_points.shape}\"\n+        )\n+    if end_points.shape[-2:] != (4, 2) or end_points.dim() not in (2, 3):\n+        raise ValueError(\n+            \"Invalid end_points shape: expected (4,2) for a single image\"\n+            f\" or (N,4,2) for a batch. Received shape: {end_points.shape}\"\n+        )\n+    if start_points.shape != end_points.shape:\n+        raise ValueError(\n+            \"start_points and end_points must have the same shape.\"\n+            f\" Received start_points.shape={start_points.shape}, \"\n+            f\"end_points.shape={end_points.shape}\"\n+        )\n+\n+    need_squeeze = False\n+    if images.ndim == 3:\n+        images = images.unsqueeze(dim=0)\n+        need_squeeze = True\n+\n+    if start_points.ndim == 2:\n+        start_points = start_points.unsqueeze(dim=0)\n+    if end_points.ndim == 2:\n+        end_points = end_points.unsqueeze(dim=0)\n+\n+    if data_format == \"channels_first\":\n+        images = images.permute((0, 2, 3, 1))\n+\n+    batch_size, height, width, channels = images.shape\n+\n+    transforms = compute_homography_matrix(start_points, end_points)\n+\n+    if transforms.dim() == 1:\n+        transforms = transforms.unsqueeze(0)\n+    if transforms.shape[0] == 1 and batch_size > 1:\n+        transforms = transforms.repeat(batch_size, 1)\n+\n+    grid_x, grid_y = torch.meshgrid(\n+        torch.arange(width, dtype=torch.float32, device=images.device),\n+        torch.arange(height, dtype=torch.float32, device=images.device),\n+        indexing=\"xy\",\n+    )\n+\n+    output = torch.empty(\n+        [batch_size, height, width, channels], device=images.device\n+    )\n+\n+    for i in range(batch_size):\n+        a0, a1, a2, a3, a4, a5, a6, a7 = transforms[i]\n+        denom = a6 * grid_x + a7 * grid_y + 1.0\n+        x_in = (a0 * grid_x + a1 * grid_y + a2) / denom\n+        y_in = (a3 * grid_x + a4 * grid_y + a5) / denom\n+\n+        coords = torch.stack([y_in.flatten(), x_in.flatten()], dim=0)\n+        mapped_channels = []\n+        for channel in range(channels):\n+            channel_img = images[i, :, :, channel]\n+            mapped_channel = map_coordinates(\n+                channel_img,\n+                coords,\n+                order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                fill_mode=\"constant\",\n+                fill_value=fill_value,\n+            )\n+            mapped_channels.append(mapped_channel.reshape(height, width))\n+        output[i] = torch.stack(mapped_channels, dim=-1)\n+\n+    if data_format == \"channels_first\":\n+        output = output.permute((0, 3, 1, 2))\n+    if need_squeeze:\n+        output = output.squeeze(dim=0)\n+\n+    return output\n+\n+\n+def compute_homography_matrix(start_points, end_points):\n+    start_points = convert_to_tensor(start_points, dtype=torch.float32)\n+    end_points = convert_to_tensor(end_points, dtype=torch.float32)\n+\n+    start_x1, start_y1 = start_points[:, 0, 0], start_points[:, 0, 1]\n+    start_x2, start_y2 = start_points[:, 1, 0], start_points[:, 1, 1]\n+    start_x3, start_y3 = start_points[:, 2, 0], start_points[:, 2, 1]\n+    start_x4, start_y4 = start_points[:, 3, 0], start_points[:, 3, 1]\n+\n+    end_x1, end_y1 = end_points[:, 0, 0], end_points[:, 0, 1]\n+    end_x2, end_y2 = end_points[:, 1, 0], end_points[:, 1, 1]\n+    end_x3, end_y3 = end_points[:, 2, 0], end_points[:, 2, 1]\n+    end_x4, end_y4 = end_points[:, 3, 0], end_points[:, 3, 1]\n+\n+    coefficient_matrix = torch.stack(\n+        [\n+            torch.stack(\n+                [\n+                    end_x1,\n+                    end_y1,\n+                    torch.ones_like(end_x1),\n+                    torch.zeros_like(end_x1),\n+                    torch.zeros_like(end_x1),\n+                    torch.zeros_like(end_x1),\n+                    -start_x1 * end_x1,\n+                    -start_x1 * end_y1,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    torch.zeros_like(end_x1),\n+                    torch.zeros_like(end_x1),\n+                    torch.zeros_like(end_x1),\n+                    end_x1,\n+                    end_y1,\n+                    torch.ones_like(end_x1),\n+                    -start_y1 * end_x1,\n+                    -start_y1 * end_y1,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    end_x2,\n+                    end_y2,\n+                    torch.ones_like(end_x2),\n+                    torch.zeros_like(end_x2),\n+                    torch.zeros_like(end_x2),\n+                    torch.zeros_like(end_x2),\n+                    -start_x2 * end_x2,\n+                    -start_x2 * end_y2,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    torch.zeros_like(end_x2),\n+                    torch.zeros_like(end_x2),\n+                    torch.zeros_like(end_x2),\n+                    end_x2,\n+                    end_y2,\n+                    torch.ones_like(end_x2),\n+                    -start_y2 * end_x2,\n+                    -start_y2 * end_y2,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    end_x3,\n+                    end_y3,\n+                    torch.ones_like(end_x3),\n+                    torch.zeros_like(end_x3),\n+                    torch.zeros_like(end_x3),\n+                    torch.zeros_like(end_x3),\n+                    -start_x3 * end_x3,\n+                    -start_x3 * end_y3,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    torch.zeros_like(end_x3),\n+                    torch.zeros_like(end_x3),\n+                    torch.zeros_like(end_x3),\n+                    end_x3,\n+                    end_y3,\n+                    torch.ones_like(end_x3),\n+                    -start_y3 * end_x3,\n+                    -start_y3 * end_y3,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    end_x4,\n+                    end_y4,\n+                    torch.ones_like(end_x4),\n+                    torch.zeros_like(end_x4),\n+                    torch.zeros_like(end_x4),\n+                    torch.zeros_like(end_x4),\n+                    -start_x4 * end_x4,\n+                    -start_x4 * end_y4,\n+                ],\n+                dim=-1,\n+            ),\n+            torch.stack(\n+                [\n+                    torch.zeros_like(end_x4),\n+                    torch.zeros_like(end_x4),\n+                    torch.zeros_like(end_x4),\n+                    end_x4,\n+                    end_y4,\n+                    torch.ones_like(end_x4),\n+                    -start_y4 * end_x4,\n+                    -start_y4 * end_y4,\n+                ],\n+                dim=-1,\n+            ),\n+        ],\n+        dim=1,\n+    )\n+\n+    target_vector = torch.stack(\n+        [\n+            start_x1,\n+            start_y1,\n+            start_x2,\n+            start_y2,\n+            start_x3,\n+            start_y3,\n+            start_x4,\n+            start_y4,\n+        ],\n+        dim=-1,\n+    ).unsqueeze(-1)\n+\n+    homography_matrix = torch.linalg.solve(coefficient_matrix, target_vector)\n+    homography_matrix = homography_matrix.reshape(-1, 8)\n+\n+    return homography_matrix\n+\n+\n def _mirror_index_fixer(index, size):\n     s = size - 1  # Half-wavelength of triangular wave\n     # Scaled, integer-valued version of the triangular wave |x - round(x)|\n\n@@ -1230,3 +1230,146 @@ def _crop_images(\n \n     cropped_images = ops.slice(images, start_indices, shape)\n     return cropped_images\n+\n+\n+class PerspectiveTransform(Operation):\n+    def __init__(\n+        self,\n+        interpolation=\"bilinear\",\n+        fill_value=0,\n+        data_format=None,\n+    ):\n+        super().__init__()\n+        self.interpolation = interpolation\n+        self.fill_value = fill_value\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+    def call(self, images, start_points, end_points):\n+        return backend.image.perspective_transform(\n+            images,\n+            start_points,\n+            end_points,\n+            interpolation=self.interpolation,\n+            fill_value=self.fill_value,\n+            data_format=self.data_format,\n+        )\n+\n+    def compute_output_spec(self, images, start_points, end_points):\n+        if len(images.shape) not in (3, 4):\n+            raise ValueError(\n+                \"Invalid images rank: expected rank 3 (single image) \"\n+                \"or rank 4 (batch of images). Received input with shape: \"\n+                f\"images.shape={images.shape}\"\n+            )\n+        if start_points.shape[-2:] != (4, 2) or start_points.ndim not in (2, 3):\n+            raise ValueError(\n+                \"Invalid start_points shape: expected (4,2) for a single image\"\n+                f\" or (N,4,2) for a batch. Received shape: {start_points.shape}\"\n+            )\n+        if end_points.shape[-2:] != (4, 2) or end_points.ndim not in (2, 3):\n+            raise ValueError(\n+                \"Invalid end_points shape: expected (4,2) for a single image\"\n+                f\" or (N,4,2) for a batch. Received shape: {end_points.shape}\"\n+            )\n+        if start_points.shape != end_points.shape:\n+            raise ValueError(\n+                \"start_points and end_points must have the same shape.\"\n+                f\" Received start_points.shape={start_points.shape}, \"\n+                f\"end_points.shape={end_points.shape}\"\n+            )\n+        return KerasTensor(images.shape, dtype=images.dtype)\n+\n+\n+@keras_export(\"keras.ops.image.perspective_transform\")\n+def perspective_transform(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    \"\"\"Applies a perspective transformation to the image(s).\n+\n+    Args:\n+        images: Input image or batch of images. Must be 3D or 4D.\n+        start_points: A tensor of shape `(N, 4, 2)` or `(4, 2)`,\n+            representing the source points in the original image\n+            that define the transformation.\n+        end_points: A tensor of shape `(N, 4, 2)` or `(4, 2)`,\n+            representing the target points in the output image\n+            after transformation.\n+        interpolation: Interpolation method. Available methods are `\"nearest\"`,\n+            and `\"bilinear\"`. Defaults to `\"bilinear\"`.\n+        fill_value: Value used for points outside the boundaries of the input if\n+            extrapolation is needed. Defaults to `0`.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will default to\n+            `keras.config.image_data_format`.\n+\n+    Returns:\n+        Applied perspective transform image or batch of images.\n+\n+    Examples:\n+\n+    >>> x = np.random.random((2, 64, 80, 3))  # batch of 2 RGB images\n+    >>> start_points = np.array(\n+    ...     [\n+    ...         [[0, 0], [0, 64], [80, 0], [80, 64]],\n+    ...         [[0, 0], [0, 64], [80, 0], [80, 64]],\n+    ...     ]\n+    ... )\n+    >>> end_points = np.array(\n+    ...     [\n+    ...         [[3, 5], [7, 64], [76, -10], [84, 61]],\n+    ...         [[8, 10], [10, 61], [65, 3], [88, 43]],\n+    ...     ]\n+    ... )\n+    >>> y = keras.ops.image.perspective_transform(x, start_points, end_points)\n+    >>> y.shape\n+    (2, 64, 80, 3)\n+\n+    >>> x = np.random.random((64, 80, 3))  # single RGB image\n+    >>> start_points = np.array([[0, 0], [0, 64], [80, 0], [80, 64]])\n+    >>> end_points = np.array([[3, 5], [7, 64], [76, -10], [84, 61]])\n+    >>> y = keras.ops.image.perspective_transform(x, start_points, end_points)\n+    >>> y.shape\n+    (64, 80, 3)\n+\n+    >>> x = np.random.random((2, 3, 64, 80))  # batch of 2 RGB images\n+    >>> start_points = np.array(\n+    ...     [\n+    ...         [[0, 0], [0, 64], [80, 0], [80, 64]],\n+    ...         [[0, 0], [0, 64], [80, 0], [80, 64]],\n+    ...     ]\n+    ... )\n+    >>> end_points = np.array(\n+    ...     [\n+    ...         [[3, 5], [7, 64], [76, -10], [84, 61]],\n+    ...         [[8, 10], [10, 61], [65, 3], [88, 43]],\n+    ...     ]\n+    ... )\n+    >>> y = keras.ops.image.perspective_transform(\n+    ...     x, start_points, end_points, data_format=\"channels_first\"\n+    ... )\n+    >>> y.shape\n+    (2, 3, 64, 80)\n+    \"\"\"\n+    if any_symbolic_tensors((images, start_points, end_points)):\n+        return PerspectiveTransform(\n+            interpolation=interpolation,\n+            fill_value=fill_value,\n+            data_format=data_format,\n+        ).symbolic_call(images, start_points, end_points)\n+    return backend.image.perspective_transform(\n+        images,\n+        start_points,\n+        end_points,\n+        interpolation=interpolation,\n+        fill_value=fill_value,\n+        data_format=data_format,\n+    )\n\n@@ -163,6 +163,22 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (3, 10, 20))\n \n+    def test_perspective_transform(self):\n+        # Test channels_last\n+        x = KerasTensor([None, 20, 20, 3])\n+        start_points = KerasTensor([None, 4, 2])\n+        end_points = KerasTensor([None, 4, 2])\n+        out = kimage.perspective_transform(x, start_points, end_points)\n+        self.assertEqual(out.shape, (None, 20, 20, 3))\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([None, 3, 20, 20])\n+        start_points = KerasTensor([None, 4, 2])\n+        end_points = KerasTensor([None, 4, 2])\n+        out = kimage.perspective_transform(x, start_points, end_points)\n+        self.assertEqual(out.shape, (None, 3, 20, 20))\n+\n \n class ImageOpsStaticShapeTest(testing.TestCase):\n     def setUp(self):\n@@ -362,6 +378,22 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(out_batch.shape, (2, 3, 10, 20))\n \n+    def test_perspective_transform(self):\n+        # Test channels_last\n+        x = KerasTensor([20, 20, 3])\n+        start_points = KerasTensor([4, 2])\n+        end_points = KerasTensor([4, 2])\n+        out = kimage.perspective_transform(x, start_points, end_points)\n+        self.assertEqual(out.shape, (20, 20, 3))\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = KerasTensor([3, 20, 20])\n+        start_points = KerasTensor([4, 2])\n+        end_points = KerasTensor([4, 2])\n+        out = kimage.perspective_transform(x, start_points, end_points)\n+        self.assertEqual(out.shape, (3, 20, 20))\n+\n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n     \"nearest\": 0,\n@@ -441,6 +473,219 @@ def _fixed_map_coordinates(\n     return result\n \n \n+def _perspective_transform_numpy(\n+    images,\n+    start_points,\n+    end_points,\n+    interpolation=\"bilinear\",\n+    fill_value=0,\n+    data_format=None,\n+):\n+    data_format = backend.standardize_data_format(data_format)\n+\n+    need_squeeze = False\n+    if len(images.shape) == 3:\n+        images = np.expand_dims(images, axis=0)\n+        need_squeeze = True\n+\n+    if len(start_points.shape) == 2:\n+        start_points = np.expand_dims(start_points, axis=0)\n+    if len(end_points.shape) == 2:\n+        end_points = np.expand_dims(end_points, axis=0)\n+\n+    if data_format == \"channels_first\":\n+        images = np.transpose(images, (0, 2, 3, 1))\n+\n+    batch_size, height, width, channels = images.shape\n+\n+    transforms = _compute_homography_matrix(start_points, end_points)\n+\n+    if len(transforms.shape) == 1:\n+        transforms = np.expand_dims(transforms, axis=0)\n+    if transforms.shape[0] == 1 and batch_size > 1:\n+        transforms = np.tile(transforms, (batch_size, 1))\n+\n+    x, y = np.meshgrid(\n+        np.arange(width, dtype=np.float32),\n+        np.arange(height, dtype=np.float32),\n+        indexing=\"xy\",\n+    )\n+\n+    output = np.empty((batch_size, height, width, channels))\n+\n+    for i in range(batch_size):\n+        a0, a1, a2, a3, a4, a5, a6, a7 = transforms[i]\n+        denom = a6 * x + a7 * y + 1.0\n+        x_in = (a0 * x + a1 * y + a2) / denom\n+        y_in = (a3 * x + a4 * y + a5) / denom\n+\n+        coords = np.stack([y_in.ravel(), x_in.ravel()], axis=0)\n+\n+        mapped_channels = []\n+        for channel in range(channels):\n+            channel_img = images[i, :, :, channel]\n+\n+            mapped_channel = _fixed_map_coordinates(\n+                channel_img,\n+                coords,\n+                order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                fill_mode=\"constant\",\n+                fill_value=fill_value,\n+            )\n+            mapped_channels.append(mapped_channel.reshape(height, width))\n+\n+        output[i] = np.stack(mapped_channels, axis=-1)\n+\n+    if data_format == \"channels_first\":\n+        output = np.transpose(output, (0, 3, 1, 2))\n+    if need_squeeze:\n+        output = np.squeeze(output, axis=0)\n+\n+    return output\n+\n+\n+def _compute_homography_matrix(start_points, end_points):\n+    start_x1, start_y1 = start_points[:, 0, 0], start_points[:, 0, 1]\n+    start_x2, start_y2 = start_points[:, 1, 0], start_points[:, 1, 1]\n+    start_x3, start_y3 = start_points[:, 2, 0], start_points[:, 2, 1]\n+    start_x4, start_y4 = start_points[:, 3, 0], start_points[:, 3, 1]\n+\n+    end_x1, end_y1 = end_points[:, 0, 0], end_points[:, 0, 1]\n+    end_x2, end_y2 = end_points[:, 1, 0], end_points[:, 1, 1]\n+    end_x3, end_y3 = end_points[:, 2, 0], end_points[:, 2, 1]\n+    end_x4, end_y4 = end_points[:, 3, 0], end_points[:, 3, 1]\n+\n+    coefficient_matrix = np.stack(\n+        [\n+            np.stack(\n+                [\n+                    end_x1,\n+                    end_y1,\n+                    np.ones_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    -start_x1 * end_x1,\n+                    -start_x1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    np.zeros_like(end_x1),\n+                    end_x1,\n+                    end_y1,\n+                    np.ones_like(end_x1),\n+                    -start_y1 * end_x1,\n+                    -start_y1 * end_y1,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x2,\n+                    end_y2,\n+                    np.ones_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    -start_x2 * end_x2,\n+                    -start_x2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    np.zeros_like(end_x2),\n+                    end_x2,\n+                    end_y2,\n+                    np.ones_like(end_x2),\n+                    -start_y2 * end_x2,\n+                    -start_y2 * end_y2,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x3,\n+                    end_y3,\n+                    np.ones_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    -start_x3 * end_x3,\n+                    -start_x3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    np.zeros_like(end_x3),\n+                    end_x3,\n+                    end_y3,\n+                    np.ones_like(end_x3),\n+                    -start_y3 * end_x3,\n+                    -start_y3 * end_y3,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    end_x4,\n+                    end_y4,\n+                    np.ones_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    -start_x4 * end_x4,\n+                    -start_x4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+            np.stack(\n+                [\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    np.zeros_like(end_x4),\n+                    end_x4,\n+                    end_y4,\n+                    np.ones_like(end_x4),\n+                    -start_y4 * end_x4,\n+                    -start_y4 * end_y4,\n+                ],\n+                axis=-1,\n+            ),\n+        ],\n+        axis=1,\n+    )\n+\n+    target_vector = np.stack(\n+        [\n+            start_x1,\n+            start_y1,\n+            start_x2,\n+            start_y2,\n+            start_x3,\n+            start_y3,\n+            start_x4,\n+            start_y4,\n+        ],\n+        axis=-1,\n+    )\n+    target_vector = np.expand_dims(target_vector, axis=-1)\n+\n+    homography_matrix = np.linalg.solve(coefficient_matrix, target_vector)\n+    homography_matrix = np.reshape(homography_matrix, [-1, 8])\n+\n+    return homography_matrix\n+\n+\n class ImageOpsCorrectnessTest(testing.TestCase):\n     def setUp(self):\n         # Defaults to channels_last\n@@ -1213,6 +1458,50 @@ class ImageOpsCorrectnessTest(testing.TestCase):\n         )(image)\n         self.assertAllClose(ref_cropped_image, cropped_image)\n \n+    @parameterized.named_parameters(\n+        named_product(\n+            interpolation=[\"bilinear\", \"nearest\"],\n+        )\n+    )\n+    def test_perspective_transform(self, interpolation):\n+        # Test channels_last\n+        np.random.seed(42)\n+        x = np.random.uniform(size=(50, 50, 3)).astype(\"float32\")\n+        start_points = np.random.uniform(size=(1, 4, 2)).astype(\"float32\")\n+        end_points = np.random.uniform(size=(1, 4, 2)).astype(\"float32\")\n+\n+        out = kimage.perspective_transform(\n+            x, start_points, end_points, interpolation=interpolation\n+        )\n+\n+        ref_out = _perspective_transform_numpy(\n+            x, start_points, end_points, interpolation=interpolation\n+        )\n+\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=1e-2, rtol=1e-2)\n+\n+        # Test channels_first\n+        backend.set_image_data_format(\"channels_first\")\n+        x = np.random.uniform(size=(3, 50, 50)).astype(\"float32\")\n+        start_points = np.random.uniform(size=(1, 4, 2)).astype(\"float32\")\n+        end_points = np.random.uniform(size=(1, 4, 2)).astype(\"float32\")\n+\n+        out = kimage.perspective_transform(\n+            x, start_points, end_points, interpolation=interpolation\n+        )\n+\n+        ref_out = _perspective_transform_numpy(\n+            x,\n+            start_points,\n+            end_points,\n+            interpolation=interpolation,\n+            data_format=\"channels_first\",\n+        )\n+\n+        self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n+        self.assertAllClose(ref_out, out, atol=1e-2, rtol=1e-2)\n+\n \n class ImageOpsBehaviorTests(testing.TestCase):\n     def setUp(self):\n@@ -1446,3 +1735,83 @@ class ImageOpsBehaviorTests(testing.TestCase):\n             ValueError, \"When the width of the images is unknown\"\n         ):\n             kimage.crop_images(x, 2, 3, 4, 5)\n+\n+    def test_perspective_transform_invalid_images_rank(self):\n+        # Test rank=2\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        start_points = np.random.uniform(size=(6,))\n+        end_points = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.perspective_transform(\n+                invalid_image, start_points, end_points\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.PerspectiveTransform()(\n+                invalid_image, start_points, end_points\n+            )\n+\n+        # Test rank=5\n+        invalid_image = np.random.uniform(size=(2, 10, 10, 3, 1))\n+        start_points = np.random.uniform(size=(6,))\n+        end_points = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.perspective_transform(\n+                invalid_image, start_points, end_points\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.PerspectiveTransform()(\n+                invalid_image, start_points, end_points\n+            )\n+\n+        # Test rank=2, symbolic tensor\n+        invalid_image = KerasTensor(shape=(10, 10))\n+        start_points = KerasTensor(shape=(6,))\n+        end_points = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid images rank: expected rank 3\"\n+        ):\n+            kimage.perspective_transform(\n+                invalid_image, start_points, end_points\n+            )\n+\n+    def test_perspective_transform_invalid_points_rank(self):\n+        # Test rank=3\n+        images = np.random.uniform(size=(10, 10, 3))\n+        start_points = np.random.uniform(size=(2, 2, 4, 2))\n+        end_points = np.random.uniform(size=(2, 2, 4, 2))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid start_points shape: expected\"\n+        ):\n+            kimage.perspective_transform(images, start_points, end_points)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid start_points shape: expected\"\n+        ):\n+            kimage.PerspectiveTransform()(images, start_points, end_points)\n+\n+        # Test rank=0\n+        start_points = np.random.uniform(size=())\n+        end_points = np.random.uniform(size=())\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid start_points shape: expected\"\n+        ):\n+            kimage.perspective_transform(images, start_points, end_points)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid start_points shape: expected\"\n+        ):\n+            kimage.PerspectiveTransform()(images, start_points, end_points)\n+\n+        # Test rank=3, symbolic tensor\n+        images = KerasTensor(shape=(10, 10, 3))\n+        start_points = KerasTensor(shape=(2, 3, 2))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid start_points shape: expected\"\n+        ):\n+            kimage.perspective_transform(images, start_points, end_points)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
