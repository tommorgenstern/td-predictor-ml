{"custom_id": "scrapy#0350bed8fd29bf75a610182e6083a90977c21480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 409 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -152,9 +152,14 @@ class Command(ScrapyCommand):\n                 else:\n                     cb = 'parse'\n \n-            cb = cb if callable(cb) else getattr(self.spider, cb, None)\n-            if not cb:\n-                log.msg('Cannot find callback %r in spider: %s' % (callback, self.spider.name))\n+            if not callable(cb):\n+                cb_method = getattr(self.spider, cb, None)\n+                if callable(cb_method):\n+                    cb = cb_method\n+                else:\n+                    log.msg('Cannot find callback %r in spider: %s' % \\\n+                            (cb, self.spider.name), level=log.ERROR)\n+                    return\n \n             # parse items and requests\n             depth = response.meta['_depth']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be206ca5abf73b06a9639e34068f67c67c7f0d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 10 | Churn Cumulative: 2038 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -213,13 +213,14 @@ class ExecutionEngine(object):\n         return dwld\n \n     @defer.inlineCallbacks\n-    def open_spider(self, spider, start_requests=None, close_if_idle=True):\n+    def open_spider(self, spider, start_requests=(), close_if_idle=True):\n         assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n-        slot = Slot(start_requests or (), close_if_idle, nextcall, scheduler)\n+        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n+        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.slots[spider] = slot\n         yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n\n@@ -29,6 +29,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             self.methods['process_spider_output'].insert(0, mw.process_spider_output)\n         if hasattr(mw, 'process_spider_exception'):\n             self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)\n+        if hasattr(mw, 'process_start_requests'):\n+            self.methods['process_start_requests'].insert(0, mw.process_start_requests)\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n@@ -68,3 +70,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         dfd.addErrback(process_spider_exception)\n         dfd.addCallback(process_spider_output)\n         return dfd\n+\n+    def process_start_requests(self, start_requests, spider):\n+        return self._process_chain('process_start_requests', start_requests, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcef7b03c17bd395defddab27fa66778e4051c18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 196 | Lines Deleted: 105 | Files Changed: 28 | Hunks: 77 | Methods Changed: 55 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 301 | Churn Cumulative: 7595 | Contributors (this commit): 15 | Commits (past 90d): 43 | Contributors (cumulative): 84 | DMM Complexity: 0.8028169014084507\n\nDIFF:\n@@ -113,19 +113,22 @@ class Command(ScrapyCommand):\n                 if rule.link_extractor.matches(response.url) and rule.callback:\n                     return rule.callback\n         else:\n-            log.msg(\"No CrawlSpider rules found in spider %r, please specify \"\n-                \"a callback to use for parsing\" % self.spider.name, log.ERROR)\n+            log.msg(format='No CrawlSpider rules found in spider %(spider)r, '\n+                           'please specify a callback to use for parsing',\n+                    level=log.ERROR, spider=self.spider.name)\n \n     def set_spider(self, url, opts):\n         if opts.spider:\n             try:\n                 self.spider = self.crawler.spiders.create(opts.spider)\n             except KeyError:\n-                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n+                log.msg(format='Unable to find spider: %(spider)s',\n+                        level=log.ERROR, spider=opts.spider)\n         else:\n             self.spider = create_spider_for_request(self.crawler.spiders, url)\n             if not self.spider:\n-                log.msg('Unable to find spider for: %s' % request, log.ERROR)\n+                log.msg(format='Unable to find spider for: %(url)s',\n+                        level=log.ERROR, url=url)\n \n     def start_parsing(self, url, opts):\n         request = Request(url, opts.callback)\n@@ -135,8 +138,8 @@ class Command(ScrapyCommand):\n         self.crawler.start()\n \n         if not self.first_response:\n-            log.msg('No response downloaded for: %s' % request, log.ERROR, \\\n-                spider=self.spider)\n+            log.msg(format='No response downloaded for: %(request)s',\n+                    level=log.ERROR, request=request)\n \n     def prepare_request(self, request, opts):\n         def callback(response):\n@@ -157,8 +160,8 @@ class Command(ScrapyCommand):\n                 if callable(cb_method):\n                     cb = cb_method\n                 else:\n-                    log.msg('Cannot find callback %r in spider: %s' % \\\n-                            (cb, self.spider.name), level=log.ERROR)\n+                    log.msg(format='Cannot find callback %(callback)r in spider: %(spider)s',\n+                            callback=callback, spider=self.spider.name, level=log.ERROR)\n                     return\n \n             # parse items and requests\n\n@@ -60,12 +60,13 @@ class RedirectMiddleware(object):\n                 [request.url]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n-            log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request),\n-                    spider=spider, level=log.DEBUG)\n+            log.msg(format=\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n+                    level=log.DEBUG, spider=spider, request=request,\n+                    redirected=redirected, reason=reason)\n             return redirected\n         else:\n-            log.msg(\"Discarding %s: max redirections reached\" % request,\n-                    spider=spider, level=log.DEBUG)\n+            log.msg(format=\"Discarding %(request)s: max redirections reached\",\n+                    level=log.DEBUG, spider=spider, request=request)\n             raise IgnoreRequest\n \n     def _redirect_request_using_get(self, request, redirect_url):\n\n@@ -64,14 +64,13 @@ class RetryMiddleware(object):\n         retries = request.meta.get('retry_times', 0) + 1\n \n         if retries <= self.max_retry_times:\n-            log.msg(\"Retrying %s (failed %d times): %s\" % (request, retries, reason),\n-                    spider=spider, level=log.DEBUG)\n+            log.msg(format=\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n+                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)\n             retryreq = request.copy()\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n             retryreq.priority = request.priority + self.priority_adjust\n             return retryreq\n         else:\n-            log.msg(\"Gave up retrying %s (failed %d times): %s\" % (request, retries, reason),\n-                    spider=spider, level=log.DEBUG)\n-\n+            log.msg(format=\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n+                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)\n\n@@ -33,7 +33,8 @@ class RobotsTxtMiddleware(object):\n         useragent = self._useragents[spider]\n         rp = self.robot_parser(request, spider)\n         if rp and not rp.can_fetch(useragent, request.url):\n-            log.msg(\"Forbidden by robots.txt: %s\" % request, log.DEBUG)\n+            log.msg(format=\"Forbidden by robots.txt: %(request)s\",\n+                    level=log.DEBUG, request=request)\n             raise IgnoreRequest\n \n     def robot_parser(self, request, spider):\n\n@@ -67,12 +67,14 @@ class MemoryUsage(object):\n         if self.get_virtual_size() > self.limit:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit/1024/1024\n-            log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n+            log.msg(format=\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n+                    level=log.ERROR, memusage=mem)\n             if self.notify_mails:\n                 subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/limit_notified', 1)\n+\n             open_spiders = self.crawler.engine.open_spiders\n             if open_spiders:\n                 for spider in open_spiders:\n@@ -86,7 +88,8 @@ class MemoryUsage(object):\n         if self.get_virtual_size() > self.warning:\n             self.crawler.stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning/1024/1024\n-            log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n+            log.msg(format=\"Memory usage reached %(memusage)dM\",\n+                    level=log.WARNING, memusage=mem)\n             if self.notify_mails:\n                 subj = \"%s warning: memory usage reached %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n\n@@ -177,26 +177,29 @@ class ImagesPipeline(MediaPipeline):\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            log.msg('Image (code: %s): Error downloading image from %s referred in <%s>' \\\n-                    % (response.status, request, referer), level=log.WARNING, spider=info.spider)\n+            log.msg(format='Image (code: %(status)s): Error downloading image from %(request)s referred in <%(referer)s>',\n+                    level=log.WARNING, spider=info.spider,\n+                    status=response.status, request=request, referer=referer)\n             raise ImageException\n \n         if not response.body:\n-            log.msg('Image (empty-content): Empty image from %s referred in <%s>: no-content' \\\n-                    % (request, referer), level=log.WARNING, spider=info.spider)\n+            log.msg(format='Image (empty-content): Empty image from %(request)s referred in <%(referer)s>: no-content',\n+                    level=log.WARNING, spider=info.spider,\n+                    request=request, referer=referer)\n             raise ImageException\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n-        msg = 'Image (%s): Downloaded image from %s referred in <%s>' % \\\n-                (status, request, referer)\n-        log.msg(msg, level=log.DEBUG, spider=info.spider)\n+        log.msg(format='Image (%(status)s): Downloaded image from %(request)s referred in <%(referer)s>',\n+                level=log.DEBUG, spider=info.spider,\n+                status=status, request=request, referer=referer)\n         self.inc_stats(info.spider, status)\n \n         try:\n             key = self.image_key(request.url)\n             checksum = self.image_downloaded(response, request, info)\n         except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, spider=info.spider)\n+            log.err('image_downloaded hook failed',\n+                    level=log.WARNING, spider=info.spider)\n             raise\n         except Exception:\n             log.err(spider=info.spider)\n@@ -207,9 +210,12 @@ class ImagesPipeline(MediaPipeline):\n     def media_failed(self, failure, request, info):\n         if not isinstance(failure.value, IgnoreRequest):\n             referer = request.headers.get('Referer')\n-            msg = 'Image (unknown-error): Error downloading %s from %s referred in <%s>: %s' \\\n-                    % (self.MEDIA_NAME, request, referer, str(failure))\n-            log.msg(msg, level=log.WARNING, spider=info.spider)\n+            log.msg(format='Image (unknown-error): Error downloading '\n+                           '%(medianame)s from %(request)s referred in '\n+                           '<%(referer)s>: %(exception)s',\n+                    level=log.WARNING, spider=info.spider, exception=failure.value,\n+                    medianame=self.MEDIA_NAME, request=request, referer=referer)\n+\n         raise ImageException\n \n     def media_to_download(self, request, info):\n@@ -227,8 +233,9 @@ class ImagesPipeline(MediaPipeline):\n                 return # returning None force download\n \n             referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate): Downloaded %s from <%s> referred in <%s>' % \\\n-                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, spider=info.spider)\n+            log.msg(format='Image (uptodate): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>',\n+                    level=log.DEBUG, spider=info.spider,\n+                    medianame=self.MEDIA_NAME, request=request, referer=referer)\n             self.inc_stats(info.spider, 'uptodate')\n \n             checksum = result.get('checksum', None)\n\n@@ -31,8 +31,9 @@ class DepthMiddleware(object):\n                 if self.prio:\n                     request.priority -= depth * self.prio\n                 if self.maxdepth and depth > self.maxdepth:\n-                    log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), \\\n-                        level=log.DEBUG, spider=spider)\n+                    log.msg(format=\"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n+                            level=log.DEBUG, spider=spider,\n+                            maxdepth=self.maxdepth, requrl=request.url)\n                     return False\n                 elif self.stats:\n                     if self.verbose_stats:\n\n@@ -32,9 +32,9 @@ class OffsiteMiddleware(object):\n                 else:\n                     domain = urlparse_cached(x).hostname\n                     if domain and domain not in self.domains_seen[spider]:\n-                        log.msg(\"Filtered offsite request to %r: %s\" % (domain, x),\n-                            level=log.DEBUG, spider=spider)\n                         self.domains_seen[spider].add(domain)\n+                        log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n+                                level=log.DEBUG, spider=spider, domain=domain, request=x)\n             else:\n                 yield x\n \n\n@@ -23,8 +23,9 @@ class UrlLengthMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n-                log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), \\\n-                    level=log.DEBUG, spider=spider)\n+                log.msg(format=\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n+                        level=log.DEBUG, spider=spider,\n+                        maxlength=self.maxlength, url=request.url)\n                 return False\n             else:\n                 return True\n\n@@ -31,7 +31,8 @@ class SitemapSpider(BaseSpider):\n         else:\n             body = self._get_sitemap_body(response)\n             if body is None:\n-                log.msg(\"Ignoring invalid sitemap: %s\" % response, log.WARNING)\n+                log.msg(format=\"Ignoring invalid sitemap: %(response)s\",\n+                        level=log.WARNING, spider=self, response=response)\n                 return\n \n             s = Sitemap(body)\n\n@@ -75,7 +75,7 @@ class DecompressionMiddleware(object):\n         for fmt, func in self._formats.iteritems():\n             new_response = func(response)\n             if new_response:\n-                log.msg('Decompressed response with format: %s' % \\\n-                        fmt, log.DEBUG, spider=spider)\n+                log.msg(format='Decompressed response with format: %(responsefmt)s',\n+                        level=log.DEBUG, spider=spider, responsefmt=fmt)\n                 return new_response\n         return response\n\n@@ -193,8 +193,8 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(log.formatter.crawled(request, response, spider), \\\n-                    level=log.DEBUG, spider=spider)\n+                logkws = log.formatter.crawled(request, response, spider)\n+                log.msg(level=log.DEBUG, spider=spider, **logkws)\n                 self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n             return response\n@@ -252,7 +252,8 @@ class ExecutionEngine(object):\n         slot = self.slots[spider]\n         if slot.closing:\n             return slot.closing\n-        log.msg(\"Closing spider (%s)\" % reason, spider=spider)\n+        log.msg(format=\"Closing spider (%(reason)s)\", reason=reason, spider=spider)\n+        log.msg(format='hohohoohoo %(aaa)s', spider=spider, aaa='12')\n \n         dfd = slot.close()\n \n@@ -269,7 +270,7 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log.err, spider=spider)\n \n-        dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n+        dfd.addBoth(lambda _: log.msg(format=\"Spider closed (%(reason)s)\", reason=reason, spider=spider))\n \n         dfd.addBoth(lambda _: self.slots.pop(spider))\n         dfd.addErrback(log.err, spider=spider)\n\n@@ -64,8 +64,9 @@ class Scheduler(object):\n             self.dqs.push(reqd, -request.priority)\n         except ValueError, e: # non serializable request\n             if self.logunser:\n-                log.msg(\"Unable to serialize request: %s - reason: %s\" % \\\n-                    (request, str(e)), level=log.ERROR, spider=self.spider)\n+                log.msg(format=\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n+                        level=log.ERROR, spider=self.spider,\n+                        request=request, reason=e)\n             return\n         else:\n             if self.stats:\n@@ -98,8 +99,8 @@ class Scheduler(object):\n             prios = ()\n         q = PriorityQueue(self._newdq, startprios=prios)\n         if q:\n-            log.msg(\"Resuming crawl (%d requests scheduled)\" % len(q), \\\n-                spider=self.spider)\n+            log.msg(format=\"Resuming crawl (%(queuesize)d requests scheduled)\",\n+                    spider=self.spider, queuesize=len(q))\n         return q\n \n     def _dqdir(self, jobdir):\n\n@@ -174,8 +174,10 @@ class Scraper(object):\n         elif output is None:\n             pass\n         else:\n-            log.msg(\"Spider must return Request, BaseItem or None, got %r in %s\" % \\\n-                (type(output).__name__, request), log.ERROR, spider=spider)\n+            typename = type(output).__name__\n+            log.msg(format='Spider must return Request, BaseItem or None, '\n+                           'got %(typename)r in %(request)s',\n+                    level=log.ERROR, spider=spider, request=request, typename=typename)\n \n     def _log_download_errors(self, spider_failure, download_failure, request, spider):\n         \"\"\"Log and silence errors that come from the engine (typically download\n@@ -185,7 +187,8 @@ class Scraper(object):\n             errmsg = spider_failure.getErrorMessage()\n             spider_failure.printTraceback()\n             if errmsg:\n-                log.msg(\"Error downloading %s: %s\" % (request, errmsg), log.ERROR, spider=spider)\n+                log.msg(format='Error downloading %(request)s: %(errmsg)s',\n+                        level=log.ERROR, spider=spider, request=request, errmsg=errmsg)\n             return\n         return spider_failure\n \n@@ -196,15 +199,15 @@ class Scraper(object):\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                log.msg(log.formatter.dropped(item, ex, response, spider), \\\n-                    level=log.WARNING, spider=spider)\n+                logkws = log.formatter.dropped(item, ex, response, spider)\n+                log.msg(level=log.WARNING, spider=spider, **logkws)\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n-                log.err(output, 'Error processing %s' % item, spider=spider)\n+                log.err(output, 'Error processing %(item)s', item=item, spider=spider)\n         else:\n-            log.msg(log.formatter.scraped(output, response, spider), \\\n-                log.DEBUG, spider=spider)\n+            logkws = log.formatter.scraped(output, response, spider)\n+            log.msg(level=log.DEBUG, spider=spider, **logkws)\n             return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, response=response, spider=spider)\n \n\n@@ -91,13 +91,13 @@ class CrawlerProcess(Crawler):\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n         signame = signal_names[signum]\n-        log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n-            \"unclean shutdown\" % signame, level=log.INFO)\n+        log.msg(format=\"Received %(signame)s, shutting down gracefully. Send again to force \",\n+                level=log.INFO, signame=signame)\n         reactor.callFromThread(self.stop)\n \n     def _signal_kill(self, signum, _):\n         install_shutdown_handlers(signal.SIG_IGN)\n         signame = signal_names[signum]\n-        log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n-            level=log.INFO)\n+        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n+                level=log.INFO, signame=signame)\n         reactor.callFromThread(self._stop_reactor)\n\n@@ -56,28 +56,41 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=\n     ev = eventDict.copy()\n     if ev['isError']:\n         ev.setdefault('logLevel', ERROR)\n+\n     # ignore non-error messages from outside scrapy\n     if ev.get('system') != 'scrapy' and not ev['isError']:\n         return\n+\n     level = ev.get('logLevel')\n     if level < log_level:\n         return\n+\n     spider = ev.get('spider')\n     if spider:\n         ev['system'] = spider.name\n-    message = ev.get('message')\n+\n     lvlname = level_names.get(level, 'NOLEVEL')\n+    message = ev.get('message')\n     if message:\n         message = [unicode_to_str(x, encoding) for x in message]\n         if prepend_level:\n             message[0] = \"%s: %s\" % (lvlname, message[0])\n         ev['message'] = message\n+\n     why = ev.get('why')\n     if why:\n         why = unicode_to_str(why, encoding)\n         if prepend_level:\n             why = \"%s: %s\" % (lvlname, why)\n         ev['why'] = why\n+\n+    fmt = ev.get('format')\n+    if fmt:\n+        fmt = unicode_to_str(fmt, encoding)\n+        if prepend_level:\n+            fmt = \"%s: %s\" % (lvlname, fmt)\n+        ev['format'] = fmt\n+\n     return ev\n \n def _get_log_level(level_name_or_id=None):\n@@ -111,14 +124,17 @@ def start(logfile=None, loglevel=None, logstdout=None):\n         msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n             settings['BOT_NAME']))\n \n-def msg(message, level=INFO, **kw):\n+def msg(message=None, _level=INFO, **kw):\n+    kw['logLevel'] = kw.pop('level', _level)\n     kw.setdefault('system', 'scrapy')\n-    kw['logLevel'] = level\n+    if message is None:\n+        log.msg(**kw)\n+    else:\n         log.msg(message, **kw)\n \n def err(_stuff=None, _why=None, **kw):\n-    kw.setdefault('system', 'scrapy')\n     kw['logLevel'] = kw.pop('level', ERROR)\n+    kw.setdefault('system', 'scrapy')\n     log.err(_stuff, _why, **kw)\n \n formatter = load_object(settings['LOG_FORMATTER'])()\n\n@@ -2,6 +2,11 @@ import os\n \n from twisted.python.failure import Failure\n \n+\n+SCRAPEDFMT = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n+DROPPEDFMT = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n+CRAWLEDFMT = u\"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s\"\n+\n class LogFormatter(object):\n     \"\"\"Class for generating log messages for different actions. All methods\n     must return a plain string which doesn't include the log level or the\n@@ -9,14 +14,26 @@ class LogFormatter(object):\n     \"\"\"\n \n     def crawled(self, request, response, spider):\n-        referer = request.headers.get('Referer')\n         flags = ' %s' % str(response.flags) if response.flags else ''\n-        return u\"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n-            request, referer, flags)\n+        return {\n+            'format': CRAWLEDFMT,\n+            'status': response.status,\n+            'request': request,\n+            'referer': request.headers.get('Referer'),\n+            'flags': flags,\n+        }\n \n     def scraped(self, item, response, spider):\n         src = response.getErrorMessage() if isinstance(response, Failure) else response\n-        return u\"Scraped from %s%s%s\" % (src, os.linesep, item)\n+        return {\n+            'format': SCRAPEDFMT,\n+            'src': src,\n+            'item': item,\n+        }\n \n     def dropped(self, item, exception, response, spider):\n-        return u\"Dropped: %s%s%s\" % (exception, os.linesep, item)\n+        return {\n+            'format': DROPPEDFMT,\n+            'exception': exception,\n+            'item': item,\n+        }\n\n@@ -70,8 +70,8 @@ class MailSender(object):\n                        cc=cc, attach=attachs, msg=msg)\n \n         if self.debug:\n-            log.msg('Debug mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n-                (to, cc, subject, len(attachs)), level=log.DEBUG)\n+            log.msg(format='Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n+                    level=log.DEBUG, mailto=to, mailcc=cc, mailsubject=subject, mailattachs=len(attachs))\n             return\n \n         dfd = self._sendmail(rcpts, msg.as_string())\n@@ -82,13 +82,17 @@ class MailSender(object):\n         return dfd\n \n     def _sent_ok(self, result, to, cc, subject, nattachs):\n-        log.msg('Mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n-            (to, cc, subject, nattachs))\n+        log.msg(format='Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n+                       'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n+                mailto=to, mailcc=cc, mailsubject=subject, mailattachs=nattachs)\n \n     def _sent_failed(self, failure, to, cc, subject, nattachs):\n         errstr = str(failure.value)\n-        log.msg('Unable to send mail: To=%s Cc=%s Subject=\"%s\" Attachs=%d - %s' % \\\n-            (to, cc, subject, nattachs, errstr), level=log.ERROR)\n+        log.msg(format='Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n+                       'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n+                       '- %(mailerr)s',\n+                level=log.ERROR, mailto=to, mailcc=cc, mailsubject=subject,\n+                mailattachs=nattachs, mailerr=errstr)\n \n     def _sendmail(self, to_addrs, msg):\n         msg = StringIO(msg)\n\n@@ -37,10 +37,12 @@ class MiddlewareManager(object):\n             except NotConfigured, e:\n                 if e.args:\n                     clsname = clspath.split('.')[-1]\n-                    log.msg(\"Disabled %s: %s\" % (clsname, e.args[0]), log.WARNING)\n+                    log.msg(format=\"Disabled %(clsname)s: %(eargs)s\",\n+                            level=log.WARNING, clsname=clsname, eargs=e.args[0])\n+\n         enabled = [x.__class__.__name__ for x in middlewares]\n-        log.msg(\"Enabled %ss: %s\" % (cls.component_name, \", \".join(enabled)), \\\n-            level=log.DEBUG)\n+        log.msg(format=\"Enabled %(componentname)ss: %(enabledlist)s\", level=log.DEBUG,\n+                componentname=cls.component_name, enabledlist=', '.join(enabled))\n         return cls(*middlewares)\n \n     @classmethod\n\n@@ -46,7 +46,8 @@ class TelnetConsole(protocol.ServerFactory):\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n-        log.msg(\"Telnet console listening on %s:%d\" % (h.host, h.port), log.DEBUG)\n+        log.msg(format=\"Telnet console listening on %(host)s:%(port)d\",\n+                level=log.DEBUG, host=h.host, port=h.port)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n@@ -1,6 +1,7 @@\n-import sys\n import os\n+import sys\n import subprocess\n+from time import sleep\n from os.path import exists, join, abspath\n from shutil import rmtree\n from tempfile import mkdtemp\n@@ -32,8 +33,20 @@ class ProjectTest(unittest.TestCase):\n \n     def proc(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n-        return subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, \\\n-            cwd=self.cwd, env=self.env, **kwargs)\n+        p = subprocess.Popen(args, cwd=self.cwd, env=self.env,\n+                             stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n+                             **kwargs)\n+\n+        waited = 0\n+        interval = 0.2\n+        while p.poll() is None:\n+            sleep(interval)\n+            waited += interval\n+            if waited > 5:\n+                p.kill()\n+                assert False, 'Command took too much time to complete'\n+\n+        return p\n \n \n class StartprojectTest(ProjectTest):\n@@ -125,10 +138,10 @@ class MySpider(BaseSpider):\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"[myspider] DEBUG: It Works!\" in log)\n-        self.assert_(\"[myspider] INFO: Spider opened\" in log)\n-        self.assert_(\"[myspider] INFO: Closing spider (finished)\" in log)\n-        self.assert_(\"[myspider] INFO: Spider closed (finished)\" in log)\n+        self.assert_(\"[myspider] DEBUG: It Works!\" in log, log)\n+        self.assert_(\"[myspider] INFO: Spider opened\" in log, log)\n+        self.assert_(\"[myspider] INFO: Closing spider (finished)\" in log, log)\n+        self.assert_(\"[myspider] INFO: Spider closed (finished)\" in log, log)\n \n     def test_runspider_no_spider_found(self):\n         tmpdir = self.mktemp()\n\n@@ -23,19 +23,25 @@ class LoggingContribTest(unittest.TestCase):\n     def test_crawled(self):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n-        self.assertEqual(self.formatter.crawled(req, res, self.spider),\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['format'] % logkws\n+        self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n \n         req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n         res = Response(\"http://www.example.com\", flags=['cached'])\n-        self.assertEqual(self.formatter.crawled(req, res, self.spider),\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['format'] % logkws\n+        self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n     def test_dropped(self):\n         item = {}\n         exception = Exception(u\"\\u2018\")\n         response = Response(\"http://www.example.com\")\n-        lines = self.formatter.dropped(item, exception, response, self.spider).splitlines()\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        logline = logkws['format'] % logkws\n+        lines = logline.splitlines()\n         assert all(isinstance(x, unicode) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n@@ -43,7 +49,9 @@ class LoggingContribTest(unittest.TestCase):\n         item = CustomItem()\n         item['name'] = u'\\xa3'\n         response = Response(\"http://www.example.com\")\n-        lines = self.formatter.scraped(item, response, self.spider).splitlines()\n+        logkws = self.formatter.scraped(item, response, self.spider)\n+        logline = logkws['format'] % logkws\n+        lines = logline.splitlines()\n         assert all(isinstance(x, unicode) for x in lines)\n         self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n \n\n@@ -61,7 +61,8 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     while True:\n         row = _getrow(csv_r)\n         if len(row) != len(headers):\n-            log.msg(\"ignoring row %d (length: %d, should be: %d)\" % (csv_r.line_num, len(row), len(headers)), log.WARNING)\n+            log.msg(format=\"ignoring row %(csvlnum)d (length: %(csvrow)d, should be: %(csvheader)d)\",\n+                    level=log.WARNING, csvlnum=csv_r.line_num, csvrow=len(row), csvheader=len(headers))\n             continue\n         else:\n             yield dict(zip(headers, row))\n\n@@ -21,8 +21,8 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n             response = robustApply(receiver, signal=signal, sender=sender,\n                 *arguments, **named)\n             if isinstance(response, Deferred):\n-                log.msg(\"Cannot return deferreds from signal handler: %s\" % \\\n-                    receiver, log.ERROR, spider=spider)\n+                log.msg(format=\"Cannot return deferreds from signal handler: %(receiver)s\",\n+                        level=log.ERROR, spider=spider, receiver=receiver)\n         except dont_log:\n             result = Failure()\n         except Exception:\n\n@@ -38,10 +38,14 @@ def create_spider_for_request(spidermanager, request, default_spider=None, \\\n     snames = spidermanager.find_by_request(request)\n     if len(snames) == 1:\n         return spidermanager.create(snames[0], **spider_kwargs)\n+\n     if len(snames) > 1 and log_multiple:\n-        log.msg('More than one spider can handle: %s - %s' % \\\n-            (request, \", \".join(snames)), log.ERROR)\n+        log.msg(format='More than one spider can handle: %(request)s - %(snames)s',\n+                level=log.ERROR, request=request, snames=', '.join(snames))\n+\n     if len(snames) == 0 and log_none:\n-        log.msg('Unable to find spider that handles: %s' % request, log.ERROR)\n+        log.msg(format='Unable to find spider that handles: %(request)s',\n+                level=log.ERROR, request=request)\n+\n     return default_spider\n \n\n@@ -89,7 +89,8 @@ class WebService(server.Site):\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n-        log.msg(\"Web service listening on %s:%d\" % (h.host, h.port), log.DEBUG)\n+        log.msg(format='Web service listening on %(host)s:%(port)d',\n+                level=log.DEBUG, host=h.host, port=h.port)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n@@ -35,7 +35,8 @@ def application(config):\n \n     timer = TimerService(5, poller.poll)\n     webservice = TCPServer(http_port, server.Site(Root(config, app)), interface=bind_address)\n-    log.msg(\"Scrapyd web console available at http://%s:%s/\" % (bind_address, http_port))\n+    log.msg(format=\"Scrapyd web console available at http://%(bind_address)s:%(http_port)s/\",\n+            bind_address=bind_address, http_port=http_port)\n \n     launcher.setServiceParent(app)\n     timer.setServiceParent(app)\n\n@@ -25,8 +25,9 @@ class Launcher(Service):\n     def startService(self):\n         for slot in range(self.max_proc):\n             self._wait_for_project(slot)\n-        log.msg(\"%s started: max_proc=%r, runner=%r\" % (self.parent.name, \\\n-            self.max_proc, self.runner), system=\"Launcher\")\n+        log.msg(format='%(parent)s started: max_proc=%(max_proc)r, runner=%(runner)r',\n+                parent=self.parent.name, max_proc=self.max_proc,\n+                runner=self.runner, system='Launcher')\n \n     def _wait_for_project(self, slot):\n         poller = self.app.getComponent(IPoller)\n@@ -96,6 +97,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"project=%r spider=%r job=%r pid=%r log=%r items=%r\" % (self.project, \\\n-            self.spider, self.job, self.pid, self.logfile, self.itemsfile)\n-        log.msg(msg, system=\"Launcher\")\n+        fmt = 'project=%(project)r spider=%(spider)r job=%(job)r pid=%(pid)r log=%(log)r items=%(items)r'\n+        log.msg(format=fmt, project=self.project, spider=self.spider,\n+                job=self.job, pid=self.pid, log=self.logfile, items=self.itemsfile)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c8af83138001ac39e225aff34612cfa28f8b10f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ def setup(app):\n     )\n     app.add_role('source', source_role)\n     app.add_role('commit', commit_role)\n+    app.add_role('issue', issue_role)\n     app.add_role('rev', rev_role)\n \n def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n@@ -32,6 +33,12 @@ def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n     node = nodes.reference(rawtext, text, refuri=ref, **options)\n     return [node], []\n \n+def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n+    ref = 'https://github.com/scrapy/scrapy/issues/' + text\n+    set_classes(options)\n+    node = nodes.reference(rawtext, 'issue ' + text, refuri=ref, **options)\n+    return [node], []\n+\n def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n     ref = 'https://github.com/scrapy/scrapy/commit/' + text\n     set_classes(options)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6435c5aa7f935bf43d6b368db06210cae77a720", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 246 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -96,7 +96,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n             self.log(\"Process died: exitstatus=%r \" % status.value.exitCode)\n         self.deferred.callback(self)\n \n-    def log(self, msg):\n-        fmt = 'project=%(project)r spider=%(spider)r job=%(job)r pid=%(pid)r log=%(log)r items=%(items)r'\n-        log.msg(format=fmt, project=self.project, spider=self.spider,\n+    def log(self, action):\n+        fmt = '%(action)s project=%(project)r spider=%(spider)r job=%(job)r pid=%(pid)r log=%(log)r items=%(items)r'\n+        log.msg(format=fmt, action=action, project=self.project, spider=self.spider,\n                 job=self.job, pid=self.pid, log=self.logfile, items=self.itemsfile)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b768ae9894df0873aa0f41f7bc172ad0a9e2ddf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1974 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -253,7 +253,6 @@ class ExecutionEngine(object):\n         if slot.closing:\n             return slot.closing\n         log.msg(format=\"Closing spider (%(reason)s)\", reason=reason, spider=spider)\n-        log.msg(format='hohohoohoo %(aaa)s', spider=spider, aaa='12')\n \n         dfd = slot.close()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd1398b280334b1014d2dbed009331ceed7c4fb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 20 | Files Changed: 6 | Hunks: 19 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 41 | Churn Cumulative: 1332 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,7 +70,7 @@ class Downloader(object):\n         self.signals = crawler.signals\n         self.slots = {}\n         self.active = set()\n-        self.handlers = DownloadHandlers()\n+        self.handlers = DownloadHandlers(crawler.settings)\n         self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n         self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n\n@@ -2,13 +2,12 @@\n \n from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.conf import settings\n from scrapy.utils.misc import load_object\n \n \n class DownloadHandlers(object):\n \n-    def __init__(self):\n+    def __init__(self, settings):\n         self._handlers = {}\n         self._notconfigured = {}\n         handlers = settings.get('DOWNLOAD_HANDLERS_BASE')\n@@ -16,7 +15,7 @@ class DownloadHandlers(object):\n         for scheme, clspath in handlers.iteritems():\n             cls = load_object(clspath)\n             try:\n-                dh = cls()\n+                dh = cls(settings)\n             except NotConfigured, ex:\n                 self._notconfigured[scheme] = str(ex)\n             else:\n\n@@ -4,6 +4,9 @@ from scrapy.utils.decorator import defers\n \n class FileDownloadHandler(object):\n \n+    def __init__(self, settings):\n+        pass\n+\n     @defers\n     def download_request(self, request, spider):\n         filepath = file_uri_to_path(request.url)\n\n@@ -4,23 +4,19 @@ from twisted.internet import reactor\n \n from scrapy.exceptions import NotSupported\n from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n from scrapy import optional_features\n \n ssl_supported = 'ssl' in optional_features\n \n-HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n-\n-\n class HttpDownloadHandler(object):\n \n-    def __init__(self, httpclientfactory=HTTPClientFactory):\n-        self.httpclientfactory = httpclientfactory\n+    def __init__(self, settings):\n+        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n-        factory = self.httpclientfactory(request)\n+        factory = self.HTTPClientFactory(request)\n         self._connect(factory)\n         return factory.deferred\n \n@@ -29,7 +25,7 @@ class HttpDownloadHandler(object):\n         if factory.scheme == 'https':\n             if ssl_supported:\n                 return reactor.connectSSL(host, port, factory, \\\n-                        ClientContextFactory())\n+                        self.ClientContextFactory())\n             raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n         else:\n             return reactor.connectTCP(host, port, factory)\n\n@@ -1,7 +1,6 @@\n from scrapy import optional_features\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.conf import settings\n from .http import HttpDownloadHandler\n \n try:\n@@ -30,7 +29,7 @@ else:\n \n class S3DownloadHandler(object):\n \n-    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, \\\n+    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n             httpdownloadhandler=HttpDownloadHandler):\n         if 'boto' not in optional_features:\n             raise NotConfigured(\"missing boto library\")\n@@ -44,7 +43,7 @@ class S3DownloadHandler(object):\n             self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)\n         except Exception, ex:\n             raise NotConfigured(str(ex))\n-        self._download_http = httpdownloadhandler().download_request\n+        self._download_http = httpdownloadhandler(settings).download_request\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n@@ -15,6 +15,7 @@ from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n+from scrapy.settings import Settings\n from scrapy import optional_features\n \n \n@@ -25,7 +26,7 @@ class FileTestCase(unittest.TestCase):\n         fd = open(self.tmpname + '^', 'w')\n         fd.write('0123456789')\n         fd.close()\n-        self.download_request = FileDownloadHandler().download_request\n+        self.download_request = FileDownloadHandler(Settings()).download_request\n \n     def test_download(self):\n         def _test(response):\n@@ -60,7 +61,7 @@ class HttpTestCase(unittest.TestCase):\n         self.wrapper = WrappingFactory(self.site)\n         self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpDownloadHandler().download_request\n+        self.download_request = HttpDownloadHandler(Settings()).download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -148,7 +149,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpDownloadHandler().download_request\n+        self.download_request = HttpDownloadHandler(Settings()).download_request\n \n     def tearDown(self):\n         return self.port.stopListening()\n@@ -177,6 +178,9 @@ class HttpProxyTestCase(unittest.TestCase):\n \n \n class HttpDownloadHandlerMock(object):\n+    def __init__(self, settings):\n+        pass\n+\n     def download_request(self, request, spider):\n         return request\n \n@@ -191,7 +195,7 @@ class S3TestCase(unittest.TestCase):\n     AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n \n     def setUp(self):\n-        s3reqh = S3DownloadHandler(self.AWS_ACCESS_KEY_ID, \\\n+        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID, \\\n                 self.AWS_SECRET_ACCESS_KEY, \\\n                 httpdownloadhandler=HttpDownloadHandlerMock)\n         self.download_request = s3reqh.download_request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
