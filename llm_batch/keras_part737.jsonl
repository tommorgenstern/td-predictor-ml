{"custom_id": "keras#cc9a181e78630e1e2e5faa9b3774cbe64505a462", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1072 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -231,6 +231,7 @@ class Discretization(TFDataLayer):\n         return\n \n     def call(self, inputs):\n+        self._convert_input_args = True\n         indices = ops.digitize(inputs, self.bin_boundaries)\n         outputs = encode_categorical_inputs(\n             indices,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01f1ec5c7366115a71b631758333bed7cd60bca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1284 | Contributors (this commit): 9 | Commits (past 90d): 33 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -154,8 +154,8 @@ class Discretization(TFDataLayer):\n             self.summary = None\n         else:\n             self.summary = np.array([[], []], dtype=\"float32\")\n-        self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n+        self._convert_input_args = True\n \n     def build(self, input_shape=None):\n         self.built = True\n@@ -231,7 +231,6 @@ class Discretization(TFDataLayer):\n         return\n \n     def call(self, inputs):\n-        self._convert_input_args = True\n         indices = ops.digitize(inputs, self.bin_boundaries)\n         outputs = encode_categorical_inputs(\n             indices,\n\n@@ -3,6 +3,7 @@ import os\n import numpy as np\n from tensorflow import data as tf_data\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import models\n from keras_core import testing\n@@ -40,6 +41,7 @@ class DicretizationTest(testing.TestCase):\n             bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"int\"\n         )\n         output = layer(np.array([[-1.0, 0.0, 0.1, 0.8, 1.2]]))\n+        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 1, 2, 3]]))\n \n         # one_hot mode\n@@ -47,6 +49,7 @@ class DicretizationTest(testing.TestCase):\n             bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"one_hot\"\n         )\n         output = layer(np.array([0.1, 0.8]))\n+        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))\n \n         # multi_hot mode\n@@ -54,6 +57,7 @@ class DicretizationTest(testing.TestCase):\n             bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"multi_hot\"\n         )\n         output = layer(np.array([[0.1, 0.8]]))\n+        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 1, 0]]))\n \n         # count mode\n@@ -61,6 +65,7 @@ class DicretizationTest(testing.TestCase):\n             bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"count\"\n         )\n         output = layer(np.array([[0.1, 0.8, 0.9]]))\n+        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 2, 0]]))\n \n     def test_tf_data_compatibility(self):\n@@ -73,6 +78,7 @@ class DicretizationTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n+        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n \n         # With adapt flow\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccd1c826d2bfa2893f11306c9d89f649170a818d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 210 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -78,7 +78,6 @@ class DicretizationTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n-        self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n \n         # With adapt flow\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#641fd28eb88d02be8a043e9c4103aa46b837d941", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 57 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/8 | Churn Δ: 108 | Churn Cumulative: 3138 | Contributors (this commit): 12 | Commits (past 90d): 47 | Contributors (cumulative): 21 | DMM Complexity: 0.2\n\nDIFF:\n@@ -255,48 +255,3 @@ def compute_conv_transpose_output_shape(\n     else:\n         output_shape = [input_shape[0], filters] + output_shape\n     return output_shape\n-\n-\n-def encode_categorical_inputs(\n-    inputs,\n-    output_mode,\n-    depth,\n-    dtype=\"float32\",\n-    sparse=False,\n-    count_weights=None,\n-):\n-    from keras_core import ops\n-\n-    \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n-    if output_mode == \"int\":\n-        return ops.cast(inputs, dtype=dtype)\n-\n-    original_shape = inputs.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if len(ops.shape(inputs)) == 0:\n-        inputs = ops.expand_dims(inputs, -1)\n-    # One hot will unprank only if the final output dimension is not already 1.\n-    if output_mode == \"one_hot\":\n-        if ops.shape(inputs)[-1] != 1:\n-            inputs = ops.expand_dims(inputs, -1)\n-\n-    if len(ops.shape(inputs)) > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n-    bincounts = ops.bincount(\n-        inputs,\n-        weights=count_weights,\n-        minlength=depth,\n-    )\n-    if binary_output:\n-        one_hot_input = ops.one_hot(inputs, depth)\n-        bincounts = ops.where(ops.any(one_hot_input, axis=-2), 1, 0)\n-    bincounts = ops.cast(bincounts, dtype)\n-\n-    return bincounts\n\n@@ -1,12 +1,10 @@\n import numpy as np\n \n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.backend_utils import encode_categorical_inputs\n from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.utils import argument_validation\n-from keras_core.utils import backend_utils\n+from keras_core.utils import numerical_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n@@ -154,8 +152,6 @@ class Discretization(TFDataLayer):\n             self.summary = None\n         else:\n             self.summary = np.array([[], []], dtype=\"float32\")\n-        self._allow_non_tensor_positional_args = True\n-        self._convert_input_args = True\n \n     def build(self, input_shape=None):\n         self.built = True\n@@ -231,18 +227,14 @@ class Discretization(TFDataLayer):\n         return\n \n     def call(self, inputs):\n-        indices = ops.digitize(inputs, self.bin_boundaries)\n-        outputs = encode_categorical_inputs(\n+        indices = self.backend.numpy.digitize(inputs, self.bin_boundaries)\n+        outputs = numerical_utils.encode_categorical_inputs(\n             indices,\n             output_mode=self.output_mode,\n             depth=len(self.bin_boundaries) + 1,\n             dtype=self.compute_dtype,\n+            backend_module=self.backend,\n         )\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n     def get_config(self):\n\n@@ -105,3 +105,50 @@ def to_categorical(x, num_classes=None):\n     output_shape = input_shape + (num_classes,)\n     categorical = np.reshape(categorical, output_shape)\n     return categorical\n+\n+\n+def encode_categorical_inputs(\n+    inputs,\n+    output_mode,\n+    depth,\n+    dtype=\"float32\",\n+    count_weights=None,\n+    backend_module=None,\n+):\n+    \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n+    backend_module = backend_module or backend\n+\n+    if output_mode == \"int\":\n+        return backend_module.cast(inputs, dtype=dtype)\n+\n+    original_shape = inputs.shape\n+    # In all cases, we should uprank scalar input to a single sample.\n+    if len(backend_module.shape(inputs)) == 0:\n+        inputs = backend_module.numpy.expand_dims(inputs, -1)\n+    # One hot will unprank only if the final output dimension is not already 1.\n+    if output_mode == \"one_hot\":\n+        if backend_module.shape(inputs)[-1] != 1:\n+            inputs = backend_module.numpy.expand_dims(inputs, -1)\n+\n+    if len(backend_module.shape(inputs)) > 2:\n+        raise ValueError(\n+            \"When output_mode is not `'int'`, maximum supported output rank \"\n+            f\"is 2. Received output_mode {output_mode} and input shape \"\n+            f\"{original_shape}, \"\n+            f\"which would result in output rank {inputs.shape.rank}.\"\n+        )\n+\n+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    bincounts = backend_module.numpy.bincount(\n+        inputs,\n+        weights=count_weights,\n+        minlength=depth,\n+    )\n+    if binary_output:\n+        one_hot_input = backend_module.nn.one_hot(inputs, depth)\n+        bincounts = backend_module.numpy.where(\n+            backend_module.numpy.any(one_hot_input, axis=-2), 1, 0\n+        )\n+    bincounts = backend_module.cast(bincounts, dtype)\n+\n+    return bincounts\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3390beea11f2f0f87aebbd2f3ceed2159cf56e4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 9 | Methods Changed: 12 | Complexity Δ (Sum/Max): 20/10 | Churn Δ: 110 | Churn Cumulative: 7737 | Contributors (this commit): 23 | Commits (past 90d): 104 | Contributors (cumulative): 50 | DMM Complexity: 0.7162162162162162\n\nDIFF:\n@@ -115,7 +115,7 @@ class KerasVariable:\n \n     def assign(self, value):\n         value = self._convert_to_tensor(value, dtype=self.dtype)\n-        if not shape_equal(value, self.value):\n+        if not shape_equal(value.shape, self.shape):\n             raise ValueError(\n                 \"The shape of the target variable and \"\n                 \"the shape of the target value in \"\n@@ -446,11 +446,11 @@ def standardize_shape(shape):\n     return shape\n \n \n-def shape_equal(a, b):\n-    \"\"\"Return whether a.shape == b.shape (allows None entries).\"\"\"\n-    if len(a.shape) != len(b.shape):\n+def shape_equal(a_shape, b_shape):\n+    \"\"\"Return whether a_shape == b_shape (allows None entries).\"\"\"\n+    if len(a_shape) != len(b_shape):\n         return False\n-    for e1, e2 in zip(a.shape, b.shape):\n+    for e1, e2 in zip(a_shape, b_shape):\n         if e1 is not None and e2 is not None and e1 != e2:\n             return False\n     return True\n\n@@ -44,7 +44,14 @@ class JAXTrainer(base_trainer.Trainer):\n             return_losses=True,\n             **kwargs,\n         )\n-        loss = self.compute_loss(x, y, y_pred, sample_weight, allow_empty=True)\n+\n+        trainable_mapping = zip(self.trainable_variables, trainable_variables)\n+        with backend.StatelessScope(state_mapping=trainable_mapping):\n+            # Note that this is needed for the regularization loss, which need\n+            # the latest value of train/non-trainable variables.\n+            loss = self.compute_loss(\n+                x, y, y_pred, sample_weight, allow_empty=True\n+            )\n         if losses:\n             loss += ops.sum(losses)\n         unscaled_loss = loss\n@@ -418,6 +425,7 @@ class JAXTrainer(base_trainer.Trainer):\n             optimizer_variables = [v.value for v in self.optimizer.variables]\n             metrics_variables = [v.value for v in self.metrics_variables]\n \n+            self._purge_model_variables()\n             for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n@@ -576,6 +584,7 @@ class JAXTrainer(base_trainer.Trainer):\n         ]\n         metrics_variables = [v.value for v in self.metrics_variables]\n \n+        self._purge_model_variables(optimizer_variables=False)\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_test_batch_begin(step)\n \n@@ -907,3 +916,33 @@ class JAXTrainer(base_trainer.Trainer):\n             optimizer_variables,\n             metrics_variables,\n         )\n+\n+    def _purge_model_variables(\n+        self,\n+        trainable_variables=True,\n+        non_trainable_variables=True,\n+        optimizer_variables=True,\n+        metric_variables=True,\n+    ):\n+        \"\"\"Remove all the model variable for memory saving.\n+\n+        During JAX training, since the training function are stateless, we have\n+        to pass in and get the model weights over and over, during which the\n+        copy of the weights that attached to the KerasVariable are still and\n+        occupying extra memory. We remove those variable to save memory (for\n+        better memory utilization) at the beginning of the epoch, and reattach\n+        the value back to variables at the end of the epoch, via\n+        `jax_state_sync()`.\n+        \"\"\"\n+        if trainable_variables:\n+            for v in self.trainable_variables:\n+                v._value = None\n+        if non_trainable_variables:\n+            for v in self.non_trainable_variables:\n+                v._value = None\n+        if optimizer_variables:\n+            for v in self.optimizer.variables:\n+                v._value = None\n+        if metric_variables:\n+            for v in self.metrics_variables:\n+                v._value = None\n\n@@ -1040,6 +1040,8 @@ class Layer(BackendLayer, Operation):\n             losses.extend(layer._get_own_losses())\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n+            if backend.in_stateless_scope():\n+                v = backend.get_stateless_scope().get_current_value(v)\n             regularizer = getattr(v, \"regularizer\", None)\n             if regularizer:\n                 weight_regularization_losses.append(regularizer(v))\n\n@@ -262,6 +262,63 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(outputs[\"y_one\"], 4 * np.ones((100, 3)))\n         self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"Memory optimization is only implemented in JAX\",\n+    )\n+    def test_fit_eval_flow_for_jax_model_weights(self):\n+        model = ExampleModel(units=3)\n+        epochs = 3\n+        batch_size = 20\n+        steps_per_epoch = 7\n+        dataset_size = batch_size * (steps_per_epoch - 2)\n+        x = np.ones((dataset_size, 4))\n+        y = np.zeros((dataset_size, 3))\n+\n+        class ModelWeightCheck(Callback):\n+            def __init__(self):\n+                super().__init__()\n+\n+            # Note that we access model via self._model since self.model\n+            # will trigger a sync of the jax training state back to the model.\n+            def on_train_batch_begin(self, batch, logs=None):\n+                for v in self._model.trainable_variables:\n+                    assert v._value is None\n+                for v in self._model.non_trainable_variables:\n+                    assert v._value is None\n+                for v in self._model.optimizer.variables:\n+                    assert v._value is None\n+                for v in self._model.metrics_variables:\n+                    assert v._value is None\n+\n+            def on_test_batch_begin(self, batch, logs=None):\n+                for v in self._model.non_trainable_variables:\n+                    assert v._value is None\n+                for v in self._model.metrics_variables:\n+                    assert v._value is None\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+\n+        model.fit(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch,\n+            epochs=epochs,\n+            callbacks=[ModelWeightCheck()],\n+        )\n+\n+        model.evaluate(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            callbacks=[ModelWeightCheck()],\n+        )\n+\n     @pytest.mark.requires_trainable_backend\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3569c123716340dda8f43be87363dfa6eb97f87d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 136 | Files Changed: 38 | Hunks: 117 | Methods Changed: 47 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 273 | Churn Cumulative: 32732 | Contributors (this commit): 36 | Commits (past 90d): 250 | Contributors (cumulative): 193 | DMM Complexity: None\n\nDIFF:\n@@ -167,7 +167,7 @@ def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n       texts (list): List of string i.e input texts\n       vocab_size (int): vocab size\n       max_seq (int): Maximum sequence lenght.\n-      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n+      special_tokens (list, optional): List of special tokens. Defaults to `['[MASK]']`.\n \n     Returns:\n         layers.Layer: Return TextVectorization Keras Layer\n\n@@ -315,9 +315,9 @@ class ShiftViTBlock(layers.Layer):\n     \"\"\"A unit ShiftViT Block\n \n     Args:\n-        shift_pixel (int): The number of pixels to shift. Default to 1.\n+        shift_pixel (int): The number of pixels to shift. Defaults to `1`.\n         mlp_expand_ratio (int): The ratio with which MLP features are\n-            expanded. Default to 2.\n+            expanded. Defaults to `2`.\n         mlp_dropout_rate (float): The dropout rate used in MLP.\n         num_div (int): The number of divisions of the feature map's channel.\n             Totally, 4/num_div of channels will be shifted. Defaults to 12.\n@@ -493,10 +493,10 @@ class StackedShiftBlocks(layers.Layer):\n         stochastic_depth_rate (float): The maximum drop path rate chosen.\n         is_merge (boolean): A flag that determines the use of the Patch Merge\n             layer after the shift vit blocks.\n-        num_div (int): The division of channels of the feature map. Defaults to 12.\n-        shift_pixel (int): The number of pixels to shift. Defaults to 1.\n+        num_div (int): The division of channels of the feature map. Defaults to `12`.\n+        shift_pixel (int): The number of pixels to shift. Defaults to `1`.\n         mlp_expand_ratio (int): The ratio with which the initial dense layer of\n-            the MLP is expanded Defaults to 2.\n+            the MLP is expanded Defaults to `2`.\n     \"\"\"\n \n     def __init__(\n@@ -578,10 +578,10 @@ class ShiftViTModel(keras.Model):\n         mlp_dropout_rate (float): The dropout rate used in the MLP block.\n         stochastic_depth_rate (float): The maximum drop rate probability.\n         num_div (int): The number of divisions of the channesl of the feature\n-            map. Defaults to 12.\n-        shift_pixel (int): The number of pixel to shift. Default to 1.\n+            map. Defaults to `12`.\n+        shift_pixel (int): The number of pixel to shift. Defaults to `1`.\n         mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n-            is expanded to. Defaults to 2.\n+            is expanded to. Defaults to `2`.\n     \"\"\"\n \n     def __init__(\n\n@@ -192,7 +192,7 @@ Args:\n     classifier_activation: A `str` or callable. The activation function to use\n         on the \"top\" layer. Ignored unless `include_top=True`. Set\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n-        Defaults to 'softmax'.\n+        Defaults to `'softmax'`.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n \n\n@@ -62,7 +62,7 @@ PREPROCESS_INPUT_MODE_DOC = \"\"\"\n       - torch: will scale pixels between 0 and 1 and then\n           will normalize each channel with respect to the\n           ImageNet dataset.\n-      Defaults to \"caffe\".\n+      Defaults to `\"caffe\"`.\n   \"\"\"\n \n PREPROCESS_INPUT_DEFAULT_ERROR_DOC = \"\"\"\n@@ -119,7 +119,7 @@ def decode_predictions(preds, top=5):\n \n     Args:\n         preds: NumPy array encoding a batch of predictions.\n-        top: Integer, how many top-guesses to return. Defaults to 5.\n+        top: Integer, how many top-guesses to return. Defaults to `5`.\n \n     Returns:\n         A list of lists of top class prediction tuples\n\n@@ -222,8 +222,8 @@ def residual_block_v1(\n     Args:\n         x: Input tensor.\n         filters: No of filters in the bottleneck layer.\n-        kernel_size: Kernel size of the bottleneck layer. Defaults to 3\n-        stride: Stride of the first layer. Defaults to 1\n+        kernel_size: Kernel size of the bottleneck layer. Defaults to `3`.\n+        stride: Stride of the first layer. Defaults to `1`.\n         conv_shortcut: Use convolution shortcut if `True`, otherwise\n             use identity shortcut. Defaults to `True`\n         name(optional): Name of the block\n@@ -278,7 +278,7 @@ def stack_residual_blocks_v1(x, filters, blocks, stride1=2, name=None):\n         x: Input tensor.\n         filters: Number of filters in the bottleneck layer in a block.\n         blocks: Number of blocks in the stacked blocks.\n-        stride1: Stride of the first layer in the first block. Defaults to 2.\n+        stride1: Stride of the first layer in the first block. Defaults to `2`.\n         name: Stack label.\n \n     Returns:\n@@ -301,8 +301,8 @@ def residual_block_v2(\n     Args:\n         x: Input tensor.\n         filters: No of filters in the bottleneck layer.\n-        kernel_size: Kernel size of the bottleneck layer. Defaults to 3\n-        stride: Stride of the first layer. Defaults to 1\n+        kernel_size: Kernel size of the bottleneck layer. Defaults to `3`.\n+        stride: Stride of the first layer. Defaults to `1`.\n         conv_shortcut: Use convolution shortcut if `True`, otherwise\n             use identity shortcut. Defaults to `True`\n         name(optional): Name of the block\n@@ -363,7 +363,7 @@ def stack_residual_blocks_v2(x, filters, blocks, stride1=2, name=None):\n         x: Input tensor.\n         filters: Number of filters in the bottleneck layer in a block.\n         blocks: Number of blocks in the stacked blocks.\n-        stride1: Stride of the first layer in the first block. Defaults to 2.\n+        stride1: Stride of the first layer in the first block. Defaults to `2`.\n         name: Stack label.\n \n     Returns:\n\n@@ -14,9 +14,9 @@ def list_devices(device_type=None):\n     Note that this should return the global devices in a distributed setting.\n \n     Args:\n-        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Default to `gpu` or\n-            `tpu` if available when device_type is not provided. Otherwise\n-            will return the `cpu` devices.\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Defaults to `\"gpu\"`\n+            or `\"tpu\"` if available when device_type is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n \n     Return:\n         List of devices that are available for distribute computation.\n\n@@ -29,7 +29,7 @@ def list_devices(device_type=None):\n \n     Args:\n         device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n-            Default to `\"gpu\"` or `\"tpu\"` if available when\n+            Defaults to `\"gpu\"` or `\"tpu\"` if available when\n             `device_type` is not provided. Otherwise\n             will return the `\"cpu\"` devices.\n \n@@ -60,7 +60,7 @@ class DeviceMesh:\n             the rank of the `shape`. The `axis_names` will be used to\n             match/create the `TensorLayout` when distribute the data and\n             variables.\n-        devices: Optional list of devices. Default to all the available\n+        devices: Optional list of devices. Defaults to all the available\n             devices locally from `keras_core.distribution.list_devices()`.\n     \"\"\"\n \n\n@@ -15,7 +15,7 @@ class ELU(Layer):\n     ```\n \n     Args:\n-        alpha: float, slope of negative section. Defaults to 1.0.\n+        alpha: float, slope of negative section. Defaults to `1.0`.\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n\n@@ -29,7 +29,7 @@ class LeakyReLU(Layer):\n \n     Args:\n         negative_slope: Float >= 0.0. Negative slope coefficient.\n-          Defaults to 0.3.\n+          Defaults to `0.3`.\n         **kwargs: Base layer keyword arguments, such as\n             `name` and `dtype`.\n \n\n@@ -30,9 +30,10 @@ class ReLU(Layer):\n     Args:\n         max_value: Float >= 0. Maximum activation value. None means unlimited.\n             Defaults to `None`.\n-        negative_slope: Float >= 0. Negative slope coefficient. Defaults to 0.0.\n+        negative_slope: Float >= 0. Negative slope coefficient.\n+            Defaults to `0.0`.\n         threshold: Float >= 0. Threshold value for thresholded activation.\n-            Defaults to 0.0.\n+            Defaults to `0.0`.\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n\n@@ -26,7 +26,7 @@ class AdditiveAttention(Attention):\n         use_scale: If `True`, will create a scalar variable to scale the\n             attention scores.\n         dropout: Float between 0 and 1. Fraction of the units to drop for the\n-            attention scores. Defaults to 0.0.\n+            attention scores. Defaults to `0.0`.\n \n     Call Args:\n         inputs: List of the following tensors:\n\n@@ -26,7 +26,7 @@ class Attention(Layer):\n         use_scale: If `True`, will create a scalar variable to scale the\n             attention scores.\n         dropout: Float between 0 and 1. Fraction of the units to drop for the\n-            attention scores. Defaults to 0.0.\n+            attention scores. Defaults to `0.0`.\n         score_mode: Function to use to compute attention scores, one of\n             `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the\n             query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n\n@@ -80,7 +80,7 @@ class Layer(BackendLayer, Operation):\n         dtype: The dtype of the layer's computations and weights. Can also be a\n             `keras_core.mixed_precision.DTypePolicy`,\n             which allows the computation and\n-            weight dtype to differ. Default of `None` means to use\n+            weight dtype to differ. Defaults to `None`. `None` means to use\n             `keras_core.mixed_precision.dtype_policy()`,\n             which is a `float32` policy unless set to different value\n             (via `keras_core.mixed_precision.set_dtype_policy()`).\n\n@@ -34,7 +34,7 @@ class GroupNormalization(Layer):\n         axis: Integer or List/Tuple. The axis or axes to normalize across.\n             Typically, this is the features axis/axes. The left-out axes are\n             typically the batch axis/axes. -1 is the last dimension in the\n-            input. Defaults to -1.\n+            input. Defaults to `-1`.\n         epsilon: Small float added to variance to avoid dividing by zero.\n             Defaults to 1e-3.\n         center: If `True`, add offset of `beta` to normalized tensor.\n\n@@ -61,7 +61,7 @@ class IntegerLookup(IndexLookup):\n             If this value is more than 1, OOV inputs are modulated to\n             determine their OOV value.\n             If this value is 0, OOV inputs will cause an error when calling\n-            the layer. Defaults to 1.\n+            the layer. Defaults to `1`.\n         mask_token: An integer token that represents masked inputs. When\n             `output_mode` is `\"int\"`, the token is included in vocabulary\n             and mapped to index 0. In other output modes,\n@@ -69,7 +69,7 @@ class IntegerLookup(IndexLookup):\n             of the mask token in the input will be dropped.\n             If set to None, no mask term will be added. Defaults to `None`.\n         oov_token: Only used when `invert` is `True`. The token to return\n-            for OOV indices. Defaults to -1.\n+            for OOV indices. Defaults to `-1`.\n         vocabulary: Optional. Either an array of integers or a string path to a\n             text file. If passing an array, can pass a tuple, list,\n             1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n\n@@ -60,7 +60,7 @@ class StringLookup(IndexLookup):\n             If this value is more than 1, OOV inputs are modulated to\n             determine their OOV value.\n             If this value is 0, OOV inputs will cause an error when calling\n-            the layer. Defaults to 1.\n+            the layer. Defaults to `1`.\n         mask_token: A token that represents masked inputs. When `output_mode` is\n             `\"int\"`, the token is included in vocabulary and mapped to index 0.\n             In other output modes, the token will not appear\n\n@@ -34,7 +34,7 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n             `channels_first`. When unspecified, uses\n             `image_data_format` value found in your Keras config file at\n             `~/.keras/keras.json` (if exists) else 'channels_last'.\n-            Defaults to 'channels_last'.\n+            Defaults to `'channels_last'`.\n         dilation_rate: An integer or tuple/list of n integers, specifying the\n             dilation rate to use for dilated convolution.\n             Currently, specifying any `dilation_rate` value != 1 is\n@@ -405,7 +405,7 @@ class ConvLSTM(RNN):\n             When unspecified, uses\n             `image_data_format` value found in your Keras config file at\n             `~/.keras/keras.json` (if exists) else 'channels_last'.\n-            Defaults to 'channels_last'.\n+            Defaults to `'channels_last'`.\n         dilation_rate: An integer or tuple/list of n integers, specifying\n             the dilation rate to use for dilated convolution.\n             Currently, specifying any `dilation_rate` value != 1 is\n\n@@ -165,7 +165,7 @@ class CosineSimilarity(LossFunctionWrapper):\n \n     Args:\n         axis: The axis along which the cosine similarity is computed\n-            (the features axis). Defaults to -1.\n+            (the features axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n@@ -395,7 +395,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n             squeezes the labels towards 0.5. Larger values of\n             `label_smoothing` correspond to heavier smoothing.\n         axis: The axis along which to compute crossentropy (the features axis).\n-            Defaults to -1.\n+            Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n@@ -676,7 +676,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             `0.1`, use `0.1 / num_classes` for non-target labels and\n             `0.9 + 0.1 / num_classes` for target labels.\n         axis: The axis along which to compute crossentropy (the features\n-            axis). Defaults to -1.\n+            axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n@@ -807,7 +807,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             `0.1`, use `0.1 / num_classes` for non-target labels and\n             `0.9 + 0.1 / num_classes` for target labels.\n         axis: The axis along which to compute crossentropy (the features\n-            axis). Defaults to -1.\n+            axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n             Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n@@ -1295,7 +1295,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     Args:\n         y_true: Tensor of true targets.\n         y_pred: Tensor of predicted targets.\n-        axis: Axis along which to determine similarity. Defaults to -1.\n+        axis: Axis along which to determine similarity. Defaults to `-1`.\n \n     Returns:\n         Cosine similarity tensor.\n@@ -1343,7 +1343,7 @@ def huber(y_true, y_pred, delta=1.0):\n         y_true: tensor of true targets.\n         y_pred: tensor of predicted targets.\n         delta: A float, the point where the Huber loss function changes from a\n-            quadratic to linear. Defaults to 1.\n+            quadratic to linear. Defaults to `1.0`.\n \n     Returns:\n         Tensor with one scalar loss entry per sample.\n@@ -1517,7 +1517,7 @@ def categorical_crossentropy(\n         label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n             example, if `0.1`, use `0.1 / num_classes` for non-target labels\n             and `0.9 + 0.1 / num_classes` for target labels.\n-        axis: Defaults to -1. The dimension along which the entropy is\n+        axis: Defaults to `-1`. The dimension along which the entropy is\n             computed.\n \n     Returns:\n@@ -1595,7 +1595,7 @@ def categorical_focal_crossentropy(\n         label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n             example, if `0.1`, use `0.1 / num_classes` for non-target labels\n             and `0.9 + 0.1 / num_classes` for target labels.\n-        axis: Defaults to -1. The dimension along which the entropy is\n+        axis: Defaults to `-1`. The dimension along which the entropy is\n             computed.\n \n     Returns:\n@@ -1678,7 +1678,7 @@ def sparse_categorical_crossentropy(\n             problems featuring a \"void\" class (commonly -1 or 255) in\n             segmentation maps. By default (`ignore_class=None`), all classes are\n             considered.\n-        axis: Defaults to -1. The dimension along which the entropy is\n+        axis: Defaults to `-1`. The dimension along which the entropy is\n             computed.\n \n     Returns:\n@@ -1741,7 +1741,7 @@ def binary_crossentropy(\n             squeezing them towards 0.5, that is,\n             using `1. - 0.5 * label_smoothing` for the target class\n             and `0.5 * label_smoothing` for the non-target class.\n-        axis: The axis along which the mean is computed. Defaults to -1.\n+        axis: The axis along which the mean is computed. Defaults to `-1`.\n \n     Returns:\n         Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n\n@@ -326,7 +326,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Args:\n         k: (Optional) Number of top elements to look at for computing accuracy.\n-            Defaults to 5.\n+            Defaults to `5`.\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n@@ -402,7 +402,7 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Args:\n         k: (Optional) Number of top elements to look at for computing accuracy.\n-            Defaults to 5.\n+            Defaults to `5`.\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n\n@@ -16,11 +16,11 @@ class _ConfusionMatrixConditionCount(Metric):\n     Args:\n         confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix`\n             conditions.\n-        thresholds: (Optional) Defaults to 0.5. A float value or a python list /\n-            tuple of float threshold values in `[0, 1]`. A threshold is compared\n-            with prediction values to determine the truth value of predictions\n-            (i.e., above the threshold is `True`, below is `False`). One metric\n-            value is generated for each threshold value.\n+        thresholds: (Optional) Defaults to `0.5`. A float value or a python list\n+            / tuple of float threshold values in `[0, 1]`. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `True`, below is `False`).\n+            One metric value is generated for each threshold value.\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n     \"\"\"\n@@ -49,7 +49,7 @@ class _ConfusionMatrixConditionCount(Metric):\n         Args:\n             y_true: The ground truth values.\n             y_pred: The predicted values.\n-            sample_weight: Optional weighting of each example. Defaults to 1.\n+            sample_weight: Optional weighting of each example. Defaults to `1`.\n                 Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n@@ -87,7 +87,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python\n             list/tuple of float threshold values in `[0, 1]`. A threshold is\n             compared with prediction values to determine the truth value of\n             predictions (i.e., above the threshold is `True`, below is `False`).\n@@ -131,7 +131,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python\n             list/tuple of float threshold values in `[0, 1]`. A threshold is\n             compared with prediction values to determine the truth value of\n             predictions (i.e., above the threshold is `True`, below is `False`).\n@@ -175,7 +175,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python\n             list/tuple of float threshold values in `[0, 1]`. A threshold is\n             compared with prediction values to determine the truth value of\n             predictions (i.e., above the threshold is `True`, below is `False`).\n@@ -219,7 +219,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python\n             list/tuple of float threshold values in `[0, 1]`. A threshold is\n             compared with prediction values to determine the truth value of\n             predictions (i.e., above the threshold is `True`, below is `False`).\n@@ -368,7 +368,7 @@ class Precision(Metric):\n                 `y_pred`. Will be cast to `bool`.\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n-            sample_weight: Optional weighting of each example. Defaults to 1.\n+            sample_weight: Optional weighting of each example. Defaults to `1`.\n                 Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n@@ -509,7 +509,7 @@ class Recall(Metric):\n                 `y_pred`. Will be cast to `bool`.\n             y_pred: The predicted values. Each element must be in the range\n                 `[0, 1]`.\n-            sample_weight: Optional weighting of each example. Defaults to 1.\n+            sample_weight: Optional weighting of each example. Defaults to `1`.\n                 Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n@@ -607,7 +607,7 @@ class SensitivitySpecificityBase(Metric):\n         Args:\n             y_true: The ground truth values.\n             y_pred: The predicted values.\n-            sample_weight: Optional weighting of each example. Defaults to 1.\n+            sample_weight: Optional weighting of each example. Defaults to `1`.\n                 Can be a tensor whose rank is either 0, or the same rank as\n                 `y_true`, and must be broadcastable to `y_true`.\n         \"\"\"\n\n@@ -23,7 +23,7 @@ class FBetaScore(Metric):\n         average: Type of averaging to be performed across per-class results\n             in the multi-class case.\n             Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n-            `\"weighted\"`. Default value is `None`.\n+            `\"weighted\"`. Defaults to `None`.\n             If `None`, no averaging is performed and `result()` will return\n             the score for each class.\n             If `\"micro\"`, compute metrics globally by counting the total\n@@ -38,7 +38,7 @@ class FBetaScore(Metric):\n             It can result in an score that is not between precision and recall.\n         beta: Determines the weight of given to recall\n             in the harmonic mean between precision and recall (see pseudocode\n-            equation above). Default value is 1.\n+            equation above). Defaults to `1`.\n         threshold: Elements of `y_pred` greater than `threshold` are\n             converted to be 1, and the rest 0. If `threshold` is\n             `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.\n@@ -261,7 +261,7 @@ class F1Score(FBetaScore):\n     Args:\n         average: Type of averaging to be performed on data.\n             Acceptable values are `None`, `\"micro\"`, `\"macro\"`\n-            and `\"weighted\"`. Default value is `None`.\n+            and `\"weighted\"`. Defaults to `None`.\n             If `None`, no averaging is performed and `result()` will return\n             the score for each class.\n             If `\"micro\"`, compute metrics globally by counting the total\n\n@@ -186,7 +186,7 @@ class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n             on label values are relaxed. e.g. `label_smoothing=0.2` means\n             that we will use a value of 0.1 for label\n             \"0\" and 0.9 for label \"1\".\n-        axis: (Optional) Defaults to -1.\n+        axis: (Optional) Defaults to `-1`.\n             The dimension along which entropy is computed.\n \n     Examples:\n@@ -271,7 +271,7 @@ class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n         from_logits: (Optional) Whether output is expected\n             to be a logits tensor. By default, we consider that output\n             encodes a probability distribution.\n-        axis: (Optional) Defaults to -1.\n+        axis: (Optional) Defaults to `-1`.\n             The dimension along which entropy is computed.\n \n     Examples:\n\n@@ -259,7 +259,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n     Args:\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n-        axis: (Optional) Defaults to -1. The dimension along which the cosine\n+        axis: (Optional) Defaults to `-1`. The dimension along which the cosine\n             similarity is computed.\n \n     Examples:\n@@ -581,7 +581,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     Args:\n         y_true: Tensor of true targets.\n         y_pred: Tensor of predicted targets.\n-        axis: Axis along which to determine similarity. Defaults to -1.\n+        axis: Axis along which to determine similarity. Defaults to `-1`.\n \n     Returns:\n         Cosine similarity tensor.\n\n@@ -40,7 +40,7 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n             segments. If not specified, it is inferred from the maximum\n             value in `segment_ids`.\n         sorted: A boolean indicating whether `segment_ids` is sorted.\n-            Default is `False`.\n+            Defaults to`False`.\n \n     Returns:\n         A tensor containing the sum of segments, where each element\n@@ -93,7 +93,7 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n             segments. If not specified, it is inferred from the maximum\n             value in `segment_ids`.\n         sorted: A boolean indicating whether `segment_ids` is sorted.\n-            Default is `False`.\n+            Defaults to`False`.\n \n     Returns:\n         A tensor containing the max of segments, where each element\n@@ -141,7 +141,7 @@ def top_k(x, k, sorted=True):\n         x: Input tensor.\n         k: An integer representing the number of top elements to retrieve.\n         sorted: A boolean indicating whether to sort the output in\n-        descending order. Default is `True`.\n+        descending order. Defaults to`True`.\n \n     Returns:\n         A tuple containing two tensors. The first tensor contains the\n@@ -225,9 +225,9 @@ def logsumexp(x, axis=None, keepdims=False):\n         x: Input tensor.\n         axis: An integer or a tuple of integers specifying the axis/axes\n             along which to compute the sum. If `None`, the sum is computed\n-            over all elements. Default is `None`.\n+            over all elements. Defaults to`None`.\n         keepdims: A boolean indicating whether to keep the dimensions of\n-            the input tensor when computing the sum. Default is `False`.\n+            the input tensor when computing the sum. Defaults to`False`.\n \n     Returns:\n         A tensor containing the logarithm of the sum of exponentials of\n\n@@ -1231,8 +1231,8 @@ def one_hot(x, num_classes, axis=-1, dtype=None):\n         x : Integer tensor to be encoded. The shape can be\n             arbitrary, but the dtype should be integer.\n         num_classes: Number of classes for the one-hot encoding.\n-        axis: Axis along which the encoding is performed. Default is\n-            -1, which represents the last axis.\n+        axis: Axis along which the encoding is performed. Defaults to\n+            `-1`, which represents the last axis.\n         dtype: (Optional) Data type of the output tensor. If not\n             provided, it defaults to the default data type of the backend.\n \n@@ -1302,7 +1302,7 @@ def binary_crossentropy(target, output, from_logits=False):\n             probabilities.\n             Set it to `True` if `output` represents logits; otherwise,\n             set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n+            Defaults to`False`.\n \n     Returns:\n         Integer tensor: The computed binary cross-entropy loss between\n@@ -1377,10 +1377,10 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             probabilities.\n             Set it to `True` if `output` represents logits; otherwise,\n             set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n+            Defaults to`False`.\n         axis: (optional) The axis along which the categorical cross-entropy\n             is computed.\n-            Default is -1, which corresponds to the last dimension of\n+            Defaults to `-1`, which corresponds to the last dimension of\n             the tensors.\n \n     Returns:\n@@ -1465,10 +1465,10 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             or probabilities.\n             Set it to `True` if `output` represents logits; otherwise,\n             set it to `False` if `output` represents probabilities.\n-            Default is `False`.\n+            Defaults to`False`.\n         axis: (optional) The axis along which the sparse categorical\n             cross-entropy is computed.\n-            Default is -1, which corresponds to the last dimension\n+            Defaults to `-1`, which corresponds to the last dimension\n             of the tensors.\n \n     Returns:\n@@ -1545,7 +1545,7 @@ def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n         inputs: Tensor of integer labels to be converted to multi-hot vectors.\n         num_tokens: Integer, the total number of unique tokens or classes.\n         axis: (optional) Axis along which the multi-hot encoding should be\n-            added. Default is -1, which corresponds to the last dimension.\n+            added. Defaults to `-1`, which corresponds to the last dimension.\n         dtype: (optional) The data type of the resulting tensor. Default\n             is backend's float type.\n \n\n@@ -203,10 +203,10 @@ def shape_equal(shape1, shape2, axis=None, allow_none=True):\n         shape1: A list or tuple of integers for first shape to be compared.\n         shape2: A list or tuple of integers for second shape to be compared.\n         axis: An integer, list, or tuple of integers (optional):\n-            Axes to ignore during comparison. Default is `None`.\n+            Axes to ignore during comparison. Defaults to `None`.\n         allow_none (bool, optional): If `True`, allows `None` in a shape\n             to match any value in the corresponding position of the other shape.\n-            Default is `True`.\n+            Defaults to `True`.\n \n     Returns:\n         bool: `True` if shapes are considered equal based on the criteria,\n@@ -378,7 +378,7 @@ def all(x, axis=None, keepdims=False):\n             for the last to the first axis.\n         keepdims: If `True`, axes which are reduced are left in the result as\n             dimensions with size one. With this option, the result will\n-            broadcast correctly against the input array. Default is `False`.\n+            broadcast correctly against the input array. Defaults to`False`.\n \n     Returns:\n         The tensor containing the logical AND reduction over the `axis`.\n@@ -442,7 +442,7 @@ def any(x, axis=None, keepdims=False):\n             for the last to the first axis.\n         keepdims: If `True`, axes which are reduced are left in the result as\n             dimensions with size one. With this option, the result will\n-            broadcast correctly against the input array. Default is `False`.\n+            broadcast correctly against the input array. Defaults to`False`.\n \n     Returns:\n         The tensor containing the logical OR reduction over the `axis`.\n@@ -1047,7 +1047,7 @@ def argsort(x, axis=-1):\n \n     Args:\n         x: Input tensor.\n-        axis: Axis along which to sort. Default is `-1` (the last axis). If\n+        axis: Axis along which to sort. Defaults to`-1` (the last axis). If\n             `None`, the flattened tensor is used.\n \n     Returns:\n@@ -1416,7 +1416,7 @@ def concatenate(xs, axis=0):\n \n     Args:\n         xs: The sequence of tensors to concatenate.\n-        axis: The axis along which the tensors will be joined. Defaults to 0.\n+        axis: The axis along which the tensors will be joined. Defaults to `0`.\n \n     Returns:\n         The concatenated tensor.\n@@ -1652,8 +1652,8 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     Args:\n         x1: Components of the first vector(s).\n         x2: Components of the second vector(s).\n-        axisa: Axis of `x1` that defines the vector(s). Defaults to -1.\n-        axisb: Axis of `x2` that defines the vector(s). Defaults to -1.\n+        axisa: Axis of `x1` that defines the vector(s). Defaults to `-1`.\n+        axisb: Axis of `x2` that defines the vector(s). Defaults to `-1`.\n         axisc: Axis of the result containing the cross product vector(s).\n             Ignored if both input vectors have dimension 2, as the return is\n             scalar. By default, the last axis.\n@@ -1796,7 +1796,7 @@ def diag(x, k=0):\n     Args:\n         x: Input tensor. If `x` is 2-D, returns the k-th diagonal of `x`.\n             If `x` is 1-D, return a 2-D tensor with `x` on the k-th diagonal.\n-        k: The diagonal to consider. Defaults to 0. Use `k > 0` for diagonals\n+        k: The diagonal to consider. Defaults to `0`. Use `k > 0` for diagonals\n             above the main diagonal, and `k < 0` for diagonals below\n             the main diagonal.\n \n@@ -1888,11 +1888,11 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     Args:\n         x: Input tensor.\n         offset: Offset of the diagonal from the main diagonal.\n-            Can be positive or negative. Defaults to 0 (main diagonal).\n+            Can be positive or negative. Defaults to `0`.(main diagonal).\n         axis1: Axis to be used as the first axis of the 2-D sub-arrays.\n-            Defaults to 0 (first axis).\n+            Defaults to `0`.(first axis).\n         axis2: Axis to be used as the second axis of the 2-D sub-arrays.\n-            Defaults to 1 (second axis).\n+            Defaults to `1` (second axis).\n \n     Returns:\n         Tensor of diagonals.\n@@ -2994,15 +2994,15 @@ def linspace(\n             `False`. In that case, the sequence consists of all but the last\n             of `num + 1` evenly spaced samples, so that `stop` is excluded.\n             Note that the step size changes when `endpoint` is `False`.\n-        num: Number of samples to generate. Default is 50. Must be\n+        num: Number of samples to generate. Defaults to `50`. Must be\n             non-negative.\n         endpoint: If `True`, `stop` is the last sample. Otherwise, it is\n-            not included. Default is `True`.\n+            not included. Defaults to`True`.\n         retstep: If `True`, return `(samples, step)`, where `step` is the\n             spacing between samples.\n         dtype: The type of the output tensor.\n         axis: The axis in the result to store the samples. Relevant only if\n-            start or stop are array-like. Default is 0.\n+            start or stop are array-like. Defaults to `0`.\n \n     Note:\n         Torch backend does not support `axis` argument.\n@@ -3304,10 +3304,10 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n             In that case, `num + 1` values are spaced over the interval in\n             log-space, of which all but the last (a sequence of length `num`)\n             are returned.\n-        num: Number of samples to generate. Default is 50.\n+        num: Number of samples to generate. Defaults to `50`.\n         endpoint: If `True`, `stop` is the last sample. Otherwise, it is not\n-            included. Default is `True`.\n-        base: The base of the log space. Default is 10\n+            included. Defaults to`True`.\n+        base: The base of the log space. Defaults to `10`.\n         dtype: The type of the output tensor.\n         axis: The axis in the result to store the samples. Relevant only\n             if start or stop are array-like.\n@@ -3422,8 +3422,8 @@ def max(x, axis=None, keepdims=False, initial=None):\n         axis: Axis or axes along which to operate. By default, flattened input\n             is used.\n         keepdims: If this is set to `True`, the axes which are reduced are left\n-            in the result as dimensions with size one. Default is `False`.\n-        initial: The minimum value of an output element. Default is `None`.\n+            in the result as dimensions with size one. Defaults to`False`.\n+        initial: The minimum value of an output element. Defaults to`None`.\n \n     Returns:\n         Maximum of `x`.\n@@ -3561,8 +3561,8 @@ def min(x, axis=None, keepdims=False, initial=None):\n         axis: Axis or axes along which to operate. By default, flattened input\n             is used.\n         keepdims: If this is set to `True`, the axes which are reduced are left\n-            in the result as dimensions with size one. Default is `False`.\n-        initial: The maximum value of an output element. Default is `None`.\n+            in the result as dimensions with size one. Defaults to`False`.\n+        initial: The maximum value of an output element. Defaults to`None`.\n \n     Returns:\n         Minimum of `x`.\n@@ -3950,7 +3950,7 @@ def pad(x, pad_width, mode=\"constant\"):\n         mode: One of `\"constant\"`, `\"edge\"`, `\"linear_ramp\"`,\n             `\"maximum\"`, `\"mean\"`, `\"median\"`, `\"minimum\"`,\n             `\"reflect\"`, `\"symmetric\"`, `\"wrap\"`, `\"empty\"`,\n-            `\"circular\"`. Default is `\"constant\"`.\n+            `\"circular\"`. Defaults to`\"constant\"`.\n \n     Note:\n         Torch backend only supports modes `\"constant\"`, `\"reflect\"`,\n@@ -4236,7 +4236,7 @@ def round(x, decimals=0):\n \n     Args:\n         x: Input tensor.\n-        decimals: Number of decimal places to round to. Default is 0.\n+        decimals: Number of decimal places to round to. Defaults to `0`.\n \n     Returns:\n         Output tensor.\n@@ -4357,7 +4357,7 @@ def sort(x, axis=-1):\n     Args:\n         x: Input tensor.\n         axis: Axis along which to sort. If `None`, the tensor is flattened\n-            before sorting. Default is the last axis.\n+            before sorting. Defaults to `-1`; the last axis.\n \n     Returns:\n         Sorted tensor.\n@@ -4425,7 +4425,7 @@ def split(x, indices_or_sections, axis=0):\n             equal sections along `axis`. If a 1-D array of sorted integers,\n             the entries indicate indices at which the tensor will be split\n             along `axis`.\n-        axis: Axis along which to split. Default is 0.\n+        axis: Axis along which to split. Defaults to `0`.\n \n     Note:\n         A split does not have to result in equal division when using\n@@ -4477,7 +4477,7 @@ def stack(x, axis=0):\n \n     Args:\n         x: A sequence of tensors.\n-        axis: Axis along which to stack. Default is 0.\n+        axis: Axis along which to stack. Defaults to `0`.\n \n     Returns:\n         The stacked tensor.\n@@ -4857,11 +4857,11 @@ def trace(x, offset=0, axis1=0, axis2=1):\n     Args:\n         x: Input tensor.\n         offset: Offset of the diagonal from the main diagonal. Can be\n-            both positive and negative. Defaults to 0.\n+            both positive and negative. Defaults to `0`.\n         axis1: Axis to be used as the first axis of the 2-D sub-arrays.\n-            Defaults to 0 (first axis).\n+            Defaults to `0`.(first axis).\n         axis2: Axis to be used as the second axis of the 2-D sub-arrays.\n-            Defaults to 1 (second axis).\n+            Defaults to `1` (second axis).\n \n     Returns:\n         If `x` is 2-D, the sum of the diagonal is returned. If `x` has\n@@ -4923,7 +4923,7 @@ def tril(x, k=0):\n \n     Args:\n         x: Input tensor.\n-        k: Diagonal above which to zero elements. Defaults to 0, the\n+        k: Diagonal above which to zero elements. Defaults to `0`. the\n             main diagonal. `k < 0` is below it, and `k > 0` is above it.\n \n     Returns:\n@@ -4955,7 +4955,7 @@ def triu(x, k=0):\n \n     Args:\n         x: Input tensor.\n-        k: Diagonal below which to zero elements. Defaults to 0, the\n+        k: Diagonal below which to zero elements. Defaults to `0`. the\n             main diagonal. `k < 0` is below it, and `k > 0` is above it.\n \n     Returns:\n\n@@ -24,11 +24,11 @@ class Adadelta(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001. Note that `Adadelta`\n+            use. The learning rate. Defaults to `0.001`. Note that `Adadelta`\n             tends to benefit from higher initial learning rate values compared\n             to other optimizers. To match the exact form in the original paper,\n             use 1.0.\n-        rho: A floating point value. The decay rate. Defaults to 0.95.\n+        rho: A floating point value. The decay rate. Defaults to `0.95`.\n         epsilon: Small floating point value for maintaining numerical stability.\n         {{base_optimizer_keyword_args}}\n \n\n@@ -20,7 +20,7 @@ class Adafactor(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n         epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n             away from 0.\n\n@@ -17,10 +17,10 @@ class Adagrad(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001. Note that `Adagrad` tends\n-            to benefit from higher initial learning rate values compared to\n-            other optimizers. To match the exact form in the original paper,\n-            use 1.0.\n+            use. The learning rate. Defaults to `0.001`. Note that `Adagrad`\n+            tends to benefit from higher initial learning rate values compared\n+            to other optimizers. To match the exact form in the original paper,\n+            use `1.0`.\n         initial_accumulator_value: Floating point value. Starting value for the\n             accumulators (per-parameter momentum values). Must be non-negative.\n         epsilon: Small floating point value for maintaining numerical stability.\n\n@@ -21,7 +21,7 @@ class Adam(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates. Defaults to\n\n@@ -37,7 +37,7 @@ class Adamax(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor. The exponential decay\n             rate for the 1st moment estimates.\n         beta_2: A float value or a constant float tensor. The exponential decay\n\n@@ -24,15 +24,15 @@ class AdamW(adam.Adam):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n-            Defaults to 0.9.\n+            Defaults to `0.9`.\n         beta_2: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 2nd moment estimates.\n-            Defaults to 0.999.\n+            Defaults to `0.999`.\n         epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just\n             before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n\n@@ -55,7 +55,7 @@ class Ftrl(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         learning_rate_power: A float value, must be less or equal to zero.\n             Controls how the learning rate decreases during training. Use zero\n             for a fixed learning rate.\n@@ -71,7 +71,7 @@ class Ftrl(optimizer.Optimizer):\n             magnitude penalty. When input is sparse shrinkage will only happen\n             on the active weights.\n         beta: A float value, representing the beta value from the paper.\n-            Defaults to 0.0.\n+            Defaults to `0.0`.\n         {{base_optimizer_keyword_args}}\n     \"\"\"\n \n\n@@ -15,7 +15,7 @@ class Nadam(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates.\n\n@@ -21,7 +21,7 @@ class RMSprop(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n         rho: float, defaults to 0.9. Discounting factor for the old gradients.\n         momentum: float, defaults to 0.0. If not 0.0., the optimizer tracks the\n             momentum value, with a decay rate equals to `1 - momentum`.\n\n@@ -31,10 +31,10 @@ class SGD(optimizer.Optimizer):\n         learning_rate: A float, a\n             `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.01.\n+            use. The learning rate. Defaults to `0.01`.\n         momentum: float hyperparameter >= 0 that accelerates gradient descent in\n-            the relevant direction and dampens oscillations. Defaults to 0,\n-            i.e., vanilla gradient descent.\n+            the relevant direction and dampens oscillations. 0 is vanilla\n+            gradient descent. Defaults to `0.0`.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n             Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n\n@@ -291,7 +291,7 @@ class OrthogonalRegularizer(Regularizer):\n             will be proportional to `factor` times the mean of the dot products\n             between the L2-normalized rows (if `mode=\"rows\"`, or columns if\n             `mode=\"columns\"`) of the inputs, excluding the product of each\n-            row/column with itself.  Defaults to 0.01.\n+            row/column with itself.  Defaults to `0.01`.\n         mode: String, one of `{\"rows\", \"columns\"}`. Defaults to `\"rows\"`. In\n             rows mode, the regularization effect seeks to make the rows of the\n             input orthogonal to each other. In columns mode, it seeks to make\n\n@@ -57,9 +57,9 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n             changed it, it defaults to `\"channels_last\"`).\n         scale: Whether to rescale the image such that minimum and maximum values\n             are 0 and 255 respectively. Defaults to `True`.\n-        dtype: Dtype to use. Default to `None`, in which case the global setting\n+        dtype: Dtype to use. `None` means the global setting\n             `keras_core.backend.floatx()` is used (unless you changed it, it\n-            defaults to `\"float32\"`).\n+            defaults to `\"float32\"`). Defaults to `None`.\n \n     Returns:\n         A PIL Image instance.\n@@ -131,7 +131,7 @@ def img_to_array(img, data_format=None, dtype=None):\n             `\"channels_last\"`. Defaults to `None`, in which case the global\n             setting `keras_core.backend.image_data_format()` is used (unless you\n             changed it, it defaults to `\"channels_last\"`).\n-        dtype: Dtype to use. Default to `None`, in which case the global setting\n+        dtype: Dtype to use. `None` means the global setting\n             `keras_core.backend.floatx()` is used (unless you changed it, it\n             defaults to `\"float32\"`).\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
