{"custom_id": "keras#1a45e5cd1765da3a58def1593958cd9bf86186f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 330 | Lines Deleted: 1 | Files Changed: 7 | Hunks: 11 | Methods Changed: 27 | Complexity Δ (Sum/Max): 27/10 | Churn Δ: 331 | Churn Cumulative: 470 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 10 | DMM Complexity: 0.9919354838709677\n\nDIFF:\n@@ -0,0 +1,112 @@\n+import jax\n+import numpy as np\n+\n+from keras_core import Model\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import optimizers\n+\n+\n+class MyDense(layers.Layer):\n+    def __init__(self, units, name=None):\n+        super().__init__(name=name)\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        input_dim = input_shape[-1]\n+        w_shape = (input_dim, self.units)\n+        w_value = initializers.GlorotUniform()(w_shape)\n+        self.w = backend.Variable(w_value, name=\"kernel\")\n+\n+        b_shape = (self.units,)\n+        b_value = initializers.Zeros()(b_shape)\n+        self.b = backend.Variable(b_value, name=\"bias\")\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+class MyModel(Model):\n+    def __init__(self, hidden_dim, output_dim):\n+        super().__init__()\n+        self.dense1 = MyDense(hidden_dim)\n+        self.dense2 = MyDense(hidden_dim)\n+        self.dense3 = MyDense(output_dim)\n+\n+    def call(self, x):\n+        x = self.dense1(x)\n+        x = self.dense2(x)\n+        return self.dense3(x)\n+\n+\n+def Dataset():\n+    for _ in range(20):\n+        yield (np.random.random((32, 128)), np.random.random((32, 16)))\n+\n+\n+def loss_fn(y_true, y_pred):\n+    return ops.sum((y_true - y_pred) ** 2)\n+\n+\n+model = MyModel(hidden_dim=256, output_dim=16)\n+\n+optimizer = optimizers.SGD(learning_rate=0.0001)\n+dataset = Dataset()\n+\n+# # Build model\n+x = ops.convert_to_tensor(np.random.random((1, 128)))\n+model(x)\n+# Build optimizer\n+optimizer.build(model.trainable_variables)\n+\n+\n+def compute_loss_and_updates(\n+    trainable_variables, non_trainable_variables, x, y\n+):\n+    y_pred, non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x\n+    )\n+\n+    loss = loss_fn(y, y_pred)\n+    return loss, non_trainable_variables\n+\n+\n+grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n+\n+\n+@jax.jit\n+def train_step(state, data):\n+    trainable_variables, non_trainable_variables, optimizer_variables = state\n+    x, y = data\n+    (loss, non_trainable_variables), grads = grad_fn(\n+        trainable_variables, non_trainable_variables, x, y\n+    )\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        grads, trainable_variables, optimizer_variables\n+    )\n+    # Return updated state\n+    return loss, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    )\n+\n+\n+trainable_variables = model.trainable_variables\n+non_trainable_variables = model.non_trainable_variables\n+optimizer_variables = optimizer.variables\n+state = trainable_variables, non_trainable_variables, optimizer_variables\n+# Training loop\n+for data in dataset:\n+    loss, state = train_step(state, data)\n+    print(\"Loss:\", loss)\n+\n+# Post-processing model state update\n+for variable, value in zip(model.trainable_variables, trainable_variables):\n+    variable.assign(value)\n+for variable, value in zip(\n+    model.non_trainable_variables, non_trainable_variables\n+):\n+    variable.assign(value)\n\n@@ -0,0 +1,64 @@\n+import numpy as np\n+\n+from keras_core import Model\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import losses\n+from keras_core import metrics\n+from keras_core import operations as ops\n+from keras_core import optimizers\n+\n+\n+class MyDense(layers.Layer):\n+    def __init__(self, units, name=None):\n+        super().__init__(name=name)\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        input_dim = input_shape[-1]\n+        w_shape = (input_dim, self.units)\n+        w_value = initializers.GlorotUniform()(w_shape)\n+        self.w = backend.Variable(w_value, name=\"kernel\")\n+\n+        b_shape = (self.units,)\n+        b_value = initializers.Zeros()(b_shape)\n+        self.b = backend.Variable(b_value, name=\"bias\")\n+\n+    def call(self, inputs):\n+        # Use Keras ops to create backend-agnostic layers/metrics/etc.\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+class MyModel(Model):\n+    def __init__(self, hidden_dim, output_dim):\n+        super().__init__()\n+        self.dense1 = MyDense(hidden_dim)\n+        self.dense2 = MyDense(hidden_dim)\n+        self.dense3 = MyDense(output_dim)\n+\n+    def call(self, x):\n+        x1 = self.dense1(x)\n+        x2 = self.dense2(x)\n+        # Why not use some ops here as well\n+        x = ops.concatenate([x1, x2], axis=-1)\n+        return self.dense3(x)\n+\n+\n+model = MyModel(hidden_dim=256, output_dim=16)\n+model.summary()\n+\n+x = np.random.random((50000, 128))\n+y = np.random.random((50000, 16))\n+batch_size = 32\n+epochs = 10\n+\n+model.compile(\n+    optimizer=optimizers.SGD(learning_rate=0.001),\n+    loss=losses.MeanSquaredError(),\n+    metrics=[metrics.MeanSquaredError()],\n+)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+\n+print(\"History:\")\n+print(history.history)\n\n@@ -0,0 +1,78 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import Model\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import optimizers\n+\n+\n+class MyDense(layers.Layer):\n+    def __init__(self, units, name=None):\n+        super().__init__(name=name)\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        input_dim = input_shape[-1]\n+        w_shape = (input_dim, self.units)\n+        w_value = initializers.GlorotUniform()(w_shape)\n+        self.w = backend.Variable(w_value, name=\"kernel\")\n+\n+        b_shape = (self.units,)\n+        b_value = initializers.Zeros()(b_shape)\n+        self.b = backend.Variable(b_value, name=\"bias\")\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+class MyModel(Model):\n+    def __init__(self, hidden_dim, output_dim):\n+        super().__init__()\n+        self.dense1 = MyDense(hidden_dim)\n+        self.dense2 = MyDense(hidden_dim)\n+        self.dense3 = MyDense(output_dim)\n+\n+    def call(self, x):\n+        x = self.dense1(x)\n+        x = self.dense2(x)\n+        return self.dense3(x)\n+\n+\n+def Dataset():\n+    for _ in range(20):\n+        yield (\n+            np.random.random((32, 128)).astype(\"float32\"),\n+            np.random.random((32, 16)).astype(\"float32\"),\n+        )\n+\n+\n+def loss_fn(y_true, y_pred):\n+    return ops.sum((y_true - y_pred) ** 2)\n+\n+\n+model = MyModel(hidden_dim=256, output_dim=16)\n+\n+optimizer = optimizers.SGD(learning_rate=0.0001)\n+dataset = Dataset()\n+\n+\n+@tf.function(jit_compile=True)\n+def train_step(data):\n+    x, y = data\n+    with tf.GradientTape() as tape:\n+        y_pred = model(x)\n+        loss = loss_fn(y, y_pred)\n+    # !! Glitch to be resolved !!\n+    gradients = tape.gradient(\n+        loss, [v.value for v in model.trainable_variables]\n+    )\n+    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n+    return loss\n+\n+\n+for data in dataset:\n+    loss = train_step(data)\n+    print(\"Loss:\", float(loss))\n\n@@ -0,0 +1,30 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import losses\n+from keras_core import metrics\n+from keras_core import optimizers\n+from keras_core.models import Functional\n+\n+inputs = layers.Input((128,), batch_size=32)\n+x = layers.Dense(256)(inputs)\n+x = layers.Dense(256)(x)\n+x = layers.Dense(256)(x)\n+outputs = layers.Dense(16)(x)\n+model = Functional(inputs, outputs)\n+model.summary()\n+\n+x = np.random.random((50000, 128))\n+y = np.random.random((50000, 16))\n+batch_size = 32\n+epochs = 10\n+\n+model.compile(\n+    optimizer=optimizers.SGD(learning_rate=0.001),\n+    loss=losses.MeanSquaredError(),\n+    metrics=[metrics.MeanSquaredError()],\n+)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+\n+print(\"History:\")\n+print(history.history)\n\n@@ -0,0 +1,39 @@\n+import numpy as np\n+\n+from keras_core import Model\n+from keras_core import layers\n+from keras_core import losses\n+from keras_core import metrics\n+from keras_core import optimizers\n+\n+\n+class MyModel(Model):\n+    def __init__(self, hidden_dim, output_dim):\n+        super().__init__()\n+        self.dense1 = layers.Dense(hidden_dim)\n+        self.dense2 = layers.Dense(hidden_dim)\n+        self.dense3 = layers.Dense(output_dim)\n+\n+    def call(self, x):\n+        x = self.dense1(x)\n+        x = self.dense2(x)\n+        return self.dense3(x)\n+\n+\n+model = MyModel(hidden_dim=256, output_dim=16)\n+model.summary()\n+\n+x = np.random.random((50000, 128))\n+y = np.random.random((50000, 16))\n+batch_size = 32\n+epochs = 10\n+\n+model.compile(\n+    optimizer=optimizers.SGD(learning_rate=0.001),\n+    loss=losses.MeanSquaredError(),\n+    metrics=[metrics.MeanSquaredError()],\n+)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+\n+print(\"History:\")\n+print(history.history)\n\n@@ -1,2 +1,5 @@\n from keras_core import backend\n+from keras_core import layers\n+from keras_core import models\n from keras_core import operations\n+from keras_core.models import Model\n\n@@ -274,6 +274,7 @@ class Layer(Operation):\n                     \"Only input tensors may be passed as \"\n                     \"positional arguments. The following argument value \"\n                     f\"should be passed as a keyword argument: {arg} \"\n+                    f\"(of type {type(arg)})\"\n                 )\n \n         # 4. Check input spec for 1st positional arg.\n@@ -420,7 +421,7 @@ class Layer(Operation):\n             losses.extend(layer._losses)\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n-            regularizer = getattr(v, \"regularizer\")\n+            regularizer = getattr(v, \"regularizer\", None)\n             if regularizer:\n                 weight_regularization_losses.append(regularizer(v))\n         losses.extend(weight_regularization_losses)\n@@ -552,9 +553,11 @@ class Layer(Operation):\n             if id(layer) in seen_object_ids:\n                 continue\n             seen_object_ids.add(id(layer))\n+            layers.append(layer)\n             # Introspect recursively through sublayers.\n             if recursive:\n                 deque.extendleft(layer._layers)\n+        return layers\n \n \n def get_arguments_dict(fn, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd144b60977a487f623e0dd06698ebee3da316fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 36 | Churn Cumulative: 707 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,23 +30,40 @@ class MyDense(layers.Layer):\n         return ops.matmul(inputs, self.w) + self.b\n     \n \n+class MyDropout(layers.Layer):\n+    def __init__(self, rate, name=None):\n+        super().__init__(name=name)\n+        self.rate = rate\n+        # Use seed_generator for managing RNG state.\n+        # It is a state element and its seed variable is\n+        # tracked as part of `layer.variables`.\n+        self.seed_generator = backend.random.SeedGenerator(1337)\n+\n+    def call(self, inputs):\n+        # Use `backend.random` for random ops.\n+        return backend.random.dropout(\n+            inputs, self.rate, seed=self.seed_generator\n+        )\n+\n+\n class MyModel(Model):\n     def __init__(self, hidden_dim, output_dim):\n         super().__init__()\n         self.dense1 = MyDense(hidden_dim)\n         self.dense2 = MyDense(hidden_dim)\n         self.dense3 = MyDense(output_dim)\n+        self.dp = MyDropout(0.5)\n \n     def call(self, x):\n         x1 = self.dense1(x)\n         x2 = self.dense2(x)\n         # Why not use some ops here as well\n         x = ops.concatenate([x1, x2], axis=-1)\n+        x = self.dp(x)\n         return self.dense3(x)\n \n \n model = MyModel(hidden_dim=256, output_dim=16)\n-model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n@@ -60,5 +77,7 @@ model.compile(\n )\n history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n \n+model.summary()\n+\n print(\"History:\")\n print(history.history)\n\n@@ -82,13 +82,13 @@ class MyModel(Layer):\n         super().__init__()\n         self.dense1 = MiniDense(units)\n         # self.bn = MiniBatchNorm()\n-        # self.dropout = MiniDropout(0.5)\n+        self.dropout = MiniDropout(0.5)\n         self.dense2 = MiniDense(num_classes)\n \n     def call(self, x):\n         x = self.dense1(x)\n         # x = self.bn(x)\n-        # x = self.dropout(x)\n+        x = self.dropout(x)\n         return self.dense2(x)\n \n \n\n@@ -71,6 +71,8 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n def dropout(inputs, rate, noise_shape=None, seed=None):\n     seed = draw_seed(seed)\n     keep_prob = 1.0 - rate\n+    if noise_shape is None:\n+        noise_shape = inputs.shape\n     mask = jax.random.bernoulli(seed, p=keep_prob, shape=noise_shape)\n     mask = jax.numpy.broadcast_to(mask, inputs.shape)\n     return jax.lax.select(\n\n@@ -49,6 +49,7 @@ class Layer(Operation):\n \n         self._layers = []\n         self._metrics = []\n+        self._seed_generators = []\n         self._losses = []\n         self._variables = []\n         self._trainable_variables = []\n@@ -71,7 +72,7 @@ class Layer(Operation):\n                     and not isinstance(x, Metric),\n                     self._layers,\n                 ),\n-                # TODO: SeedGenerator tracking\n+                \"seed_generators\": (lambda x: isinstance(x, backend.random.SeedGenerator), self._seed_generators),\n             }\n         )\n \n@@ -176,8 +177,12 @@ class Layer(Operation):\n \n     @property\n     def variables(self):\n-        # TODO: include not just weights by any variables (also from metrics, optimizers, SeedGenerators)\n+        # Includes weights, seed generator state, and metric variables.\n         variables = self.weights[:]\n+        for m in self._metrics:\n+            variables.extend(m.variables)\n+        for sg in self._seed_generators:\n+            variables.append(sg.state)\n         return variables\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6df67f2d261d6993f54247e9da10fc7955db052", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 207 | Lines Deleted: 142 | Files Changed: 15 | Hunks: 36 | Methods Changed: 25 | Complexity Δ (Sum/Max): 20/8 | Churn Δ: 349 | Churn Cumulative: 9575 | Contributors (this commit): 3 | Commits (past 90d): 93 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n import json\n import os\n \n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common import standardize_shape\n+from keras_core.backend.common.variables import standardize_dtype\n+from keras_core.backend.common.variables import standardize_shape\n+from keras_core.backend.config import backend\n from keras_core.backend.config import epsilon\n from keras_core.backend.config import floatx\n from keras_core.backend.config import image_data_format\n@@ -13,91 +14,16 @@ from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.keras_tensor import any_symbolic_tensors\n from keras_core.backend.keras_tensor import is_keras_tensor\n from keras_core.backend.stateless_scope import StatelessScope\n+from keras_core.backend.stateless_scope import get_stateless_scope\n+from keras_core.backend.stateless_scope import in_stateless_scope\n from keras_core.utils.io_utils import print_msg\n \n-# Set Keras base dir path given KERAS_HOME env variable, if applicable.\n-# Otherwise either ~/.keras or /tmp.\n-if \"KERAS_HOME\" in os.environ:\n-    _keras_dir = os.environ.get(\"KERAS_HOME\")\n-else:\n-    _keras_base_dir = os.path.expanduser(\"~\")\n-    if not os.access(_keras_base_dir, os.W_OK):\n-        _keras_base_dir = \"/tmp\"\n-    _keras_dir = os.path.join(_keras_base_dir, \".keras\")\n-\n-# Default backend: TensorFlow.\n-_BACKEND = \"tensorflow\"\n-\n-# Attempt to read Keras config file.\n-_config_path = os.path.expanduser(os.path.join(_keras_dir, \"keras.json\"))\n-if os.path.exists(_config_path):\n-    try:\n-        with open(_config_path) as f:\n-            _config = json.load(f)\n-    except ValueError:\n-        _config = {}\n-    _floatx = _config.get(\"floatx\", floatx())\n-    assert _floatx in {\"float16\", \"float32\", \"float64\"}\n-    _epsilon = _config.get(\"epsilon\", epsilon())\n-    assert isinstance(_epsilon, float)\n-    _backend = _config.get(\"backend\", _BACKEND)\n-    _image_data_format = _config.get(\"image_data_format\", image_data_format())\n-    assert _image_data_format in {\"channels_last\", \"channels_first\"}\n-\n-    set_floatx(_floatx)\n-    set_epsilon(_epsilon)\n-    set_image_data_format(_image_data_format)\n-    _BACKEND = _backend\n-\n-# Save config file, if possible.\n-if not os.path.exists(_keras_dir):\n-    try:\n-        os.makedirs(_keras_dir)\n-    except OSError:\n-        # Except permission denied and potential race conditions\n-        # in multi-threaded environments.\n-        pass\n-\n-if not os.path.exists(_config_path):\n-    _config = {\n-        \"floatx\": floatx(),\n-        \"epsilon\": epsilon(),\n-        \"backend\": _BACKEND,\n-        \"image_data_format\": image_data_format(),\n-    }\n-    try:\n-        with open(_config_path, \"w\") as f:\n-            f.write(json.dumps(_config, indent=4))\n-    except IOError:\n-        # Except permission denied.\n-        pass\n-\n-# Set backend based on KERAS_BACKEND flag, if applicable.\n-if \"KERAS_BACKEND\" in os.environ:\n-    _backend = os.environ[\"KERAS_BACKEND\"]\n-    if _backend:\n-        _BACKEND = _backend\n-\n # Import backend functions.\n-if _BACKEND == \"tensorflow\":\n+if backend() == \"tensorflow\":\n     print_msg(\"Using TensorFlow backend\")\n     from keras_core.backend.tensorflow import *\n-elif _BACKEND == \"jax\":\n+elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n     from keras_core.backend.jax import *\n else:\n-    raise ValueError(f\"Unable to import backend : {_BACKEND}\")\n-\n-\n-def backend():\n-    \"\"\"Publicly accessible method for determining the current backend.\n-\n-    Returns:\n-        String, the name of the backend Keras is currently using.\n-\n-    Example:\n-\n-    >>> keras.backend.backend()\n-    'tensorflow'\n-    \"\"\"\n-    return _BACKEND\n+    raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -0,0 +1,4 @@\n+from keras_core.backend.common import random\n+from keras_core.backend.common.variables import KerasVariable\n+from keras_core.backend.common.variables import standardize_dtype\n+from keras_core.backend.common.variables import standardize_shape\n\n@@ -1,7 +1,5 @@\n import random as python_random\n \n-from keras_core.backend import backend\n-\n \n class SeedGenerator:\n     def __init__(self, seed):\n@@ -37,9 +35,3 @@ def draw_seed(seed):\n         \"or an instance of `SeedGenerator`. \"\n         f\"Received: seed={seed} (of type {type(seed)})\"\n     )\n-\n-\n-if backend() == \"jax\":\n-    from keras_core.backend.jax.random import *\n-else:\n-    from keras_core.backend.tensorflow.random import *\n\n\n@@ -1,3 +1,8 @@\n+import json\n+import os\n+\n+from keras_core.utils.io_utils import print_msg\n+\n # The type of float to use throughout a session.\n _FLOATX = \"float32\"\n \n@@ -7,36 +12,8 @@ _EPSILON = 1e-7\n # Default image data format, one of \"channels_last\", \"channels_first\".\n _IMAGE_DATA_FORMAT = \"channels_last\"\n \n-\n-def epsilon():\n-    \"\"\"Return the value of the fuzz factor used in numeric expressions.\n-\n-    Returns:\n-        A float.\n-\n-    Example:\n-    >>> keras_core.backend.epsilon()\n-    1e-07\n-    \"\"\"\n-    return _EPSILON\n-\n-\n-def set_epsilon(value):\n-    \"\"\"Set the value of the fuzz factor used in numeric expressions.\n-\n-    Args:\n-        value: float. New value of epsilon.\n-\n-    Example:\n-    >>> keras_core.backend.epsilon()\n-    1e-07\n-    >>> keras_core.backend.set_epsilon(1e-5)\n-    >>> keras_core.backend.epsilon()\n-    1e-05\n-     >>> keras_core.backend.set_epsilon(1e-7)\n-    \"\"\"\n-    global _EPSILON\n-    _EPSILON = value\n+# Default backend: TensorFlow.\n+_BACKEND = \"tensorflow\"\n \n \n def floatx():\n@@ -88,6 +65,37 @@ def set_floatx(value):\n     _FLOATX = str(value)\n \n \n+def epsilon():\n+    \"\"\"Return the value of the fuzz factor used in numeric expressions.\n+\n+    Returns:\n+        A float.\n+\n+    Example:\n+    >>> keras_core.backend.epsilon()\n+    1e-07\n+    \"\"\"\n+    return _EPSILON\n+\n+\n+def set_epsilon(value):\n+    \"\"\"Set the value of the fuzz factor used in numeric expressions.\n+\n+    Args:\n+        value: float. New value of epsilon.\n+\n+    Example:\n+    >>> keras_core.backend.epsilon()\n+    1e-07\n+    >>> keras_core.backend.set_epsilon(1e-5)\n+    >>> keras_core.backend.epsilon()\n+    1e-05\n+     >>> keras_core.backend.set_epsilon(1e-7)\n+    \"\"\"\n+    global _EPSILON\n+    _EPSILON = value\n+\n+\n def image_data_format():\n     \"\"\"Return the default image data format convention.\n \n@@ -126,3 +134,79 @@ def set_image_data_format(data_format):\n             f\"Expected one of {accepted_formats}\"\n         )\n     _IMAGE_DATA_FORMAT = str(data_format)\n+\n+\n+# Set Keras base dir path given KERAS_HOME env variable, if applicable.\n+# Otherwise either ~/.keras or /tmp.\n+if \"KERAS_HOME\" in os.environ:\n+    _keras_dir = os.environ.get(\"KERAS_HOME\")\n+else:\n+    _keras_base_dir = os.path.expanduser(\"~\")\n+    if not os.access(_keras_base_dir, os.W_OK):\n+        _keras_base_dir = \"/tmp\"\n+    _keras_dir = os.path.join(_keras_base_dir, \".keras\")\n+\n+\n+# Attempt to read Keras config file.\n+_config_path = os.path.expanduser(os.path.join(_keras_dir, \"keras.json\"))\n+if os.path.exists(_config_path):\n+    try:\n+        with open(_config_path) as f:\n+            _config = json.load(f)\n+    except ValueError:\n+        _config = {}\n+    _floatx = _config.get(\"floatx\", floatx())\n+    assert _floatx in {\"float16\", \"float32\", \"float64\"}\n+    _epsilon = _config.get(\"epsilon\", epsilon())\n+    assert isinstance(_epsilon, float)\n+    _backend = _config.get(\"backend\", _BACKEND)\n+    _image_data_format = _config.get(\"image_data_format\", image_data_format())\n+    assert _image_data_format in {\"channels_last\", \"channels_first\"}\n+\n+    set_floatx(_floatx)\n+    set_epsilon(_epsilon)\n+    set_image_data_format(_image_data_format)\n+    _BACKEND = _backend\n+\n+# Save config file, if possible.\n+if not os.path.exists(_keras_dir):\n+    try:\n+        os.makedirs(_keras_dir)\n+    except OSError:\n+        # Except permission denied and potential race conditions\n+        # in multi-threaded environments.\n+        pass\n+\n+if not os.path.exists(_config_path):\n+    _config = {\n+        \"floatx\": floatx(),\n+        \"epsilon\": epsilon(),\n+        \"backend\": _BACKEND,\n+        \"image_data_format\": image_data_format(),\n+    }\n+    try:\n+        with open(_config_path, \"w\") as f:\n+            f.write(json.dumps(_config, indent=4))\n+    except IOError:\n+        # Except permission denied.\n+        pass\n+\n+# Set backend based on KERAS_BACKEND flag, if applicable.\n+if \"KERAS_BACKEND\" in os.environ:\n+    _backend = os.environ[\"KERAS_BACKEND\"]\n+    if _backend:\n+        _BACKEND = _backend\n+\n+\n+def backend():\n+    \"\"\"Publicly accessible method for determining the current backend.\n+\n+    Returns:\n+        String, the name of the backend Keras is currently using.\n+\n+    Example:\n+\n+    >>> keras.backend.backend()\n+    'tensorflow'\n+    \"\"\"\n+    return _BACKEND\n\n@@ -5,6 +5,8 @@ from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.jax import numpy\n+from keras_core.backend.jax import random\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.backend.stateless_scope import get_stateless_scope\n\n@@ -0,0 +1,25 @@\n+import jax.numpy as jnp\n+\n+\n+def add(x1, x2):\n+    return jnp.add(x1, x2)\n+\n+\n+def subtract(x1, x2):\n+    return jnp.subtract(x1, x2)\n+\n+\n+def matmul(x1, x2):\n+    return jnp.matmul(x1, x2)\n+\n+\n+def multiply(x1, x2):\n+    return jnp.multiply(x1, x2)\n+\n+\n+def mean(x, axis=None, keepdims=False):\n+    return jnp.mean(x, axis=axis, keepdims=keepdims)\n+\n+\n+def max(x, axis=None, keepdims=False):\n+    return jnp.max(x, axis=axis, keepdims=keepdims)\n\n@@ -1,7 +1,9 @@\n import jax\n \n-from keras_core.backend import floatx\n-from keras_core.backend.random import draw_seed\n+from keras_core.backend.common.random import SeedGenerator\n+from keras_core.backend.common.random import draw_seed\n+from keras_core.backend.common.random import make_default_seed\n+from keras_core.backend.config import floatx\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n\n@@ -2,8 +2,7 @@ import threading\n \n from keras_core.backend.common import KerasVariable\n \n-### Stateless context manager\n-\n+# Stateless context manager\n GLOBAL_SCOPE_TRACKER = threading.local()\n \n \n\n@@ -6,6 +6,8 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.tensorflow import numpy\n+from keras_core.backend.tensorflow import random\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = True\n\n@@ -0,0 +1,25 @@\n+from tensorflow.experimental import numpy as tfnp\n+\n+\n+def add(x1, x2):\n+    return tfnp.add(x1, x2)\n+\n+\n+def subtract(x1, x2):\n+    return tfnp.subtract(x1, x2)\n+\n+\n+def matmul(x1, x2):\n+    return tfnp.matmul(x1, x2)\n+\n+\n+def multiply(x1, x2):\n+    return tfnp.multiply(x1, x2)\n+\n+\n+def mean(x, axis=None, keepdims=False):\n+    return tfnp.mean(x, axis=axis, keepdims=keepdims)\n+\n+\n+def max(x, axis=None, keepdims=False):\n+    return tfnp.max(x, axis=axis, keepdims=keepdims)\n\n@@ -1,7 +1,9 @@\n import tensorflow as tf\n \n-from keras_core.backend import floatx\n-from keras_core.backend.random import draw_seed\n+from keras_core.backend.common.random import SeedGenerator\n+from keras_core.backend.common.random import draw_seed\n+from keras_core.backend.common.random import make_default_seed\n+from keras_core.backend.config import floatx\n \n \n def tf_draw_seed(seed):\n\n@@ -72,7 +72,10 @@ class Layer(Operation):\n                     and not isinstance(x, Metric),\n                     self._layers,\n                 ),\n-                \"seed_generators\": (lambda x: isinstance(x, backend.random.SeedGenerator), self._seed_generators),\n+                \"seed_generators\": (\n+                    lambda x: isinstance(x, backend.random.SeedGenerator),\n+                    self._seed_generators,\n+                ),\n             }\n         )\n \n\n@@ -243,7 +243,7 @@ def abs(x):\n \n class Add(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"add\", x1, x2)\n+        return backend.numpy.add(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -255,7 +255,7 @@ class Add(Operation):\n def add(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Add().symbolic_call(x1, x2)\n-    return backend.execute(\"add\", x1, x2)\n+    return backend.numpy.add(x1, x2)\n \n \n class All(Operation):\n@@ -1630,7 +1630,7 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n \n class Matmul(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"matmul\", x1, x2)\n+        return backend.numpy.matmul(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1666,7 +1666,7 @@ def matmul(x1, x2):\n     # The below conversion works around an outstanding JAX bug.\n     x1 = backend.convert_to_tensor(x1)\n     x2 = backend.convert_to_tensor(x2)\n-    return backend.execute(\"matmul\", x1, x2)\n+    return backend.numpy.matmul(x1, x2)\n \n \n class Max(Operation):\n@@ -1679,7 +1679,7 @@ class Max(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"max\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.max(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -1691,7 +1691,7 @@ class Max(Operation):\n def max(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Max(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"max\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.max(x, axis=axis, keepdims=keepdims)\n \n \n class Maximum(Operation):\n@@ -1879,6 +1879,7 @@ class Nonzero(Operation):\n     def call(self, x):\n         return backend.execute(\"nonzero\", x)\n \n+\n def nonzero(x):\n     return backend.execute(\"nonzero\", x)\n \n@@ -2664,7 +2665,7 @@ def where(condition, x1, x2):\n \n class Subtract(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"subtract\", x1, x2)\n+        return backend.numpy.subtract(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2676,12 +2677,12 @@ class Subtract(Operation):\n def subtract(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Subtract().symbolic_call(x1, x2)\n-    return backend.execute(\"subtract\", x1, x2)\n+    return backend.numpy.subtract(x1, x2)\n \n \n class Multiply(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"multiply\", x1, x2)\n+        return backend.numpy.multiply(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2693,7 +2694,7 @@ class Multiply(Operation):\n def multiply(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Multiply().symbolic_call(x1, x2)\n-    return backend.execute(\"multiply\", x1, x2)\n+    return backend.numpy.multiply(x1, x2)\n \n \n class Divide(Operation):\n@@ -2857,9 +2858,7 @@ class Mean(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\n-            \"mean\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return backend.numpy.mean(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -2871,7 +2870,7 @@ class Mean(Operation):\n def mean(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Mean(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"mean\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.mean(x, axis=axis, keepdims=keepdims)\n \n \n class Var(Operation):\n\n@@ -1,5 +1,5 @@\n-import pytest\n import numpy as np\n+import pytest\n from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import testing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3aa1d977b1cc58eb92a767a150564c57f9859bc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 119 | Churn Cumulative: 175 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,11 +5,12 @@ class SeedGenerator:\n     def __init__(self, seed):\n         from keras_core.backend import Variable\n \n+        if seed is None:\n+            seed = make_default_seed()\n         if not isinstance(seed, int):\n             raise ValueError(\n                 \"Argument `seed` must be an integer. \" f\"Received: seed={seed}\"\n             )\n-        seed = seed or make_default_seed()\n         self.state = Variable([seed, 0], dtype=\"uint32\", trainable=False)\n \n \n\n@@ -2,5 +2,4 @@ from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n from keras_core.layers.layer import Layer\n-\n-# from keras_core.layers.regularization.dropout import Dropout\n+from keras_core.layers.regularization.dropout import Dropout\n\n@@ -0,0 +1,77 @@\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.layers.Dropout\")\n+class Dropout(layers.Layer):\n+    \"\"\"Applies dropout to the input.\n+\n+    The `Dropout` layer randomly sets input units to 0 with a frequency of `rate`\n+    at each step during training time, which helps prevent overfitting.\n+    Inputs not set to 0 are scaled up by `1 / (1 - rate)` such that the sum over\n+    all inputs is unchanged.\n+\n+    Note that the `Dropout` layer only applies when `training` is set to `True`\n+    in `call()`, such that no values are dropped during inference.\n+    When using `model.fit`, `training` will be appropriately set to `True`\n+    automatically. In other contexts, you can set the argument explicitly\n+    to `True` when calling the layer.\n+\n+    (This is in contrast to setting `trainable=False` for a `Dropout` layer.\n+    `trainable` does not affect the layer's behavior, as `Dropout` does\n+    not have any variables/weights that can be frozen during training.)\n+\n+    Args:\n+        rate: Float between 0 and 1. Fraction of the input units to drop.\n+        noise_shape: 1D integer tensor representing the shape of the\n+            binary dropout mask that will be multiplied with the input.\n+            For instance, if your inputs have shape\n+            `(batch_size, timesteps, features)` and\n+            you want the dropout mask to be the same for all timesteps,\n+            you can use `noise_shape=(batch_size, 1, features)`.\n+        seed: A Python integer to use as random seed.\n+\n+    Call arguments:\n+        inputs: Input tensor (of any rank).\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (doing nothing).\n+    \"\"\"\n+\n+    def __init__(\n+        self, rate, noise_shape=None, seed=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+            raise ValueError(\n+                f\"Invalid value received for argument \"\n+                \"`rate`. Expected a float value between 0 and 1. \"\n+                f\"Received: rate={rate}\"\n+            )\n+        self.rate = rate\n+        self.seed = seed\n+        self.noise_shape = noise_shape\n+        self.seed_generator = backend.random.SeedGenerator(seed)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, training=False):\n+        if training and self.rate > 0:\n+            return backend.random.dropout(\n+                inputs,\n+                self.rate,\n+                noise_shape=self.noise_shape,\n+                seed=self.seed_generator,\n+            )\n+        return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"rate\": self.rate,\n+            \"seed\": self.seed,\n+            \"noise_shape\": self.noise_shape,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,34 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class DropoutTest(testing.TestCase):\n+    def test_dropout_supports_masking(self):\n+        dropout = layers.Dropout(0.5)\n+        self.assertEqual(True, dropout.supports_masking)\n+\n+    def test_dropout_rescaling(self):\n+        inputs = np.ones((20, 500))\n+        layer = layers.Dropout(0.5, seed=1337)\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(np.mean(outputs), 1.0, atol=0.02)\n+        self.assertAllClose(np.max(outputs), 2.0)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"Requires dynamic shapes\"\n+    )\n+    def test_dropout_partial_noise_shape_dynamic(self):\n+        inputs = np.ones((20, 5, 10))\n+        layer = layers.Dropout(0.5, noise_shape=(None, 1, None))\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, :], outputs[:, 1, :])\n+\n+    def test_dropout_partial_noise_shape_static(self):\n+        inputs = np.ones((20, 5, 10))\n+        layer = layers.Dropout(0.5, noise_shape=(20, 1, 10))\n+        outputs = layer(inputs, training=True)\n+        self.assertAllClose(outputs[:, 0, :], outputs[:, 1, :])\n\n@@ -1,8 +1,6 @@\n import numpy as np\n \n from keras_core import backend\n-from keras_core import initializers\n-from keras_core import operations as ops\n from keras_core import regularizers\n from keras_core import testing\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efe400a388d9c1cdee1a036dbf667297e31806f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 6 | Hunks: 18 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 1557 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n@@ -41,7 +41,7 @@ class Trainer(base_trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapters_utils.train_validation_split(\n+            ), validation_data = data_adapter_utils.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n@@ -50,7 +50,7 @@ class Trainer(base_trainer.Trainer):\n                 val_x,\n                 val_y,\n                 val_sample_weight,\n-            ) = data_adapters_utils.unpack_x_y_sample_weight(validation_data)\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Create an iterator that yields batches for one epoch.\n         epoch_iterator = EpochIterator(\n@@ -70,7 +70,7 @@ class Trainer(base_trainer.Trainer):\n                     x,\n                     y,\n                     sample_weight,\n-                ) = data_adapters_utils.unpack_x_y_sample_weight(data)\n+                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n                 # Build model\n                 y_pred = self(x)\n                 # Build metrics\n@@ -123,7 +123,7 @@ class Trainer(base_trainer.Trainer):\n                 optimizer_variables,\n                 metrics_variables,\n             ) = state\n-            x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(\n+            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n                 data\n             )\n             (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n\n@@ -3,13 +3,13 @@ import tensorflow as tf\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class Trainer(base_trainer.Trainer):\n     def train_step(self, data):\n-        x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Forward pass\n         with tf.GradientTape() as tape:\n@@ -93,7 +93,7 @@ class Trainer(base_trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapters_utils.train_validation_split(\n+            ), validation_data = data_adapter_utils.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n@@ -102,7 +102,7 @@ class Trainer(base_trainer.Trainer):\n                 val_x,\n                 val_y,\n                 val_sample_weight,\n-            ) = data_adapters_utils.unpack_x_y_sample_weight(validation_data)\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Create an iterator that yields batches for one epoch.\n         epoch_iterator = EpochIterator(\n\n@@ -5,7 +5,7 @@ import tensorflow as tf\n from tensorflow import nest\n \n from keras_core import backend\n-from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n \n try:\n@@ -30,7 +30,7 @@ class ArrayDataAdapter(DataAdapter):\n         types_struct = nest.map_structure(lambda x: type(x), x)\n         flat_types = nest.flatten(types_struct)\n         if not all(\n-            issubclass(c, data_adapters_utils.ARRAY_TYPES) for c in flat_types\n+            issubclass(c, data_adapter_utils.ARRAY_TYPES) for c in flat_types\n         ):\n             raise ValueError(\n                 \"Expected all elements of `x` to be array-like. Received invalid types: \"\n@@ -80,9 +80,9 @@ class ArrayDataAdapter(DataAdapter):\n                 y, class_weight\n             )\n \n-        inputs = data_adapters_utils.pack_x_y_sample_weight(x, y, sample_weight)\n+        inputs = data_adapter_utils.pack_x_y_sample_weight(x, y, sample_weight)\n \n-        data_adapters_utils.check_data_cardinality(inputs)\n+        data_adapter_utils.check_data_cardinality(inputs)\n         num_samples = set(i.shape[0] for i in nest.flatten(inputs)).pop()\n         self._num_samples = num_samples\n         self._inputs = inputs\n@@ -100,7 +100,7 @@ class ArrayDataAdapter(DataAdapter):\n     def get_numpy_iterator(self):\n         inputs = self._inputs\n         if self._shuffle:\n-            inputs = data_adapters_utils.sync_shuffle(\n+            inputs = data_adapter_utils.sync_shuffle(\n                 inputs, num_samples=self._num_samples\n             )\n         for i in range(self._size):\n\n\n@@ -1,6 +1,6 @@\n import tensorflow as tf\n \n-from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n \n \n@@ -69,7 +69,7 @@ def make_class_weight_map_fn(class_weight):\n \n     def class_weights_map_fn(*data):\n         \"\"\"Convert `class_weight` to `sample_weight`.\"\"\"\n-        x, y, sw = data_adapters_utils.unpack_x_y_sample_weight(data)\n+        x, y, sw = data_adapter_utils.unpack_x_y_sample_weight(data)\n         if sw is not None:\n             raise ValueError(\n                 \"You cannot `class_weight` and `sample_weight` at the same time.\"\n\n@@ -43,7 +43,7 @@ import warnings\n import tensorflow as tf\n \n from keras_core.trainers.data_adapters import array_data_adapter\n-from keras_core.trainers.data_adapters import data_adapters_utils\n+from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.data_adapters import tf_dataset_adapter\n \n \n@@ -61,7 +61,7 @@ class EpochIterator:\n         self.steps_per_epoch = steps_per_epoch\n         if steps_per_epoch:\n             self._current_iterator = None\n-        if isinstance(x, data_adapters_utils.ARRAY_TYPES):\n+        if isinstance(x, data_adapter_utils.ARRAY_TYPES):\n             self.data_adapter = array_data_adapter.ArrayDataAdapter(\n                 x,\n                 y,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83f5a25a4bb596571388f62f2fd5258fbf4675d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 269 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): 18/13 | Churn Δ: 269 | Churn Cumulative: 269 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,5 @@\n+from keras_core.constraints.constraints import Constraint\n+from keras_core.constraints.constraints import MaxNorm\n+from keras_core.constraints.constraints import MinMaxNorm\n+from keras_core.constraints.constraints import NonNeg\n+from keras_core.constraints.constraints import UnitNorm\n\n@@ -0,0 +1,219 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.constraints.Constraint\")\n+class Constraint:\n+    \"\"\"Base class for weight constraints.\n+\n+    A `Constraint` instance works like a stateless function.\n+    Users who subclass this\n+    class should override the `__call__()` method, which takes a single\n+    weight parameter and return a projected version of that parameter\n+    (e.g. normalized or clipped). Constraints can be used with various Keras\n+    layers via the `kernel_constraint` or `bias_constraint` arguments.\n+\n+    Here's a simple example of a non-negative weight constraint:\n+\n+    >>> class NonNegative(keras_core.constraints.Constraint):\n+    ...\n+    ...  def __call__(self, w):\n+    ...    return w * ops.cast(ops.greater_equal(w, 0.), dtype=w.dtype)\n+\n+    >>> weight = ops.convert_to_tensor((-1.0, 1.0))\n+    >>> NonNegative()(weight)\n+    [0.,  1.]\n+\n+    Usage in a layer:\n+\n+    >>> keras_core.layers.Dense(4, kernel_constraint=NonNegative())\n+    \"\"\"\n+\n+    def __call__(self, w):\n+        \"\"\"Applies the constraint to the input weight variable.\n+\n+        By default, the inputs weight variable is not modified.\n+        Users should override this method to implement their own projection\n+        function.\n+\n+        Args:\n+            w: Input weight variable.\n+\n+        Returns:\n+            Projected variable (by default, returns unmodified inputs).\n+        \"\"\"\n+        return w\n+\n+    def get_config(self):\n+        \"\"\"Returns a Python dict of the object config.\n+\n+        A constraint config is a Python dictionary (JSON-serializable) that can\n+        be used to reinstantiate the same object.\n+\n+        Returns:\n+            Python dict containing the configuration of the constraint object.\n+        \"\"\"\n+        return {}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Instantiates a weight constraint from a configuration dictionary.\n+\n+        Example:\n+\n+        ```python\n+        constraint = UnitNorm()\n+        config = constraint.get_config()\n+        constraint = UnitNorm.from_config(config)\n+        ```\n+\n+        Args:\n+            config: A Python dictionary, the output of `get_config()`.\n+\n+        Returns:\n+            A `keras_core.constraints.Constraint` instance.\n+        \"\"\"\n+        return cls(**config)\n+\n+\n+@keras_core_export(\n+    [\"keras_core.constraints.MaxNorm\", \"keras_core.constraints.max_norm\"]\n+)\n+class MaxNorm(Constraint):\n+    \"\"\"MaxNorm weight constraint.\n+\n+    Constrains the weights incident to each hidden unit\n+    to have a norm less than or equal to a desired value.\n+\n+    Also available via the shortcut function `keras_core.constraints.max_norm`.\n+\n+    Args:\n+        max_value: the maximum norm value for the incoming weights.\n+        axis: integer, axis along which to calculate weight norms.\n+            For instance, in a `Dense` layer the weight matrix\n+            has shape `(input_dim, output_dim)`,\n+            set `axis` to `0` to constrain each weight vector\n+            of length `(input_dim,)`.\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n+            the weight tensor has shape\n+            `(rows, cols, input_depth, output_depth)`,\n+            set `axis` to `[0, 1, 2]`\n+            to constrain the weights of each filter tensor of size\n+            `(rows, cols, input_depth)`.\n+\n+    \"\"\"\n+\n+    def __init__(self, max_value=2, axis=0):\n+        self.max_value = max_value\n+        self.axis = axis\n+\n+    def __call__(self, w):\n+        w = backend.convert_to_tensor(w)\n+        norms = ops.sqrt(ops.sum(ops.square(w), axis=self.axis, keepdims=True))\n+        desired = ops.clip(norms, 0, self.max_value)\n+        return w * (desired / (backend.epsilon() + norms))\n+\n+    def get_config(self):\n+        return {\"max_value\": self.max_value, \"axis\": self.axis}\n+\n+\n+@keras_core_export(\n+    [\"keras_core.constraints.NonNeg\", \"keras_core.constraints.non_neg\"]\n+)\n+class NonNeg(Constraint):\n+    \"\"\"Constrains the weights to be non-negative.\"\"\"\n+\n+    def __call__(self, w):\n+        w = backend.convert_to_tensor(w)\n+        return w * ops.cast(ops.greater_equal(w, 0.0), dtype=w.dtype)\n+\n+\n+@keras_core_export(\n+    [\"keras_core.constraints.UnitNorm\", \"keras_core.constraints.unit_norm\"]\n+)\n+class UnitNorm(Constraint):\n+    \"\"\"Constrains the weights incident to each hidden unit to have unit norm.\n+\n+    Args:\n+        axis: integer, axis along which to calculate weight norms.\n+            For instance, in a `Dense` layer the weight matrix\n+            has shape `(input_dim, output_dim)`,\n+            set `axis` to `0` to constrain each weight vector\n+            of length `(input_dim,)`.\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n+            the weight tensor has shape\n+            `(rows, cols, input_depth, output_depth)`,\n+            set `axis` to `[0, 1, 2]`\n+            to constrain the weights of each filter tensor of size\n+            `(rows, cols, input_depth)`.\n+    \"\"\"\n+\n+    def __init__(self, axis=0):\n+        self.axis = axis\n+\n+    def __call__(self, w):\n+        w = backend.convert_to_tensor(w)\n+        return w / (\n+            backend.epsilon()\n+            + ops.sqrt(ops.sum(ops.square(w), axis=self.axis, keepdims=True))\n+        )\n+\n+    def get_config(self):\n+        return {\"axis\": self.axis}\n+\n+\n+@keras_core_export(\n+    [\"keras_core.constraints.MinMaxNorm\", \"keras_core.constraints.min_max_norm\"]\n+)\n+class MinMaxNorm(Constraint):\n+    \"\"\"MinMaxNorm weight constraint.\n+\n+    Constrains the weights incident to each hidden unit\n+    to have the norm between a lower bound and an upper bound.\n+\n+    Args:\n+        min_value: the minimum norm for the incoming weights.\n+        max_value: the maximum norm for the incoming weights.\n+        rate: rate for enforcing the constraint: weights will be\n+            rescaled to yield\n+            `(1 - rate) * norm + rate * norm.clip(min_value, max_value)`.\n+            Effectively, this means that rate=1.0 stands for strict\n+            enforcement of the constraint, while rate<1.0 means that\n+            weights will be rescaled at each step to slowly move\n+            towards a value inside the desired interval.\n+        axis: integer, axis along which to calculate weight norms.\n+            For instance, in a `Dense` layer the weight matrix\n+            has shape `(input_dim, output_dim)`,\n+            set `axis` to `0` to constrain each weight vector\n+            of length `(input_dim,)`.\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n+            the weight tensor has shape\n+            `(rows, cols, input_depth, output_depth)`,\n+            set `axis` to `[0, 1, 2]`\n+            to constrain the weights of each filter tensor of size\n+            `(rows, cols, input_depth)`.\n+    \"\"\"\n+\n+    def __init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0):\n+        self.min_value = min_value\n+        self.max_value = max_value\n+        self.rate = rate\n+        self.axis = axis\n+\n+    def __call__(self, w):\n+        w = backend.convert_to_tensor(w)\n+        norms = ops.sqrt(ops.sum(ops.square(w), axis=self.axis, keepdims=True))\n+        desired = (\n+            self.rate * ops.clip(norms, self.min_value, self.max_value)\n+            + (1 - self.rate) * norms\n+        )\n+        return w * (desired / (backend.epsilon() + norms))\n+\n+    def get_config(self):\n+        return {\n+            \"min_value\": self.min_value,\n+            \"max_value\": self.max_value,\n+            \"rate\": self.rate,\n+            \"axis\": self.axis,\n+        }\n\n@@ -0,0 +1,45 @@\n+import numpy as np\n+\n+from keras_core import constraints\n+from keras_core import testing\n+\n+\n+def get_example_array():\n+    np.random.seed(3537)\n+    example_array = np.random.random((100, 100)) * 100.0 - 50.0\n+    example_array[0, 0] = 0.0  # Possible edge case\n+    return example_array\n+\n+\n+class ConstraintsTest(testing.TestCase):\n+    def test_max_norm(self):\n+        constraint_fn = constraints.MaxNorm(2.0)\n+        x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n+        target = np.array(\n+            [\n+                [0, 0, 0],\n+                [1.0, 0, 0],\n+                [2.0, 0, 0],\n+                [2.0 / np.sqrt(3), 2.0 / np.sqrt(3), 2.0 / np.sqrt(3)],\n+            ]\n+        ).T\n+        output = constraint_fn(x)\n+        self.assertAllClose(target, output)\n+\n+    def test_non_neg(self):\n+        constraint_fn = constraints.NonNeg()\n+        output = constraint_fn(get_example_array())\n+        self.assertTrue((np.min(output, axis=1) >= 0.0).all())\n+\n+    def test_unit_norm(self):\n+        constraint_fn = constraints.UnitNorm()\n+        output = constraint_fn(get_example_array())\n+        l2 = np.sqrt(np.sum(np.square(output), axis=0))\n+        self.assertAllClose(l2, 1.0)\n+\n+    def test_min_max_norm(self):\n+        constraint_fn = constraints.MinMaxNorm(min_value=0.2, max_value=0.5)\n+        output = constraint_fn(get_example_array())\n+        l2 = np.sqrt(np.sum(np.square(output), axis=0))\n+        self.assertFalse(l2[l2 < 0.2])\n+        self.assertFalse(l2[l2 > 0.5 + 1e-6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
