{"custom_id": "scrapy#4c61df7abb0ed6e4cb970f4a2f71b0235018307b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 38 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,7 @@ def get_spider_list_from_eggfile(eggfile, project):\n             env = os.environ.copy()\n             env['SCRAPY_PROJECT'] = project\n             env['SCRAPY_EGGFILE'] = f.name\n+            env.pop('SCRAPY_SETTINGS_DISABLED', None)\n             proc = Popen(pargs, stdout=PIPE, cwd=tmpdir, env=env)\n             out = proc.communicate()[0]\n             return out.splitlines()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ffc8d29b67e68f492d21d6705417a33f6f30574", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 313 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/topics/logging.rst\n \"\"\"\n import sys\n import logging\n+import warnings\n \n from twisted.python import log\n \n@@ -103,13 +104,15 @@ def start(logfile=None, loglevel=None, logstdout=None):\n         if logstdout is None:\n             logstdout = settings.getbool('LOG_STDOUT')\n         sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])\n+        _oldshowwarning = warnings.showwarning\n         log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n+        # restore warnings, wrongly silenced by Twisted\n+        warnings.showwarning = _oldshowwarning\n         msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n             settings['BOT_NAME']))\n \n def msg(message, level=INFO, **kw):\n     if 'component' in kw:\n-        import warnings\n         warnings.warn(\"Argument `component` of scrapy.log.msg() is deprecated\", \\\n             DeprecationWarning, stacklevel=2)\n     kw.setdefault('system', 'scrapy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3769651af2a375572f72025dfdeaf6d6cf1f8ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 38 | Churn Cumulative: 189 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,8 +9,7 @@ class QueuePoller(object):\n     implements(IPoller)\n \n     def __init__(self, config):\n-        self.eggs_dir = config.get('eggs_dir', 'eggs')\n-        self.dbs_dir = config.get('dbs_dir', 'dbs')\n+        self.config = config\n         self.update_projects()\n         self.dq = DeferredQueue(size=1)\n \n@@ -25,7 +24,7 @@ class QueuePoller(object):\n         return self.dq.get()\n \n     def update_projects(self):\n-        self.queues = get_spider_queues(self.eggs_dir, self.dbs_dir)\n+        self.queues = get_spider_queues(self.config)\n \n     def _message(self, project):\n         return {'project': str(project)}\n\n@@ -8,8 +8,7 @@ class SpiderScheduler(object):\n     implements(ISpiderScheduler)\n \n     def __init__(self, config):\n-        self.eggs_dir = config.get('eggs_dir', 'eggs')\n-        self.dbs_dir = config.get('dbs_dir', 'dbs')\n+        self.config = config\n         self.update_projects()\n \n     def schedule(self, project, spider_name, **spider_args):\n@@ -20,4 +19,4 @@ class SpiderScheduler(object):\n         return self.queues.keys()\n \n     def update_projects(self):\n-        self.queues = get_spider_queues(self.eggs_dir, self.dbs_dir)\n+        self.queues = get_spider_queues(self.config)\n\n@@ -21,7 +21,7 @@ class QueuePollerTest(unittest.TestCase):\n         os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n         os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n         config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n-        self.queues = get_spider_queues(eggs_dir, dbs_dir)\n+        self.queues = get_spider_queues(config)\n         self.poller = QueuePoller(config)\n \n     def test_interface(self):\n\n@@ -21,7 +21,7 @@ class SpiderSchedulerTest(unittest.TestCase):\n         os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n         os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n         config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n-        self.queues = get_spider_queues(eggs_dir, dbs_dir)\n+        self.queues = get_spider_queues(config)\n         self.sched = SpiderScheduler(config)\n \n     def test_interface(self):\n\n@@ -1,12 +1,30 @@\n import os\n-import pkg_resources\n+from ConfigParser import NoSectionError\n \n from scrapy.spiderqueue import SqliteSpiderQueue\n \n-def get_spider_queues(eggsdir, dbsdir):\n+def get_spider_queues(config):\n     \"\"\"Return a dict of Spider Quees keyed by project name\"\"\"\n+    dbsdir = config.get('dbs_dir', 'dbs')\n+    if not os.path.exists(dbsdir):\n+        os.makedirs(dbsdir)\n     d = {}\n-    for project in os.listdir(eggsdir):\n+    for project in get_project_list(config):\n         dbpath = os.path.join(dbsdir, '%s.db' % project)\n         d[project] = SqliteSpiderQueue(dbpath)\n     return d\n+\n+def get_project_list(config):\n+    \"\"\"Get list of projects by inspecting the eggs dir and the ones defined in\n+    the scrapyd.conf [settings] section\n+    \"\"\"\n+    eggs_dir = config.get('eggs_dir', 'eggs')\n+    if os.path.exists(eggs_dir):\n+        projects = os.listdir(eggs_dir)\n+    else:\n+        projects = []\n+    try:\n+        projects += [x[0] for x in config.cp.items('settings')]\n+    except NoSectionError:\n+        pass\n+    return projects\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ebfa7e68d99bb58cf965d89527a467d27e3d00d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ sys.path.append(path.join(path.dirname(path.dirname(__file__)), \"scrapy\"))\n \n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n-extensions = ['scrapydocs', 'sphinx.ext.autodoc']\n+extensions = ['scrapydocs']\n \n # Add any paths that contain templates here, relative to this directory.\n templates_path = ['_templates']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4639ffb06ff261e9587588ae58794a39b3b0f31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 438 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,7 +34,4 @@ def get_project_settings():\n \n     return settings\n \n-if os.environ.get('SCRAPY_SETTINGS_DISABLED'):\n-    settings = CrawlerSettings()\n-else:\n settings = get_project_settings()\n\n@@ -10,7 +10,6 @@ class CmdlineTest(unittest.TestCase):\n     def setUp(self):\n         self.env = os.environ.copy()\n         self.env['PYTHONPATH'] = os.path.dirname(scrapy.__path__[0])\n-        self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n\n@@ -61,7 +61,6 @@ class CommandTest(ProjectTest):\n         super(CommandTest, self).setUp()\n         self.call('startproject', self.project_name)\n         self.cwd = join(self.temp_path, self.project_name)\n-        self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n \n \n\n@@ -18,7 +18,6 @@ def get_spider_list_from_eggfile(eggfile, project):\n             env = os.environ.copy()\n             env['SCRAPY_PROJECT'] = project\n             env['SCRAPY_EGGFILE'] = f.name\n-            env.pop('SCRAPY_SETTINGS_DISABLED', None)\n             proc = Popen(pargs, stdout=PIPE, cwd=tmpdir, env=env)\n             out = proc.communicate()[0]\n             return out.splitlines()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2459d20cc0d3fd45acd1eeb2a98a6f12a2d57d2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 39 | Churn Cumulative: 467 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.6296296296296297\n\nDIFF:\n@@ -54,6 +54,7 @@ class Crawler(object):\n     @defer.inlineCallbacks\n     def _start_spider(self, spider, requests):\n         \"\"\"Don't call this method. Use self.queue to start new spiders\"\"\"\n+        spider.set_crawler(self)\n         yield defer.maybeDeferred(self.engine.open_spider, spider)\n         for request in requests:\n             self.engine.crawl(request, spider)\n\n@@ -57,3 +57,25 @@ class CrawlerSettings(Settings):\n \n     def __str__(self):\n         return \"<CrawlerSettings module=%r>\" % self.settings_module.__name__\n+\n+\n+class SpiderSettings(Settings):\n+\n+    def __init__(self, spider, crawler_settings, **kw):\n+        super(SpiderSettings, self).__init__(**kw)\n+        self.spider = spider\n+        self.cset = crawler_settings\n+\n+    def __getitem__(self, opt_name):\n+        if opt_name in self.cset.overrides:\n+            return self.cset.overrides[opt_name]\n+        if hasattr(self.spider, opt_name):\n+            return getattr(self.spider, opt_name)\n+        if self.cset.settings_module and hasattr(self.cset.settings_module, opt_name):\n+            return getattr(self.cset.settings_module, opt_name)\n+        if opt_name in self.cset.defaults:\n+            return self.cset.defaults[opt_name]\n+        return super(SpiderSettings, self).__getitem__(opt_name)\n+\n+    def __str__(self):\n+        return \"<SpiderSettings spider=%r>\" % self.spider.name\n\n@@ -5,6 +5,7 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n from scrapy import log\n+from scrapy.settings import SpiderSettings\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.trackref import object_ref\n@@ -33,6 +34,21 @@ class BaseSpider(object_ref):\n         \"\"\"\n         log.msg(message, spider=self, level=level)\n \n+    def set_crawler(self, crawler):\n+        assert not hasattr(self, '_crawler'), \"Spider already bounded to %s\" % crawler\n+        self._crawler = crawler\n+\n+    @property\n+    def crawler(self):\n+        assert hasattr(self, '_crawler'), \"Spider not bounded to any crawler\"\n+        return self._crawler\n+\n+    @property\n+    def settings(self):\n+        if not hasattr(self, '_settings'):\n+            self._settings = SpiderSettings(self, self.crawler.settings)\n+        return self._settings\n+\n     def start_requests(self):\n         reqs = []\n         for url in self.start_urls:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed4aec187f7868a7fcc7c7279489f02be79a8cf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 93 | Files Changed: 12 | Hunks: 50 | Methods Changed: 35 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 212 | Churn Cumulative: 1231 | Contributors (this commit): 6 | Commits (past 90d): 38 | Contributors (cumulative): 27 | DMM Complexity: 0.8235294117647058\n\nDIFF:\n@@ -10,18 +10,10 @@ from scrapy.utils.python import WeakKeyCache\n class DefaultHeadersMiddleware(object):\n \n     def __init__(self, settings=conf.settings):\n-        self.global_default_headers = settings.get('DEFAULT_REQUEST_HEADERS')\n         self._headers = WeakKeyCache(self._default_headers)\n \n     def _default_headers(self, spider):\n-        headers = dict(self.global_default_headers)\n-        spider_headers = getattr(spider, 'default_request_headers', None) or {}\n-        for k, v in spider_headers.iteritems():\n-            if v:\n-                headers[k] = v\n-            else:\n-                headers.pop(k, None)\n-        return headers.items()\n+        return spider.settings.get('DEFAULT_REQUEST_HEADERS').items()\n \n     def process_request(self, request, spider):\n         for k, v in self._headers[spider]:\n\n@@ -4,6 +4,7 @@ Download timeout middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n from scrapy.utils.python import WeakKeyCache\n+from scrapy.utils import deprecate\n \n \n class DownloadTimeoutMiddleware(object):\n@@ -12,7 +13,10 @@ class DownloadTimeoutMiddleware(object):\n         self._cache = WeakKeyCache(self._download_timeout)\n \n     def _download_timeout(self, spider):\n-        return getattr(spider, \"download_timeout\", None)\n+        if hasattr(spider, 'download_timeout'):\n+            deprecate.attribute(spider, 'download_timeout', 'DOWNLOAD_TIMEOUT')\n+            return spider.download_timeout\n+        return spider.settings.getint('DOWNLOAD_TIMEOUT')\n \n     def process_request(self, request, spider):\n         timeout = self._cache[spider]\n\n@@ -54,8 +54,7 @@ class RobotsTxtMiddleware(object):\n \n     def spider_opened(self, spider):\n         self._spider_netlocs[spider] = set()\n-        self._useragents[spider] = getattr(spider, 'user_agent', None) \\\n-            or settings['USER_AGENT']\n+        self._useragents[spider] = spider.settings['USER_AGENT']\n \n     def spider_closed(self, spider):\n         for netloc in self._spider_netlocs[spider]:\n\n@@ -1,18 +1,20 @@\n \"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n \n-from scrapy.conf import settings\n from scrapy.utils.python import WeakKeyCache\n+from scrapy.utils import deprecate\n \n \n class UserAgentMiddleware(object):\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    def __init__(self, settings=settings):\n+    def __init__(self):\n         self.cache = WeakKeyCache(self._user_agent)\n-        self.default_useragent = settings.get('USER_AGENT')\n \n     def _user_agent(self, spider):\n-        return getattr(spider, 'user_agent', None) or self.default_useragent\n+        if hasattr(spider, 'user_agent'):\n+            deprecate.attribute(spider, 'user_agent', 'USER_AGENT')\n+            return spider.user_agent\n+        return spider.settings['USER_AGENT']\n \n     def process_request(self, request, spider):\n         ua = self.cache[spider]\n\n@@ -12,6 +12,7 @@ from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.signal import send_catch_log\n+from scrapy.utils import deprecate\n from scrapy import signals\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n@@ -21,18 +22,21 @@ from .handlers import DownloadHandlers\n class SpiderInfo(object):\n     \"\"\"Simple class to keep information and state for each open spider\"\"\"\n \n-    def __init__(self, download_delay=None, max_concurrent_requests=None):\n-        if download_delay is None:\n-            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')\n+    def __init__(self, spider):\n+        if hasattr(spider, 'download_delay'):\n+            deprecate.attribute(spider, 'download_delay', 'DOWNLOAD_DELAY')\n+            self._download_delay = spider.download_delay\n         else:\n-            self._download_delay = float(download_delay)\n+            self._download_delay = spider.settings.getfloat('DOWNLOAD_DELAY')\n         if self._download_delay:\n             self.max_concurrent_requests = 1\n-        elif max_concurrent_requests is None:\n-            self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n         else:\n-            self.max_concurrent_requests =  max_concurrent_requests\n-        if self._download_delay and settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n+            if hasattr(spider, 'max_concurrent_requests'):\n+                deprecate.attribute(spider, 'max_concurrent_requests', 'CONCURRENT_REQUESTS_PER_SPIDER')\n+                self.max_concurrent_requests = spider.max_concurrent_requests\n+            else:\n+                self.max_concurrent_requests = spider.settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n+        if self._download_delay and spider.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n             # same policy as wget --random-wait\n             self.random_delay_interval = (0.5*self._download_delay, \\\n                 1.5*self._download_delay)\n@@ -178,10 +182,7 @@ class Downloader(object):\n     def open_spider(self, spider):\n         \"\"\"Allocate resources to begin processing a spider\"\"\"\n         assert spider not in self.sites, \"Spider already opened: %s\" % spider\n-        self.sites[spider] = SpiderInfo(\n-            download_delay=getattr(spider, 'download_delay', None),\n-            max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n-        )\n+        self.sites[spider] = SpiderInfo(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n\n@@ -8,10 +8,6 @@ from twisted.internet import defer\n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.core.downloader.responsetypes import responsetypes\n-from scrapy.conf import settings\n-\n-\n-DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')\n \n \n def _parsed_url_args(parsed):\n@@ -89,7 +85,7 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     followRedirect = False\n     afterFoundGet = False\n \n-    def __init__(self, request, timeout=DOWNLOAD_TIMEOUT):\n+    def __init__(self, request, timeout=180):\n         self.url = urldefrag(request.url)[0]\n         self.method = request.method\n         self.body = request.body or None\n\n@@ -4,39 +4,44 @@ from scrapy.conf import settings\n from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n+from scrapy.utils.test import get_crawler\n \n \n class TestDefaultHeadersMiddleware(TestCase):\n \n-    def setUp(self):\n-        self.spider = BaseSpider('foo')\n-        self.mw = DefaultHeadersMiddleware()\n-        self.default_request_headers = dict([(k, [v]) for k, v in \\\n-            settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n+    def get_defaults_spider_mw(self):\n+        crawler = get_crawler()\n+        spider = BaseSpider('foo')\n+        spider.set_crawler(crawler)\n+        defaults = dict([(k, [v]) for k, v in \\\n+            crawler.settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n+        return defaults, spider, DefaultHeadersMiddleware()\n \n     def test_process_request(self):\n+        defaults, spider, mw = self.get_defaults_spider_mw()\n         req = Request('http://www.scrapytest.org')\n-        self.mw.process_request(req, self.spider)\n-        self.assertEquals(req.headers, self.default_request_headers)\n+        mw.process_request(req, spider)\n+        self.assertEquals(req.headers, defaults)\n \n     def test_spider_default_request_headers(self):\n+        defaults, spider, mw = self.get_defaults_spider_mw()\n         spider_headers = {'Unexistant-Header': ['value']}\n         # override one of the global default headers by spider\n-        if self.default_request_headers:\n-            k = set(self.default_request_headers).pop()\n+        if defaults:\n+            k = set(defaults).pop()\n             spider_headers[k] = ['__newvalue__']\n-        self.spider.default_request_headers = spider_headers\n+        spider.DEFAULT_REQUEST_HEADERS = spider_headers\n \n         req = Request('http://www.scrapytest.org')\n-        self.mw.process_request(req, self.spider)\n-        self.assertEquals(req.headers, dict(self.default_request_headers, **spider_headers))\n+        mw.process_request(req, spider)\n+        self.assertEquals(req.headers, dict(spider_headers))\n \n     def test_update_headers(self):\n+        defaults, spider, mw = self.get_defaults_spider_mw()\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n         req = Request('http://www.scrapytest.org', headers=headers)\n         self.assertEquals(req.headers, headers)\n \n-        self.mw.process_request(req, self.spider)\n-        self.default_request_headers.update(headers)\n-        self.assertEquals(req.headers, self.default_request_headers)\n-\n+        mw.process_request(req, spider)\n+        defaults.update(headers)\n+        self.assertEquals(req.headers, defaults)\n\n@@ -3,31 +3,32 @@ import unittest\n from scrapy.contrib.downloadermiddleware.downloadtimeout import DownloadTimeoutMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n+from scrapy.utils.test import get_crawler\n \n \n class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n-    def setUp(self):\n-        self.mw = DownloadTimeoutMiddleware()\n-        self.spider = BaseSpider('foo')\n-        self.req = Request('http://scrapytest.org/')\n+    def get_request_spider_mw(self):\n+        crawler = get_crawler()\n+        spider = BaseSpider('foo')\n+        spider.set_crawler(crawler)\n+        request = Request('http://scrapytest.org/')\n+        return request, spider, DownloadTimeoutMiddleware()\n \n-    def tearDown(self):\n-        del self.mw\n-        del self.spider\n-        del self.req\n-\n-    def test_spider_has_no_download_timeout(self):\n-        assert self.mw.process_request(self.req, self.spider) is None\n-        assert 'download_timeout' not in self.req.meta\n+    def test_default_download_timeout(self):\n+        req, spider, mw = self.get_request_spider_mw()\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta.get('download_timeout'), 180)\n \n     def test_spider_has_download_timeout(self):\n-        self.spider.download_timeout = 2\n-        assert self.mw.process_request(self.req, self.spider) is None\n-        self.assertEquals(self.req.meta.get('download_timeout'), 2)\n+        req, spider, mw = self.get_request_spider_mw()\n+        spider.DOWNLOAD_TIMEOUT = 2\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta.get('download_timeout'), 2)\n \n     def test_request_has_download_timeout(self):\n-        self.spider.download_timeout = 2\n-        self.req.meta['download_timeout'] = 1\n-        assert self.mw.process_request(self.req, self.spider) is None\n-        self.assertEquals(self.req.meta.get('download_timeout'), 1)\n+        req, spider, mw = self.get_request_spider_mw()\n+        spider.DOWNLOAD_TIMEOUT = 2\n+        req.meta['download_timeout'] = 1\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta.get('download_timeout'), 1)\n\n@@ -3,47 +3,49 @@ from unittest import TestCase\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n+from scrapy.utils.test import get_crawler\n \n \n class UserAgentMiddlewareTest(TestCase):\n \n-    def setUp(self):\n-        self.spider = BaseSpider('foo')\n-        self.mw = UserAgentMiddleware()\n-\n-    def tearDown(self):\n-        del self.mw\n+    def get_spider_and_mw(self, default_useragent):\n+        crawler = get_crawler({'USER_AGENT': default_useragent})\n+        spider = BaseSpider('foo')\n+        spider.set_crawler(crawler)\n+        return spider, UserAgentMiddleware()\n \n     def test_default_agent(self):\n-        self.mw.default_useragent = 'default_useragent'\n+        spider, mw = self.get_spider_and_mw('default_useragent')\n         req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, self.spider) is None\n+        assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n \n-        # None or not present user_agent attribute is the same\n-        self.spider.user_agent = None\n+    def test_remove_agent(self):\n+        # settings UESR_AGENT to None should remove the user agent\n+        spider, mw = self.get_spider_and_mw('default_useragent')\n+        spider.USER_AGENT = None\n         req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers['User-Agent'], 'default_useragent')\n+        assert mw.process_request(req, spider) is None\n+        assert req.headers.get('User-Agent') is None\n \n     def test_spider_agent(self):\n-        self.mw.default_useragent = 'default_useragent'\n-        self.spider.user_agent = 'spider_useragent'\n+        spider, mw = self.get_spider_and_mw('default_useragent')\n+        spider.USER_AGENT = 'spider_useragent'\n         req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, self.spider) is None\n+        assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'spider_useragent')\n \n     def test_header_agent(self):\n-        self.mw.default_useragent = 'default_useragent'\n-        self.spider.user_agent = 'spider_useragent'\n+        spider, mw = self.get_spider_and_mw('default_useragent')\n+        spider.USER_AGENT = 'spider_useragent'\n         req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})\n-        assert self.mw.process_request(req, self.spider) is None\n+        assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'header_useragent')\n \n     def test_no_agent(self):\n-        self.mw.default_useragent = None\n-        self.spider.user_agent = None\n+        spider, mw = self.get_spider_and_mw(None)\n+        spider.USER_AGENT = None\n         req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, self.spider) is None\n+        assert mw.process_request(req, spider) is None\n         assert 'User-Agent' not in req.headers\n \n\n@@ -17,8 +17,7 @@ from twisted.web import server, static, util\n from twisted.trial import unittest\n \n from scrapy import signals\n-from scrapy.settings import Settings\n-from scrapy.crawler import Crawler\n+from scrapy.utils.test import get_crawler\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.tests import tests_datadir\n from scrapy.spider import BaseSpider\n@@ -95,8 +94,7 @@ class CrawlerRun(object):\n         dispatcher.connect(self.request_received, signals.request_received)\n         dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n-        settings = Settings()\n-        self.crawler = Crawler(settings)\n+        self.crawler = get_crawler()\n         self.crawler.install()\n         self.crawler.configure()\n         self.crawler.queue.append_spider(self.spider)\n\n@@ -0,0 +1,9 @@\n+\"\"\"Some helpers for deprecation messages\"\"\"\n+\n+import warnings\n+\n+def attribute(obj, oldattr, newattr, version='0.12'):\n+    cname = obj.__class__.__name__\n+    warnings.warn(\"%s.%s attribute is deprecated and will be no longer supported \"\n+        \"in Scrapy %s, use %s.%s attribute instead\" % \\\n+        (cname, oldattr, version, cname, newattr), DeprecationWarning, stacklevel=3)\n\n@@ -7,6 +7,9 @@ import os\n import libxml2\n from twisted.trial.unittest import SkipTest\n \n+from scrapy.crawler import Crawler\n+from scrapy.settings import CrawlerSettings\n+\n def libxml2debug(testfunction):\n     \"\"\"Decorator for debugging libxml2 memory leaks inside a function.\n     \n@@ -39,3 +42,17 @@ def assert_aws_environ():\n \n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n         raise SkipTest(\"AWS keys not found\")\n+\n+def get_crawler(settings_dict=None):\n+    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n+    will be used as the settings present in the settings module of the\n+    CrawlerSettings.\n+    \"\"\"\n+    class SettingsModuleMock(object):\n+        pass\n+    settings_module = SettingsModuleMock()\n+    if settings_dict:\n+        for k, v in settings_dict.items():\n+            setattr(settings_module, k, v)\n+    settings = CrawlerSettings(settings_module)\n+    return Crawler(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97d77c79c28bfbde9781ae88e45d0a236cc10a6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 54 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n import unittest\n \n-from scrapy.settings import Settings\n+from scrapy.settings import Settings, SpiderSettings\n+from scrapy.utils.test import get_crawler\n+from scrapy.spider import BaseSpider\n \n class SettingsTest(unittest.TestCase):\n \n@@ -46,6 +48,56 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(settings.get('TEST_STRx'), None)\n         self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')\n \n+class CrawlerSettingsTest(unittest.TestCase):\n+\n+    def test_global_defaults(self):\n+        crawler = get_crawler()\n+        self.assertEqual(crawler.settings.getint('DOWNLOAD_TIMEOUT'), 180)\n+\n+    def test_defaults(self):\n+        crawler = get_crawler()\n+        crawler.settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n+        self.assertEqual(crawler.settings.getint('DOWNLOAD_TIMEOUT'), 99)\n+\n+    def test_settings_module(self):\n+        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n+        self.assertEqual(crawler.settings.getint('DOWNLOAD_TIMEOUT'), 3)\n+\n+    def test_overrides(self):\n+        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n+        crawler.settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n+        self.assertEqual(crawler.settings.getint('DOWNLOAD_TIMEOUT'), 15)\n+\n+class SpiderSettingsTest(unittest.TestCase):\n+\n+    def test_global_defaults(self):\n+        crawler = get_crawler()\n+        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n+        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 180)\n+\n+    def test_defaults(self):\n+        crawler = get_crawler()\n+        crawler.settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n+        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n+        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 99)\n+\n+    def test_crawler_defaults(self):\n+        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n+        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n+        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 3)\n+\n+    def test_spider_overrides_crawler(self):\n+        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n+        crawler.settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n+        settings = SpiderSettings(BaseSpider('name', DOWNLOAD_TIMEOUT='12'), crawler.settings)\n+        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 12)\n+\n+    def test_overrides_most_precedence(self):\n+        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n+        crawler.settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n+        settings = SpiderSettings(BaseSpider('name', DOWNLOAD_TIMEOUT='12'), crawler.settings)\n+        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 15)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9599bde3e90ea9758e3f74e39aa87d70779b9758", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 68 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 68 | Churn Cumulative: 199 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,65 +0,0 @@\n-\"\"\"\n-Request Limit Spider middleware\n-\n-See documentation in docs/topics/spider-middleware.rst\n-\"\"\"\n-from itertools import imap\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy import signals\n-from scrapy.project import crawler\n-from scrapy.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.http import Request\n-from scrapy import log\n-\n-class RequestLimitMiddleware(object):\n-\n-    def __init__(self):\n-        self.max_queue_size = settings.getint(\"REQUESTS_QUEUE_SIZE\")\n-        if not self.max_queue_size:\n-            raise NotConfigured\n-\n-        self.max_pending = {}\n-        self.dropped_count = {}\n-\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-\n-    def spider_opened(self, spider):\n-        self.max_pending[spider] = getattr(spider, 'requests_queue_size', self.max_queue_size)\n-        self.dropped_count[spider] = 0\n-\n-    def spider_closed(self, spider):\n-        dropped_count = self.dropped_count[spider]\n-        if dropped_count:\n-            max_pending = self.max_pending[spider]\n-            log.msg('Dropped %d request(s) because the scheduler queue size limit (%d requests) was exceeded' % \\\n-                    (dropped_count, max_pending), level=log.DEBUG, spider=spider)\n-        del self.dropped_count[spider]\n-        del self.max_pending[spider]\n-\n-    def process_spider_output(self, response, result, spider):\n-        max_pending = self.max_pending.get(spider, 0)\n-        if max_pending:\n-            return imap(lambda v: self._limit_requests(v, spider, max_pending), result)\n-        else:\n-            return result\n-\n-    def _limit_requests(self, request_or_other, spider, max_pending):\n-        if isinstance(request_or_other, Request):\n-            free_slots = max_pending - self._pending_count(spider)\n-            if free_slots > 0:\n-                # Scheduler isn't saturated and it is fine to schedule more requests.\n-                return request_or_other\n-            else:\n-                # Skip the request and give engine time to handle other tasks.\n-                self.dropped_count[spider] += 1\n-                return None\n-        else:\n-            # Return others (non-requests) as is.\n-            return request_or_other\n-\n-    def _pending_count(self, spider):\n-        pending = crawler.engine.scheduler.pending_requests.get(spider, [])\n-        return len(pending)\n\n@@ -194,8 +194,6 @@ REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n-REQUESTS_QUEUE_SIZE = 0\n-\n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n@@ -220,7 +218,6 @@ SPIDER_MIDDLEWARES = {}\n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n-    'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,\n     'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n     'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n     'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f29b346f793e58743eb62e6224f780566d7933da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 396 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -89,7 +89,7 @@ def _check_deprecated_scrapy_ctl(argv, inproject):\n             with open(cfg_path, 'w') as f:\n                 f.write(\"# generated automatically - feel free to edit\" + os.linesep)\n                 f.write(\"[settings]\" + os.linesep)\n-                f.write(\"default = %s\" % settings.settings_module_path + os.linesep)\n+                f.write(\"default = %s\" % settings.settings_module.__name__ + os.linesep)\n \n def _run_print_help(parser, func, *a, **kw):\n     try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37c25fe9a89f5f319d781738d3e8f7631ac6668a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,7 +56,11 @@ class CrawlerSettings(Settings):\n         return super(CrawlerSettings, self).__getitem__(opt_name)\n \n     def __str__(self):\n-        return \"<CrawlerSettings module=%r>\" % self.settings_module.__name__\n+        if self.settings_module:\n+            return \"<CrawlerSettings module=%r path=%r>\" % \\\n+                (self.settings_module.__name__, self.settings_module.__file__)\n+        else:\n+            return \"<CrawlerSettings module=None>\"\n \n \n class SpiderSettings(Settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#754d0f53f9d6381e9eeb2b87fe23a0c0ef01e6f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 465 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,10 +56,14 @@ class Shell(object):\n         if spider is None:\n             spider = create_spider_for_request(self.crawler.spiders, request, \\\n                 BaseSpider('default'), log_multiple=True)\n+        spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider)\n         return self.crawler.engine.schedule(request, spider)\n \n     def fetch(self, request_or_url, spider=None):\n+        # we enclose all this code in a try/except block to see errors when\n+        # they happen in a thread\n+        try:\n             if isinstance(request_or_url, Request):\n                 request = request_or_url\n                 url = request.url\n@@ -67,12 +71,11 @@ class Shell(object):\n                 url = any_to_uri(request_or_url)\n                 request = Request(url, dont_filter=True)\n             response = None\n-        try:\n             response = threads.blockingCallFromThread(reactor, \\\n                 self._schedule, request, spider)\n-        except:\n-            log.err(Failure(), \"Error fetching response\", spider=spider)\n             self.populate_vars(url, response, request, spider)\n+        except:\n+            log.err(Failure(), \"Error fetching: %s\" % request_or_url, spider=spider)\n \n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79c0e34968a740f67c3ee3df4aff5a4c93adadbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -56,11 +56,7 @@ class CrawlerSettings(Settings):\n         return super(CrawlerSettings, self).__getitem__(opt_name)\n \n     def __str__(self):\n-        if self.settings_module:\n-            return \"<CrawlerSettings module=%r path=%r>\" % \\\n-                (self.settings_module.__name__, self.settings_module.__file__)\n-        else:\n-            return \"<CrawlerSettings module=None>\"\n+        return \"<CrawlerSettings module=%r>\" % self.settings_module\n \n \n class SpiderSettings(Settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5ee05e8140e0a055bfecb6283bfbd432a66418f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 451 | Contributors (this commit): 1 | Commits (past 90d): 21 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -128,6 +128,7 @@ def execute(argv=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n+    sys.exit(cmd.exitcode)\n \n def _run_command(cmd, args, opts):\n     if opts.profile or opts.lsprof:\n\n@@ -21,6 +21,8 @@ class ScrapyCommand(object):\n     # default settings to be used for this command instead of global defaults\n     default_settings = {}\n \n+    exitcode = 0\n+\n     def set_crawler(self, crawler):\n         self._crawler = crawler\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#318f7f4c58f6031485ba8966aff55676b77b7e62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 28 | Churn Cumulative: 527 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/shell.rst\n \n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n+from scrapy import log\n \n class Command(ScrapyCommand):\n \n@@ -21,6 +22,11 @@ class Command(ScrapyCommand):\n     def long_desc(self):\n         return \"Interactive console for scraping the given url\"\n \n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-c\", dest=\"code\",\n+            help=\"evaluate the code in the shell, print the result and exit\")\n+\n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n         available in the shell\n@@ -29,6 +35,12 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True)\n-        shell.start(url=url).addBoth(lambda _: self.crawler.stop())\n+        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True, \\\n+            code=opts.code)\n+        def err(f):\n+            log.err(f, \"Shell error\")\n+            self.exitcode = 1\n+        d = shell.start(url=url)\n+        d.addErrback(err)\n+        d.addBoth(lambda _: self.crawler.stop())\n         self.crawler.start()\n\n@@ -27,12 +27,13 @@ class Shell(object):\n     relevant_classes = (BaseSpider, Request, Response, BaseItem, \\\n         XPathSelector, Settings)\n \n-    def __init__(self, crawler, update_vars=None, inthread=False):\n+    def __init__(self, crawler, update_vars=None, inthread=False, code=None):\n         self.crawler = crawler\n         self.vars = {}\n         self.update_vars = update_vars or (lambda x: None)\n         self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n         self.inthread = inthread\n+        self.code = code\n \n     def start(self, *a, **kw):\n         # disable accidental Ctrl-C key press from shutting down the engine\n@@ -50,6 +51,9 @@ class Shell(object):\n         elif response:\n             request = response.request\n             self.populate_vars(request.url, response, request, spider)\n+        if self.code:\n+            print eval(self.code, globals(), self.vars)\n+        else:\n             start_python_console(self.vars)\n \n     def _schedule(self, request, spider):\n@@ -61,9 +65,6 @@ class Shell(object):\n         return self.crawler.engine.schedule(request, spider)\n \n     def fetch(self, request_or_url, spider=None):\n-        # we enclose all this code in a try/except block to see errors when\n-        # they happen in a thread\n-        try:\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n             url = request.url\n@@ -74,8 +75,6 @@ class Shell(object):\n         response = threads.blockingCallFromThread(reactor, \\\n             self._schedule, request, spider)\n         self.populate_vars(url, response, request, spider)\n-        except:\n-            log.err(Failure(), \"Error fetching: %s\" % request_or_url, spider=spider)\n \n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n@@ -93,6 +92,7 @@ class Shell(object):\n         self.vars['view'] = open_in_browser\n         self.vars['shelp'] = self.print_help\n         self.update_vars(self.vars)\n+        if not self.code:\n             self.print_help()\n \n     def print_help(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b78284b680c36884e9520d7d42c3bc256c97b4bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 483 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,7 +62,9 @@ class Shell(object):\n                 BaseSpider('default'), log_multiple=True)\n         spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider)\n-        return self.crawler.engine.schedule(request, spider)\n+        d = self.crawler.engine.schedule(request, spider)\n+        d.addCallback(lambda x: (x, spider))\n+        return d\n \n     def fetch(self, request_or_url, spider=None):\n         if isinstance(request_or_url, Request):\n@@ -72,7 +74,7 @@ class Shell(object):\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n         response = None\n-        response = threads.blockingCallFromThread(reactor, \\\n+        response, spider = threads.blockingCallFromThread(reactor, \\\n             self._schedule, request, spider)\n         self.populate_vars(url, response, request, spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#622834bc089d3462ede466e4d46975e878e1da47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 490 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,9 +7,7 @@ See documentation in docs/topics/shell.rst\n import signal\n \n from twisted.internet import reactor, threads\n-from twisted.python.failure import Failure\n \n-from scrapy import log\n from scrapy.item import BaseItem\n from scrapy.spider import BaseSpider\n from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n@@ -18,7 +16,6 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n from scrapy.utils.url import any_to_uri\n from scrapy.utils.console import start_python_console\n-from scrapy.conf import settings\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse\n \n@@ -31,7 +28,7 @@ class Shell(object):\n         self.crawler = crawler\n         self.vars = {}\n         self.update_vars = update_vars or (lambda x: None)\n-        self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n+        self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])\n         self.inthread = inthread\n         self.code = code\n \n@@ -81,7 +78,7 @@ class Shell(object):\n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n         self.vars['item'] = item\n-        self.vars['settings'] = settings\n+        self.vars['settings'] = self.crawler.settings\n         if url:\n             if isinstance(response, TextResponse):\n                 self.vars['xxs'] = XmlXPathSelector(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
