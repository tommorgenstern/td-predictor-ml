{"custom_id": "keras#f6bcaffe4a67012b4b067038bdbc5d43b807540b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 2818 | Contributors (this commit): 29 | Commits (past 90d): 48 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,7 +39,7 @@ x_decoded_mean = decoder_mean(h_decoded)\n \n \n def vae_loss(x, x_decoded_mean):\n-    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n+    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n     return xent_loss + kl_loss\n \n\n@@ -33,24 +33,26 @@ f = Flatten()(c)\n h = Dense(intermediate_dim, activation='relu')(f)\n \n z_mean = Dense(latent_dim)(h)\n-z_log_std = Dense(latent_dim)(h)\n+z_log_var = Dense(latent_dim)(h)\n+\n \n def sampling(args):\n-    z_mean, z_log_std = args\n+    z_mean, z_log_var = args\n     epsilon = K.random_normal(shape=(batch_size, latent_dim),\n                               mean=0., std=epsilon_std)\n-    return z_mean + K.exp(z_log_std) * epsilon\n+    return z_mean + K.exp(z_log_var) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n-# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n-z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n+# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n+z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n \n # we instantiate these layers separately so as to reuse them later\n decoder_h = Dense(intermediate_dim, activation='relu')\n decoder_f = Dense(nb_filters*img_rows*img_cols, activation='relu')\n decoder_c = Reshape((nb_filters, img_rows, img_cols))\n decoder_mean = Deconvolution2D(img_chns, nb_conv, nb_conv,\n-                               (batch_size, img_chns, img_rows, img_cols), border_mode='same')\n+                               (batch_size, img_chns, img_rows, img_cols),\n+                               border_mode='same')\n \n h_decoded = decoder_h(z)\n f_decoded = decoder_f(h_decoded)\n@@ -63,7 +65,7 @@ def vae_loss(x, x_decoded_mean):\n     x = K.flatten(x)\n     x_decoded_mean = K.flatten(x_decoded_mean)\n     xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n-    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n+    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n     return xent_loss + kl_loss\n \n vae = Model(x, x_decoded_mean)\n@@ -111,7 +113,7 @@ grid_y = np.linspace(-15, 15, n)\n \n for i, yi in enumerate(grid_x):\n     for j, xi in enumerate(grid_y):\n-        z_sample = np.array([[xi, yi]]) * epsilon_std\n+        z_sample = np.array([[xi, yi]])\n         x_decoded = generator.predict(z_sample)\n         digit = x_decoded[0].reshape(digit_size, digit_size)\n         figure[i * digit_size: (i + 1) * digit_size,\n\n@@ -1250,10 +1250,10 @@ def l2_normalize(x, axis):\n \n # CONVOLUTIONS\n \n-def _preprocess_deconv_output_shape(sh, dim_ordering):\n-    if dim_ordering == \"th\":\n-        sh = (sh[0], sh[2], sh[3], sh[1])\n-    return sh\n+def _preprocess_deconv_output_shape(shape, dim_ordering):\n+    if dim_ordering == 'th':\n+        shape = (shape[0], shape[2], shape[3], shape[1])\n+    return shape\n \n \n def _preprocess_conv2d_input(x, dim_ordering):\n@@ -1383,7 +1383,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n     x = _preprocess_conv2d_input(x, dim_ordering)\n     output_shape = _preprocess_deconv_output_shape(output_shape, dim_ordering)\n     kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n-    kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels\n+    kernel = tf.transpose(kernel, (0, 1, 3, 2))\n     padding = _preprocess_border_mode(border_mode)\n     strides = (1,) + strides + (1,)\n \n\n@@ -36,7 +36,8 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n \n \n def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n-               input_data=None, expected_output=None, expected_output_dtype=None, fixed_batch_size=False):\n+               input_data=None, expected_output=None,\n+               expected_output_dtype=None, fixed_batch_size=False):\n     '''Test routine for a layer with a single input tensor\n     and single output tensor.\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3726aba2ee3a76f4ef6c77a1ac30b845db142315", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 98 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ epochs = 25\n lahead = 1\n \n \n-def gen_cosine_amp(amp=100, period=25, x0=0, xn=50000, step=1, k=0.0001):\n+def gen_cosine_amp(amp=100, period=1000, x0=0, xn=50000, step=1, k=0.0001):\n     \"\"\"Generates an absolute cosine time series with the amplitude\n     exponentially decreasing\n \n@@ -31,7 +31,7 @@ def gen_cosine_amp(amp=100, period=25, x0=0, xn=50000, step=1, k=0.0001):\n     cos = np.zeros(((xn - x0) * step, 1, 1))\n     for i in range(len(cos)):\n         idx = x0 + i * step\n-        cos[i, 0, 0] = amp * np.cos(idx / (2 * np.pi * period))\n+        cos[i, 0, 0] = amp * np.cos(2 * np.pi * idx / period)\n         cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)\n     return cos\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea561ba6d87879b9a67a7454e1d29777fb59132d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 450 | Lines Deleted: 41 | Files Changed: 9 | Hunks: 55 | Methods Changed: 31 | Complexity Δ (Sum/Max): 50/31 | Churn Δ: 491 | Churn Cumulative: 16836 | Contributors (this commit): 84 | Commits (past 90d): 88 | Contributors (cumulative): 125 | DMM Complexity: 0.5886792452830188\n\nDIFF:\n@@ -239,16 +239,20 @@ class ModelCheckpoint(Callback):\n             this should be `max`, for `val_loss` this should\n             be `min`, etc. In `auto` mode, the direction is\n             automatically inferred from the name of the monitored quantity.\n+        save_weights_only: if True, then only the model's weights will be\n+            saved (`model.save_weights(filepath)`), else the full model\n+            is saved (`model.save(filepath)`).\n \n     '''\n     def __init__(self, filepath, monitor='val_loss', verbose=0,\n-                 save_best_only=False, mode='auto'):\n-\n+                 save_best_only=False, save_weights_only=False,\n+                 mode='auto'):\n         super(ModelCheckpoint, self).__init__()\n         self.monitor = monitor\n         self.verbose = verbose\n         self.filepath = filepath\n         self.save_best_only = save_best_only\n+        self.save_weights_only = save_weights_only\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('ModelCheckpoint mode %s is unknown, '\n@@ -285,7 +289,10 @@ class ModelCheckpoint(Callback):\n                               % (epoch, self.monitor, self.best,\n                                  current, filepath))\n                     self.best = current\n+                    if self.save_weights_only:\n                         self.model.save_weights(filepath, overwrite=True)\n+                    else:\n+                        self.model.save(filepath, overwrite=True)\n                 else:\n                     if self.verbose > 0:\n                         print('Epoch %05d: %s did not improve' %\n@@ -293,7 +300,10 @@ class ModelCheckpoint(Callback):\n         else:\n             if self.verbose > 0:\n                 print('Epoch %05d: saving model to %s' % (epoch, filepath))\n+            if self.save_weights_only:\n                 self.model.save_weights(filepath, overwrite=True)\n+            else:\n+                self.model.save(filepath, overwrite=True)\n \n \n class EarlyStopping(Callback):\n\n@@ -10,9 +10,11 @@ import marshal\n import types as python_types\n import warnings\n import copy\n+import os\n from six.moves import zip\n \n-from keras import backend as K\n+from .. import backend as K\n+from ..utils.io_utils import ask_to_proceed_with_overwrite\n \n \n def to_list(x):\n@@ -2310,7 +2312,38 @@ class Container(Layer):\n             output_tensors.append(layer_output_tensors[tensor_index])\n         return cls(input=input_tensors, output=output_tensors, name=name)\n \n-    def save_weights(self, filepath, overwrite=False):\n+    def save(self, filepath, overwrite=True):\n+        '''Save into a single HDF5 file:\n+            - the model architecture, allowing to re-instantiate the model\n+            - the model weights\n+            - the state of the optimizer, allowing to resume training\n+                exactly where you left off.\n+\n+        This allows you to save the entirety of the state of a model\n+        in a single file.\n+\n+        Saved models can be reinstantiated via `keras.models.load_model`.\n+        The model returned by `load_model`\n+        is a compiled model ready to be used (unless the saved model\n+        was never compiled in the first place).\n+\n+        # Example usage\n+\n+        ```python\n+        from keras.models import load_model\n+\n+        model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n+        del model  # deletes the existing model\n+\n+        # returns a compiled model\n+        # identical to the previous one\n+        model = load_model('my_model.h5')\n+        ```\n+        '''\n+        from ..models import save_model\n+        save_model(self, filepath, overwrite)\n+\n+    def save_weights(self, filepath, overwrite=True):\n         '''Dumps all layer weights to a HDF5 file.\n \n         The weight file has:\n@@ -2323,28 +2356,23 @@ class Container(Layer):\n                     storing the weight value, named after the weight tensor\n         '''\n         import h5py\n-        import os.path\n         # if file exists and should not be overwritten\n         if not overwrite and os.path.isfile(filepath):\n-            import sys\n-            get_input = input\n-            if sys.version_info[:2] <= (2, 7):\n-                get_input = raw_input\n-            overwrite = get_input('[WARNING] %s already exists - overwrite? '\n-                                  '[y/n]' % (filepath))\n-            while overwrite not in ['y', 'n']:\n-                overwrite = get_input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n-            if overwrite == 'n':\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n                 return\n-            print('[TIP] Next time specify overwrite=True in save_weights!')\n+        f = h5py.File(filepath, 'w')\n+        self.save_weights_to_hdf5_group(f)\n+        f.flush()\n+        f.close()\n \n+    def save_weights_to_hdf5_group(self, f):\n         if hasattr(self, 'flattened_layers'):\n             # support for legacy Sequential/Merge behavior\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n \n-        f = h5py.File(filepath, 'w')\n         f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in flattened_layers]\n \n         for layer in flattened_layers:\n@@ -2362,21 +2390,38 @@ class Container(Layer):\n             for name, val in zip(weight_names, weight_values):\n                 param_dset = g.create_dataset(name, val.shape,\n                                               dtype=val.dtype)\n+                if not val.shape:\n+                    # scalar\n+                    param_dset[()] = val\n+                else:\n                     param_dset[:] = val\n-        f.flush()\n-        f.close()\n \n     def load_weights(self, filepath):\n         '''Load all layer weights from a HDF5 save file.\n         '''\n         import h5py\n         f = h5py.File(filepath, mode='r')\n+        self.load_weights_from_hdf5_group(f)\n+        f.close()\n \n+    def load_weights_from_hdf5_group(self, f):\n+        '''Weight loading is based on layer order in a list\n+        (matching model.flattened_layers for Sequential models,\n+        and model.layers for Model class instances), not\n+        on layer names.\n+        Layers that have no weights are skipped.\n+        '''\n         if hasattr(self, 'flattened_layers'):\n             # support for legacy Sequential/Merge behavior\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n+        filtered_layers = []\n+        for layer in flattened_layers:\n+            weights = layer.trainable_weights + layer.non_trainable_weights\n+            if weights:\n+                filtered_layers.append(layer)\n+        flattened_layers = filtered_layers\n \n         if 'nb_layers' in f.attrs:\n             # legacy format\n@@ -2394,6 +2439,13 @@ class Container(Layer):\n         else:\n             # new file format\n             layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n+            filtered_layer_names = []\n+            for name in layer_names:\n+                g = f[name]\n+                weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+                if len(weight_names):\n+                    filtered_layer_names.append(name)\n+            layer_names = filtered_layer_names\n             if len(layer_names) != len(flattened_layers):\n                 raise Exception('You are trying to load a weight file '\n                                 'containing ' + str(len(layer_names)) +\n@@ -2406,7 +2458,6 @@ class Container(Layer):\n             for k, name in enumerate(layer_names):\n                 g = f[name]\n                 weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n-                if len(weight_names):\n                 weight_values = [g[weight_name] for weight_name in weight_names]\n                 layer = flattened_layers[k]\n                 symbolic_weights = layer.trainable_weights + layer.non_trainable_weights\n@@ -2423,7 +2474,6 @@ class Container(Layer):\n                                     ' elements.')\n                 weight_value_tuples += zip(symbolic_weights, weight_values)\n             K.batch_set_value(weight_value_tuples)\n-        f.close()\n \n     def _updated_config(self):\n         '''shared between different serialization methods'''\n@@ -2435,14 +2485,6 @@ class Container(Layer):\n             'config': config,\n             'keras_version': keras_version\n         }\n-\n-        if hasattr(self, 'optimizer'):\n-            model_config['optimizer'] = self.optimizer.get_config()\n-            model_config['loss'] = getattr(self.loss, '__name__', self.loss)\n-            model_config['sample_weight_mode'] = self.sample_weight_mode\n-\n-        if hasattr(self, 'loss_weights'):\n-            model_config['loss_weights'] = self.loss_weights\n         return model_config\n \n     def to_json(self, **kwargs):\n@@ -2462,7 +2504,7 @@ class Container(Layer):\n             if type(obj).__name__ == type.__name__:\n                 return obj.__name__\n \n-            raise TypeError('Not JSON Serializable')\n+            raise TypeError('Not JSON Serializable:', obj)\n \n         model_config = self._updated_config()\n         return json.dumps(model_config, default=get_json_type, **kwargs)\n\n@@ -608,8 +608,9 @@ class Model(Container):\n             self.targets.append(K.placeholder(ndim=len(shape), name=name + '_target'))\n \n         # prepare metrics\n+        self.metrics = metrics\n         self.metrics_names = ['loss']\n-        self.metrics = []\n+        self.metrics_tensors = []\n \n         # compute total loss\n         total_loss = None\n@@ -623,7 +624,7 @@ class Model(Container):\n             output_loss = weighted_loss(y_true, y_pred,\n                                         sample_weight, mask)\n             if len(self.outputs) > 1:\n-                self.metrics.append(output_loss)\n+                self.metrics_tensors.append(output_loss)\n                 self.metrics_names.append(self.output_names[i] + '_loss')\n             if total_loss is None:\n                 total_loss = loss_weight * output_loss\n@@ -648,21 +649,21 @@ class Model(Container):\n                     output_shape = self.internal_output_shapes[i]\n                     if output_shape[-1] == 1 or self.loss_functions[i] == objectives.binary_crossentropy:\n                         # case: binary accuracy\n-                        self.metrics.append(metrics_module.binary_accuracy(y_true, y_pred))\n+                        self.metrics_tensors.append(metrics_module.binary_accuracy(y_true, y_pred))\n                     elif self.loss_functions[i] == objectives.sparse_categorical_crossentropy:\n                         # case: categorical accuracy with sparse targets\n-                        self.metrics.append(\n+                        self.metrics_tensors.append(\n                             metrics_module.sparse_categorical_accuracy(y_true, y_pred))\n                     else:\n                         # case: categorical accuracy with dense targets\n-                        self.metrics.append(metrics_module.categorical_accuracy(y_true, y_pred))\n+                        self.metrics_tensors.append(metrics_module.categorical_accuracy(y_true, y_pred))\n                     if len(self.output_names) == 1:\n                         self.metrics_names.append('acc')\n                     else:\n                         self.metrics_names.append(self.output_layers[i].name + '_acc')\n                 else:\n                     metric_fn = metrics_module.get(metric)\n-                    self.metrics.append(metric_fn(y_true, y_pred))\n+                    self.metrics_tensors.append(metric_fn(y_true, y_pred))\n                     if len(self.output_names) == 1:\n                         self.metrics_names.append(metric_fn.__name__)\n                     else:\n@@ -698,7 +699,7 @@ class Model(Container):\n \n             # returns loss and metrics. Updates weights at each call.\n             self.train_function = K.function(inputs,\n-                                             [self.total_loss] + self.metrics,\n+                                             [self.total_loss] + self.metrics_tensors,\n                                              updates=updates,\n                                              **self._function_kwargs)\n \n@@ -713,7 +714,7 @@ class Model(Container):\n             # return loss and metrics, no gradient updates.\n             # Does update the network states.\n             self.test_function = K.function(inputs,\n-                                            [self.total_loss] + self.metrics,\n+                                            [self.total_loss] + self.metrics_tensors,\n                                             updates=self.state_updates,\n                                             **self._function_kwargs)\n \n\n@@ -1,13 +1,172 @@\n from __future__ import print_function\n import warnings\n import copy\n+import json\n+import os\n+import numpy as np\n \n from . import backend as K\n+from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n from .engine.topology import get_source_inputs, Node\n+from .optimizers import optimizer_from_config\n from .legacy.models import Graph\n \n \n+def save_model(model, filepath, overwrite=True):\n+\n+    def get_json_type(obj):\n+        # if obj is a serializable Keras class instance\n+        # e.g. optimizer, layer\n+        if hasattr(obj, 'get_config'):\n+            return {'class_name': obj.__class__.__name__,\n+                    'config': obj.get_config()}\n+\n+        # if obj is any numpy type\n+        if type(obj).__module__ == np.__name__:\n+            return obj.item()\n+\n+        # misc functions (e.g. loss function)\n+        if hasattr(obj, '__call__'):\n+            return obj.__name__\n+\n+        # if obj is a python 'type'\n+        if type(obj).__name__ == type.__name__:\n+            return obj.__name__\n+\n+        raise TypeError('Not JSON Serializable:', obj)\n+\n+    import h5py\n+    # if file exists and should not be overwritten\n+    if not overwrite and os.path.isfile(filepath):\n+        proceed = ask_to_proceed_with_overwrite(filepath)\n+        if not proceed:\n+            return\n+\n+    f = h5py.File(filepath, 'w')\n+\n+    f.attrs['model_config'] = json.dumps({\n+        'class_name': model.__class__.__name__,\n+        'config': model.get_config()\n+    }, default=get_json_type).encode('utf8')\n+\n+    model_weights_group = f.create_group('model_weights')\n+    model.save_weights_to_hdf5_group(model_weights_group)\n+\n+    if hasattr(model, 'optimizer'):\n+        f.attrs['training_config'] = json.dumps({\n+            'optimizer_config': {\n+                'class_name': model.optimizer.__class__.__name__,\n+                'config': model.optimizer.get_config()\n+            },\n+            'loss': model.loss,\n+            'metrics': model.metrics,\n+            'sample_weight_mode': model.sample_weight_mode,\n+            'loss_weights': model.loss_weights,\n+        }, default=get_json_type).encode('utf8')\n+\n+        # save optimizer weights\n+        symbolic_weights = getattr(model.optimizer, 'weights')\n+        if symbolic_weights:\n+            optimizer_weights_group = f.create_group('optimizer_weights')\n+            weight_values = K.batch_get_value(symbolic_weights)\n+            weight_names = []\n+            for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n+                if hasattr(w, 'name') and w.name:\n+                    name = str(w.name)\n+                else:\n+                    name = 'param_' + str(i)\n+                weight_names.append(name.encode('utf8'))\n+            optimizer_weights_group.attrs['weight_names'] = weight_names\n+            for name, val in zip(weight_names, weight_values):\n+                param_dset = optimizer_weights_group.create_dataset(\n+                    name,\n+                    val.shape,\n+                    dtype=val.dtype)\n+                if not val.shape:\n+                    # scalar\n+                    param_dset[()] = val\n+                else:\n+                    param_dset[:] = val\n+    f.flush()\n+    f.close()\n+\n+\n+def load_model(filepath, custom_objects={}):\n+\n+    def deserialize(obj):\n+        if type(obj) is list:\n+            deserialized = []\n+            for value in obj:\n+                if value in custom_objects:\n+                    deserialized.append(custom_objects[value])\n+                else:\n+                    deserialized.append(value)\n+            return deserialized\n+        if type(obj) is dict:\n+            deserialized = {}\n+            for key, value in obj.items():\n+                if value in custom_objects:\n+                    deserialized[key] = custom_objects[value]\n+                else:\n+                    deserialized[key] = value\n+            return deserialized\n+        if obj in custom_objects:\n+            return custom_objects[obj]\n+        return obj\n+\n+    import h5py\n+    f = h5py.File(filepath, mode='r')\n+\n+    # instantiate model\n+    model_config = f.attrs.get('model_config')\n+    if model_config is None:\n+        raise ValueError('No model found in config file.')\n+    model_config = json.loads(model_config.decode('utf-8'))\n+    model = model_from_config(model_config, custom_objects=custom_objects)\n+\n+    # set weights\n+    model.load_weights_from_hdf5_group(f['model_weights'])\n+\n+    # instantiate optimizer\n+    training_config = f.attrs.get('training_config')\n+    if training_config is None:\n+        warnings.warn('No training configuration found in save file: '\n+                      'the model was *not* compiled. Compile it manually.')\n+        f.close()\n+        return model\n+    training_config = json.loads(training_config.decode('utf-8'))\n+    optimizer_config = training_config['optimizer_config']\n+    optimizer = optimizer_from_config(optimizer_config)\n+\n+    # recover loss functions and metrics\n+    loss = deserialize(training_config['loss'])\n+    metrics = deserialize(training_config['metrics'])\n+    sample_weight_mode = training_config['sample_weight_mode']\n+    loss_weights = training_config['loss_weights']\n+\n+    # compile model\n+    model.compile(optimizer=optimizer,\n+                  loss=loss,\n+                  metrics=metrics,\n+                  loss_weights=loss_weights,\n+                  sample_weight_mode=sample_weight_mode)\n+\n+    # set optimizer weights\n+    if 'optimizer_weights' in f:\n+        # build train function (to get weight updates)\n+        if model.__class__.__name__ == 'Sequential':\n+            model.model._make_train_function()\n+        else:\n+            model._make_train_function()\n+        optimizer_weights_group = f['optimizer_weights']\n+        optimizer_weight_names = [n.decode('utf8') for n in optimizer_weights_group.attrs['weight_names']]\n+        optimizer_weight_values = [optimizer_weights_group[n] for n in optimizer_weight_names]\n+        model.optimizer.set_weights(optimizer_weight_values)\n+    f.close()\n+    return model\n+\n+\n def model_from_config(config, custom_objects={}):\n     from keras.utils.layer_utils import layer_from_config\n     if isinstance(config, list):\n@@ -362,6 +521,9 @@ class Sequential(Model):\n                            **kwargs)\n         self.optimizer = self.model.optimizer\n         self.loss = self.model.loss\n+        self.loss_weights = self.model.loss_weights\n+        self.metrics = self.model.metrics\n+        self.metrics_tensors = self.model.metrics_tensors\n         self.metrics_names = self.model.metrics_names\n         self.sample_weight_mode = self.model.sample_weight_mode\n \n\n@@ -1,6 +1,5 @@\n from __future__ import absolute_import\n from . import backend as K\n-import numpy as np\n from .utils.generic_utils import get_from_module\n from six.moves import zip\n \n@@ -11,8 +10,24 @@ def clip_norm(g, c, n):\n     return g\n \n \n-def kl_divergence(p, p_hat):\n-    return p_hat - p + p * K.log(p / p_hat)\n+def optimizer_from_config(config, custom_objects={}):\n+    all_classes = {\n+        'sgd': SGD,\n+        'rmsprop': RMSprop,\n+        'adagrad': Adagrad,\n+        'adadelta': Adadelta,\n+        'adam': Adam,\n+        'adamax': Adamax,\n+        'nadam': Nadam,\n+    }\n+    class_name = config['class_name']\n+    if class_name in custom_objects:\n+        cls = custom_objects[class_name]\n+    else:\n+        if class_name.lower() not in all_classes:\n+            raise ValueError('Optimizer class not found:', class_name)\n+        cls = all_classes[class_name.lower()]\n+    return cls.from_config(config['config'])\n \n \n class Optimizer(object):\n@@ -90,13 +105,17 @@ class Optimizer(object):\n         return K.batch_get_value(self.weights)\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__}\n+        config = {}\n         if hasattr(self, 'clipnorm'):\n             config['clipnorm'] = self.clipnorm\n         if hasattr(self, 'clipvalue'):\n             config['clipvalue'] = self.clipvalue\n         return config\n \n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n \n class SGD(Optimizer):\n     '''Stochastic gradient descent, with support for momentum,\n\n@@ -1,6 +1,8 @@\n from __future__ import absolute_import\n+from __future__ import print_function\n import h5py\n import numpy as np\n+import sys\n from collections import defaultdict\n \n \n@@ -69,3 +71,17 @@ def load_array(name):\n     a[:] = array[:]\n     f.close()\n     return a\n+\n+\n+def ask_to_proceed_with_overwrite(filepath):\n+    get_input = input\n+    if sys.version_info[:2] <= (2, 7):\n+        get_input = raw_input\n+    overwrite = get_input('[WARNING] %s already exists - overwrite? '\n+                          '[y/n]' % (filepath))\n+    while overwrite not in ['y', 'n']:\n+        overwrite = get_input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n+    if overwrite == 'n':\n+        return False\n+    print('[TIP] Next time specify overwrite=True!')\n+    return True\n\n@@ -4,10 +4,12 @@ import pytest\n from keras.models import Sequential\n from keras.engine.training import weighted_objective\n from keras.layers.core import TimeDistributedDense, Masking\n+from keras.utils.test_utils import keras_test\n from keras import objectives\n from keras import backend as K\n \n \n+@keras_test\n def test_masking():\n     np.random.seed(1337)\n     X = np.array([[[1], [1]],\n@@ -22,6 +24,7 @@ def test_masking():\n     assert loss == 0\n \n \n+@keras_test\n def test_loss_masking():\n     weighted_loss = weighted_objective(objectives.get('mae'))\n     shape = (3, 4, 2)\n\n@@ -8,6 +8,7 @@ from keras.utils.test_utils import get_test_data\n from keras.models import Sequential, Graph\n from keras.layers import Dense, Activation, RepeatVector, TimeDistributedDense, GRU\n from keras.utils import np_utils\n+from keras.utils.test_utils import keras_test\n \n nb_classes = 10\n batch_size = 128\n@@ -69,6 +70,7 @@ def create_temporal_sequential_model():\n     return model\n \n \n+@keras_test\n def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n                              X_train=X_train, Y_train=Y_train,\n                              X_test=X_test, Y_test=Y_test):\n@@ -108,6 +110,7 @@ model.compile(loss=loss, optimizer='rmsprop')\n standard_score_sequential = _test_weights_sequential(model)\n \n \n+@keras_test\n def test_sequential_class_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n@@ -115,6 +118,7 @@ def test_sequential_class_weights():\n     assert(score < standard_score_sequential)\n \n \n+@keras_test\n def test_sequential_sample_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n@@ -122,6 +126,7 @@ def test_sequential_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n+@keras_test\n def test_sequential_temporal_sample_weights():\n     model = create_temporal_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop',\n\n@@ -0,0 +1,151 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras.models import Model, Sequential\n+from keras.layers import Dense, Dropout, RepeatVector, TimeDistributed\n+from keras.layers import Input\n+from keras import optimizers\n+from keras import objectives\n+from keras import metrics\n+from keras.utils.test_utils import keras_test\n+from keras.models import save_model, load_model\n+\n+\n+@keras_test\n+def test_sequential_model_saving():\n+    model = Sequential()\n+    model.add(Dense(2, input_dim=3))\n+    model.add(Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+\n+    new_model = load_model(fname)\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2)\n+\n+    # test that new updates are the same with both models\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+    new_model.train_on_batch(x, y)\n+    out = model.predict(x)\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2)\n+\n+\n+@keras_test\n+def test_sequential_model_saving_2():\n+    # test with funkier config\n+    model = Sequential()\n+    model.add(Dense(2, input_dim=3))\n+    model.add(RepeatVector(3))\n+    model.add(TimeDistributed(Dense(3)))\n+    model.compile(loss=objectives.MSE,\n+                  optimizer=optimizers.RMSprop(lr=0.0001),\n+                  metrics=[metrics.categorical_accuracy],\n+                  sample_weight_mode='temporal')\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+\n+    new_model = load_model(fname)\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2)\n+\n+    # test that new updates are the same with both models\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3, 3))\n+    model.train_on_batch(x, y)\n+    new_model.train_on_batch(x, y)\n+    out = model.predict(x)\n+    out2 = new_model.predict(x)\n+    assert_allclose(out, out2)\n+\n+\n+@keras_test\n+def test_sequential_model_saving_3():\n+    # test with custom optimizer, loss\n+    custom_opt = optimizers.rmsprop\n+    custom_loss = objectives.mse\n+    model = Sequential()\n+    model.add(Dense(2, input_dim=3))\n+    model.add(Dense(3))\n+    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n+\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+\n+    model = load_model(fname,\n+                       custom_objects={'custom_opt': custom_opt,\n+                                       'custom_loss': custom_loss})\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2)\n+\n+\n+@keras_test\n+def test_fuctional_model_saving():\n+    input = Input(shape=(3,))\n+    x = Dense(2)(input)\n+    output = Dense(3)(x)\n+\n+    model = Model(input, output)\n+    model.compile(loss=objectives.MSE,\n+                  optimizer=optimizers.RMSprop(lr=0.0001),\n+                  metrics=[metrics.categorical_accuracy])\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+\n+    model = load_model(fname)\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2)\n+\n+\n+@keras_test\n+def test_saving_without_compilation():\n+    model = Sequential()\n+    model.add(Dense(2, input_dim=3))\n+    model.add(Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+    model = load_model(fname)\n+\n+\n+@keras_test\n+def test_saving_right_after_compilation():\n+    model = Sequential()\n+    model.add(Dense(2, input_dim=3))\n+    model.add(Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+    model.model._make_train_function()\n+\n+    fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n+    save_model(model, fname)\n+    model = load_model(fname)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05abe814acd329d426645ecb4ca8c1e39defdac0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7851 | Contributors (this commit): 47 | Commits (past 90d): 13 | Contributors (cumulative): 47 | DMM Complexity: 0.0\n\nDIFF:\n@@ -37,6 +37,8 @@ def save_model(model, filepath, overwrite=True):\n         raise TypeError('Not JSON Serializable:', obj)\n \n     import h5py\n+    from keras import __version__ as keras_version\n+\n     # if file exists and should not be overwritten\n     if not overwrite and os.path.isfile(filepath):\n         proceed = ask_to_proceed_with_overwrite(filepath)\n@@ -44,7 +46,7 @@ def save_model(model, filepath, overwrite=True):\n             return\n \n     f = h5py.File(filepath, 'w')\n-\n+    f.attrs['keras_version'] = str(keras_version).encode('utf8')\n     f.attrs['model_config'] = json.dumps({\n         'class_name': model.__class__.__name__,\n         'config': model.get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b91854ea9d9764ab670b7cccdcec40ae964195be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 163 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ def test_sequential_model_saving():\n \n     new_model = load_model(fname)\n     out2 = new_model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n     # test that new updates are the same with both models\n     x = np.random.random((1, 3))\n@@ -38,7 +38,7 @@ def test_sequential_model_saving():\n     new_model.train_on_batch(x, y)\n     out = model.predict(x)\n     out2 = new_model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n \n @keras_test\n@@ -62,7 +62,7 @@ def test_sequential_model_saving_2():\n \n     new_model = load_model(fname)\n     out2 = new_model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n     # test that new updates are the same with both models\n     x = np.random.random((1, 3))\n@@ -71,7 +71,7 @@ def test_sequential_model_saving_2():\n     new_model.train_on_batch(x, y)\n     out = model.predict(x)\n     out2 = new_model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n \n @keras_test\n@@ -96,7 +96,7 @@ def test_sequential_model_saving_3():\n                        custom_objects={'custom_opt': custom_opt,\n                                        'custom_loss': custom_loss})\n     out2 = model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n \n @keras_test\n@@ -119,7 +119,7 @@ def test_fuctional_model_saving():\n \n     model = load_model(fname)\n     out2 = model.predict(x)\n-    assert_allclose(out, out2)\n+    assert_allclose(out, out2, atol=1e-05)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8778add0d66aed64a8970c34576bf5800bc19170", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 176 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import pytest\n+import os\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -17,7 +18,7 @@ def test_sequential_model_saving():\n     model = Sequential()\n     model.add(Dense(2, input_dim=3))\n     model.add(Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n \n     x = np.random.random((1, 3))\n     y = np.random.random((1, 3))\n@@ -28,6 +29,8 @@ def test_sequential_model_saving():\n     save_model(model, fname)\n \n     new_model = load_model(fname)\n+    os.remove(fname)\n+\n     out2 = new_model.predict(x)\n     assert_allclose(out, out2, atol=1e-05)\n \n@@ -61,6 +64,8 @@ def test_sequential_model_saving_2():\n     save_model(model, fname)\n \n     new_model = load_model(fname)\n+    os.remove(fname)\n+\n     out2 = new_model.predict(x)\n     assert_allclose(out, out2, atol=1e-05)\n \n@@ -95,6 +100,8 @@ def test_sequential_model_saving_3():\n     model = load_model(fname,\n                        custom_objects={'custom_opt': custom_opt,\n                                        'custom_loss': custom_loss})\n+    os.remove(fname)\n+\n     out2 = model.predict(x)\n     assert_allclose(out, out2, atol=1e-05)\n \n@@ -118,6 +125,8 @@ def test_fuctional_model_saving():\n     save_model(model, fname)\n \n     model = load_model(fname)\n+    os.remove(fname)\n+\n     out2 = model.predict(x)\n     assert_allclose(out, out2, atol=1e-05)\n \n@@ -132,6 +141,7 @@ def test_saving_without_compilation():\n     fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n     save_model(model, fname)\n     model = load_model(fname)\n+    os.remove(fname)\n \n \n @keras_test\n@@ -145,6 +155,7 @@ def test_saving_right_after_compilation():\n     fname = 'tmp_' + str(np.random.randint(10000)) + '.h5'\n     save_model(model, fname)\n     model = load_model(fname)\n+    os.remove(fname)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0179bad2f16792eef30210e206792abd0272bbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 25 | Files Changed: 7 | Hunks: 23 | Methods Changed: 10 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 117 | Churn Cumulative: 1224 | Contributors (this commit): 14 | Commits (past 90d): 14 | Contributors (cumulative): 23 | DMM Complexity: 0.5405405405405406\n\nDIFF:\n@@ -19,8 +19,7 @@ maxlen = 100  # cut texts after this number of words (among top max_features mos\n batch_size = 32\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n-                                                      test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n\n@@ -30,8 +30,7 @@ hidden_dims = 250\n nb_epoch = 2\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n-                                                      test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n\n@@ -40,7 +40,7 @@ Only 2 epochs are needed as the dataset is very small.\n '''\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n\n@@ -28,8 +28,7 @@ maxlen = 80  # cut texts after this number of words (among top max_features most\n batch_size = 32\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n-                                                      test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n\n@@ -4,26 +4,58 @@ import gzip\n from ..utils.data_utils import get_file\n from six.moves import zip\n import numpy as np\n+import sys\n \n \n-def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0,\n-              maxlen=None, test_split=0.2, seed=113,\n+def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n+              maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n+    '''\n+    # Arguments\n+        path: where to store the data (in `/.keras/dataset`)\n+        nb_words: max number of words to include. Words are ranked\n+            by how often they occur (in the training set) and only\n+            the most frequent words are kept\n+        skip_top: skip the top N most frequently occuring words\n+            (which may not be informative).\n+        maxlen: truncate sequences after this length.\n+        seed: random seed for sample shuffling.\n+        start_char: The start of a sequence will be marked with this character.\n+            Set to 1 because 0 is usually the padding character.\n+        oov_char: words that were cut out because of the `nb_words`\n+            or `skip_top` limit will be replaced with this character.\n+        index_from: index actual words with this index and higher.\n \n-    path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/imdb.pkl\")\n+    Note that the 'out of vocabulary' character is only used for\n+    words that were present in the training set but are not included\n+    because they're not making the `nb_words` cut here.\n+    Words that were not seen in the trining set but are in the test set\n+    have simply been skipped.\n+    '''\n+    path = get_file(path,\n+                    origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n+                    md5_hash='d091312047c43cf9e4e38fef92437263')\n \n-    if path.endswith(\".gz\"):\n+    if path.endswith('.gz'):\n         f = gzip.open(path, 'rb')\n     else:\n         f = open(path, 'rb')\n \n-    X, labels = cPickle.load(f)\n+    (x_train, labels_train), (x_test, labels_test) = cPickle.load(f)\n     f.close()\n \n     np.random.seed(seed)\n-    np.random.shuffle(X)\n+    np.random.shuffle(x_train)\n     np.random.seed(seed)\n-    np.random.shuffle(labels)\n+    np.random.shuffle(labels_train)\n+\n+    np.random.seed(seed * 2)\n+    np.random.shuffle(x_test)\n+    np.random.seed(seed * 2)\n+    np.random.shuffle(labels_test)\n+\n+    X = x_train + x_test\n+    labels = labels_train + labels_test\n \n     if start_char is not None:\n         X = [[start_char] + [w + index_from for w in x] for x in X]\n@@ -60,10 +92,25 @@ def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0,\n             nX.append(nx)\n         X = nX\n \n-    X_train = np.array(X[:int(len(X) * (1 - test_split))])\n-    y_train = np.array(labels[:int(len(X) * (1 - test_split))])\n+    X_train = np.array(X[:len(x_train)])\n+    y_train = np.array(labels[:len(x_train)])\n \n-    X_test = np.array(X[int(len(X) * (1 - test_split)):])\n-    y_test = np.array(labels[int(len(X) * (1 - test_split)):])\n+    X_test = np.array(X[len(x_train):])\n+    y_test = np.array(labels[len(x_train):])\n \n     return (X_train, y_train), (X_test, y_test)\n+\n+\n+def get_word_index(path='imdb_word_index.pkl'):\n+    path = get_file(path,\n+                    origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl',\n+                    md5_hash='72d94b01291be4ff843198d3b0e1e4d7')\n+    f = open(path, 'rb')\n+\n+    if sys.version_info < (3,):\n+        data = cPickle.load(f)\n+    else:\n+        data = cPickle.load(f, encoding='latin1')\n+\n+    f.close()\n+    return data\n\n@@ -7,11 +7,11 @@ import numpy as np\n import sys\n \n \n-def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0,\n+def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n \n-    path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n+    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.pkl')\n     f = open(path, 'rb')\n     X, labels = cPickle.load(f)\n     f.close()\n@@ -62,14 +62,14 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0,\n     return (X_train, y_train), (X_test, y_test)\n \n \n-def get_word_index(path=\"reuters_word_index.pkl\"):\n-    path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n+def get_word_index(path='reuters_word_index.pkl'):\n+    path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl')\n     f = open(path, 'rb')\n \n     if sys.version_info < (3,):\n         data = cPickle.load(f)\n     else:\n-        data = cPickle.load(f, encoding=\"latin1\")\n+        data = cPickle.load(f, encoding='latin1')\n \n     f.close()\n     return data\n\n@@ -5,6 +5,7 @@ import tarfile\n import os\n import sys\n import shutil\n+import hashlib\n from six.moves.urllib.request import urlopen\n from six.moves.urllib.error import URLError, HTTPError\n \n@@ -36,7 +37,7 @@ else:\n     from six.moves.urllib.request import urlretrieve\n \n \n-def get_file(fname, origin, untar=False):\n+def get_file(fname, origin, untar=False, md5_hash=None):\n     datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n     if not os.access(datadir_base, os.W_OK):\n         datadir_base = os.path.join('/tmp', '.keras')\n@@ -50,7 +51,18 @@ def get_file(fname, origin, untar=False):\n     else:\n         fpath = os.path.join(datadir, fname)\n \n-    if not os.path.exists(fpath):\n+    download = False\n+    if os.path.exists(fpath):\n+        # file found; verify integrity if a hash was provided\n+        if md5_hash is not None:\n+            if not validate_file(fpath, md5_hash):\n+                print('A local file was found, but it seems to be '\n+                      'incomplete or outdated.')\n+                download = True\n+    else:\n+        download = True\n+\n+    if download:\n         print('Downloading data from',  origin)\n         global progbar\n         progbar = None\n@@ -93,3 +105,14 @@ def get_file(fname, origin, untar=False):\n         return untar_fpath\n \n     return fpath\n+\n+\n+def validate_file(fpath, md5_hash):\n+    hasher = hashlib.md5()\n+    with open(fpath, 'rb') as f:\n+        buf = f.read()\n+        hasher.update(buf)\n+    if str(hasher.hexdigest()) == str(md5_hash):\n+        return True\n+    else:\n+        return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
