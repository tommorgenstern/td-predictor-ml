{"custom_id": "keras#b7152afc52b3b8659d765923594af023e1843788", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -43,11 +43,11 @@ class LeakyReLU(Layer):\n                 \"Use `negative_slope` instead.\"\n             )\n         super().__init__(**kwargs)\n-        if negative_slope is None:\n+        if negative_slope is None or negative_slope < 0:\n             raise ValueError(\n                 \"The negative_slope value of a Leaky ReLU layer \"\n-                \"cannot be None. Expected a float. Received: \"\n-                f\"negative_slope={negative_slope}\"\n+                \"cannot be None or negative value. Expected a float.\"\n+                f\" Received: negative_slope={negative_slope}\"\n             )\n         self.supports_masking = True\n         self.negative_slope = negative_slope\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8a06ffb959f8f52a9c3149dda8715b9b9a117b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -286,6 +286,11 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n                 \"output_b\": [\"mean_squared_error\", \"accuracy\"],\n             },\n         )\n+        # Check dict outputs.\n+        outputs = model.predict(x)\n+        self.assertIsInstance(outputs, dict)\n+        self.assertEqual(outputs[\"output_a\"].shape, (8, 1))\n+        self.assertEqual(outputs[\"output_b\"].shape, (8, 1))\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(\n             x,\n@@ -332,6 +337,11 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n                 \"output_b\": [\"mean_squared_error\", \"accuracy\"],\n             },\n         )\n+        # Check list outputs.\n+        outputs = model.predict(x)\n+        self.assertIsInstance(outputs, list)\n+        self.assertEqual(outputs[0].shape, (8, 1))\n+        self.assertEqual(outputs[1].shape, (8, 1))\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n\n@@ -4,6 +4,7 @@ import tree\n \n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n+from keras.backend.config import backend\n from keras.ops.operation import Operation\n from keras.utils.nest import pack_sequence_as\n \n@@ -46,10 +47,21 @@ class Function(Operation):\n     def __init__(self, inputs, outputs, name=None):\n         super().__init__(name=name)\n \n+        if backend() == \"tensorflow\":\n+            # Temporary work around for\n+            # https://github.com/keras-team/keras/issues/931\n+            # This stop tensorflow from wrapping tf.function output in a\n+            # _DictWrapper object.\n+            _self_setattr_tracking = getattr(\n+                self, \"_self_setattr_tracking\", True\n+            )\n+            self._self_setattr_tracking = False\n         self._inputs_struct = tree.map_structure(lambda x: x, inputs)\n         self._outputs_struct = tree.map_structure(lambda x: x, outputs)\n         self._inputs = tree.flatten(inputs)\n         self._outputs = tree.flatten(outputs)\n+        if backend() == \"tensorflow\":\n+            self._self_setattr_tracking = _self_setattr_tracking\n \n         (nodes, nodes_by_depth, operations, operations_by_depth) = map_graph(\n             self._inputs, self._outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e66fcecb08abd6bd36f13fff12cf8445d7e0ee3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -340,11 +340,13 @@ class OrthogonalRegularizer(Regularizer):\n \n def validate_float_arg(value, name):\n     \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n-    if not isinstance(value, (float, int)) or (\n-        math.isinf(value) or math.isnan(value)\n+    if (\n+        not isinstance(value, (float, int))\n+        or (math.isinf(value) or math.isnan(value))\n+        or value < 0\n     ):\n         raise ValueError(\n-            f\"Invalid value for argument {name}: expected a float. \"\n+            f\"Invalid value for argument {name}: expected a non-negative float.\"\n             f\"Received: {name}={value}\"\n         )\n     return float(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85dcc8b9f5168f6154a10d7fb3bce14a4d506e8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 15 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 85 | Churn Cumulative: 141 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,7 +44,9 @@ def test_model_fit():\n     # Fit from numpy arrays:\n     with strategy.scope():\n         model.compile(\n-            optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.01),\n+            optimizer=optimizers.LossScaleOptimizer(\n+                optimizers.SGD(learning_rate=0.001, momentum=0.01)\n+            ),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n             # TODO(scottzhu): Find out where is the variable\n\n@@ -89,8 +89,20 @@ class LossScaleOptimizer(optimizer.Optimizer):\n                 \"must be built (i.e. its variables must have been created). \"\n                 \"You can build it via `optimizer.build(trainable_variables)`.\"\n             )\n+        finite = self.check_finite(grads)\n+        return ops.cond(\n+            finite,\n+            lambda: self._stateless_handle_finite_grads(\n+                optimizer_variables, grads, trainable_variables\n+            ),\n+            lambda: self._stateless_handle_non_finite_grads(\n+                optimizer_variables, trainable_variables\n+            ),\n+        )\n \n-        def handle_finite_grads():\n+    def _stateless_handle_finite_grads(\n+        self, optimizer_variables, grads, trainable_variables\n+    ):\n         def upscale():\n             mapping = list(zip(self.variables, optimizer_variables))\n             with backend.StatelessScope(state_mapping=mapping) as scope:\n@@ -130,7 +142,9 @@ class LossScaleOptimizer(optimizer.Optimizer):\n         new_optimizer_variables = own_variables + new_inner_variables\n         return new_trainable_variables, new_optimizer_variables\n \n-        def handle_non_finite_grads():\n+    def _stateless_handle_non_finite_grads(\n+        self, optimizer_variables, trainable_variables\n+    ):\n         mapping = list(zip(self.variables, optimizer_variables))\n         with backend.StatelessScope(state_mapping=mapping) as scope:\n             self.step_counter.assign(0)\n@@ -140,9 +154,6 @@ class LossScaleOptimizer(optimizer.Optimizer):\n             new_optimizer_variables.append(scope.get_current_value(v))\n         return trainable_variables, new_optimizer_variables\n \n-        finite = self.check_finite(grads)\n-        return ops.cond(finite, handle_finite_grads, handle_non_finite_grads)\n-\n     def apply(self, grads, trainable_variables=None):\n         # Optionally build optimizer.\n         if not self.built:\n@@ -150,7 +161,12 @@ class LossScaleOptimizer(optimizer.Optimizer):\n                 self.build(trainable_variables)\n             self.built = True\n \n-        def handle_finite_grads():\n+        if backend.backend() == \"tensorflow\":\n+            self._tf_apply(grads, trainable_variables)\n+        else:\n+            self._common_apply(grads, trainable_variables)\n+\n+    def _stateful_handle_finite_grads(self, grads, trainable_variables):\n         scale = self.dynamic_scale\n         # Unscale gradients.\n         unscaled_grads = [\n@@ -174,13 +190,62 @@ class LossScaleOptimizer(optimizer.Optimizer):\n             increment,\n         )\n \n-        def handle_non_finite_grads():\n+    def _stateful_handle_non_finite_grads(self):\n         # If any inf or nan in grads, downscale loss and reset counter.\n         self.step_counter.assign(0)\n         self.dynamic_scale.assign(self.dynamic_scale / 2.0)\n \n+    def _common_apply(self, grads, trainable_variables=None):\n         finite = self.check_finite(grads)\n-        ops.cond(finite, handle_finite_grads, handle_non_finite_grads)\n+        ops.cond(\n+            finite,\n+            lambda: self._stateful_handle_finite_grads(\n+                grads, trainable_variables\n+            ),\n+            self._stateful_handle_non_finite_grads,\n+        )\n+\n+    def _tf_apply(self, grads, trainable_variables=None):\n+        \"\"\"Tensorflow specific logic for apply, which handles distribution.\"\"\"\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        if tf.distribute.in_cross_replica_context():\n+            raise ValueError(\"apply() must be called in a replica context.\")\n+\n+        if tf.__internal__.distribute.strategy_supports_no_merge_call():\n+            self._common_apply(grads, trainable_variables=trainable_variables)\n+        else:\n+\n+            def _handle_cross_replica(distribution, grads, trainable_variables):\n+                finite_per_replica = (\n+                    distribution.extended.call_for_each_replica(\n+                        self.check_finite, args=(grads,)\n+                    )\n+                )\n+                # Each replica computed the same `finite` value, since\n+                # `grads` is all-reduced across replicas. Arbitrarily take\n+                # `finite` from the first replica.\n+                finite = distribution.experimental_local_results(\n+                    finite_per_replica\n+                )[0]\n+\n+                def apply_fn():\n+                    distribution.extended.call_for_each_replica(\n+                        self._stateful_handle_finite_grads,\n+                        args=(grads, trainable_variables),\n+                    )\n+\n+                # Note: We must call this cond() in a cross-replica context.\n+                # DistributionStrategy does not support having a cond in a\n+                # replica context with a branch that calls `merge_call`, and\n+                # self._optimizer.apply_gradients calls `merge_call`.\n+                ops.cond(\n+                    finite, apply_fn, self._stateful_handle_non_finite_grads\n+                )\n+\n+            tf.distribute.get_replica_context().merge_call(\n+                _handle_cross_replica, args=(grads, trainable_variables)\n+            )\n \n     def check_finite(self, grads):\n         tensor_grads = [g for g in grads if g is not None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfcd9e5751cc83b1f02d0fb7a12a67bc081d386b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -119,12 +119,13 @@ class Progbar:\n                 numdigits = int(math.log10(self.target)) + 1\n                 bar = (\"%\" + str(numdigits) + \"d/%d\") % (current, self.target)\n                 bar = f\"\\x1b[1m{bar}\\x1b[0m \"\n+                special_char_len += 8\n                 prog = float(current) / self.target\n                 prog_width = int(self.width * prog)\n \n                 if prog_width > 0:\n                     bar += \"\\33[32m\" + \"━\" * prog_width + \"\\x1b[0m\"\n-                    special_char_len += 17\n+                    special_char_len += 9\n                 bar += \"\\33[37m\" + \"━\" * (self.width - prog_width) + \"\\x1b[0m\"\n                 special_char_len += 9\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c1c1d03351066864186b6d536157f97f81be06e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 463 | Lines Deleted: 72 | Files Changed: 13 | Hunks: 46 | Methods Changed: 28 | Complexity Δ (Sum/Max): 21/12 | Churn Δ: 535 | Churn Cumulative: 3187 | Contributors (this commit): 10 | Commits (past 90d): 32 | Contributors (cumulative): 59 | DMM Complexity: 0.9541284403669725\n\nDIFF:\n@@ -1,6 +1,7 @@\n import tensorflow as tf\n \n from keras import backend\n+from keras.backend.common import KerasVariable\n from keras.optimizers import base_optimizer\n \n \n@@ -39,6 +40,24 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             \"(as it is incompatible with tf.distribute).\"\n         )\n \n+    def assign_add(self, variable, value):\n+        if isinstance(variable, KerasVariable):\n+            variable = variable.value\n+        value = tf.cast(value, variable.dtype)\n+        if isinstance(value, tf.IndexedSlices):\n+            variable.scatter_add(value)\n+        else:\n+            variable.assign_add(value)\n+\n+    def assign_sub(self, variable, value):\n+        if isinstance(variable, KerasVariable):\n+            variable = variable.value\n+        value = tf.cast(value, variable.dtype)\n+        if isinstance(value, tf.IndexedSlices):\n+            variable.scatter_sub(value)\n+        else:\n+            variable.assign_sub(value)\n+\n     def _var_key(self, variable):\n         if isinstance(variable, backend.Variable):\n             variable = variable.value  # Convert to tf.Variable\n@@ -63,7 +82,7 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n                 if self._use_weight_decay(variable):\n                     lr = tf.cast(self.learning_rate, variable.dtype)\n                     wd = tf.cast(self.weight_decay, variable.dtype)\n-                    variable.assign(variable - variable * wd * lr)\n+                    variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n                 if isinstance(variable, backend.Variable):\n@@ -99,7 +118,6 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n \n         def apply_grad_to_update_var(var, grad):\n             learning_rate = self._get_current_learning_rate()\n-            grad = tf.convert_to_tensor(grad)\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n@@ -123,7 +141,7 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n                     ),\n                     false_fn=lambda: None,\n                 )\n-        self.iterations.assign(self.iterations + 1)\n+        self.iterations.assign_add(1)\n \n     def _overwrite_model_variables_with_average_value(self, var_list):\n         \"\"\"Overwrite model variables with their moving average values.\n\n@@ -0,0 +1,223 @@\n+from unittest import mock\n+\n+import pytest\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras import backend\n+from keras import optimizers\n+from keras import testing\n+\n+TEST_CASES = [\n+    {\n+        \"testcase_name\": \"adadelta\",\n+        \"optimizer_class\": optimizers.Adadelta,\n+        \"expect_model_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"adafactor\",\n+        \"optimizer_class\": optimizers.Adafactor,\n+        \"init_kwargs\": {\"clip_threshold\": 0.5},\n+        \"expect_model_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"adagrad\",\n+        \"optimizer_class\": optimizers.Adagrad,\n+        \"expect_model_sparse_variable_updates\": True,\n+        \"expect_optimizer_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"adam\",\n+        \"optimizer_class\": optimizers.Adam,\n+    },\n+    {\n+        \"testcase_name\": \"adam_amsgrad\",\n+        \"optimizer_class\": optimizers.Adam,\n+        \"init_kwargs\": {\"amsgrad\": True},\n+    },\n+    {\n+        \"testcase_name\": \"adamax\",\n+        \"optimizer_class\": optimizers.Adamax,\n+    },\n+    {\n+        \"testcase_name\": \"adamw\",\n+        \"optimizer_class\": optimizers.AdamW,\n+    },\n+    {\n+        \"testcase_name\": \"adamw_amsgrad\",\n+        \"optimizer_class\": optimizers.AdamW,\n+        \"init_kwargs\": {\"amsgrad\": True},\n+    },\n+    {\n+        \"testcase_name\": \"ftrl\",\n+        \"optimizer_class\": optimizers.Ftrl,\n+    },\n+    {\n+        \"testcase_name\": \"lion\",\n+        \"optimizer_class\": optimizers.Lion,\n+    },\n+    {\n+        \"testcase_name\": \"loss_scale_optimizer_sgd\",\n+        \"optimizer_class\": lambda: optimizers.LossScaleOptimizer(\n+            optimizers.SGD(learning_rate=0.5)\n+        ),\n+        \"expect_model_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"nadam\",\n+        \"optimizer_class\": optimizers.Nadam,\n+    },\n+    {\n+        \"testcase_name\": \"rmsprop\",\n+        \"optimizer_class\": optimizers.RMSprop,\n+        \"expect_model_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"rmsprop_momentum\",\n+        \"optimizer_class\": optimizers.RMSprop,\n+        \"init_kwargs\": {\"momentum\": 0.05},\n+    },\n+    {\n+        \"testcase_name\": \"rmsprop_momentum_centered\",\n+        \"optimizer_class\": optimizers.RMSprop,\n+        \"init_kwargs\": {\"momentum\": 0.05, \"centered\": True},\n+    },\n+    {\n+        \"testcase_name\": \"sgd\",\n+        \"optimizer_class\": optimizers.SGD,\n+        \"expect_model_sparse_variable_updates\": True,\n+    },\n+    {\n+        \"testcase_name\": \"sgd_momentum\",\n+        \"optimizer_class\": optimizers.SGD,\n+        \"init_kwargs\": {\"momentum\": 0.05},\n+    },\n+    {\n+        \"testcase_name\": \"sgd_momentum_nesterov\",\n+        \"optimizer_class\": optimizers.SGD,\n+        \"init_kwargs\": {\"momentum\": 0.05, \"nesterov\": True},\n+    },\n+]\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\",\n+    reason=\"The TensorFlow sparse test can only run with TensorFlow backend.\",\n+)\n+class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_sparse_gradients(\n+        self,\n+        optimizer_class,\n+        init_kwargs={},\n+        expect_model_sparse_variable_updates=False,\n+        expect_optimizer_sparse_variable_updates=False,\n+    ):\n+        # This test verifies that:\n+        # - Optimizers use Keras ops everywhere instead of native operators\n+        #   (e.g. `ops.add()` instead of `+`) where sparse gradients are handled\n+        # - The used ops handle sparse gradients (`tf.IndexedSlices`)\n+        # - Optimizers use `self.assign/assign_add/assign_sub` instead of\n+        #   calling the method on the variable directly. Otherwise, the sparse\n+        #   updates are densified before being applied.\n+        # - For some optimizers, a sparse gradient actually results in a sparse\n+        #   variable update as per `expect_model_sparse_variable_updates` and\n+        #   `expect_optimizer_sparse_variable_updates`\n+\n+        model_variable = backend.Variable(initializer=tf.ones, shape=(5, 10))\n+        optimizer = optimizer_class(**init_kwargs)\n+\n+        # Mocking \"tensorflow.Variable\" won't work as it gets substituted with\n+        # the resource variable class.\n+        tf_variable_class = model_variable._value.__class__\n+\n+        optimizer_to_patch = (\n+            optimizer.inner_optimizer\n+            if isinstance(optimizer, optimizers.LossScaleOptimizer)\n+            else optimizer\n+        )\n+\n+        model_sparse_variable_updates = False\n+        optimizer_sparse_variable_updates = False\n+\n+        def mock_optimizer_assign(variable, value):\n+            nonlocal model_sparse_variable_updates\n+            nonlocal optimizer_sparse_variable_updates\n+            if isinstance(variable, backend.Variable):\n+                variable = variable._value\n+            if isinstance(value, tf.IndexedSlices):\n+                if variable is model_variable._value:\n+                    model_sparse_variable_updates = True\n+                elif any(variable is v._value for v in optimizer.variables):\n+                    optimizer_sparse_variable_updates = True\n+\n+        def mock_variable_assign(variable, value):\n+            # Make an exception for scalar variables\n+            if len(variable.shape):\n+                pytest.fail(\n+                    \"Optimizer is calling `assign`, `assign_add` or \"\n+                    \"`assign_sub` directly on a variable. Use \"\n+                    \"`self.assign/assign_add/assign_sub(variable, value)` \"\n+                    \"instead to support sparse updates.\"\n+                )\n+\n+        # patch \"_apply_weight_decay\" to exclude this special case.\n+        # patch the optimizer \"assign\" methods to detect sparse udpates.\n+        # patch the tf.Variable \"assign\" methods to detect direct assign calls.\n+        with mock.patch.object(\n+            optimizer_to_patch, \"_apply_weight_decay\", autospec=True\n+        ), mock.patch.object(\n+            optimizer_to_patch, \"assign\", autospec=True\n+        ) as optimizer_assign, mock.patch.object(\n+            optimizer_to_patch, \"assign_add\", autospec=True\n+        ) as optimizer_assign_add, mock.patch.object(\n+            optimizer_to_patch, \"assign_sub\", autospec=True\n+        ) as optimizer_assign_sub, mock.patch.object(\n+            tf_variable_class, \"assign\", autospec=True\n+        ) as variable_assign, mock.patch.object(\n+            tf_variable_class, \"assign_add\", autospec=True\n+        ) as variable_assign_add, mock.patch.object(\n+            tf_variable_class, \"assign_sub\", autospec=True\n+        ) as variable_assign_sub:\n+            optimizer_assign.side_effect = mock_optimizer_assign\n+            optimizer_assign_add.side_effect = mock_optimizer_assign\n+            optimizer_assign_sub.side_effect = mock_optimizer_assign\n+            variable_assign.side_effect = mock_variable_assign\n+            variable_assign_add.side_effect = mock_variable_assign\n+            variable_assign_sub.side_effect = mock_variable_assign\n+\n+            grad = tf.IndexedSlices(\n+                values=tf.ones((3, 10)), indices=(0, 2, 4), dense_shape=(5, 10)\n+            )\n+            optimizer.apply_gradients(zip([grad], [model_variable]))\n+\n+        self.assertEqual(\n+            model_sparse_variable_updates, expect_model_sparse_variable_updates\n+        )\n+        self.assertEqual(\n+            optimizer_sparse_variable_updates,\n+            expect_optimizer_sparse_variable_updates,\n+        )\n+\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_sparse_correctness(\n+        self, optimizer_class, init_kwargs={}, **kwargs\n+    ):\n+        # This test verifies that applying a sparse gradient gives the same\n+        # numerical results as the same dense gradient.\n+\n+        optimizer_sparse = optimizer_class(**init_kwargs)\n+        optimizer_dense = optimizer_class(**init_kwargs)\n+        var_sparse = backend.Variable(initializer=tf.ones, shape=(5, 3, 2))\n+        var_dense = backend.Variable(initializer=tf.ones, shape=(5, 3, 2))\n+\n+        for i in range(5):\n+            grads_sparse = tf.IndexedSlices(\n+                values=tf.ones((3, 3, 2)) * (10.0 - i),\n+                indices=(0, 2, 4),\n+                dense_shape=(5, 3, 2),\n+            )\n+            grads_dense = tf.convert_to_tensor(grads_sparse)\n+            optimizer_sparse.apply_gradients(zip([grads_sparse], [var_sparse]))\n+            optimizer_dense.apply_gradients(zip([grads_dense], [var_dense]))\n+            self.assertAllClose(var_sparse, var_dense)\n\n@@ -95,16 +95,28 @@ class Adadelta(optimizer.Optimizer):\n         ]\n \n         def rms(x):\n-            return ops.sqrt(x + self.epsilon)\n+            return ops.sqrt(ops.add(x, self.epsilon))\n \n-        accumulated_grad.assign(\n-            rho * accumulated_grad + (1 - rho) * grad * grad\n+        self.assign(\n+            accumulated_grad,\n+            ops.add(\n+                rho * accumulated_grad, ops.multiply(1 - rho, ops.square(grad))\n+            ),\n         )\n-        delta_var = -rms(accumulated_delta_var) * grad / rms(accumulated_grad)\n-        accumulated_delta_var.assign(\n-            rho * accumulated_delta_var + (1 - rho) * delta_var * delta_var\n+        delta_var = ops.negative(\n+            ops.divide(\n+                ops.multiply(rms(accumulated_delta_var), grad),\n+                rms(accumulated_grad),\n             )\n-        variable.assign(variable + lr * delta_var)\n+        )\n+        self.assign(\n+            accumulated_delta_var,\n+            ops.add(\n+                ops.multiply(rho, accumulated_delta_var),\n+                ops.multiply(1 - rho, ops.square(delta_var)),\n+            ),\n+        )\n+        self.assign_add(variable, ops.multiply(lr, delta_var))\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -147,35 +147,43 @@ class Adafactor(optimizer.Optimizer):\n \n         rho_t = ops.minimum(lr, 1 / ops.sqrt(local_step))\n         alpha_t = ops.maximum(epsilon_2, self._rms(variable)) * rho_t\n-        regulated_grad_square = ops.square(gradient) + self.epsilon_1\n+        regulated_grad_square = ops.add(ops.square(gradient), self.epsilon_1)\n         beta_2_t = 1 - ops.power(local_step, self.beta_2_decay)\n \n         if len(variable.shape) >= 2:\n             # `r` deletes the last dimension of gradient, so it is of shape\n             # `gradient.shape[:-1]`.\n-            r.assign(\n+            self.assign(\n+                r,\n                 beta_2_t * r\n-                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-1)\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-1),\n             )\n             # `c` deletes the second last dimension of gradient, so it is of\n             # shape `gradient.shape[:-2] + gradient.shape[-1]`.\n-            c.assign(\n+            self.assign(\n+                c,\n                 beta_2_t * c\n-                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-2)\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-2),\n             )\n-            v.assign(\n+            self.assign(\n+                v,\n                 ops.expand_dims(\n                     r / ops.mean(r, axis=-1, keepdims=True), axis=-1\n                 )\n-                * ops.expand_dims(c, -2)\n+                * ops.expand_dims(c, -2),\n             )\n         else:\n-            v.assign(beta_2_t * v + (1 - beta_2_t) * regulated_grad_square)\n+            self.assign(\n+                v, beta_2_t * v + (1 - beta_2_t) * regulated_grad_square\n+            )\n \n         # `convert_to_tensor` unifies the handling of sparse and dense grads.\n-        u_t = gradient / ops.sqrt(v)\n-        u_t_hat = u_t / ops.maximum(one, (self._rms(u_t) / self.clip_threshold))\n-        variable.assign(variable - alpha_t * u_t_hat)\n+        u_t = ops.divide(gradient, ops.sqrt(v))\n+        u_t_hat = ops.divide(\n+            u_t,\n+            ops.maximum(one, ops.divide(self._rms(u_t), self.clip_threshold)),\n+        )\n+        self.assign_sub(variable, ops.multiply(alpha_t, u_t_hat))\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -85,9 +85,13 @@ class Adagrad(optimizer.Optimizer):\n \n         accumulator = self._accumulators[self._get_variable_index(variable)]\n \n-        accumulator.assign(accumulator + gradient * gradient)\n-        variable.assign(\n-            variable - (lr * gradient / ops.sqrt(accumulator + self.epsilon))\n+        self.assign_add(accumulator, ops.square(gradient))\n+        self.assign_sub(\n+            variable,\n+            ops.divide(\n+                ops.multiply(lr, gradient),\n+                ops.sqrt(ops.add(accumulator, self.epsilon)),\n+            ),\n         )\n \n     def get_config(self):\n\n@@ -125,13 +125,25 @@ class Adam(optimizer.Optimizer):\n \n         alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n \n-        m.assign(m + (gradient - m) * (1 - self.beta_1))\n-        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n+        self.assign_add(\n+            m, ops.multiply(ops.subtract(gradient, m), 1 - self.beta_1)\n+        )\n+        self.assign_add(\n+            v,\n+            ops.multiply(\n+                ops.subtract(ops.square(gradient), v), 1 - self.beta_2\n+            ),\n+        )\n         if self.amsgrad:\n             v_hat = self._velocity_hats[self._get_variable_index(variable)]\n-            v_hat.assign(ops.maximum(v_hat, v))\n+            self.assign(v_hat, ops.maximum(v_hat, v))\n             v = v_hat\n-        variable.assign(variable - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n+        self.assign_sub(\n+            variable,\n+            ops.divide(\n+                ops.multiply(m, alpha), ops.add(ops.sqrt(v), self.epsilon)\n+            ),\n+        )\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -120,10 +120,18 @@ class Adamax(optimizer.Optimizer):\n         m = self._m[self._get_variable_index(variable)]\n         u = self._u[self._get_variable_index(variable)]\n \n-        m.assign(m + (gradient - m) * (1 - self.beta_1))\n-        u.assign(ops.maximum(self.beta_2 * u, ops.abs(gradient)))\n-        variable.assign(\n-            variable - (lr * m) / ((1 - beta_1_power) * (u + self.epsilon))\n+        self.assign_add(\n+            m, ops.multiply(ops.subtract(gradient, m), (1 - self.beta_1))\n+        )\n+        self.assign(\n+            u, ops.maximum(ops.multiply(self.beta_2, u), ops.abs(gradient))\n+        )\n+        self.assign_sub(\n+            variable,\n+            ops.divide(\n+                ops.multiply(lr, m),\n+                ops.multiply((1 - beta_1_power), ops.add(u, self.epsilon)),\n+            ),\n         )\n \n     def get_config(self):\n\n@@ -154,7 +154,7 @@ class BaseOptimizer:\n     def add_variable(\n         self,\n         shape,\n-        initializer,\n+        initializer=\"zeros\",\n         dtype=None,\n         name=None,\n     ):\n@@ -198,6 +198,48 @@ class BaseOptimizer:\n                     \"recreate a new optimizer instance.\"\n                 )\n \n+    def assign(self, variable, value):\n+        \"\"\"Assign a value to a variable.\n+\n+        This should be used in optimizers instead of `variable.assign(value)` to\n+        support backend specific optimizations.\n+        Note that the variable can be a model variable or an optimizer variable;\n+        it can be a backend native variable or a Keras variable.\n+\n+        Args:\n+            variable: The variable to update.\n+            value: The value to add to the variable.\n+        \"\"\"\n+        variable.assign(value)\n+\n+    def assign_add(self, variable, value):\n+        \"\"\"Add a value to a variable.\n+\n+        This should be used in optimizers instead of\n+        `variable.assign_add(value)` to support backend specific optimizations.\n+        Note that the variable can be a model variable or an optimizer variable;\n+        it can be a backend native variable or a Keras variable.\n+\n+        Args:\n+            variable: The variable to update.\n+            value: The value to add to the variable.\n+        \"\"\"\n+        variable.assign_add(value)\n+\n+    def assign_sub(self, variable, value):\n+        \"\"\"Subtract a value from a variable.\n+\n+        This should be used in optimizers instead of\n+        `variable.assign_sub(value)` to support backend specific optimizations.\n+        Note that the variable can be a model variable or an optimizer variable;\n+        it can be a backend native variable or a Keras variable.\n+\n+        Args:\n+            variable: The variable to update.\n+            value: The value to add to the variable.\n+        \"\"\"\n+        variable.assign_sub(value)\n+\n     def update_step(self, gradient, variable, learning_rate):\n         raise NotImplementedError\n \n\n@@ -187,27 +187,42 @@ class Ftrl(optimizer.Optimizer):\n         l2_reg = self.l2_regularization_strength\n         l2_reg = l2_reg + self.beta / (2.0 * lr)\n \n-        # Ftrl optimizer has the same implementation for sparse and dense\n-        # gradients update.\n-        grad_to_use = (\n-            gradient + 2 * self.l2_shrinkage_regularization_strength * variable\n+        grad_to_use = ops.add(\n+            gradient,\n+            ops.multiply(\n+                2 * self.l2_shrinkage_regularization_strength, variable\n+            ),\n         )\n-        new_accum = accum + ops.power(gradient, 2)\n-        linear.assign(\n-            linear\n-            + grad_to_use\n-            - (ops.power(new_accum, -lr_power) - ops.power(accum, -lr_power))\n-            / lr\n-            * variable\n+        new_accum = ops.add(accum, ops.square(gradient))\n+        self.assign_add(\n+            linear,\n+            ops.subtract(\n+                grad_to_use,\n+                ops.multiply(\n+                    ops.divide(\n+                        ops.subtract(\n+                            ops.power(new_accum, -lr_power),\n+                            ops.power(accum, -lr_power),\n+                        ),\n+                        lr,\n+                    ),\n+                    variable,\n+                ),\n+            ),\n+        )\n+        quadratic = ops.add(\n+            ops.divide(ops.power(new_accum, (-lr_power)), lr), 2 * l2_reg\n         )\n-        quadratic = ops.power(new_accum, (-lr_power)) / lr + 2 * l2_reg\n         linear_clipped = ops.clip(\n             linear,\n             -self.l1_regularization_strength,\n             self.l1_regularization_strength,\n         )\n-        variable.assign((linear_clipped - linear) / quadratic)\n-        accum.assign(new_accum)\n+        self.assign(\n+            variable,\n+            ops.divide(ops.subtract(linear_clipped, linear), quadratic),\n+        )\n+        self.assign(accum, new_accum)\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -103,11 +103,24 @@ class Lion(optimizer.Optimizer):\n         beta_2 = ops.cast(self.beta_2, variable.dtype)\n         m = self._momentums[self._get_variable_index(variable)]\n \n-        # TODO: currently only support dense gradients\n-        variable.assign_sub(\n-            lr * ops.sign(m * beta_1 + gradient * (1.0 - beta_1))\n+        self.assign_sub(\n+            variable,\n+            ops.multiply(\n+                lr,\n+                ops.sign(\n+                    ops.add(\n+                        ops.multiply(m, beta_1),\n+                        ops.multiply(gradient, (1.0 - beta_1)),\n+                    )\n+                ),\n+            ),\n+        )\n+        self.assign(\n+            m,\n+            ops.add(\n+                ops.multiply(m, beta_2), ops.multiply(gradient, (1.0 - beta_2))\n+            ),\n         )\n-        m.assign(m * beta_2 + gradient * (1.0 - beta_2))\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -101,13 +101,14 @@ class Nadam(optimizer.Optimizer):\n \n     def _internal_apply_gradients(self, grads_and_vars):\n         dtype = self._u_product.dtype\n-        self._u_product.assign(\n+        self.assign(\n+            self._u_product,\n             self._u_product\n             * self.beta_1\n             * (\n                 1.0\n                 - 0.5 * ops.power(0.96, ops.cast(self.iterations + 1, dtype))\n-            )\n+            ),\n         )\n         super()._internal_apply_gradients(grads_and_vars)\n \n@@ -132,15 +133,23 @@ class Nadam(optimizer.Optimizer):\n         m = self._momentums[self._get_variable_index(variable)]\n         v = self._velocities[self._get_variable_index(variable)]\n \n-        m.assign(m + (gradient - m) * (1 - beta_1))\n-        v.assign(v + (ops.square(gradient) - v) * (1 - beta_2))\n-        m_hat = u_t_1 * m / (1 - u_product_t_1) + (1 - u_t) * gradient / (\n-            1 - u_product_t\n+        self.assign_add(\n+            m, ops.multiply(ops.subtract(gradient, m), (1 - beta_1))\n         )\n-        v_hat = v / (1 - beta_2_power)\n+        self.assign_add(\n+            v, ops.multiply(ops.subtract(ops.square(gradient), v), (1 - beta_2))\n+        )\n+        m_hat = ops.add(\n+            ops.divide(ops.multiply(u_t_1, m), 1 - u_product_t_1),\n+            ops.divide(ops.multiply(1 - u_t, gradient), 1 - u_product_t),\n+        )\n+        v_hat = ops.divide(v, (1 - beta_2_power))\n \n-        variable.assign(\n-            variable - (m_hat * lr) / (ops.sqrt(v_hat) + self.epsilon)\n+        self.assign_sub(\n+            variable,\n+            ops.divide(\n+                ops.multiply(m_hat, lr), ops.add(ops.sqrt(v_hat), self.epsilon)\n+            ),\n         )\n \n     def get_config(self):\n\n@@ -127,18 +127,35 @@ class RMSprop(optimizer.Optimizer):\n \n         rho = self.rho\n \n-        velocity.assign(rho * velocity + (1 - rho) * ops.square(gradient))\n+        self.assign(\n+            velocity,\n+            ops.add(\n+                ops.multiply(rho, velocity),\n+                ops.multiply(1 - rho, ops.square(gradient)),\n+            ),\n+        )\n         if self.centered:\n-            average_grad.assign(rho * average_grad + (1 - rho) * gradient)\n+            self.assign(\n+                average_grad,\n+                ops.add(\n+                    ops.multiply(rho, average_grad),\n+                    ops.multiply(1 - rho, gradient),\n+                ),\n+            )\n             denominator = velocity - ops.square(average_grad) + self.epsilon\n         else:\n-            denominator = velocity + self.epsilon\n-        increment = lr * gradient / ops.sqrt(denominator)\n+            denominator = ops.add(velocity, self.epsilon)\n+        increment = ops.divide(\n+            ops.multiply(lr, gradient), ops.sqrt(denominator)\n+        )\n         if self.momentum > 0:\n-            momentum.assign(self.momentum * momentum + increment)\n-            variable.assign(variable - momentum)\n+            self.assign(\n+                momentum,\n+                ops.add(ops.multiply(self.momentum, momentum), increment),\n+            )\n+            self.assign_sub(variable, momentum)\n         else:\n-            variable.assign(variable - increment)\n+            self.assign_sub(variable, increment)\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -103,15 +103,25 @@ class SGD(optimizer.Optimizer):\n \n         if m is not None:\n             momentum = ops.cast(self.momentum, variable.dtype)\n-            m.assign(-gradient * learning_rate + m * momentum)\n+            self.assign(\n+                m,\n+                ops.subtract(\n+                    ops.multiply(m, momentum),\n+                    ops.multiply(gradient, learning_rate),\n+                ),\n+            )\n             if self.nesterov:\n-                variable.assign(\n-                    variable - gradient * learning_rate + m * momentum\n+                self.assign_add(\n+                    variable,\n+                    ops.subtract(\n+                        ops.multiply(m, momentum),\n+                        ops.multiply(gradient, learning_rate),\n+                    ),\n                 )\n             else:\n-                variable.assign(variable + m)\n+                self.assign_add(variable, m)\n         else:\n-            variable.assign(variable - gradient * learning_rate)\n+            self.assign_sub(variable, ops.multiply(gradient, learning_rate))\n \n     def get_config(self):\n         config = super().get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2b7917829aa09c1dca9a860c4f2d2e1394b19e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 567 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 567 | Churn Cumulative: 567 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.5314285714285715\n\nDIFF:\n@@ -0,0 +1,567 @@\n+\"\"\"\n+Title: A Transformer-based recommendation system\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2020/12/30\n+Last modified: 2023/10/25\n+Description: Rating rate prediction using the Behavior Sequence Transformer (BST) model on the Movielens.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates the [Behavior Sequence Transformer (BST)](https://arxiv.org/abs/1905.06874)\n+model, by Qiwei Chen et al., using the [Movielens dataset](https://grouplens.org/datasets/movielens/).\n+The BST model leverages the sequential behaviour of the users in watching and rating movies,\n+as well as user profile and movie features, to predict the rating of the user to a target movie.\n+\n+More precisely, the BST model aims to predict the rating of a target movie by accepting\n+the following inputs:\n+\n+1. A fixed-length *sequence* of `movie_ids` watched by a user.\n+2. A fixed-length *sequence* of the `ratings` for the movies watched by a user.\n+3. A *set* of user features, including `user_id`, `sex`, `occupation`, and `age_group`.\n+4. A *set* of `genres` for each movie in the input sequence and the target movie.\n+5. A `target_movie_id` for which to predict the rating.\n+\n+This example modifies the original BST model in the following ways:\n+\n+1. We incorporate the movie features (genres) into the processing of the embedding of each\n+movie of the input sequence and the target movie, rather than treating them as \"other features\"\n+outside the transformer layer.\n+2. We utilize the ratings of movies in the input sequence, along with the their positions\n+in the sequence, to update them before feeding them into the self-attention layer.\n+\n+\n+Note that this example should be run with TensorFlow 2.4 or higher.\n+\"\"\"\n+\n+\"\"\"\n+## The dataset\n+\n+We use the [1M version of the Movielens dataset](https://grouplens.org/datasets/movielens/1m/).\n+The dataset includes around 1 million ratings from 6000 users on 4000 movies,\n+along with some user features, movie genres. In addition, the timestamp of each user-movie\n+rating is provided, which allows creating sequences of movie ratings for each user,\n+as expected by the BST model.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+import math\n+from zipfile import ZipFile\n+from urllib.request import urlretrieve\n+import numpy as np\n+import pandas as pd\n+import tensorflow as tf\n+import keras\n+from keras import layers\n+from keras.layers import StringLookup\n+\n+\n+\"\"\"\n+## Prepare the data\n+\n+### Download and prepare the DataFrames\n+\n+First, let's download the movielens data.\n+\n+The downloaded folder will contain three data files: `users.dat`, `movies.dat`,\n+and `ratings.dat`.\n+\"\"\"\n+\n+urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n+ZipFile(\"movielens.zip\", \"r\").extractall()\n+\n+\"\"\"\n+Then, we load the data into pandas DataFrames with their proper column names.\n+\"\"\"\n+\n+users = pd.read_csv(\n+    \"ml-1m/users.dat\",\n+    sep=\"::\",\n+    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n+)\n+\n+ratings = pd.read_csv(\n+    \"ml-1m/ratings.dat\",\n+    sep=\"::\",\n+    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n+)\n+\n+movies = pd.read_csv(\n+    \"ml-1m/movies.dat\", sep=\"::\", names=[\"movie_id\", \"title\", \"genres\"], encoding='latin-1'\n+)\n+\n+\"\"\"\n+Here, we do some simple data processing to fix the data types of the columns.\n+\"\"\"\n+\n+users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n+users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n+users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n+\n+movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n+\n+ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n+ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n+ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n+\n+\"\"\"\n+Each movie has multiple genres. We split them into separate columns in the `movies`\n+DataFrame.\n+\"\"\"\n+\n+genres = [\n+    \"Action\",\n+    \"Adventure\",\n+    \"Animation\",\n+    \"Children's\",\n+    \"Comedy\",\n+    \"Crime\",\n+    \"Documentary\",\n+    \"Drama\",\n+    \"Fantasy\",\n+    \"Film-Noir\",\n+    \"Horror\",\n+    \"Musical\",\n+    \"Mystery\",\n+    \"Romance\",\n+    \"Sci-Fi\",\n+    \"Thriller\",\n+    \"War\",\n+    \"Western\",\n+]\n+\n+for genre in genres:\n+    movies[genre] = movies[\"genres\"].apply(\n+        lambda values: int(genre in values.split(\"|\"))\n+    )\n+\n+\n+\"\"\"\n+### Transform the movie ratings data into sequences\n+\n+First, let's sort the the ratings data using the `unix_timestamp`, and then group the\n+`movie_id` values and the `rating` values by `user_id`.\n+\n+The output DataFrame will have a record for each `user_id`, with two ordered lists\n+(sorted by rating datetime): the movies they have rated, and their ratings of these movies.\n+\"\"\"\n+\n+ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n+\n+ratings_data = pd.DataFrame(\n+    data={\n+        \"user_id\": list(ratings_group.groups.keys()),\n+        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n+        \"ratings\": list(ratings_group.rating.apply(list)),\n+        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n+    }\n+)\n+\n+\n+\"\"\"\n+Now, let's split the `movie_ids` list into a set of sequences of a fixed length.\n+We do the same for the `ratings`. Set the `sequence_length` variable to change the length\n+of the input sequence to the model. You can also change the `step_size` to control the\n+number of sequences to generate for each user.\n+\"\"\"\n+\n+sequence_length = 4\n+step_size = 2\n+\n+\n+def create_sequences(values, window_size, step_size):\n+    sequences = []\n+    start_index = 0\n+    while True:\n+        end_index = start_index + window_size\n+        seq = values[start_index:end_index]\n+        if len(seq) < window_size:\n+            seq = values[-window_size:]\n+            if len(seq) == window_size:\n+                sequences.append(seq)\n+            break\n+        sequences.append(seq)\n+        start_index += step_size\n+    return sequences\n+\n+\n+ratings_data.movie_ids = ratings_data.movie_ids.apply(\n+    lambda ids: create_sequences(ids, sequence_length, step_size)\n+)\n+\n+ratings_data.ratings = ratings_data.ratings.apply(\n+    lambda ids: create_sequences(ids, sequence_length, step_size)\n+)\n+\n+del ratings_data[\"timestamps\"]\n+\n+\"\"\"\n+After that, we process the output to have each sequence in a separate records in\n+the DataFrame. In addition, we join the user features with the ratings data.\n+\"\"\"\n+\n+ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n+    \"movie_ids\", ignore_index=True\n+)\n+ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n+ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n+ratings_data_transformed = ratings_data_transformed.join(\n+    users.set_index(\"user_id\"), on=\"user_id\"\n+)\n+ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n+    lambda x: \",\".join(x)\n+)\n+ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n+    lambda x: \",\".join([str(v) for v in x])\n+)\n+\n+del ratings_data_transformed[\"zip_code\"]\n+\n+ratings_data_transformed.rename(\n+    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n+    inplace=True,\n+)\n+\n+\"\"\"\n+With `sequence_length` of 4 and `step_size` of 2, we end up with 498,623 sequences.\n+\n+Finally, we split the data into training and testing splits, with 85% and 15% of\n+the instances, respectively, and store them to CSV files.\n+\"\"\"\n+\n+random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n+train_data = ratings_data_transformed[random_selection]\n+test_data = ratings_data_transformed[~random_selection]\n+\n+train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n+test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)\n+\n+\"\"\"\n+## Define metadata\n+\"\"\"\n+\n+CSV_HEADER = list(ratings_data_transformed.columns)\n+\n+CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n+    \"user_id\": list(users.user_id.unique()),\n+    \"movie_id\": list(movies.movie_id.unique()),\n+    \"sex\": list(users.sex.unique()),\n+    \"age_group\": list(users.age_group.unique()),\n+    \"occupation\": list(users.occupation.unique()),\n+}\n+\n+USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n+\n+MOVIE_FEATURES = [\"genres\"]\n+\n+\"\"\"\n+## Create `tf.data.Dataset` for training and evaluation\n+\"\"\"\n+\n+\n+def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n+    def process(features):\n+        movie_ids_string = features[\"sequence_movie_ids\"]\n+        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n+\n+        # The last movie id in the sequence is the target movie.\n+        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n+        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n+\n+        ratings_string = features[\"sequence_ratings\"]\n+        sequence_ratings = tf.strings.to_number(\n+            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n+        ).to_tensor()\n+\n+        # The last rating in the sequence is the target for the model to predict.\n+        target = sequence_ratings[:, -1]\n+        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n+\n+        return features, target\n+\n+    dataset = tf.data.experimental.make_csv_dataset(\n+        csv_file_path,\n+        batch_size=batch_size,\n+        column_names=CSV_HEADER,\n+        num_epochs=1,\n+        header=False,\n+        field_delim=\"|\",\n+        shuffle=shuffle,\n+    ).map(process)\n+\n+    return dataset\n+\n+\n+\"\"\"\n+## Create model inputs\n+\"\"\"\n+\n+\n+def create_model_inputs():\n+    return {\n+        \"user_id\": layers.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n+        \"sequence_movie_ids\": layers.Input(\n+            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n+        ),\n+        \"target_movie_id\": layers.Input(\n+            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n+        ),\n+        \"sequence_ratings\": layers.Input(\n+            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=\"float32\"\n+        ),\n+        \"sex\": layers.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n+        \"age_group\": layers.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n+        \"occupation\": layers.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n+    }\n+\n+\n+\"\"\"\n+## Encode input features\n+\n+The `encode_input_features` method works as follows:\n+\n+1. Each categorical user feature is encoded using `layers.Embedding`, with embedding\n+dimension equals to the square root of the vocabulary size of the feature.\n+The embeddings of these features are concatenated to form a single input tensor.\n+\n+2. Each movie in the movie sequence and the target movie is encoded `layers.Embedding`,\n+where the dimension size is the square root of the number of movies.\n+\n+3. A multi-hot genres vector for each movie is concatenated with its embedding vector,\n+and processed using a non-linear `layers.Dense` to output a vector of the same movie\n+embedding dimensions.\n+\n+4. A positional embedding is added to each movie embedding in the sequence, and then\n+multiplied by its rating from the ratings sequence.\n+\n+5. The target movie embedding is concatenated to the sequence movie embeddings, producing\n+a tensor with the shape of `[batch size, sequence length, embedding size]`, as expected\n+by the attention layer for the transformer architecture.\n+\n+6. The method returns a tuple of two elements:  `encoded_transformer_features` and\n+`encoded_other_features`.\n+\"\"\"\n+\n+\n+def encode_input_features(\n+    inputs,\n+    include_user_id=True,\n+    include_user_features=True,\n+    include_movie_features=True,\n+):\n+    encoded_transformer_features = []\n+    encoded_other_features = []\n+\n+    other_feature_names = []\n+    if include_user_id:\n+        other_feature_names.append(\"user_id\")\n+    if include_user_features:\n+        other_feature_names.extend(USER_FEATURES)\n+\n+    ## Encode user features\n+    for feature_name in other_feature_names:\n+        # Convert the string input values into integer indices.\n+        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n+        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n+            inputs[feature_name]\n+        )\n+        # Compute embedding dimensions\n+        embedding_dims = int(math.sqrt(len(vocabulary)))\n+        # Create an embedding layer with the specified dimensions.\n+        embedding_encoder = layers.Embedding(\n+            input_dim=len(vocabulary),\n+            output_dim=embedding_dims,\n+            name=f\"{feature_name}_embedding\",\n+        )\n+        # Convert the index values to embedding representations.\n+        encoded_other_features.append(embedding_encoder(idx))\n+\n+    ## Create a single embedding vector for the user features\n+    if len(encoded_other_features) > 1:\n+        encoded_other_features = layers.concatenate(encoded_other_features)\n+    elif len(encoded_other_features) == 1:\n+        encoded_other_features = encoded_other_features[0]\n+    else:\n+        encoded_other_features = None\n+\n+    ## Create a movie embedding encoder\n+    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n+    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n+    # Create a lookup to convert string values to integer indices.\n+    movie_index_lookup = StringLookup(\n+        vocabulary=movie_vocabulary,\n+        mask_token=None,\n+        num_oov_indices=0,\n+        name=\"movie_index_lookup\",\n+    )\n+    # Create an embedding layer with the specified dimensions.\n+    movie_embedding_encoder = layers.Embedding(\n+        input_dim=len(movie_vocabulary),\n+        output_dim=movie_embedding_dims,\n+        name=f\"movie_embedding\",\n+    )\n+    # Create a vector lookup for movie genres.\n+    genre_vectors = movies[genres].to_numpy()\n+    movie_genres_lookup = layers.Embedding(\n+        input_dim=genre_vectors.shape[0],\n+        output_dim=genre_vectors.shape[1],\n+        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n+        trainable=False,\n+        name=\"genres_vector\",\n+    )\n+    # Create a processing layer for genres.\n+    movie_embedding_processor = layers.Dense(\n+        units=movie_embedding_dims,\n+        activation=\"relu\",\n+        name=\"process_movie_embedding_with_genres\",\n+    )\n+\n+    ## Define a function to encode a given movie id.\n+    def encode_movie(movie_id):\n+        # Convert the string input values into integer indices.\n+        movie_idx = movie_index_lookup(movie_id)\n+        movie_embedding = movie_embedding_encoder(movie_idx)\n+        encoded_movie = movie_embedding\n+        if include_movie_features:\n+            movie_genres_vector = movie_genres_lookup(movie_idx)\n+            encoded_movie = movie_embedding_processor(\n+                layers.concatenate([movie_embedding, movie_genres_vector])\n+            )\n+        return encoded_movie\n+\n+    ## Encoding target_movie_id\n+    target_movie_id = inputs[\"target_movie_id\"]\n+    encoded_target_movie = encode_movie(target_movie_id)\n+\n+    ## Encoding sequence movie_ids.\n+    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n+    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n+    # Create positional embedding.\n+    position_embedding_encoder = layers.Embedding(\n+        input_dim=sequence_length,\n+        output_dim=movie_embedding_dims,\n+        name=\"position_embedding\",\n+    )\n+    positions = keras.ops.arange(start=0, stop=sequence_length - 1, step=1)\n+    encodded_positions = position_embedding_encoder(positions)\n+    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n+    sequence_ratings = keras.ops.expand_dims(inputs[\"sequence_ratings\"], -1)\n+    # Add the positional encoding to the movie encodings and multiply them by rating.\n+    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n+        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n+    )\n+\n+    # Construct the transformer inputs.\n+    for encoded_movie in keras.ops.unstack(\n+        encoded_sequence_movies_with_poistion_and_rating, axis=1\n+    ):\n+        encoded_transformer_features.append(keras.ops.expand_dims(encoded_movie, 1))\n+    encoded_transformer_features.append(encoded_target_movie)\n+\n+    encoded_transformer_features = layers.concatenate(\n+        encoded_transformer_features, axis=1\n+    )\n+\n+    return encoded_transformer_features, encoded_other_features\n+\n+\n+\"\"\"\n+## Create a BST model\n+\"\"\"\n+\n+include_user_id = False\n+include_user_features = False\n+include_movie_features = False\n+\n+hidden_units = [256, 128]\n+dropout_rate = 0.1\n+num_heads = 3\n+\n+\n+def create_model():\n+    inputs = create_model_inputs()\n+    transformer_features, other_features = encode_input_features(\n+        inputs, include_user_id, include_user_features, include_movie_features\n+    )\n+\n+    # Create a multi-headed attention layer.\n+    attention_output = layers.MultiHeadAttention(\n+        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n+    )(transformer_features, transformer_features)\n+\n+    # Transformer block.\n+    attention_output = layers.Dropout(dropout_rate)(attention_output)\n+    x1 = layers.Add()([transformer_features, attention_output])\n+    x1 = layers.LayerNormalization()(x1)\n+    x2 = layers.LeakyReLU()(x1)\n+    x2 = layers.Dense(units=x2.shape[-1])(x2)\n+    x2 = layers.Dropout(dropout_rate)(x2)\n+    transformer_features = layers.Add()([x1, x2])\n+    transformer_features = layers.LayerNormalization()(transformer_features)\n+    features = layers.Flatten()(transformer_features)\n+\n+    # Included the other features.\n+    if other_features is not None:\n+        features = layers.concatenate(\n+            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n+        )\n+\n+    # Fully-connected layers.\n+    for num_units in hidden_units:\n+        features = layers.Dense(num_units)(features)\n+        features = layers.BatchNormalization()(features)\n+        features = layers.LeakyReLU()(features)\n+        features = layers.Dropout(dropout_rate)(features)\n+\n+    outputs = layers.Dense(units=1)(features)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+model = create_model()\n+\n+\"\"\"\n+## Run training and evaluation experiment\n+\"\"\"\n+\n+# Compile the model.\n+model.compile(\n+    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n+    loss=keras.losses.MeanSquaredError(),\n+    metrics=[keras.metrics.MeanAbsoluteError()],\n+)\n+\n+# Read the training data.\n+train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n+\n+# Fit the model with the training data.\n+model.fit(train_dataset, epochs=5)\n+\n+# Read the test data.\n+test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n+\n+# Evaluate the model on the test data.\n+_, rmse = model.evaluate(test_dataset, verbose=0)\n+print(f\"Test MAE: {round(rmse, 3)}\")\n+\n+\"\"\"\n+You should achieve a Mean Absolute Error (MAE) at or around 0.7 on the test data.\n+\"\"\"\n+\n+\"\"\"\n+## Conclusion\n+\n+The BST model uses the Transformer layer in its architecture to capture the sequential signals underlying\n+users’ behavior sequences for recommendation.\n+\n+You can try training this model with different configurations, for example, by increasing\n+the input sequence length and training the model for a larger number of epochs. In addition,\n+you can try including other features like movie release year and customer\n+zipcode, and including cross features like sex X genre.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
