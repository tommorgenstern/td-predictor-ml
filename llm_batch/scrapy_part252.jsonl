{"custom_id": "scrapy#83ec947fe732035e147c21df352e199ce2cce5c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 40 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -19,12 +19,12 @@ def identity(x):\n \n class Rule(object):\n \n-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity):\n+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n         self.link_extractor = link_extractor\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n-        self.process_request_function = process_request\n+        self.process_request_function = process_request or identity\n         if follow is None:\n             self.follow = False if callback else True\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25e616fa04bf4199b9017553b3e57699c40b0683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 224 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -127,7 +127,7 @@ class ImagesPipeline(FilesPipeline):\n             raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n                                  (width, height, self.min_width, self.min_height))\n \n-        image, buf = self.convert_image(orig_image)\n+        image, buf = self.convert_image(orig_image, BytesIO(response.body))\n         yield path, image, buf\n \n         for thumb_id, size in six.iteritems(self.thumbs):\n@@ -135,7 +135,7 @@ class ImagesPipeline(FilesPipeline):\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n \n-    def convert_image(self, image, size=None):\n+    def convert_image(self, image, response_body, size=None):\n         if image.format == 'PNG' and image.mode == 'RGBA':\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n@@ -152,6 +152,9 @@ class ImagesPipeline(FilesPipeline):\n             image = image.copy()\n             image.thumbnail(size, Image.ANTIALIAS)\n \n+        if not size and image.format == 'JPEG':\n+            return image, response_body\n+\n         buf = BytesIO()\n         image.save(buf, 'JPEG')\n         return image, buf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01ed605d02013b1d7955369562b2443d2a561599", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 485 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -105,11 +105,11 @@ class SpiderTest(unittest.TestCase):\n \n     def test_logger(self):\n         spider = self.spider_class('example.com')\n-        with LogCapture() as l:\n+        with LogCapture() as lc:\n             spider.logger.info('test log msg')\n-        l.check(('example.com', 'INFO', 'test log msg'))\n+        lc.check(('example.com', 'INFO', 'test log msg'))\n \n-        record = l.records[0]\n+        record = lc.records[0]\n         self.assertIn('spider', record.__dict__)\n         self.assertIs(record.spider, spider)\n \n@@ -190,8 +190,7 @@ class CrawlSpiderTest(SpiderTest):\n \n     def test_process_links(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n-            body=self.test_body)\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n@@ -214,8 +213,7 @@ class CrawlSpiderTest(SpiderTest):\n \n     def test_process_links_filter(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n-            body=self.test_body)\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n \n         class _CrawlSpider(self.spider_class):\n             import re\n@@ -226,6 +224,7 @@ class CrawlSpiderTest(SpiderTest):\n                 Rule(LinkExtractor(), process_links=\"filter_process_links\"),\n             )\n             _test_regex = re.compile('nofollow')\n+\n             def filter_process_links(self, links):\n                 return [link for link in links\n                         if not self._test_regex.search(link.url)]\n@@ -240,8 +239,7 @@ class CrawlSpiderTest(SpiderTest):\n \n     def test_process_links_generator(self):\n \n-        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n-            body=self.test_body)\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n \n         class _CrawlSpider(self.spider_class):\n             name = \"test\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07487dd487f7a10fbf0693e378b7b66909a67d6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 495 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -79,28 +79,28 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\n         COLOUR = (0, 127, 255)\n-        im = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n-        converted, _ = self.pipeline.convert_image(im)\n+        im, buf = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n+        converted, buf = self.pipeline.convert_image(im, buf)\n         self.assertEqual(converted.mode, 'RGB')\n         self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n \n         # check that thumbnail keep image ratio\n-        thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n+        thumbnail, buf = self.pipeline.convert_image(converted, buf, size=(10, 25))\n         self.assertEqual(thumbnail.mode, 'RGB')\n         self.assertEqual(thumbnail.size, (10, 10))\n \n         # transparency case: RGBA and PNG\n         COLOUR = (0, 127, 255, 50)\n-        im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n-        converted, _ = self.pipeline.convert_image(im)\n+        im, buf = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+        converted, buf = self.pipeline.convert_image(im, buf)\n         self.assertEqual(converted.mode, 'RGB')\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n         # transparency case with palette: P and PNG\n         COLOUR = (0, 127, 255, 50)\n-        im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+        im, buf = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n         im = im.convert('P')\n-        converted, _ = self.pipeline.convert_image(im)\n+        converted, buf = self.pipeline.convert_image(im, buf)\n         self.assertEqual(converted.mode, 'RGB')\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n@@ -406,7 +406,7 @@ def _create_image(format, *a, **kw):\n     buf = io.BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n-    return Image.open(buf)\n+    return Image.open(buf), buf\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#645e8d16a4c966b50bd39667aaef28dc1eeb43b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 515 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,7 @@ class MetaSpider(MockServerSpider):\n class KeywordArgumentsSpider(MockServerSpider):\n \n     name = 'kwargs'\n-    checks = set()\n+    checks = list()\n \n     def start_requests(self):\n         data = {'key': 'value', 'number': 123}\n@@ -41,25 +41,30 @@ class KeywordArgumentsSpider(MockServerSpider):\n         yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)\n \n     def parse_first(self, response, key, number):\n-        self.checks.add(key == 'value')\n-        self.checks.add(number == 123)\n+        self.checks.append(key == 'value')\n+        self.checks.append(number == 123)\n+        self.crawler.stats.inc_value('boolean_checks', 2)\n         yield response.follow(\n             self.mockserver.url('/two'),\n             self.parse_second,\n             kwargs={'new_key': 'new_value'})\n \n     def parse_second(self, response, new_key):\n-        self.checks.add(new_key == 'new_value')\n+        self.checks.append(new_key == 'new_value')\n+        self.crawler.stats.inc_value('boolean_checks')\n \n     def parse_general(self, response, **kwargs):\n         if response.url.endswith('/general_with'):\n-            self.checks.add(kwargs['key'] == 'value')\n-            self.checks.add(kwargs['number'] == 123)\n+            self.checks.append(kwargs['key'] == 'value')\n+            self.checks.append(kwargs['number'] == 123)\n+            self.crawler.stats.inc_value('boolean_checks', 2)\n         elif response.url.endswith('/general_without'):\n-            self.checks.add(kwargs == {})\n+            self.checks.append(kwargs == {})\n+            self.crawler.stats.inc_value('boolean_checks')\n \n     def parse_no_kwargs(self, response):\n-        pass\n+        self.checks.append(response.url.endswith('/no_kwargs'))\n+        self.crawler.stats.inc_value('boolean_checks')\n \n \n class FollowAllSpider(MetaSpider):\n\n@@ -27,7 +27,8 @@ class CrawlTestCase(TestCase):\n     def test_callback_kwargs(self):\n         crawler = self.runner.create_crawler(KeywordArgumentsSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(crawler.spider.checks, set([True]))\n+        self.assertTrue(all(crawler.spider.checks))\n+        self.assertEqual(len(crawler.spider.checks), crawler.stats.get_value('boolean_checks'))\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6760bca74b1f51ce83ed73318d5ddcef03c9d129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 23 | Files Changed: 8 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 48 | Churn Cumulative: 3393 | Contributors (this commit): 34 | Commits (past 90d): 31 | Contributors (cumulative): 72 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,7 +143,9 @@ class Scraper(object):\n     def call_spider(self, result, request, spider):\n         result.request = request\n         dfd = defer_result(result)\n-        dfd.addCallbacks(request.callback or spider.parse, request.errback, callbackKeywords=request.kwargs)\n+        dfd.addCallbacks(callback=request.callback or spider.parse,\n+                         errback=request.errback,\n+                         callbackKeywords=request.cb_kwargs)\n         return dfd.addCallback(iterate_spider_output)\n \n     def handle_spider_error(self, _failure, request, response, spider):\n\n@@ -18,7 +18,7 @@ class Request(object_ref):\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                  cookies=None, meta=None, encoding='utf-8', priority=0,\n-                 dont_filter=False, errback=None, flags=None, kwargs=None):\n+                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n \n         self._encoding = encoding  # this one has to be set first\n         self.method = str(method).upper()\n@@ -40,14 +40,14 @@ class Request(object_ref):\n         self.dont_filter = dont_filter\n \n         self._meta = dict(meta) if meta else None\n-        self._kwargs = dict(kwargs) if kwargs else None\n+        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n         self.flags = [] if flags is None else list(flags)\n \n     @property\n-    def kwargs(self):\n-        if self._kwargs is None:\n-            self._kwargs = {}\n-        return self._kwargs\n+    def cb_kwargs(self):\n+        if self._cb_kwargs is None:\n+            self._cb_kwargs = {}\n+        return self._cb_kwargs\n \n     @property\n     def meta(self):\n@@ -99,7 +99,7 @@ class Request(object_ref):\n         given new values.\n         \"\"\"\n         for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n-                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'kwargs']:\n+                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -106,7 +106,7 @@ class Response(object_ref):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding='utf-8', priority=0,\n-               dont_filter=False, errback=None, kwargs=None):\n+               dont_filter=False, errback=None, cb_kwargs=None):\n         # type: (...) -> Request\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -133,4 +133,4 @@ class Response(object_ref):\n                        priority=priority,\n                        dont_filter=dont_filter,\n                        errback=errback,\n-                       kwargs=kwargs)\n+                       cb_kwargs=cb_kwargs)\n\n@@ -123,7 +123,7 @@ class TextResponse(Response):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding=None, priority=0,\n-               dont_filter=False, errback=None, kwargs=None):\n+               dont_filter=False, errback=None, cb_kwargs=None):\n         # type: (...) -> Request\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -155,7 +155,7 @@ class TextResponse(Response):\n             priority=priority,\n             dont_filter=dont_filter,\n             errback=errback,\n-            kwargs=kwargs,\n+            cb_kwargs=cb_kwargs,\n         )\n \n \n\n@@ -33,7 +33,7 @@ def request_to_dict(request, spider=None):\n         'priority': request.priority,\n         'dont_filter': request.dont_filter,\n         'flags': request.flags,\n-        'kwargs': request.kwargs,\n+        'cb_kwargs': request.cb_kwargs,\n     }\n     if type(request) is not Request:\n         d['_class'] = request.__module__ + '.' + request.__class__.__name__\n@@ -66,7 +66,7 @@ def request_from_dict(d, spider=None):\n         priority=d['priority'],\n         dont_filter=d['dont_filter'],\n         flags=d.get('flags'),\n-        kwargs=d.get('kwargs'),\n+        cb_kwargs=d.get('cb_kwargs'),\n     )\n \n \n\n@@ -35,8 +35,8 @@ class KeywordArgumentsSpider(MockServerSpider):\n \n     def start_requests(self):\n         data = {'key': 'value', 'number': 123}\n-        yield Request(self.mockserver.url('/first'), self.parse_first, kwargs=data)\n-        yield Request(self.mockserver.url('/general_with'), self.parse_general, kwargs=data)\n+        yield Request(self.mockserver.url('/first'), self.parse_first, cb_kwargs=data)\n+        yield Request(self.mockserver.url('/general_with'), self.parse_general, cb_kwargs=data)\n         yield Request(self.mockserver.url('/general_without'), self.parse_general)\n         yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)\n \n@@ -47,7 +47,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         yield response.follow(\n             self.mockserver.url('/two'),\n             self.parse_second,\n-            kwargs={'new_key': 'new_value'})\n+            cb_kwargs={'new_key': 'new_value'})\n \n     def parse_second(self, response, new_key):\n         self.checks.append(new_key == 'new_value')\n\n@@ -177,7 +177,7 @@ class RequestTest(unittest.TestCase):\n         r1 = self.request_class(\"http://www.example.com\", flags=['f1', 'f2'],\n                                 callback=somecallback, errback=somecallback)\n         r1.meta['foo'] = 'bar'\n-        r1.kwargs['key'] = 'value'\n+        r1.cb_kwargs['key'] = 'value'\n         r2 = r1.copy()\n \n         # make sure copy does not propagate callbacks\n@@ -190,9 +190,9 @@ class RequestTest(unittest.TestCase):\n         assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n         self.assertEqual(r1.flags, r2.flags)\n \n-        # make sure kwargs dict is shallow copied\n-        assert r1.kwargs is not r2.kwargs, \"kwargs must be a shallow copy, not identical\"\n-        self.assertEqual(r1.kwargs, r2.kwargs)\n+        # make sure cb_kwargs dict is shallow copied\n+        assert r1.cb_kwargs is not r2.cb_kwargs, \"cb_kwargs must be a shallow copy, not identical\"\n+        self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n \n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n\n@@ -26,7 +26,7 @@ class RequestSerializationTest(unittest.TestCase):\n             encoding='latin-1',\n             priority=20,\n             meta={'a': 'b'},\n-            kwargs={'k': 'v'},\n+            cb_kwargs={'k': 'v'},\n             flags=['testFlag'])\n         self._assert_serializes_ok(r, spider=self.spider)\n \n@@ -53,7 +53,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1.headers, r2.headers)\n         self.assertEqual(r1.cookies, r2.cookies)\n         self.assertEqual(r1.meta, r2.meta)\n-        self.assertEqual(r1.kwargs, r2.kwargs)\n+        self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n         self.assertEqual(r1._encoding, r2._encoding)\n         self.assertEqual(r1.priority, r2.priority)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92bbc5290d2b381ea60d68442a887d1ba020874e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 56 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -24,22 +24,22 @@ class Rule(object):\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n-        self.process_request_function = process_request or identity\n+        self.process_request = process_request or identity\n         if follow is None:\n             self.follow = False if callback else True\n         else:\n             self.follow = follow\n \n-    def process_request(self, request, response):\n+    def _process_request(self, request, response):\n         \"\"\"\n         Wrapper around the request processing function to maintain backward compatibility\n         with functions that do not take a Response object as parameter.\n         \"\"\"\n-        argcount = self.process_request_function.__code__.co_argcount\n-        if getattr(self.process_request_function, '__self__', None):\n+        argcount = self.process_request.__code__.co_argcount\n+        if hasattr(self.process_request, '__self__'):\n             argcount = argcount - 1\n         args = [request] if argcount == 1 else [request, response]\n-        return self.process_request_function(*args)\n+        return self.process_request(*args)\n \n \n class CrawlSpider(Spider):\n@@ -75,8 +75,8 @@ class CrawlSpider(Spider):\n                 links = rule.process_links(links)\n             for link in links:\n                 seen.add(link)\n-                r = self._build_request(n, link)\n-                yield rule.process_request(r, response)\n+                request = self._build_request(n, link)\n+                yield rule._process_request(request, response)\n \n     def _response_downloaded(self, response):\n         rule = self._rules[response.meta['rule']]\n@@ -104,7 +104,7 @@ class CrawlSpider(Spider):\n         for rule in self._rules:\n             rule.callback = get_method(rule.callback)\n             rule.process_links = get_method(rule.process_links)\n-            rule.process_request_function = get_method(rule.process_request_function)\n+            rule.process_request = get_method(rule.process_request)\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#282f24c510bdc6fff0e86ca16b7d681617af25ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 969 | Contributors (this commit): 24 | Commits (past 90d): 11 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import six\n-from six.moves.urllib.parse import urljoin, urlencode, urlsplit, parse_qsl, urlunsplit\n+from six.moves.urllib.parse import urljoin, urlencode, urlsplit, urlunsplit\n \n import lxml.html\n from parsel.selector import create_root_node\n@@ -33,12 +33,7 @@ class FormRequest(Request):\n                 self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                 self._set_body(form_query_str)\n             else:\n-                url_split = urlsplit(self.url)\n-                formdata_keys = set(dict(parse_qsl(form_query_str)).keys())\n-                items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_keys]\n-                url_query_str = _urlencode(items, self.encoding)\n-                query = (url_query_str + '&' + form_query_str) if url_query_str else form_query_str\n-                self._set_url(urlunsplit(url_split._replace(query = query)))\n+                self._set_url(urlunsplit(urlsplit(self.url)._replace(query = form_query_str)))\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c89e53e684987c61857ef810374101440d5616c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1106 | Contributors (this commit): 17 | Commits (past 90d): 14 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -275,26 +275,17 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(r1.body, b'')\n         \n     def test_formdata_overrides_querystring_duplicates(self):\n-        #Both fragment and query in url\n         data = (('a', 'one'), ('a', 'two'), ('b', '2'))\n         url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n-        self.assertEqual(fs[b'c'], [b'3'])\n \n-        #None of both in url\n         data = {'a' : '1', 'b' : '2'}\n         fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n \n-        #Duplicate GET arguments are preserved\n-        data={'foo' : 'bar'}\n-        fs = _qs(self.request_class('http://example.com/?foo=1&foo=2&a=1&a=2', method='GET', formdata=data))\n-        self.assertEqual(fs[b'foo'], [b'bar'])\n-        self.assertEqual(set(fs[b'a']), {b'1', b'2'})\n-\n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n         data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#821f5bb26077d7f9a6b2b1a72f210f81779f5393", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 578 | Lines Deleted: 23 | Files Changed: 5 | Hunks: 12 | Methods Changed: 49 | Complexity Δ (Sum/Max): 117/66 | Churn Δ: 601 | Churn Cumulative: 1790 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: 0.8106508875739645\n\nDIFF:\n@@ -75,6 +75,8 @@ def _get_concurrency_delay(concurrency, spider, settings):\n \n class Downloader(object):\n \n+    DOWNLOAD_SLOT = 'download_slot'\n+\n     def __init__(self, crawler):\n         self.settings = crawler.settings\n         self.signals = crawler.signals\n@@ -111,8 +113,8 @@ class Downloader(object):\n         return key, self.slots[key]\n \n     def _get_slot_key(self, request, spider):\n-        if 'download_slot' in request.meta:\n-            return request.meta['download_slot']\n+        if self.DOWNLOAD_SLOT in request.meta:\n+            return request.meta[self.DOWNLOAD_SLOT]\n \n         key = urlparse_cached(request).hostname or ''\n         if self.ip_concurrency:\n@@ -122,7 +124,7 @@ class Downloader(object):\n \n     def _enqueue_request(self, request, spider):\n         key, slot = self._get_slot(request, spider)\n-        request.meta['download_slot'] = key\n+        request.meta[self.DOWNLOAD_SLOT] = key\n \n         def _deactivate(response):\n             slot.active.remove(request)\n\n@@ -1,15 +0,0 @@\n-import uuid\n-import os.path\n-\n-\n-def unique_files_queue(queue_class):\n-\n-    class UniqueFilesQueue(queue_class):\n-        def __init__(self, path):\n-            path = path + \"-\" + uuid.uuid4().hex\n-            while os.path.exists(path):\n-                path = path + \"-\" + uuid.uuid4().hex\n-\n-            super().__init__(path)\n-\n-    return UniqueFilesQueue\n\n@@ -13,7 +13,7 @@ logger = logging.getLogger(__name__)\n class Scheduler(object):\n \n     def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n-                 logunser=False, stats=None, pqclass=None):\n+                 logunser=False, stats=None, pqclass=None, crawler=None):\n         self.df = dupefilter\n         self.dqdir = self._dqdir(jobdir)\n         self.pqclass = pqclass\n@@ -21,6 +21,7 @@ class Scheduler(object):\n         self.mqclass = mqclass\n         self.logunser = logunser\n         self.stats = stats\n+        self.crawler = crawler\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -32,14 +33,15 @@ class Scheduler(object):\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n         logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS', settings.getbool('SCHEDULER_DEBUG'))\n         return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n-                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)\n+                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n+                   mqclass=mqclass, crawler=crawler)\n \n     def has_pending_requests(self):\n         return len(self) > 0\n \n     def open(self, spider):\n         self.spider = spider\n-        self.mqs = self.pqclass(self._newmq)\n+        self.mqs = create_instance(self.pqclass, None, self.crawler, self._newmq)\n         self.dqs = self._dq() if self.dqdir else None\n         return self.df.open()\n \n@@ -111,7 +113,7 @@ class Scheduler(object):\n         return self.mqclass()\n \n     def _newdq(self, priority):\n-        return self.dqclass(join(self.dqdir, 'p%s' % priority))\n+        return self.dqclass(join(self.dqdir, 'p%s' % (priority, )))\n \n     def _dq(self):\n         activef = join(self.dqdir, 'active.json')\n@@ -120,7 +122,12 @@ class Scheduler(object):\n                 prios = json.load(f)\n         else:\n             prios = ()\n-        q = self.pqclass(self._newdq, startprios=prios)\n+\n+        q = create_instance(self.pqclass,\n+                            None,\n+                            self.crawler,\n+                            self._newdq,\n+                            startprios=prios)\n         if q:\n             logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                         {'queuesize': len(q)}, extra={'spider': self.spider})\n\n@@ -0,0 +1,246 @@\n+from collections import deque\n+import hashlib\n+import logging\n+from six import text_type\n+from six.moves.urllib.parse import urlparse\n+\n+from queuelib import PriorityQueue\n+\n+from scrapy.core.downloader import Downloader\n+from scrapy.http import Request\n+from scrapy.signals import request_reached_downloader, response_downloaded\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+SCHEDULER_SLOT_META_KEY = Downloader.DOWNLOAD_SLOT\n+\n+\n+def _get_from_request(request, key, default=None):\n+    if isinstance(request, dict):\n+        return request.get(key, default)\n+\n+    if isinstance(request, Request):\n+        return getattr(request, key, default)\n+\n+    raise ValueError('Bad type of request \"%s\"' % (request.__class__, ))\n+\n+\n+def _scheduler_slot_read(request, default=None):\n+    meta = _get_from_request(request, 'meta', dict())\n+    slot = meta.get(SCHEDULER_SLOT_META_KEY, default)\n+    return slot\n+\n+\n+def _scheduler_slot_write(request, slot):\n+    meta = _get_from_request(request, 'meta', None)\n+    if not isinstance(meta, dict):\n+        raise ValueError('No meta attribute in %s' % (request, ))\n+    meta[SCHEDULER_SLOT_META_KEY] = slot\n+\n+\n+def _scheduler_slot(request):\n+\n+    slot = _scheduler_slot_read(request, None)\n+    if slot is None:\n+        url = _get_from_request(request, 'url')\n+        slot = urlparse(url).hostname or ''\n+        _scheduler_slot_write(request, slot)\n+\n+    return slot\n+\n+\n+def _pathable(x):\n+    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_' for c in x])\n+\n+    \"\"\"\n+        as we replace some letters we can get collision for different slots\n+        add we add unique part\n+    \"\"\"\n+    unique_slot = hashlib.md5(x.encode('utf8')).hexdigest()\n+\n+    return '-'.join([pathable_slot, unique_slot])\n+\n+\n+class PrioritySlot:\n+    __slots__ = ('priority', 'slot')\n+\n+    def __init__(self, priority=0, slot=None):\n+        self.priority = priority\n+        self.slot = slot\n+\n+    def __hash__(self):\n+       return hash((self.priority, self.slot))\n+\n+    def __eq__(self, other):\n+       return (self.priority, self.slot) == (other.priority, other.slot)\n+\n+    def __lt__(self, other):\n+       return (self.priority, self.slot) < (other.priority, other.slot)\n+\n+    def __str__(self):\n+       return '_'.join([text_type(self.priority), _pathable(text_type(self.slot))])\n+\n+\n+class PriorityAsTupleQueue(PriorityQueue):\n+    \"\"\"\n+        Python structures is not directly (de)serialized (to)from json.\n+        We need this modified queue to transform custom structure (from)to\n+        json serializable structures\n+    \"\"\"\n+    def __init__(self, qfactory, startprios=()):\n+\n+        super(PriorityAsTupleQueue, self).__init__(\n+                qfactory,\n+                [PrioritySlot(priority=p[0], slot=p[1]) for p in startprios]\n+                )\n+\n+    def close(self):\n+        startprios = super(PriorityAsTupleQueue, self).close()\n+        return [(s.priority, s.slot) for s in startprios]\n+\n+    def is_empty(self):\n+        return not self.queues or len(self) == 0\n+\n+\n+class SlotBasedPriorityQueue(object):\n+\n+    def __init__(self, qfactory, startprios={}):\n+        self.pqueues = dict()     # slot -> priority queue\n+        self.qfactory = qfactory  # factory for creating new internal queues\n+\n+        if not startprios:\n+            return\n+\n+        if not isinstance(startprios, dict):\n+            raise ValueError(\"Looks like your priorities file malforfemed. \"\n+                             \"Possible reason: You run scrapy with previous \"\n+                             \"version. Interrupted it. Updated scrapy. And \"\n+                             \"run again.\")\n+\n+        for slot, prios in startprios.items():\n+            self.pqueues[slot] = PriorityAsTupleQueue(self.qfactory, prios)\n+\n+    def pop_slot(self, slot):\n+        queue = self.pqueues[slot]\n+        request = queue.pop()\n+        is_empty = queue.is_empty()\n+        if is_empty:\n+            del self.pqueues[slot]\n+\n+        return request, is_empty\n+\n+    def push_slot(self, request, priority):\n+        slot = _scheduler_slot(request)\n+        is_new = False\n+        if slot not in self.pqueues:\n+            is_new = True\n+            self.pqueues[slot] = PriorityAsTupleQueue(self.qfactory)\n+        self.pqueues[slot].push(request, PrioritySlot(priority=priority, slot=slot))\n+        return slot, is_new\n+\n+    def close(self):\n+        startprios = dict()\n+        for slot, queue in self.pqueues.items():\n+            prios = queue.close()\n+            startprios[slot] = prios\n+        self.pqueues.clear()\n+        return startprios\n+\n+    def __len__(self):\n+        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n+\n+\n+class RoundRobinPriorityQueue(SlotBasedPriorityQueue):\n+\n+    def __init__(self, qfactory, startprios={}):\n+        super(RoundRobinPriorityQueue, self).__init__(qfactory, startprios)\n+        self._slots = deque()\n+        for slot in self.pqueues:\n+            self._slots.append(slot)\n+\n+    def push(self, request, priority):\n+        slot, is_new = self.push_slot(request, priority)\n+        if is_new:\n+            self._slots.append(slot)\n+\n+    def pop(self):\n+        if not self._slots:\n+            return\n+\n+        slot = self._slots.popleft()\n+        request, is_empty = self.pop_slot(slot)\n+\n+        if not is_empty:\n+            self._slots.append(slot)\n+\n+        return request\n+\n+    def close(self):\n+        self._slots.clear()\n+        return super(RoundRobinPriorityQueue, self).close()\n+\n+\n+class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n+\n+    _DOWNLOADER_AWARE_PQ_ID = 'DOWNLOADER_AWARE_PQ_ID'\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, qfactory, startprios={}):\n+        return cls(crawler, qfactory, startprios)\n+\n+    def __init__(self, crawler, qfactory, startprios={}):\n+        super(DownloaderAwarePriorityQueue, self).__init__(qfactory, startprios)\n+        self._slots = {slot: 0 for slot in self.pqueues}\n+        crawler.signals.connect(self.on_response_download,\n+                                signal=response_downloaded)\n+        crawler.signals.connect(self.on_request_reached_downloader,\n+                                signal=request_reached_downloader)\n+\n+    def mark(self, request):\n+        meta = _get_from_request(request, 'meta', None)\n+        if not isinstance(meta, dict):\n+            raise ValueError('No meta attribute in %s' % (request, ))\n+        meta[self._DOWNLOADER_AWARE_PQ_ID] = id(self)\n+\n+    def check_mark(self, request):\n+        return request.meta.get(self._DOWNLOADER_AWARE_PQ_ID, None) == id(self)\n+\n+    def pop(self):\n+        slots = [(d, s) for s,d in self._slots.items() if s in self.pqueues]\n+\n+        if not slots:\n+            return\n+\n+        slot = min(slots)[1]\n+        request, _ = self.pop_slot(slot)\n+        self.mark(request)\n+        return request\n+\n+    def push(self, request, priority):\n+        slot, _ = self.push_slot(request, priority)\n+        if slot not in self._slots:\n+            self._slots[slot] = 0\n+\n+    def on_response_download(self, response, request, spider):\n+        if not self.check_mark(request):\n+            return\n+\n+        slot = _scheduler_slot_read(request)\n+        if slot not in self._slots or self._slots[slot] <= 0:\n+            raise ValueError('Get response for wrong slot \"%s\"' % (slot, ))\n+        self._slots[slot] = self._slots[slot] - 1\n+        if self._slots[slot] == 0 and slot not in self.pqueues:\n+            del self._slots[slot]\n+\n+    def on_request_reached_downloader(self, request, spider):\n+        if not self.check_mark(request):\n+            return\n+\n+        slot = _scheduler_slot_read(request)\n+        self._slots[slot] = self._slots.get(slot, 0) + 1\n+\n+    def close(self):\n+        self._slots.clear()\n+        return super(DownloaderAwarePriorityQueue, self).close()\n\n@@ -0,0 +1,315 @@\n+import contextlib\n+import shutil\n+import tempfile\n+import unittest\n+\n+from scrapy.crawler import Crawler\n+from scrapy.core.scheduler import Scheduler\n+from scrapy.http import Request\n+from scrapy.pqueues import _scheduler_slot_read, _scheduler_slot_write\n+from scrapy.signals import request_reached_downloader, response_downloaded\n+from scrapy.spiders import Spider\n+\n+class MockCrawler(Crawler):\n+    def __init__(self, priority_queue_cls, jobdir):\n+\n+        settings = dict(LOG_UNSERIALIZABLE_REQUESTS=False,\n+                       SCHEDULER_DISK_QUEUE='scrapy.squeues.PickleLifoDiskQueue',\n+                       SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',\n+                       SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n+                       JOBDIR=jobdir,\n+                       DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter')\n+        super(MockCrawler, self).__init__(Spider, settings)\n+\n+\n+class SchedulerHandler:\n+    priority_queue_cls = None\n+    jobdir = None\n+\n+    def create_scheduler(self):\n+        self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)\n+        self.scheduler = Scheduler.from_crawler(self.mock_crawler)\n+        self.spider = Spider(name='spider')\n+        self.scheduler.open(self.spider)\n+\n+    def close_scheduler(self):\n+        self.scheduler.close('finished')\n+        self.mock_crawler.stop()\n+\n+    def setUp(self):\n+        self.create_scheduler()\n+\n+    def tearDown(self):\n+        self.close_scheduler()\n+\n+\n+_PRIORITIES = [(\"http://foo.com/a\", -2),\n+               (\"http://foo.com/d\", 1),\n+               (\"http://foo.com/b\", -1),\n+               (\"http://foo.com/c\", 0),\n+               (\"http://foo.com/e\", 2)]\n+\n+\n+_URLS = {\"http://foo.com/a\", \"http://foo.com/b\", \"http://foo.com/c\"}\n+\n+\n+class BaseSchedulerInMemoryTester(SchedulerHandler):\n+    def test_length(self):\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), 0)\n+\n+        for url in _URLS:\n+            self.scheduler.enqueue_request(Request(url))\n+\n+        self.assertTrue(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), len(_URLS))\n+\n+    def test_dequeue(self):\n+        for url in _URLS:\n+            self.scheduler.enqueue_request(Request(url))\n+\n+        urls = set()\n+        while self.scheduler.has_pending_requests():\n+            urls.add(self.scheduler.next_request().url)\n+\n+        self.assertEqual(urls, _URLS)\n+\n+    def test_dequeue_priorities(self):\n+        for url, priority in _PRIORITIES:\n+            self.scheduler.enqueue_request(Request(url, priority=priority))\n+\n+        priorities = list()\n+        while self.scheduler.has_pending_requests():\n+            priorities.append(self.scheduler.next_request().priority)\n+\n+        self.assertEqual(priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+\n+\n+class BaseSchedulerOnDiskTester(SchedulerHandler):\n+\n+    def setUp(self):\n+        self.jobdir = tempfile.mkdtemp()\n+        self.create_scheduler()\n+\n+    def tearDown(self):\n+        self.close_scheduler()\n+\n+        shutil.rmtree(self.jobdir)\n+        self.jobdir = None\n+\n+    def test_length(self):\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), 0)\n+\n+        for url in _URLS:\n+            self.scheduler.enqueue_request(Request(url))\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        self.assertTrue(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), len(_URLS))\n+\n+    def test_dequeue(self):\n+        for url in _URLS:\n+            self.scheduler.enqueue_request(Request(url))\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        urls = set()\n+        while self.scheduler.has_pending_requests():\n+            urls.add(self.scheduler.next_request().url)\n+\n+        self.assertEqual(urls, _URLS)\n+\n+    def test_dequeue_priorities(self):\n+        for url, priority in _PRIORITIES:\n+            self.scheduler.enqueue_request(Request(url, priority=priority))\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        priorities = list()\n+        while self.scheduler.has_pending_requests():\n+            priorities.append(self.scheduler.next_request().priority)\n+\n+        self.assertEqual(priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+\n+\n+class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n+    priority_queue_cls = 'queuelib.PriorityQueue'\n+\n+\n+class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n+    priority_queue_cls = 'queuelib.PriorityQueue'\n+\n+\n+_SLOTS = [(\"http://foo.com/a\", 'a'),\n+          (\"http://foo.com/b\", 'a'),\n+          (\"http://foo.com/c\", 'b'),\n+          (\"http://foo.com/d\", 'b'),\n+          (\"http://foo.com/e\", 'c'),\n+          (\"http://foo.com/f\", 'c')]\n+\n+\n+class TestSchedulerWithRoundRobinInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n+    priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n+\n+    def test_round_robin(self):\n+        for url, slot in _SLOTS:\n+            request = Request(url)\n+            _scheduler_slot_write(request, slot)\n+            self.scheduler.enqueue_request(request)\n+\n+        slots = list()\n+        while self.scheduler.has_pending_requests():\n+            slots.append(_scheduler_slot_read(self.scheduler.next_request()))\n+\n+        for i in range(0, len(_SLOTS), 2):\n+            self.assertNotEqual(slots[i], slots[i+1])\n+\n+    def test_is_meta_set(self):\n+        url = \"http://foo.com/a\"\n+        request = Request(url)\n+        if _scheduler_slot_read(request):\n+            _scheduler_slot_write(request, None)\n+        self.scheduler.enqueue_request(request)\n+        self.assertIsNotNone(_scheduler_slot_read(request, None), None)\n+\n+\n+class TestSchedulerWithRoundRobinOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n+    priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n+\n+    def test_round_robin(self):\n+        for url, slot in _SLOTS:\n+            request = Request(url)\n+            _scheduler_slot_write(request, slot)\n+            self.scheduler.enqueue_request(request)\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        slots = list()\n+        while self.scheduler.has_pending_requests():\n+            slots.append(_scheduler_slot_read(self.scheduler.next_request()))\n+\n+        for i in range(0, len(_SLOTS), 2):\n+            self.assertNotEqual(slots[i], slots[i+1])\n+\n+    def test_is_meta_set(self):\n+        url = \"http://foo.com/a\"\n+        request = Request(url)\n+        if _scheduler_slot_read(request):\n+            _scheduler_slot_write(request, None)\n+        self.scheduler.enqueue_request(request)\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        self.assertIsNotNone(_scheduler_slot_read(request, None), None)\n+\n+\n+@contextlib.contextmanager\n+def mkdtemp():\n+    dir = tempfile.mkdtemp()\n+    try:\n+        yield dir\n+    finally:\n+        shutil.rmtree(dir)\n+\n+\n+def _migration():\n+\n+    with mkdtemp() as tmp_dir:\n+        prev_scheduler_handler = SchedulerHandler()\n+        prev_scheduler_handler.priority_queue_cls = 'queuelib.PriorityQueue'\n+        prev_scheduler_handler.jobdir = tmp_dir\n+\n+        prev_scheduler_handler.create_scheduler()\n+        for url in _URLS:\n+            prev_scheduler_handler.scheduler.enqueue_request(Request(url))\n+        prev_scheduler_handler.close_scheduler()\n+\n+        next_scheduler_handler = SchedulerHandler()\n+        next_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n+        next_scheduler_handler.jobdir = tmp_dir\n+\n+        next_scheduler_handler.create_scheduler()\n+\n+\n+class TestMigration(unittest.TestCase):\n+    def test_migration(self):\n+        self.assertRaises(ValueError, _migration)\n+\n+\n+class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n+    priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+\n+    def test_logic(self):\n+        for url, slot in _SLOTS:\n+            request = Request(url)\n+            _scheduler_slot_write(request, slot)\n+            self.scheduler.enqueue_request(request)\n+\n+        slots = list()\n+        requests = list()\n+        while self.scheduler.has_pending_requests():\n+            request = self.scheduler.next_request()\n+            slots.append(_scheduler_slot_read(request))\n+            self.mock_crawler.signals.send_catch_log(\n+                    signal=request_reached_downloader,\n+                    request=request,\n+                    spider=self.spider\n+                    )\n+            requests.append(request)\n+        self.assertEqual(len(slots), len(_SLOTS))\n+\n+        for request in requests:\n+            self.mock_crawler.signals.send_catch_log(signal=response_downloaded,\n+                                                     request=request,\n+                                                     response=None,\n+                                                     spider=self.spider)\n+\n+        unique_slots = len(set(s for _, s in _SLOTS))\n+        for i in range(0, len(_SLOTS), unique_slots):\n+            part = slots[i:i + unique_slots]\n+            self.assertEqual(len(part), len(set(part)))\n+\n+\n+class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n+    priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+    def test_logic(self):\n+        for url, slot in _SLOTS:\n+            request = Request(url)\n+            _scheduler_slot_write(request, slot)\n+            self.scheduler.enqueue_request(request)\n+\n+        self.close_scheduler()\n+        self.create_scheduler()\n+\n+        slots = list()\n+        requests = list()\n+        while self.scheduler.has_pending_requests():\n+            request = self.scheduler.next_request()\n+            slots.append(_scheduler_slot_read(request))\n+            self.mock_crawler.signals.send_catch_log(\n+                    signal=request_reached_downloader,\n+                    request=request,\n+                    spider=self.spider\n+                    )\n+            requests.append(request)\n+\n+        self.assertEqual(self.scheduler.mqs._slots, {})\n+        self.assertEqual(len(slots), len(_SLOTS))\n+\n+        for request in requests:\n+            self.mock_crawler.signals.send_catch_log(signal=response_downloaded,\n+                                                     request=request,\n+                                                     response=None,\n+                                                     spider=self.spider)\n+\n+        unique_slots = len(set(s for _, s in _SLOTS))\n+        for i in range(0, len(_SLOTS), unique_slots):\n+            part = slots[i:i + unique_slots]\n+            self.assertEqual(len(part), len(set(part)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
