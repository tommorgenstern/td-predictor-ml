{"custom_id": "keras#119cd4655d01570a70c70879dff4461ea46161bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 2753 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -209,7 +209,7 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n     super(BinaryAccuracy, self).__init__(\n-        binary_accuracy, name, dtype=dtype, threshold=threshold)\n+        metrics_utils.binary_matches, name, dtype=dtype, threshold=threshold)\n \n \n @keras_export('keras.metrics.CategoricalAccuracy')\n@@ -3211,10 +3211,8 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n   Returns:\n     Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n   \"\"\"\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  threshold = tf.cast(threshold, y_pred.dtype)\n-  y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n-  return backend.mean(tf.equal(y_true, y_pred), axis=-1)\n+\n+  return backend.mean(metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1)\n \n \n @keras_export('keras.metrics.categorical_accuracy')\n\n@@ -839,3 +839,21 @@ def _assert_splits_match(nested_splits_lists):\n       for splits_list in nested_splits_lists[1:]\n       for (s1, s2) in zip(nested_splits_lists[0], splits_list)\n   ]\n+\n+\n+def binary_matches(y_true, y_pred, threshold=0.5):\n+  \"\"\"Creates int Tensor, 1 for label-prediction match, 0 for mismatch.\n+\n+  Args:\n+    y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+    y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+    threshold: (Optional) Float representing the threshold for deciding whether\n+      prediction values are 1 or 0.\n+\n+  Returns:\n+    Binary matches. shape = `[batch_size, d0, .. dN]`\n+  \"\"\"\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  threshold = tf.cast(threshold, y_pred.dtype)\n+  y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n+  return tf.cast(tf.equal(y_true, y_pred), tf.int8)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33f395aeceaad95910ce6f0931621bb82d3e967c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 49 | Files Changed: 2 | Hunks: 9 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/13 | Churn Δ: 158 | Churn Cumulative: 2911 | Contributors (this commit): 9 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -261,7 +261,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='categorical_accuracy', dtype=None):\n     super(CategoricalAccuracy, self).__init__(\n-        categorical_accuracy, name, dtype=dtype)\n+        metrics_utils.categorical_matches, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.SparseCategoricalAccuracy')\n@@ -312,7 +312,7 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n     super(SparseCategoricalAccuracy, self).__init__(\n-        sparse_categorical_accuracy, name, dtype=dtype)\n+        metrics_utils.sparse_categorical_matches, name, dtype=dtype)\n \n \n _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n@@ -377,7 +377,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n     super(TopKCategoricalAccuracy, self).__init__(\n-        top_k_categorical_accuracy, name, dtype=dtype, k=k)\n+        metrics_utils.top_k_categorical_matches, name, dtype=dtype, k=k)\n \n \n @keras_export('keras.metrics.SparseTopKCategoricalAccuracy')\n@@ -415,7 +415,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n     super(SparseTopKCategoricalAccuracy, self).__init__(\n-        sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n+        metrics_utils.sparse_top_k_categorical_matches, name, dtype=dtype, k=k)\n \n \n SparseTopKCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n@@ -3238,8 +3238,7 @@ def categorical_accuracy(y_true, y_pred):\n   Returns:\n     Categorical accuracy values.\n   \"\"\"\n-  y_true = tf.math.argmax(y_true, axis=-1)\n-  return sparse_categorical_accuracy(y_true, y_pred)\n+  return metrics_utils.categorical_matches(y_true, y_pred)\n \n \n @keras_export('keras.metrics.sparse_categorical_accuracy')\n@@ -3265,22 +3264,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n   Returns:\n     Sparse categorical accuracy values.\n   \"\"\"\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.convert_to_tensor(y_true)\n-  y_pred_rank = y_pred.shape.ndims\n-  y_true_rank = y_true.shape.ndims\n-  # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n-  if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n-      backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n-    y_true = tf.squeeze(y_true, [-1])\n-  y_pred = tf.math.argmax(y_pred, axis=-1)\n-\n-  # If the predicted output and actual output types don't match, force cast them\n-  # to match.\n-  if backend.dtype(y_pred) != backend.dtype(y_true):\n-    y_pred = tf.cast(y_pred, backend.dtype(y_true))\n-\n-  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n+  return metrics_utils.sparse_categorical_matches(y_true, y_pred)\n \n \n @keras_export('keras.metrics.top_k_categorical_accuracy')\n@@ -3305,8 +3289,7 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Top K categorical accuracy value.\n   \"\"\"\n-  y_true = tf.math.argmax(y_true, axis=-1)\n-  return sparse_top_k_categorical_accuracy(y_true, y_pred, k=k)\n+  return metrics_utils.top_k_categorical_matches(y_true, y_pred, k)\n \n \n @keras_export('keras.metrics.sparse_top_k_categorical_accuracy')\n@@ -3332,31 +3315,7 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Sparse top K categorical accuracy value.\n   \"\"\"\n-  reshape_matches = False\n-  y_true = tf.convert_to_tensor(y_true)\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  y_true_rank = y_true.shape.ndims\n-  y_pred_rank = y_pred.shape.ndims\n-  y_true_org_shape = tf.shape(y_true)\n-\n-  # Flatten y_pred to (batch_size, num_samples) and y_true to (num_samples,)\n-  if (y_true_rank is not None) and (y_pred_rank is not None):\n-    if y_pred_rank > 2:\n-      y_pred = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n-    if y_true_rank > 1:\n-      reshape_matches = True\n-      y_true = tf.reshape(y_true, [-1])\n-\n-  matches = tf.cast(\n-      tf.math.in_top_k(\n-          predictions=y_pred, targets=tf.cast(y_true, 'int32'), k=k),\n-      dtype=backend.floatx())\n-\n-  # returned matches is expected to have same shape as y_true input\n-  if reshape_matches:\n-    return tf.reshape(matches, shape=y_true_org_shape)\n-\n-  return matches\n+  return metrics_utils.sparse_top_k_categorical_matches(y_true, y_pred, k)\n \n \n def cosine_similarity(y_true, y_pred, axis=-1):\n\n@@ -857,3 +857,104 @@ def binary_matches(y_true, y_pred, threshold=0.5):\n   threshold = tf.cast(threshold, y_pred.dtype)\n   y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n   return tf.cast(tf.equal(y_true, y_pred), tf.int8)\n+\n+\n+def sparse_categorical_matches(y_true, y_pred):\n+  \"\"\"Creates float Tensor, 1.0 for label-prediction match, 0.0 for mismatch.\n+\n+  You can provide logits of classes as `y_pred`, since argmax of\n+  logits and probabilities are same.\n+\n+  Args:\n+    y_true: Integer ground truth values.\n+    y_pred: The prediction values.\n+\n+  Returns:\n+    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n+  \"\"\"\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true = tf.convert_to_tensor(y_true)\n+  y_pred_rank = y_pred.shape.ndims\n+  y_true_rank = y_true.shape.ndims\n+  # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n+  if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n+      backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n+    y_true = tf.squeeze(y_true, [-1])\n+  y_pred = tf.math.argmax(y_pred, axis=-1)\n+\n+  # If the predicted output and actual output types don't match, force cast them\n+  # to match.\n+  if backend.dtype(y_pred) != backend.dtype(y_true):\n+    y_pred = tf.cast(y_pred, backend.dtype(y_true))\n+  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n+\n+\n+def categorical_matches(y_true, y_pred):\n+  \"\"\"Creates float Tensor for how often predictions match one-hot labels.\n+\n+  You can provide logits of classes as `y_pred`, since argmax of\n+  logits and probabilities are same.\n+\n+  Args:\n+    y_true: One-hot ground truth values.\n+    y_pred: The prediction values.\n+\n+  Returns:\n+    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n+  \"\"\"\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_categorical_matches(y_true, y_pred)\n+\n+\n+def sparse_top_k_categorical_matches(y_true, y_pred, k=5):\n+  \"\"\"Creates float Tensor, 1.0 for label-TopK_prediction match, 0.0 for mismatch.\n+\n+  Args:\n+    y_true: tensor of true targets.\n+    y_pred: tensor of predicted targets.\n+    k: (Optional) Number of top elements to look at for computing accuracy.\n+      Defaults to 5.\n+\n+  Returns:\n+    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n+  \"\"\"\n+  reshape_matches = False\n+  y_true = tf.convert_to_tensor(y_true)\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true_rank = y_true.shape.ndims\n+  y_pred_rank = y_pred.shape.ndims\n+  y_true_org_shape = tf.shape(y_true)\n+\n+  # Flatten y_pred to (batch_size, num_samples) and y_true to (num_samples,)\n+  if (y_true_rank is not None) and (y_pred_rank is not None):\n+    if y_pred_rank > 2:\n+      y_pred = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n+    if y_true_rank > 1:\n+      reshape_matches = True\n+      y_true = tf.reshape(y_true, [-1])\n+\n+  matches = tf.cast(\n+      tf.math.in_top_k(\n+          predictions=y_pred, targets=tf.cast(y_true, 'int32'), k=k),\n+      dtype=backend.floatx())\n+\n+  # returned matches is expected to have same shape as y_true input\n+  if reshape_matches:\n+    return tf.reshape(matches, shape=y_true_org_shape)\n+\n+  return matches\n+\n+def top_k_categorical_matches(y_true, y_pred, k=5):\n+  \"\"\"Creates float Tensor for how often topK_predictions match one-hot labels.\n+\n+  Args:\n+    y_true: The ground truth values.\n+    y_pred: The prediction values.\n+    k: (Optional) Number of top elements to look at for computing accuracy.\n+      Defaults to 5.\n+\n+  Returns:\n+    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n+  \"\"\"\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_top_k_categorical_matches(y_true, y_pred, k=k)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6ba55e98fc30ee4ca63d9ac41fe5e2067a32afa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1792 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -856,7 +856,7 @@ def binary_matches(y_true, y_pred, threshold=0.5):\n   y_pred = tf.convert_to_tensor(y_pred)\n   threshold = tf.cast(threshold, y_pred.dtype)\n   y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n-  return tf.cast(tf.equal(y_true, y_pred), tf.int8)\n+  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n \n \n def sparse_categorical_matches(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82d97545730974297137cec59e3b9c09d07b1ee9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 2935 | Contributors (this commit): 9 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -3194,6 +3194,10 @@ def accuracy(y_true, y_pred):\n def binary_accuracy(y_true, y_pred, threshold=0.5):\n   \"\"\"Calculates how often predictions match binary labels.\n \n+  Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n+  public facing binary_accuracy behavior and decouples it from the vital\n+  behavior of the binary_matches method needed in backend dependencies.\n+\n   Standalone usage:\n   >>> y_true = [[1], [1], [0], [0]]\n   >>> y_pred = [[1], [1], [0], [0]]\n@@ -3220,6 +3224,10 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n def categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match one-hot labels.\n \n+  Note: wraps metrics_utils.categorical_matches. This decouples public facing\n+  categorical_accuracy behavior from the vital behavior of the \n+  categorical_matches method needed in backend dependencies.\n+\n   Standalone usage:\n   >>> y_true = [[0, 0, 1], [0, 1, 0]]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3246,6 +3254,10 @@ def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match integer labels.\n \n+  Note: wraps metrics_utils.sparse_categorical_matches. This decouples\n+  public facing sparse_categorical_accuracy behavior from the vital behavior\n+  of the sparse_categorical_matches method needed in backend dependencies.\n+\n   Standalone usage:\n   >>> y_true = [2, 1]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3272,6 +3284,10 @@ def sparse_categorical_accuracy(y_true, y_pred):\n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often targets are in the top `K` predictions.\n \n+  Note: wraps metrics_utils.top_k_categorical_matches. This decouples\n+  public facing top_k_categorical_accuracy behavior from the vital behavior\n+  of the top_k_categorical_matches method needed in backend dependencies.\n+\n   Standalone usage:\n   >>> y_true = [[0, 0, 1], [0, 1, 0]]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n@@ -3297,6 +3313,11 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n+  Note: wraps metrics_utils.sparse_top_k_categorical_matches. This decouples\n+  public facing sparse_top_k_categorical_accuracy behavior from the vital\n+  behavior of the sparse_top_k_categorical_matches method needed in backend\n+  dependencies.\n+\n   Standalone usage:\n   >>> y_true = [2, 1]\n   >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n\n@@ -944,6 +944,7 @@ def sparse_top_k_categorical_matches(y_true, y_pred, k=5):\n \n   return matches\n \n+\n def top_k_categorical_matches(y_true, y_pred, k=5):\n   \"\"\"Creates float Tensor for how often topK_predictions match one-hot labels.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7b4343f127784a84ef344ecb728b4de123155ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 29 | Churn Cumulative: 3381 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -222,7 +222,8 @@ class CenterCrop(base_layer.Layer):\n \n \n @keras_export('keras.layers.RandomCrop',\n-              'keras.layers.experimental.preprocessing.RandomCrop')\n+              'keras.layers.experimental.preprocessing.RandomCrop',\n+              v1=[])\n class RandomCrop(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n@@ -265,8 +266,6 @@ class RandomCrop(base_layer.BaseRandomLayer):\n     self.seed = seed\n \n   def call(self, inputs, training=True):\n-    if training is None:\n-      training = backend.learning_phase()\n     inputs = utils.ensure_tensor(inputs, dtype=self.compute_dtype)\n     input_shape = tf.shape(inputs)\n     h_diff = input_shape[H_AXIS] - self.height\n@@ -369,7 +368,8 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n \n @keras_export('keras.layers.RandomFlip',\n-              'keras.layers.experimental.preprocessing.RandomFlip')\n+              'keras.layers.experimental.preprocessing.RandomFlip',\n+              v1=[])\n class RandomFlip(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n@@ -463,7 +463,8 @@ class RandomFlip(base_layer.BaseRandomLayer):\n \n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.RandomTranslation',\n-              'keras.layers.experimental.preprocessing.RandomTranslation')\n+              'keras.layers.experimental.preprocessing.RandomTranslation',\n+              v1=[])\n class RandomTranslation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly translates images during training.\n \n@@ -793,7 +794,8 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n \n @keras_export('keras.layers.RandomRotation',\n-              'keras.layers.experimental.preprocessing.RandomRotation')\n+              'keras.layers.experimental.preprocessing.RandomRotation',\n+              v1=[])\n class RandomRotation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n@@ -922,7 +924,8 @@ class RandomRotation(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.RandomZoom',\n-              'keras.layers.experimental.preprocessing.RandomZoom')\n+              'keras.layers.experimental.preprocessing.RandomZoom',\n+              v1=[])\n class RandomZoom(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n@@ -1130,7 +1133,8 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n \n @keras_export('keras.layers.RandomContrast',\n-              'keras.layers.experimental.preprocessing.RandomContrast')\n+              'keras.layers.experimental.preprocessing.RandomContrast',\n+              v1=[])\n class RandomContrast(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n@@ -1217,7 +1221,8 @@ class RandomContrast(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.RandomHeight',\n-              'keras.layers.experimental.preprocessing.RandomHeight')\n+              'keras.layers.experimental.preprocessing.RandomHeight',\n+              v1=[])\n class RandomHeight(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image height during training.\n \n@@ -1326,7 +1331,8 @@ class RandomHeight(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.RandomWidth',\n-              'keras.layers.experimental.preprocessing.RandomWidth')\n+              'keras.layers.experimental.preprocessing.RandomWidth',\n+              v1=[])\n class RandomWidth(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image width during training.\n \n@@ -1390,7 +1396,6 @@ class RandomWidth(base_layer.BaseRandomLayer):\n \n   def call(self, inputs, training=True):\n     inputs = utils.ensure_tensor(inputs)\n-\n     def random_width_inputs(inputs):\n       \"\"\"Inputs width-adjusted with random ops.\"\"\"\n       inputs_shape = tf.shape(inputs)\n\n@@ -29,7 +29,7 @@ import tensorflow.compat.v2 as tf\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n         strategy_combinations.multi_worker_mirrored_strategies,\n-        mode=[\"eager\", \"graph\"]))\n+        mode=[\"eager\"]))\n class ImagePreprocessingDistributionTest(\n     test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e71cbf258c718739d5c57ab8b0b09c882eac6239", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1596 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -389,7 +389,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     During `adapt()`, the layer will build a vocabulary of all string tokens\n     seen in the dataset, sorted by occurance count, with ties broken by sort\n     order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n     adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n     tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n     will also learn the document frequencies of each token in the input dataset.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59568edf69803a56d419cd211dd1ae6024d0195a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 808 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -637,10 +637,6 @@ def type_spec_with_shape(spec, shape):\n     # private fields.)\n     shape = tf.TensorShape(shape)\n     spec._shape = shape\n-    if shape.rank is None:\n-      spec._shape_tuple = None\n-    else:\n-      spec._shape_tuple = tuple(shape.as_list())\n     return spec\n   elif isinstance(spec, tf.RaggedTensorSpec):\n     return tf.RaggedTensorSpec(shape, spec.dtype, spec.ragged_rank,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa283f9874d6964f92bd4bb5ecca661f1bf1a98c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1152 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3195,7 +3195,7 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n   \"\"\"Calculates how often predictions match binary labels.\n \n   Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n-  public facing binary_accuracy behavior and decouples it from the vital\n+  public facing binary_accuracy behavior and seperates it from the vital\n   behavior of the binary_matches method needed in backend dependencies.\n \n   Standalone usage:\n@@ -3224,7 +3224,7 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n def categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match one-hot labels.\n \n-  Note: wraps metrics_utils.categorical_matches. This decouples public facing\n+  Note: wraps metrics_utils.categorical_matches. This seperates public facing\n   categorical_accuracy behavior from the vital behavior of the \n   categorical_matches method needed in backend dependencies.\n \n@@ -3254,7 +3254,7 @@ def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n   \"\"\"Calculates how often predictions match integer labels.\n \n-  Note: wraps metrics_utils.sparse_categorical_matches. This decouples\n+  Note: wraps metrics_utils.sparse_categorical_matches. This seperates\n   public facing sparse_categorical_accuracy behavior from the vital behavior\n   of the sparse_categorical_matches method needed in backend dependencies.\n \n@@ -3284,7 +3284,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often targets are in the top `K` predictions.\n \n-  Note: wraps metrics_utils.top_k_categorical_matches. This decouples\n+  Note: wraps metrics_utils.top_k_categorical_matches. This seperates\n   public facing top_k_categorical_accuracy behavior from the vital behavior\n   of the top_k_categorical_matches method needed in backend dependencies.\n \n@@ -3313,7 +3313,7 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n-  Note: wraps metrics_utils.sparse_top_k_categorical_matches. This decouples\n+  Note: wraps metrics_utils.sparse_top_k_categorical_matches. This seperates\n   public facing sparse_top_k_categorical_accuracy behavior from the vital\n   behavior of the sparse_top_k_categorical_matches method needed in backend\n   dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3f05f06498f2adeb312d50cfe80ccab528a7f64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 49 | Files Changed: 2 | Hunks: 28 | Methods Changed: 13 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 93 | Churn Cumulative: 144 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,11 +44,11 @@ class _BaseOptimizer(tf.Module):\n                jit_compile=False,\n                **kwargs):\n     self._name = name\n-    self._clipnorm = clipnorm\n-    self._global_clipnorm = global_clipnorm\n-    self._clipvalue = clipvalue\n-    self._use_ema = use_ema\n-    self._jit_compile = jit_compile\n+    self.clipnorm = clipnorm\n+    self.global_clipnorm = global_clipnorm\n+    self.clipvalue = clipvalue\n+    self.use_ema = use_ema\n+    self.jit_compile = jit_compile\n     if use_ema:\n       # Verify the arguments related to EMA.\n       if ema_momentum > 1 or ema_momentum < 0:\n@@ -59,13 +59,13 @@ class _BaseOptimizer(tf.Module):\n         raise ValueError(\n             \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n             f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\")\n-    self._ema_momentum = ema_momentum\n-    self._ema_overwrite_frequency = ema_overwrite_frequency\n+    self.ema_momentum = ema_momentum\n+    self.ema_overwrite_frequency = ema_overwrite_frequency\n \n-    if self._clipnorm is not None and self._global_clipnorm is not None:\n+    if self.clipnorm is not None and self.global_clipnorm is not None:\n       raise ValueError(f\"At most one of `clipnorm` and `global_clipnorm` can \"\n-                       f\"be set. Received: clipnorm={self._clipnorm}, \"\n-                       f\"global_clipnorm={self._global_clipnorm}.\")\n+                       f\"be set. Received: clipnorm={self.clipnorm}, \"\n+                       f\"global_clipnorm={self.global_clipnorm}.\")\n \n     self._create_iteration_variable()\n     self._process_kwargs(kwargs)\n@@ -134,7 +134,7 @@ class _BaseOptimizer(tf.Module):\n     return self.update_step(gradient, variable)\n \n   def _update_step(self, gradient, variable):\n-    if self._jit_compile:\n+    if self.jit_compile:\n       self._update_step_xla(gradient, variable, id(self._var_key(variable)))\n     else:\n       self.update_step(gradient, variable)\n@@ -168,18 +168,18 @@ class _BaseOptimizer(tf.Module):\n \n   def _clip_gradients(self, grads):\n     clipped_grads = []\n-    if self._clipnorm and self._clipnorm > 0:\n+    if self.clipnorm and self.clipnorm > 0:\n       for g in grads:\n         if g is None:\n           clipped_grads.append(g)\n         else:\n-          clipped_grads.append(tf.clip_by_norm(g, self._clipnorm))\n+          clipped_grads.append(tf.clip_by_norm(g, self.clipnorm))\n       return clipped_grads\n \n-    if self._global_clipnorm and self._global_clipnorm > 0:\n-      return tf.clip_by_global_norm(grads, self._global_clipnorm)[0]\n+    if self.global_clipnorm and self.global_clipnorm > 0:\n+      return tf.clip_by_global_norm(grads, self.global_clipnorm)[0]\n \n-    if self._clipvalue and self._clipvalue > 0:\n+    if self.clipvalue and self.clipvalue > 0:\n       for g in grads:\n         if g is None:\n           clipped_grads.append(g)\n@@ -187,17 +187,12 @@ class _BaseOptimizer(tf.Module):\n           clipped_grads.append(\n               tf.clip_by_value(\n                   g,\n-                  clip_value_min=-self._clipvalue,  # pylint: disable=invalid-unary-operand-type\n-                  clip_value_max=self._clipvalue))\n+                  clip_value_min=-self.clipvalue,  # pylint: disable=invalid-unary-operand-type\n+                  clip_value_max=self.clipvalue))\n       return clipped_grads\n \n     return grads\n \n-  @property\n-  def use_ema(self):\n-    \"\"\"Returns whether the optimizer uses EMA of weights.\"\"\"\n-    return self._use_ema\n-\n   @property\n   def iterations(self):\n     \"\"\"The number of training steps this `optimizer` has run.\n@@ -286,7 +281,7 @@ class _BaseOptimizer(tf.Module):\n     if getattr(self, \"_built\", False):\n       return\n     self._build_index_dict(var_list)\n-    if self._use_ema:\n+    if self.use_ema:\n       self._model_variables_moving_average = []\n       for var in var_list:\n         # Make a copy of the model variables, we will use the copy to store the\n@@ -429,10 +424,10 @@ class _BaseOptimizer(tf.Module):\n \n   def _update_model_variables_moving_average(self, var_list):\n     \"\"\"Update the stored moving average using the latest value.\"\"\"\n-    if self._use_ema:\n+    if self.use_ema:\n       for (var, average) in zip(var_list, self._model_variables_moving_average):\n-        average.assign(self._ema_momentum * average +\n-                       (1 - self._ema_momentum) * var)\n+        average.assign(self.ema_momentum * average +\n+                       (1 - self.ema_momentum) * var)\n \n   def _overwrite_model_variables_with_average_value(self, var_list):\n     \"\"\"Overwrite model variables with its moving average.\"\"\"\n@@ -458,7 +453,7 @@ class _BaseOptimizer(tf.Module):\n     Args:\n       var_list: list of model variables.\n     \"\"\"\n-    if self._use_ema:\n+    if self.use_ema:\n       # If the optimizer uses EMA, then when finalizing, we replace the model\n       # variable value with its moving average stored inside optimizer.\n       self._overwrite_model_variables_with_average_value(var_list)\n@@ -488,13 +483,13 @@ class _BaseOptimizer(tf.Module):\n         Python dictionary.\n     \"\"\"\n     config = {\n-        \"clipnorm\": self._clipnorm,\n-        \"global_clipnorm\": self._global_clipnorm,\n-        \"clipvalue\": self._clipvalue,\n-        \"use_ema\": self._use_ema,\n-        \"ema_momentum\": self._ema_momentum,\n-        \"ema_overwrite_frequency\": self._ema_overwrite_frequency,\n-        \"jit_compile\": self._jit_compile,\n+        \"clipnorm\": self.clipnorm,\n+        \"global_clipnorm\": self.global_clipnorm,\n+        \"clipvalue\": self.clipvalue,\n+        \"use_ema\": self.use_ema,\n+        \"ema_momentum\": self.ema_momentum,\n+        \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n+        \"jit_compile\": self.jit_compile,\n     }\n     return config\n \n@@ -827,10 +822,10 @@ class Optimizer(_BaseOptimizer):\n \n   def _update_model_variables_moving_average(self, var_list):\n     \"\"\"Update the stored moving average using the latest value.\"\"\"\n-    if self._use_ema:\n+    if self.use_ema:\n       def update_average(average, var):\n-        average.assign(self._ema_momentum * average +\n-                       (1 - self._ema_momentum) * var)\n+        average.assign(self.ema_momentum * average +\n+                       (1 - self.ema_momentum) * var)\n \n       for (var, average) in zip(var_list, self._model_variables_moving_average):\n         self._distribution_strategy.extended.update(\n@@ -848,14 +843,14 @@ class Optimizer(_BaseOptimizer):\n           var, apply_grad_to_update_var, args=(grad,), group=False)\n     self.iterations.assign_add(1)\n \n-    if self._use_ema:\n+    if self.use_ema:\n       _, var_list = zip(*grads_and_vars)\n       self._update_model_variables_moving_average(var_list)\n-      if self._ema_overwrite_frequency:\n-        # Only when self._ema_overwrite_frequency is not None, we overwrite the\n+      if self.ema_overwrite_frequency:\n+        # Only when self.ema_overwrite_frequency is not None, we overwrite the\n         # model variables.\n         should_overwrite_model_vars = (\n-            self.iterations % self._ema_overwrite_frequency == 0)\n+            self.iterations % self.ema_overwrite_frequency == 0)\n         tf.cond(\n             tf.cast(should_overwrite_model_vars, tf.bool),\n             true_fn=lambda: self._overwrite_model_variables_with_average_value(  # pylint: disable=g-long-lambda\n\n@@ -118,7 +118,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testPassingLegacyClipnorm(self):\n     optimizer = adam_new.Adam(clipnorm=1)\n-    self.assertEqual(optimizer._clipnorm, 1)\n+    self.assertEqual(optimizer.clipnorm, 1)\n \n   def testReturnAllOptimizerVariables(self):\n     x = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n@@ -268,7 +268,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         [keras.layers.Input(shape=(1,)),\n          keras.layers.Dense(1)])\n     optimizer = optimizer_fn()\n-    optimizer._clipnorm = 0.1\n+    optimizer.clipnorm = 0.1\n     x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     model.compile(loss=\"mse\", optimizer=optimizer)\n@@ -282,7 +282,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     loaded_optimizer = loaded_model.optimizer\n     self.assertEqual(type(optimizer), type(loaded_optimizer))\n     self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n-    self.assertEqual(loaded_optimizer._clipnorm, 0.1)\n+    self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n \n     # Save in Keras SavedModel format.\n     model.fit(x, y)\n@@ -293,7 +293,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     loaded_optimizer = loaded_model.optimizer\n     self.assertEqual(type(optimizer), type(loaded_optimizer))\n     self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n-    self.assertEqual(loaded_optimizer._clipnorm, 0.1)\n+    self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n \n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n@@ -352,14 +352,14 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n       y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n       model.compile(loss=\"mse\", optimizer=optimizer)\n     model.fit(x, y, epochs=1, steps_per_epoch=5)\n-    if optimizer._name == \"Adam\":\n+    if optimizer.name == \"Adam\":\n       # Assert the momentum variable is not 0.\n       self.assertNotEqual(self.evaluate(optimizer._momentums._storage[0]), 0)\n-    elif optimizer._name == \"Adadelta\":\n+    elif optimizer.name == \"Adadelta\":\n       # Assert the accumulated variable is not 0.\n       self.assertNotEqual(\n           self.evaluate(optimizer._accumulated_grads._storage[0]), 0)\n-    elif optimizer._name == \"Adagrad\":\n+    elif optimizer.name == \"Adagrad\":\n       # Assert the accumulated variable is not 0.\n       self.assertNotEqual(self.evaluate(optimizer._accumulators._storage[0]), 0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61dcf4ac788f346493cb15ec785504569cbf91c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 269 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 18 | Complexity Δ (Sum/Max): 27/17 | Churn Δ: 269 | Churn Cumulative: 6112 | Contributors (this commit): 7 | Commits (past 90d): 17 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1220,6 +1220,144 @@ class RandomContrast(base_layer.BaseRandomLayer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n+@keras_export('keras.layers.RandomBrightness', v1=[])\n+class RandomBrightness(base_layer.BaseRandomLayer):\n+  \"\"\"A preprocessing layer which randomly adjusts brightness during training.\n+\n+  This layer will randomly increase/reduce the brightness for the input RGB\n+  images. At inference time, the output will be identical to the input.\n+  Call the layer with `training=True` to adjust the brightness of the input.\n+\n+  Note that different brightness adjustment factors\n+  will be apply to each the images in the batch.\n+\n+  For an overview and full list of preprocessing layers, see the preprocessing\n+  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n+\n+  Args:\n+    factor: Float or a list/tuple of 2 floats between -1.0 and 1.0. The\n+      factor is used to determine the lower bound and upper bound of the\n+      brightness adjustment. A float value will be chosen randomly between\n+      the limits. When -1.0 is chosen, the output image will be black, and\n+      when 1.0 is chosen, the image will be fully white. When only one float\n+      is provided, eg, 0.2, then -0.2 will be used for lower bound and 0.2\n+      will be used for upper bound.\n+    value_range: Optional list/tuple of 2 floats for the lower and upper limit\n+      of the values of the input data. Defaults to [0.0, 255.0]. Can be changed\n+      to e.g. [0.0, 1.0] if the image input has been scaled before this layer.\n+      The brightness adjustment will be scaled to this range, and the\n+      output values will be clipped to this range.\n+    seed: optional integer, for fixed RNG behavior.\n+\n+  Inputs: 3D (HWC) or 4D (NHWC) tensor, with float or int dtype. Input pixel\n+    values can be of any range (e.g. `[0., 1.)` or `[0, 255]`)\n+\n+  Output: 3D (HWC) or 4D (NHWC) tensor with brightness adjusted based on the\n+    `factor`. By default, the layer will output floats. The output value will\n+    be clipped to the range `[0, 255]`, the valid range of RGB colors, and\n+    rescaled based on the `value_range` if needed.\n+\n+  Sample usage:\n+\n+  ```python\n+  random_bright = tf.keras.layers.RandomBrightness(factor=0.2)\n+\n+  # An image with shape [2, 2, 3]\n+  image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]\n+\n+  # Assume we randomly select the factor to be 0.1, then it will apply\n+  # 0.1 * 255 to all the channel\n+  output = random_bright(image, training=True)\n+\n+  # output will be int64 with 25.5 added to each channel and round down.\n+  tf.Tensor([[[26.5, 27.5, 28.5]\n+              [29.5, 30.5, 31.5]]\n+             [[32.5, 33.5, 34.5]\n+              [35.5, 36.5, 37.5]]],\n+            shape=(2, 2, 3), dtype=int64)\n+  ```\n+  \"\"\"\n+  _FACTOR_VALIDATION_ERROR = (\n+      'The `factor` argument should be a number (or a list of two numbers) '\n+      'in the range [-1.0, 1.0]. ')\n+  _VALUE_RANGE_VALIDATION_ERROR = (\n+      'The `value_range` argument should be a list of two numbers. ')\n+\n+  def __init__(self, factor, value_range=(0, 255), seed=None, **kwargs):\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomBrightness').set(\n+        True)\n+    super().__init__(seed=seed, force_generator=True, **kwargs)\n+    self._set_factor(factor)\n+    self._set_value_range(value_range)\n+    self._seed = seed\n+\n+  def _set_value_range(self, value_range):\n+    if not isinstance(value_range, (tuple, list)):\n+      raise ValueError(\n+          self._VALUE_RANGE_VALIDATION_ERROR + f'Got {value_range}')\n+    if len(value_range) != 2:\n+      raise ValueError(\n+          self._VALUE_RANGE_VALIDATION_ERROR + f'Got {value_range}')\n+    self._value_range = sorted(value_range)\n+\n+  def _set_factor(self, factor):\n+    if isinstance(factor, (tuple, list)):\n+      if len(factor) != 2:\n+        raise ValueError(self._FACTOR_VALIDATION_ERROR + f'Got {factor}')\n+      self._check_factor_range(factor[0])\n+      self._check_factor_range(factor[1])\n+      self._factor = sorted(factor)\n+    elif isinstance(factor, (int, float)):\n+      self._check_factor_range(factor)\n+      factor = abs(factor)\n+      self._factor = [-factor, factor]\n+    else:\n+      raise ValueError(self._FACTOR_VALIDATION_ERROR + f'Got {factor}')\n+\n+  def _check_factor_range(self, input_number):\n+    if input_number > 1.0 or input_number < -1.0:\n+      raise ValueError(self._FACTOR_VALIDATION_ERROR + f'Got {input_number}')\n+\n+  def call(self, inputs, training=True):\n+    if training:\n+      return self._brightness_adjust(inputs)\n+    else:\n+      return inputs\n+\n+  def _brightness_adjust(self, images):\n+    images = utils.ensure_tensor(images, self.compute_dtype)\n+    rank = images.shape.rank\n+    if rank == 3:\n+      rgb_delta_shape = (1, 1, 1)\n+    elif rank == 4:\n+      # Keep only the batch dim. This will ensure to have same adjustment\n+      # with in one image, but different across the images.\n+      rgb_delta_shape = [tf.shape(images)[0], 1, 1, 1]\n+    else:\n+      raise ValueError(\n+          'Expected the input image to be rank 3 or 4. Got '\n+          f'inputs.shape = {images.shape}')\n+    rgb_delta = self._random_generator.random_uniform(\n+        shape=rgb_delta_shape,\n+        minval=self._factor[0],\n+        maxval=self._factor[1],\n+    )\n+    rgb_delta = rgb_delta * (self._value_range[1] - self._value_range[0])\n+    rgb_delta = tf.cast(rgb_delta, images.dtype)\n+    images += rgb_delta\n+    return tf.clip_by_value(\n+        images, self._value_range[0], self._value_range[1])\n+\n+  def get_config(self):\n+    config = {\n+        'factor': self._factor,\n+        'value_range': self._value_range,\n+        'seed': self._seed,\n+    }\n+    base_config = super().get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n @keras_export('keras.layers.RandomHeight',\n               'keras.layers.experimental.preprocessing.RandomHeight',\n               v1=[])\n\n@@ -48,6 +48,7 @@ class ImagePreprocessingDistributionTest(\n           image_preprocessing.Resizing(height=256, width=256),\n           image_preprocessing.RandomCrop(height=224, width=224),\n           image_preprocessing.RandomTranslation(.1, .1),\n+          image_preprocessing.RandomBrightness(.1, value_range=(0, 1)),\n           image_preprocessing.RandomRotation(.2),\n           image_preprocessing.RandomFlip(),\n           image_preprocessing.RandomZoom(.2, .2)])\n\n@@ -701,6 +701,136 @@ class RandomContrastTest(test_combinations.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomBrightnessTest(test_combinations.TestCase):\n+\n+  def test_factor_input_validation(self):\n+    with self.assertRaisesRegex(ValueError, r'in the range \\[-1.0, 1.0\\]'):\n+      image_preprocessing.RandomBrightness(2.0)\n+\n+    with self.assertRaisesRegex(ValueError, 'list of two numbers'):\n+      image_preprocessing.RandomBrightness([1.0])\n+\n+    with self.assertRaisesRegex(ValueError, 'should be a number'):\n+      image_preprocessing.RandomBrightness('one')\n+\n+  def test_factor_normalize(self):\n+    layer = image_preprocessing.RandomBrightness(1.0)\n+    self.assertEqual(layer._factor, [-1.0, 1.0])\n+\n+    layer = image_preprocessing.RandomBrightness((0.5, 0.3))\n+    self.assertEqual(layer._factor, [0.3, 0.5])\n+\n+    layer = image_preprocessing.RandomBrightness(-0.2)\n+    self.assertEqual(layer._factor, [-0.2, 0.2])\n+\n+  @test_utils.run_v2_only\n+  def test_output_value_range(self):\n+    # Always scale up to 255\n+    layer = image_preprocessing.RandomBrightness([1.0, 1.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+    output_min = tf.math.reduce_min(output)\n+    output_max = tf.math.reduce_max(output)\n+    self.assertEqual(output_min, 255)\n+    self.assertEqual(output_max, 255)\n+\n+    # Always scale down to 0\n+    layer = image_preprocessing.RandomBrightness([-1.0, -1.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+    output_min = tf.math.reduce_min(output)\n+    output_max = tf.math.reduce_max(output)\n+    self.assertEqual(output_min, 0)\n+    self.assertEqual(output_max, 0)\n+\n+  def test_output(self):\n+    # Always scale up, but randomly between 0 ~ 255\n+    layer = image_preprocessing.RandomBrightness([0, 1.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+    diff = output - inputs\n+    self.assertGreaterEqual(tf.math.reduce_min(diff), 0)\n+    self.assertGreater(tf.math.reduce_mean(diff), 0)\n+\n+    # Always scale down, but randomly between 0 ~ 255\n+    layer = image_preprocessing.RandomBrightness([-1.0, 0.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+    diff = output - inputs\n+    self.assertLessEqual(tf.math.reduce_max(diff), 0)\n+    self.assertLess(tf.math.reduce_mean(diff), 0)\n+\n+  @test_utils.run_v2_only\n+  def test_scale_output(self):\n+    layer = image_preprocessing.RandomBrightness([0, 1.0], seed=1337)\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+\n+    # Create a new layer with same seed but different value range\n+    layer2 = image_preprocessing.RandomBrightness(\n+        [0, 1.0], value_range=[0, 1], seed=1337)\n+    inputs2 = inputs / 255.0\n+    output2 = layer2(inputs2)\n+    # Make sure the outputs are the same, but just scaled with 255\n+    self.assertAllClose(output, output2 * 255.0)\n+\n+  def test_different_adjustment_within_batch(self):\n+    layer = image_preprocessing.RandomBrightness([0.2, 0.3])\n+    inputs = np.zeros(shape=(2, 10, 10, 3))  # 2 images with all zeros\n+    output = layer(inputs)\n+    diff = output - inputs\n+    # Make sure two images gets the different adjustment\n+    self.assertNotAllClose(diff[0], diff[1])\n+    # Make sure all the pixel are the same with the same image\n+    image1 = output[0]\n+    # The reduced mean pixel value among width and height are the same as\n+    # any of the pixel in the image.\n+    self.assertAllClose(\n+        tf.reduce_mean(image1), image1[0, 0, 0], rtol=1e-5, atol=1e-5)\n+\n+  def test_inference(self):\n+    layer = image_preprocessing.RandomBrightness([0, 1.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs, training=False)\n+    self.assertAllClose(inputs, output)\n+\n+  @test_utils.run_v2_only\n+  def test_dtype(self):\n+    layer = image_preprocessing.RandomBrightness([0, 1.0])\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer(inputs)\n+    self.assertEqual(output.dtype, tf.float32)\n+\n+    layer = image_preprocessing.RandomBrightness([0, 1.0], dtype='uint8')\n+    output = layer(inputs)\n+    self.assertEqual(output.dtype, tf.uint8)\n+\n+  def test_seed(self):\n+    layer = image_preprocessing.RandomBrightness([0, 1.0], seed=1337)\n+    inputs = np.random.randint(0, 255, size=(224, 224, 3))\n+    output_1 = layer(inputs)\n+\n+    layer2 = image_preprocessing.RandomBrightness([0, 1.0], seed=1337)\n+    output_2 = layer2(inputs)\n+\n+    self.assertAllClose(output_1, output_2)\n+\n+  def test_config(self):\n+    layer = image_preprocessing.RandomBrightness(\n+        [0, 1.0], value_range=[0.0, 1.0], seed=1337)\n+    config = layer.get_config()\n+    self.assertEqual(config['factor'], [0.0, 1.0])\n+    self.assertEqual(config['value_range'], [0.0, 1.0])\n+    self.assertEqual(config['seed'], 1337)\n+\n+    reconstructed_layer = image_preprocessing.RandomBrightness.from_config(\n+        config)\n+    self.assertEqual(reconstructed_layer._factor, layer._factor)\n+    self.assertEqual(reconstructed_layer._value_range, layer._value_range)\n+    self.assertEqual(reconstructed_layer._seed, layer._seed)\n+\n+\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class RandomTranslationTest(test_combinations.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
