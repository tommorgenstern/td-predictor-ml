{"custom_id": "keras#37055edc159d9c851262bb657b0f3e1e2254a435", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 73 | Files Changed: 3 | Hunks: 36 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 159 | Churn Cumulative: 923 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,8 +64,8 @@ class _FloatExtractor(object):\n             dot_digits=r\"(?:\\.[0-9]+)\",\n             # digits: \"12\"\n             digits=r\"(?:[0-9]+)\",\n-            # The exponent: An \"e\" or \"E\", optional sign, and at least one digit.\n-            # \"e-123\", \"E+12\", \"e12\"\n+            # The exponent: An \"e\" or \"E\", optional sign, and at least one\n+            # digit.  \"e-123\", \"E+12\", \"e12\"\n             exponent=r\"(?:[eE][-+]?[0-9]+)\",\n         ),\n         re.VERBOSE,\n@@ -84,8 +84,9 @@ class _FloatExtractor(object):\n           string: the string to extract floats from.\n \n         Returns:\n-          A (string, array) pair, where `string` has each float replaced by \"...\"\n-          and `array` is a `float32` `numpy.array` containing the extracted floats.\n+          A (string, array) pair, where `string` has each float replaced by\n+          \"...\" and `array` is a `float32` `numpy.array` containing the\n+          extracted floats.\n         \"\"\"\n         texts = []\n         floats = []\n@@ -128,13 +129,15 @@ class KerasDoctestOutputChecker(doctest.OutputChecker, object):\n     )\n \n     def check_output(self, want, got, optionflags):\n-        \"\"\"Compares the docstring output to the output gotten by running the code.\n+        \"\"\"Compares the docstring output to the output gotten by running the\n+        code.\n \n         Python addresses in the output are replaced with wildcards.\n \n         Float values in the output compared as using `np.allclose`:\n \n-          * Float values are extracted from the text and replaced with wildcards.\n+          * Float values are extracted from the text and replaced with\n+            wildcards.\n           * The wildcard text is compared to the actual output.\n           * The float values are compared using `np.allclose`.\n \n@@ -157,15 +160,16 @@ class KerasDoctestOutputChecker(doctest.OutputChecker, object):\n \n         # If the docstring's output is empty and there is some output generated\n         # after running the snippet, return True. This is because if the user\n-        # doesn't want to display output, respect that over what the doctest wants.\n+        # doesn't want to display output, respect that over what the doctest\n+        # wants.\n         if got and not want:\n             return True\n \n         if want is None:\n             want = \"\"\n \n-        # Replace python's addresses with ellipsis (`...`) since it can change on\n-        # each execution.\n+        # Replace python's addresses with ellipsis (`...`) since it can change\n+        # on each execution.\n         want = self._ADDRESS_RE.sub(\"at ...>\", want)\n \n         # Replace tf.Tensor strings with only their numpy field values.\n@@ -188,8 +192,9 @@ class KerasDoctestOutputChecker(doctest.OutputChecker, object):\n             return False\n \n         if self.want_floats.size == 0:\n-            # If there are no floats in the \"want\" string, ignore all the floats in\n-            # the result. \"np.array([ ... ])\" matches \"np.array([ 1.0, 2.0 ])\"\n+            # If there are no floats in the \"want\" string, ignore all the floats\n+            # in the result. \"np.array([ ... ])\" matches \"np.array([ 1.0, 2.0\n+            # ])\"\n             return True\n \n         self.float_size_good = self.want_floats.size == self.got_floats.size\n@@ -202,10 +207,10 @@ class KerasDoctestOutputChecker(doctest.OutputChecker, object):\n     def output_difference(self, example, got, optionflags):\n         got = [got]\n \n-        # If the some of the float output is hidden with `...`, `float_size_good`\n-        # will be False. This is because the floats extracted from the string is\n-        # converted into a 1-D numpy array. Hence hidding floats is not allowed\n-        # anymore.\n+        # If the some of the float output is hidden with `...`,\n+        # `float_size_good` will be False. This is because the floats extracted\n+        # from the string is converted into a 1-D numpy array. Hence hidding\n+        # floats is not allowed anymore.\n         if self.text_good:\n             if not self.float_size_good:\n                 got.append(\n\n@@ -43,18 +43,18 @@ class TestCase(tf.test.TestCase, parameterized.TestCase):\n def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n     \"\"\"Execute the decorated test with all Keras saved model formats).\n \n-    This decorator is intended to be applied either to individual test methods in\n-    a `test_combinations.TestCase` class, or directly to a test class that\n-    extends it. Doing so will cause the contents of the individual test\n-    method (or all test methods in the class) to be executed multiple times - once\n-    for each Keras saved model format.\n+    This decorator is intended to be applied either to individual test methods\n+    in a `test_combinations.TestCase` class, or directly to a test class that\n+    extends it. Doing so will cause the contents of the individual test method\n+    (or all test methods in the class) to be executed multiple times - once for\n+    each Keras saved model format.\n \n     The Keras saved model formats include:\n     1. HDF5: 'h5'\n     2. SavedModel: 'tf'\n \n-    Note: if stacking this decorator with absl.testing's parameterized decorators,\n-    those should be at the bottom of the stack.\n+    Note: if stacking this decorator with absl.testing's parameterized\n+    decorators, those should be at the bottom of the stack.\n \n     Various methods in `testing_utils` to get file path for saved models will\n     auto-generate a string of the two saved model formats. This allows unittests\n@@ -84,8 +84,8 @@ def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n     This test tries to save the model into the formats of 'hdf5', 'h5', 'keras',\n     'tensorflow', and 'tf'.\n \n-    We can also annotate the whole class if we want this to apply to all tests in\n-    the class:\n+    We can also annotate the whole class if we want this to apply to all tests\n+    in the class:\n     ```python\n     @test_utils.run_with_all_saved_model_formats\n     class MyTests(test_utils.KerasTestCase):\n@@ -115,8 +115,8 @@ def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n         Defaults to None.\n \n     Returns:\n-      Returns a decorator that will run the decorated test method multiple times:\n-      once for each desired Keras saved model format.\n+      Returns a decorator that will run the decorated test method multiple\n+      times: once for each desired Keras saved model format.\n \n     Raises:\n       ImportError: If abseil parameterized is not installed or not included as\n@@ -134,7 +134,8 @@ def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n \n     def single_method_decorator(f):\n         \"\"\"Decorator that constructs the test cases.\"\"\"\n-        # Use named_parameters so it can be individually run from the command line\n+        # Use named_parameters so it can be individually run from the command\n+        # line\n         @parameterized.named_parameters(*params)\n         @functools.wraps(f)\n         def decorated(self, saved_format, *args, **kwargs):\n@@ -180,16 +181,16 @@ def run_with_all_weight_formats(test_or_class=None, exclude_formats=None):\n def run_with_all_model_types(test_or_class=None, exclude_models=None):\n     \"\"\"Execute the decorated test with all Keras model types.\n \n-    This decorator is intended to be applied either to individual test methods in\n-    a `test_combinations.TestCase` class, or directly to a test class that\n-    extends it. Doing so will cause the contents of the individual test\n-    method (or all test methods in the class) to be executed multiple times - once\n-    for each Keras model type.\n+    This decorator is intended to be applied either to individual test methods\n+    in a `test_combinations.TestCase` class, or directly to a test class that\n+    extends it. Doing so will cause the contents of the individual test method\n+    (or all test methods in the class) to be executed multiple times - once for\n+    each Keras model type.\n \n     The Keras model types are: ['functional', 'subclass', 'sequential']\n \n-    Note: if stacking this decorator with absl.testing's parameterized decorators,\n-    those should be at the bottom of the stack.\n+    Note: if stacking this decorator with absl.testing's parameterized\n+    decorators, those should be at the bottom of the stack.\n \n     Various methods in `testing_utils` to get models will auto-generate a model\n     of the currently active Keras model type. This allows unittests to confirm\n@@ -224,8 +225,8 @@ def run_with_all_model_types(test_or_class=None, exclude_models=None):\n     This test tries building a small mlp as both a functional model and as a\n     subclass model.\n \n-    We can also annotate the whole class if we want this to apply to all tests in\n-    the class:\n+    We can also annotate the whole class if we want this to apply to all tests\n+    in the class:\n     ```python\n     @test_utils.run_with_all_model_types(exclude_models = ['sequential'])\n     class MyTests(test_utils.KerasTestCase):\n@@ -260,8 +261,8 @@ def run_with_all_model_types(test_or_class=None, exclude_models=None):\n         Defaults to None.\n \n     Returns:\n-      Returns a decorator that will run the decorated test method multiple times:\n-      once for each desired Keras model type.\n+      Returns a decorator that will run the decorated test method multiple\n+      times: once for each desired Keras model type.\n \n     Raises:\n       ImportError: If abseil parameterized is not installed or not included as\n@@ -276,7 +277,8 @@ def run_with_all_model_types(test_or_class=None, exclude_models=None):\n \n     def single_method_decorator(f):\n         \"\"\"Decorator that constructs the test cases.\"\"\"\n-        # Use named_parameters so it can be individually run from the command line\n+        # Use named_parameters so it can be individually run from the command\n+        # line\n         @parameterized.named_parameters(*params)\n         @functools.wraps(f)\n         def decorated(self, model_type, *args, **kwargs):\n@@ -319,19 +321,19 @@ def run_all_keras_modes(\n ):\n     \"\"\"Execute the decorated test with all keras execution modes.\n \n-    This decorator is intended to be applied either to individual test methods in\n-    a `test_combinations.TestCase` class, or directly to a test class that\n-    extends it. Doing so will cause the contents of the individual test\n-    method (or all test methods in the class) to be executed multiple times -\n-    once executing in legacy graph mode, once running eagerly and with\n+    This decorator is intended to be applied either to individual test methods\n+    in a `test_combinations.TestCase` class, or directly to a test class that\n+    extends it. Doing so will cause the contents of the individual test method\n+    (or all test methods in the class) to be executed multiple times - once\n+    executing in legacy graph mode, once running eagerly and with\n     `should_run_eagerly` returning True, and once running eagerly with\n     `should_run_eagerly` returning False.\n \n     If Tensorflow v2 behavior is enabled, legacy graph mode will be skipped, and\n     the test will only run twice.\n \n-    Note: if stacking this decorator with absl.testing's parameterized decorators,\n-    those should be at the bottom of the stack.\n+    Note: if stacking this decorator with absl.testing's parameterized\n+    decorators, those should be at the bottom of the stack.\n \n     For example, consider the following unittest:\n \n@@ -379,7 +381,8 @@ def run_all_keras_modes(\n        rolled out yet\n \n     Returns:\n-      Returns a decorator that will run the decorated test method multiple times.\n+      Returns a decorator that will run the decorated test method multiple\n+      times.\n \n     Raises:\n       ImportError: If abseil parameterized is not installed or not included as\n@@ -397,7 +400,8 @@ def run_all_keras_modes(\n     def single_method_decorator(f):\n         \"\"\"Decorator that constructs the test cases.\"\"\"\n \n-        # Use named_parameters so it can be individually run from the command line\n+        # Use named_parameters so it can be individually run from the command\n+        # line\n         @parameterized.named_parameters(*params)\n         @functools.wraps(f)\n         def decorated(self, run_mode, *args, **kwargs):\n@@ -447,7 +451,8 @@ def _test_or_class_decorator(test_or_class, single_method_decorator):\n       parameterized decorators w/ each other, and to apply them to test methods\n       that have already been marked with an absl parameterized decorator.\n \n-    Otherwise, treat the obj as a single method and apply the decorator directly.\n+    Otherwise, treat the obj as a single method and apply the decorator\n+    directly.\n \n     Args:\n       test_or_class: A test method (that may have already been decorated with a\n\n@@ -123,8 +123,8 @@ def layer_test(\n         in the layer class. This is helpful for testing custom layers.\n       test_harness: The Tensorflow test, if any, that this function is being\n         called in.\n-      supports_masking: Optional boolean to check the `supports_masking` property\n-        of the layer. If None, the check will not be performed.\n+      supports_masking: Optional boolean to check the `supports_masking`\n+        property of the layer. If None, the check will not be performed.\n \n     Returns:\n       The output data (Numpy array) returned by the layer, for additional\n@@ -213,7 +213,8 @@ def layer_test(\n         )\n \n     def assert_shapes_equal(expected, actual):\n-        \"\"\"Asserts that the output shape from the layer matches the actual shape.\"\"\"\n+        \"\"\"Asserts that the output shape from the layer matches the actual\n+        shape.\"\"\"\n         if len(expected) != len(actual):\n             raise AssertionError(\n                 \"When testing layer %s, for input %s, found output_shape=\"\n@@ -315,7 +316,8 @@ def layer_test(\n                 raise AssertionError(\n                     \"When testing layer %s **after deserialization**, \"\n                     \"for input %s, found output_shape=\"\n-                    \"%s but expected to find inferred shape %s.\\nFull kwargs: %s\"\n+                    \"%s but expected to find inferred shape %s.\\n\"\n+                    \"Full kwargs: %s\"\n                     % (\n                         layer_cls.__name__,\n                         x,\n@@ -377,8 +379,8 @@ def run_eagerly_scope(value):\n     The boolean gets restored to its original value upon exiting the scope.\n \n     Args:\n-       value: Bool specifying if we should run models eagerly in the active test.\n-       Should be True or False.\n+       value: Bool specifying if we should run models eagerly in the active\n+         test. Should be True or False.\n \n     Yields:\n       The provided value.\n@@ -434,8 +436,8 @@ def get_save_format():\n     if _thread_local_data.saved_model_format is None:\n         raise ValueError(\n             \"Cannot call `get_save_format()` outside of a \"\n-            \"`saved_model_format_scope()` or `run_with_all_saved_model_formats` \"\n-            \"decorator.\"\n+            \"`saved_model_format_scope()` or \"\n+            \"`run_with_all_saved_model_formats` decorator.\"\n         )\n     return _thread_local_data.saved_model_format\n \n@@ -444,8 +446,8 @@ def get_save_kwargs():\n     if _thread_local_data.save_kwargs is None:\n         raise ValueError(\n             \"Cannot call `get_save_kwargs()` outside of a \"\n-            \"`saved_model_format_scope()` or `run_with_all_saved_model_formats` \"\n-            \"decorator.\"\n+            \"`saved_model_format_scope()` or \"\n+            \"`run_with_all_saved_model_formats` decorator.\"\n         )\n     return _thread_local_data.save_kwargs or {}\n \n@@ -561,14 +563,14 @@ class _SubclassModel(models.Model):\n         Args:\n           model_layers: a list of layers to be added to the model.\n           *args: Model's args\n-          **kwargs: Model's keyword args, at most one of input_tensor -> the input\n-            tensor required for ragged/sparse input.\n+          **kwargs: Model's keyword args, at most one of input_tensor -> the\n+            input tensor required for ragged/sparse input.\n         \"\"\"\n \n         inputs = kwargs.pop(\"input_tensor\", None)\n         super().__init__(*args, **kwargs)\n-        # Note that clone and build doesn't support lists of layers in subclassed\n-        # models. Adding each layer directly here.\n+        # Note that clone and build doesn't support lists of layers in\n+        # subclassed models. Adding each layer directly here.\n         for i, layer in enumerate(model_layers):\n             setattr(self, self._layer_name_for_i(i), layer)\n \n@@ -801,8 +803,8 @@ def get_multi_io_model(\n \n     To build a two-input, two-output model:\n       Specify a list of layers for branch a and branch b, but do not specify any\n-      shared input branch or shared output branch. The resulting model will apply\n-      each branch to a different input, to produce two outputs.\n+      shared input branch or shared output branch. The resulting model will\n+      apply each branch to a different input, to produce two outputs.\n \n       The first value in branch_a must be the Keras 'Input' layer for branch a,\n       and the first value in branch_b must be the Keras 'Input' layer for\n@@ -862,8 +864,9 @@ def get_multi_io_model(\n       branch_a: A sequence of layers for branch a of the model.\n       branch_b: A sequence of layers for branch b of the model.\n       shared_input_branch: An optional sequence of layers to apply to a single\n-        input, before applying both branches to that intermediate result. If set,\n-        the model will take only one input instead of two. Defaults to None.\n+        input, before applying both branches to that intermediate result. If\n+        set, the model will take only one input instead of two. Defaults to\n+        None.\n       shared_output_branch: An optional sequence of layers to merge the\n         intermediate results produced by branch a and branch b. If set,\n         the model will produce only one output instead of two. Defaults to None.\n@@ -957,9 +960,8 @@ def get_v2_optimizer(name, **kwargs):\n         return _V2_OPTIMIZER_MAP[name](**kwargs)\n     except KeyError:\n         raise ValueError(\n-            \"Could not find requested v2 optimizer: {}\\nValid choices: {}\".format(\n-                name, list(_V2_OPTIMIZER_MAP.keys())\n-            )\n+            \"Could not find requested v2 optimizer: \"\n+            \"{}\\nValid choices: {}\".format(name, list(_V2_OPTIMIZER_MAP.keys()))\n         )\n \n \n@@ -1096,11 +1098,12 @@ def run_v2_only(obj=None):\n \n     Args:\n       obj: function to be annotated. If None, return a\n-        decorator the can be applied to a function or class. If `obj` is not None,\n-        return the decorator applied to `obj`.\n+        decorator the can be applied to a function or class. If `obj` is not\n+        None, return the decorator applied to `obj`.\n \n     Returns:\n-      Returns a decorator that will conditionally skip the decorated test method.\n+      Returns a decorator that will conditionally skip the decorated test\n+      method.\n     \"\"\"\n     condition = not tf.__internal__.tf2.enabled()\n     reason = \"Test is only compatible with TF v2.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e023c3e59ecbb4c84894623a3aa0a418d6f7b7a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 59 | Files Changed: 9 | Hunks: 34 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 139 | Churn Cumulative: 7855 | Contributors (this commit): 11 | Commits (past 90d): 41 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -203,9 +203,10 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n         )\n \n     def testV2SummaryWithKerasSequentialModel(self):\n-        # Histogram summaries require the MLIR bridge; see b/178826597#comment107.\n-        # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove this\n-        #   if histogram summaries are supported fully on non-MLIR bridge or\n+        # Histogram summaries require the MLIR bridge; see\n+        # b/178826597#comment107.\n+        # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove\n+        # this if histogram summaries are supported fully on non-MLIR bridge or\n         # non-MLIR bridge is no longer run.\n         enable_histograms = tf_test_utils.is_mlir_bridge_enabled()\n         strategy = get_tpu_strategy()\n@@ -231,7 +232,8 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n                 os.path.join(self.summary_dir, \"train\", \"event*\")\n             )\n             # Since total of 10 steps are ran and summary ops should be invoked\n-            # every 2 batches, we should see total of 5 event logs for each summary.\n+            # every 2 batches, we should see total of 5 event logs for each\n+            # summary.\n             expected_event_counts = {\n                 \"sequential/layer_for_histogram_summary/custom_histogram_summary_v2\": 5\n                 if enable_histograms\n@@ -243,9 +245,10 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n             )\n \n     def testV2SummaryWithKerasSubclassedModel(self):\n-        # Histogram summaries require the MLIR bridge; see b/178826597#comment107.\n-        # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove this\n-        #   if histogram summaries are supported fully on non-MLIR bridge or\n+        # Histogram summaries require the MLIR bridge; see\n+        # b/178826597#comment107.\n+        # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove\n+        # this if histogram summaries are supported fully on non-MLIR bridge or\n         # non-MLIR bridge is no longer run.\n         enable_histograms = tf_test_utils.is_mlir_bridge_enabled()\n         strategy = get_tpu_strategy()\n@@ -268,7 +271,8 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n                 os.path.join(self.summary_dir, \"train\", \"event*\")\n             )\n             # Since total of 10 steps are ran and summary ops should be invoked\n-            # every 2 batches, we should see total of 5 event logs for each summary.\n+            # every 2 batches, we should see total of 5 event logs for each\n+            # summary.\n             expected_event_counts = {\n                 (\n                     \"custom_model/layer_for_scalar_summary/\"\n\n@@ -65,8 +65,8 @@ class VariablesToConstantsTest(tf.test.TestCase):\n         self.assertEqual(0, self._getNumVariables(constant_graph_def))\n         self.assertFalse(self._hasStatefulPartitionedCallOp(constant_graph_def))\n \n-        # Check that the converted ConcreteFunction produces the same result as the\n-        # original Function.\n+        # Check that the converted ConcreteFunction produces the same result as\n+        # the original Function.\n         expected_value = tf.nest.flatten(func(**input_data))\n         actual_value = tf.nest.flatten(converted_concrete_func(**input_data))\n \n\n@@ -393,9 +393,9 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n     \"\"\"Tests a very simple token classification model.\n \n     The main purpose of this test is to verify that everything works as expected\n-    when input sequences have variable length, and batches are padded only to the\n-    maximum length of each batch. This is very common in NLP, and results in the\n-    sequence dimension varying with each batch step for both the features\n+    when input sequences have variable length, and batches are padded only to\n+    the maximum length of each batch. This is very common in NLP, and results in\n+    the sequence dimension varying with each batch step for both the features\n     and the labels.\n     \"\"\"\n \n\n@@ -101,7 +101,8 @@ class ModelSubclassCompiledTest(test_combinations.TestCase):\n             _ = model.evaluate(dataset, steps=10, verbose=0)\n \n     def test_attributes(self):\n-        # layers, weights, trainable_weights, non_trainable_weights, inputs, outputs\n+        # layers, weights, trainable_weights, non_trainable_weights, inputs,\n+        # outputs\n \n         num_classes = (2, 3)\n         num_samples = 100\n\n@@ -104,7 +104,8 @@ class NestedTestModel2(keras.Model):\n \n def get_nested_model_3(input_dim, num_classes):\n     # A functional-API model with a subclassed model inside.\n-    # NOTE: this requires the inner subclass to implement `compute_output_shape`.\n+    # NOTE: this requires the inner subclass to implement\n+    # `compute_output_shape`.\n \n     inputs = keras.Input(shape=(input_dim,))\n     x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n\n@@ -149,7 +149,8 @@ class TestMetricsCorrectnessMultiIOTemporal(test_combinations.TestCase):\n         #   mae (y1 - y_pred_1) = [[[.4], [.9]], [[.9], [1.4]], [[1.4], [.4]]]\n         #   mae                 = [[2.7/3, 2.7/3]] = [[0.9, 0.9]] = 1.8/2 = 0.9\n         #   mae_2 (y2 - y_pred_2) = [[[.4], [1.4]], [[.9], [.4]], [[1.4], [.9]]]\n-        #   mae_2                 = [[2.7/3, 2.7/3]] = [[0.9, 0.9]] = 1.8/2 = 0.9\n+        #   mae_2                 = [[2.7/3, 2.7/3]] = [[0.9, 0.9]] = 1.8/2 =\n+        #   0.9\n \n         self.expected_fit_result = {\n             \"output_1_mae\": [1, 0.9],\n@@ -185,8 +186,10 @@ class TestMetricsCorrectnessMultiIOTemporal(test_combinations.TestCase):\n         #   mae_2 (sum over bs)   = [[6/3, 1.5/3]] = [[2, .5]] = 2.5/2 = 1.25\n \n         # Epoch 2 - bias = 0.125 (2.5/2 * 0.1)\n-        #   y_pred_1 = [[[0.125], [0.125]], [[1.125], [1.125]], [[2.125], [2.125]]]\n-        #   y_pred_2 = [[[0.125], [0.125]], [[1.125], [1.125]], [[2.125], [2.125]]]\n+        #   y_pred_1 = [[[0.125], [0.125]], [[1.125], [1.125]], [[2.125],\n+        #   [2.125]]]\n+        #   y_pred_2 = [[[0.125], [0.125]], [[1.125], [1.125]], [[2.125],\n+        #   [2.125]]]\n \n         #   mae (y1 - y_pred_1) = [[[.375], [.875]],\n         #                          [[.875], [1.375]],\n@@ -196,7 +199,8 @@ class TestMetricsCorrectnessMultiIOTemporal(test_combinations.TestCase):\n         #                          [[1.375 * .5], [.375 * 2.]]]\n         #   mae (w/o weights)   = [[2.625/3, 2.625/3]] = (.875+.875)/2 = .875\n         #   mae (weighted mean) = [[1.3125/1.5,  5.25/6]] = (.875+.875)/2 = .875\n-        #   mae (sum over bs)   = [[1.3125/3,  5.25/3]] = (0.4375+1.75)/2 = 1.09375\n+        #   mae (sum over bs)   = [[1.3125/3,  5.25/3]] = (0.4375+1.75)/2 =\n+        #   1.09375\n \n         #   mae_2 (y2 - y_pred_2) = [[[.375], [1.375]],\n         #                            [[.875], [.375]],\n@@ -205,8 +209,10 @@ class TestMetricsCorrectnessMultiIOTemporal(test_combinations.TestCase):\n         #                            [[.875 * 2.], [.375 * .5]],\n         #                            [[1.375 * 2.], [.875 * .5]]]\n         #   mae_2 (w/o weights)   = [[2.625/3, 2.625/3]] = (.875+.875)/2 = .875\n-        #   mae_2 (weighted mean) = [[5.25/6, 1.3125/1.5]] = (.875+.875)/2 = .875\n-        #   mae_2 (sum over bs)  = [[5.25/3, 1.3125/3]] = (1.75+0.4375)/2 = 1.09375\n+        #   mae_2 (weighted mean) = [[5.25/6, 1.3125/1.5]] = (.875+.875)/2 =\n+        #   .875\n+        #   mae_2 (sum over bs)  = [[5.25/3, 1.3125/3]] = (1.75+0.4375)/2 =\n+        #   1.09375\n \n         self.expected_fit_result_with_weights = {\n             \"output_1_mae\": [1, 0.875],\n\n@@ -400,9 +400,9 @@ class MappingTests(test_combinations.TestCase):\n         # This update() is super tricky. If the dict wrapper subclasses dict,\n         # CPython will access its storage directly instead of calling any\n         # methods/properties on the object. So the options are either not to\n-        # subclass dict (in which case update will call normal iter methods, but the\n-        # object won't pass isinstance checks) or to subclass dict and keep that\n-        # storage updated (no shadowing all its methods like ListWrapper).\n+        # subclass dict (in which case update will call normal iter methods, but\n+        # the object won't pass isinstance checks) or to subclass dict and keep\n+        # that storage updated (no shadowing all its methods like ListWrapper).\n         new_dict.update(model.d)\n         self.assertEqual({1: 3}, new_dict)\n \n\n@@ -88,8 +88,8 @@ class CheckpointingTests(test_combinations.TestCase):\n     def testNamingWithOptimizer(self):\n         input_value = tf.constant([[3.0]])\n         model = MyModel()\n-        # A nuisance Model using the same optimizer. Its slot variables should not\n-        # go in the checkpoint, since it is never depended on.\n+        # A nuisance Model using the same optimizer. Its slot variables should\n+        # not go in the checkpoint, since it is never depended on.\n         other_model = MyModel()\n         optimizer = adam.Adam(0.001)\n         step = tf.compat.v1.train.get_or_create_global_step()\n@@ -151,7 +151,8 @@ class CheckpointingTests(test_combinations.TestCase):\n         self.assertEqual(\n             len(expected_checkpoint_names), len(named_variables.keys())\n         )\n-        # Check that we've created the right full_names of objects (not exhaustive)\n+        # Check that we've created the right full_names of objects (not\n+        # exhaustive)\n         expected_names = {\n             \"step\" + suffix: \"global_step\",\n             \"model/_second/kernel\" + suffix: \"my_model/dense_1/kernel\",\n@@ -238,7 +239,8 @@ class CheckpointingTests(test_combinations.TestCase):\n             self.assertAllEqual(1, self.evaluate(root_trackable.save_counter))\n             self.assertAllEqual([1.5], self.evaluate(m_bias_slot))\n             if not tf.executing_eagerly():\n-                return  # Restore-on-create is only supported when executing eagerly\n+                # Restore-on-create is only supported when executing eagerly\n+                return\n             on_create_model = MyModel()\n             on_create_optimizer = adam.Adam(0.001)\n             on_create_root = tf.train.Checkpoint(\n@@ -273,7 +275,8 @@ class CheckpointingTests(test_combinations.TestCase):\n             status.assert_consumed()\n             self.assertAllEqual(\n                 optimizer_variables,\n-                # Creation order is different, so .variables() needs to be re-sorted.\n+                # Creation order is different, so .variables() needs to be\n+                # re-sorted.\n                 self.evaluate(\n                     sorted(optimizer.variables(), key=lambda v: v.name)\n                 ),\n@@ -362,7 +365,8 @@ class CheckpointingTests(test_combinations.TestCase):\n     )\n     def testAgnosticUsage(self):\n         \"\"\"Graph/eager agnostic usage.\"\"\"\n-        # Does create garbage when executing eagerly due to ops.Graph() creation.\n+        # Does create garbage when executing eagerly due to ops.Graph()\n+        # creation.\n         with self.test_session():\n             num_training_steps = 10\n             checkpoint_directory = self.get_temp_dir()\n@@ -525,8 +529,8 @@ class CheckpointingTests(test_combinations.TestCase):\n             gradients = [1.0]\n             train_op = optimizer.apply_gradients(zip(gradients, variables))\n             # Note that `optimizer` has not been added as a dependency of\n-            # `root`. Create a one-off grouping so that slot variables for `root.var`\n-            # get initialized too.\n+            # `root`. Create a one-off grouping so that slot variables for\n+            # `root.var` get initialized too.\n             self.evaluate(\n                 trackable_utils.gather_initializers(\n                     tf.train.Checkpoint(root=root, optimizer=optimizer)\n@@ -569,8 +573,8 @@ class CheckpointingTests(test_combinations.TestCase):\n                     slot_status.assert_consumed()\n             self.assertEqual(12.0, self.evaluate(new_root.var))\n             if tf.executing_eagerly():\n-                # Slot variables are only created with restoring initializers when\n-                # executing eagerly.\n+                # Slot variables are only created with restoring initializers\n+                # when executing eagerly.\n                 self.assertEqual(\n                     14.0,\n                     self.evaluate(\n@@ -588,13 +592,13 @@ class CheckpointingTests(test_combinations.TestCase):\n             train_op = new_root.optimizer.apply_gradients(\n                 zip(gradients, variables)\n             )\n-            # The slot variable now exists; restore() didn't create it, but we should\n-            # now have a restore op for it.\n+            # The slot variable now exists; restore() didn't create it, but we\n+            # should now have a restore op for it.\n             slot_status.run_restore_ops()\n             if not tf.executing_eagerly():\n-                # The train op hasn't run when graph building, so the slot variable has\n-                # its restored value. It has run in eager, so the value will\n-                # be different.\n+                # The train op hasn't run when graph building, so the slot\n+                # variable has its restored value. It has run in eager, so the\n+                # value will be different.\n                 self.assertEqual(\n                     14.0,\n                     self.evaluate(\n@@ -716,8 +720,8 @@ class CheckpointingTests(test_combinations.TestCase):\n                 if not tf.executing_eagerly():\n                     train_fn = functools.partial(self.evaluate, train_fn())\n                 status.initialize_or_restore()\n-                # TODO(tanzheny): Add hyper variables to .variables(), and set them with\n-                # set_weights etc.\n+                # TODO(tanzheny): Add hyper variables to .variables(), and set\n+                # them with set_weights etc.\n                 variables_not_in_the_variables_property = [\n                     obj\n                     for obj in optimizer._hyper.values()\n@@ -960,8 +964,8 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n                     status.assert_existing_objects_matched()\n                     status.assert_nontrivial_match()\n                 else:\n-                    # When graph building, we haven't read any keys, so we don't know\n-                    # whether the restore will be complete.\n+                    # When graph building, we haven't read any keys, so we don't\n+                    # know whether the restore will be complete.\n                     with self.assertRaisesRegex(AssertionError, \"not restored\"):\n                         status.assert_consumed()\n                     with self.assertRaisesRegex(AssertionError, \"not restored\"):\n@@ -975,8 +979,8 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n                 status.initialize_or_restore()\n                 status.assert_nontrivial_match()\n                 self._check_sentinels(root)\n-                # Check that there is no error when keys are missing from the name-based\n-                # checkpoint.\n+                # Check that there is no error when keys are missing from the\n+                # name-based checkpoint.\n                 root.not_in_name_checkpoint = tf.Variable([1.0])\n                 status = object_saver.read(save_path)\n                 with self.assertRaises(AssertionError):\n@@ -1014,8 +1018,8 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n         checkpoint_directory = self.get_temp_dir()\n         checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n         with self.cached_session() as session:\n-            # Create and save a model using Saver() before using a Checkpoint. This\n-            # generates a snapshot without the Checkpoint's `save_counter`.\n+            # Create and save a model using Saver() before using a Checkpoint.\n+            # This generates a snapshot without the Checkpoint's `save_counter`.\n             model = sequential.Sequential()\n             model.add(reshaping.Flatten(input_shape=(1,)))\n             model.add(core.Dense(1))\n\n@@ -61,8 +61,8 @@ class CheckpointingTests(test_combinations.TestCase):\n     def testNamingWithOptimizer(self):\n         input_value = tf.constant([[3.0]])\n         model = MyModel()\n-        # A nuisance Model using the same optimizer. Its slot variables should not\n-        # go in the checkpoint, since it is never depended on.\n+        # A nuisance Model using the same optimizer. Its slot variables should\n+        # not go in the checkpoint, since it is never depended on.\n         other_model = MyModel()\n         optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n         optimizer_step = tf.compat.v1.train.get_or_create_global_step()\n@@ -119,7 +119,8 @@ class CheckpointingTests(test_combinations.TestCase):\n         self.assertEqual(\n             len(expected_checkpoint_names), len(named_variables.keys())\n         )\n-        # Check that we've created the right full_names of objects (not exhaustive)\n+        # Check that we've created the right full_names of objects (not\n+        # exhaustive)\n         expected_names = {\n             \"optimizer_step\" + suffix: \"global_step\",\n             \"model/_second/kernel\" + suffix: \"my_model/dense_1/kernel\",\n@@ -205,8 +206,9 @@ class CheckpointingTests(test_combinations.TestCase):\n                 optimizer.minimize(lambda: model(input_value))\n             else:\n                 train_op = optimizer.minimize(model(input_value))\n-                # TODO(allenl): Make initialization more pleasant when graph building.\n-                root_trackable.save_counter  # pylint: disable=pointless-statement\n+                # TODO(allenl): Make initialization more pleasant when graph\n+                # building.\n+                root_trackable.save_counter\n                 self.evaluate(\n                     trackable_utils.gather_initializers(root_trackable)\n                 )\n@@ -237,7 +239,8 @@ class CheckpointingTests(test_combinations.TestCase):\n             self.assertAllEqual(1, self.evaluate(root_trackable.save_counter))\n             self.assertAllEqual([1.5], self.evaluate(m_bias_slot))\n             if not tf.executing_eagerly():\n-                return  # Restore-on-create is only supported when executing eagerly\n+                # Restore-on-create is only supported when executing eagerly\n+                return\n             on_create_model = MyModel()\n             on_create_optimizer = tf.compat.v1.train.AdamOptimizer(\n                 0.001,\n@@ -449,7 +452,8 @@ class CheckpointingTests(test_combinations.TestCase):\n     )\n     def testAgnosticUsage(self):\n         \"\"\"Graph/eager agnostic usage.\"\"\"\n-        # Does create garbage when executing eagerly due to ops.Graph() creation.\n+        # Does create garbage when executing eagerly due to ops.Graph()\n+        # creation.\n         with self.test_session():\n             num_training_steps = 10\n             checkpoint_directory = self.get_temp_dir()\n@@ -586,7 +590,8 @@ class CheckpointingTests(test_combinations.TestCase):\n                 model = MyModel()\n                 optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n                 root = tf.train.Checkpoint(\n-                    model=model,  # Do not save the optimizer with the checkpoint.\n+                    # Do not save the optimizer with the checkpoint.\n+                    model=model,\n                     global_step=tf.compat.v1.train.get_or_create_global_step(),\n                 )\n                 optimizer_checkpoint = tf.train.Checkpoint(optimizer=optimizer)\n@@ -756,8 +761,8 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n                     status.assert_existing_objects_matched()\n                     status.assert_nontrivial_match()\n                 else:\n-                    # When graph building, we haven't read any keys, so we don't know\n-                    # whether the restore will be complete.\n+                    # When graph building, we haven't read any keys, so we don't\n+                    # know whether the restore will be complete.\n                     with self.assertRaisesRegex(AssertionError, \"not restored\"):\n                         status.assert_consumed()\n                     with self.assertRaisesRegex(AssertionError, \"not restored\"):\n@@ -770,8 +775,8 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n                 status = object_saver.read(save_path)\n                 status.initialize_or_restore()\n                 self._check_sentinels(root)\n-                # Check that there is no error when keys are missing from the name-based\n-                # checkpoint.\n+                # Check that there is no error when keys are missing from the\n+                # name-based checkpoint.\n                 root.not_in_name_checkpoint = tf.Variable([1.0])\n                 status = object_saver.read(save_path)\n                 with self.assertRaises(AssertionError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80ee2fa4e1db2dda14370110830db82be3eb97b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 609 | Lines Deleted: 512 | Files Changed: 32 | Hunks: 219 | Methods Changed: 123 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1121 | Churn Cumulative: 31717 | Contributors (this commit): 128 | Commits (past 90d): 198 | Contributors (cumulative): 346 | DMM Complexity: 0.08333333333333333\n\nDIFF:\n@@ -96,7 +96,8 @@ def verify_python_files_in_pip(pip_root, bazel_root):\n         python_files = set(fnmatch.filter(files, \"*.py\"))\n         python_test_files = set(fnmatch.filter(files, \"*test.py\"))\n         python_benchmark_files = set(fnmatch.filter(files, \"*benchmark.py\"))\n-        # We only care about python files in the pip package, see create_init_files.\n+        # We only care about python files in the pip package, see\n+        # create_init_files.\n         files = python_files - python_test_files - python_benchmark_files\n         for f in files:\n             pip_path = os.path.join(\n@@ -108,8 +109,9 @@ def verify_python_files_in_pip(pip_root, bazel_root):\n             if not path_exists and not file_excluded:\n                 raise PipPackagingError(\n                     (\n-                        \"Pip package missing the file %s. If this is expected, add it \"\n-                        \"to PIP_EXCLUDED_FILES in create_pip_helper.py. Otherwise, \"\n+                        \"Pip package missing the file %s. If this is expected, \"\n+                        \"add it to PIP_EXCLUDED_FILES in \"\n+                        \"create_pip_helper.py. Otherwise, \"\n                         \"make sure it is a build dependency of the pip package\"\n                     )\n                     % file_name\n\n@@ -129,10 +129,11 @@ def audio_dataset_from_directory(\n     if labels not in (\"inferred\", None):\n         if not isinstance(labels, (list, tuple)):\n             raise ValueError(\n-                \"The `labels` argument should be a list/tuple of integer labels, of \"\n-                \"the same size as the number of audio files in the target \"\n-                \"directory. If you wish to infer the labels from the subdirectory \"\n-                'names in the target directory, pass `labels=\"inferred\"`. '\n+                \"The `labels` argument should be a list/tuple of integer \"\n+                \"labels, of the same size as the number of audio files in \"\n+                \"the target directory. If you wish to infer the labels from \"\n+                \"the subdirectory names in the target directory,\"\n+                ' pass `labels=\"inferred\"`. '\n                 \"If you wish to get a dataset that only contains audio samples \"\n                 f\"(no labels), pass `labels=None`. Received: labels={labels}\"\n             )\n@@ -144,7 +145,8 @@ def audio_dataset_from_directory(\n             )\n     if label_mode not in {\"int\", \"categorical\", \"binary\", None}:\n         raise ValueError(\n-            '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n+            '`label_mode` argument must be one of \"int\", \"categorical\", '\n+            '\"binary\", '\n             f\"or None. Received: label_mode={label_mode}\"\n         )\n \n@@ -169,7 +171,8 @@ def audio_dataset_from_directory(\n         if tfio is None:\n             raise ImportError(\n                 \"To use the argument `sampling_rate`, you should install \"\n-                \"tensorflow_io. You can install it via `pip install tensorflow-io`.\"\n+                \"tensorflow_io. You can install it via `pip install \"\n+                \"tensorflow-io`.\"\n             )\n \n     if labels is None or label_mode is None:\n\n@@ -89,8 +89,8 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         return temp_dir\n \n     def test_audio_dataset_from_directory_standalone(self):\n-        # Test retrieving audio samples withouts labels from a directory and its subdirs.\n-\n+        # Test retrieving audio samples withouts labels from a directory and its\n+        # subdirs.\n         # Save a few extra audio in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i, audio in enumerate(self._get_audio_samples(3)):\n@@ -292,14 +292,15 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n     def test_audio_dataset_from_directory_no_output_sequence_length_no_ragged(\n         self,\n     ):\n-        # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n-        # are not passed while the input sequence lengths are different.\n+        # This test case tests `audio_dataset_from_directory` when `ragged` and\n+        # `output_sequence_length` are not passed while the input sequence\n+        # lengths are different.\n         directory = self._prepare_directory(\n             num_classes=2, count=16, different_sequence_lengths=True\n         )\n         # The tensor shapes are different and output_sequence_length is None\n-        # should work fine and pad each sequence to the length of the longest sequence\n-        # in it's batch\n+        # should work fine and pad each sequence to the length of the longest\n+        # sequence in it's batch\n         min_sequence_length, max_sequence_length = 10, 30\n         possible_sequence_lengths = [\n             i for i in range(min_sequence_length, max_sequence_length + 1)\n@@ -314,14 +315,15 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n     def test_audio_dataset_from_directory_no_output_sequence_length_same_lengths(\n         self,\n     ):\n-        # This test case tests `audio_dataset_from_directory` when `ragged` and `output_sequence_length`\n-        # are not passed while the input sequence lengths are the same\n+        # This test case tests `audio_dataset_from_directory` when `ragged` and\n+        # `output_sequence_length` are not passed while the input sequence\n+        # lengths are the same\n         directory = self._prepare_directory(\n             num_classes=2, count=16, different_sequence_lengths=False\n         )\n         # The tensor shapes are different and output_sequence_length is None\n-        # should work fine and pad each sequence to the length of the longest sequence\n-        # in it's batch\n+        # should work fine and pad each sequence to the length of the longest\n+        # sequence in it's batch\n         dataset = audio_dataset.audio_dataset_from_directory(\n             directory, batch_size=2\n         )\n\n@@ -54,7 +54,8 @@ class ToDense(Layer):\n         else:\n             raise TypeError(\"Unexpected tensor type %s\" % type(inputs).__name__)\n \n-        # Return a float so that we can compile models with this as the final layer.\n+        # Return a float so that we can compile models with this as the final\n+        # layer.\n         return tf.cast(output, tf.float32)\n \n \n@@ -87,8 +88,8 @@ class _SubclassModel(keras.Model):\n \n     def __init__(self, layers, i_layer=None):\n         super().__init__()\n-        # Note that clone and build doesn't support lists of layers in subclassed\n-        # models. Adding each layer directly here.\n+        # Note that clone and build doesn't support lists of layers in\n+        # subclassed models. Adding each layer directly here.\n         for i, layer in enumerate(layers):\n             setattr(self, self._layer_name_for_i(i), layer)\n         self.num_layers = len(layers)\n@@ -181,9 +182,9 @@ class CompositeTensorInternalTest(test_combinations.TestCase):\n \n     def test_training_internal_ragged_tensors(self):\n         # Create a model that implements y=Mx. This is easy to learn and will\n-        # demonstrate appropriate gradient passing. (We have to use RaggedTensors\n-        # for this test, as ToSparse() doesn't support gradient propagation through\n-        # the layer.) TODO(b/124796939): Investigate this.\n+        # demonstrate appropriate gradient passing. (We have to use\n+        # RaggedTensors for this test, as ToSparse() doesn't support gradient\n+        # propagation through the layer.) TODO(b/124796939): Investigate this.\n         layers = [core.Dense(2), ToRagged(padding=0), ToDense(default_value=-1)]\n         model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n \n@@ -195,8 +196,8 @@ class CompositeTensorInternalTest(test_combinations.TestCase):\n         model.compile(loss=\"mse\", optimizer=\"adam\", **get_test_mode_kwargs())\n         history = model.fit(input_data, expected_data, epochs=10, verbose=0)\n \n-        # If the model trained, the loss stored at history[0] should be different\n-        # than the one stored at history[-1].\n+        # If the model trained, the loss stored at history[0] should be\n+        # different than the one stored at history[-1].\n         self.assertNotEqual(\n             history.history[\"loss\"][-1], history.history[\"loss\"][0]\n         )\n@@ -371,7 +372,8 @@ class SparseTensorInputTest(test_combinations.TestCase):\n                 result = model.evaluate(input_data, expected_output, **kwargs)\n                 self.assertAllEqual(1.0, result[-1])\n             if action == \"fit\":\n-                # TODO(momernick): What's the best way of validating that fit happened?\n+                # TODO(momernick): What's the best way of validating that fit\n+                # happened?\n                 _ = model.fit(\n                     input_data, expected_output, shuffle=False, **kwargs\n                 )\n@@ -381,9 +383,10 @@ class SparseTensorInputTest(test_combinations.TestCase):\n @test_combinations.run_all_keras_modes\n class ScipySparseTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n     def test_sparse_scipy_predict_inputs_via_input_layer_args(self):\n-        # Create a model that accepts a sparse input and converts the sparse tensor\n-        # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n-        # a one-dimensional shape; note also that scipy's default dtype is int64.\n+        # Create a model that accepts a sparse input and converts the sparse\n+        # tensor back to a dense tensor. Scipy sparse matrices are limited to\n+        # 2D, so use a one-dimensional shape; note also that scipy's default\n+        # dtype is int64.\n         model_input = input_layer.Input(shape=(3,), sparse=True, dtype=tf.int64)\n         layers = [ToDense(default_value=-1)]\n         model = get_model_from_layers_with_input(\n@@ -405,9 +408,10 @@ class ScipySparseTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n         self.assertAllEqual(expected_output_2, output_2)\n \n     def test_sparse_scipy_eval_inputs(self):\n-        # Create a model that accepts a sparse input and converts the sparse tensor\n-        # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n-        # a one-dimensional shape; note also that scipy's default dtype is int64.\n+        # Create a model that accepts a sparse input and converts the sparse\n+        # tensor back to a dense tensor. Scipy sparse matrices are limited to\n+        # 2D, so use a one-dimensional shape; note also that scipy's default\n+        # dtype is int64.\n         model_input = input_layer.Input(shape=(3,), sparse=True, dtype=tf.int64)\n         layers = [ToDense(default_value=-1)]\n         model = get_model_from_layers_with_input(\n@@ -431,9 +435,10 @@ class ScipySparseTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n         self.assertAllEqual(1.0, output_2[-1])\n \n     def test_sparse_scipy_predict_input_dicts_via_input_layer_args(self):\n-        # Create a model that accepts a sparse input and converts the sparse tensor\n-        # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n-        # a one-dimensional shape; note also that scipy's default dtype is int64.\n+        # Create a model that accepts a sparse input and converts the sparse\n+        # tensor back to a dense tensor. Scipy sparse matrices are limited to\n+        # 2D, so use a one-dimensional shape; note also that scipy's default\n+        # dtype is int64.\n         if test_utils.get_model_type() == \"subclass\":\n             input_name = \"input_1\"  # Subclass models don\"t support input names.\n         else:\n@@ -465,9 +470,10 @@ class ScipySparseTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n         self.assertAllEqual(expected_output_2, output_2)\n \n     def test_sparse_scipy_eval_input_dicts(self):\n-        # Create a model that accepts a sparse input and converts the sparse tensor\n-        # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n-        # a one-dimensional shape; note also that scipy's default dtype is int64.\n+        # Create a model that accepts a sparse input and converts the sparse\n+        # tensor back to a dense tensor. Scipy sparse matrices are limited to\n+        # 2D, so use a one-dimensional shape; note also that scipy's default\n+        # dtype is int64.\n         if test_utils.get_model_type() == \"subclass\":\n             input_name = \"input_1\"  # Subclass models don\"t support input names.\n         else:\n@@ -553,7 +559,8 @@ class RaggedTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n                 result = model.evaluate(input_data, expected_output)\n                 self.assertAllEqual(1.0, result[-1])\n             if action == \"fit\":\n-                # TODO(momernick): What's the best way of validating that fit happened?\n+                # TODO(momernick): What's the best way of validating that fit\n+                # happened?\n                 _ = model.fit(input_data, expected_output, shuffle=False)\n \n \n@@ -656,7 +663,8 @@ class CompositeTensorModelPredictTest(test_combinations.TestCase):\n         return shape\n \n     def test_sparse_tensor_model_predict(self):\n-        # Create a model that accepts a sparse input and runs a \"Dense\" layer on it.\n+        # Create a model that accepts a sparse input and runs a \"Dense\" layer on\n+        # it.\n         model_input = input_layer.Input(\n             shape=(3,), sparse=True, dtype=tf.float32\n         )\n@@ -682,7 +690,8 @@ class CompositeTensorModelPredictTest(test_combinations.TestCase):\n         self.assertEqual((6, 2), self._normalize_shape(shape))\n \n     def test_ragged_tensor_model_predict(self):\n-        # Create a model that accepts a sparse input and runs a \"Dense\" layer on it.\n+        # Create a model that accepts a sparse input and runs a \"Dense\" layer on\n+        # it.\n         model_input = input_layer.Input(shape=(None,), ragged=True)\n         self.assertEqual([None, None], model_input.shape.as_list())\n \n\n@@ -52,9 +52,10 @@ def GetContainingWhileContext(ctxt, stop_ctxt=None):\n         if it sees stop_ctxt.\n \n     Returns:\n-      `ctxt` if `ctxt` is a WhileContext, the most nested WhileContext containing\n-      `ctxt`, or None if `ctxt` is not in a while loop.  If `stop_ctxt` is not\n-      `None`, this returns `ctxt` if it matches `stop_ctxt` in its traversal.\n+      `ctxt` if `ctxt` is a WhileContext, the most nested WhileContext\n+      containing `ctxt`, or None if `ctxt` is not in a while loop.  If\n+      `stop_ctxt` is not `None`, this returns `ctxt` if it matches `stop_ctxt`\n+      in its traversal.\n     \"\"\"\n     while ctxt:\n         if ctxt.IsWhileContext() or ctxt == stop_ctxt:\n\n@@ -32,7 +32,8 @@ def convert_data_format(data_format, ndim):\n             return \"NDHWC\"\n         else:\n             raise ValueError(\n-                f\"Input rank not supported: {ndim}. Expected values are [3, 4, 5]\"\n+                f\"Input rank not supported: {ndim}. \"\n+                \"Expected values are [3, 4, 5]\"\n             )\n     elif data_format == \"channels_first\":\n         if ndim == 3:\n@@ -43,7 +44,8 @@ def convert_data_format(data_format, ndim):\n             return \"NCDHW\"\n         else:\n             raise ValueError(\n-                f\"Input rank not supported: {ndim}. Expected values are [3, 4, 5]\"\n+                f\"Input rank not supported: {ndim}. \"\n+                \"Expected values are [3, 4, 5]\"\n             )\n     else:\n         raise ValueError(\n@@ -177,8 +179,8 @@ def deconv_output_length(\n         input_length: Integer.\n         filter_size: Integer.\n         padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n-        output_padding: Integer, amount of padding along the output dimension. Can\n-          be set to `None` in which case the output length is inferred.\n+        output_padding: Integer, amount of padding along the output dimension.\n+          Can be set to `None` in which case the output length is inferred.\n         stride: Integer.\n         dilation: Integer.\n \n@@ -245,9 +247,10 @@ def conv_kernel_mask(input_shape, kernel_shape, strides, padding):\n \n     Assume a convolution with given parameters is applied to an input having N\n     spatial dimensions with `input_shape = (d_in1, ..., d_inN)` to produce an\n-    output with shape `(d_out1, ..., d_outN)`. This method returns a boolean array\n-    of shape `(d_in1, ..., d_inN, d_out1, ..., d_outN)` with `True` entries\n-    indicating pairs of input and output locations that are connected by a weight.\n+    output with shape `(d_out1, ..., d_outN)`. This method returns a boolean\n+    array of shape `(d_in1, ..., d_inN, d_out1, ..., d_outN)` with `True`\n+    entries indicating pairs of input and output locations that are connected by\n+    a weight.\n \n     Example:\n \n@@ -369,15 +372,15 @@ def conv_kernel_idxs(\n       data_format: string, \"channels_first\" or \"channels_last\".\n \n     Yields:\n-      The next tuple `(output_idx, input_idx)`, where\n-      `output_idx` is an integer index in a flattened tensor representing a single\n-      output image of a convolutional layer that is connected (via the layer\n-      weights) to the respective single input image at `input_idx`.\n+      The next tuple `(output_idx, input_idx)`, where `output_idx` is an integer\n+      index in a flattened tensor representing a single output image of a\n+      convolutional layer that is connected (via the layer weights) to the\n+      respective single input image at `input_idx`.\n \n     Raises:\n-        ValueError: if `data_format` is neither\n-        `\"channels_last\"` nor `\"channels_first\"`, or if number of strides, input,\n-        and kernel number of dimensions do not match.\n+        ValueError: if `data_format` is neither `\"channels_last\"` nor\n+          `\"channels_first\"`, or if number of strides, input, and kernel number\n+          of dimensions do not match.\n \n         NotImplementedError: if `padding` is neither `\"same\"` nor `\"valid\"`.\n     \"\"\"\n@@ -466,8 +469,8 @@ def conv_connected_inputs(\n         input.\n       kernel_shape: tuple of size N, spatial shape of the convolutional kernel /\n         receptive field.\n-      output_position: tuple of size N: `(p_out1, ..., p_outN)`, a single position\n-        in the output of the convolution.\n+      output_position: tuple of size N: `(p_out1, ..., p_outN)`, a single\n+        position in the output of the convolution.\n       strides: tuple of size N, strides along each spatial dimension.\n       padding: type of padding, string `\"same\"` or `\"valid\"`.\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n\n@@ -44,9 +44,7 @@ from keras.utils import tf_inspect\n from keras.utils.generic_utils import Progbar\n \n # Required to support google internal urlretrieve\n-if (\n-    True\n-):  # This gets transformed to `if sys.version_info[0] == 2:` in OSS.  # pylint: disable=using-constant-test\n+if True:  # This gets transformed to `if sys.version_info[0] == 2:` in OSS.\n \n     def urlretrieve(url, filename, reporthook=None, data=None):\n         \"\"\"Replacement for `urlretrieve` for Python 2.\n@@ -57,10 +55,11 @@ if (\n         Args:\n             url: url to retrieve.\n             filename: where to store the retrieved data locally.\n-            reporthook: a hook function that will be called once on establishment of\n-              the network connection and once after each block read thereafter. The\n-              hook will be passed three arguments; a count of blocks transferred so\n-              far, a block size in bytes, and the total size of the file.\n+            reporthook: a hook function that will be called once on\n+              establishment of the network connection and once after each block\n+              read thereafter. The hook will be passed three arguments; a count\n+              of blocks transferred so far, a block size in bytes, and the total\n+              size of the file.\n             data: `data` argument passed to `urlopen`.\n         \"\"\"\n \n@@ -234,7 +233,8 @@ def get_file(\n         fname = os.path.basename(urlsplit(origin).path)\n         if not fname:\n             raise ValueError(\n-                f\"Can't parse the file name from the origin provided: '{origin}'.\"\n+                \"Can't parse the file name from the origin provided: \"\n+                f\"'{origin}'.\"\n                 \"Please specify the `fname` as the input param.\"\n             )\n \n@@ -258,7 +258,8 @@ def get_file(\n                 io_utils.print_msg(\n                     \"A local file was found, but it seems to be \"\n                     f\"incomplete or outdated because the {hash_algorithm} \"\n-                    f\"file hash does not match the original value of {file_hash} \"\n+                    f\"file hash does not match the original value of \"\n+                    f\"{file_hash} \"\n                     \"so we will re-download the data.\"\n                 )\n                 download = True\n@@ -301,13 +302,15 @@ def get_file(\n             raise\n \n         # Validate download if succeeded and user provided an expected hash\n-        # Security conscious users would get the hash of the file from a separate\n-        # channel and pass it to this API to prevent MITM / corruption:\n+        # Security conscious users would get the hash of the file from a\n+        # separate channel and pass it to this API to prevent MITM / corruption:\n         if os.path.exists(fpath) and file_hash is not None:\n             if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n                 raise ValueError(\n-                    f\"Incomplete or corrupted file detected. The {hash_algorithm} \"\n-                    f\"file hash does not match the provided value of {file_hash}.\"\n+                    f\"Incomplete or corrupted file detected. \"\n+                    f\"The {hash_algorithm} \"\n+                    f\"file hash does not match the provided value \"\n+                    f\"of {file_hash}.\"\n                 )\n \n     if untar:\n@@ -399,13 +402,14 @@ class ThreadsafeIter:\n         self.it = it\n         self.lock = threading.Lock()\n \n-        # After a generator throws an exception all subsequent next() calls raise a\n-        # StopIteration Exception. This, however, presents an issue when mixing\n-        # generators and threading because it means the order of retrieval need not\n-        # match the order in which the generator was called. This can make it appear\n-        # that a generator exited normally when in fact the terminating exception is\n-        # just in a different thread. In order to provide thread safety, once\n-        # self.it has thrown an exception we continue to throw the same exception.\n+        # After a generator throws an exception all subsequent next() calls\n+        # raise a StopIteration Exception. This, however, presents an issue when\n+        # mixing generators and threading because it means the order of\n+        # retrieval need not match the order in which the generator was called.\n+        # This can make it appear that a generator exited normally when in fact\n+        # the terminating exception is just in a different thread. In order to\n+        # provide thread safety, once self.it has thrown an exception we\n+        # continue to throw the same exception.\n         self._exception = None\n \n     def __iter__(self):\n@@ -830,8 +834,8 @@ def init_pool_generator(gens, random_seed=None, id_queue=None):\n \n     worker_proc = multiprocessing.current_process()\n \n-    # name isn't used for anything, but setting a more descriptive name is helpful\n-    # when diagnosing orphaned processes.\n+    # name isn't used for anything, but setting a more descriptive name is\n+    # helpful when diagnosing orphaned processes.\n     worker_proc.name = \"Keras_worker_{}\".format(worker_proc.name)\n \n     if random_seed is not None:\n\n@@ -23,10 +23,10 @@ from tensorflow.python.util.tf_export import keras_export\n class DatasetCreator:\n     \"\"\"Object that returns a `tf.data.Dataset` upon invoking.\n \n-    `tf.keras.utils.experimental.DatasetCreator` is designated as a supported type\n-    for `x`, or the input, in `tf.keras.Model.fit`. Pass an instance of this class\n-    to `fit` when using a callable (with a `input_context` argument) that returns\n-    a `tf.data.Dataset`.\n+    `tf.keras.utils.experimental.DatasetCreator` is designated as a supported\n+    type for `x`, or the input, in `tf.keras.Model.fit`. Pass an instance of\n+    this class to `fit` when using a callable (with a `input_context` argument)\n+    that returns a `tf.data.Dataset`.\n \n     ```python\n     model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n@@ -73,10 +73,10 @@ class DatasetCreator:\n \n     Args:\n       dataset_fn: A callable that takes a single argument of type\n-        `tf.distribute.InputContext`, which is used for batch size calculation and\n-        cross-worker input pipeline sharding (if neither is needed, the\n-        `InputContext` parameter can be ignored in the `dataset_fn`), and returns\n-        a `tf.data.Dataset`.\n+        `tf.distribute.InputContext`, which is used for batch size calculation\n+        and cross-worker input pipeline sharding (if neither is needed, the\n+        `InputContext` parameter can be ignored in the `dataset_fn`), and\n+        returns a `tf.data.Dataset`.\n       input_options: Optional `tf.distribute.InputOptions`, used for specific\n         options when used with distribution, for example, whether to prefetch\n         dataset elements to accelerator device memory or host device memory, and\n@@ -103,8 +103,8 @@ class DatasetCreator:\n         self.input_options = input_options\n \n     def __call__(self, *args, **kwargs):\n-        # When a `DatasetCreator` is invoked, it forwards args/kwargs straight to\n-        # the callable.\n+        # When a `DatasetCreator` is invoked, it forwards args/kwargs straight\n+        # to the callable.\n         dataset = self.dataset_fn(*args, **kwargs)\n         if not isinstance(dataset, tf.data.Dataset):\n             raise TypeError(\n\n@@ -38,12 +38,12 @@ def split_dataset(\n           same length.\n         left_size: If float, it should be in range `[0, 1]` range and signifies\n           the fraction of the data to pack in the left dataset. If integer, it\n-          signifies the number of samples to pack in the left dataset. If `None`,\n-          it defaults to the complement to `right_size`.\n+          signifies the number of samples to pack in the left dataset. If\n+          `None`, it defaults to the complement to `right_size`.\n         right_size: If float, it should be in range `[0, 1]` range and signifies\n           the fraction of the data to pack in the right dataset. If integer, it\n-          signifies the number of samples to pack in the right dataset. If `None`,\n-          it defaults to the complement to `left_size`.\n+          signifies the number of samples to pack in the right dataset. If\n+          `None`, it defaults to the complement to `left_size`.\n         shuffle: Boolean, whether to shuffle the data before splitting it.\n         seed: A random seed for shuffling.\n \n@@ -116,9 +116,9 @@ def _convert_dataset_to_list(\n     Args:\n         dataset : A `tf.data.Dataset` object or a list/tuple of arrays.\n         dataset_type_spec : the type of the dataset\n-        data_size_warning_flag (bool, optional): If set to True, a warning will be\n-          issued if the dataset takes longer than 10 seconds to iterate. Defaults\n-          to True.\n+        data_size_warning_flag (bool, optional): If set to True, a warning will\n+          be issued if the dataset takes longer than 10 seconds to iterate.\n+          Defaults to True.\n         ensure_shape_similarity (bool, optional): If set to True, the shape of\n           the first sample will be used to validate the shape of rest of the\n           samples. Defaults to True.\n@@ -237,11 +237,11 @@ def _get_next_sample(\n         ensure_shape_similarity (bool, optional): If set to True, the shape of\n           the first sample will be used to validate the shape of rest of the\n           samples. Defaults to True.\n-        data_size_warning_flag (bool, optional): If set to True, a warning will be\n-          issued if the dataset takes longer than 10 seconds to iterate. Defaults\n-          to True.\n-        start_time (float): the start time of the dataset iteration. this is used\n-          only if `data_size_warning_flag` is set to true.\n+        data_size_warning_flag (bool, optional): If set to True, a warning will\n+          be issued if the dataset takes longer than 10 seconds to iterate.\n+          Defaults to True.\n+        start_time (float): the start time of the dataset iteration. this is\n+          used only if `data_size_warning_flag` is set to true.\n \n     Raises:\n         ValueError: - If the dataset is empty.\n@@ -284,9 +284,10 @@ def _get_next_sample(\n                 if int(cur_time - start_time) > 10 and data_size_warning_flag:\n                     warnings.warn(\n                         \"The dataset is taking longer than 10 seconds to \"\n-                        \"iterate over. This may be due to the size of the dataset. \"\n-                        \"Keep in mind that the `split_dataset` utility is only for \"\n-                        \"small in-memory dataset (e.g. < 10,000 samples).\",\n+                        \"iterate over. This may be due to the size of the \"\n+                        \"dataset. Keep in mind that the `split_dataset` \"\n+                        \"utility is only for small in-memory dataset \"\n+                        \"(e.g. < 10,000 samples).\",\n                         category=ResourceWarning,\n                         source=\"split_dataset\",\n                     )\n@@ -383,7 +384,8 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n             f\"{left_size}\"\n         )\n \n-    # check right_size is non-negative and less than 1 and less than total_length\n+    # check right_size is non-negative and less than 1 and less than\n+    # total_length\n     if (\n         right_size_type == int\n         and (right_size <= 0 or right_size >= total_length)\n@@ -397,7 +399,8 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n             f\"{right_size}\"\n         )\n \n-    # check sum of left_size and right_size is less than or equal to total_length\n+    # check sum of left_size and right_size is less than or equal to\n+    # total_length\n     if (\n         right_size_type == left_size_type == float\n         and right_size + left_size > 1\n@@ -509,7 +512,8 @@ def index_directory(\n       tuple (file_paths, labels, class_names).\n         file_paths: list of file paths (strings).\n         labels: list of matching integer labels (same length as file_paths)\n-        class_names: names of the classes corresponding to these labels, in order.\n+        class_names: names of the classes corresponding to these labels, in\n+          order.\n     \"\"\"\n     if labels is None:\n         # in the no-label case, index from the parent directory down.\n@@ -670,8 +674,8 @@ def labels_to_dataset(labels, label_mode, num_classes):\n       label_mode: String describing the encoding of `labels`. Options are:\n       - 'binary' indicates that the labels (there can be only 2) are encoded as\n         `float32` scalars with values 0 or 1 (e.g. for `binary_crossentropy`).\n-      - 'categorical' means that the labels are mapped into a categorical vector.\n-        (e.g. for `categorical_crossentropy` loss).\n+      - 'categorical' means that the labels are mapped into a categorical\n+        vector.  (e.g. for `categorical_crossentropy` loss).\n       num_classes: number of classes of labels.\n \n     Returns:\n@@ -718,7 +722,7 @@ def check_validation_split_arg(validation_split, subset, shuffle, seed):\n         )\n     if validation_split and shuffle and seed is None:\n         raise ValueError(\n-            \"If using `validation_split` and shuffling the data, you must provide \"\n-            \"a `seed` argument, to make sure that there is no overlap between the \"\n-            \"training and validation subset.\"\n+            \"If using `validation_split` and shuffling the data, you must \"\n+            \"provide a `seed` argument, to make sure that there is no \"\n+            \"overlap between the training and validation subset.\"\n         )\n\n@@ -48,7 +48,7 @@ _LAYER_UNDEFINED_CONFIG_KEY = \"layer was saved without config\"\n \n \n @keras_export(\n-    \"keras.utils.custom_object_scope\",  # pylint: disable=g-classes-have-attributes\n+    \"keras.utils.custom_object_scope\",\n     \"keras.utils.CustomObjectScope\",\n )\n class CustomObjectScope:\n@@ -65,7 +65,8 @@ class CustomObjectScope:\n \n     ```python\n     layer = Dense(3, kernel_regularizer=my_regularizer)\n-    config = layer.get_config()  # Config contains a reference to `my_regularizer`\n+    # Config contains a reference to `my_regularizer`\n+    config = layer.get_config()\n     ...\n     # Later:\n     with custom_object_scope({'my_regularizer': my_regularizer}):\n@@ -167,8 +168,8 @@ class DisableSharedObjectScope:\n class NoopLoadingScope:\n     \"\"\"The default shared object loading scope. It does nothing.\n \n-    Created to simplify serialization code that doesn't care about shared objects\n-    (e.g. when serializing a single object).\n+    Created to simplify serialization code that doesn't care about shared\n+    objects (e.g. when serializing a single object).\n     \"\"\"\n \n     def get(self, unused_object_id):\n@@ -200,8 +201,8 @@ class SharedObjectLoadingScope:\n         \"\"\"Given a shared object ID, returns a previously instantiated object.\n \n         Args:\n-          object_id: shared object ID to use when attempting to find already-loaded\n-            object.\n+          object_id: shared object ID to use when attempting to find\n+            already-loaded object.\n \n         Returns:\n           The object, if we've seen this ID before. Else, `None`.\n@@ -247,9 +248,9 @@ class SharedObjectConfig(dict):\n \n     def increment_ref_count(self):\n         # As soon as we've seen the object more than once, we want to attach the\n-        # shared object ID. This allows us to only attach the shared object ID when\n-        # it's strictly necessary, making backwards compatibility breakage less\n-        # likely.\n+        # shared object ID. This allows us to only attach the shared object ID\n+        # when it's strictly necessary, making backwards compatibility breakage\n+        # less likely.\n         if self.ref_count == 1:\n             self[SHARED_OBJECT_KEY] = self.object_id\n         self.ref_count += 1\n@@ -264,11 +265,11 @@ class SharedObjectSavingScope:\n \n         global SHARED_OBJECT_SAVING\n \n-        # Serialization can happen at a number of layers for a number of reasons.\n-        # We may end up with a case where we're opening a saving scope within\n-        # another saving scope. In that case, we'd like to use the outermost scope\n-        # available and ignore inner scopes, since there is not (yet) a reasonable\n-        # use case for having these nested and distinct.\n+        # Serialization can happen at a number of layers for a number of\n+        # reasons.  We may end up with a case where we're opening a saving scope\n+        # within another saving scope. In that case, we'd like to use the\n+        # outermost scope available and ignore inner scopes, since there is not\n+        # (yet) a reasonable use case for having these nested and distinct.\n         if _shared_object_saving_scope() is not None:\n             self._passthrough = True\n             return _shared_object_saving_scope()\n@@ -293,9 +294,10 @@ class SharedObjectSavingScope:\n         try:\n             shared_object_config = self._shared_objects_config[obj]\n         except (TypeError, KeyError):\n-            # If the object is unhashable (e.g. a subclass of `AbstractBaseClass`\n-            # that has not overridden `__hash__`), a `TypeError` will be thrown.\n-            # We'll just continue on without shared object support.\n+            # If the object is unhashable (e.g. a subclass of\n+            # `AbstractBaseClass` that has not overridden `__hash__`), a\n+            # `TypeError` will be thrown.  We'll just continue on without shared\n+            # object support.\n             return None\n         shared_object_config.increment_ref_count()\n         return shared_object_config\n@@ -307,9 +309,10 @@ class SharedObjectSavingScope:\n         try:\n             self._shared_objects_config[obj] = shared_object_config\n         except TypeError:\n-            # If the object is unhashable (e.g. a subclass of `AbstractBaseClass`\n-            # that has not overridden `__hash__`), a `TypeError` will be thrown.\n-            # We'll just continue on without shared object support.\n+            # If the object is unhashable (e.g. a subclass of\n+            # `AbstractBaseClass` that has not overridden `__hash__`), a\n+            # `TypeError` will be thrown.  We'll just continue on without shared\n+            # object support.\n             pass\n         return shared_object_config\n \n@@ -331,10 +334,10 @@ def serialize_keras_class_and_config(\n     if shared_object_id is not None:\n         base_config[SHARED_OBJECT_KEY] = shared_object_id\n \n-    # If we have an active `SharedObjectSavingScope`, check whether we've already\n-    # serialized this config. If so, just use that config. This will store an\n-    # extra ID field in the config, allowing us to re-create the shared object\n-    # relationship at load time.\n+    # If we have an active `SharedObjectSavingScope`, check whether we've\n+    # already serialized this config. If so, just use that config. This will\n+    # store an extra ID field in the config, allowing us to re-create the shared\n+    # object relationship at load time.\n     if _shared_object_saving_scope() is not None and obj is not None:\n         shared_object_config = _shared_object_saving_scope().get_config(obj)\n         if shared_object_config is None:\n@@ -373,13 +376,13 @@ def register_keras_serializable(package=\"Custom\", name=None):\n     ```\n \n     Args:\n-      package: The package that this class belongs to. This is used for the `key`\n-        (which is 'package>name') to idenfify the class. Note that this is the\n-        first argument passed into the decorator.\n+      package: The package that this class belongs to. This is used for the\n+        `key` (which is 'package>name') to idenfify the class. Note that this is\n+        the first argument passed into the decorator.\n       name: The name to serialize this class under in this package. If not\n         provided or `None`, the class' name will be used (note that this is the\n-        case when the decorator is used with only one argument, which becomes the\n-        `package`).\n+        case when the decorator is used with only one argument, which becomes\n+        the `package`).\n \n     Returns:\n       A decorator that registers the decorated class with the passed names.\n@@ -392,7 +395,8 @@ def register_keras_serializable(package=\"Custom\", name=None):\n \n         if tf_inspect.isclass(arg) and not hasattr(arg, \"get_config\"):\n             raise ValueError(\n-                \"Cannot register a class that does not have a get_config() method.\"\n+                \"Cannot register a class that does not have a \"\n+                \"get_config() method.\"\n             )\n \n         if registered_name in _GLOBAL_CUSTOM_OBJECTS:\n@@ -403,7 +407,8 @@ def register_keras_serializable(package=\"Custom\", name=None):\n \n         if arg in _GLOBAL_CUSTOM_NAMES:\n             raise ValueError(\n-                f\"{arg} has already been registered to {_GLOBAL_CUSTOM_NAMES[arg]}\"\n+                f\"{arg} has already been registered to \"\n+                f\"{_GLOBAL_CUSTOM_NAMES[arg]}\"\n             )\n         _GLOBAL_CUSTOM_OBJECTS[registered_name] = arg\n         _GLOBAL_CUSTOM_NAMES[arg] = registered_name\n@@ -542,8 +547,8 @@ def serialize_keras_object(instance):\n                 serialization_config[key] = item\n                 continue\n \n-            # Any object of a different type needs to be converted to string or dict\n-            # for serialization (e.g. custom functions, custom classes)\n+            # Any object of a different type needs to be converted to string or\n+            # dict for serialization (e.g. custom functions, custom classes)\n             try:\n                 serialized_item = serialize_keras_object(item)\n                 if isinstance(serialized_item, dict) and not isinstance(\n@@ -596,7 +601,8 @@ def class_and_config_for_serialized_keras_object(\n     cls = get_registered_object(class_name, custom_objects, module_objects)\n     if cls is None:\n         raise ValueError(\n-            f\"Unknown {printable_module_name}: {class_name}. Please ensure this \"\n+            f\"Unknown {printable_module_name}: {class_name}. \"\n+            \"Please ensure this \"\n             \"object is passed to the `custom_objects` argument. See \"\n             \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n             \"#registering_the_custom_object for details.\"\n@@ -631,14 +637,15 @@ def class_and_config_for_serialized_keras_object(\n         elif isinstance(item, str) and tf_inspect.isfunction(\n             get_registered_object(item, custom_objects)\n         ):\n-            # Handle custom functions here. When saving functions, we only save the\n-            # function's name as a string. If we find a matching string in the custom\n-            # objects during deserialization, we convert the string back to the\n-            # original function.\n-            # Note that a potential issue is that a string field could have a naming\n-            # conflict with a custom function name, but this should be a rare case.\n-            # This issue does not occur if a string field has a naming conflict with\n-            # a custom object, since the config of an object will always be a dict.\n+            # Handle custom functions here. When saving functions, we only save\n+            # the function's name as a string. If we find a matching string in\n+            # the custom objects during deserialization, we convert the string\n+            # back to the original function.\n+            # Note that a potential issue is that a string field could have a\n+            # naming conflict with a custom function name, but this should be a\n+            # rare case.  This issue does not occur if a string field has a\n+            # naming conflict with a custom object, since the config of an\n+            # object will always be a dict.\n             deserialized_objects[key] = get_registered_object(\n                 item, custom_objects\n             )\n@@ -659,24 +666,26 @@ def deserialize_keras_object(\n \n     This function is for mid-level library implementers rather than end users.\n \n-    Importantly, this utility requires you to provide the dict of `module_objects`\n-    to use for looking up the object config; this is not populated by default.\n-    If you need a deserialization utility that has preexisting knowledge of\n-    built-in Keras objects, use e.g. `keras.layers.deserialize(config)`,\n-    `keras.metrics.deserialize(config)`, etc.\n+    Importantly, this utility requires you to provide the dict of\n+    `module_objects` to use for looking up the object config; this is not\n+    populated by default. If you need a deserialization utility that has\n+    preexisting knowledge of built-in Keras objects, use e.g.\n+    `keras.layers.deserialize(config)`, `keras.metrics.deserialize(config)`,\n+    etc.\n \n     Calling `deserialize_keras_object` while underneath the\n-    `SharedObjectLoadingScope` context manager will cause any already-seen shared\n-    objects to be returned as-is rather than creating a new object.\n+    `SharedObjectLoadingScope` context manager will cause any already-seen\n+    shared objects to be returned as-is rather than creating a new object.\n \n     Args:\n       identifier: the serialized form of the object.\n       module_objects: A dictionary of built-in objects to look the name up in.\n-        Generally, `module_objects` is provided by midlevel library implementers.\n+        Generally, `module_objects` is provided by midlevel library\n+        implementers.\n       custom_objects: A dictionary of custom objects to look the name up in.\n         Generally, `custom_objects` is provided by the end user.\n-      printable_module_name: A human-readable string representing the type of the\n-        object. Printed in case of exception.\n+      printable_module_name: A human-readable string representing the type of\n+        the object. Printed in case of exception.\n \n     Returns:\n       The deserialized object.\n@@ -708,8 +717,8 @@ def deserialize_keras_object(\n             config, module_objects, custom_objects, printable_module_name\n         )\n \n-        # If this object has already been loaded (i.e. it's shared between multiple\n-        # objects), return the already-loaded object.\n+        # If this object has already been loaded (i.e. it's shared between\n+        # multiple objects), return the already-loaded object.\n         shared_object_id = config.get(SHARED_OBJECT_KEY)\n         shared_object = _shared_object_loading_scope().get(\n             shared_object_id\n@@ -755,8 +764,9 @@ def deserialize_keras_object(\n             obj = module_objects.get(object_name)\n             if obj is None:\n                 raise ValueError(\n-                    f\"Unknown {printable_module_name}: {object_name}. Please ensure \"\n-                    \"this object is passed to the `custom_objects` argument. See \"\n+                    f\"Unknown {printable_module_name}: {object_name}. Please \"\n+                    \"ensure this object is passed to the `custom_objects` \"\n+                    \"argument. See \"\n                     \"https://www.tensorflow.org/guide/keras/save_and_serialize\"\n                     \"#registering_the_custom_object for details.\"\n                 )\n@@ -771,7 +781,8 @@ def deserialize_keras_object(\n         return identifier\n     else:\n         raise ValueError(\n-            f\"Could not interpret serialized {printable_module_name}: {identifier}\"\n+            f\"Could not interpret serialized \"\n+            f\"{printable_module_name}: {identifier}\"\n         )\n \n \n@@ -854,8 +865,8 @@ def has_arg(fn, name, accept_all=False):\n     Args:\n         fn: Callable to inspect.\n         name: Check if `fn` can be called with `name` as a keyword argument.\n-        accept_all: What to return if there is no parameter called `name` but the\n-          function accepts a `**kwargs` argument.\n+        accept_all: What to return if there is no parameter called `name` but\n+          the function accepts a `**kwargs` argument.\n \n     Returns:\n         bool, whether `fn` accepts a `name` keyword argument.\n@@ -874,9 +885,9 @@ class Progbar:\n         target: Total number of steps expected, None if unknown.\n         width: Progress bar width on screen.\n         verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n-        stateful_metrics: Iterable of string names of metrics that should *not* be\n-          averaged over time. Metrics in this list will be displayed as-is. All\n-          others will be averaged by the progbar before display.\n+        stateful_metrics: Iterable of string names of metrics that should *not*\n+          be averaged over time. Metrics in this list will be displayed as-is.\n+          All others will be averaged by the progbar before display.\n         interval: Minimum visual progress update interval (in seconds).\n         unit_name: Display name for step counts (usually \"step\" or \"sample\").\n     \"\"\"\n@@ -923,9 +934,10 @@ class Progbar:\n \n         Args:\n             current: Index of current step.\n-            values: List of tuples: `(name, value_for_last_step)`. If `name` is in\n-              `stateful_metrics`, `value_for_last_step` will be displayed as-is.\n-              Else, an average of the metric over time will be displayed.\n+            values: List of tuples: `(name, value_for_last_step)`. If `name` is\n+              in `stateful_metrics`, `value_for_last_step` will be displayed\n+              as-is. Else, an average of the metric over time will be\n+              displayed.\n             finalize: Whether this is the last update for the progress bar. If\n               `None`, defaults to `current >= self.target`.\n         \"\"\"\n@@ -940,10 +952,11 @@ class Progbar:\n             if k not in self._values_order:\n                 self._values_order.append(k)\n             if k not in self.stateful_metrics:\n-                # In the case that progress bar doesn't have a target value in the first\n-                # epoch, both on_batch_end and on_epoch_end will be called, which will\n-                # cause 'current' and 'self._seen_so_far' to have the same value. Force\n-                # the minimal value to 1 here, otherwise stateful_metric will be 0s.\n+                # In the case that progress bar doesn't have a target value in\n+                # the first epoch, both on_batch_end and on_epoch_end will be\n+                # called, which will cause 'current' and 'self._seen_so_far' to\n+                # have the same value. Force the minimal value to 1 here,\n+                # otherwise stateful_metric will be 0s.\n                 value_base = max(current - self._seen_so_far, 1)\n                 if k not in self._values:\n                     self._values[k] = [v * value_base, value_base]\n@@ -1093,22 +1106,25 @@ class Progbar:\n     def _estimate_step_duration(self, current, now):\n         \"\"\"Estimate the duration of a single step.\n \n-        Given the step number `current` and the corresponding time `now`\n-        this function returns an estimate for how long a single step\n-        takes. If this is called before one step has been completed\n-        (i.e. `current == 0`) then zero is given as an estimate. The duration\n-        estimate ignores the duration of the (assumed to be non-representative)\n-        first step for estimates when more steps are available (i.e. `current>1`).\n+        Given the step number `current` and the corresponding time `now` this\n+        function returns an estimate for how long a single step takes. If this\n+        is called before one step has been completed (i.e. `current == 0`) then\n+        zero is given as an estimate. The duration estimate ignores the duration\n+        of the (assumed to be non-representative) first step for estimates when\n+        more steps are available (i.e. `current>1`).\n+\n         Args:\n           current: Index of current step.\n           now: The current time.\n+\n         Returns: Estimate of the duration of a single step.\n         \"\"\"\n         if current:\n             # there are a few special scenarios here:\n-            # 1) somebody is calling the progress bar without ever supplying step 1\n-            # 2) somebody is calling the progress bar and supplies step one multiple\n-            #    times, e.g. as part of a finalizing call\n+            # 1) somebody is calling the progress bar without ever supplying\n+            #    step 1\n+            # 2) somebody is calling the progress bar and supplies step one\n+            #    multiple times, e.g. as part of a finalizing call\n             # in these cases, we just fall back to the simple calculation\n             if self._time_after_first_step is not None and current > 1:\n                 time_per_unit = (now - self._time_after_first_step) / (\n@@ -1236,8 +1252,8 @@ def check_for_unexpected_keys(name, input_dict, expected_values):\n     unknown = set(input_dict.keys()).difference(expected_values)\n     if unknown:\n         raise ValueError(\n-            f\"Unknown entries in {name} dictionary: {list(unknown)}. Only expected \"\n-            f\"following keys: {expected_values}\"\n+            f\"Unknown entries in {name} dictionary: {list(unknown)}. \"\n+            f\"Only expected following keys: {expected_values}\"\n         )\n \n \n@@ -1290,8 +1306,8 @@ class LazyLoader(python_types.ModuleType):\n         module = importlib.import_module(self.__name__)\n         self._parent_module_globals[self._local_name] = module\n         # Update this object's dict so that if someone keeps a reference to the\n-        #   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\n-        #   that fail).\n+        # LazyLoader, lookups are efficient (__getattr__ is only called on\n+        # lookups that fail).\n         self.__dict__.update(module.__dict__)\n         return module\n \n\n@@ -131,7 +131,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n             ValueError, \".*has already been registered.*\"\n         ):\n \n-            @keras.utils.generic_utils.register_keras_serializable()  # pylint: disable=function-redefined\n+            @keras.utils.generic_utils.register_keras_serializable()\n             class TestClass:  # pylint: disable=function-redefined\n                 def __init__(self, value):\n                     self._value = value\n@@ -197,7 +197,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n             \"Cannot register a class that does \" \"not have a get_config.*\",\n         ):\n \n-            @keras.utils.generic_utils.register_keras_serializable(  # pylint: disable=unused-variable\n+            @keras.utils.generic_utils.register_keras_serializable(\n                 \"TestPackage\", \"TestClass\"\n             )\n             class TestClass:\n@@ -476,8 +476,8 @@ class SharedObjectScopeTest(tf.test.TestCase):\n         with generic_utils.SharedObjectSavingScope() as scope_1:\n             scope_1.create_config({}, my_obj)\n             with generic_utils.SharedObjectSavingScope() as scope_2:\n-                # Nesting saving scopes should return the original scope and should\n-                # not clear any objects we're tracking.\n+                # Nesting saving scopes should return the original scope and\n+                # should not clear any objects we're tracking.\n                 self.assertIs(scope_1, scope_2)\n                 self.assertIsNotNone(scope_2.get_config(my_obj))\n             self.assertIsNotNone(scope_1.get_config(my_obj))\n@@ -529,8 +529,8 @@ class SharedObjectScopeTest(tf.test.TestCase):\n         with self.captureWritesToStream(sys.stderr) as printed:\n             loaded_model.fit(x, y, epochs=1)\n             if tf.__internal__.tf2.enabled():\n-                # `tf.print` message is only available in stderr in TF2. Check that\n-                # custom `train_step` is used.\n+                # `tf.print` message is only available in stderr in TF2. Check\n+                # that custom `train_step` is used.\n                 self.assertRegex(printed.contents(), train_step_message)\n \n         # Check that the custom class does get used.\n\n@@ -63,9 +63,9 @@ def image_dataset_from_directory(\n     ......b_image_2.jpg\n     ```\n \n-    Then calling `image_dataset_from_directory(main_directory, labels='inferred')`\n-    will return a `tf.data.Dataset` that yields batches of images from\n-    the subdirectories `class_a` and `class_b`, together with labels\n+    Then calling `image_dataset_from_directory(main_directory,\n+    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n+    images from the subdirectories `class_a` and `class_b`, together with labels\n     0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n \n     Supported image formats: jpeg, png, bmp, gif.\n@@ -124,8 +124,8 @@ def image_dataset_from_directory(\n           Defaults to False.\n       crop_to_aspect_ratio: If True, resize the images without aspect\n         ratio distortion. When the original aspect ratio differs from the target\n-        aspect ratio, the output image will be cropped so as to return the largest\n-        possible window in the image (of size `image_size`) that matches\n+        aspect ratio, the output image will be cropped so as to return the\n+        largest possible window in the image (of size `image_size`) that matches\n         the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n         aspect ratio may not be preserved.\n       **kwargs: Legacy keyword arguments.\n@@ -163,9 +163,10 @@ def image_dataset_from_directory(\n     if labels not in (\"inferred\", None):\n         if not isinstance(labels, (list, tuple)):\n             raise ValueError(\n-                \"`labels` argument should be a list/tuple of integer labels, of \"\n-                \"the same size as the number of image files in the target \"\n-                \"directory. If you wish to infer the labels from the subdirectory \"\n+                \"`labels` argument should be a list/tuple of integer labels, \"\n+                \"of the same size as the number of image files in the target \"\n+                \"directory. If you wish to infer the labels from the \"\n+                \"subdirectory \"\n                 'names in the target directory, pass `labels=\"inferred\"`. '\n                 \"If you wish to get a dataset that only contains images \"\n                 f\"(no labels), pass `labels=None`. Received: labels={labels}\"\n@@ -178,7 +179,8 @@ def image_dataset_from_directory(\n             )\n     if label_mode not in {\"int\", \"categorical\", \"binary\", None}:\n         raise ValueError(\n-            '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n+            '`label_mode` argument must be one of \"int\", '\n+            '\"categorical\", \"binary\", '\n             f\"or None. Received: label_mode={label_mode}\"\n         )\n     if labels is None or label_mode is None:\n\n@@ -95,7 +95,8 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n         return temp_dir\n \n     def test_image_dataset_from_directory_standalone(self):\n-        # Test retrieving images without labels from a directory and its subdirs.\n+        # Test retrieving images without labels from a directory and its\n+        # subdirs.\n         if PIL is None:\n             return  # Skip test if PIL is not available.\n \n\n@@ -62,15 +62,15 @@ def smart_resize(x, size, interpolation=\"bilinear\"):\n \n     Warning: `tf.keras.preprocessing.image.smart_resize` is not recommended for\n     new code. Prefer `tf.keras.layers.Resizing`, which provides the same\n-    functionality as a preprocessing layer and adds `tf.RaggedTensor` support. See\n-    the [preprocessing layer guide](\n+    functionality as a preprocessing layer and adds `tf.RaggedTensor` support.\n+    See the [preprocessing layer guide](\n     https://www.tensorflow.org/guide/keras/preprocessing_layers)\n     for an overview of preprocessing layers.\n \n     TensorFlow image datasets typically yield images that have each a different\n     size. However, these images need to be batched before they can be\n-    processed by Keras layers. To be batched, images need to share the same height\n-    and width.\n+    processed by Keras layers. To be batched, images need to share the same\n+    height and width.\n \n     You could simply do:\n \n@@ -101,9 +101,10 @@ def smart_resize(x, size, interpolation=\"bilinear\"):\n \n     The resizing process is:\n \n-    1. Take the largest centered crop of the image that has the same aspect ratio\n-    as the target size. For instance, if `size=(200, 200)` and the input image has\n-    size `(340, 500)`, we take a crop of `(340, 340)` centered along the width.\n+    1. Take the largest centered crop of the image that has the same aspect\n+    ratio as the target size. For instance, if `size=(200, 200)` and the input\n+    image has size `(340, 500)`, we take a crop of `(340, 340)` centered along\n+    the width.\n     2. Resize the cropped image to the target size. In the example above,\n     we resize the `(340, 340)` crop to `(200, 200)`.\n \n@@ -129,8 +130,8 @@ def smart_resize(x, size, interpolation=\"bilinear\"):\n     if img.shape.rank is not None:\n         if img.shape.rank < 3 or img.shape.rank > 4:\n             raise ValueError(\n-                \"Expected an image array with shape `(height, width, channels)`, \"\n-                \"or `(batch_size, height, width, channels)`, but \"\n+                \"Expected an image array with shape `(height, width, \"\n+                \"channels)`, or `(batch_size, height, width, channels)`, but \"\n                 f\"got input with incorrect rank, of shape {img.shape}.\"\n             )\n     shape = tf.shape(img)\n@@ -148,7 +149,8 @@ def smart_resize(x, size, interpolation=\"bilinear\"):\n         tf.cast(height * target_width, \"float32\") / target_height, \"int32\"\n     )\n \n-    # Set back to input height / width if crop_height / crop_width is not smaller.\n+    # Set back to input height / width if crop_height / crop_width is not\n+    # smaller.\n     crop_height = tf.minimum(height, crop_height)\n     crop_width = tf.minimum(width, crop_width)\n \n@@ -207,14 +209,14 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n     Args:\n         x: Input data, in any form that can be converted to a Numpy array.\n         data_format: Image data format, can be either `\"channels_first\"` or\n-          `\"channels_last\"`. Defaults to `None`, in which case the global setting\n-          `tf.keras.backend.image_data_format()` is used (unless you changed it,\n-          it defaults to `\"channels_last\"`).\n+          `\"channels_last\"`. Defaults to `None`, in which case the global\n+          setting `tf.keras.backend.image_data_format()` is used (unless you\n+          changed it, it defaults to `\"channels_last\"`).\n         scale: Whether to rescale the image such that minimum and maximum values\n           are 0 and 255 respectively. Defaults to `True`.\n         dtype: Dtype to use. Default to `None`, in which case the global setting\n-          `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n-          to `\"float32\"`)\n+          `tf.keras.backend.floatx()` is used (unless you changed it, it\n+          defaults to `\"float32\"`)\n \n     Returns:\n         A PIL Image instance.\n@@ -289,12 +291,12 @@ def img_to_array(img, data_format=None, dtype=None):\n     Args:\n         img: Input PIL Image instance.\n         data_format: Image data format, can be either `\"channels_first\"` or\n-          `\"channels_last\"`. Defaults to `None`, in which case the global setting\n-          `tf.keras.backend.image_data_format()` is used (unless you changed it,\n-          it defaults to `\"channels_last\"`).\n+          `\"channels_last\"`. Defaults to `None`, in which case the global\n+          setting `tf.keras.backend.image_data_format()` is used (unless you\n+          changed it, it defaults to `\"channels_last\"`).\n         dtype: Dtype to use. Default to `None`, in which case the global setting\n-          `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n-          to `\"float32\"`).\n+          `tf.keras.backend.floatx()` is used (unless you changed it, it\n+          defaults to `\"float32\"`).\n \n     Returns:\n         A 3D Numpy array.\n@@ -335,9 +337,9 @@ def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n         x: Numpy array.\n         data_format: Image data format, either `\"channels_first\"` or\n           `\"channels_last\"`.\n-        file_format: Optional file format override. If omitted, the format to use\n-          is determined from the filename extension. If a file object was used\n-          instead of a filename, this parameter should always be used.\n+        file_format: Optional file format override. If omitted, the format to\n+          use is determined from the filename extension. If a file object was\n+          used instead of a filename, this parameter should always be used.\n         scale: Whether to rescale image values to be within `[0, 255]`.\n         **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n     \"\"\"\n\n@@ -58,11 +58,11 @@ def exact_gaussian_kernel(x, y, stddev):\n \n     The Gaussian kernel for vectors u, v is defined as follows:\n          K(u, v) = exp(-||u-v||^2 / (2* stddev^2))\n-    where the norm is the l2-norm. x, y can be either vectors or matrices. If they\n-    are vectors, they must have the same dimension. If they are matrices, they\n-    must have the same number of columns. In the latter case, the method returns\n-    (as a matrix) K(u, v) values for all pairs (u, v) where u is a row from x and\n-    v is a row from y.\n+    where the norm is the l2-norm. x, y can be either vectors or matrices. If\n+    they are vectors, they must have the same dimension. If they are matrices,\n+    they must have the same number of columns. In the latter case, the method\n+    returns (as a matrix) K(u, v) values for all pairs (u, v) where u is a row\n+    from x and v is a row from y.\n \n     Args:\n       x: a tensor of rank 1 or 2. It's shape should be either [dim] or [m, dim].\n@@ -70,9 +70,9 @@ def exact_gaussian_kernel(x, y, stddev):\n       stddev: The width of the Gaussian kernel.\n \n     Returns:\n-      A single value (scalar) with shape (1, 1) (if x, y are vectors) or a matrix\n-        of shape (m, n) with entries K(u, v) (where K is the Gaussian kernel) for\n-        all (u,v) pairs where u, v are rows from x and y respectively.\n+      A single value (scalar) with shape (1, 1) (if x, y are vectors) or a\n+      matrix of shape (m, n) with entries K(u, v) (where K is the Gaussian\n+      kernel) for all (u,v) pairs where u, v are rows from x and y respectively.\n \n     Raises:\n       ValueError: if the shapes of x, y are not compatible.\n@@ -89,11 +89,11 @@ def exact_laplacian_kernel(x, y, stddev):\n \n     The Laplacian kernel for vectors u, v is defined as follows:\n          K(u, v) = exp(-||u-v|| / stddev)\n-    where the norm is the l1-norm. x, y can be either vectors or matrices. If they\n-    are vectors, they must have the same dimension. If they are matrices, they\n-    must have the same number of columns. In the latter case, the method returns\n-    (as a matrix) K(u, v) values for all pairs (u, v) where u is a row from x and\n-    v is a row from y.\n+    where the norm is the l1-norm. x, y can be either vectors or matrices. If\n+    they are vectors, they must have the same dimension. If they are matrices,\n+    they must have the same number of columns. In the latter case, the method\n+    returns (as a matrix) K(u, v) values for all pairs (u, v) where u is a row\n+    from x and v is a row from y.\n \n     Args:\n       x: a tensor of rank 1 or 2. It's shape should be either [dim] or [m, dim].\n\n@@ -61,8 +61,8 @@ class KernelizedUtilsTest(tf.test.TestCase, parameterized.TestCase):\n         exact_kernel = exact_kernel_fn(x, y)\n         shape = exact_kernel.shape.as_list()\n         self.assertLen(shape, 2)\n-        # x and y are almost identical and therefore K(x, y) will be almost equal to\n-        # the identity value of the kernel.\n+        # x and y are almost identical and therefore K(x, y) will be almost\n+        # equal to the identity value of the kernel.\n         self.assertAllClose(expected_values, exact_kernel, atol=1e-3)\n \n     @parameterized.named_parameters(\n@@ -70,7 +70,8 @@ class KernelizedUtilsTest(tf.test.TestCase, parameterized.TestCase):\n         (\"laplacian\", _exact_laplacian(stddev=5.0), [[0.96], [0.94]]),\n     )\n     def test_similar_matrices(self, exact_kernel_fn, expected_values):\n-        \"\"\"Pairwise \"close\" vectors give high kernel values (similarity scores).\"\"\"\n+        \"\"\"Pairwise \"close\" vectors give high kernel values (similarity\n+        scores).\"\"\"\n         x = tf.constant([1.0, 3.4, -2.1, 0.9, 3.3, -2.0], shape=[2, 3])\n         y = tf.constant([1.1, 3.35, -2.05])\n         exact_kernel = exact_kernel_fn(x, y)\n\n@@ -41,8 +41,8 @@ class DistributeKplTestUtils(tf.test.TestCase):\n         \"\"\"Function that defines KPL used for unit tests of tf.distribute.\n \n         Args:\n-          use_adapt: if adapt will be called. False means there will be precomputed\n-            statistics.\n+          use_adapt: if adapt will be called. False means there will be\n+            precomputed statistics.\n \n         Returns:\n           feature_mapper: a simple keras model with one keras StringLookup layer\n\n@@ -272,8 +272,9 @@ def print_summary(\n         name = layer.name\n         cls_name = layer.__class__.__name__\n         if not layer.built and not getattr(layer, \"_is_graph_network\", False):\n-            # If a subclassed model has a layer that is not called in Model.call, the\n-            # layer will not be built and we cannot call layer.count_params().\n+            # If a subclassed model has a layer that is not called in\n+            # Model.call, the layer will not be built and we cannot call\n+            # layer.count_params().\n             params = \"0 (unused)\"\n         else:\n             params = layer.count_params()\n@@ -441,8 +442,8 @@ def cached_per_instance(f):\n     For classes with custom getattr / setattr behavior (such as trackable\n     objects), storing cache results as object attributes is not performant.\n     Instead, a specialized cache can significantly reduce property lookup\n-    overhead. (While still allowing the decorated property to be lazily computed.)\n-    Consider the following class:\n+    overhead. (While still allowing the decorated property to be lazily\n+    computed.) Consider the following class:\n \n     ```\n     class MyClass:\n@@ -472,8 +473,8 @@ def cached_per_instance(f):\n \n     Slows down attribute assignment by nearly 10x.\n \n-    By contrast, replacing the definition of `thing` with the following sidesteps\n-    the expensive __setattr__ altogether:\n+    By contrast, replacing the definition of `thing` with the following\n+    sidesteps the expensive __setattr__ altogether:\n \n     '''\n     @property\n@@ -486,7 +487,8 @@ def cached_per_instance(f):\n \n     Performance:\n     The overhead for this decorator is ~0.4 us / call. A much lower overhead\n-    implementation (~0.085 us / call) can be achieved by using a custom dict type:\n+    implementation (~0.085 us / call) can be achieved by using a custom dict\n+    type:\n \n     ```\n     def dict_based_cache(f):\n@@ -545,7 +547,8 @@ def filter_empty_layer_containers(layer_list):\n \n \n class CallFunctionSpec:\n-    \"\"\"Caches the spec and provides utilities for handling call function args.\"\"\"\n+    \"\"\"Caches the spec and provides utilities for handling call function\n+    args.\"\"\"\n \n     def __init__(self, full_argspec):\n         \"\"\"Initialies a `CallFunctionSpec`.\n@@ -571,12 +574,12 @@ class CallFunctionSpec:\n \n         call_fn_defaults = self._full_argspec.defaults or []\n         defaults = dict()\n-        # The call arg defaults are an n-tuple of the last n elements of the args\n-        # list. (n = # of elements that have a default argument)\n+        # The call arg defaults are an n-tuple of the last n elements of the\n+        # args list. (n = # of elements that have a default argument)\n         for i in range(-1 * len(call_fn_defaults), 0):\n             defaults[self._arg_names[i]] = call_fn_defaults[i]\n-        # The default training arg will be any (non-None) default specified in the\n-        # method signature, or None if no value is specified.\n+        # The default training arg will be any (non-None) default specified in\n+        # the method signature, or None if no value is specified.\n         defaults.update(self._full_argspec.kwonlydefaults or {})\n         self._default_training_arg = defaults.get(\"training\")\n \n@@ -599,7 +602,8 @@ class CallFunctionSpec:\n     @cached_per_instance\n     def arg_positions(self):\n         \"\"\"Returns a dict mapping arg names to their index positions.\"\"\"\n-        # `arg_positions` is not accurate if the layer has variable positional args.\n+        # `arg_positions` is not accurate if the layer has variable positional\n+        # args.\n         call_fn_arg_positions = dict()\n         for pos, arg in enumerate(self._arg_names):\n             call_fn_arg_positions[arg] = pos\n@@ -635,8 +639,8 @@ class CallFunctionSpec:\n           arg_name: String name of the argument to find.\n           args: Tuple of args passed to the call function.\n           kwargs: Dictionary of kwargs  passed to the call function.\n-          inputs_in_args: Whether the input argument (the first argument in the call\n-            function) is included in `args`. Defaults to `False`.\n+          inputs_in_args: Whether the input argument (the first argument in the\n+            call function) is included in `args`. Defaults to `False`.\n \n         Returns:\n           True if argument with `arg_name` is present in `args` or `kwargs`.\n@@ -660,12 +664,12 @@ class CallFunctionSpec:\n           arg_name: String name of the argument to find.\n           args: Tuple of args passed to the call function.\n           kwargs: Dictionary of kwargs  passed to the call function.\n-          inputs_in_args: Whether the input argument (the first argument in the call\n-            function) is included in `args`. Defaults to `False`.\n+          inputs_in_args: Whether the input argument (the first argument in the\n+            call function) is included in `args`. Defaults to `False`.\n \n         Returns:\n-          The value of the argument with name `arg_name`, extracted from `args` or\n-          `kwargs`.\n+          The value of the argument with name `arg_name`, extracted from `args`\n+          or `kwargs`.\n \n         Raises:\n           KeyError if the value of `arg_name` cannot be found.\n@@ -695,10 +699,10 @@ class CallFunctionSpec:\n           new_value: New value to give to the argument.\n           args: Tuple of args passed to the call function.\n           kwargs: Dictionary of kwargs  passed to the call function.\n-          inputs_in_args: Whether the input argument (the first argument in the call\n-            function) is included in `args`. Defaults to `False`.\n-          pop_kwarg_if_none: If the new value is `None`, and this is `True`, then\n-            the argument is deleted from `kwargs`.\n+          inputs_in_args: Whether the input argument (the first argument in the\n+            call function) is included in `args`. Defaults to `False`.\n+          pop_kwarg_if_none: If the new value is `None`, and this is `True`,\n+            then the argument is deleted from `kwargs`.\n \n         Returns:\n           The updated `(args, kwargs)`.\n\n@@ -229,8 +229,8 @@ class LayerUtilsTest(tf.test.TestCase):\n             reader = open(fpath, \"r\")\n             lines = reader.readlines()\n             reader.close()\n-            # The output content are slightly different for the input shapes between\n-            # v1 and v2.\n+            # The output content are slightly different for the input shapes\n+            # between v1 and v2.\n             if tf.__internal__.tf2.enabled():\n                 self.assertEqual(len(lines), 39)\n             else:\n@@ -414,7 +414,8 @@ class LayerUtilsTest(tf.test.TestCase):\n         self.assertEqual(first_object.test_property, id(first_object))\n         self.assertEqual(second_object.test_property, id(second_object))\n \n-        # Count the function calls to make sure the cache is actually being used.\n+        # Count the function calls to make sure the cache is actually being\n+        # used.\n         self.assertAllEqual(tuple(test_counter.values()), (1, 1))\n \n     def test_property_cache_threaded(self):\n@@ -429,10 +430,10 @@ class LayerUtilsTest(tf.test.TestCase):\n                 call_count[\"test_property\"] += 1\n                 time.sleep(np.random.random() + 1.0)\n \n-                # Use a RandomState which is seeded off the instance's id (the mod is\n-                # because numpy limits the range of seeds) to ensure that an instance\n-                # returns the same value in different threads, but different instances\n-                # return different values.\n+                # Use a RandomState which is seeded off the instance's id (the\n+                # mod is because numpy limits the range of seeds) to ensure that\n+                # an instance returns the same value in different threads, but\n+                # different instances return different values.\n                 return int(\n                     np.random.RandomState(id(self) % (2**31)).randint(2**16)\n                 )\n@@ -442,12 +443,12 @@ class LayerUtilsTest(tf.test.TestCase):\n                 return self.test_property\n \n         # Test that multiple threads return the same value. This requires that\n-        # the underlying function is repeatable, as cached_property makes no attempt\n-        # to prioritize the first call.\n+        # the underlying function is repeatable, as cached_property makes no\n+        # attempt to prioritize the first call.\n         test_obj = MyObject()\n         with contextlib.closing(multiprocessing.dummy.Pool(32)) as pool:\n-            # Intentionally make a large pool (even when there are only a small number\n-            # of cpus) to ensure that the runtime switches threads.\n+            # Intentionally make a large pool (even when there are only a small\n+            # number of cpus) to ensure that the runtime switches threads.\n             results = pool.map(test_obj.get_test_property, range(64))\n         self.assertEqual(len(set(results)), 1)\n \n@@ -462,15 +463,15 @@ class LayerUtilsTest(tf.test.TestCase):\n             results = pool.map(test_obj.get_test_property, range(4))\n         total_time = timeit.default_timer() - start_time\n \n-        # Note(taylorrobie): The reason that it is safe to time a unit test is that\n-        #                    a cache hit will be << 1 second, and a cache miss is\n-        #                    guaranteed to be >= 1 second. Empirically confirmed by\n-        #                    100,000 runs with no flakes.\n+        # Note(taylorrobie): The reason that it is safe to time a unit test is\n+        # that a cache hit will be << 1 second, and a cache miss is guaranteed\n+        # to be >= 1 second. Empirically confirmed by 100,000 runs with no\n+        # flakes.\n         self.assertLess(total_time, 0.95)\n \n     def test_property_cache_serialization(self):\n-        # Reset call count. .keys() must be wrapped in a list, because otherwise we\n-        # would mutate the iterator while iterating.\n+        # Reset call count. .keys() must be wrapped in a list, because otherwise\n+        # we would mutate the iterator while iterating.\n         for k in list(_PICKLEABLE_CALL_COUNT.keys()):\n             _PICKLEABLE_CALL_COUNT.pop(k)\n \n\n@@ -29,31 +29,32 @@ class ReductionV2:\n \n     Contains the following values:\n \n-    * `AUTO`: Indicates that the reduction option will be determined by the usage\n-       context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When\n-       used with `tf.distribute.Strategy`, outside of built-in training loops such\n-       as `tf.keras` `compile` and `fit`, we expect reduction value to be\n-       `SUM` or `NONE`. Using `AUTO` in that case will raise an error.\n-    * `NONE`: No **additional** reduction is applied to the output of the wrapped\n-       loss function. When non-scalar losses are returned to Keras functions like\n-       `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer\n-       but the reported loss will be a scalar value.\n+    * `AUTO`: Indicates that the reduction option will be determined by the\n+      usage context. For almost all cases this defaults to\n+      `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of\n+      built-in training loops such as `tf.keras` `compile` and `fit`, we expect\n+      reduction value to be `SUM` or `NONE`. Using `AUTO` in that case will\n+      raise an error.\n+    * `NONE`: No **additional** reduction is applied to the output of the\n+      wrapped loss function. When non-scalar losses are returned to Keras\n+      functions like `fit`/`evaluate`, the unreduced vector loss is passed to\n+      the optimizer but the reported loss will be a scalar value.\n \n        Caution: **Verify the shape of the outputs when using** `Reduction.NONE`.\n-       The builtin loss functions wrapped by the loss classes reduce\n-       one dimension (`axis=-1`, or `axis` if specified by loss function).\n-       `Reduction.NONE` just means that no **additional** reduction is applied by\n-       the class wrapper. For categorical losses with an example input shape of\n-       `[batch, W, H, n_classes]` the `n_classes` dimension is reduced. For\n+       The builtin loss functions wrapped by the loss classes reduce one\n+       dimension (`axis=-1`, or `axis` if specified by loss function).\n+       `Reduction.NONE` just means that no **additional** reduction is applied\n+       by the class wrapper. For categorical losses with an example input shape\n+       of `[batch, W, H, n_classes]` the `n_classes` dimension is reduced. For\n        pointwise losses you must include a dummy axis so that `[batch, W, H, 1]`\n        is reduced to `[batch, W, H]`. Without the dummy axis `[batch, W, H]`\n        will be incorrectly reduced to `[batch, W]`.\n \n     * `SUM`: Scalar sum of weighted losses.\n-    * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses.\n-       This reduction type is not supported when used with\n-       `tf.distribute.Strategy` outside of built-in training loops like `tf.keras`\n-       `compile`/`fit`.\n+    * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in\n+       losses.  This reduction type is not supported when used with\n+       `tf.distribute.Strategy` outside of built-in training loops like\n+       `tf.keras` `compile`/`fit`.\n \n        You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n        ```\n@@ -96,10 +97,10 @@ def remove_squeezable_dimensions(\n     defaults to 0, and we squeeze the last dimension of the larger rank if they\n     differ by 1.\n \n-    But, for example, if `labels` contains class IDs and `predictions` contains 1\n-    probability per class, we expect `predictions` to have 1 more dimension than\n-    `labels`, so `expected_rank_diff` would be 1. In this case, we'd squeeze\n-    `labels` if `rank(predictions) - rank(labels) == 0`, and\n+    But, for example, if `labels` contains class IDs and `predictions` contains\n+    1 probability per class, we expect `predictions` to have 1 more dimension\n+    than `labels`, so `expected_rank_diff` would be 1. In this case, we'd\n+    squeeze `labels` if `rank(predictions) - rank(labels) == 0`, and\n     `predictions` if `rank(predictions) - rank(labels) == 2`.\n \n     This will use static shape if available. Otherwise, it will add graph\n@@ -185,10 +186,10 @@ def squeeze_or_expand_dimensions(y_pred, y_true=None, sample_weight=None):\n     y_pred_rank = y_pred_shape.ndims\n     if y_true is not None:\n \n-        # If sparse matrix is provided as `y_true`, the last dimension in `y_pred`\n-        # may be > 1. Eg: y_true = [0, 1, 2] (shape=(3,)),\n-        # y_pred = [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]] (shape=(3, 3))\n-        # In this case, we should not try to remove squeezable dimension.\n+        # If sparse matrix is provided as `y_true`, the last dimension in\n+        # `y_pred` may be > 1. Eg: y_true = [0, 1, 2] (shape=(3,)), y_pred =\n+        # [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]] (shape=(3, 3)) In\n+        # this case, we should not try to remove squeezable dimension.\n         y_true_shape = y_true.shape\n         y_true_rank = y_true_shape.ndims\n         if (y_true_rank is not None) and (y_pred_rank is not None):\n@@ -198,9 +199,7 @@ def squeeze_or_expand_dimensions(y_pred, y_true=None, sample_weight=None):\n         else:\n             # Use dynamic rank.\n             rank_diff = tf.rank(y_pred) - tf.rank(y_true)\n-            squeeze_dims = lambda: remove_squeezable_dimensions(  # pylint: disable=g-long-lambda\n-                y_true, y_pred\n-            )\n+            squeeze_dims = lambda: remove_squeezable_dimensions(y_true, y_pred)\n             is_last_dim_1 = tf.equal(1, tf.shape(y_pred)[-1])\n             maybe_squeeze_dims = (\n                 lambda: tf.cond(  # pylint: disable=g-long-lambda\n@@ -298,14 +297,15 @@ def compute_weighted_loss(\n \n     Args:\n       losses: `Tensor` of shape `[batch_size, d1, ... dN]`.\n-      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n-        `losses`, or be broadcastable to `losses`.\n-      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n-        Default value is `SUM_OVER_BATCH_SIZE`.\n+      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank\n+        as `losses`, or be broadcastable to `losses`.\n+      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n+        loss. Default value is `SUM_OVER_BATCH_SIZE`.\n       name: Optional name for the op.\n \n     Raises:\n-      ValueError: If the shape of `sample_weight` is not compatible with `losses`.\n+      ValueError: If the shape of `sample_weight` is not compatible with\n+        `losses`.\n \n     Returns:\n       Weighted loss `Tensor` of the same type as `losses`. If `reduction` is\n@@ -334,8 +334,8 @@ def compute_weighted_loss(\n         ):\n             sample_weight = tf.convert_to_tensor(sample_weight)\n \n-        # Convert any non float dtypes to floats, to avoid it loss any precision for\n-        # dtype like int or bool.\n+        # Convert any non float dtypes to floats, to avoid it loss any precision\n+        # for dtype like int or bool.\n         if not losses.dtype.is_floating:\n             input_dtype = losses.dtype\n             losses = tf.cast(losses, \"float32\")\n@@ -343,14 +343,13 @@ def compute_weighted_loss(\n         else:\n             input_casted = False\n         sample_weight = tf.cast(sample_weight, losses.dtype)\n-        # Update dimensions of `sample_weight` to match with `losses` if possible.\n+        # Update dimensions of `sample_weight` to match with `losses` if\n+        # possible.\n         (\n             losses,\n             _,\n             sample_weight,\n-        ) = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n-            losses, None, sample_weight\n-        )\n+        ) = squeeze_or_expand_dimensions(losses, None, sample_weight)\n         weighted_losses = tf.multiply(losses, sample_weight)\n \n         # Apply reduction function to the individual weighted losses.\n@@ -373,8 +372,8 @@ def cast_losses_to_common_dtype(losses):\n     \"\"\"Cast a list of losses to a common dtype.\n \n     If any loss is floating-point, they will all be casted to the most-precise\n-    floating-point loss. Otherwise the losses are not casted. We also skip casting\n-    losses if there are any complex losses.\n+    floating-point loss. Otherwise the losses are not casted. We also skip\n+    casting losses if there are any complex losses.\n \n     Args:\n       losses: A list of losses.\n\n@@ -69,7 +69,8 @@ def update_state_wrapper(update_state_fn):\n                 raise ValueError(\n                     \"Trying to run metric.update_state in replica context when \"\n                     \"the metric was not created in TPUStrategy scope. \"\n-                    \"Make sure the keras Metric is created in TPUstrategy scope. \"\n+                    \"Make sure the keras Metric is created in TPUstrategy \"\n+                    \"scope. \"\n                 )\n \n         with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\n@@ -104,28 +105,30 @@ def result_wrapper(result_fn):\n         \"\"\"Decorated function with merge_call.\"\"\"\n         replica_context = tf.distribute.get_replica_context()\n \n-        # The purpose of using `merge_call` to call `result()` is to trigger cross\n-        # replica aggregation of metric state variables (SyncOnReadVariable). After\n-        # we introduced `variable_sync_on_read_context`, in principle there is no\n-        # need to use `merge_call` here. However the branch still exists because:\n+        # The purpose of using `merge_call` to call `result()` is to trigger\n+        # cross replica aggregation of metric state variables\n+        # (SyncOnReadVariable). After we introduced\n+        # `variable_sync_on_read_context`, in principle there is no need to use\n+        # `merge_call` here. However the branch still exists because:\n         #\n-        # 1. Keras V1 training code sometimes assumes `result_t` is the same tensor\n-        #    across replicas (achieved by `merge_call`). With\n+        # 1. Keras V1 training code sometimes assumes `result_t` is the same\n+        #    tensor across replicas (achieved by `merge_call`). With\n         #    `variable_sync_on_read_context` each replica gets their own tensors\n         #    residing on replica's device, thus breaking the assumption.\n-        # 2. Keras c/fit creates a tf.function (a.k.a, train_function) that returns\n-        #    the metric values of the first replica. With\n+        # 2. Keras c/fit creates a tf.function (a.k.a, train_function) that\n+        #    returns the metric values of the first replica. With\n         #    `variable_sync_on_read_context` since each replica gets their own\n-        #    tensors, the metric result tensors on the non-first replicas are not in\n-        #    the return value of train_function, making TF graph optimizer prune the\n-        #    branch that computes and aggregates those metric results. As a result,\n-        #    if NCCL is used to do the aggregation, the program will hang because\n-        #    NCCL ops are only launched on the non-pruned first replica.\n+        #    tensors, the metric result tensors on the non-first replicas are\n+        #    not in the return value of train_function, making TF graph\n+        #    optimizer prune the branch that computes and aggregates those\n+        #    metric results. As a result, if NCCL is used to do the aggregation,\n+        #    the program will hang because NCCL ops are only launched on the\n+        #    non-pruned first replica.\n         #\n-        # We condition on strategy_supports_no_merge_call() since we know if it is\n-        # True, the program uses `jit_compile` to compile replica fn, meaning it is\n-        # not V1 training (hence #1 is okay), and no pruning will happen as\n-        # compiled functions are not inlined (hence #2 is okay).\n+        # We condition on strategy_supports_no_merge_call() since we know if it\n+        # is True, the program uses `jit_compile` to compile replica fn, meaning\n+        # it is not V1 training (hence #1 is okay), and no pruning will happen\n+        # as compiled functions are not inlined (hence #2 is okay).\n         if (\n             replica_context is None\n             or tf.__internal__.distribute.strategy_supports_no_merge_call()\n@@ -146,38 +149,42 @@ def result_wrapper(result_fn):\n                         result_t = tf.identity(raw_result)\n                     except (ValueError, TypeError):\n                         raise RuntimeError(\n-                            \"The output of `metric.result()` can only be a single \"\n-                            \"Tensor/Variable, or a dict of Tensors/Variables. \"\n-                            f\"For metric {metric_obj.name}, got result {raw_result}.\"\n+                            \"The output of `metric.result()` can only be a \"\n+                            \"single Tensor/Variable, or a dict of \"\n+                            \"Tensors/Variables. \"\n+                            f\"For metric {metric_obj.name}, \"\n+                            f\"got result {raw_result}.\"\n                         )\n         else:\n-            # TODO(psv): Test distribution of metrics using different distribution\n-            # strategies.\n+            # TODO(psv): Test distribution of metrics using different\n+            # distribution strategies.\n \n-            # Creating a wrapper for merge_fn. merge_call invokes the given merge_fn\n-            # with distribution object as the first parameter. We create a wrapper\n-            # here so that the result function need not have that parameter.\n+            # Creating a wrapper for merge_fn. merge_call invokes the given\n+            # merge_fn with distribution object as the first parameter. We\n+            # create a wrapper here so that the result function need not have\n+            # that parameter.\n             def merge_fn_wrapper(distribution, merge_fn, *args):\n-                # We will get `PerReplica` merge function. Taking the first one as all\n-                # are identical copies of the function that we had passed below.\n+                # We will get `PerReplica` merge function. Taking the first one\n+                # as all are identical copies of the function that we had passed\n+                # below.\n                 result = distribution.experimental_local_results(merge_fn)[0](\n                     *args\n                 )\n \n                 # Wrapping result in identity so that control dependency between\n-                # update_op from `update_state` and result works in case result returns\n-                # a tensor.\n+                # update_op from `update_state` and result works in case result\n+                # returns a tensor.\n                 return tf.identity(result)\n \n-            # Wrapping result in merge_call. merge_call is used when we want to leave\n-            # replica mode and compute a value in cross replica mode.\n+            # Wrapping result in merge_call. merge_call is used when we want to\n+            # leave replica mode and compute a value in cross replica mode.\n             result_t = replica_context.merge_call(\n                 merge_fn_wrapper, args=(result_fn,) + args\n             )\n \n         # We are saving the result op here to be used in train/test execution\n-        # functions. This basically gives the result op that was generated with a\n-        # control dep to the updates for these workflows.\n+        # functions. This basically gives the result op that was generated with\n+        # a control dep to the updates for these workflows.\n         metric_obj._call_result = result_t\n         return result_t\n \n@@ -206,7 +213,8 @@ def assert_thresholds_range(thresholds):\n         ]\n         if invalid_thresholds:\n             raise ValueError(\n-                f\"Threshold values must be in [0, 1]. Received: {invalid_thresholds}\"\n+                f\"Threshold values must be in [0, 1]. \"\n+                f\"Received: {invalid_thresholds}\"\n             )\n \n \n@@ -337,10 +345,10 @@ def _update_confusion_matrix_variables_optimized(\n     tp_bucket_value = tf.math.unsorted_segment_sum(true_labels, bucket_indices,\n                                                    num_segments=num_thresholds)\n                     = [1, 1, 0]\n-    # For [1, 1, 0] here, it means there is 1 true value contributed by bucket 0,\n-    # and 1 value contributed by bucket 1. When we aggregate them to together,\n-    # the result become [a + b + c, b + c, c], since large thresholds will always\n-    # contribute to the value for smaller thresholds.\n+    # For [1, 1, 0] here, it means there is 1 true value contributed by bucket\n+    # 0, and 1 value contributed by bucket 1. When we aggregate them to\n+    # together, the result become [a + b + c, b + c, c], since large thresholds\n+    # will always contribute to the value for smaller thresholds.\n     true_positive = tf.math.cumsum(tp_bucket_value, reverse=True)\n                   = [2, 1, 0]\n \n@@ -352,27 +360,31 @@ def _update_confusion_matrix_variables_optimized(\n     Args:\n       variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n         and corresponding variables to update as values.\n-      y_true: A floating point `Tensor` whose shape matches `y_pred`. Will be cast\n-        to `bool`.\n-      y_pred: A floating point `Tensor` of arbitrary shape and whose values are in\n-        the range `[0, 1]`.\n+      y_true: A floating point `Tensor` whose shape matches `y_pred`. Will be\n+        cast to `bool`.\n+      y_pred: A floating point `Tensor` of arbitrary shape and whose values are\n+        in the range `[0, 1]`.\n       thresholds: A sorted floating point `Tensor` with value in `[0, 1]`.\n-        It need to be evenly distributed (the diff between each element need to be\n-        the same).\n+        It need to be evenly distributed (the diff between each element need to\n+        be the same).\n       multi_label: Optional boolean indicating whether multidimensional\n-        prediction/labels should be treated as multilabel responses, or flattened\n-        into a single label. When True, the valus of `variables_to_update` must\n-        have a second dimension equal to the number of labels in y_true and\n-        y_pred, and those tensors must not be RaggedTensors.\n+        prediction/labels should be treated as multilabel responses, or\n+        flattened into a single label. When True, the valus of\n+        `variables_to_update` must have a second dimension equal to the number\n+        of labels in y_true and y_pred, and those tensors must not be\n+        RaggedTensors.\n       sample_weights: Optional `Tensor` whose rank is either 0, or the same rank\n         as `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions\n-        must be either `1`, or the same as the corresponding `y_true` dimension).\n+        must be either `1`, or the same as the corresponding `y_true`\n+        dimension).\n       label_weights: Optional tensor of non-negative weights for multilabel\n-        data. The weights are applied when calculating TP, FP, FN, and TN without\n-        explicit multilabel handling (i.e. when the data is to be flattened).\n-      thresholds_with_epsilon: Optional boolean indicating whether the leading and\n-        tailing thresholds has any epsilon added for floating point imprecisions.\n-        It will change how we handle the leading and tailing bucket.\n+        data. The weights are applied when calculating TP, FP, FN, and TN\n+        without explicit multilabel handling (i.e. when the data is to be\n+        flattened).\n+      thresholds_with_epsilon: Optional boolean indicating whether the leading\n+        and tailing thresholds has any epsilon added for floating point\n+        imprecisions.  It will change how we handle the leading and tailing\n+        bucket.\n \n     Returns:\n       Update op.\n@@ -427,7 +439,8 @@ def _update_confusion_matrix_variables_optimized(\n     if multi_label:\n         # We need to run bucket segment sum for each of the label class. In the\n         # multi_label case, the rank of the label is 2. We first transpose it so\n-        # that the label dim becomes the first and we can parallel run though them.\n+        # that the label dim becomes the first and we can parallel run though\n+        # them.\n         true_labels = tf.transpose(true_labels)\n         false_labels = tf.transpose(false_labels)\n         bucket_indices = tf.transpose(bucket_indices)\n@@ -504,8 +517,8 @@ def is_evenly_distributed_thresholds(thresholds):\n     evaluated.\n \n     Args:\n-      thresholds: A python list or tuple, or 1D numpy array whose value is ranged\n-        in [0, 1].\n+      thresholds: A python list or tuple, or 1D numpy array whose value is\n+        ranged in [0, 1].\n \n     Returns:\n       boolean, whether the values in the inputs are evenly distributed.\n@@ -541,11 +554,11 @@ def update_confusion_matrix_variables(\n     true_negatives: y_true == False and y_pred <= thresholds\n     false_positive: y_true == False and y_pred > thresholds\n \n-    The results will be weighted and added together. When multiple thresholds are\n-    provided, we will repeat the same for every threshold.\n+    The results will be weighted and added together. When multiple thresholds\n+    are provided, we will repeat the same for every threshold.\n \n-    For estimation of these metrics over a stream of data, the function creates an\n-    `update_op` operation that updates the given variables.\n+    For estimation of these metrics over a stream of data, the function creates\n+    an `update_op` operation that updates the given variables.\n \n     If `sample_weight` is `None`, weights default to 1.\n     Use weights of 0 to mask values.\n@@ -554,25 +567,28 @@ def update_confusion_matrix_variables(\n       variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n         and corresponding variables to update as values.\n       y_true: A `Tensor` whose shape matches `y_pred`. Will be cast to `bool`.\n-      y_pred: A floating point `Tensor` of arbitrary shape and whose values are in\n-        the range `[0, 1]`.\n+      y_pred: A floating point `Tensor` of arbitrary shape and whose values are\n+        in the range `[0, 1]`.\n       thresholds: A float value, float tensor, python list, or tuple of float\n         thresholds in `[0, 1]`, or NEG_INF (used when top_k is set).\n-      top_k: Optional int, indicates that the positive labels should be limited to\n-        the top k predictions.\n+      top_k: Optional int, indicates that the positive labels should be limited\n+        to the top k predictions.\n       class_id: Optional int, limits the prediction and labels to the class\n         specified by this argument.\n-      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n-        `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions must\n-        be either `1`, or the same as the corresponding `y_true` dimension).\n+      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank\n+        as `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions\n+        must be either `1`, or the same as the corresponding `y_true`\n+        dimension).\n       multi_label: Optional boolean indicating whether multidimensional\n-        prediction/labels should be treated as multilabel responses, or flattened\n-        into a single label. When True, the valus of `variables_to_update` must\n-        have a second dimension equal to the number of labels in y_true and\n-        y_pred, and those tensors must not be RaggedTensors.\n+        prediction/labels should be treated as multilabel responses, or\n+        flattened into a single label. When True, the valus of\n+        `variables_to_update` must have a second dimension equal to the number\n+        of labels in y_true and y_pred, and those tensors must not be\n+        RaggedTensors.\n       label_weights: (optional) tensor of non-negative weights for multilabel\n-        data. The weights are applied when calculating TP, FP, FN, and TN without\n-        explicit multilabel handling (i.e. when the data is to be flattened).\n+        data. The weights are applied when calculating TP, FP, FN, and TN\n+        without explicit multilabel handling (i.e. when the data is to be\n+        flattened).\n       thresholds_distributed_evenly: Boolean, whether the thresholds are evenly\n         distributed within the list. An optimized method will be used if this is\n         the case. See _update_confusion_matrix_variables_optimized() for more\n@@ -583,8 +599,8 @@ def update_confusion_matrix_variables(\n \n     Raises:\n       ValueError: If `y_pred` and `y_true` have mismatched shapes, or if\n-        `sample_weight` is not `None` and its shape doesn't match `y_pred`, or if\n-        `variables_to_update` contains invalid keys.\n+        `sample_weight` is not `None` and its shape doesn't match `y_pred`, or\n+        if `variables_to_update` contains invalid keys.\n     \"\"\"\n     if multi_label and label_weights is not None:\n         raise ValueError(\n@@ -600,7 +616,8 @@ def update_confusion_matrix_variables(\n         raise ValueError(\n             \"Please provide at least one valid confusion matrix \"\n             \"variable to update. Valid variable key options are: \"\n-            f'\"{list(ConfusionMatrix)}\". Received: \"{variables_to_update.keys()}\"'\n+            f'\"{list(ConfusionMatrix)}\". '\n+            f'Received: \"{variables_to_update.keys()}\"'\n         )\n \n     variable_dtype = list(variables_to_update.values())[0].dtype\n@@ -610,10 +627,10 @@ def update_confusion_matrix_variables(\n \n     if thresholds_distributed_evenly:\n         # Check whether the thresholds has any leading or tailing epsilon added\n-        # for floating point imprecision. The leading and tailing threshold will be\n-        # handled bit differently as the corner case.\n-        # At this point, thresholds should be a list/array with more than 2 items,\n-        # and ranged between [0, 1]. See is_evenly_distributed_thresholds() for more\n+        # for floating point imprecision. The leading and tailing threshold will\n+        # be handled bit differently as the corner case.  At this point,\n+        # thresholds should be a list/array with more than 2 items, and ranged\n+        # between [0, 1]. See is_evenly_distributed_thresholds() for more\n         # details.\n         thresholds_with_epsilon = thresholds[0] < 0.0 or thresholds[-1] > 1.0\n \n@@ -782,8 +799,8 @@ def update_confusion_matrix_variables(\n def _filter_top_k(x, k):\n     \"\"\"Filters top-k values in the last dim of x and set the rest to NEG_INF.\n \n-    Used for computing top-k prediction values in dense labels (which has the same\n-    shape as predictions) for recall and precision top-k metrics.\n+    Used for computing top-k prediction values in dense labels (which has the\n+    same shape as predictions) for recall and precision top-k metrics.\n \n     Args:\n       x: tensor with any dimensions.\n@@ -814,7 +831,8 @@ def ragged_assert_compatible_and_get_flat_values(values, mask=None):\n     Returns:\n        A tuple in which the first element is the list of tensors and the second\n        is the mask tensor. ([Values], mask). Mask and the element in Values\n-       are equal to the flat_values of the input arguments (if they were ragged).\n+       are equal to the flat_values of the input arguments (if they were\n+       ragged).\n     \"\"\"\n     if isinstance(values, list):\n         is_all_ragged = all(isinstance(rt, tf.RaggedTensor) for rt in values)\n@@ -829,12 +847,13 @@ def ragged_assert_compatible_and_get_flat_values(values, mask=None):\n             to_be_stripped = True\n \n         # NOTE: we leave the flat_values compatibility to\n-        # tf.TensorShape `assert_is_compatible_with`\n-        # check if both dynamic dimensions are equal and then use the flat_values.\n+        # tf.TensorShape `assert_is_compatible_with` check if both dynamic\n+        # dimensions are equal and then use the flat_values.\n         nested_row_split_list = [rt.nested_row_splits for rt in values]\n         assertion_list = _assert_splits_match(nested_row_split_list)\n \n-        # if both are ragged sample_weights also should be ragged with same dims.\n+        # if both are ragged sample_weights also should be ragged with same\n+        # dims.\n         if isinstance(mask, tf.RaggedTensor):\n             assertion_list_for_mask = _assert_splits_match(\n                 [nested_row_split_list[0], mask.nested_row_splits]\n@@ -873,9 +892,9 @@ def _assert_splits_match(nested_splits_lists):\n     fully identical.\n \n     Args:\n-      nested_splits_lists: A list of nested_splits_lists, where each split_list is\n-        a list of `splits` tensors from a `RaggedTensor`, ordered from outermost\n-        ragged dimension to innermost ragged dimension.\n+      nested_splits_lists: A list of nested_splits_lists, where each split_list\n+        is a list of `splits` tensors from a `RaggedTensor`, ordered from\n+        outermost ragged dimension to innermost ragged dimension.\n \n     Returns:\n       A list of control dependency op tensors.\n@@ -904,8 +923,8 @@ def binary_matches(y_true, y_pred, threshold=0.5):\n     Args:\n       y_true: Ground truth values, of shape (batch_size, d0, .. dN).\n       y_pred: The predicted values, of shape (batch_size, d0, .. dN).\n-      threshold: (Optional) Float representing the threshold for deciding whether\n-        prediction values are 1 or 0.\n+      threshold: (Optional) Float representing the threshold for deciding\n+        whether prediction values are 1 or 0.\n \n     Returns:\n       Binary matches, of shape (batch_size, d0, .. dN).\n@@ -946,8 +965,8 @@ def sparse_categorical_matches(y_true, y_pred):\n         reshape_matches = True\n     y_pred = tf.math.argmax(y_pred, axis=-1)\n \n-    # If the predicted output and actual output types don't match, force cast them\n-    # to match.\n+    # If the predicted output and actual output types don't match, force cast\n+    # them to match.\n     if backend.dtype(y_pred) != backend.dtype(y_true):\n         y_pred = tf.cast(y_pred, backend.dtype(y_true))\n     matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n@@ -957,7 +976,8 @@ def sparse_categorical_matches(y_true, y_pred):\n \n \n def sparse_top_k_categorical_matches(y_true, y_pred, k=5):\n-    \"\"\"Creates float Tensor, 1.0 for label-TopK_prediction match, 0.0 for mismatch.\n+    \"\"\"Creates float Tensor, 1.0 for label-TopK_prediction match, 0.0 for\n+    mismatch.\n \n     Args:\n       y_true: tensor of true targets.\n\n@@ -199,8 +199,8 @@ class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n     # we do not support such cases that ragged_ranks are different but overall\n-    # dimension shapes and sizes are identical due to adding too much performance\n-    # overheads to the overall use cases.\n+    # dimension shapes and sizes are identical due to adding too much\n+    # performance overheads to the overall use cases.\n     def test_failing_different_ragged_ranks(self):\n         dt = tf.constant([[[1, 2]]])\n         # adding a ragged dimension\n@@ -308,8 +308,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n         y_pred = tf.constant(np.random.random((6, 7)))\n         self.assertEqual(matches_method(y_true, y_pred).dtype, backend.floatx())\n \n-        # Tests that resulting Tensor always has same shape as y_true. Tests from\n-        # 1 dim to 4 dims\n+        # Tests that resulting Tensor always has same shape as y_true. Tests\n+        # from 1 dim to 4 dims\n         dims = []\n         for _ in range(4):\n             dims.append(np.random.randint(1, 7))\n@@ -331,8 +331,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n             matches_method(y_true, y_pred), [[0.0], [1.0], [1.0], [1.0]]\n         )\n \n-        # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-        # y_pred is (batch_size, seq_length, num_classes)\n+        # Test correctness if the shape of y_true is (batch_size, seq_length)\n+        # and y_pred is (batch_size, seq_length, num_classes)\n         y_pred = tf.constant(\n             [\n                 [[0.2, 0.3, 0.1], [0.1, 0.2, 0.7]],\n@@ -354,8 +354,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n             matches_method(y_true, y_pred, 1).dtype, backend.floatx()\n         )\n \n-        # Tests that resulting Tensor always has same shape as y_true. Tests from\n-        # 1 dim to 4 dims\n+        # Tests that resulting Tensor always has same shape as y_true. Tests\n+        # from 1 dim to 4 dims\n         dims = []\n         for _ in range(4):\n             dims.append(np.random.randint(1, 7))\n@@ -365,7 +365,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n                 matches_method(y_true, y_pred, 1).shape, y_true.shape\n             )\n \n-        # Test correctness if the shape of y_true is (num_samples,) for k = 1,2,3\n+        # Test correctness if the shape of y_true is (num_samples,) for k =\n+        # 1,2,3\n         y_true = tf.constant([1.0, 0.0, 0.0, 0.0])\n         y_pred = tf.constant(\n             [[0.7, 0.2, 0.1], [0.5, 0.3, 0.2], [0.6, 0.3, 0.1], [0.0, 0.1, 0.9]]\n@@ -396,8 +397,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n             matches_method(y_true, y_pred, 3), [[1.0], [1.0], [1.0], [1.0]]\n         )\n \n-        # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-        # y_pred is (batch_size, seq_length, num_classes) for k = 1,2,3\n+        # Test correctness if the shape of y_true is (batch_size, seq_length)\n+        # and y_pred is (batch_size, seq_length, num_classes) for k = 1,2,3\n         y_pred = tf.constant(\n             [\n                 [[0.2, 0.3, 0.1], [0.1, 0.2, 0.7]],\n@@ -425,8 +426,8 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n             matches_method(y_true, y_pred, 0.5).dtype, backend.floatx()\n         )\n \n-        # Tests that resulting Tensor always has same shape as y_true. Tests from\n-        # 1 dim to 4 dims.\n+        # Tests that resulting Tensor always has same shape as y_true. Tests\n+        # from 1 dim to 4 dims.\n         dims = []\n         for _ in range(4):\n             dims.append(np.random.randint(1, 7))\n\n@@ -40,7 +40,8 @@ class _ObjectIdentityWrapper:\n         if not isinstance(other, _ObjectIdentityWrapper):\n             raise TypeError(\n                 \"Cannot compare wrapped object with unwrapped object. \"\n-                f\"Expect the object to be `_ObjectIdentityWrapper`. Got: {other}\"\n+                f\"Expect the object to be `_ObjectIdentityWrapper`. \"\n+                f\"Got: {other}\"\n             )\n \n     def __lt__(self, other):\n\n@@ -53,9 +53,9 @@ def text_dataset_from_directory(\n     ......b_text_2.txt\n     ```\n \n-    Then calling `text_dataset_from_directory(main_directory, labels='inferred')`\n-    will return a `tf.data.Dataset` that yields batches of texts from\n-    the subdirectories `class_a` and `class_b`, together with labels\n+    Then calling `text_dataset_from_directory(main_directory,\n+    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n+    texts from the subdirectories `class_a` and `class_b`, together with labels\n     0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n \n     Only `.txt` files are supported at this time.\n@@ -124,10 +124,11 @@ def text_dataset_from_directory(\n     if labels not in (\"inferred\", None):\n         if not isinstance(labels, (list, tuple)):\n             raise ValueError(\n-                \"`labels` argument should be a list/tuple of integer labels, of \"\n-                \"the same size as the number of text files in the target \"\n-                \"directory. If you wish to infer the labels from the subdirectory \"\n-                'names in the target directory, pass `labels=\"inferred\"`. '\n+                \"`labels` argument should be a list/tuple of integer labels, \"\n+                \"of the same size as the number of text files in the target \"\n+                \"directory. If you wish to infer the labels from the \"\n+                \"subdirectory names in the target directory, \"\n+                'pass `labels=\"inferred\"`. '\n                 \"If you wish to get a dataset that only contains text samples \"\n                 f\"(no labels), pass `labels=None`. Received: labels={labels}\"\n             )\n@@ -139,7 +140,8 @@ def text_dataset_from_directory(\n             )\n     if label_mode not in {\"int\", \"categorical\", \"binary\", None}:\n         raise ValueError(\n-            '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n+            '`label_mode` argument must be one of \"int\", '\n+            '\"categorical\", \"binary\", '\n             f\"or None. Received: label_mode={label_mode}\"\n         )\n     if labels is None or label_mode is None:\n\n@@ -69,8 +69,8 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n         return temp_dir\n \n     def test_text_dataset_from_directory_standalone(self):\n-        # Test retrieving txt files without labels from a directory and its subdirs.\n-        # Save a few extra files in the parent directory.\n+        # Test retrieving txt files without labels from a directory and its\n+        # subdirs. Save a few extra files in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i in range(3):\n             filename = \"text_%s.txt\" % (i,)\n\n@@ -173,7 +173,8 @@ def _get_argspec_for_partial(obj):\n     # When callable is a functools.partial object, we construct its ArgSpec with\n     # following strategy:\n     # - If callable partial contains default value for positional arguments (ie.\n-    # object.args), then final ArgSpec doesn't contain those positional arguments.\n+    # object.args), then final ArgSpec doesn't contain those positional\n+    # arguments.\n     # - If callable partial contains default value for keyword arguments (ie.\n     # object.keywords), then we merge them with wrapped target. Default values\n     # from callable partial takes precedence over those from wrapped target.\n@@ -192,12 +193,13 @@ def _get_argspec_for_partial(obj):\n     #   return 2 * m + n\n     # partialed = functools.partial(func, m=1)\n     #\n-    # This example will result in m having a default value but n doesn't. This is\n-    # usually not allowed in Python and can not be expressed in ArgSpec correctly.\n+    # This example will result in m having a default value but n doesn't. This\n+    # is usually not allowed in Python and can not be expressed in ArgSpec\n+    # correctly.\n     #\n-    # Thus, we must detect cases like this by finding first argument with default\n-    # value and ensures all following arguments also have default values. When\n-    # this is not true, a ValueError is raised.\n+    # Thus, we must detect cases like this by finding first argument with\n+    # default value and ensures all following arguments also have default\n+    # values. When this is not true, a ValueError is raised.\n \n     n_prune_args = len(obj.args)\n     partial_keywords = obj.keywords or {}\n@@ -287,9 +289,9 @@ def getcallargs(*func_and_positional, **named):\n       A dictionary mapping `func`'s named arguments to the values they would\n       receive if `func(*positional, **named)` were called.\n \n-    `getcallargs` will use the argspec from the outermost decorator that provides\n-    it. If no attached decorators modify argspec, the final unwrapped target's\n-    argspec will be used.\n+    `getcallargs` will use the argspec from the outermost decorator that\n+    provides it. If no attached decorators modify argspec, the final unwrapped\n+    target's argspec will be used.\n     \"\"\"\n     func = func_and_positional[0]\n     positional = func_and_positional[1:]\n\n@@ -33,10 +33,11 @@ from keras.utils import tf_contextlib\n def set_random_seed(seed):\n     \"\"\"Sets all random seeds for the program (Python, NumPy, and TensorFlow).\n \n-    You can use this utility to make almost any Keras program fully deterministic.\n-    Some limitations apply in cases where network communications are involved\n-    (e.g. parameter server distribution), which creates additional sources of\n-    randomness, or when certain non-deterministic cuDNN ops are involved.\n+    You can use this utility to make almost any Keras program fully\n+    deterministic. Some limitations apply in cases where network communications\n+    are involved (e.g. parameter server distribution), which creates additional\n+    sources of randomness, or when certain non-deterministic cuDNN ops are\n+    involved.\n \n     Calling this utility is equivalent to the following:\n \n@@ -85,7 +86,8 @@ def get_reachable_from_inputs(inputs, targets=None):\n       targets: List of tensors.\n \n     Returns:\n-      A set of tensors reachable from the inputs (includes the inputs themselves).\n+      A set of tensors reachable from the inputs (includes the inputs\n+      themselves).\n     \"\"\"\n     inputs = tf.nest.flatten(inputs, expand_composites=True)\n     reachable = object_identity.ObjectIdentitySet(inputs)\n@@ -114,7 +116,8 @@ def get_reachable_from_inputs(inputs, targets=None):\n             outputs = x.consumers()\n         else:\n             raise TypeError(\n-                f\"Expected tf.Operation, tf.Variable, or tf.Tensor. Received: {x}\"\n+                f\"Expected tf.Operation, tf.Variable, or tf.Tensor. \"\n+                f\"Received: {x}\"\n             )\n \n         for y in outputs:\n@@ -194,9 +197,10 @@ def convert_shapes(input_shape, to_tuples=True):\n     - None\n \n     Args:\n-      input_shape: A nested structure of objects to be converted to TensorShapes.\n-      to_tuples: If `True`, converts all TensorShape to tuples. Otherwise converts\n-        all tuples representing shapes to TensorShapes.\n+      input_shape: A nested structure of objects to be converted to\n+        TensorShapes.\n+      to_tuples: If `True`, converts all TensorShape to tuples. Otherwise\n+        converts all tuples representing shapes to TensorShapes.\n \n     Returns:\n       Nested structure of shapes in desired format.\n@@ -283,12 +287,13 @@ class ListWrapper:\n \n \n def convert_inner_node_data(nested, wrap=False):\n-    \"\"\"Either wraps or unwraps innermost node data lists in `ListWrapper` objects.\n+    \"\"\"Either wraps or unwraps innermost node data lists in `ListWrapper`\n+    objects.\n \n     Args:\n       nested: A nested data structure.\n-      wrap: If `True`, wrap innermost lists in `ListWrapper` objects. If `False`,\n-        unwraps `ListWrapper` objects into lists.\n+      wrap: If `True`, wrap innermost lists in `ListWrapper` objects. If\n+        `False`, unwraps `ListWrapper` objects into lists.\n \n     Returns:\n       Structure of same type as nested, with lists wrapped/unwrapped.\n@@ -382,7 +387,8 @@ def is_extension_type(tensor):\n \n \n def is_symbolic_tensor(tensor):\n-    \"\"\"Returns whether a tensor is symbolic (from a TF graph) or an eager tensor.\n+    \"\"\"Returns whether a tensor is symbolic (from a TF graph) or an eager\n+    tensor.\n \n     A Variable can be seen as either: it is considered symbolic\n     when we are in a graph scope, and eager when we are in an eager scope.\n@@ -508,8 +514,8 @@ def assert_no_legacy_layers(layers):\n             \"To use keras as a \"\n             \"framework (for instance using the Network, Model, or Sequential \"\n             \"classes), please use the tf.keras.layers implementation instead. \"\n-            \"(Or, if writing custom layers, subclass from tf.keras.layers rather \"\n-            \"than tf.layers)\"\n+            \"(Or, if writing custom layers, subclass from tf.keras.layers \"\n+            \"rather than tf.layers)\"\n         )\n \n \n@@ -535,7 +541,8 @@ def maybe_init_scope(layer):\n \n @tf_contextlib.contextmanager\n def graph_context_for_symbolic_tensors(*args, **kwargs):\n-    \"\"\"Returns graph context manager if any of the inputs is a symbolic tensor.\"\"\"\n+    \"\"\"Returns graph context manager if any of the inputs is a symbolic\n+    tensor.\"\"\"\n     if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\n         with backend.get_graph().as_default():\n             yield\n@@ -593,14 +600,15 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n \n \n def sync_to_numpy_or_python_type(tensors):\n-    \"\"\"Syncs and converts a structure of `Tensor`s to `NumPy` arrays or Python scalar types.\n+    \"\"\"Syncs and converts a structure of `Tensor`s to `NumPy` arrays or Python\n+    scalar types.\n \n     For each tensor, it calls `tensor.numpy()`. If the result is a scalar value,\n     it converts it to a Python type, such as a float or int, by calling\n     `result.item()`.\n \n-    Numpy scalars are converted, as Python types are often more convenient to deal\n-    with. This is especially useful for bfloat16 Numpy scalars, which don't\n+    Numpy scalars are converted, as Python types are often more convenient to\n+    deal with. This is especially useful for bfloat16 Numpy scalars, which don't\n     support as many operations as other Numpy values.\n \n     Async strategies (such as `TPUStrategy` and `ParameterServerStrategy`) are\n@@ -621,7 +629,8 @@ def sync_to_numpy_or_python_type(tensors):\n         # Don't turn ragged or sparse tensors to NumPy.\n         if isinstance(t, tf.Tensor):\n             t = t.numpy()\n-        # Strings, ragged and sparse tensors don't have .item(). Return them as-is.\n+        # Strings, ragged and sparse tensors don't have .item(). Return them\n+        # as-is.\n         if not isinstance(t, (np.ndarray, np.generic)):\n             return t\n         return t.item() if np.ndim(t) == 0 else t\n\n@@ -115,7 +115,8 @@ def timeseries_dataset_from_array(\n     for batch in dataset:\n       inputs, targets = batch\n       assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]\n-      assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10\n+      # Corresponding target: step 10\n+      assert np.array_equal(targets[0], data[10])\n       break\n     ```\n \n@@ -206,7 +207,8 @@ def timeseries_dataset_from_array(\n     if end_index is None:\n         end_index = len(data)\n \n-    # Determine the lowest dtype to store start positions (to lower memory usage).\n+    # Determine the lowest dtype to store start positions (to lower memory\n+    # usage).\n     num_seqs = end_index - start_index - (sequence_length * sampling_rate) + 1\n     if targets is not None:\n         num_seqs = min(num_seqs, len(targets))\n\n@@ -102,7 +102,8 @@ class TimeseriesDatasetTest(tf.test.TestCase):\n             self.assertNotAllClose(x, np.arange(0, 5))\n             self.assertAllClose(x[:, 0] * 2, y)\n             first_seq = x\n-        # Check that a new iteration with the same dataset yields different results\n+        # Check that a new iteration with the same dataset yields different\n+        # results\n         for x, _ in dataset.take(1):\n             self.assertNotAllClose(x, first_seq)\n         # Check determism with same seed\n\n@@ -51,7 +51,8 @@ def _process_traceback_frames(tb):\n \n \n def filter_traceback(fn):\n-    \"\"\"Filter out Keras-internal stack trace frames in exceptions raised by fn.\"\"\"\n+    \"\"\"Filter out Keras-internal stack trace frames in exceptions raised by\n+    fn.\"\"\"\n     if sys.version_info.major != 3 or sys.version_info.minor < 7:\n         return fn\n \n@@ -123,8 +124,8 @@ def inject_argument_info_in_traceback(fn, object_name=None):\n                 if isinstance(e, tf.errors.OpError):\n                     message = e.message\n                 elif e.args:\n-                    # Canonically, the 1st argument in an exception is the error message.\n-                    # This works for all built-in Python exceptions.\n+                    # Canonically, the 1st argument in an exception is the error\n+                    # message.  This works for all built-in Python exceptions.\n                     message = e.args[0]\n                 else:\n                     message = \"\"\n@@ -141,10 +142,12 @@ def inject_argument_info_in_traceback(fn, object_name=None):\n                     new_e = e.__class__(e.node_def, e.op, message, e.error_code)\n                 else:\n                     try:\n-                        # For standard exceptions such as ValueError, TypeError, etc.\n+                        # For standard exceptions such as ValueError, TypeError,\n+                        # etc.\n                         new_e = e.__class__(message)\n                     except TypeError:\n-                        # For any custom error that doesn't have a standard signature.\n+                        # For any custom error that doesn't have a standard\n+                        # signature.\n                         new_e = RuntimeError(message)\n                 new_e._keras_call_info_injected = (\n                     True  # pylint: disable=protected-access\n\n@@ -70,8 +70,8 @@ class TensorBoardVersionSelector:\n             start_cls == callbacks_v1.TensorBoard\n             and cls == callbacks.TensorBoard\n         ):\n-            # Since the v2 class is not a subclass of the v1 class, __init__ has to\n-            # be called manually.\n+            # Since the v2 class is not a subclass of the v1 class, __init__ has\n+            # to be called manually.\n             return cls(*args, **kwargs)\n         return super(TensorBoardVersionSelector, cls).__new__(cls)\n \n@@ -109,8 +109,8 @@ def swap_class(cls, v2_cls, v1_cls, use_v2):\n             # `v1_cls` often extends `v2_cls`, so it may still call `swap_class`\n             # even if it doesn't need to. That being said, it may be the safest\n             # not to over optimize this logic for the sake of correctness,\n-            # especially if we swap v1 & v2 classes that don't extend each other,\n-            # or when the inheritance order is different.\n+            # especially if we swap v1 & v2 classes that don't extend each\n+            # other, or when the inheritance order is different.\n             or (not use_v2 and issubclass(base, v2_cls))\n         ):\n             new_base = swap_class(base, v2_cls, v1_cls, use_v2)\n@@ -124,10 +124,11 @@ def swap_class(cls, v2_cls, v1_cls, use_v2):\n def disallow_legacy_graph(cls_name, method_name):\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n         error_msg = (\n-            f\"Calling `{cls_name}.{method_name}` in graph mode is not supported \"\n-            f\"when the `{cls_name}` instance was constructed with eager mode \"\n-            f\"enabled. Please construct your `{cls_name}` instance in graph mode or\"\n-            f\" call `{cls_name}.{method_name}` with eager mode enabled.\"\n+            f\"Calling `{cls_name}.{method_name}` in graph mode is not \"\n+            f\"supported when the `{cls_name}` instance was constructed with \"\n+            f\"eager mode enabled. Please construct your `{cls_name}` instance \"\n+            f\"in graph mode or call `{cls_name}.{method_name}` with \"\n+            \"eager mode enabled.\"\n         )\n         raise ValueError(error_msg)\n \n\n@@ -424,13 +424,14 @@ def plot_model(\n       expand_nested: Whether to expand nested models into clusters.\n       dpi: Dots per inch.\n       layer_range: input of `list` containing two `str` items, which is the\n-        starting layer name and ending layer name (both inclusive) indicating the\n-        range of layers for which the plot will be generated. It also accepts\n-        regex patterns instead of exact name. In such case, start predicate will\n-        be the first element it matches to `layer_range[0]` and the end predicate\n-        will be the last element it matches to `layer_range[1]`. By default `None`\n-        which considers all layers of model. Note that you must pass range such\n-        that the resultant subgraph must be complete.\n+        starting layer name and ending layer name (both inclusive) indicating\n+        the range of layers for which the plot will be generated. It also\n+        accepts regex patterns instead of exact name. In such case, start\n+        predicate will be the first element it matches to `layer_range[0]` and\n+        the end predicate will be the last element it matches to\n+        `layer_range[1]`. By default `None` which considers all layers of model.\n+        Note that you must pass range such that the resultant subgraph must be\n+        complete.\n       show_layer_activations: Display layer activations (only for layers that\n         have an `activation` property).\n \n@@ -458,8 +459,8 @@ def plot_model(\n             \"for plot_model to work.\"\n         )\n         if \"IPython.core.magics.namespace\" in sys.modules:\n-            # We don't raise an exception here in order to avoid crashing notebook\n-            # tests where graphviz is not available.\n+            # We don't raise an exception here in order to avoid crashing\n+            # notebook tests where graphviz is not available.\n             io_utils.print_msg(message)\n             return\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
