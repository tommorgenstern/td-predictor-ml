{"custom_id": "scrapy#797a6690c07ae90f7a2d703832e2fe44466d656f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 63 | Files Changed: 6 | Hunks: 30 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 126 | Churn Cumulative: 1293 | Contributors (this commit): 15 | Commits (past 90d): 12 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_df_from_crawler_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': __name__ + '.FromCrawlerRFPDupeFilter'}\n+                    'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n@@ -51,14 +51,14 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_df_from_settings_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n-                    'DUPEFILTER_CLASS': __name__ + '.FromSettingsRFPDupeFilter'}\n+                    'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, 'from_settings')\n \n     def test_df_direct_scheduler(self):\n-        settings = {'DUPEFILTER_CLASS': __name__ + '.DirectDupeFilter'}\n+        settings = {'DUPEFILTER_CLASS': DirectDupeFilter}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertEqual(scheduler.df.method, 'n/a')\n@@ -162,7 +162,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_log(self):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': False,\n-                        'DUPEFILTER_CLASS': __name__ + '.FromCrawlerRFPDupeFilter'}\n+                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             scheduler = Scheduler.from_crawler(crawler)\n             spider = SimpleSpider.from_crawler(crawler)\n@@ -191,7 +191,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_log_debug(self):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': True,\n-                        'DUPEFILTER_CLASS': __name__ + '.FromCrawlerRFPDupeFilter'}\n+                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             scheduler = Scheduler.from_crawler(crawler)\n             spider = SimpleSpider.from_crawler(crawler)\n\n@@ -193,7 +193,7 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n         self.base_settings = {\n             'LOG_LEVEL': 'DEBUG',\n             'ITEM_PIPELINES': {\n-                __name__ + '.DropSomeItemsPipeline': 300,\n+                DropSomeItemsPipeline: 300,\n             },\n         }\n \n@@ -212,7 +212,7 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n     @defer.inlineCallbacks\n     def test_skip_messages(self):\n         settings = self.base_settings.copy()\n-        settings['LOG_FORMATTER'] = __name__ + '.SkipMessagesLogFormatter'\n+        settings['LOG_FORMATTER'] = SkipMessagesLogFormatter\n         crawler = CrawlerRunner(settings).create_crawler(ItemSpider)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n\n@@ -68,7 +68,7 @@ class PipelineTestCase(unittest.TestCase):\n \n     def _create_crawler(self, pipeline_class):\n         settings = {\n-            'ITEM_PIPELINES': {__name__ + '.' + pipeline_class.__name__: 1},\n+            'ITEM_PIPELINES': {pipeline_class: 1},\n         }\n         crawler = get_crawler(ItemSpider, settings)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n\n@@ -92,7 +92,7 @@ class CrawlTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\")\n         runner = CrawlerRunner(settings={\n             \"DOWNLOADER_MIDDLEWARES\": {\n-                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n+                RaiseExceptionRequestMiddleware: 590,\n             },\n         })\n         crawler = runner.create_crawler(SingleRequestSpider)\n@@ -119,7 +119,7 @@ class CrawlTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\")\n         runner = CrawlerRunner(settings={\n             \"DOWNLOADER_MIDDLEWARES\": {\n-                __name__ + \".ProcessResponseMiddleware\": 595,\n+                ProcessResponseMiddleware: 595,\n             }\n         })\n         crawler = runner.create_crawler(SingleRequestSpider)\n@@ -149,8 +149,8 @@ class CrawlTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\")\n         runner = CrawlerRunner(settings={\n             \"DOWNLOADER_MIDDLEWARES\": {\n-                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n-                __name__ + \".CatchExceptionOverrideRequestMiddleware\": 595,\n+                RaiseExceptionRequestMiddleware: 590,\n+                CatchExceptionOverrideRequestMiddleware: 595,\n             },\n         })\n         crawler = runner.create_crawler(SingleRequestSpider)\n@@ -170,8 +170,8 @@ class CrawlTestCase(TestCase):\n         url = self.mockserver.url(\"/status?n=200\")\n         runner = CrawlerRunner(settings={\n             \"DOWNLOADER_MIDDLEWARES\": {\n-                __name__ + \".RaiseExceptionRequestMiddleware\": 590,\n-                __name__ + \".CatchExceptionDoNotOverrideRequestMiddleware\": 595,\n+                RaiseExceptionRequestMiddleware: 590,\n+                CatchExceptionDoNotOverrideRequestMiddleware: 595,\n             },\n         })\n         crawler = runner.create_crawler(SingleRequestSpider)\n@@ -188,7 +188,7 @@ class CrawlTestCase(TestCase):\n         \"\"\"\n         runner = CrawlerRunner(settings={\n             \"DOWNLOADER_MIDDLEWARES\": {\n-                __name__ + \".AlternativeCallbacksMiddleware\": 595,\n+                AlternativeCallbacksMiddleware: 595,\n             }\n         })\n         crawler = runner.create_crawler(AlternativeCallbacksSpider)\n\n@@ -50,10 +50,10 @@ class KeywordArgumentsSpider(MockServerSpider):\n     name = 'kwargs'\n     custom_settings = {\n         'DOWNLOADER_MIDDLEWARES': {\n-            __name__ + '.InjectArgumentsDownloaderMiddleware': 750,\n+            InjectArgumentsDownloaderMiddleware: 750,\n         },\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.InjectArgumentsSpiderMiddleware': 750,\n+            InjectArgumentsSpiderMiddleware: 750,\n         },\n     }\n \n\n@@ -16,11 +16,20 @@ class LogExceptionMiddleware:\n \n # ================================================================================\n # (0) recover from an exception on a spider callback\n+class RecoveryMiddleware:\n+    def process_spider_exception(self, response, exception, spider):\n+        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n+        return [\n+            {'from': 'process_spider_exception'},\n+            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n+        ]\n+\n+\n class RecoverySpider(Spider):\n     name = 'RecoverySpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.RecoveryMiddleware': 10,\n+            RecoveryMiddleware: 10,\n         },\n     }\n \n@@ -34,15 +43,6 @@ class RecoverySpider(Spider):\n             raise TabError()\n \n \n-class RecoveryMiddleware:\n-    def process_spider_exception(self, response, exception, spider):\n-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n-        return [\n-            {'from': 'process_spider_exception'},\n-            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n-        ]\n-\n-\n # ================================================================================\n # (1) exceptions from a spider middleware's process_spider_input method\n class FailProcessSpiderInputMiddleware:\n@@ -56,9 +56,8 @@ class ProcessSpiderInputSpiderWithoutErrback(Spider):\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n             # spider\n-            __name__ + '.LogExceptionMiddleware': 10,\n-            __name__ + '.FailProcessSpiderInputMiddleware': 8,\n-            __name__ + '.LogExceptionMiddleware': 6,\n+            FailProcessSpiderInputMiddleware: 8,\n+            LogExceptionMiddleware: 6,\n             # engine\n         }\n     }\n@@ -87,7 +86,7 @@ class GeneratorCallbackSpider(Spider):\n     name = 'GeneratorCallbackSpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 10,\n+            LogExceptionMiddleware: 10,\n         },\n     }\n \n@@ -106,7 +105,7 @@ class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider)\n     name = 'GeneratorCallbackSpiderMiddlewareRightAfterSpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 100000,\n+            LogExceptionMiddleware: 100000,\n         },\n     }\n \n@@ -117,7 +116,7 @@ class NotGeneratorCallbackSpider(Spider):\n     name = 'NotGeneratorCallbackSpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 10,\n+            LogExceptionMiddleware: 10,\n         },\n     }\n \n@@ -134,32 +133,13 @@ class NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackS\n     name = 'NotGeneratorCallbackSpiderMiddlewareRightAfterSpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 100000,\n+            LogExceptionMiddleware: 100000,\n         },\n     }\n \n \n # ================================================================================\n # (4) exceptions from a middleware process_spider_output method (generator)\n-class GeneratorOutputChainSpider(Spider):\n-    name = 'GeneratorOutputChainSpider'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            __name__ + '.GeneratorFailMiddleware': 10,\n-            __name__ + '.GeneratorDoNothingAfterFailureMiddleware': 8,\n-            __name__ + '.GeneratorRecoverMiddleware': 5,\n-            __name__ + '.GeneratorDoNothingAfterRecoveryMiddleware': 3,\n-        },\n-    }\n-\n-    def start_requests(self):\n-        yield Request(self.mockserver.url('/status?n=200'))\n-\n-    def parse(self, response):\n-        yield {'processed': ['parse-first-item']}\n-        yield {'processed': ['parse-second-item']}\n-\n-\n class _GeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         for r in result:\n@@ -205,26 +185,28 @@ class GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n     pass\n \n \n-# ================================================================================\n-# (5) exceptions from a middleware process_spider_output method (not generator)\n-class NotGeneratorOutputChainSpider(Spider):\n-    name = 'NotGeneratorOutputChainSpider'\n+class GeneratorOutputChainSpider(Spider):\n+    name = 'GeneratorOutputChainSpider'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            __name__ + '.NotGeneratorFailMiddleware': 10,\n-            __name__ + '.NotGeneratorDoNothingAfterFailureMiddleware': 8,\n-            __name__ + '.NotGeneratorRecoverMiddleware': 5,\n-            __name__ + '.NotGeneratorDoNothingAfterRecoveryMiddleware': 3,\n+            GeneratorFailMiddleware: 10,\n+            GeneratorDoNothingAfterFailureMiddleware: 8,\n+            GeneratorRecoverMiddleware: 5,\n+            GeneratorDoNothingAfterRecoveryMiddleware: 3,\n         },\n     }\n \n     def start_requests(self):\n-        return [Request(self.mockserver.url('/status?n=200'))]\n+        yield Request(self.mockserver.url('/status?n=200'))\n \n     def parse(self, response):\n-        return [{'processed': ['parse-first-item']}, {'processed': ['parse-second-item']}]\n+        yield {'processed': ['parse-first-item']}\n+        yield {'processed': ['parse-second-item']}\n \n \n+# ================================================================================\n+# (5) exceptions from a middleware process_spider_output method (not generator)\n+\n class _NotGeneratorDoNothingMiddleware:\n     def process_spider_output(self, response, result, spider):\n         out = []\n@@ -276,6 +258,24 @@ class NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddlew\n     pass\n \n \n+class NotGeneratorOutputChainSpider(Spider):\n+    name = 'NotGeneratorOutputChainSpider'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            NotGeneratorFailMiddleware: 10,\n+            NotGeneratorDoNothingAfterFailureMiddleware: 8,\n+            NotGeneratorRecoverMiddleware: 5,\n+            NotGeneratorDoNothingAfterRecoveryMiddleware: 3,\n+        },\n+    }\n+\n+    def start_requests(self):\n+        return [Request(self.mockserver.url('/status?n=200'))]\n+\n+    def parse(self, response):\n+        return [{'processed': ['parse-first-item']}, {'processed': ['parse-second-item']}]\n+\n+\n # ================================================================================\n class TestSpiderMiddleware(TestCase):\n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c24cdb2573958cc0fb9127c6f195d3174640ff4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1487 | Contributors (this commit): 22 | Commits (past 90d): 10 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,7 @@ class ImagesPipeline(FilesPipeline):\n         if self._deprecated_convert_image is None:\n             self._deprecated_convert_image = 'response_body' not in get_func_args(self.convert_image)\n             if self._deprecated_convert_image:\n-                warnings.warn('ImagesPipeline.convert_image() method overriden in a deprecated way, '\n+                warnings.warn(f'{self.__class__.__name__}.convert_image() method overriden in a deprecated way, '\n                               'overriden method does not accept response_body argument.',\n                               category=ScrapyDeprecationWarning)\n \n@@ -155,7 +155,7 @@ class ImagesPipeline(FilesPipeline):\n \n     def convert_image(self, image, size=None, response_body=None):\n         if response_body is None:\n-            warnings.warn('ImagesPipeline.convert_image() method called in a deprecated way, '\n+            warnings.warn(f'{self.__class__.__name__}.convert_image() method called in a deprecated way, '\n                           'method called without response_body argument.',\n                           category=ScrapyDeprecationWarning, stacklevel=2)\n \n\n@@ -162,7 +162,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                 self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n                 self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n \n-                expected_warning_msg = ('ImagesPipeline.convert_image() method overriden in a deprecated way, '\n+                expected_warning_msg = ('.convert_image() method overriden in a deprecated way, '\n                                         'overriden method does not accept response_body argument.')\n                 self.assertEqual(len([warning for warning in w if expected_warning_msg in str(warning.message)]), 1)\n \n@@ -199,7 +199,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n             self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n             # ensure that we recieved deprecation warnings\n-            expected_warning_msg = 'ImagesPipeline.convert_image() method called in a deprecated way'\n+            expected_warning_msg = '.convert_image() method called in a deprecated way'\n             self.assertTrue(len([warning for warning in w if expected_warning_msg in str(warning.message)]) == 4)\n \n     def test_convert_image_new(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#892dd9da57c5cf005670743c8abfdfffc7027acc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 65 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,6 +14,12 @@ try:\n except ImportError:\n     pass\n \n+try:\n+    import zstd\n+    ACCEPTED_ENCODINGS.append(b'zstd')\n+except ImportError:\n+    pass\n+\n \n class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n@@ -67,4 +73,6 @@ class HttpCompressionMiddleware:\n                 body = zlib.decompress(body, -15)\n         if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n             body = brotli.decompress(body)\n+        if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:\n+            body = zstd.decompress(body)\n         return body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da3171d4f71d50a15b08b76b2b1e06bddfa56694", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 73 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import zlib\n+import io\n \n from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n@@ -15,7 +16,7 @@ except ImportError:\n     pass\n \n try:\n-    import zstd\n+    import zstandard\n     ACCEPTED_ENCODINGS.append(b'zstd')\n except ImportError:\n     pass\n@@ -74,5 +75,8 @@ class HttpCompressionMiddleware:\n         if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n             body = brotli.decompress(body)\n         if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:\n-            body = zstd.decompress(body)\n+            # Using its streaming API since its simple API could handle only cases\n+            # where there is content size data embedded in the frame\n+            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\n+            body = reader.read()\n         return body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50e1f35d1fcaeb2e0a0c4fb29894cb7c686a33cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 26 | Churn Cumulative: 254 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,12 @@ FORMAT = {\n     'rawdeflate': ('html-rawdeflate.bin', 'deflate'),\n     'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),\n     'br': ('html-br.bin', 'br'),\n+    # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin\n+    'zstd-static-content-size': ('html-zstd-static-content-size.bin', 'zstd'),\n+    # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin\n+    'zstd-static-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),\n+    # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin\n+    'zstd-streaming-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),\n }\n \n \n@@ -80,6 +86,26 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b\"<!DOCTYPE\")\n         assert 'Content-Encoding' not in newresponse.headers\n \n+    def test_process_response_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n+        raw_content = None\n+        for check_key in FORMAT:\n+            if not check_key.startswith('zstd-'):\n+                continue\n+            response = self._getresponse(check_key)\n+            request = response.request\n+            self.assertEqual(response.headers['Content-Encoding'], b'zstd')\n+            newresponse = self.mw.process_response(request, response, self.spider)\n+            if raw_content is None:\n+                raw_content = newresponse.body\n+            assert raw_content == newresponse.body\n+            assert newresponse is not response\n+            assert newresponse.body.startswith(b\"<!DOCTYPE\")\n+            assert 'Content-Encoding' not in newresponse.headers\n+\n     def test_process_response_rawdeflate(self):\n         response = self._getresponse('rawdeflate')\n         request = response.request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6032a9a31065a88886ef0f600c57ba63500b95ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 255 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,6 +101,7 @@ class HttpCompressionTest(TestCase):\n             newresponse = self.mw.process_response(request, response, self.spider)\n             if raw_content is None:\n                 raw_content = newresponse.body\n+            else:\n                 assert raw_content == newresponse.body\n             assert newresponse is not response\n             assert newresponse.body.startswith(b\"<!DOCTYPE\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b1f86b613d2039b0a66ba2b527a7e9bffadaf3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 4 | Churn Cumulative: 409 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ def xmliter(obj, nodename):\n     namespaces = {}\n     if header_end:\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n-            tag = re.search(r'<\\s*%s.*?xmlns[:=][^>]*>' % tagname, text[:header_end_idx[1]], re.S)\n+            tag = re.search(fr'<\\s*{tagname}.*?xmlns[:=][^>]*>', text[:header_end_idx[1]], re.S)\n             if tag:\n                 namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))\n \n@@ -45,7 +45,7 @@ def xmliter(obj, nodename):\n             document_header\n             + match.group().replace(\n                 nodename,\n-                '%s %s' % (nodename, ' '.join(namespaces.values())),\n+                f'{nodename} {\" \".join(namespaces.values())}',\n                 1\n             )\n             + header_end\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6050604f626a5ee38239ef1eff44d0c867469a3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 13 | Churn Cumulative: 268 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,6 +77,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n     def test_process_response_br(self):\n         try:\n@@ -91,6 +92,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b\"<!DOCTYPE\")\n         assert 'Content-Encoding' not in newresponse.headers\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n     def test_process_response_rawdeflate(self):\n         response = self._getresponse('rawdeflate')\n@@ -102,6 +104,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74840)\n \n     def test_process_response_zlibdelate(self):\n         response = self._getresponse('zlibdeflate')\n@@ -113,6 +116,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74840)\n \n     def test_process_response_plain(self):\n         response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')\n@@ -123,6 +127,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is response\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         self.assertStatsEqual('httpcompression/response_count', None)\n+        self.assertStatsEqual('httpcompression/response_bytes', None)\n \n     def test_multipleencodings(self):\n         response = self._getresponse('gzip')\n@@ -151,6 +156,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 104)\n \n     def test_process_response_force_recalculate_encoding(self):\n         headers = {\n@@ -171,6 +177,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 104)\n \n     def test_process_response_no_content_type_header(self):\n         headers = {\n@@ -187,6 +194,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 104)\n \n     def test_process_response_gzipped_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -198,6 +206,7 @@ class HttpCompressionTest(TestCase):\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n     def test_process_response_gzip_app_octetstream_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -209,6 +218,7 @@ class HttpCompressionTest(TestCase):\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n     def test_process_response_gzip_binary_octetstream_contenttype(self):\n         response = self._getresponse('x-gzip')\n@@ -220,6 +230,7 @@ class HttpCompressionTest(TestCase):\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n     def test_process_response_gzipped_gzip_file(self):\n         \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n@@ -263,6 +274,7 @@ class HttpCompressionTest(TestCase):\n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertEqual(gunzip(newresponse.body), plainbody)\n         self.assertStatsEqual('httpcompression/response_count', 1)\n+        self.assertStatsEqual('httpcompression/response_bytes', 230)\n \n     def test_process_response_head_request_no_decode_required(self):\n         response = self._getresponse('gzip')\n@@ -274,3 +286,4 @@ class HttpCompressionTest(TestCase):\n         self.assertIs(newresponse, response)\n         self.assertEqual(response.body, b'')\n         self.assertStatsEqual('httpcompression/response_count', None)\n+        self.assertStatsEqual('httpcompression/response_bytes', None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e40788153ca7dee9e0d4f8ac16a9e17278f79058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 15 | Churn Cumulative: 1848 | Contributors (this commit): 28 | Commits (past 90d): 5 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,7 +65,11 @@ class Request(object_ref):\n         s = safe_url_string(url, self.encoding)\n         self._url = escape_ajax(s)\n \n-        if ('://' not in self._url) and (not self._url.startswith('data:')):\n+        if (\n+            '://' not in self._url\n+            and not self._url.startswith('about:')\n+            and not self._url.startswith('data:')\n+        ):\n             raise ValueError(f'Missing scheme in request url: {self._url}')\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\n@@ -43,6 +43,15 @@ class RequestTest(unittest.TestCase):\n         assert r.headers is not headers\n         self.assertEqual(r.headers[b\"caca\"], b\"coco\")\n \n+    def test_url_scheme(self):\n+        # This test passes by not raising any (ValueError) exception\n+        self.request_class('http://example.org')\n+        self.request_class('https://example.org')\n+        self.request_class('s3://example.org')\n+        self.request_class('ftp://example.org')\n+        self.request_class('about:config')\n+        self.request_class('data:,Hello%2C%20World!')\n+\n     def test_url_no_scheme(self):\n         self.assertRaises(ValueError, self.request_class, 'foo')\n         self.assertRaises(ValueError, self.request_class, '/foo/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f02df20c51c80739d2362c388930ad93e3de34a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 591 | Contributors (this commit): 27 | Commits (past 90d): 9 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,6 @@ install_requires = [\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n-    'PyDispatcher>=2.0.5',\n     'pyOpenSSL>=16.2.0',\n     'queuelib>=1.4.2',\n     'service_identity>=16.0.0',\n@@ -38,6 +37,7 @@ extras_require = {}\n if has_environment_marker_platform_impl_support():\n     extras_require[':platform_python_implementation == \"CPython\"'] = [\n         'lxml>=3.5.0',\n+        'PyDispatcher>=2.0.5',\n     ]\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n         # Earlier lxml versions are affected by\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e734e6b35f686f62dea11e9484c7646c7c49ca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 75 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n-import zlib\n import io\n+import zlib\n \n from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#156bb0a1d413c7c6acafc30003ef98030a06e47f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 2 | Churn Cumulative: 270 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ FORMAT = {\n     # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin\n     'zstd-static-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),\n     # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin\n-    'zstd-streaming-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),\n+    'zstd-streaming-no-content-size': ('html-zstd-streaming-no-content-size.bin', 'zstd'),\n }\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b55c911ddc21a41a6e4204aaf239a16e892de7b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 57 | Churn Cumulative: 348 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from twisted.internet import defer\n+from twisted.internet._resolver import HostResolution\n from twisted.internet.base import ThreadedResolver\n from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver, IResolverSimple\n from zope.interface.declarations import implementer, provider\n@@ -50,6 +51,27 @@ class CachingThreadedResolver(ThreadedResolver):\n         return result\n \n \n+@provider(IResolutionReceiver)\n+class _CachingResolutionReceiver:\n+    def __init__(self, resolutionReceiver, hostName):\n+        self.resolutionReceiver = resolutionReceiver\n+        self.hostName = hostName\n+        self.addresses = []\n+\n+    def resolutionBegan(self, resolution):\n+        self.resolutionReceiver.resolutionBegan(resolution)\n+        self.resolution = resolution\n+\n+    def addressResolved(self, address):\n+        self.resolutionReceiver.addressResolved(address)\n+        self.addresses.append(address)\n+\n+    def resolutionComplete(self):\n+        self.resolutionReceiver.resolutionComplete()\n+        if self.addresses:\n+            dnscache[self.hostName] = self.addresses\n+\n+\n @implementer(IHostnameResolver)\n class CachingHostnameResolver:\n     \"\"\"\n@@ -73,33 +95,22 @@ class CachingHostnameResolver:\n     def install_on_reactor(self):\n         self.reactor.installNameResolver(self)\n \n-    def resolveHostName(self, resolutionReceiver, hostName, portNumber=0,\n-                        addressTypes=None, transportSemantics='TCP'):\n-\n-        @provider(IResolutionReceiver)\n-        class CachingResolutionReceiver(resolutionReceiver):\n-\n-            def resolutionBegan(self, resolution):\n-                super().resolutionBegan(resolution)\n-                self.resolution = resolution\n-                self.resolved = False\n-\n-            def addressResolved(self, address):\n-                super().addressResolved(address)\n-                self.resolved = True\n-\n-            def resolutionComplete(self):\n-                super().resolutionComplete()\n-                if self.resolved:\n-                    dnscache[hostName] = self.resolution\n-\n+    def resolveHostName(\n+        self, resolutionReceiver, hostName, portNumber=0, addressTypes=None, transportSemantics=\"TCP\"\n+    ):\n         try:\n-            return dnscache[hostName]\n+            addresses = dnscache[hostName]\n         except KeyError:\n             return self.original_resolver.resolveHostName(\n-                CachingResolutionReceiver(),\n+                _CachingResolutionReceiver(resolutionReceiver, hostName),\n                 hostName,\n                 portNumber,\n                 addressTypes,\n-                transportSemantics\n+                transportSemantics,\n             )\n+        else:\n+            resolutionReceiver.resolutionBegan(HostResolution(hostName))\n+            for addr in addresses:\n+                resolutionReceiver.addressResolved(addr)\n+            resolutionReceiver.resolutionComplete()\n+            return resolutionReceiver\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fe5876597825a4df03bd15e6b1cd1682b806de8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 360 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n from twisted.internet import defer\n-from twisted.internet._resolver import HostResolution\n from twisted.internet.base import ThreadedResolver\n-from twisted.internet.interfaces import IHostnameResolver, IResolutionReceiver, IResolverSimple\n+from twisted.internet.interfaces import IHostResolution, IHostnameResolver, IResolutionReceiver, IResolverSimple\n from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n@@ -51,6 +50,15 @@ class CachingThreadedResolver(ThreadedResolver):\n         return result\n \n \n+@implementer(IHostResolution)\n+class HostResolution:\n+    def __init__(self, name):\n+        self.name = name\n+\n+    def cancel(self):\n+        raise NotImplementedError()\n+\n+\n @provider(IResolutionReceiver)\n class _CachingResolutionReceiver:\n     def __init__(self, resolutionReceiver, hostName):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#868826b346714ceff821886536a3b259072f8396", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 20 | Files Changed: 5 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 89 | Churn Cumulative: 903 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,15 +0,0 @@\n-import scrapy\n-from scrapy.crawler import CrawlerProcess\n-\n-\n-class IPv6Spider(scrapy.Spider):\n-    name = \"ipv6_spider\"\n-    start_urls = [\"http://[::1]\"]\n-\n-\n-process = CrawlerProcess(settings={\n-    \"RETRY_ENABLED\": False,\n-    \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n-})\n-process.crawl(IPv6Spider)\n-process.start()\n\n@@ -0,0 +1,30 @@\n+import sys\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class CachingHostnameResolverSpider(scrapy.Spider):\n+    \"\"\"\n+    Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n+    \"\"\"\n+    name = \"caching_hostname_resolver_spider\"\n+\n+    def start_requests(self):\n+        yield scrapy.Request(self.url)\n+\n+    def parse(self, response):\n+        for _ in range(10):\n+            yield scrapy.Request(response.url, dont_filter=True, callback=self.ignore_response)\n+\n+    def ignore_response(self, response):\n+        self.logger.info(repr(response.ip_address))\n+\n+\n+if __name__ == \"__main__\":\n+    process = CrawlerProcess(settings={\n+        \"RETRY_ENABLED\": False,\n+        \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n+    })\n+    process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n+    process.start()\n\n@@ -0,0 +1,19 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class CachingHostnameResolverSpider(scrapy.Spider):\n+    \"\"\"\n+    Finishes without a twisted.internet.error.DNSLookupError exception\n+    \"\"\"\n+    name = \"caching_hostname_resolver_spider\"\n+    start_urls = [\"http://[::1]\"]\n+\n+\n+if __name__ == \"__main__\":\n+    process = CrawlerProcess(settings={\n+        \"RETRY_ENABLED\": False,\n+        \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n+    })\n+    process.crawl(CachingHostnameResolverSpider)\n+    process.start()\n\n@@ -3,10 +3,15 @@ from scrapy.crawler import CrawlerProcess\n \n \n class IPv6Spider(scrapy.Spider):\n+    \"\"\"\n+    Raises a twisted.internet.error.DNSLookupError:\n+    the default name resolver does not handle IPv6 addresses.\n+    \"\"\"\n     name = \"ipv6_spider\"\n     start_urls = [\"http://[::1]\"]\n \n \n+if __name__ == \"__main__\":\n     process = CrawlerProcess(settings={\"RETRY_ENABLED\": False})\n     process.crawl(IPv6Spider)\n     process.start()\n\n@@ -22,6 +22,8 @@ from scrapy.extensions.throttle import AutoThrottle\n from scrapy.extensions import telnet\n from scrapy.utils.test import get_testenv\n \n+from tests.mockserver import MockServer\n+\n \n class BaseCrawlerTest(unittest.TestCase):\n \n@@ -280,9 +282,9 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n \n class ScriptRunnerMixin:\n-    def run_script(self, script_name):\n+    def run_script(self, script_name, *script_args):\n         script_path = os.path.join(self.script_dir, script_name)\n-        args = (sys.executable, script_path)\n+        args = [sys.executable, script_path] + list(script_args)\n         p = subprocess.Popen(args, env=get_testenv(),\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n         stdout, stderr = p.communicate()\n@@ -321,9 +323,17 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n             log)\n \n-    def test_ipv6_alternative_name_resolver(self):\n-        log = self.run_script('alternative_name_resolver.py')\n-        self.assertIn('Spider closed (finished)', log)\n+    def test_caching_hostname_resolver_ipv6(self):\n+        log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n+\n+    def test_caching_hostname_resolver_finite_execution(self):\n+        with MockServer() as mock_server:\n+            log = self.run_script(\"caching_hostname_resolver.py\", mock_server.http_address)\n+            self.assertIn(\"Spider closed (finished)\", log)\n+            self.assertNotIn(\"ERROR: Error downloading\", log)\n+            self.assertNotIn(\"TimeoutError\", log)\n             self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n \n     def test_reactor_select(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd663fd4ad5a69ba0403a2eec5bdc29a0109b0d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 78 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,7 @@ except ImportError:\n class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n-    def __init__(self, stats):\n+    def __init__(self, stats=None):\n         self.stats = stats\n \n     @classmethod\n@@ -40,6 +40,7 @@ class HttpCompressionMiddleware:\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n+                if self.stats:\n                     self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n                     self.stats.inc_value('httpcompression/response_count', spider=spider)\n                 respcls = responsetypes.from_args(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d32d0d27393ce55490c44d9fb039130320461865", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 14 | Churn Cumulative: 284 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,7 @@ class HttpCompressionTest(TestCase):\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider('scrapytest.org')\n-        self.mw = HttpCompressionMiddleware(self.crawler.stats)\n+        self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n         self.crawler.stats.open_spider(self.spider)\n \n     def _getresponse(self, coding):\n@@ -79,6 +79,18 @@ class HttpCompressionTest(TestCase):\n         self.assertStatsEqual('httpcompression/response_count', 1)\n         self.assertStatsEqual('httpcompression/response_bytes', 74837)\n \n+    def test_process_response_gzip_no_stats(self):\n+        mw = HttpCompressionMiddleware()\n+        response = self._getresponse('gzip')\n+        request = response.request\n+\n+        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n+        newresponse = mw.process_response(request, response, self.spider)\n+        self.assertEqual(mw.stats, None)\n+        assert newresponse is not response\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n+        assert 'Content-Encoding' not in newresponse.headers\n+\n     def test_process_response_br(self):\n         try:\n             import brotli  # noqa: F401\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#872b2e4ce414c1268dcadcd8f3da7dab546e39f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 306 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from gzip import GzipFile\n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n+from scrapy.exceptions import NotConfigured\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n@@ -60,6 +61,27 @@ class HttpCompressionTest(TestCase):\n             str(self.crawler.stats.get_stats(self.spider))\n         )\n \n+    def test_setting_false_compression_enabled(self):\n+        self.assertRaises(\n+            NotConfigured,\n+            HttpCompressionMiddleware.from_crawler,\n+            get_crawler(settings_dict={'COMPRESSION_ENABLED': False})\n+        )\n+\n+    def test_setting_default_compression_enabled(self):\n+        self.assertIsInstance(\n+            HttpCompressionMiddleware.from_crawler(get_crawler()),\n+            HttpCompressionMiddleware\n+        )\n+\n+    def test_setting_true_compression_enabled(self):\n+        self.assertIsInstance(\n+            HttpCompressionMiddleware.from_crawler(\n+                get_crawler(settings_dict={'COMPRESSION_ENABLED': True})\n+            ),\n+            HttpCompressionMiddleware\n+        )\n+\n     def test_process_request(self):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
