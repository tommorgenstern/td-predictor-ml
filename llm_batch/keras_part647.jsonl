{"custom_id": "keras#49c6ddbc428829d89c1cbdb413465079f00ddcc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 458 | Contributors (this commit): 5 | Commits (past 90d): 26 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -146,7 +146,7 @@ def reduce_weighted_values(\n         sample_weight = ops.cast(sample_weight, values.dtype)\n         # Update dimensions of `sample_weight` to match `losses`.\n         values, sample_weight = squeeze_to_same_rank(values, sample_weight)\n-        values *= sample_weight\n+        values = values * sample_weight\n \n     # Apply reduction function to the individual weighted losses.\n     loss = reduce_values(values, reduction)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2114c02b9ce6b60edefd23104793e919d5a77e0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 430 | Lines Deleted: 293 | Files Changed: 5 | Hunks: 38 | Methods Changed: 39 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 723 | Churn Cumulative: 7543 | Contributors (this commit): 11 | Commits (past 90d): 171 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,24 +62,16 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build optimizer\n             self.optimizer.build(self.trainable_variables)\n \n-    def make_train_function(self, force=False):\n-        if self.train_function is not None and not force:\n-            return self.train_function\n-\n-        grad_fn = jax.value_and_grad(\n-            self.compute_loss_and_updates, has_aux=True\n-        )\n-\n-        def one_train_step(state, data):\n-            data = data[0]\n+    def train_step(self, state, data):\n         (\n             trainable_variables,\n             non_trainable_variables,\n             optimizer_variables,\n             metrics_variables,\n         ) = state\n-            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n-                data\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        grad_fn = jax.value_and_grad(\n+            self.compute_loss_and_updates, has_aux=True\n         )\n         (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n             trainable_variables,\n@@ -100,9 +92,7 @@ class JAXTrainer(base_trainer.Trainer):\n         with backend.StatelessScope(\n             state_mapping=[\n                 (ref_v, v)\n-                    for ref_v, v in zip(\n-                        self.metrics_variables, metrics_variables\n-                    )\n+                for ref_v, v in zip(self.metrics_variables, metrics_variables)\n             ]\n         ) as scope:\n             self._loss_tracker.update_state(loss)\n@@ -124,6 +114,69 @@ class JAXTrainer(base_trainer.Trainer):\n         )\n         return logs, state\n \n+    def test_step(self, state, data):\n+        (\n+            trainable_variables,\n+            non_trainable_variables,\n+            metrics_variables,\n+        ) = state\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        loss, (\n+            y_pred,\n+            non_trainable_variables,\n+        ) = self.compute_loss_and_updates(\n+            trainable_variables,\n+            non_trainable_variables,\n+            x,\n+            y,\n+            sample_weight,\n+            training=False,\n+        )\n+\n+        with backend.StatelessScope(\n+            state_mapping=[\n+                (ref_v, v)\n+                for ref_v, v in zip(self.metrics_variables, metrics_variables)\n+            ]\n+        ) as scope:\n+            self._loss_tracker.update_state(loss)\n+            logs = self.compute_metrics(x, y, y_pred, sample_weight)\n+\n+        new_metrics_variables = []\n+        for ref_v in self.metrics_variables:\n+            new_v = scope.get_current_value(ref_v)\n+            if new_v is None:\n+                new_v = ref_v.value\n+            new_metrics_variables.append(new_v)\n+        metrics_variables = new_metrics_variables\n+\n+        state = (\n+            trainable_variables,\n+            non_trainable_variables,\n+            metrics_variables,\n+        )\n+        return logs, state\n+\n+    def predict_step(self, state, data):\n+        trainable_variables, non_trainable_variables = state\n+        kwargs = {}\n+        if self._call_has_training_arg():\n+            kwargs[\"training\"] = False\n+\n+        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        outputs, _ = self.stateless_call(\n+            trainable_variables, non_trainable_variables, x, **kwargs\n+        )\n+        return outputs\n+\n+    def make_train_function(self, force=False):\n+        if self.train_function is not None and not force:\n+            return self.train_function\n+\n+        def one_train_step(state, data):\n+            data = data[0]\n+            return self.train_step(state, data)\n+\n         def multi_train_steps(state, data):\n             for single_step_data in data:\n                 logs, state = one_train_step(state, [single_step_data])\n@@ -151,51 +204,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         def one_test_step(state, data):\n             data = data[0]\n-            (\n-                trainable_variables,\n-                non_trainable_variables,\n-                metrics_variables,\n-            ) = state\n-            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n-                data\n-            )\n-            loss, (\n-                y_pred,\n-                non_trainable_variables,\n-            ) = self.compute_loss_and_updates(\n-                trainable_variables,\n-                non_trainable_variables,\n-                x,\n-                y,\n-                sample_weight,\n-                training=False,\n-            )\n-\n-            with backend.StatelessScope(\n-                state_mapping=[\n-                    (ref_v, v)\n-                    for ref_v, v in zip(\n-                        self.metrics_variables, metrics_variables\n-                    )\n-                ]\n-            ) as scope:\n-                self._loss_tracker.update_state(loss)\n-                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n-\n-            new_metrics_variables = []\n-            for ref_v in self.metrics_variables:\n-                new_v = scope.get_current_value(ref_v)\n-                if new_v is None:\n-                    new_v = ref_v.value\n-                new_metrics_variables.append(new_v)\n-            metrics_variables = new_metrics_variables\n-\n-            state = (\n-                trainable_variables,\n-                non_trainable_variables,\n-                metrics_variables,\n-            )\n-            return logs, state\n+            return self.test_step(state, data)\n \n         def multi_test_steps(state, data):\n             for single_step_data in data:\n@@ -222,29 +231,15 @@ class JAXTrainer(base_trainer.Trainer):\n         if self.predict_function is not None and not force:\n             return self.predict_function\n \n-        def one_predict_step(\n-            trainable_variables, non_trainable_variables, data\n-        ):\n-            kwargs = {}\n-            if self._call_has_training_arg():\n-                kwargs[\"training\"] = False\n+        def one_predict_step(state, data):\n+            data = data[0]\n+            return self.predict_step(state, data)\n \n-            x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data[0])\n-            outputs, _ = self.stateless_call(\n-                trainable_variables, non_trainable_variables, x, **kwargs\n-            )\n-            return outputs\n-\n-        def multi_predict_steps(\n-            trainable_variables, non_trainable_variables, data\n-        ):\n-            outputs = one_predict_step(\n-                trainable_variables, non_trainable_variables, data[:1]\n-            )\n+        def multi_predict_steps(state, data):\n+            outputs = one_predict_step(state, data[:1])\n             for single_step_data in data[1:]:\n                 step_outputs = one_predict_step(\n-                    trainable_variables,\n-                    non_trainable_variables,\n+                    state,\n                     [single_step_data],\n                 )\n                 outputs = tf.nest.map_structure(\n@@ -262,12 +257,8 @@ class JAXTrainer(base_trainer.Trainer):\n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n-            def compiled_predict_step(\n-                trainable_variables, non_trainable_variables, data\n-            ):\n-                return predict_step(\n-                    trainable_variables, non_trainable_variables, data\n-                )\n+            def compiled_predict_step(state, data):\n+                return predict_step(state, data)\n \n             self.predict_function = compiled_predict_step\n \n@@ -603,7 +594,7 @@ class JAXTrainer(base_trainer.Trainer):\n         for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n             batch_outputs = self.predict_function(\n-                trainable_variables, non_trainable_variables, x\n+                [trainable_variables, non_trainable_variables], x\n             )\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n@@ -620,34 +611,6 @@ class JAXTrainer(base_trainer.Trainer):\n         class_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Runs a single gradient update on a single batch of data.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) to apply to the model's loss for the samples\n-                from this class during training. This can be useful to tell the\n-                model to \"pay more attention\" to samples from an\n-                under-represented class. When `class_weight` is specified\n-                and targets have a rank of 2 or greater, either `y` must\n-                be one-hot encoded, or an explicit final dimension of 1\n-                must be included for sparse class labels.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"train_on_batch\")\n         if class_weight is not None:\n             if sample_weight is not None:\n@@ -707,26 +670,6 @@ class JAXTrainer(base_trainer.Trainer):\n         sample_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Test the model on a single batch of samples.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"test_on_batch\")\n \n         data = (x, y, sample_weight)\n@@ -760,14 +703,6 @@ class JAXTrainer(base_trainer.Trainer):\n         return self._flatten_metrics_in_order(logs)\n \n     def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        Args:\n-            x: Input data. It must be array-like.\n-\n-        Returns:\n-            NumPy array(s) of predictions.\n-        \"\"\"\n         if not self.built:\n             # Build model\n             with backend.StatelessScope():\n@@ -777,7 +712,7 @@ class JAXTrainer(base_trainer.Trainer):\n         trainable_variables = self.trainable_variables\n         non_trainable_variables = self.non_trainable_variables\n         batch_outputs = self.predict_function(\n-            trainable_variables, non_trainable_variables, [x]\n+            [trainable_variables, non_trainable_variables], [x]\n         )\n         batch_outputs = tf.nest.map_structure(\n             lambda x: np.array(x), batch_outputs\n\n@@ -498,34 +498,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         class_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Runs a single gradient update on a single batch of data.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) to apply to the model's loss for the samples\n-                from this class during training. This can be useful to tell the\n-                model to \"pay more attention\" to samples from an\n-                under-represented class. When `class_weight` is specified\n-                and targets have a rank of 2 or greater, either `y` must\n-                be one-hot encoded, or an explicit final dimension of 1\n-                must be included for sparse class labels.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"train_on_batch\")\n         self.make_train_function()\n         if class_weight is not None:\n@@ -556,26 +528,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         sample_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Test the model on a single batch of samples.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"test_on_batch\")\n         self.make_test_function()\n \n@@ -589,14 +541,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return self._flatten_metrics_in_order(logs)\n \n     def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        Args:\n-            x: Input data. It must be array-like.\n-\n-        Returns:\n-            NumPy array(s) of predictions.\n-        \"\"\"\n         self.make_predict_function()\n         batch_outputs = self.predict_function((x,))\n         batch_outputs = tf.nest.map_structure(\n\n@@ -18,16 +18,8 @@ class TorchTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n-    def make_train_function(self, force=False):\n-        if self.train_function is not None and not force:\n-            return self.train_function\n-\n-        def one_step_on_data(data):\n-            \"\"\"Runs a single training step on a batch of data.\"\"\"\n-            data = data[0]\n-            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n-                data\n-            )\n+    def train_step(self, data):\n+        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Compute prediction error\n         if self._call_has_training_arg():\n@@ -63,20 +55,9 @@ class TorchTrainer(base_trainer.Trainer):\n         else:\n             warnings.warn(\"The model does not have any trainable weights.\")\n \n-            return self.compute_metrics(\n-                x, y, y_pred, sample_weight=sample_weight\n-            )\n+        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n-        self.train_function = one_step_on_data\n-\n-    def make_test_function(self, force=False):\n-        if self.test_function is not None and not force:\n-            return self.test_function\n-\n-        def one_step_on_data(data):\n-            \"\"\"Runs a single test step on a batch of data.\"\"\"\n-            with torch.no_grad():\n-                data = data[0]\n+    def test_step(self, data):\n         (\n             x,\n             y,\n@@ -90,26 +71,66 @@ class TorchTrainer(base_trainer.Trainer):\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n         self._loss_tracker.update_state(loss)\n-                return self.compute_metrics(\n-                    x, y, y_pred, sample_weight=sample_weight\n+        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n+\n+    def predict_step(self, data):\n+        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        if self._call_has_training_arg():\n+            y_pred = self(x, training=False)\n+        else:\n+            y_pred = self(x)\n+        return y_pred\n+\n+    def make_train_function(self, force=False):\n+        if self.train_function is not None and not force:\n+            return self.train_function\n+\n+        if self.steps_per_execution > 1:\n+            raise ValueError(\n+                \"`steps_per_execution` must be 1 with the PyTorch backend. \"\n+                f\"Received: steps_per_execution={self.steps_per_execution}\"\n             )\n \n+        def one_step_on_data(data):\n+            \"\"\"Runs a single training step on a batch of data.\"\"\"\n+            data = data[0]\n+            return self.train_step(data)\n+\n+        self.train_function = one_step_on_data\n+\n+    def make_test_function(self, force=False):\n+        if self.test_function is not None and not force:\n+            return self.test_function\n+\n+        if self.steps_per_execution > 1:\n+            raise ValueError(\n+                \"`steps_per_execution` must be 1 with the PyTorch backend. \"\n+                f\"Received: steps_per_execution={self.steps_per_execution}\"\n+            )\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a single test step on a batch of data.\"\"\"\n+            data = data[0]\n+            with torch.no_grad():\n+                return self.test_step(data)\n+\n         self.test_function = one_step_on_data\n \n     def make_predict_function(self, force=False):\n         if self.predict_function is not None and not force:\n             return self.predict_function\n \n+        if self.steps_per_execution > 1:\n+            raise ValueError(\n+                \"`steps_per_execution` must be 1 with the PyTorch backend. \"\n+                f\"Received: steps_per_execution={self.steps_per_execution}\"\n+            )\n+\n         def one_step_on_data(data):\n             \"\"\"Runs a predict test step on a batch of data.\"\"\"\n-            with torch.no_grad():\n             data = data[0]\n-                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-                if self._call_has_training_arg():\n-                    y_pred = self(x, training=False)\n-                else:\n-                    y_pred = self(x)\n-                return y_pred\n+            with torch.no_grad():\n+                return self.predict_step(data)\n \n         self.predict_function = one_step_on_data\n \n@@ -377,34 +398,6 @@ class TorchTrainer(base_trainer.Trainer):\n         class_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Runs a single gradient update on a single batch of data.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            class_weight: Optional dictionary mapping class indices (integers)\n-                to a weight (float) to apply to the model's loss for the samples\n-                from this class during training. This can be useful to tell the\n-                model to \"pay more attention\" to samples from an\n-                under-represented class. When `class_weight` is specified\n-                and targets have a rank of 2 or greater, either `y` must\n-                be one-hot encoded, or an explicit final dimension of 1\n-                must be included for sparse class labels.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"train_on_batch\")\n         self.make_train_function()\n         if class_weight is not None:\n@@ -433,26 +426,6 @@ class TorchTrainer(base_trainer.Trainer):\n         sample_weight=None,\n         return_dict=False,\n     ):\n-        \"\"\"Test the model on a single batch of samples.\n-\n-        Args:\n-            x: Input data. Must be array-like.\n-            y: Target data. Must be array-like.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape `(samples, sequence_length)`, to apply a different\n-                weight to every timestep of every sample.\n-            return_dict: If `True`, loss and metric results are returned as a\n-                dict, with each key being the name of the metric. If `False`,\n-                they are returned as a list.\n-\n-        Returns:\n-            A scalar loss value (when no metrics and `return_dict=False`),\n-            a list of loss and metric values\n-            (if there are metrics and `return_dict=False`), or a dict of\n-            metric and loss values (if `return_dict=True`).\n-        \"\"\"\n         self._assert_compile_called(\"test_on_batch\")\n         self.make_test_function()\n \n@@ -465,14 +438,6 @@ class TorchTrainer(base_trainer.Trainer):\n         return self._flatten_metrics_in_order(logs)\n \n     def predict_on_batch(self, x):\n-        \"\"\"Returns predictions for a single batch of samples.\n-\n-        Args:\n-            x: Input data. It must be array-like.\n-\n-        Returns:\n-            NumPy array(s) of predictions.\n-        \"\"\"\n         self.make_predict_function()\n         batch_outputs = self.predict_function((x,))\n         batch_outputs = tf.nest.map_structure(\n\n@@ -100,6 +100,10 @@ class Trainer:\n             metrics += [self._compile_metrics]\n         return metrics\n \n+    @property\n+    def metrics_names(self):\n+        return [m.name for m in self.metrics]\n+\n     @property\n     def metrics_variables(self):\n         vars = []\n@@ -194,9 +198,10 @@ class Trainer:\n                 metric_results = super().compute_metrics(\n                     x, y, y_pred, sample_weight)\n \n-            # Note that `self.custom_metric` is not listed in `self.metrics`.\n+                # Note that `self.custom_metric` is not listed\n+                # in `self.metrics`.\n                 self.custom_metric.update_state(x, y, y_pred, sample_weight)\n-            metric_results['custom_metric_name'] = self.custom_metric.result()\n+                metric_results['metric_name'] = self.custom_metric.result()\n                 return metric_results\n         ```\n \n@@ -208,7 +213,7 @@ class Trainer:\n \n         Returns:\n             A `dict` containing values that will be passed to\n-            `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically,\n+            `keras_core.callbacks.CallbackList.on_train_batch_end()`. Typically,\n             the values of the metrics listed in `self.metrics` are returned.\n             Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n@@ -225,8 +230,7 @@ class Trainer:\n \n         Returns:\n             A `dict` containing values of the metrics listed in `self.metrics`.\n-            Example:\n-            `{'loss': 0.2, 'accuracy': 0.7}`.\n+            Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n         return_metrics = {}\n         for metric in self.metrics:\n@@ -256,6 +260,181 @@ class Trainer:\n         validation_batch_size=None,\n         validation_freq=1,\n     ):\n+        \"\"\"Trains the model for a fixed number of epochs (dataset iterations).\n+\n+        Args:\n+            x: Input data. It could be:\n+              - A NumPy array (or array-like), or a list of arrays\n+                (in case the model has multiple inputs).\n+              - A tensor, or a list of tensors\n+                (in case the model has multiple inputs).\n+              - A dict mapping input names to the corresponding array/tensors,\n+                if the model has named inputs.\n+              - A `tf.data.Dataset`. Should return a tuple\n+                of either `(inputs, targets)` or\n+                `(inputs, targets, sample_weights)`.\n+              - A `keras_core.utils.PyDataset` returning `(inputs,\n+                targets)` or `(inputs, targets, sample_weights)`.\n+            y: Target data. Like the input data `x`,\n+                it could be either NumPy array(s) or backend-native tensor(s).\n+                If `x` is a dataset, generator,\n+                or `keras_core.utils.PyDataset` instance, `y` should\n+                not be specified (since targets will be obtained from `x`).\n+            batch_size: Integer or `None`.\n+                Number of samples per gradient update.\n+                If unspecified, `batch_size` will default to 32.\n+                Do not specify the `batch_size` if your data is in the\n+                form of datasets, generators, or `keras_core.utils.PyDataset`\n+                instances (since they generate batches).\n+            epochs: Integer. Number of epochs to train the model.\n+                An epoch is an iteration over the entire `x` and `y`\n+                data provided\n+                (unless the `steps_per_epoch` flag is set to\n+                something other than None).\n+                Note that in conjunction with `initial_epoch`,\n+                `epochs` is to be understood as \"final epoch\".\n+                The model is not trained for a number of iterations\n+                given by `epochs`, but merely until the epoch\n+                of index `epochs` is reached.\n+            verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+                \"auto\" becomes 1 for most cases.\n+                Note that the progress bar is not\n+                particularly useful when logged to a file,\n+                so `verbose=2` is recommended when not running interactively\n+                (e.g., in a production environment). Defaults to `\"auto\"`.\n+            callbacks: List of `keras_core.callbacks.Callback` instances.\n+                List of callbacks to apply during training.\n+                See `keras_core.callbacks`. Note\n+                `keras_core.callbacks.ProgbarLogger` and\n+                `keras_core.callbacks.History` callbacks are created\n+                automatically and need not be passed to `model.fit()`.\n+                `keras_core.callbacks.ProgbarLogger` is created\n+                or not based on the `verbose` argument in `model.fit()`.\n+            validation_split: Float between 0 and 1.\n+                Fraction of the training data to be used as validation data.\n+                The model will set apart this fraction of the training data,\n+                will not train on it, and will evaluate\n+                the loss and any model metrics\n+                on this data at the end of each epoch.\n+                The validation data is selected from the last samples\n+                in the `x` and `y` data provided, before shuffling. This\n+                argument is not supported when `x` is a dataset, generator or\n+                `keras_core.utils.PyDataset` instance.\n+                If both `validation_data` and `validation_split` are provided,\n+                `validation_data` will override `validation_split`.\n+            validation_data: Data on which to evaluate\n+                the loss and any model metrics at the end of each epoch.\n+                The model will not be trained on this data. Thus, note the fact\n+                that the validation loss of data provided using\n+                `validation_split` or `validation_data` is not affected by\n+                regularization layers like noise and dropout.\n+                `validation_data` will override `validation_split`.\n+                `validation_data` could be:\n+                  - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n+                  - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n+                    arrays.\n+                  - A `tf.data.Dataset`.\n+                  - A Python generator or `keras_core.utils.PyDataset` returning\n+                  `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n+            shuffle: Boolean, whether to shuffle the training data\n+                before each epoch. This argument is\n+                ignored when `x` is a generator or a `tf.data.Dataset`.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only).\n+                This can be useful to tell the model to\n+                \"pay more attention\" to samples from\n+                an under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must be\n+                one-hot encoded, or an explicit final dimension of `1` must\n+                be included for sparse class labels.\n+            sample_weight: Optional NumPy array of weights for\n+                the training samples, used for weighting the loss function\n+                (during training only). You can either pass a flat (1D)\n+                NumPy array with the same length as the input samples\n+                (1:1 mapping between weights and samples),\n+                or in the case of temporal data,\n+                you can pass a 2D array with shape\n+                `(samples, sequence_length)`,\n+                to apply a different weight to every timestep of every sample.\n+                This argument is not supported when `x` is a dataset, generator,\n+                or `keras_core.utils.PyDataset` instance, instead provide the\n+                sample_weights as the third element of `x`.\n+                Note that sample weighting does not apply to metrics specified\n+                via the `metrics` argument in `compile()`. To apply sample\n+                weighting to your metrics, you can specify them via the\n+                `weighted_metrics` in `compile()` instead.\n+            initial_epoch: Integer.\n+                Epoch at which to start training\n+                (useful for resuming a previous training run).\n+            steps_per_epoch: Integer or `None`.\n+                Total number of steps (batches of samples)\n+                before declaring one epoch finished and starting the\n+                next epoch. When training with input tensors such as\n+                backend-native tensors, the default `None` is equal to\n+                the number of samples in your dataset divided by\n+                the batch size, or 1 if that cannot be determined. If `x` is a\n+                `tf.data.Dataset`, and `steps_per_epoch`\n+                is `None`, the epoch will run until the input dataset is\n+                exhausted.  When passing an infinitely repeating dataset, you\n+                must specify the `steps_per_epoch` argument. If\n+                `steps_per_epoch=-1` the training will run indefinitely with an\n+                infinitely repeating dataset.\n+            validation_steps: Only relevant if `validation_data` is provided.\n+                Total number of steps (batches of\n+                samples) to draw before stopping when performing validation\n+                at the end of every epoch. If `validation_steps` is `None`,\n+                validation will run until the `validation_data` dataset is\n+                exhausted. In the case of an infinitely repeated dataset, it\n+                will run into an infinite loop. If `validation_steps` is\n+                specified and only part of the dataset will be consumed, the\n+                evaluation will start from the beginning of the dataset at each\n+                epoch. This ensures that the same validation samples are used\n+                every time.\n+            validation_batch_size: Integer or `None`.\n+                Number of samples per validation batch.\n+                If unspecified, will default to `batch_size`.\n+                Do not specify the `validation_batch_size` if your data is in\n+                the form of datasets or `keras_core.utils.PyDataset`\n+                instances (since they generate batches).\n+            validation_freq: Only relevant if validation data is provided.\n+              Specifies how many training epochs to run\n+              before a new validation run is performed, e.g. `validation_freq=2`\n+              runs validation every 2 epochs.\n+\n+        Unpacking behavior for iterator-like inputs:\n+            A common pattern is to pass an iterator like object such as a\n+            `tf.data.Dataset` or a `keras_core.utils.PyDataset` to `fit()`,\n+            which will in fact yield not only features (`x`)\n+            but optionally targets (`y`) and sample weights (`sample_weight`).\n+            Keras requires that the output of such iterator-likes be\n+            unambiguous. The iterator should return a tuple\n+            of length 1, 2, or 3, where the optional second and third elements\n+            will be used for `y` and `sample_weight` respectively.\n+            Any other type provided will be wrapped in\n+            a length-one tuple, effectively treating everything as `x`. When\n+            yielding dicts, they should still adhere to the top-level tuple\n+            structure,\n+            e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n+            features, targets, and weights from the keys of a single dict.\n+            A notable unsupported data type is the `namedtuple`. The reason is\n+            that it behaves like both an ordered datatype (tuple) and a mapping\n+            datatype (dict). So given a namedtuple of the form:\n+            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n+            it is ambiguous whether to reverse the order of the elements when\n+            interpreting the value. Even worse is a tuple of the form:\n+            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n+            where it is unclear if the tuple was intended to be unpacked\n+            into `x`, `y`, and `sample_weight` or passed through\n+            as a single element to `x`.\n+\n+        Returns:\n+            A `History` object. Its `History.history` attribute is\n+            a record of training loss values and metrics values\n+            at successive epochs, as well as validation loss values\n+            and validation metrics values (if applicable).\n+        \"\"\"\n         raise NotImplementedError\n \n     def evaluate(\n@@ -270,11 +449,120 @@ class Trainer:\n         return_dict=False,\n         **kwargs,\n     ):\n+        \"\"\"Returns the loss value & metrics values for the model in test mode.\n+\n+        Computation is done in batches (see the `batch_size` arg.)\n+\n+        Args:\n+            x: Input data. It could be:\n+                - A NumPy array (or array-like), or a list of arrays\n+                    (in case the model has multiple inputs).\n+                - A tensor, or a list of tensors\n+                    (in case the model has multiple inputs).\n+                - A dict mapping input names to the corresponding array/tensors,\n+                    if the model has named inputs.\n+                - A `tf.data.Dataset`. Should return a tuple\n+                    of either `(inputs, targets)` or\n+                    `(inputs, targets, sample_weights)`.\n+                - A generator or `keras_core.utils.PyDataset` returning\n+                    `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n+            y: Target data. Like the input data `x`, it could be either NumPy\n+                array(s) or backend-native tensor(s).\n+                If `x` is a `tf.data.Dataset` or `keras_core.utils.PyDataset`\n+                instance, `y` should not be specified\n+                (since targets will be obtained from the iterator/dataset).\n+            batch_size: Integer or `None`. Number of samples per batch of\n+                computation. If unspecified, `batch_size` will default to 32. Do\n+                not specify the `batch_size` if your data is in the form of a\n+                dataset, generators, or `keras_core.utils.PyDataset` instances\n+                (since they generate batches).\n+            verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = single line.\n+                `\"auto\"` becomes 1 for most cases.\n+                Note that the progress bar is not\n+                particularly useful when logged to a file, so `verbose=2` is\n+                recommended when not running interactively\n+                (e.g. in a production environment). Defaults to `\"auto\"`.\n+            sample_weight: Optional NumPy array of weights for the test samples,\n+                used for weighting the loss function. You can either pass a flat\n+                (1D) NumPy array with the same length as the input samples\n+                (1:1 mapping between weights and samples), or in the case of\n+                temporal data, you can pass a 2D array with shape `(samples,\n+                sequence_length)`, to apply a different weight to every\n+                timestep of every sample. This argument is not supported when\n+                `x` is a dataset, instead pass sample weights as the third\n+                element of `x`.\n+            steps: Integer or `None`. Total number of steps (batches of samples)\n+                before declaring the evaluation round finished. Ignored with the\n+                default value of `None`. If `x` is a `tf.data.Dataset` and\n+                `steps` is `None`, evaluation will run until the dataset\n+                is exhausted.\n+            callbacks: List of `keras_core.callbacks.Callback` instances.\n+                List of callbacks to apply during evaluation.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric.\n+                If `False`, they are returned as a list.\n+\n+        Returns:\n+            Scalar test loss (if the model has a single output and no metrics)\n+            or list of scalars (if the model has multiple outputs\n+            and/or metrics). The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n+        \"\"\"\n         raise NotImplementedError\n \n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n+        \"\"\"Generates output predictions for the input samples.\n+\n+        Computation is done in batches. This method is designed for batch\n+        processing of large numbers of inputs. It is not intended for use inside\n+        of loops that iterate over your data and process small numbers of inputs\n+        at a time.\n+\n+        For small numbers of inputs that fit in one batch,\n+        directly use `__call__()` for faster execution, e.g.,\n+        `model(x)`, or `model(x, training=False)` if you have layers such as\n+        `BatchNormalization` that behave differently during\n+        inference.\n+\n+        Note: See [this FAQ entry](\n+        https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n+        for more details about the difference between `Model` methods\n+        `predict()` and `__call__()`.\n+\n+        Args:\n+            x: Input samples. It could be:\n+                - A NumPy array (or array-like), or a list of arrays\n+                    (in case the model has multiple inputs).\n+                - A tensor, or a list of tensors\n+                    (in case the model has multiple inputs).\n+                - A `tf.data.Dataset`.\n+                - A `keras_core.utils.PyDataset` instance.\n+            batch_size: Integer or `None`.\n+                Number of samples per batch.\n+                If unspecified, `batch_size` will default to 32.\n+                Do not specify the `batch_size` if your data is in the\n+                form of dataset, generators, or `keras_core.utils.PyDataset`\n+                instances (since they generate batches).\n+            verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = single line.\n+                `\"auto\"` becomes 1 for most cases. Note that the progress bar\n+                is not particularly useful when logged to a file,\n+                so `verbose=2` is recommended when not running interactively\n+                (e.g. in a production environment). Defaults to `\"auto\"`.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n+                If `x` is a `tf.data.Dataset` and `steps` is `None`,\n+                `predict()` will run until the input dataset is exhausted.\n+            callbacks: List of `keras_core.callbacks.Callback` instances.\n+                List of callbacks to apply during prediction.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n         raise NotImplementedError\n \n     def train_on_batch(\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n import keras_core\n@@ -218,6 +219,10 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(outputs[\"y_one\"], 4 * np.ones((100, 3)))\n         self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"`steps_per_execution` not implemented for torch yet\",\n+    )\n     def test_steps_per_execution_steps_count(self):\n         class StepCount(Callback):\n             def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fba2f770a4cb15e4aa9ca31a0116ec5d3baa497c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 3524 | Contributors (this commit): 11 | Commits (past 90d): 71 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.saving import saving_api\n from keras_core.saving import saving_lib\n+from keras_core.trainers import trainer as base_trainer\n from keras_core.utils import io_utils\n from keras_core.utils import summary_utils\n from keras_core.utils import traceback_utils\n@@ -436,3 +437,11 @@ def inject_functional_model_class(cls):\n     cls.__new__(cls)\n \n     return cls\n+\n+\n+Model.fit.__doc__ = base_trainer.Trainer.fit.__doc__\n+Model.predict.__doc__ = base_trainer.Trainer.predict.__doc__\n+Model.evaluate.__doc__ = base_trainer.Trainer.evaluate.__doc__\n+Model.train_on_batch.__doc__ = base_trainer.Trainer.train_on_batch.__doc__\n+Model.test_on_batch.__doc__ = base_trainer.Trainer.test_on_batch.__doc__\n+Model.predict_on_batch.__doc__ = base_trainer.Trainer.predict_on_batch.__doc__\n\n@@ -18,3 +18,4 @@ class Optimizer(BackendOptimizer):\n \n \n base_optimizer_keyword_args = base_optimizer.base_optimizer_keyword_args\n+Optimizer.__doc__ = base_optimizer.BaseOptimizer.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5849a0953a644bd6af51b672b32a235510d4f43d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 26 | Churn Cumulative: 263 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,6 +71,16 @@ class StepsPerExecutionTuner:\n         self.thread.start()\n         return self.thread\n \n+    @property\n+    def steps_per_execution(self):\n+        \"\"\"Settable attribute representing`steps_per_execution` variable.\"\"\"\n+        return self._steps_per_execution\n+\n+    @steps_per_execution.setter\n+    def steps_per_execution(self, value):\n+        self._steps_per_execution.assign(value)\n+        self.init_spe = value\n+\n     def _steps_per_execution_interval_call(self):\n         while not self.steps_per_execution_stop_event.is_set():\n             self._measure_and_tune()\n@@ -127,7 +137,7 @@ class StepsPerExecutionTuner:\n             repeat_action_mult = 0.5\n             opposite_action_mult = 1.5\n \n-        spe_variable = self.steps_per_execution\n+        spe_variable = self._steps_per_execution\n         spe_limit = spe_variable.dtype.max / 1.5\n         current_spe = spe_variable.numpy().item()\n         if self.avg_rgsps > fast_threshold:\n@@ -145,7 +155,7 @@ class StepsPerExecutionTuner:\n         elif current_spe == 0:\n             new_spe = self.init_spe\n \n-        self.steps_per_execution.assign(np.round(new_spe))\n+        self._steps_per_execution.assign(np.round(new_spe))\n         self.prev_avg_rgsps = self.avg_rgsps\n \n     def _measure_and_tune(self):\n\n@@ -55,6 +55,18 @@ class StepsPerExecutionTuningTest(test_combinations.TestCase):\n         assert tuner.steps_per_execution_stop_event.is_set()\n         assert tuner.spe_measurement_count > 0\n \n+    def test_settable_steps_per_execution(self):\n+        spe_variable = tf.Variable(1)\n+        tuner = steps_per_execution_tuning.StepsPerExecutionTuner(\n+            mockOptimizer(5), spe_variable, interval=0.2\n+        )\n+        tuner.start()\n+        tuner.stop()\n+        assert tuner.init_spe == 1\n+        tuner.steps_per_execution = 5\n+        assert spe_variable.numpy().item() == 5\n+        assert tuner.init_spe == 5\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
