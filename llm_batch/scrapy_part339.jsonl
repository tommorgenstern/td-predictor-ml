{"custom_id": "scrapy#bbe24d79a5ee6a2afc8cd50bff4ac0e6df26886c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1141 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ import random\n import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest import skipIf\n from unittest.mock import patch\n \n import attr\n@@ -535,16 +534,16 @@ class NoimagesDropTestCase(unittest.TestCase):\n \n     def test_deprecation_warning(self):\n         arg = str()\n-        with warnings.catch_warnings(record=True) as warnings:\n+        with warnings.catch_warnings(record=True) as w:\n             NoimagesDrop(arg)\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-        with warnings.catch_warnings(record=True) as warnings:\n+            self.assertEqual(len(w), 1)\n+            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n+        with warnings.catch_warnings(record=True) as w:\n             class SubclassedNoimagesDrop(NoimagesDrop):\n                 pass\n             SubclassedNoimagesDrop(arg)\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(len(w), 1)\n+            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n \n \n def _create_image(format, *a, **kw):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae3fd0172972f672c3cf3291bcf9f28073e8d4d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 747 | Contributors (this commit): 19 | Commits (past 90d): 1 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ import pkg_resources\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter\n+from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n@@ -32,7 +32,7 @@ def _iter_command_classes(module_name):\n                 inspect.isclass(obj)\n                 and issubclass(obj, ScrapyCommand)\n                 and obj.__module__ == module.__name__\n-                and not obj == ScrapyCommand\n+                and obj not in (ScrapyCommand, BaseRunSpiderCommand)\n             ):\n                 yield obj\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1200a545439677942085f392d7477ee37b62691e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 750 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,8 @@ def _pop_command_name(argv):\n def _print_header(settings, inproject):\n     version = scrapy.__version__\n     if inproject:\n-        print(f\"Scrapy {version} - project: {settings['BOT_NAME']}\\n\")\n+        print(f\"Scrapy {version} - active project: {settings['BOT_NAME']}\\n\")\n+\n     else:\n         print(f\"Scrapy {version} - no active project\\n\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12d52a4f089798f266cbc6f86df3cc9c1cd58257", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 860 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -100,12 +100,13 @@ class Downloader:\n     def _get_slot(self, request, spider):\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n-            conc = self.per_slot_settings.get(key, {}).get(\n+            slot_settings = self.per_slot_settings.get(key, {})\n+            conc = slot_settings.get(\n                 'concurrency', self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n             )\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n-            delay = self.per_slot_settings.get(key, {}).get('delay', delay)\n-            randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n+            delay = slot_settings.get('delay', delay)\n+            randomize_delay = slot_settings.get('randomize_delay', self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#042012f6bdaf4fbd5b978ff7cc9b6796286ac7c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -70,6 +70,6 @@ class CrawlTestCase(TestCase):\n         tolerance = 0.3\n \n         delays_real = {k: v[1] - v[0] for k, v in times.items()}\n-        error_delta = {k: 1 - delays_real[k] / v.delay for k, v in slots.items()}\n+        error_delta = {k: 1 - min(delays_real[k], v.delay) / max(delays_real[k], v.delay)  for k, v in slots.items()}\n \n         self.assertTrue(max(list(error_delta.values())) < tolerance)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5b6c236a90aac37b7942793e4943347ebaa65b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 104 | Files Changed: 4 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 106 | Churn Cumulative: 1887 | Contributors (this commit): 34 | Commits (past 90d): 5 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,27 +1,4 @@\n \"\"\"Boto/botocore helpers\"\"\"\n-import warnings\n-\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-\n-\n-def is_botocore():\n-    \"\"\" Returns True if botocore is available, otherwise raises NotConfigured. Never returns False.\n-\n-    Previously, when boto was supported in addition to botocore, this returned False if boto was available\n-    but botocore wasn't.\n-    \"\"\"\n-    message = (\n-        'is_botocore() is deprecated and always returns True or raises an Exception, '\n-        'so it cannot be used for checking if boto is available instead of botocore. '\n-        'You can use scrapy.utils.boto.is_botocore_available() to check if botocore '\n-        'is available.'\n-    )\n-    warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n-    try:\n-        import botocore  # noqa: F401\n-        return True\n-    except ImportError:\n-        raise NotConfigured('missing botocore library')\n \n \n def is_botocore_available():\n\n@@ -2,17 +2,6 @@ import struct\n from gzip import GzipFile\n from io import BytesIO\n \n-from scrapy.utils.decorators import deprecated\n-\n-\n-# - GzipFile's read() has issues returning leftover uncompressed data when\n-#   input is corrupted\n-# - read1(), which fetches data before raising EOFError on next call\n-#   works here\n-@deprecated('GzipFile.read1')\n-def read1(gzf, size=-1):\n-    return gzf.read1(size)\n-\n \n def gunzip(data):\n     \"\"\"Gunzip the given data and return as much data as possible.\n\n@@ -1,20 +1,16 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n-import errno\n import gc\n import inspect\n import re\n import sys\n-import warnings\n import weakref\n from functools import partial, wraps\n from itertools import chain\n from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n-from scrapy.utils.decorators import deprecated\n \n \n def flatten(x):\n@@ -112,12 +108,6 @@ def to_bytes(text, encoding=None, errors='strict'):\n     return text.encode(encoding, errors)\n \n \n-@deprecated('to_unicode')\n-def to_native_str(text, encoding=None, errors='strict'):\n-    \"\"\" Return str representation of ``text``. \"\"\"\n-    return to_unicode(text, encoding, errors)\n-\n-\n def re_rsearch(pattern, text, chunk_size=1024):\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n@@ -263,30 +253,6 @@ def equal_attributes(obj1, obj2, attributes):\n     return True\n \n \n-class WeakKeyCache:\n-\n-    def __init__(self, default_factory):\n-        warnings.warn(\"The WeakKeyCache class is deprecated\", category=ScrapyDeprecationWarning, stacklevel=2)\n-        self.default_factory = default_factory\n-        self._weakdict = weakref.WeakKeyDictionary()\n-\n-    def __getitem__(self, key):\n-        if key not in self._weakdict:\n-            self._weakdict[key] = self.default_factory(key)\n-        return self._weakdict[key]\n-\n-\n-@deprecated\n-def retry_on_eintr(function, *args, **kw):\n-    \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n-    while True:\n-        try:\n-            return function(*args, **kw)\n-        except IOError as e:\n-            if e.errno != errno.EINTR:\n-                raise\n-\n-\n def without_none_values(iterable):\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n@@ -337,10 +303,6 @@ class MutableChain(Iterable):\n     def __next__(self):\n         return next(self.data)\n \n-    @deprecated(\"scrapy.utils.python.MutableChain.__next__\")\n-    def next(self):\n-        return self.__next__()\n-\n \n async def _async_chain(*iterables: Union[Iterable, AsyncIterable]) -> AsyncGenerator:\n     for it in iterables:\n\n@@ -1,18 +1,14 @@\n import functools\n-import gc\n import operator\n import platform\n-from itertools import count\n-from warnings import catch_warnings, filterwarnings\n \n from twisted.trial import unittest\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.defer import deferred_f_from_coro_f, aiter_errback\n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n-    WeakKeyCache, get_func_args, to_bytes, to_unicode,\n+    get_func_args, to_bytes, to_unicode,\n     without_none_values, MutableChain, MutableAsyncChain)\n \n \n@@ -27,12 +23,7 @@ class MutableChainTest(unittest.TestCase):\n         m.extend([9, 10], (11, 12))\n         self.assertEqual(next(m), 0)\n         self.assertEqual(m.__next__(), 1)\n-        with catch_warnings(record=True) as warnings:\n-            self.assertEqual(m.next(), 2)\n-            self.assertEqual(len(warnings), 1)\n-            self.assertIn('scrapy.utils.python.MutableChain.__next__',\n-                          str(warnings[0].message))\n-        self.assertEqual(list(m), list(range(3, 13)))\n+        self.assertEqual(list(m), list(range(2, 13)))\n \n \n class MutableAsyncChainTest(unittest.TestCase):\n@@ -209,27 +200,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n         a.meta['z'] = 2\n         self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n \n-    def test_weakkeycache(self):\n-        class _Weakme:\n-            pass\n-\n-        _values = count()\n-\n-        with catch_warnings():\n-            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n-            wk = WeakKeyCache(lambda k: next(_values))\n-\n-        k = _Weakme()\n-        v = wk[k]\n-        self.assertEqual(v, wk[k])\n-        self.assertNotEqual(v, wk[_Weakme()])\n-        self.assertEqual(v, wk[k])\n-        del k\n-        for _ in range(100):\n-            if wk._weakdict:\n-                gc.collect()\n-        self.assertFalse(len(wk._weakdict))\n-\n     def test_get_func_args(self):\n         def f1(a, b, c):\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a6408c3faadbd2b8b7622b8aee230b112620fad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 138 | Files Changed: 3 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): -7/37 | Churn Δ: 204 | Churn Cumulative: 1340 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,18 +6,6 @@ This package contains a collection of Link Extractors.\n For more info see docs/topics/link-extractors.rst\n \"\"\"\n import re\n-from urllib.parse import urlparse\n-from warnings import warn\n-\n-from parsel.csstranslator import HTMLTranslator\n-from w3lib.url import canonicalize_url\n-\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import (\n-    url_is_from_any_domain, url_has_any_extension,\n-)\n-\n \n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n@@ -55,82 +43,5 @@ def _is_valid_url(url):\n     return url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n \n \n-class FilteringLinkExtractor:\n-\n-    _csstranslator = HTMLTranslator()\n-\n-    def __new__(cls, *args, **kwargs):\n-        from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n-        if issubclass(cls, FilteringLinkExtractor) and not issubclass(cls, LxmlLinkExtractor):\n-            warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n-                 'please use scrapy.linkextractors.LinkExtractor instead',\n-                 ScrapyDeprecationWarning, stacklevel=2)\n-        return super().__new__(cls)\n-\n-    def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n-                 restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n-\n-        self.link_extractor = link_extractor\n-\n-        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)\n-                          for x in arg_to_iter(allow)]\n-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x)\n-                         for x in arg_to_iter(deny)]\n-\n-        self.allow_domains = set(arg_to_iter(allow_domains))\n-        self.deny_domains = set(arg_to_iter(deny_domains))\n-\n-        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n-        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n-                                          arg_to_iter(restrict_css)))\n-\n-        self.canonicalize = canonicalize\n-        if deny_extensions is None:\n-            deny_extensions = IGNORED_EXTENSIONS\n-        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n-        self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x)\n-                              for x in arg_to_iter(restrict_text)]\n-\n-    def _link_allowed(self, link):\n-        if not _is_valid_url(link.url):\n-            return False\n-        if self.allow_res and not _matches(link.url, self.allow_res):\n-            return False\n-        if self.deny_res and _matches(link.url, self.deny_res):\n-            return False\n-        parsed_url = urlparse(link.url)\n-        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n-            return False\n-        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n-            return False\n-        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n-            return False\n-        if self.restrict_text and not _matches(link.text, self.restrict_text):\n-            return False\n-        return True\n-\n-    def matches(self, url):\n-\n-        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n-            return False\n-        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n-            return False\n-\n-        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n-        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n-        return any(allowed) and not any(denied)\n-\n-    def _process_links(self, links):\n-        links = [x for x in links if self._link_allowed(x)]\n-        if self.canonicalize:\n-            for link in links:\n-                link.url = canonicalize_url(link.url)\n-        links = self.link_extractor._process_links(links)\n-        return links\n-\n-    def _extract_links(self, *args, **kwargs):\n-        return self.link_extractor._extract_links(*args, **kwargs)\n-\n-\n # Top-level imports\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n\n@@ -3,18 +3,20 @@ Link extractor based on lxml.html\n \"\"\"\n import operator\n from functools import partial\n-from urllib.parse import urljoin\n+from urllib.parse import urljoin, urlparse\n \n import lxml.etree as etree\n+from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n from scrapy.link import Link\n-from scrapy.linkextractors import FilteringLinkExtractor\n+from scrapy.linkextractors import (IGNORED_EXTENSIONS, _is_valid_url, _matches,\n+                                   _re_type, re)\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n-\n+from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n # from lxml/src/lxml/html/__init__.py\n XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n@@ -98,7 +100,8 @@ class LxmlParserLinkExtractor:\n         return links\n \n \n-class LxmlLinkExtractor(FilteringLinkExtractor):\n+class LxmlLinkExtractor:\n+    _csstranslator = HTMLTranslator()\n \n     def __init__(\n         self,\n@@ -118,7 +121,7 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n         restrict_text=None,\n     ):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n-        lx = LxmlParserLinkExtractor(\n+        self.link_extractor = LxmlParserLinkExtractor(\n             tag=partial(operator.contains, tags),\n             attr=partial(operator.contains, attrs),\n             unique=unique,\n@@ -126,18 +129,64 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n             strip=strip,\n             canonicalized=canonicalize\n         )\n-        super().__init__(\n-            link_extractor=lx,\n-            allow=allow,\n-            deny=deny,\n-            allow_domains=allow_domains,\n-            deny_domains=deny_domains,\n-            restrict_xpaths=restrict_xpaths,\n-            restrict_css=restrict_css,\n-            canonicalize=canonicalize,\n-            deny_extensions=deny_extensions,\n-            restrict_text=restrict_text,\n-        )\n+        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)\n+                          for x in arg_to_iter(allow)]\n+        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x)\n+                         for x in arg_to_iter(deny)]\n+\n+        self.allow_domains = set(arg_to_iter(allow_domains))\n+        self.deny_domains = set(arg_to_iter(deny_domains))\n+\n+        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n+        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n+                                          arg_to_iter(restrict_css)))\n+\n+        if deny_extensions is None:\n+            deny_extensions = IGNORED_EXTENSIONS\n+        self.canonicalize = canonicalize\n+        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n+        self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x)\n+                              for x in arg_to_iter(restrict_text)]\n+\n+    def _link_allowed(self, link):\n+        if not _is_valid_url(link.url):\n+            return False\n+        if self.allow_res and not _matches(link.url, self.allow_res):\n+            return False\n+        if self.deny_res and _matches(link.url, self.deny_res):\n+            return False\n+        parsed_url = urlparse(link.url)\n+        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n+            return False\n+        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n+            return False\n+        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n+            return False\n+        if self.restrict_text and not _matches(link.text, self.restrict_text):\n+            return False\n+        return True\n+\n+    def matches(self, url):\n+\n+        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n+            return False\n+        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n+            return False\n+\n+        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n+        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n+        return any(allowed) and not any(denied)\n+\n+    def _process_links(self, links):\n+        links = [x for x in links if self._link_allowed(x)]\n+        if self.canonicalize:\n+            for link in links:\n+                link.url = canonicalize_url(link.url)\n+        links = self.link_extractor._process_links(links)\n+        return links\n+\n+    def _extract_links(self, *args, **kwargs):\n+        return self.link_extractor._extract_links(*args, **kwargs)\n \n     def extract_links(self, response):\n         \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n\n@@ -1,12 +1,9 @@\n import pickle\n import re\n import unittest\n-from warnings import catch_warnings\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n-from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n from tests import get_testdata\n \n@@ -517,32 +514,3 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n \n     def test_restrict_xpaths_with_html_entities(self):\n         super().test_restrict_xpaths_with_html_entities()\n-\n-    def test_filteringlinkextractor_deprecation_warning(self):\n-        \"\"\"Make sure the FilteringLinkExtractor deprecation warning is not\n-        issued for LxmlLinkExtractor\"\"\"\n-        with catch_warnings(record=True) as warnings:\n-            LxmlLinkExtractor()\n-            self.assertEqual(len(warnings), 0)\n-\n-            class SubclassedLxmlLinkExtractor(LxmlLinkExtractor):\n-                pass\n-\n-            SubclassedLxmlLinkExtractor()\n-            self.assertEqual(len(warnings), 0)\n-\n-\n-class FilteringLinkExtractorTest(unittest.TestCase):\n-\n-    def test_deprecation_warning(self):\n-        args = [None] * 10\n-        with catch_warnings(record=True) as warnings:\n-            FilteringLinkExtractor(*args)\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-        with catch_warnings(record=True) as warnings:\n-            class SubclassedFilteringLinkExtractor(FilteringLinkExtractor):\n-                pass\n-            SubclassedFilteringLinkExtractor(*args)\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24a18e9af13a482c1dd6036046226cf57318477b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 48 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class MemoryUsage:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit / 1024 / 1024\n             logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n-                         {'memusage': mem}, extra={'crawler': self.crawler})\n+                         {'memusage': mem, 'virtualsize': self.get_virtual_size()}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f2adad7a7f2d858acd082afd6e9a6835bfddcd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 52 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,7 @@ class MemoryUsage:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit / 1024 / 1024\n             logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n-                         {'memusage': mem, 'virtualsize': self.get_virtual_size()}, extra={'crawler': self.crawler})\n+                         {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n@@ -92,6 +92,8 @@ class MemoryUsage:\n                 self.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')\n             else:\n                 self.crawler.stop()\n+        else:\n+            logger.info(\"Current memory usage is %(virtualsize)dM\", {'virtualsize': self.get_virtual_size()})\n \n     def _check_warning(self):\n         if self.warned:  # warn only once\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb159c78f10f546e38e62a9230608269ac13acdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 57 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,7 +75,8 @@ class MemoryUsage:\n         self.crawler.stats.max_value('memusage/max', self.get_virtual_size())\n \n     def _check_limit(self):\n-        if self.get_virtual_size() > self.limit:\n+        current_mem_usage = self.get_virtual_size()\n+        if current_mem_usage > self.limit:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit / 1024 / 1024\n             logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n@@ -93,7 +94,7 @@ class MemoryUsage:\n             else:\n                 self.crawler.stop()\n         else:\n-            logger.info(\"Current memory usage is %(virtualsize)dM\", {'virtualsize': self.get_virtual_size()})\n+            logger.info(\"Current memory usage is %(virtualsize)dM\", {'virtualsize': current_mem_usage / 1024 / 1024})\n \n     def _check_warning(self):\n         if self.warned:  # warn only once\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e0025f53dc724d986855b572b6237d0a96fd821", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 29 | Churn Cumulative: 562 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -6,7 +6,7 @@ from os.path import join, dirname, abspath, isabs, exists\n \n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import Settings\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import NotConfigured\n \n \n ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n@@ -67,23 +67,16 @@ def get_project_settings():\n     if settings_module_path:\n         settings.setmodule(settings_module_path, priority='project')\n \n-    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n-                      k.startswith('SCRAPY_')}\n     valid_envvars = {\n         'CHECK',\n         'PROJECT',\n         'PYTHON_SHELL',\n         'SETTINGS_MODULE',\n     }\n-    setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}\n-    if setting_envvars:\n-        setting_envvar_list = ', '.join(sorted(setting_envvars))\n-        warnings.warn(\n-            'Use of environment variables prefixed with SCRAPY_ to override '\n-            'settings is deprecated. The following environment variables are '\n-            f'currently defined: {setting_envvar_list}',\n-            ScrapyDeprecationWarning\n-        )\n+\n+    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n+                      k.startswith('SCRAPY_') and k.replace('SCRAPY_', '') in valid_envvars}\n+\n     settings.setdict(scrapy_envvars, priority='project')\n \n     return settings\n\n@@ -5,9 +5,6 @@ import shutil\n import contextlib\n import warnings\n \n-from pytest import warns\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.project import data_path, get_project_settings\n \n \n@@ -80,10 +77,10 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n         envvars = {\n             'SCRAPY_FOO': 'bar',\n         }\n-        with warns(ScrapyDeprecationWarning, match=': FOO') as record:\n         with set_env(**envvars):\n-                get_project_settings()\n-        assert len(record) == 1\n+            settings = get_project_settings()\n+\n+        assert settings.get(\"SCRAPY_FOO\") is None\n \n     def test_valid_and_invalid_envvars(self):\n         value = 'tests.test_cmdline.settings'\n@@ -91,8 +88,7 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n             'SCRAPY_FOO': 'bar',\n             'SCRAPY_SETTINGS_MODULE': value,\n         }\n-        with warns(ScrapyDeprecationWarning, match=': FOO') as record:\n         with set_env(**envvars):\n             settings = get_project_settings()\n-        assert len(record) == 1\n         assert settings.get('SETTINGS_MODULE') == value\n+        assert settings.get('SCRAPY_FOO') is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1506479672ee54adc2d7d1ecffc0b224f2fcf7aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 122 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,10 +31,6 @@ class CmdlineTest(unittest.TestCase):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s',\n                                        'TEST1=override'), 'override')\n \n-    def test_override_settings_using_envvar(self):\n-        self.env['SCRAPY_TEST1'] = 'override'\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'override')\n-\n     def test_profiling(self):\n         path = tempfile.mkdtemp()\n         filename = os.path.join(path, 'res.prof')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc8968672a5cc699f0103aeb2da1ebea7ed3c235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 63 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -75,8 +75,8 @@ class MemoryUsage:\n         self.crawler.stats.max_value('memusage/max', self.get_virtual_size())\n \n     def _check_limit(self):\n-        current_mem_usage = self.get_virtual_size()\n-        if current_mem_usage > self.limit:\n+        peak_mem_usage = self.get_virtual_size()\n+        if peak_mem_usage > self.limit:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit / 1024 / 1024\n             logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n@@ -94,7 +94,7 @@ class MemoryUsage:\n             else:\n                 self.crawler.stop()\n         else:\n-            logger.info(\"Current memory usage is %(virtualsize)dM\", {'virtualsize': current_mem_usage / 1024 / 1024})\n+            logger.info(\"Peak memory usage is %(virtualsize)dM\", {'virtualsize': peak_mem_usage / 1024 / 1024})\n \n     def _check_warning(self):\n         if self.warned:  # warn only once\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdc0bca5b1aab15873b82f93a4bdf8fb3ce82824", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 73 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -79,12 +79,12 @@ class MemoryUsage:\n         if peak_mem_usage > self.limit:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit / 1024 / 1024\n-            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n+            logger.error(\"Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...\",\n                          {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} terminated: \"\n-                    f\"memory usage exceeded {mem}M at {socket.gethostname()}\"\n+                    f\"memory usage exceeded {mem}MiB at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/limit_notified', 1)\n@@ -94,7 +94,7 @@ class MemoryUsage:\n             else:\n                 self.crawler.stop()\n         else:\n-            logger.info(\"Peak memory usage is %(virtualsize)dM\", {'virtualsize': peak_mem_usage / 1024 / 1024})\n+            logger.info(\"Peak memory usage is %(virtualsize)dMiB\", {'virtualsize': peak_mem_usage / 1024 / 1024})\n \n     def _check_warning(self):\n         if self.warned:  # warn only once\n@@ -102,12 +102,12 @@ class MemoryUsage:\n         if self.get_virtual_size() > self.warning:\n             self.crawler.stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning / 1024 / 1024\n-            logger.warning(\"Memory usage reached %(memusage)dM\",\n+            logger.warning(\"Memory usage reached %(memusage)dMiB\",\n                            {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = (\n                     f\"{self.crawler.settings['BOT_NAME']} warning: \"\n-                    f\"memory usage reached {mem}M at {socket.gethostname()}\"\n+                    f\"memory usage reached {mem}MiB at {socket.gethostname()}\"\n                 )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/warning_notified', 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fdd0a70a0d56bc7829aa3028c191c062cd9f935", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1518 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -151,7 +151,7 @@ class Stream:\n \n         self._deferred_response = Deferred(_cancel)\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return f'Stream(id={self.stream_id!r})'\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3b1700774bd16623622963e98fd3ec759b8a88f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 482 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -121,7 +121,7 @@ class Request(object_ref):\n     def encoding(self) -> str:\n         return self._encoding\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return f\"<{self.method} {self.url}>\"\n \n     def copy(self) -> \"Request\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e769532644e1176c7984dd513249285c7f64c7d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 56 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 60 | Churn Cumulative: 5036 | Contributors (this commit): 45 | Commits (past 90d): 6 | Contributors (cumulative): 74 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,6 @@\n import ipaddress\n import logging\n import re\n-import warnings\n from contextlib import suppress\n from io import BytesIO\n from time import time\n@@ -22,7 +21,7 @@ from zope.interface import implementer\n from scrapy import signals\n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n-from scrapy.exceptions import ScrapyDeprecationWarning, StopDownload\n+from scrapy.exceptions import StopDownload\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -279,17 +278,7 @@ class ScrapyAgent:\n             proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             proxyHost = to_unicode(proxyHost)\n-            omitConnectTunnel = b'noconnect' in proxyParams\n-            if omitConnectTunnel:\n-                warnings.warn(\n-                    \"Using HTTPS proxies in the noconnect mode is deprecated. \"\n-                    \"If you use Zyte Smart Proxy Manager, it doesn't require \"\n-                    \"this mode anymore, so you should update scrapy-crawlera \"\n-                    \"to scrapy-zyte-smartproxy and remove '?noconnect' \"\n-                    \"from the Zyte Smart Proxy Manager URL.\",\n-                    ScrapyDeprecationWarning,\n-                )\n-            if scheme == b'https' and not omitConnectTunnel:\n+            if scheme == b'https':\n                 proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                 proxyConf = (proxyHost, proxyPort, proxyAuth)\n                 return self._TunnelingAgent(\n@@ -302,8 +291,6 @@ class ScrapyAgent:\n                 )\n             else:\n                 proxyScheme = proxyScheme or b'http'\n-                proxyHost = to_bytes(proxyHost, encoding='ascii')\n-                proxyPort = to_bytes(str(proxyPort), encoding='ascii')\n                 proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n                 return self._ProxyAgent(\n                     reactor=reactor,\n\n@@ -1,4 +1,3 @@\n-import warnings\n from time import time\n from typing import Optional, Type, TypeVar\n from urllib.parse import urldefrag\n@@ -69,19 +68,8 @@ class ScrapyH2Agent:\n         if proxy:\n             _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n             scheme = _parse(request.url)[0]\n-            proxy_host = proxy_host.decode()\n-            omit_connect_tunnel = b'noconnect' in proxy_params\n-            if omit_connect_tunnel:\n-                warnings.warn(\n-                    \"Using HTTPS proxies in the noconnect mode is not \"\n-                    \"supported by the downloader handler. If you use Zyte \"\n-                    \"Smart Proxy Manager, it doesn't require this mode \"\n-                    \"anymore, so you should update scrapy-crawlera to \"\n-                    \"scrapy-zyte-smartproxy and remove '?noconnect' from the \"\n-                    \"Zyte Smart Proxy Manager URL.\"\n-                )\n \n-            if scheme == b'https' and not omit_connect_tunnel:\n+            if scheme == b'https':\n                 # ToDo\n                 raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')\n             return self._ProxyAgent(\n\n@@ -24,7 +24,7 @@ from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Headers, HtmlResponse, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n@@ -757,18 +757,6 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n-    def test_download_with_proxy_https_noconnect(self):\n-        def _test(response):\n-            self.assertEqual(response.status, 200)\n-            self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'https://example.com')\n-\n-        http_proxy = f'{self.getURL(\"\")}?noconnect'\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n-        with self.assertWarnsRegex(ScrapyDeprecationWarning,\n-                                   r'Using HTTPS proxies in the noconnect mode is deprecated'):\n-            return self.download_request(request, Spider('foo')).addCallback(_test)\n-\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n\n@@ -242,21 +242,6 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     def getURL(self, path):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n-    def test_download_with_proxy_https_noconnect(self):\n-        def _test(response):\n-            self.assertEqual(response.status, 200)\n-            self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'/')\n-\n-        http_proxy = f\"{self.getURL('')}?noconnect\"\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n-        with self.assertWarnsRegex(\n-            Warning,\n-            r'Using HTTPS proxies in the noconnect mode is not supported by the '\n-            r'downloader handler.'\n-        ):\n-            return self.download_request(request, Spider('foo')).addCallback(_test)\n-\n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n         with self.assertRaises(NotImplementedError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6e9e6592a28a11517295847b177b14f41cb8a26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 29 | Churn Cumulative: 1449 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -435,30 +435,6 @@ class BaseSettings(MutableMapping):\n             p.text(pformat(self.copy_to_dict()))\n \n \n-class _DictProxy(MutableMapping):\n-\n-    def __init__(self, settings, priority):\n-        self.o = {}\n-        self.settings = settings\n-        self.priority = priority\n-\n-    def __len__(self):\n-        return len(self.o)\n-\n-    def __getitem__(self, k):\n-        return self.o[k]\n-\n-    def __setitem__(self, k, v):\n-        self.settings.set(k, v, priority=self.priority)\n-        self.o[k] = v\n-\n-    def __delitem__(self, k):\n-        del self.o[k]\n-\n-    def __iter__(self, k, v):\n-        return iter(self.o)\n-\n-\n class Settings(BaseSettings):\n     \"\"\"\n     This object stores Scrapy settings for the configuration of internal\n\n@@ -2,6 +2,7 @@\n \n import warnings\n import inspect\n+from typing import List, Tuple\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -126,9 +127,7 @@ def _clspath(cls, forced=None):\n     return f'{cls.__module__}.{cls.__name__}'\n \n \n-DEPRECATION_RULES = [\n-    ('scrapy.telnet.', 'scrapy.extensions.telnet.'),\n-]\n+DEPRECATION_RULES: List[Tuple[str, str]] = []\n \n \n def update_classpath(path):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6e98ce6b6c766ee735f45f25f78d957327f54e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 511 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -44,14 +44,12 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n                 raise ValueError(f'Invalid value {value} for component {name}, '\n                                  'please provide a real number or None instead')\n \n-    # BEGIN Backward compatibility for old (base, custom) call signature\n     if isinstance(custom, (list, tuple)):\n         _check_components(custom)\n         return type(custom)(convert(c) for c in custom)\n \n     if custom is not None:\n         compdict.update(custom)\n-    # END Backward compatibility\n \n     _validate_values(compdict)\n     compdict = without_none_values(_map_keys(compdict))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
