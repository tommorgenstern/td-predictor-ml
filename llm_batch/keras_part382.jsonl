{"custom_id": "keras#a3678aa8dda7bb679067799c672a564edf65ec2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 54 | Files Changed: 2 | Hunks: 19 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 88 | Churn Cumulative: 2963 | Contributors (this commit): 33 | Commits (past 90d): 18 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -165,26 +165,6 @@ def print_summary(model,\n         if not sequential_like:\n           break\n     \n-  if show_trainable:\n-    # Add trainable column\n-    if sequential_like:\n-      line_length = line_length or 85\n-      positions = positions or [.33, .70, .89, 1.]\n-      if positions[-1] <= 1:\n-        positions = [int(line_length * p) for p in positions]\n-      # header names for the different log elements\n-      to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Trainable']\n-    else:\n-      line_length = line_length or 110\n-      positions = positions or [.33, .40, .55, .67, 1.]\n-      if positions[-1] <= 1:\n-        positions = [int(line_length * p) for p in positions]\n-      # header names for the different log elements\n-      to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Trainable', 'Connected to']\n-      relevant_nodes = []\n-      for v in model._nodes_by_depth.values():\n-        relevant_nodes += v\n-  else:\n   if sequential_like:\n     line_length = line_length or 65\n     positions = positions or [.45, .85, 1.]\n@@ -203,6 +183,11 @@ def print_summary(model,\n     for v in model._nodes_by_depth.values():\n       relevant_nodes += v\n \n+  if show_trainable:\n+    line_length += 11\n+    positions.append(line_length)\n+    to_display.append(\"Trainable\")\n+\n   def print_row(fields, positions, nested_level=0):\n     left_to_print = [str(x) for x in fields]\n     while any(left_to_print):\n@@ -271,11 +256,10 @@ def print_summary(model,\n       params = '0 (unused)'\n     else:\n       params = layer.count_params()\n+    fields = [name + ' (' + cls_name + ')', output_shape, params]\n \n     if show_trainable:\n-      fields = [name + ' (' + cls_name + ')', output_shape, params, \"T\" if layer.trainable else \"F\"]\n-    else:\n-      fields = [name + ' (' + cls_name + ')', output_shape, params]\n+      fields.append(\"Y\" if layer.trainable else \"N\")\n \n     print_row(fields, positions, nested_level)\n \n@@ -303,18 +287,14 @@ def print_summary(model,\n \n     name = layer.name\n     cls_name = layer.__class__.__name__\n-    if show_trainable:\n-      fields = [\n-        name + ' (' + cls_name + ')', output_shape,\n-        layer.count_params(), \n-        \"T\" if layer.trainable else \"F\", \n-        connections\n-      ]\n-    else:\n     fields = [\n         name + ' (' + cls_name + ')', output_shape,\n         layer.count_params(), connections\n     ]\n+      \n+    if show_trainable:\n+      fields.append(\"Y\" if layer.trainable else \"N\")\n+\n     print_row(fields, positions, nested_level)\n \n   def print_layer(layer, nested_level=0, is_nested_last=False):\n\n@@ -252,20 +252,20 @@ class LayerUtilsTest(tf.test.TestCase):\n       reader.close()\n       check_str = (\n         'Model: \"trainable\"\\n'\n-        '_____________________________________________________________________________________\\n'\n+        '____________________________________________________________________________\\n'\n         ' Layer (type)                Output Shape              Param #   Trainable  \\n'\n-        '=====================================================================================\\n'\n-        ' conv (Conv2D)              (None, 2, 3, 2)                62              F         \\n'\n+        '============================================================================\\n'\n+        ' conv (Conv2D)               (None, 2, 3, 2)           62        N          \\n'\n         '                                                                            \\n'\n-        ' flat (Flatten)             (None, 12)                     0               T         \\n'\n+        ' flat (Flatten)              (None, 12)                0         Y          \\n'\n         '                                                                            \\n'\n-        ' dense (Dense)              (None, 5)                      65              T         \\n'\n+        ' dense (Dense)               (None, 5)                 65        Y          \\n'\n         '                                                                            \\n'\n-        '=====================================================================================\\n'\n+        '============================================================================\\n'\n         'Total params: 127\\n'\n         'Trainable params: 65\\n'\n         'Non-trainable params: 62\\n'\n-        '_____________________________________________________________________________________\\n')\n+        '____________________________________________________________________________\\n')\n \n       fin_str = ''\n       for line in lines:\n@@ -313,30 +313,30 @@ class LayerUtilsTest(tf.test.TestCase):\n       reader.close()\n       check_str = (\n         'Model: \"model_2\"\\n'\n-        '_____________________________________________________________________________________\\n'\n+        '____________________________________________________________________________\\n'\n         ' Layer (type)                Output Shape              Param #   Trainable  \\n'\n-        '=====================================================================================\\n'\n-        ' input3 (InputLayer)        [(None, None, None, 3)]        0               T         \\n'\n+        '============================================================================\\n'\n+        ' input3 (InputLayer)         [(None, None, None, 3)]   0         Y          \\n'\n         '                                                                            \\n'\n-        ' model_1 (Functional)       (None, None, None, 3)          24              T         \\n'\n-        '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n-        '| input1 (InputLayer)      [(None, None, None, 3)]        0               T         |\\n'\n+        ' model_1 (Functional)        (None, None, None, 3)     24        Y          \\n'\n+        '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n+        '| input1 (InputLayer)       [(None, None, None, 3)]   0         Y          |\\n'\n         '|                                                                          |\\n'\n-        '| model (Functional)       (None, None, None, 3)          24              T         |\\n'\n-        '||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\\n'\n-        '|| input2 (InputLayer)    [(None, None, None, 3)]        0               T         ||\\n'\n+        '| model (Functional)        (None, None, None, 3)     24        Y          |\\n'\n+        '||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\\n'\n+        '|| input2 (InputLayer)     [(None, None, None, 3)]   0         Y          ||\\n'\n         '||                                                                        ||\\n'\n-        '|| conv2d (Conv2D)        (None, None, None, 3)          12              F         ||\\n'\n+        '|| conv2d (Conv2D)         (None, None, None, 3)     12        N          ||\\n'\n         '||                                                                        ||\\n'\n-        '|| batch_normalization (Batch  (None, None, None, 3)     12              T         ||\\n'\n-        '|| Normalization)                                                                  ||\\n'\n-        '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n-        '¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n'\n-        '=====================================================================================\\n'\n+        '|| batch_normalization (BatchN  (None, None, None, 3)  12      Y          ||\\n'\n+        '|| ormalization)                                                          ||\\n'\n+        '|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n'\n+        '¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n'\n+        '============================================================================\\n'\n         'Total params: 24\\n'\n         'Trainable params: 6\\n'\n         'Non-trainable params: 18\\n'\n-        '_____________________________________________________________________________________\\n'\n+        '____________________________________________________________________________\\n'\n       )\n \n       fin_str = ''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1c010836ef5d294ceeadd743cebc1c9d0685c59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 77 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,77 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test to demonstrate Keras training with MultiWorkerMirroredStrategy.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import os\n+from absl import logging\n+import tensorflow.compat.v2 as tf\n+\n+NUM_WORKERS = 2\n+NUM_EPOCHS = 2\n+NUM_STEPS_PER_EPOCH = 50\n+\n+\n+class MwmsMultiProcessRunnerTest(tf.test.TestCase):\n+  \"\"\"Test to demonstrate Keras training with MultiWorkerMirroredStrategy.\"\"\"\n+\n+  def testMwmsWithModelFit(self):\n+\n+    def worker_fn():\n+\n+      def dataset_fn(input_context):\n+        del input_context  # User should shard data accordingly. Omitted here.\n+        return tf.data.Dataset.from_tensor_slices((tf.random.uniform(\n+            (6, 10)), tf.random.uniform((6, 10)))).batch(2)\n+\n+      strategy = tf.distribute.MultiWorkerMirroredStrategy()\n+      with strategy.scope():\n+        model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n+      model.compile(\n+          loss=tf.keras.losses.CategoricalCrossentropy(),\n+          optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n+          metrics=['accuracy'])\n+\n+      callbacks = [\n+          tf.keras.callbacks.ModelCheckpoint(\n+              filepath=os.path.join(self.get_temp_dir(), 'checkpoint'))\n+      ]\n+      dataset = strategy.distribute_datasets_from_function(dataset_fn)\n+      model.fit(\n+          dataset,\n+          epochs=NUM_EPOCHS,\n+          steps_per_epoch=NUM_STEPS_PER_EPOCH,\n+          callbacks=callbacks)\n+\n+      logging.info('testMwmsWithModelFit successfully ends')\n+\n+    mpr_result = tf.__internal__.distribute.multi_process_runner.run(\n+        worker_fn,\n+        tf.__internal__.distribute.multi_process_runner.create_cluster_spec(\n+            num_workers=NUM_WORKERS),\n+        return_output=True)\n+\n+    # Verifying the worker functions ended successfully.\n+    self.assertTrue(\n+        any([\n+            'testMwmsWithModelFit successfully ends' in msg\n+            for msg in mpr_result.stdout\n+        ]))\n+\n+\n+if __name__ == '__main__':\n+  tf.__internal__.distribute.multi_process_runner.test_main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c194341b928da32817cca8756c2416284465d2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1667 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,7 +175,7 @@ class LossesContainer(Container):\n       regularization_losses: Additional losses to be added to the total loss.\n \n     Returns:\n-      Tuple of `(total_loss, per_output_loss_list)`\n+      The total loss as a `tf.Tensor`, or `None` if no loss results.\n     \"\"\"\n     y_true = self._conform_to_outputs(y_pred, y_true)\n     sample_weight = self._conform_to_outputs(y_pred, sample_weight)\n\n@@ -31,6 +31,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n \n     self.assertTrue(loss_container._built)\n     self.assertLen(loss_container._losses, 1)\n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(total_loss.numpy(), 1.)\n     self.assertLen(loss_container.metrics, 1)\n \n@@ -89,6 +90,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     total_loss = loss_container(y_t, y_p, sample_weight=sw)\n \n     self.assertLen(loss_container._losses, 2)\n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(total_loss.numpy(), 0.25)\n     self.assertLen(loss_container.metrics, 3)\n \n@@ -142,6 +144,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n \n     total_loss = loss_container(y_t, y_p, sample_weight=sw)\n \n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(total_loss.numpy(), 0.5)\n     self.assertLen(loss_container.metrics, 2)\n \n@@ -176,6 +179,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n \n     total_loss = loss_container(y_t, y_p, sample_weight=sw)\n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(total_loss.numpy(), 0.75)\n     self.assertLen(loss_container.metrics, 3)\n \n@@ -292,6 +296,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n \n     loss_container = compile_utils.LossesContainer(my_mae)\n     total_loss = loss_container(y_t, y_p)\n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(total_loss.dtype, tf.float64)\n \n   def test_loss_masking(self):\n@@ -384,8 +389,9 @@ class LossesContainerTest(keras_parameterized.TestCase):\n         tf.RaggedTensor.from_row_splits(v_t, [0, 2, 3]), 0)\n     y_p = tf.expand_dims(\n         tf.RaggedTensor.from_row_splits(v_p, [0, 2, 3]), 0)\n-    loss_container(y_t, y_p)\n+    total_loss = loss_container(y_t, y_p)\n \n+    self.assertIsInstance(total_loss, tf.Tensor)\n     self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44200a4b8ecbb72fc704bee8f10880206886bc90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 75 | Churn Cumulative: 21263 | Contributors (this commit): 119 | Commits (past 90d): 47 | Contributors (cumulative): 131 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,6 @@\n # ==============================================================================\n \"\"\"Utilities for `Model.compile`.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n import copy\n from keras import losses as losses_mod\n@@ -22,6 +21,7 @@ from keras import metrics as metrics_mod\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n \n \n class Container:\n@@ -243,8 +243,7 @@ class LossesContainer(Container):\n       total_loss = tf.add_n(loss_values)\n       return total_loss\n     else:\n-      # Ok for a model to have no compiled loss.\n-      return tf.zeros(shape=())\n+      return None\n \n   def reset_state(self):\n     \"\"\"Resets the state of loss metrics.\"\"\"\n\n@@ -777,6 +777,36 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   def run_eagerly(self, value):\n     self._run_eagerly = value\n \n+  def _validate_target_and_loss(self, y, loss):\n+    \"\"\"Raises error if target or loss is not found.\n+\n+    This method verifies that the target and loss are properly populated\n+    when applicable, or raises errors.\n+\n+    Args:\n+      y: the target for training.\n+      loss: the total loss tensor including loss added via `compile` and\n+        `add_loss`.\n+    \"\"\"\n+\n+    # `self.loss` references the loss added via `compile` call. If users have\n+    # provided such, the target must be provided; otherwise it's a user error.\n+    # Note that `self.loss` does not include losses added via `add_loss`, and it\n+    # is a valid use when such loss from `add_loss` exists and target does not.\n+    if self.loss and y is None:\n+      raise ValueError(\n+          'Target data is missing. Your model was compiled with '\n+          f'loss={self.loss}, '\n+          'and therefore expects target data to be provided in `fit()`.')\n+\n+    # For training, there must be compiled loss or regularization loss to exist\n+    # in order to apply the gradients. If one is not found, it means no loss\n+    # was supplied via `compile` or `add_loss`.\n+    elif loss is None:\n+      raise ValueError(\n+          'No loss found. You may have forgotten to provide a `loss` argument '\n+          'in the `compile()` method.')\n+\n   def train_step(self, data):\n     \"\"\"The logic for one training step.\n \n@@ -808,10 +838,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       y_pred = self(x, training=True)\n       loss = self.compiled_loss(\n           y, y_pred, sample_weight, regularization_losses=self.losses)\n-    if self.loss and y is None:\n-      raise TypeError(\n-          f'Target data is missing. Your model has `loss`: {self.loss}, '\n-          'and therefore expects target data to be passed in `fit()`.')\n+    self._validate_target_and_loss(y, loss)\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n\n@@ -14,16 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for training routines.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n import collections\n import io\n-import tempfile\n import sys\n-\n+import tempfile\n from absl.testing import parameterized\n-import numpy as np\n-from tensorflow.python.framework import test_util as tf_test_util\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n@@ -39,6 +35,9 @@ from keras.engine import training as training_module\n from keras.engine import training_utils_v1\n from keras.utils import data_utils\n from keras.utils import np_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import test_util as tf_test_util\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.rmsprop import RMSPropOptimizer\n \n@@ -91,6 +90,28 @@ class TrainingTest(keras_parameterized.TestCase):\n                                 'Unexpected result of `train_function`.*'):\n       model.fit(x=np.array([]), y=np.array([]))\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_fit_without_loss_at_compile(self):\n+    model = sequential.Sequential([layers_module.Dense(1)])\n+    model.compile('sgd', run_eagerly=testing_utils.should_run_eagerly())\n+    x, y = np.ones((10, 1)), np.ones((10, 1))\n+    with self.assertRaisesRegex(ValueError, 'No loss found..*'):\n+      model.fit(x, y, epochs=2)\n+\n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_fit_without_loss_at_compile_but_with_add_loss(self):\n+\n+    class MyModel(sequential.Sequential):\n+\n+      def call(self, x):\n+        self.add_loss(tf.reduce_sum(x))\n+        return x\n+\n+    model = MyModel([layers_module.Dense(1)])\n+    model.compile('sgd', run_eagerly=testing_utils.should_run_eagerly())\n+    x, y = np.ones((10, 1)), np.ones((10, 1))\n+    model.fit(x, y, epochs=2)\n+\n   @keras_parameterized.run_all_keras_modes\n   def test_run_eagerly_setting(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n@@ -1723,7 +1744,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     model = training_module.Model(inputs, outputs)\n     model.compile('rmsprop', 'mse')\n     x = np.zeros((32, 3))\n-    with self.assertRaisesRegex(TypeError, 'Target data is missing..*'):\n+    with self.assertRaisesRegex(ValueError, 'Target data is missing..*'):\n       model.fit(x)\n \n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -1737,7 +1758,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     model.compile('rmsprop', 'mse')\n     x = np.zeros((32, 3))\n     y = np.zeros((32, 2))\n-    with self.assertRaisesRegex(TypeError, 'Target data is missing..*'):\n+    with self.assertRaisesRegex(ValueError, 'Target data is missing..*'):\n       model.fit({'a': x, 'b': x, 'c': y})\n \n   @keras_parameterized.run_all_keras_modes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c9eb18f6d8e45c021a21b12f4a828ac3e7c7593", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1730 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1348,7 +1348,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n   def _create_or_restore_slot_variable(\n       self, slot_variable_position, slot_name, variable):\n-    \"\"\"Restore a slot variable's value, possibly creating it.\n+    \"\"\"Returns the slot variable that should have a value restored into it.\n+\n+    It is up to the caller to retore the value into the slot variable if a valid\n+    slot variable is returned.\n \n     Called when a variable which has an associated slot variable is created or\n     restored. When executing eagerly, we create the slot variable with a\n@@ -1366,6 +1369,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         indicating the slot variable `Trackable` object to be restored.\n       slot_name: The name of this `Optimizer`'s slot to restore into.\n       variable: The variable object this slot is being created for.\n+\n+    Returns:\n+      A slot variable that should have a value restored into it, or None if a\n+      slot variable should not be restored at this time.\n     \"\"\"\n     variable_key = _var_key(variable)\n     slot_dict = self._slots.get(variable_key, {})\n@@ -1404,7 +1411,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if slot_variable is not None:\n       # If we've either made this slot variable, or if we've pulled out an\n       # existing slot variable, we should restore it.\n-      slot_variable_position.restore(slot_variable)\n+      return slot_variable\n     else:\n       # We didn't make the slot variable. Defer restoring until it gets created\n       # normally. We keep a list rather than the one with the highest restore\n@@ -1413,6 +1420,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       self._deferred_slot_restorations.setdefault(\n           slot_name, {}).setdefault(variable_key, []).append(\n               slot_variable_position)\n+    return None\n \n   @contextlib.contextmanager\n   def _distribution_strategy_scope(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c5cb387720bbe096b22e473d9b78e70960f63ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1700 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -115,7 +115,7 @@ DEFAULT_BLOCKS_ARGS = {\n       \"output_filters\": 256,\n       \"se_ratio\": 0.25,\n       \"strides\": 2,\n-        },\n+    }\n   ],\n   \"efficientnetv2-m\": [\n     {\n@@ -715,7 +715,9 @@ def Block0(\n       if strides == 1 and input_filters == output_filters:\n         if survival_probability:\n           x = layers.Dropout(\n-              survival_probability, noise_shape=(None, 1, 1, 1), name=name + 'drop'\n+            survival_probability,\n+            noise_shape=(None, 1, 1, 1),\n+            name=name + 'drop'\n           )(x)\n \n         x = layers.add([x, inputs], name=name + 'add')\n@@ -815,7 +817,9 @@ def Block1(\n     if strides == 1 and input_filters == output_filters:\n       if survival_probability:\n         x = layers.Dropout(\n-                  survival_probability, noise_shape=(None, 1, 1, 1), name=name + 'drop'\n+              survival_probability,\n+              noise_shape=(None, 1, 1, 1),\n+              name=name + 'drop'\n             )(x)\n       x = layers.add([x, inputs], name=name + 'add')\n     return x\n@@ -918,7 +922,8 @@ def EfficientNetV2(\n     min_size=32,\n     data_format=backend.image_data_format(),\n     require_flatten=include_top,\n-      weights=weights)\n+    weights=weights\n+  )\n   \n   if input_tensor is None:\n     img_input = layers.Input(shape=input_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83bb203ae2139e2470e771eb852948ab1e46876d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 176 | Lines Deleted: 176 | Files Changed: 1 | Hunks: 118 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 352 | Churn Cumulative: 2052 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -37,20 +37,20 @@ from tensorflow.python.util.tf_export import keras_export\n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/sebastian-sz-weights/efficientnet-v2/\"\n \n WEIGHTS_HASHES = {\n-  'b0': ('42016207e946245a808dc807e21e2334',\n-         '953eb10b448fc338a0206deb1a0228c8'),\n-  'b1': ('fda7eaf4fe84f9ab1aab43ab0b7ffd26',\n-         'fa1764f91f33620586b628e18f265ed4'),\n-  'b2': ('c991196606d604057d3b713ed41087d9',\n-         'd3ff2c5f2ae01882452d0d8f1b38c3d7'),\n-  'b3': ('0e02aea454e5735908f321f3946daf0a',\n-         '0e8a2ed23f6c14cf1a4dddfcd2682cc9'),\n-  '-s': ('5c0a6e0d26188ab9a1dbe81fcf6a8a2b',\n-         '36ec622752444b5b96942d5c6d8845dc'),\n-  '-m': ('4063c4fbcf08a22a49949f1314abbe67',\n-         'ee4aa57b3e0ece509d8e17e5c50f06a9'),\n-  '-l': ('938d235ec40bfd60b8b1e075ded6ffd0',\n-           '3a032f50516a9eaaf1b5ebf2a2304657'),\n+  \"b0\": (\"42016207e946245a808dc807e21e2334\",\n+         \"953eb10b448fc338a0206deb1a0228c8\"),\n+  \"b1\": (\"fda7eaf4fe84f9ab1aab43ab0b7ffd26\",\n+         \"fa1764f91f33620586b628e18f265ed4\"),\n+  \"b2\": (\"c991196606d604057d3b713ed41087d9\",\n+         \"d3ff2c5f2ae01882452d0d8f1b38c3d7\"),\n+  \"b3\": (\"0e02aea454e5735908f321f3946daf0a\",\n+         \"0e8a2ed23f6c14cf1a4dddfcd2682cc9\"),\n+  \"-s\": (\"5c0a6e0d26188ab9a1dbe81fcf6a8a2b\",\n+         \"36ec622752444b5b96942d5c6d8845dc\"),\n+  \"-m\": (\"4063c4fbcf08a22a49949f1314abbe67\",\n+         \"ee4aa57b3e0ece509d8e17e5c50f06a9\"),\n+  \"-l\": (\"938d235ec40bfd60b8b1e075ded6ffd0\",\n+           \"3a032f50516a9eaaf1b5ebf2a2304657\"),\n }\n \n \n@@ -512,20 +512,20 @@ DEFAULT_BLOCKS_ARGS = {\n }\n \n CONV_KERNEL_INITIALIZER = {\n-  'class_name': 'VarianceScaling',\n-  'config': {\n-    'scale': 2.0,\n-    'mode': 'fan_out',\n-    'distribution': 'truncated_normal'\n+  \"class_name\": \"VarianceScaling\",\n+  \"config\": {\n+    \"scale\": 2.0,\n+    \"mode\": \"fan_out\",\n+    \"distribution\": \"truncated_normal\"\n   }\n }\n \n DENSE_KERNEL_INITIALIZER = {\n-  'class_name': 'VarianceScaling',\n-  'config': {\n-    'scale': 1. / 3.,\n-    'mode': 'fan_out',\n-    'distribution': 'uniform'\n+  \"class_name\": \"VarianceScaling\",\n+  \"config\": {\n+    \"scale\": 1. / 3.,\n+    \"mode\": \"fan_out\",\n+    \"distribution\": \"uniform\"\n   }\n }\n \n@@ -561,8 +561,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     include_top: Boolean, whether to include the fully-connected\n       layer at the top of the network. Defaults to True.\n     weights: One of `None` (random initialization),\n-      `'imagenet'` (pre-training on ImageNet),\n-      or the path to the weights file to be loaded. Defaults to `'imagenet'`.\n+      `\"imagenet\"` (pre-training on ImageNet),\n+      or the path to the weights file to be loaded. Defaults to `\"imagenet\"`.\n     input_tensor: Optional Keras tensor\n       (i.e. output of `layers.Input()`)\n       to use as image input for the model.\n@@ -574,11 +574,11 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n       - `None` means that the output of the model will be\n           the 4D tensor output of the\n           last convolutional layer.\n-      - `'avg'` means that global average pooling\n+      - `\"avg\"` means that global average pooling\n           will be applied to the output of the\n           last convolutional layer, and thus\n           the output of the model will be a 2D tensor.\n-      - `'max'` means that global max pooling will\n+      - `\"max\"` means that global max pooling will\n           be applied.\n     classes: Optional number of classes to classify images\n       into, only to be specified if `include_top` is True, and\n@@ -587,9 +587,9 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     classifier_activation: A string or callable. The activation function to use\n       on the `\"top\"` layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n-      Defaults to `'softmax'`.\n+      Defaults to `\"softmax\"`.\n       When loading pretrained weights, `classifier_activation` can only\n-      be `None` or `'softmax'`.\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n@@ -620,15 +620,15 @@ def Block0(\n   strides=1,\n   se_ratio=0.0,\n   bn_momentum=0.9,\n-  activation='swish',\n+  activation=\"swish\",\n   survival_probability: float = 0.8,\n   name=None,\n ):\n   \"\"\"MBConv block: Mobile Inverted Residual Bottleneck.\"\"\"\n-  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n+  bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n   if name is None:\n-    name = backend.get_uid('block0')\n+    name = backend.get_uid(\"block0\")\n \n   def apply(inputs):\n     # Expansion phase\n@@ -639,17 +639,17 @@ def Block0(\n         kernel_size=1,\n         strides=1,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        padding='same',\n-        data_format='channels_last',\n+        padding=\"same\",\n+        data_format=\"channels_last\",\n         use_bias=False,\n-        name=name + 'expand_conv',\n+        name=name + \"expand_conv\",\n       )(inputs)\n       x = layers.BatchNormalization(\n         axis=bn_axis,\n         momentum=bn_momentum,\n-        name=name + 'expand_bn',\n+        name=name + \"expand_bn\",\n       )(x)\n-      x = layers.Activation(activation, name=name + 'expand_activation')(x)\n+      x = layers.Activation(activation, name=name + \"expand_activation\")(x)\n     else:\n       x = inputs\n \n@@ -658,44 +658,44 @@ def Block0(\n       kernel_size=kernel_size,\n       strides=strides,\n       depthwise_initializer=CONV_KERNEL_INITIALIZER,\n-      padding='same',\n-      data_format='channels_last',\n+      padding=\"same\",\n+      data_format=\"channels_last\",\n       use_bias=False,\n-      name=name + 'dwconv2',\n+      name=name + \"dwconv2\",\n       )(x)\n     x = layers.BatchNormalization(\n-      axis=bn_axis, momentum=bn_momentum, name=name + 'bn'\n+      axis=bn_axis, momentum=bn_momentum, name=name + \"bn\"\n     )(x)\n-    x = layers.Activation(activation, name=name + 'activation')(x)\n+    x = layers.Activation(activation, name=name + \"activation\")(x)\n \n     # Squeeze and excite\n     if 0 < se_ratio <= 1:\n       filters_se = max(1, int(input_filters * se_ratio))\n-      se = layers.GlobalAveragePooling2D(name=name + 'se_squeeze')(x)\n+      se = layers.GlobalAveragePooling2D(name=name + \"se_squeeze\")(x)\n       if bn_axis == 1:\n         se_shape = (filters, 1, 1)\n       else:\n         se_shape = (1, 1, filters)\n-      se = layers.Reshape(se_shape, name=name + 'se_reshape')(se)\n+      se = layers.Reshape(se_shape, name=name + \"se_reshape\")(se)\n \n       se = layers.Conv2D(\n         filters_se,\n         1,\n-        padding='same',\n+        padding=\"same\",\n         activation=activation,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        name=name + 'se_reduce',\n+        name=name + \"se_reduce\",\n       )(se)\n       se = layers.Conv2D(\n         filters,\n         1,\n-        padding='same',\n-        activation='sigmoid',\n+        padding=\"same\",\n+        activation=\"sigmoid\",\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        name=name + 'se_expand',\n+        name=name + \"se_expand\",\n       )(se)\n \n-      x = layers.multiply([x, se], name=name + 'se_excite')\n+      x = layers.multiply([x, se], name=name + \"se_excite\")\n \n       # Output phase\n       x = layers.Conv2D(\n@@ -703,13 +703,13 @@ def Block0(\n         kernel_size=1,\n         strides=1,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        padding='same',\n-        data_format='channels_last',\n+        padding=\"same\",\n+        data_format=\"channels_last\",\n         use_bias=False,\n-        name=name + 'project_conv',\n+        name=name + \"project_conv\",\n       )(x)\n       x = layers.BatchNormalization(\n-        axis=bn_axis, momentum=bn_momentum, name=name + 'project_bn'\n+        axis=bn_axis, momentum=bn_momentum, name=name + \"project_bn\"\n       )(x)\n \n       if strides == 1 and input_filters == output_filters:\n@@ -717,10 +717,10 @@ def Block0(\n           x = layers.Dropout(\n             survival_probability,\n             noise_shape=(None, 1, 1, 1),\n-            name=name + 'drop'\n+            name=name + \"drop\"\n           )(x)\n \n-        x = layers.add([x, inputs], name=name + 'add')\n+        x = layers.add([x, inputs], name=name + \"add\")\n     return x\n   return apply\n \n@@ -733,15 +733,15 @@ def Block1(\n   strides=1,\n   se_ratio=0.0,\n   bn_momentum=0.9,\n-  activation='swish',\n+  activation=\"swish\",\n   survival_probability: float = 0.8,\n   name=None,\n ):\n   \"\"\"Fused MBConv Block: Fusing the proj conv1x1 and depthwise_conv into a conv2d.\"\"\"\n-  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n+  bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n   if name is None:\n-    name = backend.get_uid('block0')\n+    name = backend.get_uid(\"block0\")\n \n   def apply(inputs):\n     filters = input_filters * expand_ratio\n@@ -751,16 +751,16 @@ def Block1(\n         kernel_size=kernel_size,\n         strides=strides,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        data_format='channels_last',\n-        padding='same',\n+        data_format=\"channels_last\",\n+        padding=\"same\",\n         use_bias=False,\n-        name=name + 'expand_conv',\n+        name=name + \"expand_conv\",\n       )(inputs)\n       x = layers.BatchNormalization(\n-        axis=bn_axis, momentum=bn_momentum, name=name + 'expand_bn'\n+        axis=bn_axis, momentum=bn_momentum, name=name + \"expand_bn\"\n       )(x)\n       x = layers.Activation(\n-        activation=activation, name=name + 'expand_activation'\n+        activation=activation, name=name + \"expand_activation\"\n       )(x)\n     else:\n       x = inputs\n@@ -768,32 +768,32 @@ def Block1(\n     # Squeeze and excite\n     if 0 < se_ratio <= 1:\n       filters_se = max(1, int(input_filters * se_ratio))\n-      se = layers.GlobalAveragePooling2D(name=name + 'se_squeeze')(x)\n+      se = layers.GlobalAveragePooling2D(name=name + \"se_squeeze\")(x)\n       if bn_axis == 1:\n         se_shape = (filters, 1, 1)\n       else:\n         se_shape = (1, 1, filters)\n \n-      se = layers.Reshape(se_shape, name=name + 'se_reshape')(se)\n+      se = layers.Reshape(se_shape, name=name + \"se_reshape\")(se)\n \n       se = layers.Conv2D(\n           filters_se,\n           1,\n-          padding='same',\n+          padding=\"same\",\n           activation=activation,\n           kernel_initializer=CONV_KERNEL_INITIALIZER,\n-          name=name + 'se_reduce',\n+          name=name + \"se_reduce\",\n       )(se)\n       se = layers.Conv2D(\n           filters,\n           1,\n-          padding='same',\n-          activation='sigmoid',\n+          padding=\"same\",\n+          activation=\"sigmoid\",\n           kernel_initializer=CONV_KERNEL_INITIALIZER,\n-          name=name + 'se_expand',\n+          name=name + \"se_expand\",\n       )(se)\n \n-      x = layers.multiply([x, se], name=name + 'se_excite')\n+      x = layers.multiply([x, se], name=name + \"se_excite\")\n \n     # Output phase:\n     x = layers.Conv2D(\n@@ -801,16 +801,16 @@ def Block1(\n         kernel_size=1 if expand_ratio != 1 else kernel_size,\n         strides=1 if expand_ratio != 1 else strides,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n-        padding='same',\n+        padding=\"same\",\n         use_bias=False,\n-        name=name + 'project_conv',\n+        name=name + \"project_conv\",\n     )(x)\n     x = layers.BatchNormalization(\n-        axis=bn_axis, momentum=bn_momentum, name=name + 'project_bn'\n+        axis=bn_axis, momentum=bn_momentum, name=name + \"project_bn\"\n     )(x)\n     if expand_ratio == 1:\n       x = layers.Activation(\n-          activation=activation, name=name + 'project_activation'\n+          activation=activation, name=name + \"project_activation\"\n       )(x)\n \n     # Residual:\n@@ -819,9 +819,9 @@ def Block1(\n         x = layers.Dropout(\n               survival_probability,\n               noise_shape=(None, 1, 1, 1),\n-              name=name + 'drop'\n+              name=name + \"drop\"\n             )(x)\n-      x = layers.add([x, inputs], name=name + 'add')\n+      x = layers.add([x, inputs], name=name + \"add\")\n     return x\n   return apply\n \n@@ -835,16 +835,16 @@ def EfficientNetV2(\n   depth_divisor=8,\n   min_depth=8,\n   bn_momentum=0.9,\n-  activation='swish',\n-  blocks_args='default',\n-  model_name='efficientnetv2',\n+  activation=\"swish\",\n+  blocks_args=\"default\",\n+  model_name=\"efficientnetv2\",\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   \"\"\"Instantiates the EfficientNetV2 architecture using given scaling coefficients.\n@@ -864,7 +864,7 @@ def EfficientNetV2(\n     include_top: whether to include the fully-connected\n       layer at the top of the network.\n     weights: one of `None` (random initialization),\n-      `'imagenet'` (pre-training on ImageNet),\n+      `\"imagenet\"` (pre-training on ImageNet),\n       or the path to the weights file to be loaded.\n     input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n       or numpy array to use as image input for the model.\n@@ -876,11 +876,11 @@ def EfficientNetV2(\n       - `None` means that the output of the model will be\n           the 4D tensor output of the\n           last convolutional layer.\n-      - ''avg'' means that global average pooling\n+      - \"avg\" means that global average pooling\n           will be applied to the output of the\n           last convolutional layer, and thus\n           the output of the model will be a 2D tensor.\n-      - `'max'` means that global max pooling will\n+      - `\"max\"` means that global max pooling will\n           be applied.\n     classes: optional number of classes to classify images\n       into, only to be specified if `include_top` is True, and\n@@ -896,24 +896,24 @@ def EfficientNetV2(\n   Raises:\n     ValueError: in case of invalid argument for `weights`,\n       or invalid input shape.\n-    ValueError: if `classifier_activation` is not `'softmax'` or `None` when\n+    ValueError: if `classifier_activation` is not `\"softmax\"` or `None` when\n       using a pretrained top layer.\n   \"\"\"\n \n-  if blocks_args == 'default':\n+  if blocks_args == \"default\":\n     blocks_args = DEFAULT_BLOCKS_ARGS[model_name]\n   \n-  if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n-    raise ValueError('The `weights` argument should be either '\n-                     '`None` (random initialization), `imagenet` '\n-                     '(pre-training on ImageNet), '\n-                     'or the path to the weights file to be loaded.'\n-                     f'Received: weights={weights}')\n+  if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n+    raise ValueError(\"The `weights` argument should be either \"\n+                     \"`None` (random initialization), `imagenet` \"\n+                     \"(pre-training on ImageNet), \"\n+                     \"or the path to the weights file to be loaded.\"\n+                     f\"Received: weights={weights}\")\n \n-  if weights == 'imagenet' and include_top and classes != 1000:\n-    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n-                     ' as true, `classes` should be 1000'\n-                     f'Received: classes={classes}')\n+  if weights == \"imagenet\" and include_top and classes != 1000:\n+    raise ValueError(\"If using `weights` as `'imagenet'` with `include_top`\"\n+                     \" as true, `classes` should be 1000\"\n+                     f\"Received: classes={classes}\")\n   \n   # Determine proper input shape\n   input_shape = imagenet_utils.obtain_input_shape(\n@@ -933,7 +933,7 @@ def EfficientNetV2(\n     else:\n       img_input = input_tensor\n \n-  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n+  bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n   x = img_input\n \n@@ -942,7 +942,7 @@ def EfficientNetV2(\n \n   # Build stem\n   stem_filters = round_filters(\n-    filters=blocks_args[0]['input_filters'],\n+    filters=blocks_args[0][\"input_filters\"],\n     width_coefficient=width_coefficient,\n     min_depth=min_depth,\n     depth_divisor=depth_divisor,\n@@ -952,55 +952,55 @@ def EfficientNetV2(\n     kernel_size=3,\n     strides=2,\n     kernel_initializer=CONV_KERNEL_INITIALIZER,\n-    padding='same',\n+    padding=\"same\",\n     use_bias=False,\n-    name='stem_conv',\n+    name=\"stem_conv\",\n   )(x)\n   x = layers.BatchNormalization(\n     axis=bn_axis,\n     momentum=bn_momentum,\n-    name='stem_bn',\n+    name=\"stem_bn\",\n   )(x)\n-  x = layers.Activation(activation, name='stem_activation')(x)\n+  x = layers.Activation(activation, name=\"stem_activation\")(x)\n \n   # Build blocks\n   blocks_args = copy.deepcopy(blocks_args)\n   b = 0\n-  blocks = float(sum(args['num_repeat'] for args in blocks_args))\n+  blocks = float(sum(args[\"num_repeat\"] for args in blocks_args))\n \n   for (i, args) in enumerate(blocks_args):\n-    assert args['num_repeat'] > 0\n+    assert args[\"num_repeat\"] > 0\n \n     # Update block input and output filters based on depth multiplier.\n-    args['input_filters'] = round_filters(\n-      filters=args['input_filters'],\n+    args[\"input_filters\"] = round_filters(\n+      filters=args[\"input_filters\"],\n       width_coefficient=width_coefficient,\n       min_depth=min_depth,\n       depth_divisor=depth_divisor\n     )\n-    args['output_filters'] = round_filters(\n-      filters=args['output_filters'],\n+    args[\"output_filters\"] = round_filters(\n+      filters=args[\"output_filters\"],\n       width_coefficient=width_coefficient,\n       min_depth=min_depth,\n       depth_divisor=depth_divisor\n     )\n \n     # Determine which conv type to use:\n-    block = {0: Block0, 1: Block1}[args.pop('conv_type')]\n+    block = {0: Block0, 1: Block1}[args.pop(\"conv_type\")]\n     repeats = round_repeats(\n-      repeats=args.pop('num_repeat'), depth_coefficient=depth_coefficient\n+      repeats=args.pop(\"num_repeat\"), depth_coefficient=depth_coefficient\n     )\n     for j in range(repeats):\n       # The first block needs to take care of stride and filter size increase.\n       if j > 0:\n-        args['strides'] = 1\n-        args['input_filters'] = args['output_filters']\n+        args[\"strides\"] = 1\n+        args[\"input_filters\"] = args[\"output_filters\"]\n       \n       x = block(\n         activation=activation,\n         bn_momentum=bn_momentum,\n         survival_probability=drop_connect_rate * b / blocks,\n-        name='block{}{}_'.format(i + 1, chr(j + 97)),\n+        name=\"block{}{}_\".format(i + 1, chr(j + 97)),\n         **args,\n       )(x)\n   \n@@ -1016,35 +1016,35 @@ def EfficientNetV2(\n     kernel_size=1,\n     strides=1,\n     kernel_initializer=CONV_KERNEL_INITIALIZER,\n-    padding='same',\n-    data_format='channels_last',\n+    padding=\"same\",\n+    data_format=\"channels_last\",\n     use_bias=False,\n-    name='top_conv',\n+    name=\"top_conv\",\n   )(x)\n   x = layers.BatchNormalization(\n     axis=bn_axis,\n     momentum=bn_momentum,\n-    name='top_bn',\n+    name=\"top_bn\",\n   )(x)\n-  x = layers.Activation(activation=activation, name='top_activation')(x)\n+  x = layers.Activation(activation=activation, name=\"top_activation\")(x)\n   \n   if include_top:\n-    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n+    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n     if dropout_rate > 0:\n-      x = layers.Dropout(dropout_rate, name='top_dropout')(x)\n+      x = layers.Dropout(dropout_rate, name=\"top_dropout\")(x)\n     imagenet_utils.validate_activation(classifier_activation, weights)\n     x = layers.Dense(\n       classes,\n       activation=classifier_activation,\n       kernel_initializer=DENSE_KERNEL_INITIALIZER,\n       bias_initializer=tf.constant_initializer(0),\n-      name='predictions'\n+      name=\"predictions\"\n     )(x)\n   else:\n-    if pooling == 'avg':\n-      x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n-    elif pooling == 'max':\n-      x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n+    if pooling == \"avg\":\n+      x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+    elif pooling == \"max\":\n+      x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n \n   # Ensure that the model takes into account\n   # any potential predecessors of `input_tensor`.\n@@ -1057,18 +1057,18 @@ def EfficientNetV2(\n   model = training.Model(inputs, x, name=model_name)\n  \n   # Load weights.\n-  if weights == 'imagenet':\n+  if weights == \"imagenet\":\n     if include_top:\n-      file_suffix = '.h5'\n+      file_suffix = \".h5\"\n       file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n     else:\n-      file_suffix = '_notop.h5'\n+      file_suffix = \"_notop.h5\"\n       file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n     file_name = model_name + file_suffix\n     weights_path = data_utils.get_file(\n       file_name,\n       BASE_WEIGHTS_PATH + file_name,\n-      cache_subdir='models',\n+      cache_subdir=\"models\",\n       file_hash=file_hash)\n     model.load_weights(weights_path)\n   elif weights is not None:\n@@ -1077,23 +1077,23 @@ def EfficientNetV2(\n   return model\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2B0',\n-              'keras.applications.EfficientNetV2B0')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2B0\",\n+              \"keras.applications.EfficientNetV2B0\")\n def EfficientNetV2B0(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.0,\n     depth_coefficient=1.0,\n     default_size=224,\n-    model_name='efficientnetv2-b0',\n+    model_name=\"efficientnetv2-b0\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1105,23 +1105,23 @@ def EfficientNetV2B0(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2B1',\n-              'keras.applications.EfficientNetV2B1')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2B1\",\n+              \"keras.applications.EfficientNetV2B1\")\n def EfficientNetV2B1(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.0,\n     depth_coefficient=1.1,\n     default_size=240,\n-    model_name='efficientnetv2-b1',\n+    model_name=\"efficientnetv2-b1\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1133,23 +1133,23 @@ def EfficientNetV2B1(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2B2',\n-              'keras.applications.EfficientNetV2B2')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2B2\",\n+              \"keras.applications.EfficientNetV2B2\")\n def EfficientNetV2B2(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(        \n     width_coefficient=1.1,\n     depth_coefficient=1.2,\n     default_size=260,\n-    model_name='efficientnetv2-b2',\n+    model_name=\"efficientnetv2-b2\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1161,23 +1161,23 @@ def EfficientNetV2B2(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2B3',\n-              'keras.applications.EfficientNetV2B3')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2B3\",\n+              \"keras.applications.EfficientNetV2B3\")\n def EfficientNetV2B3(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.2,\n     depth_coefficient=1.4,\n     default_size=300,\n-    model_name='efficientnetv2-b3',\n+    model_name=\"efficientnetv2-b3\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1189,23 +1189,23 @@ def EfficientNetV2B3(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2S',\n-              'keras.applications.EfficientNetV2S')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2S\",\n+              \"keras.applications.EfficientNetV2S\")\n def EfficientNetV2S(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.0,\n     depth_coefficient=1.0,\n     default_size=384,\n-    model_name='efficientnetv2-s',\n+    model_name=\"efficientnetv2-s\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1217,23 +1217,23 @@ def EfficientNetV2S(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2M',\n-              'keras.applications.EfficientNetV2M')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2M\",\n+              \"keras.applications.EfficientNetV2M\")\n def EfficientNetV2M(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.0,\n     depth_coefficient=1.0,\n     default_size=480,\n-    model_name='efficientnetv2-m',\n+    model_name=\"efficientnetv2-m\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1245,23 +1245,23 @@ def EfficientNetV2M(\n   )\n \n \n-@keras_export('keras.applications.efficientnet_v2.EfficientNetV2L',\n-              'keras.applications.EfficientNetV2L')\n+@keras_export(\"keras.applications.efficientnet_v2.EfficientNetV2L\",\n+              \"keras.applications.EfficientNetV2L\")\n def EfficientNetV2L(\n   include_top=True,\n-  weights='imagenet',\n+  weights=\"imagenet\",\n   input_tensor=None,\n   input_shape=None,\n   pooling=None,\n   classes=1000,\n-  classifier_activation='softmax',\n+  classifier_activation=\"softmax\",\n   include_preprocessing=True,\n ):\n   return EfficientNetV2(\n     width_coefficient=1.0,\n     depth_coefficient=1.0,\n     default_size=480,\n-    model_name='efficientnetv2-l',\n+    model_name=\"efficientnetv2-l\",\n     include_top=include_top,\n     weights=weights,\n     input_tensor=input_tensor,\n@@ -1273,16 +1273,16 @@ def EfficientNetV2L(\n   )\n \n \n-EfficientNetV2B0.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2B0')\n-EfficientNetV2B1.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2B1')\n-EfficientNetV2B2.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2B2')\n-EfficientNetV2B3.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2B3')\n-EfficientNetV2S.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2S')\n-EfficientNetV2M.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2M')\n-EfficientNetV2L.__doc__ = BASE_DOCSTRING.format(name='EfficientNetV2L')\n+EfficientNetV2B0.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B0\")\n+EfficientNetV2B1.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B1\")\n+EfficientNetV2B2.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B2\")\n+EfficientNetV2B3.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B3\")\n+EfficientNetV2S.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2S\")\n+EfficientNetV2M.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2M\")\n+EfficientNetV2L.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2L\")\n \n \n-@keras_export('keras.applications.efficientnet_v2.preprocess_input')\n+@keras_export(\"keras.applications.efficientnet_v2.preprocess_input\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n@@ -1296,7 +1296,7 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     data_format: Optional data format of the image tensor/array. Defaults to\n       None, in which case the global setting\n       `tf.keras.backend.image_data_format()` is used (unless you changed it,\n-      it defaults to 'channels_last').{mode}\n+      it defaults to \"channels_last\").{mode}\n \n   Returns:\n     Unchanged `numpy.array` or `tf.Tensor`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
