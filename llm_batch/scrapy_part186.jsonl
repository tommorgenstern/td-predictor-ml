{"custom_id": "scrapy#543d02bc64a76d7e508b05bff31cb3a6515a2461", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 433 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -194,21 +194,21 @@ def _upload_egg(target, eggpath, project, version):\n     _log('Deploying to project \"%s\" in %s' % (project, url))\n     return _http_post(req)\n \n-def _add_auth_header(request, target):\n+def _add_auth_header(req, target):\n     if 'username' in target:\n         u, p = target.get('username'), target.get('password', '')\n-        request.add_header('Authorization', basic_auth_header(u, p))\n+        req.add_header('Authorization', basic_auth_header(u, p))\n     else: # try netrc\n         try:\n             host = urlparse(target['url']).hostname\n             a = netrc.netrc().authenticators(host)\n-            request.add_header('Authorization', basic_auth_header(a[0], a[2]))\n+            req.add_header('Authorization', basic_auth_header(a[0], a[2]))\n         except (netrc.NetrcParseError, IOError, TypeError):\n             pass\n \n-def _http_post(request):\n+def _http_post(req):\n     try:\n-        f = request.urlopen(request)\n+        f = request.urlopen(req)\n         _log(\"Server response (%s):\" % f.code)\n         print(f.read())\n         return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b4439eacc5612ce584de94053e725610f2e15a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 335 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -182,9 +182,9 @@ class SelectorList(list):\n     def extract(self):\n         return [x.extract() for x in self]\n \n-    def extract_first(self):\n+    def extract_first(self, default=None):\n         for x in self:\n-            return x.extract()\n+            return x.extract() or default\n \n     @deprecated(use_instead='.extract()')\n     def extract_unquoted(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc705843444472dbcb8eb583b8d87e055a1b9798", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 1109 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,9 +82,12 @@ class CrawlerRunner(object):\n         self.crawlers = set()\n         self._active = set()\n \n-    def crawl(self, spidercls, *args, **kwargs):\n-        crawler = self._create_crawler(spidercls)\n+    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n+        crawler = crawler_or_spidercls\n+        if not isinstance(crawler_or_spidercls, Crawler):\n+            crawler = self._create_crawler(crawler_or_spidercls)\n             self._setup_crawler_logging(crawler)\n+\n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n         self._active.add(d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24a07fd892c1bc70b284f4e41780681888516336", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 20 | Churn Cumulative: 1209 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,9 +3,11 @@ import signal\n import warnings\n \n from twisted.internet import reactor, defer\n+from zope.interface.verify import verifyClass\n \n from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n+from scrapy.interfaces import ISpiderManager\n from scrapy.extension import ExtensionManager\n from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -78,6 +80,7 @@ class CrawlerRunner(object):\n     def __init__(self, settings):\n         self.settings = settings\n         smcls = load_object(settings['SPIDER_MANAGER_CLASS'])\n+        verifyClass(ISpiderManager, smcls)\n         self.spiders = smcls.from_settings(settings.frozencopy())\n         self.crawlers = set()\n         self._active = set()\n\n@@ -2,6 +2,7 @@ import warnings\n import unittest\n \n from twisted.internet import defer\n+from zope.interface.verify import DoesNotImplement\n \n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.settings import Settings\n@@ -42,3 +43,19 @@ class CrawlerTestCase(unittest.TestCase):\n \n         self.assertFalse(settings.frozen)\n         self.assertTrue(crawler.settings.frozen)\n+\n+\n+def SpiderManagerWithWrongInterface(object):\n+\n+    def unneeded_method(self):\n+        pass\n+\n+\n+class CrawlerRunnerTestCase(unittest.TestCase):\n+\n+    def test_spider_manager_verify_interface(self):\n+        settings = Settings({\n+            'SPIDER_MANAGER_CLASS': 'tests.test_crawler.SpiderManagerWithWrongInterface'\n+        })\n+        with self.assertRaises(DoesNotImplement):\n+            CrawlerRunner(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf301b73284153f8f6de0be5eb47dbaa197a9d3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 393 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,6 +3,7 @@ XPath selectors based on lxml\n \"\"\"\n \n from lxml import etree\n+import six\n \n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n@@ -95,7 +96,8 @@ class Selector(object_ref):\n             result = xpathev(query, namespaces=self.namespaces,\n                              smart_strings=self._lxml_smart_strings)\n         except etree.XPathError:\n-            raise ValueError(\"Invalid XPath: %s\" % query)\n+            msg = u\"Invalid XPath: %s\" % query\n+            raise ValueError(msg if six.PY3 else msg.encode(\"unicode_escape\"))\n \n         if type(result) is not list:\n             result = [result]\n\n@@ -1,6 +1,7 @@\n import re\n import warnings\n import weakref\n+import six\n from twisted.trial import unittest\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n@@ -242,17 +243,19 @@ class SelectorTestCase(unittest.TestCase):\n         self.assertEqual(xs.xpath('.').extract(), [u'<root>lala</root>'])\n \n     def test_invalid_xpath(self):\n+        \"Test invalid xpath raises ValueError with the invalid xpath\"\n         response = XmlResponse(url=\"http://example.com\", body=\"<html></html>\")\n         x = self.sscls(response)\n         xpath = \"//test[@foo='bar]\"\n-        try:\n-            x.xpath(xpath)\n-        except ValueError as e:\n-            assert xpath in str(e), \"Exception message does not contain invalid xpath\"\n-        except Exception:\n-            raise AssertionError(\"A invalid XPath does not raise ValueError\")\n-        else:\n-            raise AssertionError(\"A invalid XPath does not raise an exception\")\n+        self.assertRaisesRegexp(ValueError, re.escape(xpath), x.xpath, xpath)\n+\n+    def test_invalid_xpath_unicode(self):\n+        \"Test *Unicode* invalid xpath raises ValueError with the invalid xpath\"\n+        response = XmlResponse(url=\"http://example.com\", body=\"<html></html>\")\n+        x = self.sscls(response)\n+        xpath = u\"//test[@foo='\\u0431ar]\"\n+        encoded = xpath if six.PY3 else xpath.encode('unicode_escape')\n+        self.assertRaisesRegexp(ValueError, re.escape(encoded), x.xpath, xpath)\n \n     def test_http_header_encoding_precedence(self):\n         # u'\\xa3'     = pound symbol in unicode\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1eccd34a7032c3e50c56f61da34ef39d1aaf43ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 672 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ def _urlencode(seq, enc):\n     return urlencode(values, doseq=1)\n \n \n-def _get_form(response, formname, formnumber, formxpath):\n+def _get_form(response, formname, formid, formnumber, formxpath):\n     \"\"\"Find the form element \"\"\"\n     from scrapy.selector.lxmldocument import LxmlDocument\n     root = LxmlDocument(response, lxml.html.HTMLParser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf9d848f0f8169f82a455977c2df6390a50a1e59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n-EXTENSIONS = [\n-    'tests.test_cmdline.extensions.TestExtension'\n-]\n+EXTENSIONS = {\n+    'tests.test_cmdline.extensions.TestExtension': 0,\n+}\n \n TEST1 = 'default'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#973c31f73595c73fac9b825ce7587d93d7055546", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 129 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,10 @@\n+from __future__ import absolute_import\n import sys\n import os\n \n from twisted.internet import reactor, defer, protocol\n \n+\n class ProcessTest(object):\n \n     command = None\n\n@@ -95,11 +95,11 @@ class GenspiderCommandTest(CommandTest):\n         spname = 'test_spider'\n         p = self.proc('genspider', spname, 'test.com', *args)\n         out = retry_on_eintr(p.stdout.read)\n-        self.assert_(\"Created spider %r using template %r in module\" % (spname, tplname) in out)\n-        self.assert_(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n+        self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n+        self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n         p = self.proc('genspider', spname, 'test.com', *args)\n         out = retry_on_eintr(p.stdout.read)\n-        self.assert_(\"Spider %r already exists in module\" % spname in out)\n+        self.assertIn(\"Spider %r already exists in module\" % spname, out)\n \n     def test_template_basic(self):\n         self.test_template('basic')\n@@ -148,10 +148,10 @@ class MySpider(scrapy.Spider):\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"[myspider] DEBUG: It Works!\" in log, log)\n-        self.assert_(\"[myspider] INFO: Spider opened\" in log, log)\n-        self.assert_(\"[myspider] INFO: Closing spider (finished)\" in log, log)\n-        self.assert_(\"[myspider] INFO: Spider closed (finished)\" in log, log)\n+        self.assertIn(\"[myspider] DEBUG: It Works!\", log)\n+        self.assertIn(\"[myspider] INFO: Spider opened\", log)\n+        self.assertIn(\"[myspider] INFO: Closing spider (finished)\", log)\n+        self.assertIn(\"[myspider] INFO: Spider closed (finished)\", log)\n \n     def test_runspider_no_spider_found(self):\n         tmpdir = self.mktemp()\n@@ -164,12 +164,12 @@ from scrapy.spider import Spider\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"No spider found in file\" in log)\n+        self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n         p = self.proc('runspider', 'some_non_existent_file')\n         log = p.stderr.read()\n-        self.assert_(\"File not found: some_non_existent_file\" in log)\n+        self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n         tmpdir = self.mktemp()\n@@ -179,7 +179,7 @@ from scrapy.spider import Spider\n             f.write(\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assert_(\"Unable to load\" in log)\n+        self.assertIn(\"Unable to load\", log)\n \n \n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n@@ -229,7 +229,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '-a', 'test_arg=1',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assert_(\"[parse_spider] DEBUG: It Works!\" in stderr, stderr)\n+        self.assertIn(\"[parse_spider] DEBUG: It Works!\", stderr)\n \n     @defer.inlineCallbacks\n     def test_pipelines(self):\n@@ -237,7 +237,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '--pipelines',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assert_(\"[scrapy] INFO: It Works!\" in stderr, stderr)\n+        self.assertIn(\"[scrapy] INFO: It Works!\", stderr)\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n@@ -254,4 +254,4 @@ class BenchCommandTest(CommandTest):\n         p = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n                 '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         log = p.stderr.read()\n-        self.assert_('INFO: Crawled' in log, log)\n+        self.assertIn('INFO: Crawled', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5436ba117b687eb3bd4e7afa69d5a67935056342", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 380 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -151,14 +151,6 @@ class FeedExporter(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        if len(get_func_args(cls)) < 1:\n-            # FIXME: remove for scrapy 0.17\n-            import warnings\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            warnings.warn(\"%s must receive a settings object as first constructor argument.\" % cls.__name__,\n-                ScrapyDeprecationWarning, stacklevel=2)\n-            o = cls()\n-        else:\n         o = cls(crawler.settings)\n         crawler.signals.connect(o.open_spider, signals.spider_opened)\n         crawler.signals.connect(o.close_spider, signals.spider_closed)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7644813286c230866c2f7287bbaead87f2be104b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 29 | Churn Cumulative: 1238 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n from scrapy.interfaces import ISpiderManager\n from scrapy.extension import ExtensionManager\n+from scrapy.settings import Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n@@ -19,6 +20,9 @@ from scrapy import log, signals\n class Crawler(object):\n \n     def __init__(self, spidercls, settings):\n+        if isinstance(settings, dict):\n+            settings = Settings(settings)\n+\n         self.spidercls = spidercls\n         self.settings = settings.copy()\n \n@@ -78,6 +82,8 @@ class Crawler(object):\n class CrawlerRunner(object):\n \n     def __init__(self, settings):\n+        if isinstance(settings, dict):\n+            settings = Settings(settings)\n         self.settings = settings\n         smcls = load_object(settings['SPIDER_MANAGER_CLASS'])\n         verifyClass(ISpiderManager, smcls)\n\n@@ -1,11 +1,10 @@\n import warnings\n import unittest\n \n-from twisted.internet import defer\n from zope.interface.verify import DoesNotImplement\n \n from scrapy.crawler import Crawler, CrawlerRunner\n-from scrapy.settings import Settings\n+from scrapy.settings import Settings, default_settings\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n \n@@ -44,6 +43,16 @@ class CrawlerTestCase(unittest.TestCase):\n         self.assertFalse(settings.frozen)\n         self.assertTrue(crawler.settings.frozen)\n \n+    def test_crawler_accepts_dict(self):\n+        crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n+        self.assertEqual(crawler.settings['foo'], 'bar')\n+        self.assertEqual(\n+            crawler.settings['RETRY_ENABLED'],\n+            default_settings.RETRY_ENABLED\n+        )\n+        self.assertIsInstance(crawler.settings, Settings)\n+\n+\n \n def SpiderManagerWithWrongInterface(object):\n \n@@ -59,3 +68,13 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n         })\n         with self.assertRaises(DoesNotImplement):\n             CrawlerRunner(settings)\n+\n+    def test_crawler_runner_accepts_dict(self):\n+        runner = CrawlerRunner({'foo': 'bar'})\n+        self.assertEqual(runner.settings['foo'], 'bar')\n+        self.assertEqual(\n+            runner.settings['RETRY_ENABLED'],\n+            default_settings.RETRY_ENABLED\n+        )\n+        self.assertIsInstance(runner.settings, Settings)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1534e8540bf083c8d7beb0264cccea5488ee0250", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 141 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 16 | Methods Changed: 14 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 150 | Churn Cumulative: 841 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,6 +146,7 @@ class FeedExporter(object):\n         if not self._exporter_supported(self.format):\n             raise NotConfigured\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n+        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n@@ -169,7 +170,7 @@ class FeedExporter(object):\n         uri = self.urifmt % self._get_uri_params(spider)\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n-        exporter = self._get_exporter(file)\n+        exporter = self._get_exporter(file, fields_to_export=self.export_fields)\n         exporter.start_exporting()\n         self.slot = SpiderSlot(file, exporter, storage, uri)\n \n@@ -218,8 +219,8 @@ class FeedExporter(object):\n         else:\n             log.msg(\"Unknown feed storage scheme: %s\" % scheme, log.ERROR)\n \n-    def _get_exporter(self, *a, **kw):\n-        return self.exporters[self.format](*a, **kw)\n+    def _get_exporter(self, *args, **kwargs):\n+        return self.exporters[self.format](*args, **kwargs)\n \n     def _get_storage(self, uri):\n         return self.storages[urlparse(uri).scheme](uri)\n\n@@ -126,6 +126,7 @@ FEED_URI = None\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n+FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n FEED_STORAGES_BASE = {\n     '': 'scrapy.contrib.feedexport.FileFeedStorage',\n\n@@ -191,6 +191,7 @@ class MockServer():\n         self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver'],\n                           stdout=PIPE, env=get_testenv())\n         self.proc.stdout.readline()\n+        return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n         self.proc.kill()\n\n@@ -1,16 +1,27 @@\n+from __future__ import absolute_import\n import os\n+import csv\n from io import BytesIO\n+import tempfile\n+import shutil\n from six.moves.urllib.parse import urlparse\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n from twisted.internet import defer\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.settings import Settings\n+from tests.mockserver import MockServer\n from w3lib.url import path_to_file_uri\n \n-from scrapy.spider import Spider\n-from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage\n+import scrapy\n+from scrapy.contrib.feedexport import (\n+    IFeedStorage, FileFeedStorage, FTPFeedStorage,\n+    S3FeedStorage, StdoutFeedStorage\n+)\n from scrapy.utils.test import assert_aws_environ\n \n+\n class FileFeedStorageTest(unittest.TestCase):\n \n     def test_store_file_uri(self):\n@@ -39,7 +50,7 @@ class FileFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n-        spider = Spider(\"default\")\n+        spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n@@ -61,7 +72,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n-        spider = Spider(\"default\")\n+        spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n@@ -85,7 +96,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         from boto import connect_s3\n         storage = S3FeedStorage(uri)\n         verifyObject(IFeedStorage, storage)\n-        file = storage.open(Spider(\"default\"))\n+        file = storage.open(scrapy.Spider(\"default\"))\n         file.write(\"content\")\n         yield storage.store(file)\n         u = urlparse(uri)\n@@ -99,7 +110,125 @@ class StdoutFeedStorageTest(unittest.TestCase):\n     def test_store(self):\n         out = BytesIO()\n         storage = StdoutFeedStorage('stdout:', _stdout=out)\n-        file = storage.open(Spider(\"default\"))\n+        file = storage.open(scrapy.Spider(\"default\"))\n         file.write(b\"content\")\n         yield storage.store(file)\n         self.assertEqual(out.getvalue(), b\"content\")\n+\n+\n+class FeedExportTest(unittest.TestCase):\n+\n+    class MyItem(scrapy.Item):\n+        foo = scrapy.Field()\n+        egg = scrapy.Field()\n+        baz = scrapy.Field()\n+\n+\n+    @defer.inlineCallbacks\n+    def run_and_export(self, spider_cls, settings=None):\n+        \"\"\" Run spider with specified settings; return exported data. \"\"\"\n+        tmpdir = tempfile.mkdtemp()\n+        res_name = tmpdir + '/res'\n+        defaults = {\n+            'FEED_URI': 'file://' + res_name,\n+            'FEED_FORMAT': 'csv',\n+        }\n+        defaults.update(settings or {})\n+        try:\n+            with MockServer() as s:\n+                runner = CrawlerRunner(Settings(defaults))\n+                yield runner.crawl(spider_cls)\n+\n+            with open(res_name, 'rb') as f:\n+                defer.returnValue(f.read())\n+\n+        finally:\n+            shutil.rmtree(tmpdir)\n+\n+    @defer.inlineCallbacks\n+    def exported_data(self, items, settings):\n+        \"\"\"\n+        Return exported data which a spider yielding ``items`` would return.\n+        \"\"\"\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+            start_urls = ['http://localhost:8998/']\n+\n+            def parse(self, response):\n+                for item in items:\n+                    yield item\n+\n+        data = yield self.run_and_export(TestSpider, settings)\n+        defer.returnValue(data)\n+\n+    @defer.inlineCallbacks\n+    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+        settings = settings or {}\n+        settings.update({'FEED_FORMAT': 'csv'})\n+        data = yield self.exported_data(items, settings)\n+\n+        reader = csv.DictReader(data.splitlines())\n+        got_rows = list(reader)\n+        if ordered:\n+            self.assertEqual(reader.fieldnames, header)\n+        else:\n+            self.assertEqual(set(reader.fieldnames), set(header))\n+\n+        self.assertEqual(rows, got_rows)\n+\n+    @defer.inlineCallbacks\n+    def test_export_csv_items(self):\n+        # feed exporters use field names from Item\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+        ]\n+        rows = [\n+            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n+            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n+        ]\n+        header = self.MyItem.fields.keys()\n+        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+\n+    @defer.inlineCallbacks\n+    def test_export_csv_dicts(self):\n+        # When dicts are used, only keys from the first row are used as\n+        # a header.\n+        items = [\n+            {'foo': 'bar', 'egg': 'spam'},\n+            {'foo': 'bar', 'egg': 'spam', 'baz': 'quux'},\n+        ]\n+        rows = [\n+            {'egg': 'spam', 'foo': 'bar'},\n+            {'egg': 'spam', 'foo': 'bar'}\n+        ]\n+        yield self.assertExportedCsv(items, ['egg', 'foo'], rows, ordered=False)\n+\n+    @defer.inlineCallbacks\n+    def test_export_csv_feed_export_fields(self):\n+        # FEED_EXPORT_FIELDS option allows to order export fields\n+        # and to select a subset of fields to export, both for Items and dicts.\n+\n+        for item_cls in [self.MyItem, dict]:\n+            items = [\n+                item_cls({'foo': 'bar1', 'egg': 'spam1'}),\n+                item_cls({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+            ]\n+\n+            # export all columns\n+            settings = {'FEED_EXPORT_FIELDS': 'foo,baz,egg'}\n+            rows = [\n+                {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n+                {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n+            ]\n+            yield self.assertExportedCsv(items, ['foo', 'baz', 'egg'], rows,\n+                                         settings=settings, ordered=True)\n+\n+            # export a subset of columns\n+            settings = {'FEED_EXPORT_FIELDS': 'egg,baz'}\n+            rows = [\n+                {'egg': 'spam1', 'baz': ''},\n+                {'egg': 'spam2', 'baz': 'quux2'}\n+            ]\n+            yield self.assertExportedCsv(items, ['egg', 'baz'], rows,\n+                                         settings=settings, ordered=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#647eeaea61772d80667b7715dc84cd5aff86dbfd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 36 | Churn Cumulative: 201 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -190,6 +190,42 @@ class FeedExportTest(unittest.TestCase):\n         header = self.MyItem.fields.keys()\n         yield self.assertExportedCsv(items, header, rows, ordered=False)\n \n+    @defer.inlineCallbacks\n+    def test_export_csv_multiple_item_classes(self):\n+\n+        class MyItem2(scrapy.Item):\n+            foo = scrapy.Field()\n+            hello = scrapy.Field()\n+\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n+            self.MyItem({'foo': 'bar3', 'egg': 'spam3', 'baz': 'quux3'}),\n+            {'hello': 'world4', 'egg': 'spam4'},\n+        ]\n+\n+        # by default, Scrapy uses fields of the first Item\n+        header = self.MyItem.fields.keys()\n+        rows = [\n+            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n+            {'egg': '',      'foo': 'bar2', 'baz': ''},\n+            {'egg': 'spam3', 'foo': 'bar3', 'baz': 'quux3'},\n+            {'egg': 'spam4', 'foo': '',     'baz': ''},\n+        ]\n+        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+\n+        # but it is possible to override fields using FEED_EXPORT_FIELDS\n+        header = [\"foo\", \"baz\", \"hello\"]\n+        settings = {'FEED_EXPORT_FIELDS': header}\n+        rows = [\n+            {'foo': 'bar1', 'baz': '',      'hello': ''},\n+            {'foo': 'bar2', 'baz': '',      'hello': 'world2'},\n+            {'foo': 'bar3', 'baz': 'quux3', 'hello': ''},\n+            {'foo': '',     'baz': '',      'hello': 'world4'},\n+        ]\n+        yield self.assertExportedCsv(items, header, rows,\n+                                     settings=settings, ordered=True)\n+\n     @defer.inlineCallbacks\n     def test_export_csv_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abeb8e3a0118a3defd7cefd29c9ead930ccc5e28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 204 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ import csv\n from io import BytesIO\n import tempfile\n import shutil\n+import six\n from six.moves.urllib.parse import urlparse\n \n from zope.interface.verify import verifyObject\n@@ -118,6 +119,8 @@ class StdoutFeedStorageTest(unittest.TestCase):\n \n class FeedExportTest(unittest.TestCase):\n \n+    skip = not six.PY2\n+\n     class MyItem(scrapy.Item):\n         foo = scrapy.Field()\n         egg = scrapy.Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dcd7f3ca7ce8da2f3cc45ddc780274edb121469", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 74 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2014, Scrapy developers'\n+copyright = u'2008-2015, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -95,10 +95,23 @@ pygments_style = 'sphinx'\n # Options for HTML output\n # -----------------------\n \n+# The theme to use for HTML and HTML Help pages.  See the documentation for\n+# a list of builtin themes.\n+html_theme = 'sphinx_rtd_theme'\n+\n+# Theme options are theme-specific and customize the look and feel of a theme\n+# further.  For a list of options available for each theme, see the\n+# documentation.\n+#html_theme_options = {}\n+\n+# Add any paths that contain custom themes here, relative to this directory.\n+#html_theme_path = []\n+\n+\n # The style sheet to use for HTML and HTML Help pages. A file of that name\n # must exist either in Sphinx' static/ path, or in one of the custom paths\n # given in html_static_path.\n-html_style = 'scrapydoc.css'\n+# html_style = 'scrapydoc.css'\n \n # The name for this set of Sphinx documents.  If None, it defaults to\n # \"<project> v<release> documentation\".\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#378b6efc0e0f48f9f4eac349c5e9d2be6b3cbe0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 157 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,9 +6,13 @@ See documentation in docs/topics/telnetconsole.rst\n \n import pprint\n \n+from twisted.internet import protocol\n+try:\n     from twisted.conch import manhole, telnet\n     from twisted.conch.insults import insults\n-from twisted.internet import protocol\n+    TWISTED_CONCH_AVAILABLE = True\n+except ImportError:\n+    TWISTED_CONCH_AVAILABLE = False\n \n from scrapy.exceptions import NotConfigured\n from scrapy import log, signals\n@@ -32,6 +36,8 @@ class TelnetConsole(protocol.ServerFactory):\n     def __init__(self, crawler):\n         if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n+        if not TWISTED_CONCH_AVAILABLE:\n+            raise NotConfigured\n         self.crawler = crawler\n         self.noisy = False\n         self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b764876291cbba8581ce283c97f423b796f899a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,13 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import re\n+import six\n+from w3lib import html\n from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import _noscript_re, _script_re\n-from w3lib import html\n+\n \n class AjaxCrawlMiddleware(object):\n     \"\"\"\n@@ -61,7 +63,7 @@ class AjaxCrawlMiddleware(object):\n \n \n # XXX: move it to w3lib?\n-_ajax_crawlable_re = re.compile(ur'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>')\n+_ajax_crawlable_re = re.compile(six.u(r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'))\n def _has_ajaxcrawlable_meta(text):\n     \"\"\"\n     >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#020a32a3d9639bf948b8f84c8904764f99e0092b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 63/63 | Churn Δ: 54 | Churn Cumulative: 54 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -532,6 +532,60 @@ class FormRequestTest(RequestTest):\n         self.assertRaises(IndexError, self.request_class.from_response, \\\n                           response, formname=\"form3\", formnumber=2)\n \n+    def test_from_response_formid_exists(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\n+            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"three\" value=\"3\">\n+            <input type=\"hidden\" name=\"four\" value=\"4\">\n+            </form>\"\"\")\n+        r1 = self.request_class.from_response(response, formid=\"form2\")\n+        self.assertEqual(r1.method, 'POST')\n+        fs = _qs(r1)\n+        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n+\n+    def test_from_response_formid_notexists_fallback_formname(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\n+            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"three\" value=\"3\">\n+            <input type=\"hidden\" name=\"four\" value=\"4\">\n+            </form>\"\"\")\n+        r1 = self.request_class.from_response(response, formid=\"form3\", formname=\"form2\")\n+        self.assertEqual(r1.method, 'POST')\n+        fs = _qs(r1)\n+        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n+\n+    def test_from_response_formid_notexist(self):\n+        response = _buildresponse(\n+            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            </form>\n+            <form id=\"form2\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\"\"\")\n+        r1 = self.request_class.from_response(response, formid=\"form3\")\n+        self.assertEqual(r1.method, 'POST')\n+        fs = _qs(r1)\n+        self.assertEqual(fs, {'one': ['1']})\n+\n+    def test_from_response_formid_errors_formnumber(self):\n+        response = _buildresponse(\n+            \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            </form>\n+            <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\"\"\")\n+        self.assertRaises(IndexError, self.request_class.from_response, \\\n+                          response, formid=\"form3\", formnumber=2)\n+\n     def test_from_response_select(self):\n         res = _buildresponse(\n             '''<form>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#954c8fcecbd0c5c43e371f58585a2ce18f0a6212", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 58 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -547,7 +547,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n \n-    def test_from_response_formid_notexists_fallback_formname(self):\n+    def test_from_response_formname_notexists_fallback_formid(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n@@ -557,7 +557,7 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"three\" value=\"3\">\n             <input type=\"hidden\" name=\"four\" value=\"4\">\n             </form>\"\"\")\n-        r1 = self.request_class.from_response(response, formid=\"form3\", formname=\"form2\")\n+        r1 = self.request_class.from_response(response, formname=\"form3\", formid=\"form2\")\n         self.assertEqual(r1.method, 'POST')\n         fs = _qs(r1)\n         self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1f3b3c7f05c505fa9f0b907681fc97f7548cfae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1552 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"\n Files Pipeline\n+\n+See documentation in topics/media-pipeline.rst\n \"\"\"\n \n import hashlib\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Images Pipeline\n \n-See documentation in topics/images.rst\n+See documentation in topics/media-pipeline.rst\n \"\"\"\n \n import hashlib\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#403e7c7c7059486c5a4c59f900a65378de2ba3d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 85 | Files Changed: 14 | Hunks: 41 | Methods Changed: 31 | Complexity Δ (Sum/Max): 11/12 | Churn Δ: 207 | Churn Cumulative: 2162 | Contributors (this commit): 21 | Commits (past 90d): 29 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,7 @@ from zope.interface.verify import verifyClass\n \n from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n-from scrapy.interfaces import ISpiderManager\n+from scrapy.interfaces import ISpiderLoader\n from scrapy.extension import ExtensionManager\n from scrapy.settings import Settings\n from scrapy.signalmanager import SignalManager\n@@ -44,11 +44,10 @@ class Crawler(object):\n         if not hasattr(self, '_spiders'):\n             warnings.warn(\"Crawler.spiders is deprecated, use \"\n                           \"CrawlerRunner.spiders or instantiate \"\n-                          \"scrapy.spidermanager.SpiderManager with your \"\n+                          \"scrapy.spiderloader.SpiderLoader with your \"\n                           \"settings.\",\n                           category=ScrapyDeprecationWarning, stacklevel=2)\n-            spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-            self._spiders = spman_cls.from_settings(self.settings)\n+            self._spiders = _get_spider_loader(self.settings.frozencopy())\n         return self._spiders\n \n     @defer.inlineCallbacks\n@@ -85,9 +84,7 @@ class CrawlerRunner(object):\n         if isinstance(settings, dict):\n             settings = Settings(settings)\n         self.settings = settings\n-        smcls = load_object(settings['SPIDER_MANAGER_CLASS'])\n-        verifyClass(ISpiderManager, smcls)\n-        self.spiders = smcls.from_settings(settings.frozencopy())\n+        self.spiders = _get_spider_loader(settings)\n         self.crawlers = set()\n         self._active = set()\n \n@@ -178,3 +175,18 @@ class CrawlerProcess(CrawlerRunner):\n             reactor.stop()\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n             pass\n+\n+\n+def _get_spider_loader(settings):\n+    \"\"\" Get SpiderLoader instance from settings \"\"\"\n+    if settings.get('SPIDER_MANAGER_CLASS'):\n+        warnings.warn(\n+            'SPIDER_MANAGER_CLASS option is deprecated. '\n+            'Please use SPIDER_LOADER_CLASS.',\n+            category=ScrapyDeprecationWarning, stacklevel=2\n+        )\n+    cls_path = settings.get('SPIDER_LOADER_CLASS',\n+                            settings.get('SPIDER_MANAGER_CLASS'))\n+    loader_cls = load_object(cls_path)\n+    verifyClass(ISpiderLoader, loader_cls)\n+    return loader_cls.from_settings(settings.frozencopy())\n\n@@ -1,12 +1,12 @@\n from zope.interface import Interface\n \n-class ISpiderManager(Interface):\n+class ISpiderLoader(Interface):\n \n     def from_settings(settings):\n-        \"\"\"Returns an instance of the class for the given settings\"\"\"\n+        \"\"\"Return an instance of the class for the given settings\"\"\"\n \n     def load(spider_name):\n-        \"\"\"Returns the Spider class for the given spider name. If the spider\n+        \"\"\"Return the Spider class for the given spider name. If the spider\n         name is not found, it must raise a KeyError.\"\"\"\n \n     def list():\n@@ -14,4 +14,9 @@ class ISpiderManager(Interface):\n         project\"\"\"\n \n     def find_by_request(request):\n-        \"\"\"Returns the list of spiders names that can handle the given request\"\"\"\n+        \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n+\n+\n+# ISpiderManager is deprecated, don't use it!\n+# An alias is kept for backwards compatibility.\n+ISpiderManager = ISpiderLoader\n\n@@ -215,7 +215,7 @@ SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'\n SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'\n \n-SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'\n+SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n \n SPIDER_MIDDLEWARES = {}\n \n\n@@ -97,7 +97,9 @@ class ObsoleteClass(object):\n     def __getattr__(self, name):\n         raise AttributeError(self.message)\n \n-spiders = ObsoleteClass(\"\"\"\n-\"from scrapy.spider import spiders\" no longer works - use \"from scrapy.spidermanager import SpiderManager\" and instantiate it with your project settings\"\n-\"\"\")\n+spiders = ObsoleteClass(\n+    '\"from scrapy.spider import spiders\" no longer works - use '\n+    '\"from scrapy.spiderloader import SpiderLoader\" and instantiate '\n+    'it with your project settings\"'\n+)\n \n\n@@ -0,0 +1,53 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+\n+from zope.interface import implementer\n+\n+from scrapy.interfaces import ISpiderLoader\n+from scrapy.utils.misc import walk_modules\n+from scrapy.utils.spider import iter_spider_classes\n+\n+\n+@implementer(ISpiderLoader)\n+class SpiderLoader(object):\n+    \"\"\"\n+    SpiderLoader is a class which locates and loads spiders\n+    in a Scrapy project.\n+    \"\"\"\n+    def __init__(self, settings):\n+        self.spider_modules = settings.getlist('SPIDER_MODULES')\n+        self._spiders = {}\n+        for name in self.spider_modules:\n+            for module in walk_modules(name):\n+                self._load_spiders(module)\n+\n+    def _load_spiders(self, module):\n+        for spcls in iter_spider_classes(module):\n+            self._spiders[spcls.name] = spcls\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(settings)\n+\n+    def load(self, spider_name):\n+        \"\"\"\n+        Return the Spider class for the given spider name. If the spider\n+        name is not found, raise a KeyError.\n+        \"\"\"\n+        try:\n+            return self._spiders[spider_name]\n+        except KeyError:\n+            raise KeyError(\"Spider not found: {}\".format(spider_name))\n+\n+    def find_by_request(self, request):\n+        \"\"\"\n+        Return the list of spiders names that can handle the given request.\n+        \"\"\"\n+        return [name for name, cls in self._spiders.items()\n+                if cls.handles_request(request)]\n+\n+    def list(self):\n+        \"\"\"\n+        Return a list with the names of all spiders available in the project.\n+        \"\"\"\n+        return list(self._spiders.keys())\n\n@@ -1,43 +1,7 @@\n \"\"\"\n-SpiderManager is the class which locates and manages all website-specific\n-spiders\n+Backwards compatibility shim. Use scrapy.spiderloader instead.\n \"\"\"\n+from scrapy.spiderloader import SpiderLoader\n+from scrapy.utils.deprecate import create_deprecated_class\n \n-from zope.interface import implementer\n-import six\n-\n-from scrapy.interfaces import ISpiderManager\n-from scrapy.utils.misc import walk_modules\n-from scrapy.utils.spider import iter_spider_classes\n-\n-\n-@implementer(ISpiderManager)\n-class SpiderManager(object):\n-\n-    def __init__(self, settings):\n-        self.spider_modules = settings.getlist('SPIDER_MODULES')\n-        self._spiders = {}\n-        for name in self.spider_modules:\n-            for module in walk_modules(name):\n-                self._load_spiders(module)\n-\n-    def _load_spiders(self, module):\n-        for spcls in iter_spider_classes(module):\n-            self._spiders[spcls.name] = spcls\n-\n-    @classmethod\n-    def from_settings(cls, settings):\n-        return cls(settings)\n-\n-    def load(self, spider_name):\n-        try:\n-            return self._spiders[spider_name]\n-        except KeyError:\n-            raise KeyError(\"Spider not found: {}\".format(spider_name))\n-\n-    def find_by_request(self, request):\n-        return [name for name, cls in six.iteritems(self._spiders)\n-            if cls.handles_request(request)]\n-\n-    def list(self):\n-        return list(self._spiders.keys())\n+SpiderManager = create_deprecated_class('SpiderManager', SpiderLoader)\n\n@@ -26,21 +26,21 @@ def iter_spider_classes(module):\n            getattr(obj, 'name', None):\n             yield obj\n \n-def spidercls_for_request(spidermanager, request, default_spidercls=None,\n+def spidercls_for_request(spiderloader, request, default_spidercls=None,\n                           log_none=False, log_multiple=False):\n     \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n-    the spider manager) and return a Spider class if (and only if) there is\n+    the spider loader) and return a Spider class if (and only if) there is\n     only one Spider able to handle the Request.\n \n     If multiple spiders (or no spider) are found, it will return the\n     default_spidercls passed. It can optionally log if multiple or no spiders\n     are found.\n     \"\"\"\n-    snames = spidermanager.find_by_request(request)\n+    snames = spiderloader.find_by_request(request)\n     if len(snames) == 1:\n-        return spidermanager.load(snames[0])\n+        return spiderloader.load(snames[0])\n \n     if len(snames) > 1 and log_multiple:\n         log.msg(format='More than one spider can handle: %(request)s - %(snames)s',\n\n@@ -19,7 +19,7 @@ class CrawlerTestCase(unittest.TestCase):\n             spiders = self.crawler.spiders\n             self.assertEqual(len(w), 1)\n             self.assertIn(\"Crawler.spiders\", str(w[0].message))\n-            sm_cls = load_object(self.crawler.settings['SPIDER_MANAGER_CLASS'])\n+            sm_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])\n             self.assertIsInstance(spiders, sm_cls)\n \n             self.crawler.spiders\n@@ -54,7 +54,7 @@ class CrawlerTestCase(unittest.TestCase):\n \n \n \n-def SpiderManagerWithWrongInterface(object):\n+class SpiderLoaderWithWrongInterface(object):\n \n     def unneeded_method(self):\n         pass\n@@ -64,7 +64,7 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n \n     def test_spider_manager_verify_interface(self):\n         settings = Settings({\n-            'SPIDER_MANAGER_CLASS': 'tests.test_crawler.SpiderManagerWithWrongInterface'\n+            'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'\n         })\n         with self.assertRaises(DoesNotImplement):\n             CrawlerRunner(settings)\n\n@@ -8,14 +8,15 @@ from twisted.trial import unittest\n \n # ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n-from scrapy.interfaces import ISpiderManager\n-from scrapy.spidermanager import SpiderManager\n+from scrapy.interfaces import ISpiderLoader\n+from scrapy.spiderloader import SpiderLoader\n from scrapy.settings import Settings\n from scrapy.http import Request\n \n module_dir = os.path.dirname(os.path.abspath(__file__))\n \n-class SpiderManagerTest(unittest.TestCase):\n+\n+class SpiderLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n         orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n@@ -25,53 +26,53 @@ class SpiderManagerTest(unittest.TestCase):\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n         settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n-        self.spiderman = SpiderManager.from_settings(settings)\n+        self.spiderloader = SpiderLoader.from_settings(settings)\n \n     def tearDown(self):\n-        del self.spiderman\n+        del self.spiderloader\n         del sys.modules['test_spiders_xxx']\n         sys.path.remove(self.tmpdir)\n \n     def test_interface(self):\n-        verifyObject(ISpiderManager, self.spiderman)\n+        verifyObject(ISpiderLoader, self.spiderloader)\n \n     def test_list(self):\n-        self.assertEqual(set(self.spiderman.list()),\n+        self.assertEqual(set(self.spiderloader.list()),\n             set(['spider1', 'spider2', 'spider3']))\n \n     def test_load(self):\n-        spider1 = self.spiderman.load(\"spider1\")\n+        spider1 = self.spiderloader.load(\"spider1\")\n         self.assertEqual(spider1.__name__, 'Spider1')\n \n     def test_find_by_request(self):\n-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy1.org/test')),\n+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy1.org/test')),\n             ['spider1'])\n-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy2.org/test')),\n+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy2.org/test')),\n             ['spider2'])\n-        self.assertEqual(set(self.spiderman.find_by_request(Request('http://scrapy3.org/test'))),\n+        self.assertEqual(set(self.spiderloader.find_by_request(Request('http://scrapy3.org/test'))),\n             set(['spider1', 'spider2']))\n-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy999.org/test')),\n+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n-        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com')),\n+        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com')),\n             [])\n-        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com/onlythis')),\n+        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com/onlythis')),\n             ['spider3'])\n \n     def test_load_spider_module(self):\n-        module = 'tests.test_spidermanager.test_spiders.spider1'\n+        module = 'tests.test_spiderloader.test_spiders.spider1'\n         settings = Settings({'SPIDER_MODULES': [module]})\n-        self.spiderman = SpiderManager.from_settings(settings)\n-        assert len(self.spiderman._spiders) == 1\n+        self.spiderloader = SpiderLoader.from_settings(settings)\n+        assert len(self.spiderloader._spiders) == 1\n \n     def test_load_spider_module(self):\n-        prefix = 'tests.test_spidermanager.test_spiders.'\n+        prefix = 'tests.test_spiderloader.test_spiders.'\n         module = ','.join(prefix + s for s in ('spider1', 'spider2'))\n         settings = Settings({'SPIDER_MODULES': module})\n-        self.spiderman = SpiderManager.from_settings(settings)\n-        assert len(self.spiderman._spiders) == 2\n+        self.spiderloader = SpiderLoader.from_settings(settings)\n+        assert len(self.spiderloader._spiders) == 2\n \n     def test_load_base_spider(self):\n-        module = 'tests.test_spidermanager.test_spiders.spider0'\n+        module = 'tests.test_spiderloader.test_spiders.spider0'\n         settings = Settings({'SPIDER_MODULES': [module]})\n-        self.spiderman = SpiderManager.from_settings(settings)\n-        assert len(self.spiderman._spiders) == 0\n+        self.spiderloader = SpiderLoader.from_settings(settings)\n+        assert len(self.spiderloader._spiders) == 0\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
