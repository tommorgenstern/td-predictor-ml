{"custom_id": "keras#0a108b3fb2b53a95cba0fc359171594ae1d43f61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 187 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import numpy as np\n from numpy.testing import assert_allclose\n import inspect\n+import functools\n \n from ..engine import Model, Input\n from ..models import Sequential, model_from_json\n@@ -107,6 +108,7 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n def keras_test(func):\n     '''Clean up after tensorflow tests.\n     '''\n+    @functools.wraps(func)\n     def wrapper(*args, **kwargs):\n         output = func(*args, **kwargs)\n         if K._BACKEND == 'tensorflow':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a56925176a2532e729d425a8f5e2c9a24dc5f86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 46 | Files Changed: 6 | Hunks: 41 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 90 | Churn Cumulative: 1795 | Contributors (this commit): 11 | Commits (past 90d): 17 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,13 +2,14 @@ from __future__ import print_function\n import numpy as np\n import pytest\n \n-from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import Sequential\n from keras.layers.core import Dense, Flatten, Activation\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils.np_utils import to_categorical\n \n \n+@keras_test\n def test_image_classification():\n     '''\n     Classify random 16x16 color images into several classes using logistic regression\n\n@@ -3,7 +3,7 @@ import numpy as np\n import pytest\n import string\n \n-from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import get_test_data, keras_test\n from keras.utils.np_utils import to_categorical\n from keras.models import Sequential\n from keras.layers import TimeDistributedDense\n@@ -14,6 +14,7 @@ from keras.layers import LSTM\n from keras.layers import Embedding\n \n \n+@keras_test\n def test_temporal_classification():\n     '''\n     Classify temporal sequences of float numbers\n@@ -43,6 +44,7 @@ def test_temporal_classification():\n     assert(history.history['val_acc'][-1] >= 0.85)\n \n \n+@keras_test\n def test_temporal_regression():\n     '''\n     Predict float numbers (regression) based on sequences\n@@ -63,6 +65,7 @@ def test_temporal_regression():\n     assert(history.history['val_loss'][-1] < 0.75)\n \n \n+@keras_test\n def test_sequence_to_sequence():\n     '''\n     Apply a same Dense layer for each element of time dimension of the input\n@@ -86,6 +89,7 @@ def test_sequence_to_sequence():\n     assert(history.history['val_loss'][-1] < 0.8)\n \n \n+@keras_test\n def test_stacked_lstm_char_prediction():\n     '''\n     Learn alphabetical char sequence with stacked LSTM.\n@@ -135,6 +139,7 @@ def test_stacked_lstm_char_prediction():\n     assert(generated == alphabet)\n \n \n+@keras_test\n def test_masked_temporal():\n     '''\n     Confirm that even with masking on both inputs and outputs, cross-entropies are\n@@ -182,5 +187,4 @@ def test_masked_temporal():\n     assert(np.abs(history.history['val_loss'][-1] - ground_truth) < 0.06)\n \n if __name__ == '__main__':\n-    # pytest.main([__file__])\n-    test_temporal_classification()\n+    pytest.main([__file__])\n\n@@ -2,12 +2,13 @@ from __future__ import print_function\n import numpy as np\n import pytest\n \n-from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils.np_utils import to_categorical\n \n \n+@keras_test\n def test_vector_classification():\n     '''\n     Classify random float vectors into 2 classes with logistic regression\n@@ -37,6 +38,7 @@ def test_vector_classification():\n     assert(history.history['val_acc'][-1] > 0.8)\n \n \n+@keras_test\n def test_vector_regression():\n     '''\n     Perform float data prediction (regression) using 2 layer MLP\n\n@@ -11,9 +11,9 @@ from keras.layers import convolutional\n def test_convolution_1d():\n     nb_samples = 2\n     nb_steps = 8\n-    input_dim = 5\n+    input_dim = 2\n     filter_length = 3\n-    nb_filter = 4\n+    nb_filter = 3\n \n     for border_mode in ['valid', 'same']:\n         for subsample_length in [1]:\n@@ -58,8 +58,8 @@ def test_averagepooling_1d():\n @keras_test\n def test_convolution_2d():\n     nb_samples = 2\n-    nb_filter = 3\n-    stack_size = 4\n+    nb_filter = 2\n+    stack_size = 3\n     nb_row = 10\n     nb_col = 6\n \n@@ -91,8 +91,8 @@ def test_convolution_2d():\n @keras_test\n def test_atrous_conv_2d():\n     nb_samples = 2\n-    nb_filter = 3\n-    stack_size = 4\n+    nb_filter = 2\n+    stack_size = 3\n     nb_row = 10\n     nb_col = 6\n \n@@ -130,8 +130,8 @@ def test_atrous_conv_2d():\n @keras_test\n def test_separable_conv_2d():\n     nb_samples = 2\n-    nb_filter = 8\n-    stack_size = 4\n+    nb_filter = 6\n+    stack_size = 3\n     nb_row = 10\n     nb_col = 6\n \n@@ -195,8 +195,8 @@ def test_averagepooling_2d():\n @keras_test\n def test_convolution_3d():\n     nb_samples = 2\n-    nb_filter = 5\n-    stack_size = 4\n+    nb_filter = 2\n+    stack_size = 3\n     kernel_dim1 = 2\n     kernel_dim2 = 3\n     kernel_dim3 = 1\n@@ -261,7 +261,7 @@ def test_averagepooling_3d():\n @keras_test\n def test_zero_padding_2d():\n     nb_samples = 2\n-    stack_size = 7\n+    stack_size = 2\n     input_nb_row = 11\n     input_nb_col = 12\n \n@@ -287,7 +287,7 @@ def test_zero_padding_2d():\n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_zero_padding_3d():\n     nb_samples = 2\n-    stack_size = 7\n+    stack_size = 2\n     input_len_dim1 = 10\n     input_len_dim2 = 11\n     input_len_dim3 = 12\n@@ -322,7 +322,7 @@ def test_upsampling_1d():\n @keras_test\n def test_upsampling_2d():\n     nb_samples = 2\n-    stack_size = 7\n+    stack_size = 2\n     input_nb_row = 11\n     input_nb_col = 12\n \n@@ -363,7 +363,7 @@ def test_upsampling_2d():\n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_upsampling_3d():\n     nb_samples = 2\n-    stack_size = 7\n+    stack_size = 2\n     input_len_dim1 = 10\n     input_len_dim2 = 11\n     input_len_dim3 = 12\n\n@@ -11,7 +11,7 @@ from keras.utils.test_utils import keras_test\n \n from keras import backend as K\n \n-nb_samples, timesteps, embedding_dim, output_dim = 3, 5, 10, 5\n+nb_samples, timesteps, embedding_dim, output_dim = 2, 5, 4, 3\n embedding_num = 12\n \n \n@@ -24,21 +24,21 @@ def _runner(layer_class):\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n                        'return_sequences': True},\n-               input_shape=(3, 2, 3))\n+               input_shape=(nb_samples, timesteps, embedding_dim))\n \n     # check dropout\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n                        'dropout_U': 0.1,\n                        'dropout_W': 0.1},\n-               input_shape=(3, 2, 3))\n+               input_shape=(nb_samples, timesteps, embedding_dim))\n \n     # check implementation modes\n     for mode in ['cpu', 'mem', 'gpu']:\n         layer_test(layer_class,\n                    kwargs={'output_dim': output_dim,\n                            'consume_less': mode},\n-                   input_shape=(3, 2, 3))\n+                   input_shape=(nb_samples, timesteps, embedding_dim))\n \n     # check statefulness\n     model = Sequential()\n@@ -83,7 +83,6 @@ def _runner(layer_class):\n     left_padded_input = np.ones((nb_samples, timesteps))\n     left_padded_input[0, :1] = 0\n     left_padded_input[1, :2] = 0\n-    left_padded_input[2, :3] = 0\n     out6 = model.predict(left_padded_input)\n \n     layer.reset_states()\n@@ -91,7 +90,6 @@ def _runner(layer_class):\n     right_padded_input = np.ones((nb_samples, timesteps))\n     right_padded_input[0, -1:] = 0\n     right_padded_input[1, -2:] = 0\n-    right_padded_input[2, -3:] = 0\n     out7 = model.predict(right_padded_input)\n \n     assert_allclose(out7, out6, atol=1e-5)\n\n@@ -2,14 +2,16 @@ from __future__ import print_function\n import pytest\n import numpy as np\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n+from keras.layers.core import Dense\n+from keras.utils.test_utils import keras_test\n \n \n+@keras_test\n def test_multiprocessing_training():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -27,10 +29,7 @@ def test_multiprocessing_training():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -53,11 +52,12 @@ def test_multiprocessing_training():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_training_fromfile():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n     np.savez(\"data.npz\", **{\"data\": arr_data, \"labels\": arr_labels})\n \n@@ -78,10 +78,7 @@ def test_multiprocessing_training_fromfile():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.fit_generator(myGenerator(),\n@@ -103,11 +100,12 @@ def test_multiprocessing_training_fromfile():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_predicting():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n \n     def myGenerator():\n \n@@ -123,10 +121,7 @@ def test_multiprocessing_predicting():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n     model.predict_generator(myGenerator(),\n                             val_samples=320,\n@@ -142,11 +137,12 @@ def test_multiprocessing_predicting():\n     assert reached_end\n \n \n+@keras_test\n def test_multiprocessing_evaluating():\n \n     reached_end = False\n \n-    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_data = np.random.randint(0, 256, (500, 2))\n     arr_labels = np.random.randint(0, 2, 500)\n \n     def myGenerator():\n@@ -164,10 +160,7 @@ def test_multiprocessing_evaluating():\n \n     # Build a NN\n     model = Sequential()\n-    model.add(Dense(10, input_shape=(200, )))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n+    model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n     model.evaluate_generator(myGenerator(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fa65fbb2f377c14673645800142d7334007818f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 671 | Contributors (this commit): 13 | Commits (past 90d): 10 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -35,6 +35,7 @@ class BatchNormalization(Layer):\n         weights: Initialization weights.\n             List of 2 Numpy arrays, with shapes:\n             `[(input_shape,), (input_shape,)]`\n+            Note that the order of this list is [gamma, beta, mean, std]\n         beta_init: name of initialization function for shift parameter\n             (see [initializations](../initializations.md)), or alternatively,\n             Theano/TensorFlow function to use for weights initialization.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf9922ff1dabeae72afb162d79abaef63da1f69d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 15 | Churn Cumulative: 135 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,9 +12,9 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Lambda\n+from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Embedding\n-from keras.layers import Convolution1D\n+from keras.layers import Convolution1D, MaxPooling1D\n from keras.datasets import imdb\n from keras import backend as K\n \n@@ -58,13 +58,12 @@ model.add(Convolution1D(nb_filter=nb_filter,\n                         border_mode='valid',\n                         activation='relu',\n                         subsample_length=1))\n+# we use max pooling:\n+model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n \n-# we use max over time pooling by defining a python function to use\n-# in a Lambda layer\n-def max_1d(X):\n-    return K.max(X, axis=1)\n-\n-model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n+# We flatten the output of the conv layer,\n+# so that we can add a vanilla dense layer:\n+model.add(Flatten())\n \n # We add a vanilla hidden layer:\n model.add(Dense(hidden_dims))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3533912016c64f1f03516657c1d93f02a11e5c67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 68 | Files Changed: 6 | Hunks: 45 | Methods Changed: 24 | Complexity Δ (Sum/Max): 12/6 | Churn Δ: 204 | Churn Cumulative: 6578 | Contributors (this commit): 56 | Commits (past 90d): 72 | Contributors (cumulative): 88 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,15 +201,15 @@ def eval(x):\n def zeros(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-zeros tensor variable.\n     '''\n-    return variable(lambda: tf.cast(tf.constant_initializer(0.)(shape), dtype),\n-                    dtype, name)\n+    tf_dtype = _convert_string_dtype(dtype)\n+    return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape), dtype, name)\n \n \n def ones(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-ones tensor variable.\n     '''\n-    return variable(lambda: tf.cast(tf.constant_initializer(1.)(shape), dtype),\n-                    dtype, name)\n+    tf_dtype = _convert_string_dtype(dtype)\n+    return variable(tf.constant_initializer(1., dtype=tf_dtype)(shape), dtype, name)\n \n \n def eye(size, dtype=_FLOATX, name=None):\n@@ -232,6 +232,18 @@ def ones_like(x, name=None):\n     return tf.ones_like(x, name=name)\n \n \n+def random_uniform_variable(shape, low, high, dtype=_FLOATX, name=None):\n+    tf_dtype = _convert_string_dtype(dtype)\n+    value = tf.random_uniform_initializer(low, high, dtype=tf_dtype)(shape)\n+    return variable(value, dtype=dtype, name=name)\n+\n+\n+def random_normal_variable(shape, mean, scale, dtype=_FLOATX, name=None):\n+    tf_dtype = _convert_string_dtype(dtype)\n+    value = tf.random_normal_initializer(mean, scale, dtype=tf_dtype)(shape)\n+    return variable(value, dtype=dtype, name=name)\n+\n+\n def count_params(x):\n     '''Returns the number of scalars in a tensor.\n     '''\n@@ -245,6 +257,26 @@ def cast(x, dtype):\n     return tf.cast(x, dtype)\n \n \n+# UPDATES OPS\n+\n+\n+def update(x, new_x):\n+    return tf.assign(x, new_x)\n+\n+\n+def update_add(x, increment):\n+    return tf.assign_add(x, increment)\n+\n+\n+def update_sub(x, decrement):\n+    return tf.assign_sub(x, decrement)\n+\n+\n+def moving_average_update(variable, value, momentum):\n+    return tf.python.training.moving_averages.assign_moving_average(\n+        variable, value, momentum)\n+\n+\n # LINEAR ALGEBRA\n \n def dot(x, y):\n@@ -277,7 +309,10 @@ def batch_dot(x, y, axes=None):\n         axes: list (or single) int with target dimensions\n \n     # Returns\n-        A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1).\n+        A tensor with shape equal to the concatenation of x's shape\n+        (less the dimension that was summed over) and y's shape\n+        (less the batch dimension and the dimension that was summed over).\n+        If the final rank is 1, we reshape it to (batch_size, 1).\n \n     # Examples\n         Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n@@ -286,12 +321,17 @@ def batch_dot(x, y, axes=None):\n         elements.\n \n         Shape inference:\n-        Let x's shape be (100, 20) and y's shape be (100, 30, 20). If dot_axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x's shape and y's shape:\n+        Let x's shape be (100, 20) and y's shape be (100, 30, 20).\n+        If dot_axes is (1, 2), to find the output shape of resultant tensor,\n+            loop through each dimension in x's shape and y's shape:\n         x.shape[0] : 100 : append to output shape\n-        x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. (dot_axes[0] = 1)\n-        y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y\n+        x.shape[1] : 20 : do not append to output shape,\n+            dimension 1 of x has been summed over. (dot_axes[0] = 1)\n+        y.shape[0] : 100 : do not append to output shape,\n+            always ignore first dimension of y\n         y.shape[1] : 30 : append to output shape\n-        y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. (dot_axes[1] = 2)\n+        y.shape[2] : 20 : do not append to output shape,\n+            dimension 2 of y has been summed over. (dot_axes[1] = 2)\n \n         output_shape = (100, 30)\n     '''\n\n@@ -97,6 +97,16 @@ def zeros_like(x):\n     return T.zeros_like(x)\n \n \n+def random_uniform_variable(shape, low, high, dtype=_FLOATX, name=None):\n+    return variable(np.random.uniform(low=low, high=high, size=shape),\n+                    dtype=dtype, name=name)\n+\n+\n+def random_normal_variable(shape, mean, scale, dtype=_FLOATX, name=None):\n+    return variable(np.random.normal(loc=0.0, scale=scale, size=shape),\n+                    dtype=dtype, name=name)\n+\n+\n def count_params(x):\n     '''Return number of scalars in a tensor.\n \n@@ -109,6 +119,25 @@ def cast(x, dtype):\n     return T.cast(x, dtype)\n \n \n+# UPDATES OPS\n+\n+\n+def update(x, new_x):\n+    return (x, new_x)\n+\n+\n+def update_add(x, increment):\n+    return (x, x + increment)\n+\n+\n+def update_sub(x, decrement):\n+    return (x, x - decrement)\n+\n+\n+def moving_average_update(variable, value, momentum):\n+    return (variable, variable * momentum + value * (1. - momentum))\n+\n+\n # LINEAR ALGEBRA\n \n '''\n@@ -133,7 +162,10 @@ def batch_dot(x, y, axes=None):\n         axes: list (or single) int with target dimensions\n \n     # Returns\n-        A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1).\n+        A tensor with shape equal to the concatenation of x's shape\n+        (less the dimension that was summed over) and y's shape\n+        (less the batch dimension and the dimension that was summed over).\n+        If the final rank is 1, we reshape it to (batch_size, 1).\n \n     # Examples\n         Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n@@ -142,12 +174,17 @@ def batch_dot(x, y, axes=None):\n         elements.\n \n         Shape inference:\n-        Let x's shape be (100, 20) and y's shape be (100, 30, 20). If dot_axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x's shape and y's shape:\n+        Let x's shape be (100, 20) and y's shape be (100, 30, 20).\n+        If dot_axes is (1, 2), to find the output shape of resultant tensor,\n+            loop through each dimension in x's shape and y's shape:\n         x.shape[0] : 100 : append to output shape\n-        x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. (dot_axes[0] = 1)\n-        y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y\n+        x.shape[1] : 20 : do not append to output shape,\n+            dimension 1 of x has been summed over. (dot_axes[0] = 1)\n+        y.shape[0] : 100 : do not append to output shape,\n+            always ignore first dimension of y\n         y.shape[1] : 30 : append to output shape\n-        y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. (dot_axes[1] = 2)\n+        y.shape[2] : 20 : do not append to output shape,\n+            dimension 2 of y has been summed over. (dot_axes[1] = 2)\n \n         output_shape = (100, 30)\n     '''\n\n@@ -29,13 +29,11 @@ def get_fans(shape, dim_ordering='th'):\n \n \n def uniform(shape, scale=0.05, name=None):\n-    return K.variable(np.random.uniform(low=-scale, high=scale, size=shape),\n-                      name=name)\n+    return K.random_uniform_variable(shape, -scale, scale, name=name)\n \n \n def normal(shape, scale=0.05, name=None):\n-    return K.variable(np.random.normal(loc=0.0, scale=scale, size=shape),\n-                      name=name)\n+    return K.random_normal_variable(shape, 0.0, scale, name=name)\n \n \n def lecun_uniform(shape, name=None, dim_ordering='th'):\n\n@@ -55,7 +55,7 @@ class BatchNormalization(Layer):\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.html)\n     '''\n-    def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n+    def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.99,\n                  weights=None, beta_init='zero', gamma_init='one', **kwargs):\n         self.supports_masking = True\n         self.beta_init = initializations.get(beta_init)\n@@ -99,17 +99,10 @@ class BatchNormalization(Layer):\n             broadcast_shape = [1] * len(input_shape)\n             broadcast_shape[self.axis] = input_shape[self.axis]\n \n-            # # case: train mode (uses stats of the current batch)\n-            # mean = K.mean(x, axis=reduction_axes)\n-            # brodcast_mean = K.reshape(mean, broadcast_shape)\n-            # std = K.mean(K.square(x - brodcast_mean) + self.epsilon, axis=reduction_axes)\n-            # std = K.sqrt(std)\n-            # brodcast_std = K.reshape(std, broadcast_shape)\n-\n             if self.mode == 2:\n-                x_normed, mean, std = K.normalize_batch_in_training(x, self.gamma, self.beta, reduction_axes, epsilon=self.epsilon)\n-                mean_update = self.momentum * self.running_mean + (1 - self.momentum) * mean\n-                std_update = self.momentum * self.running_std + (1 - self.momentum) * std\n+                x_normed, mean, std = K.normalize_batch_in_training(\n+                    x, self.gamma, self.beta, reduction_axes,\n+                    epsilon=self.epsilon)\n             else:\n                 # mode 0\n                 if self.called_with not in {None, x}:\n@@ -123,17 +116,17 @@ class BatchNormalization(Layer):\n                                     '(see docs for a description of '\n                                     'the behavior).')\n                 self.called_with = x\n-                x_normed, mean, std = K.normalize_batch_in_training(x, self.gamma, self.beta, reduction_axes, epsilon=self.epsilon)\n-                mean_update = self.momentum * self.running_mean + (1 - self.momentum) * mean\n-                std_update = self.momentum * self.running_std + (1 - self.momentum) * std\n-                self.updates = [(self.running_mean, mean_update),\n-                                (self.running_std, std_update)]\n+                x_normed, mean, std = K.normalize_batch_in_training(\n+                    x, self.gamma, self.beta, reduction_axes,\n+                    epsilon=self.epsilon)\n+\n+                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n+                                K.moving_average_update(self.running_std, std, self.momentum)]\n \n                 if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n-                    x_normed_running = K.batch_normalization(x, self.running_mean,\n-                                                             self.running_std,\n-                                                             self.beta,\n-                                                             self.gamma,\n+                    x_normed_running = K.batch_normalization(\n+                        x, self.running_mean, self.running_std,\n+                        self.beta, self.gamma,\n                         epsilon=self.epsilon)\n                 else:\n                     # need broadcasting\n@@ -141,10 +134,9 @@ class BatchNormalization(Layer):\n                     broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n                     broadcast_beta = K.reshape(self.beta, broadcast_shape)\n                     broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n-                    x_normed_running = K.batch_normalization(x, broadcast_running_mean,\n-                                                             broadcast_running_std,\n-                                                             broadcast_beta,\n-                                                             broadcast_gamma,\n+                    x_normed_running = K.batch_normalization(\n+                        x, broadcast_running_mean, broadcast_running_std,\n+                        broadcast_beta, broadcast_gamma,\n                         epsilon=self.epsilon)\n \n                 # pick the normalized form of x corresponding to the training phase\n\n@@ -124,13 +124,13 @@ class SGD(Optimizer):\n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         lr = self.lr * (1. / (1. + self.decay * self.iterations))\n-        self.updates = [(self.iterations, self.iterations + 1.)]\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         # momentum\n         self.weights = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n         for p, g, m in zip(params, grads, self.weights):\n             v = self.momentum * m - lr * g  # velocity\n-            self.updates.append((m, v))\n+            self.updates.append(K.update(m, v))\n \n             if self.nesterov:\n                 new_p = p + self.momentum * v - lr * g\n@@ -141,7 +141,8 @@ class SGD(Optimizer):\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n@@ -183,14 +184,14 @@ class RMSprop(Optimizer):\n         for p, g, a in zip(params, grads, self.weights):\n             # update accumulator\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n-            self.updates.append((a, new_a))\n+            self.updates.append(K.update(a, new_a))\n             new_p = p - self.lr * g / (K.sqrt(new_a) + self.epsilon)\n \n             # apply constraints\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n@@ -224,13 +225,13 @@ class Adagrad(Optimizer):\n \n         for p, g, a in zip(params, grads, self.weights):\n             new_a = a + K.square(g)  # update accumulator\n-            self.updates.append((a, new_a))\n+            self.updates.append(K.update(a, new_a))\n             new_p = p - self.lr * g / (K.sqrt(new_a) + self.epsilon)\n             # apply constraints\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n@@ -270,7 +271,7 @@ class Adadelta(Optimizer):\n         for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n             # update accumulator\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n-            self.updates.append((a, new_a))\n+            self.updates.append(K.update(a, new_a))\n \n             # use the new accumulator and the *old* delta_accumulator\n             update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n@@ -280,11 +281,11 @@ class Adadelta(Optimizer):\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n \n             # update delta_accumulator\n             new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)\n-            self.updates.append((d_a, new_d_a))\n+            self.updates.append(K.update(d_a, new_d_a))\n         return self.updates\n \n     def get_config(self):\n@@ -319,7 +320,7 @@ class Adam(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        self.updates = [(self.iterations, self.iterations + 1)]\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         t = self.iterations + 1\n         lr_t = self.lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n@@ -333,15 +334,15 @@ class Adam(Optimizer):\n             v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n             p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n \n-            self.updates.append((m, m_t))\n-            self.updates.append((v, v_t))\n+            self.updates.append(K.update(m, m_t))\n+            self.updates.append(K.update(v, v_t))\n \n             new_p = p_t\n             # apply constraints\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n@@ -378,7 +379,7 @@ class Adamax(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        self.updates = [(self.iterations, self.iterations + 1)]\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         t = self.iterations + 1\n         lr_t = self.lr / (1. - K.pow(self.beta_1, t))\n@@ -395,15 +396,15 @@ class Adamax(Optimizer):\n             u_t = K.maximum(self.beta_2 * u, K.abs(g))\n             p_t = p - lr_t * m_t / (u_t + self.epsilon)\n \n-            self.updates.append((m, m_t))\n-            self.updates.append((u, u_t))\n+            self.updates.append(K.update(m, m_t))\n+            self.updates.append(K.update(u, u_t))\n \n             new_p = p_t\n             # apply constraints\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n@@ -447,7 +448,7 @@ class Nadam(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        self.updates = [(self.iterations, self.iterations + 1)]\n+        self.updates = [K.update_add(self.iterations, 1)]\n \n         t = self.iterations + 1\n \n@@ -472,8 +473,8 @@ class Nadam(Optimizer):\n             v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n             m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n \n-            self.updates.append((m, m_t))\n-            self.updates.append((v, v_t))\n+            self.updates.append(K.update(m, m_t))\n+            self.updates.append(K.update(v, v_t))\n \n             p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n             new_p = p_t\n@@ -482,7 +483,7 @@ class Nadam(Optimizer):\n             if p in constraints:\n                 c = constraints[p]\n                 new_p = c(new_p)\n-            self.updates.append((p, new_p))\n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n     def get_config(self):\n\n@@ -28,13 +28,13 @@ def basic_batchnorm_test():\n def test_batchnorm_mode_0_or_2():\n     for mode in [0, 2]:\n         model = Sequential()\n-        norm_m0 = normalization.BatchNormalization(mode=mode, input_shape=(10,))\n+        norm_m0 = normalization.BatchNormalization(mode=mode, input_shape=(10,), momentum=0.8)\n         model.add(norm_m0)\n         model.compile(loss='mse', optimizer='sgd')\n \n         # centered on 5.0, variance 10.0\n         X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n-        model.fit(X, X, nb_epoch=5, verbose=0)\n+        model.fit(X, X, nb_epoch=4, verbose=0)\n         out = model.predict(X)\n         out -= K.eval(norm_m0.beta)\n         out /= K.eval(norm_m0.gamma)\n@@ -46,13 +46,13 @@ def test_batchnorm_mode_0_or_2():\n @keras_test\n def test_batchnorm_mode_0_convnet():\n     model = Sequential()\n-    norm_m0 = normalization.BatchNormalization(mode=0, axis=1, input_shape=(3, 4, 4))\n+    norm_m0 = normalization.BatchNormalization(mode=0, axis=1, input_shape=(3, 4, 4), momentum=0.8)\n     model.add(norm_m0)\n     model.compile(loss='mse', optimizer='sgd')\n \n     # centered on 5.0, variance 10.0\n     X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n-    model.fit(X, X, nb_epoch=5, verbose=0)\n+    model.fit(X, X, nb_epoch=4, verbose=0)\n     out = model.predict(X)\n     out -= np.reshape(K.eval(norm_m0.beta), (1, 3, 1, 1))\n     out /= np.reshape(K.eval(norm_m0.gamma), (1, 3, 1, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4302d8060d06b8a19f2d16b5565ffdabf82277dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1192 | Contributors (this commit): 14 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -162,7 +162,7 @@ def load_img(path, grayscale=False, target_size=None):\n     else:  # Ensure 3 channel even when loaded image is grayscale\n         img = img.convert('RGB')\n     if target_size:\n-        img = img.resize(target_size)\n+        img = img.resize((target_size[1], target_size[0]))\n     return img\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74c51f213c7a187b76098331ca10fee10c9e5e59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2245 | Contributors (this commit): 25 | Commits (past 90d): 32 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,6 +201,7 @@ def eval(x):\n def zeros(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-zeros tensor variable.\n     '''\n+    shape = map(int, shape)\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape), dtype, name)\n \n@@ -208,6 +209,7 @@ def zeros(shape, dtype=_FLOATX, name=None):\n def ones(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-ones tensor variable.\n     '''\n+    shape = map(int, shape)\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(1., dtype=tf_dtype)(shape), dtype, name)\n \n@@ -233,12 +235,14 @@ def ones_like(x, name=None):\n \n \n def random_uniform_variable(shape, low, high, dtype=_FLOATX, name=None):\n+    shape = map(int, shape)\n     tf_dtype = _convert_string_dtype(dtype)\n     value = tf.random_uniform_initializer(low, high, dtype=tf_dtype)(shape)\n     return variable(value, dtype=dtype, name=name)\n \n \n def random_normal_variable(shape, mean, scale, dtype=_FLOATX, name=None):\n+    shape = map(int, shape)\n     tf_dtype = _convert_string_dtype(dtype)\n     value = tf.random_normal_initializer(mean, scale, dtype=tf_dtype)(shape)\n     return variable(value, dtype=dtype, name=name)\n\n@@ -7,7 +7,7 @@ import keras.backend as K\n @keras_test\n def test_embedding():\n     layer_test(Embedding,\n-               kwargs={'output_dim': 4., 'input_dim': 10, 'input_length': 2},\n+               kwargs={'output_dim': 4, 'input_dim': 10, 'input_length': 2},\n                input_shape=(3, 2),\n                input_dtype='int32',\n                expected_output_dtype=K.floatx())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ac6811362aafdbf275c18d634ae6ba78cea406d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 460 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 460 | Churn Cumulative: 1274 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,460 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import pytest\n-import os\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras import backend as K\n-from keras.models import Graph, Sequential\n-from keras.layers.core import Dense, Activation, Merge, Lambda\n-from keras.utils.test_utils import get_test_data\n-from keras.models import model_from_json, model_from_yaml\n-from keras.utils.test_utils import keras_test\n-\n-\n-batch_size = 32\n-\n-(X_train_graph, y_train_graph), (X_test_graph, y_test_graph) = get_test_data(nb_train=100,\n-                                                                             nb_test=50,\n-                                                                             input_shape=(32,),\n-                                                                             classification=False,\n-                                                                             output_shape=(4,))\n-(X2_train_graph, y2_train_graph), (X2_test_graph, y2_test_graph) = get_test_data(nb_train=100,\n-                                                                                 nb_test=50,\n-                                                                                 input_shape=(32,),\n-                                                                                 classification=False,\n-                                                                                 output_shape=(1,))\n-\n-\n-@keras_test\n-def test_graph_fit_generator():\n-    def data_generator_graph(train):\n-        while 1:\n-            if train:\n-                yield {'input1': X_train_graph, 'output1': y_train_graph}\n-            else:\n-                yield {'input1': X_test_graph, 'output1': y_test_graph}\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1',\n-                     inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-    gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n-\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n-\n-    # test show_accuracy\n-    graph.compile('rmsprop', {'output1': 'mse'}, metrics=['accuracy'])\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n-                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n-    gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n-\n-\n-@keras_test\n-def test_1o_1i():\n-    # test a non-sequential graph with 1 input and 1 output\n-    np.random.seed(1337)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1',\n-                     inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-\n-    assert(type(out) == dict)\n-    assert(len(out) == 1)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n-\n-    # test accuracy:\n-    graph.compile('rmsprop', {'output1': 'mse'}, metrics=['accuracy'])\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=1)\n-    loss, acc = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss, acc = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss, acc = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph}, verbose=0)\n-\n-    # test validation split\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              validation_split=0.2, nb_epoch=1)\n-    # test validation data\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              validation_data={'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=1)\n-\n-\n-@keras_test\n-def test_1o_1i_2():\n-    # test a more complex non-sequential graph with 1 input and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(4), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2-0', input='input1')\n-    graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n-\n-    graph.add_node(Dense(4), name='dense3', input='dense2')\n-    graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n-                   merge_mode='sum')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-              nb_epoch=2)\n-    out = graph.predict({'input1': X_train_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n-\n-    # test serialization\n-    config = graph.get_config()\n-    new_graph = Graph.from_config(config)\n-\n-    graph.summary()\n-    json_str = graph.to_json()\n-    new_graph = model_from_json(json_str)\n-\n-    yaml_str = graph.to_yaml()\n-    new_graph = model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_1o_2i():\n-    # test a non-sequential graph with 2 inputs and 1 output\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input2')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=2)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-\n-    # test serialization\n-    config = graph.get_config()\n-    new_graph = Graph.from_config(config)\n-\n-    graph.summary()\n-    json_str = graph.to_json()\n-    new_graph = model_from_json(json_str)\n-\n-    yaml_str = graph.to_yaml()\n-    new_graph = model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_siamese_1():\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-\n-    graph.add_shared_node(Dense(4), name='shared', inputs=['input1', 'input2'], merge_mode='sum')\n-    graph.add_node(Dense(4), name='dense1', input='shared')\n-    # graph.add_node(Dense(4), name='output1', input='shared', create_output=True)\n-\n-    # graph.add_output(name='output1', inputs=['dense1', 'shared'], merge_mode='sum')\n-    graph.add_output(name='output1', input='dense1')\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n-    assert(loss < 5.0)\n-\n-    # test serialization\n-    config = graph.get_config()\n-    new_graph = Graph.from_config(config)\n-\n-    graph.summary()\n-    json_str = graph.to_json()\n-    new_graph = model_from_json(json_str)\n-\n-    yaml_str = graph.to_yaml()\n-    new_graph = model_from_yaml(yaml_str)\n-\n-\n-'''Th test below is failing because of a known bug\n-with the serialization of legacy Graph models\n-containing shared nodes with named outputs.\n-This is very low priority (= no plans to fix it),\n-since the Graph model is deprecated.\n-'''\n-# def test_siamese_2():\n-#     graph = Graph()\n-#     graph.add_input(name='input1', input_shape=(32,))\n-#     graph.add_input(name='input2', input_shape=(32,))\n-\n-#     graph.add_shared_node(Dense(4), name='shared',\n-#                           inputs=['input1', 'input2'],\n-#                           outputs=['shared_output1', 'shared_output2'])\n-#     graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n-#     graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n-\n-#     graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n-#                      merge_mode='sum')\n-#     graph.compile('rmsprop', {'output1': 'mse'})\n-\n-#     graph.fit({'input1': X_train_graph,\n-#                'input2': X2_train_graph,\n-#                'output1': y_train_graph},\n-#               nb_epoch=10)\n-#     out = graph.predict({'input1': X_test_graph,\n-#                          'input2': X2_test_graph})\n-#     assert(type(out == dict))\n-#     assert(len(out) == 1)\n-\n-#     loss = graph.test_on_batch({'input1': X_test_graph,\n-#                                 'input2': X2_test_graph,\n-#                                 'output1': y_test_graph})\n-#     loss = graph.train_on_batch({'input1': X_test_graph,\n-#                                  'input2': X2_test_graph,\n-#                                  'output1': y_test_graph})\n-#     loss = graph.evaluate({'input1': X_test_graph,\n-#                            'input2': X2_test_graph,\n-#                            'output1': y_test_graph})\n-#     # test serialization\n-#     config = graph.get_config()\n-#     new_graph = Graph.from_config(config)\n-\n-#     graph.summary()\n-#     json_str = graph.to_json()\n-#     new_graph = model_from_json(json_str)\n-\n-#     yaml_str = graph.to_yaml()\n-#     new_graph = model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_2o_1i_save_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-              nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-\n-    # test weight saving\n-    fname = 'test_2o_1i_weights_temp.h5'\n-    graph.save_weights(fname, overwrite=True)\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-    graph.load_weights('test_2o_1i_weights_temp.h5')\n-    os.remove(fname)\n-\n-    nloss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph})\n-    assert(loss == nloss)\n-\n-    # test loss weights\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'},\n-                  loss_weights={'output1': 1., 'output2': 2.})\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-              nb_epoch=1)\n-\n-\n-@keras_test\n-def test_2o_1i_sample_weights():\n-    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-    graph.add_output(name='output1', input='dense2')\n-    graph.add_output(name='output2', input='dense3')\n-\n-    weights1 = np.random.uniform(size=y_train_graph.shape[0])\n-    weights2 = np.random.uniform(size=y2_train_graph.shape[0])\n-    weights1_test = np.random.uniform(size=y_test_graph.shape[0])\n-    weights2_test = np.random.uniform(size=y2_test_graph.shape[0])\n-\n-    graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-              nb_epoch=10,\n-              sample_weight={'output1': weights1, 'output2': weights2})\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 2)\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph, 'output2': y2_test_graph},\n-                               sample_weight={'output1': weights1_test, 'output2': weights2_test})\n-    loss = graph.train_on_batch({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-                                sample_weight={'output1': weights1, 'output2': weights2})\n-    loss = graph.evaluate({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n-                          sample_weight={'output1': weights1, 'output2': weights2})\n-\n-\n-@keras_test\n-def test_recursive():\n-    # test layer-like API\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-\n-    seq = Sequential()\n-    seq.add(Dense(32, input_shape=(32,)))\n-    seq.add(graph)\n-    seq.add(Dense(4))\n-\n-    seq.compile('rmsprop', 'mse')\n-\n-    seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n-    loss = seq.evaluate(X_test_graph, y_test_graph)\n-\n-    # test serialization\n-    config = seq.get_config()\n-    new_graph = Sequential.from_config(config)\n-\n-    seq.summary()\n-    json_str = seq.to_json()\n-    new_graph = model_from_json(json_str)\n-\n-    yaml_str = seq.to_yaml()\n-    new_graph = model_from_yaml(yaml_str)\n-\n-\n-@keras_test\n-def test_create_output():\n-    # test create_output argument\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-\n-    graph.add_node(Dense(16), name='dense1', input='input1')\n-    graph.add_node(Dense(4), name='dense2', input='input1')\n-    graph.add_node(Dense(4), name='dense3', input='dense1')\n-    graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n-                   merge_mode='sum', create_output=True)\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-\n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n-                        nb_epoch=10)\n-    out = graph.predict({'input1': X_test_graph})\n-    assert(type(out == dict))\n-    assert(len(out) == 1)\n-\n-    loss = graph.test_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.train_on_batch({'input1': X_test_graph, 'output1': y_test_graph})\n-    loss = graph.evaluate({'input1': X_test_graph, 'output1': y_test_graph})\n-    assert(loss < 2.5)\n-\n-    # test serialization\n-    config = graph.get_config()\n-    graph = Graph.from_config(config)\n-    graph.compile('rmsprop', {'output1': 'mse'})\n-    out = graph.predict({'input1': X_test_graph})\n-\n-\n-@keras_test\n-def test_count_params():\n-    # test count params\n-    nb_units = 100\n-    nb_classes = 2\n-\n-    graph = Graph()\n-    graph.add_input(name='input1', input_shape=(32,))\n-    graph.add_input(name='input2', input_shape=(32,))\n-    graph.add_node(Dense(nb_units),\n-                   name='dense1', input='input1')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense2', input='input2')\n-    graph.add_node(Dense(nb_classes),\n-                   name='dense3', input='dense1')\n-    graph.add_output(name='output', inputs=['dense2', 'dense3'],\n-                     merge_mode='sum')\n-    graph.build()\n-\n-    n = 32 * nb_units + nb_units\n-    n += 32 * nb_classes + nb_classes\n-    n += nb_units * nb_classes + nb_classes\n-\n-    assert(n == graph.count_params())\n-\n-    graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n-\n-    assert(n == graph.count_params())\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
