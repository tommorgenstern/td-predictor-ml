{"custom_id": "keras#65ce238f03a572f1cad00a998545659fce2e1b83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 192 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,11 @@ for name in sorted(os.listdir(TEXT_DATA_DIR)):\n                     f = open(fpath)\n                 else:\n                     f = open(fpath, encoding='latin-1')\n-                texts.append(f.read())\n+                t = f.read()\n+                i = t.find('\\n\\n')  # skip header\n+                if 0 < i:\n+                    t = t[i:]\n+                texts.append(t)\n                 f.close()\n                 labels.append(label_id)\n \n@@ -141,4 +145,4 @@ model.compile(loss='categorical_crossentropy',\n \n # happy learning!\n model.fit(x_train, y_train, validation_data=(x_val, y_val),\n-          epochs=2, batch_size=128)\n+          epochs=10, batch_size=128)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#77ea18e8f795102da6cee35a716b29c6929574a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 628 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ def load_data(path='imdb.npz', num_words=None, skip_top=0,\n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n     because they're not making the `num_words` cut here.\n-    Words that were not seen in the trining set but are in the test set\n+    Words that were not seen in the training set but are in the test set\n     have simply been skipped.\n     \"\"\"\n     path = get_file(path,\n\n@@ -33,7 +33,7 @@ def load_data(path='reuters.npz', num_words=None, skip_top=0,\n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n     because they're not making the `num_words` cut here.\n-    Words that were not seen in the trining set but are in the test set\n+    Words that were not seen in the training set but are in the test set\n     have simply been skipped.\n     \"\"\"\n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.npz')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e45bce14b7b55e2fef99e1cd18a0a06681e6956b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 63 | Files Changed: 1 | Hunks: 13 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 96 | Churn Cumulative: 6218 | Contributors (this commit): 33 | Commits (past 90d): 19 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -274,6 +274,27 @@ class Recurrent(Layer):\n         else:\n             return last_output\n \n+    def reset_states(self):\n+        if not self.stateful:\n+            raise AttributeError('Layer must be stateful.')\n+        if not self.batch_size:\n+            raise ValueError('If a RNN is stateful, it needs to know '\n+                             'its batch size. Specify the batch size '\n+                             'of your input tensors: \\n'\n+                             '- If using a Sequential model, '\n+                             'specify the batch size by passing '\n+                             'a `batch_input_shape` '\n+                             'argument to your first layer.\\n'\n+                             '- If using the functional API, specify '\n+                             'the time dimension by passing a '\n+                             '`batch_shape` argument to your Input layer.')\n+        if self.states[0] is not None:\n+            for state in self.states:\n+                K.set_value(state, np.zeros((self.batch_size, self.units)))\n+        else:\n+            self.states = [K.zeros((self.batch_size, self.units))\n+                           for _ in self.states]\n+\n     def get_config(self):\n         config = {'return_sequences': self.return_sequences,\n                   'go_backwards': self.go_backwards,\n@@ -372,15 +393,15 @@ class SimpleRNN(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n+        self.input_dim = input_shape[2]\n+\n+        self.states = [None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # Initial states: all-zero tensor of shape (output_dim).\n-            self.states = [None]\n-        input_dim = input_shape[2]\n-        self.input_dim = input_dim\n \n-        self.kernel = self.add_weight((input_dim, self.units),\n+        self.kernel = self.add_weight((self.input_dim, self.units),\n                                       name='kernel',\n                                       initializer=self.kernel_initializer,\n                                       regularizer=self.kernel_regularizer,\n@@ -401,27 +422,6 @@ class SimpleRNN(Recurrent):\n             self.bias = None\n         self.built = True\n \n-    def reset_states(self):\n-        if not self.stateful:\n-            raise AttributeError('Layer must be stateful.')\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, it needs to know '\n-                             'its batch size. Specify the batch size '\n-                             'of your input tensors: \\n'\n-                             '- If using a Sequential model, '\n-                             'specify the batch size by passing '\n-                             'a `batch_input_shape` '\n-                             'argument to your first layer.\\n'\n-                             '- If using the functional API, specify '\n-                             'the time dimension by passing a '\n-                             '`batch_shape` argument to your Input layer.')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation > 0:\n             return inputs\n@@ -607,13 +607,13 @@ class GRU(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n         self.input_dim = input_shape[2]\n \n+        self.states = [None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # initial states: all-zero tensor of shape (output_dim)\n-            self.states = [None]\n \n         self.kernel = self.add_weight((self.input_dim, self.units * 3),\n                                       name='kernel',\n@@ -655,20 +655,6 @@ class GRU(Recurrent):\n             self.bias_h = None\n         self.built = True\n \n-    def reset_states(self):\n-        if not self.stateful:\n-            raise RuntimeError('Layer must be stateful.')\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, a complete '\n-                             'input_shape must be provided '\n-                             '(including batch size).')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation == 0:\n             input_shape = K.int_shape(inputs)\n@@ -896,13 +882,13 @@ class LSTM(Recurrent):\n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n+        self.batch_size = input_shape[0]\n         self.input_dim = input_shape[2]\n \n+        self.states = [None, None]\n         if self.stateful:\n             self.reset_states()\n-        else:\n-            # initial states: 2 all-zero tensors of shape (output_dim)\n-            self.states = [None, None]\n \n         self.kernel = self.add_weight((self.input_dim, self.units * 4),\n                                       name='kernel',\n@@ -950,22 +936,6 @@ class LSTM(Recurrent):\n             self.bias_o = None\n         self.built = True\n \n-    def reset_states(self):\n-        assert self.stateful, 'Layer must be stateful.'\n-        input_shape = self.input_spec.shape\n-        if not input_shape[0]:\n-            raise ValueError('If a RNN is stateful, a complete '\n-                             'input_shape must be provided '\n-                             '(including batch size).')\n-        if hasattr(self, 'states'):\n-            K.set_value(self.states[0],\n-                        np.zeros((input_shape[0], self.units)))\n-            K.set_value(self.states[1],\n-                        np.zeros((input_shape[0], self.units)))\n-        else:\n-            self.states = [K.zeros((input_shape[0], self.units)),\n-                           K.zeros((input_shape[0], self.units))]\n-\n     def preprocess_input(self, inputs, training=None):\n         if self.implementation == 0:\n             input_shape = K.int_shape(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5dbb6121cdb182e047a87f8b4cf99722519d1b74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 19 | Churn Cumulative: 188 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,8 @@ from keras.utils.test_utils import keras_test\n from keras.layers import recurrent\n from keras.layers import embeddings\n from keras.models import Sequential\n+from keras.models import Model\n+from keras.engine.topology import Input\n from keras.layers.core import Masking\n from keras import regularizers\n from keras import backend as K\n@@ -164,6 +166,23 @@ def test_from_config(layer_class):\n         l2 = layer_class.from_config(l1.get_config())\n         assert l1.get_config() == l2.get_config()\n \n+@rnn_test\n+def test_specify_state(layer_class):\n+    states = 2 if layer_class is recurrent.LSTM else 1\n+    inputs = Input((timesteps, embedding_dim))\n+    initial_states = [Input((units,)) for _ in range(states)]\n+    layer = layer_class(units)\n+    output = layer([inputs] + initial_states)\n+    model = Model([inputs] + initial_states, output)\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_states = [np.random.random((num_samples, units))\n+                      for _ in range(states)]\n+    targets = np.random.random((num_samples, units))\n+    model.fit([inputs] + initial_states, targets)\n+\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f94aef6685289c5451f24bcfdb2ab24e2bc87df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 27 | Churn Cumulative: 872 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -172,7 +172,6 @@ def test_conv2d_transpose():\n     num_row = 5\n     num_col = 6\n \n-    for batch_size in [None, num_samples]:\n     for padding in _convolution_paddings:\n         for strides in [(1, 1), (2, 2)]:\n             if padding == 'same' and strides != (1, 1):\n\n@@ -1,12 +1,7 @@\n import pytest\n-import numpy as np\n \n from keras.models import Sequential\n from keras.layers import Dense\n-from keras.layers import Activation\n-from keras.layers import Flatten\n-from keras.layers import ActivityRegularization\n-from keras.layers import Embedding\n from keras.utils import np_utils\n from keras.utils import test_utils\n from keras import regularizers\n@@ -18,11 +13,9 @@ batch_size = 10\n \n \n def get_data():\n-    np.random.seed(1337)\n-\n-    # the data, shuffled and split between tran and test sets\n-    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(num_train=10,\n-                                                                    num_test=10,\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n+        num_train=batch_size,\n+        num_test=batch_size,\n         input_shape=(data_dim,),\n         classification=True,\n         num_classes=num_classes)\n@@ -34,11 +27,10 @@ def get_data():\n \n def create_model(kernel_regularizer=None, activity_regularizer=None):\n     model = Sequential()\n-    model.add(Dense(50, input_shape=(data_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes, kernel_regularizer=kernel_regularizer,\n-                    activity_regularizer=activity_regularizer))\n-    model.add(Activation('softmax'))\n+    model.add(Dense(num_classes,\n+                    kernel_regularizer=kernel_regularizer,\n+                    activity_regularizer=activity_regularizer,\n+                    input_shape=(data_dim,)))\n     return model\n \n \n@@ -48,7 +40,7 @@ def test_kernel_regularization():\n                 regularizers.l2(),\n                 regularizers.l1_l2()]:\n         model = create_model(kernel_regularizer=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n         assert len(model.losses) == 1\n         model.fit(x_train, y_train, batch_size=batch_size,\n                   epochs=epochs, verbose=0)\n@@ -58,7 +50,7 @@ def test_activity_regularization():\n     (x_train, y_train), (x_test, y_test) = get_data()\n     for reg in [regularizers.l1(), regularizers.l2()]:\n         model = create_model(activity_regularizer=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n         assert len(model.losses) == 1\n         model.fit(x_train, y_train, batch_size=batch_size,\n                   epochs=epochs, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d72514d6cca9b09842ab31536c16a01e5cd51f6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 10705 | Contributors (this commit): 37 | Commits (past 90d): 54 | Contributors (cumulative): 40 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -702,7 +702,7 @@ class Conv2DTranspose(Conv2D):\n         if len(input_shape) != 4:\n             raise ValueError('Inputs should have rank ' +\n                              str(4) +\n-                             'Received input shape:', str(input_shape))\n+                             '; Received input shape:', str(input_shape))\n         if self.data_format == 'channels_first':\n             channel_axis = 1\n         else:\n\n@@ -318,6 +318,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -530,6 +531,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n                   'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                   'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n\n@@ -91,6 +91,7 @@ def test_convolutional_recurrent():\n                       'kernel_regularizer': regularizers.L1L2(l1=0.01),\n                       'recurrent_regularizer': regularizers.L1L2(l1=0.01),\n                       'bias_regularizer': 'l2',\n+                      'activity_regularizer': 'l2',\n                       'kernel_constraint': 'max_norm',\n                       'recurrent_constraint': 'max_norm',\n                       'bias_constraint': 'max_norm',\n@@ -99,8 +100,9 @@ def test_convolutional_recurrent():\n             layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n             layer.build(inputs.shape)\n             assert len(layer.losses) == 3\n+            assert layer.activity_regularizer\n             output = layer(K.variable(np.ones(inputs.shape)))\n-            assert len(layer.losses) == 3\n+            assert len(layer.losses) == 4\n             K.eval(output)\n \n             # check dropout\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9202817f3e2f65c83910015552da88969b44dbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 18 | Churn Cumulative: 6534 | Contributors (this commit): 36 | Commits (past 90d): 33 | Contributors (cumulative): 40 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -1,4 +1,7 @@\n from __future__ import absolute_import\r\n+\r\n+from . import activations\r\n+from . import applications\r\n from . import backend\r\n from . import datasets\r\n from . import engine\r\n\n@@ -345,6 +345,7 @@ class SimpleRNN(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -487,6 +488,7 @@ class SimpleRNN(Recurrent):\n                   'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                   'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n@@ -580,6 +582,7 @@ class GRU(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -766,6 +769,7 @@ class GRU(Recurrent):\n                   'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                   'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n@@ -869,6 +873,7 @@ class LSTM(Recurrent):\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n         self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n \n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.recurrent_constraint = constraints.get(recurrent_constraint)\n@@ -1063,6 +1068,7 @@ class LSTM(Recurrent):\n                   'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                   'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n\n@@ -1,9 +1,6 @@\n \"\"\"Utilities related to model visualization.\"\"\"\n import os\n \n-from ..layers.wrappers import Wrapper\n-from ..models import Sequential\n-\n try:\n     # pydot-ng is a fork of pydot that is better maintained.\n     import pydot_ng as pydot\n@@ -32,6 +29,9 @@ def model_to_dot(model, show_shapes=False, show_layer_names=True):\n     # Returns\n         A `pydot.Dot` instance representing the Keras model.\n     \"\"\"\n+    from ..layers.wrappers import Wrapper\n+    from ..models import Sequential\n+\n     _check_pydot()\n     dot = pydot.Dot()\n     dot.set('rankdir', 'TB')\n\n@@ -128,11 +128,12 @@ def test_regularizer(layer_class):\n                         batch_input_shape=(num_samples, timesteps, embedding_dim),\n                         kernel_regularizer=regularizers.l1(0.01),\n                         recurrent_regularizer=regularizers.l1(0.01),\n+                        activity_regularizer='l1',\n                         bias_regularizer='l2')\n     layer.build((None, None, 2))\n     assert len(layer.losses) == 3\n     layer(K.variable(np.ones((2, 3, 2))))\n-    assert len(layer.losses) == 3\n+    assert len(layer.losses) == 4\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5516c8fb42f01622c9a4a31c19cb46ae68cfa0f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 110 | Churn Cumulative: 12262 | Contributors (this commit): 59 | Commits (past 90d): 31 | Contributors (cumulative): 61 | DMM Complexity: 0.4084507042253521\n\nDIFF:\n@@ -19,6 +19,7 @@ from ..engine import Layer\n from ..utils.generic_utils import func_dump\n from ..utils.generic_utils import func_load\n from ..utils.generic_utils import deserialize_keras_object\n+from ..legacy import interfaces\n \n \n class Masking(Layer):\n@@ -783,6 +784,7 @@ class Dense(Layer):\n         the output would have shape `(batch_size, units)`.\n     \"\"\"\n \n+    @interfaces.legacy_dense_support\n     def __init__(self, units,\n                  activation=None,\n                  use_bias=True,\n\n@@ -0,0 +1,76 @@\n+\"\"\"Interface converters for Keras 1 support in Keras 2.\n+\"\"\"\n+import six\n+import warnings\n+\n+\n+def raise_duplicate_arg_error(old_arg, new_arg):\n+    raise TypeError('For the `' + new_arg + '` argument, '\n+                    'the layer received both '\n+                    'the legacy keyword argument '\n+                    '`' + old_arg + '` and the Keras 2 keyword argument '\n+                    '`' + new_arg + '`. Stick with the latter!')\n+\n+\n+def legacy_dense_support(func):\n+    \"\"\"Function wrapper to convert the `Dense` constructor from Keras 1 to 2.\n+\n+    # Arguments\n+        func: `__init__` method of `Dense`.\n+\n+    # Returns\n+        A constructor conversion wrapper.\n+    \"\"\"\n+    @six.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        if len(args) > 2:\n+            # The first entry in `args` is `self`.\n+            raise TypeError('The `Dense` layer can have at most '\n+                            'one positional argument (the `units` argument).')\n+\n+        # output_dim\n+        if 'output_dim' in kwargs:\n+            if len(args) > 1:\n+                raise TypeError('Got both a positional argument '\n+                                'and keyword argument for argument '\n+                                '`units` '\n+                                '(`output_dim` in the legacy interface).')\n+            if 'units' in kwargs:\n+                raise_duplicate_arg_error('output_dim', 'units')\n+            output_dim = kwargs.pop('output_dim')\n+            args = (args[0], output_dim)\n+\n+        converted = []\n+\n+        # Remaining kwargs.\n+        conversions = [\n+            ('init', 'kernel_initializer'),\n+            ('W_regularizer', 'kernel_regularizer'),\n+            ('b_regularizer', 'bias_regularizer'),\n+            ('W_constraint', 'kernel_constraint'),\n+            ('b_constraint', 'bias_constraint'),\n+            ('bias', 'use_bias'),\n+        ]\n+\n+        for old_arg, new_arg in conversions:\n+            if old_arg in kwargs:\n+                if new_arg in kwargs:\n+                    raise_duplicate_arg_error(old_arg, new_arg)\n+                arg_value = kwargs.pop(old_arg)\n+                kwargs[new_arg] = arg_value\n+                converted.append((new_arg, arg_value))\n+\n+        if converted:\n+            signature = '`Dense(' + str(args[1])\n+            for name, value in converted:\n+                signature += ', ' + name + '='\n+                if isinstance(value, six.string_types):\n+                    signature += ('\"' + value + '\"')\n+                else:\n+                    signature += str(value)\n+            signature += ')`'\n+            warnings.warn('Update your `Dense` layer call '\n+                          'to the Keras 2 API: ' + signature)\n+\n+        return func(*args, **kwargs)\n+    return wrapper\n\n@@ -0,0 +1,32 @@\n+import pytest\n+import json\n+from keras.utils.test_utils import keras_test\n+import keras\n+\n+\n+@keras_test\n+def test_dense_legacy_interface():\n+    old_layer = keras.layers.Dense(input_dim=3, output_dim=2, name='d')\n+    new_layer = keras.layers.Dense(2, input_shape=(3,), name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.Dense(2, bias=False, init='normal',\n+                                   W_regularizer='l1',\n+                                   W_constraint='max_norm', name='d')\n+    new_layer = keras.layers.Dense(2, use_bias=False,\n+                                   kernel_initializer='normal',\n+                                   kernel_regularizer='l1',\n+                                   kernel_constraint='max_norm', name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.Dense(2, bias=True,\n+                                   b_regularizer='l1',\n+                                   b_constraint='max_norm', name='d')\n+    new_layer = keras.layers.Dense(2, use_bias=True,\n+                                   bias_regularizer='l1',\n+                                   bias_constraint='max_norm', name='d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b660145a7c83385bae682e072341c1b5709dbee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4613 | Contributors (this commit): 57 | Commits (past 90d): 51 | Contributors (cumulative): 57 | DMM Complexity: 1.0\n\nDIFF:\n@@ -270,7 +270,9 @@ def count_params(x):\n \n     Return: numpy integer.\n     \"\"\"\n-    return np.prod(x.shape.eval())\n+    # We don't want those compilation to show up in Theano profiler.\n+    f = theano.function([], x.shape, profile=False)\n+    return np.prod(f())\n \n \n def cast(x, dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e99eac292db998960b9c384cb1106998a8489b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 27 | Methods Changed: 17 | Complexity Δ (Sum/Max): 19/15 | Churn Δ: 102 | Churn Cumulative: 4719 | Contributors (this commit): 57 | Commits (past 90d): 55 | Contributors (cumulative): 59 | DMM Complexity: 0.6428571428571429\n\nDIFF:\n@@ -352,7 +352,6 @@ def batch_dot(x, y, axes=None):\n \n         output_shape = (100, 30)\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     if isinstance(axes, int):\n         axes = (axes, axes)\n     if axes is None:\n@@ -361,12 +360,26 @@ def batch_dot(x, y, axes=None):\n     out = T.batched_tensordot(x, y, axes=axes)\n     if ndim(out) == 1:\n         out = expand_dims(out, 1)\n+\n+    if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):\n+        shape = []\n+        for axis in range(len(x._keras_shape)):\n+            if axis != axes[0]:\n+                shape.append(x._keras_shape[axis])\n+        for axis in range(1, len(y._keras_shape)):\n+            if axis != axes[1]:\n+                shape.append(y._keras_shape[axis])\n+        if len(shape) == 1:\n+            shape.append(1)     # Expand dims if ndim == 1\n+        out._keras_shape = tuple(shape)\n     return out\n \n \n def transpose(x):\n-    # TODO: `keras_shape` inference.\n-    return T.transpose(x)\n+    y = T.transpose(x)\n+    if hasattr(x, '_keras_shape'):\n+        y._keras_shape = tuple(reversed(x._keras_shape))\n+    return y\n \n \n def gather(reference, indices):\n@@ -673,9 +686,11 @@ def permute_dimensions(x, pattern):\n     pattern should be a tuple or list of\n     dimension indices, e.g. [0, 2, 1].\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     pattern = tuple(pattern)\n-    return x.dimshuffle(pattern)\n+    y = x.dimshuffle(pattern)\n+    if hasattr(x, '_keras_shape'):\n+        y._keras_shape = tuple(np.asarray(x._keras_shape)[list(pattern)])\n+    return y\n \n \n def repeat_elements(x, rep, axis):\n@@ -684,8 +699,12 @@ def repeat_elements(x, rep, axis):\n     If x has shape (s1, s2, s3) and axis=1, the output\n     will have shape (s1, s2 * rep, s3).\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n-    return T.repeat(x, rep, axis=axis)\n+    y = T.repeat(x, rep, axis=axis)\n+    if hasattr(x, '_keras_shape'):\n+        y._keras_shape = list(x._keras_shape)\n+        y._keras_shape[axis] = x._keras_shape[axis] * rep\n+        y._keras_shape = tuple(y._keras_shape)\n+    return y\n \n \n def resize_images(X, height_factor, width_factor, data_format):\n@@ -695,7 +714,6 @@ def resize_images(X, height_factor, width_factor, data_format):\n     by a factor of (height_factor, width_factor). Both factors should be\n     positive integers.\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     if data_format == 'channels_first':\n         output = repeat_elements(X, height_factor, axis=2)\n         output = repeat_elements(output, width_factor, axis=3)\n@@ -715,7 +733,6 @@ def resize_volumes(X, depth_factor, height_factor, width_factor, data_format):\n     by a factor of (depth_factor, height_factor, width_factor).\n     Both factors should be positive integers.\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     if data_format == 'channels_first':\n         output = repeat_elements(X, depth_factor, axis=2)\n         output = repeat_elements(output, height_factor, axis=3)\n@@ -736,10 +753,15 @@ def repeat(x, n):\n     If x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim).\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     assert x.ndim == 2\n-    x = x.dimshuffle((0, 'x', 1))\n-    return T.extra_ops.repeat(x, n, axis=1)\n+    y = x.dimshuffle((0, 'x', 1))\n+    y = T.extra_ops.repeat(y, n, axis=1)\n+    if hasattr(x, '_keras_shape'):\n+        shape = list(x._keras_shape)\n+        shape.insert(1, n)\n+        y._keras_shape = tuple(shape)\n+\n+    return y\n \n \n def arange(start, stop=None, step=1, dtype='int32'):\n@@ -761,23 +783,25 @@ def tile(x, n):\n \n \n def flatten(x):\n-    # TODO: `keras_shape` inference.\n-    return T.flatten(x)\n+    y = T.flatten(x)\n+    if hasattr(x, '_keras_shape'):\n+        y._keras_shape = (np.prod(x._keras_shape), )\n+    return y\n \n \n def batch_flatten(x):\n     \"\"\"Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n-    x = T.reshape(x, (x.shape[0], T.prod(x.shape) // x.shape[0]))\n-    return x\n+    y = T.reshape(x, (x.shape[0], T.prod(x.shape[1:])))\n+    if hasattr(x, '_keras_shape'):\n+        y._keras_shape = (x._keras_shape[0], np.prod(x._keras_shape[1:]))\n+    return y\n \n \n def expand_dims(x, axis=-1):\n     \"\"\"Add a 1-sized dimension at index \"dim\".\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     pattern = [i for i in range(x.type.ndim)]\n     if axis < 0:\n         if x.type.ndim == 0:\n@@ -785,16 +809,25 @@ def expand_dims(x, axis=-1):\n         else:\n             axis = axis % x.type.ndim + 1\n     pattern.insert(axis, 'x')\n-    return x.dimshuffle(pattern)\n+    y = x.dimshuffle(pattern)\n+    if hasattr(x, '_keras_shape'):\n+        shape = list(x._keras_shape)\n+        shape.insert(axis, 1)\n+        y._keras_shape = tuple(shape)\n+    return y\n \n \n def squeeze(x, axis):\n     \"\"\"Remove a 1-dimension from the tensor at index \"axis\".\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     shape = list(x.shape)\n     shape.pop(axis)\n-    return T.reshape(x, tuple(shape))\n+    y = T.reshape(x, tuple(shape))\n+    if hasattr(x, '_keras_shape'):\n+        kshape = list(x._keras_shape)\n+        kshape.pop(axis)\n+        y._keras_shape = tuple(kshape)\n+    return y\n \n \n def temporal_padding(x, padding=(1, 1)):\n@@ -822,7 +855,6 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     \"\"\"Pad the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n     \"\"\"\n-    # TODO: `keras_shape` inference.\n     assert len(padding) == 2\n     assert len(padding[0]) == 2\n     assert len(padding[1]) == 2\n@@ -857,7 +889,9 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n                    slice(None))\n     else:\n         raise ValueError('Invalid data_format:', data_format)\n-    return T.set_subtensor(output[indices], x)\n+    y = T.set_subtensor(output[indices], x)\n+    y._keras_shape = output_shape\n+    return y\n \n \n def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n\n@@ -21,11 +21,14 @@ def check_single_tensor_operation(function_name, input_shape, **kwargs):\n     xth = KTH.variable(val)\n     xtf = KTF.variable(val)\n \n-    zth = KTH.eval(getattr(KTH, function_name)(xth, **kwargs))\n+    _zth = getattr(KTH, function_name)(xth, **kwargs)\n+    zth = KTH.eval(_zth)\n     ztf = KTF.eval(getattr(KTF, function_name)(xtf, **kwargs))\n \n     assert zth.shape == ztf.shape\n     assert_allclose(zth, ztf, atol=1e-05)\n+    if hasattr(_zth, '_keras_shape'):\n+        assert _zth._keras_shape == zth.shape\n \n \n def check_two_tensor_operation(function_name, x_input_shape,\n@@ -40,11 +43,14 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     yth = KTH.variable(yval)\n     ytf = KTF.variable(yval)\n \n-    zth = KTH.eval(getattr(KTH, function_name)(xth, yth, **kwargs))\n+    _zth = getattr(KTH, function_name)(xth, yth, **kwargs)\n+    zth = KTH.eval(_zth)\n     ztf = KTF.eval(getattr(KTF, function_name)(xtf, ytf, **kwargs))\n \n     assert zth.shape == ztf.shape\n     assert_allclose(zth, ztf, atol=1e-05)\n+    if hasattr(_zth, '_keras_shape'):\n+        assert _zth._keras_shape == zth.shape\n \n \n def check_composed_tensor_operations(first_function_name, first_function_args,\n@@ -116,6 +122,7 @@ class TestBackend(object):\n                                       pattern=(2, 0, 1))\n         check_single_tensor_operation('repeat', (4, 1), n=3)\n         check_single_tensor_operation('flatten', (4, 1))\n+        check_single_tensor_operation('batch_flatten', (20, 2, 5))\n         check_single_tensor_operation('expand_dims', (4, 3), axis=-1)\n         check_single_tensor_operation('expand_dims', (4, 3, 2), axis=1)\n         check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n@@ -134,8 +141,8 @@ class TestBackend(object):\n \n             for rep_axis in range(ndims):\n                 np_rep = np.repeat(arr, reps, axis=rep_axis)\n-                th_rep = KTH.eval(\n-                    KTH.repeat_elements(arr_th, reps, axis=rep_axis))\n+                th_z = KTH.repeat_elements(arr_th, reps, axis=rep_axis)\n+                th_rep = KTH.eval(th_z)\n                 tf_rep = KTF.eval(\n                     KTF.repeat_elements(arr_tf, reps, axis=rep_axis))\n \n@@ -143,6 +150,8 @@ class TestBackend(object):\n                 assert tf_rep.shape == np_rep.shape\n                 assert_allclose(np_rep, th_rep, atol=1e-05)\n                 assert_allclose(np_rep, tf_rep, atol=1e-05)\n+                if hasattr(th_z, '_keras_shape'):\n+                    assert th_z._keras_shape == th_rep.shape\n \n     def test_tile(self):\n         shape = (3, 4)\n@@ -151,9 +160,12 @@ class TestBackend(object):\n         arr_tf = KTF.variable(arr)\n \n         n = (2, 1)\n-        th_rep = KTH.eval(KTH.tile(arr_th, n))\n+        th_z = KTH.tile(arr_th, n)\n+        th_rep = KTH.eval(th_z)\n         tf_rep = KTF.eval(KTF.tile(arr_tf, n))\n         assert_allclose(tf_rep, th_rep, atol=1e-05)\n+        if hasattr(th_z, '_keras_shape'):\n+            assert th_z._keras_shape == th_rep.shape\n \n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adb15756ff44b56ba12620ee7ca3bbf330a78d08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1890 | Contributors (this commit): 19 | Commits (past 90d): 17 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -644,6 +644,8 @@ def deserialize(config, custom_objects=None):\n         'nadam': Nadam,\n         'tfoptimizer': TFOptimizer,\n     }\n+    # Make deserialization case-insensitive for built-in optimizers.\n+    if config['class_name'].lower() in all_classes:\n         config['class_name'] = config['class_name'].lower()\n     return deserialize_keras_object(config,\n                                     module_objects=all_classes,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e91dc4284285d482d7e91ca33c56d6c1904274e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 12099 | Contributors (this commit): 82 | Commits (past 90d): 118 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -1356,7 +1356,7 @@ def greater_equal(x, y):\n     return tf.greater_equal(x, y)\n \n \n-def lesser(x, y):\n+def less(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n \n     # Arguments\n@@ -1369,7 +1369,7 @@ def lesser(x, y):\n     return tf.less(x, y)\n \n \n-def lesser_equal(x, y):\n+def less_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n \n     # Arguments\n\n@@ -510,11 +510,11 @@ def greater_equal(x, y):\n     return T.ge(x, y)\n \n \n-def lesser(x, y):\n+def less(x, y):\n     return T.lt(x, y)\n \n \n-def lesser_equal(x, y):\n+def less_equal(x, y):\n     return T.le(x, y)\n \n \n\n@@ -252,8 +252,8 @@ class TestBackend(object):\n         check_two_tensor_operation('not_equal', (4, 2), (4, 2))\n         check_two_tensor_operation('greater', (4, 2), (4, 2))\n         check_two_tensor_operation('greater_equal', (4, 2), (4, 2))\n-        check_two_tensor_operation('lesser', (4, 2), (4, 2))\n-        check_two_tensor_operation('lesser_equal', (4, 2), (4, 2))\n+        check_two_tensor_operation('less', (4, 2), (4, 2))\n+        check_two_tensor_operation('less_equal', (4, 2), (4, 2))\n         check_two_tensor_operation('maximum', (4, 2), (4, 2))\n         check_two_tensor_operation('minimum', (4, 2), (4, 2))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1606dc6a110ea49e9ea095ff88d684f87e398aa7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 51 | Churn Cumulative: 12313 | Contributors (this commit): 60 | Commits (past 90d): 34 | Contributors (cumulative): 64 | DMM Complexity: 0.29411764705882354\n\nDIFF:\n@@ -89,7 +89,7 @@ class Dropout(Layer):\n     # References\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n-\n+    @interfaces.legacy_dropout_support\n     def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n         super(Dropout, self).__init__(**kwargs)\n         self.rate = min(1., max(0., rate))\n\n@@ -74,3 +74,44 @@ def legacy_dense_support(func):\n \n         return func(*args, **kwargs)\n     return wrapper\n+\n+\n+def legacy_dropout_support(func):\n+    \"\"\"Function wrapper to convert the `Dropout` constructor from Keras 1 to 2.\n+\n+    # Arguments\n+        func: `__init__` method of `Dropout`.\n+\n+    # Returns\n+        A constructor conversion wrapper.\n+    \"\"\"\n+    @six.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        if len(args) > 2:\n+            # The first entry in `args` is `self`.\n+            raise TypeError('The `Dropout` layer can have at most '\n+                            'one positional argument (the `rate` argument).')\n+\n+        # Convert `p` to `rate` if keyword arguement is used\n+        if 'p' in kwargs:\n+            if len(args) > 1:\n+                raise TypeError('Got both a positional argument '\n+                                'and keyword argument for argument '\n+                                '`rate` '\n+                                '(`p` in the legacy interface).')\n+            if 'rate' in kwargs:\n+                raise_duplicate_arg_error('p', 'rate')\n+            rate = kwargs.pop('p')\n+            args = (args[0], rate)\n+            signature = '`Dropout(rate=' + str(args[1])\n+            for kwarg in kwargs:\n+                signature += ', ' + kwarg + \"=\"\n+                if isinstance(kwargs[kwarg], six.string_types):\n+                    signature += ('\"' + kwargs[kwarg] + '\"')\n+                else:\n+                    signature += str(kwargs[kwarg])\n+            signature += ')`'\n+            warnings.warn('Update your `Dropout` layer call to Keras 2 API: ' + signature)\n+\n+        return func(*args, **kwargs)\n+    return wrapper\n\n@@ -28,5 +28,13 @@ def test_dense_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n+@keras_test\n+def test_dropout_legacy_interface():\n+    old_layer = keras.layers.Dropout(p=3, name='drop')\n+    new_layer_1 = keras.layers.Dropout(rate=3, name='drop')\n+    new_layer_2 = keras.layers.Dropout(3, name='drop')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19bef655af3649ae4fba8632097636e87735fab4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 70 | Churn Cumulative: 187 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,42 @@ def raise_duplicate_arg_error(old_arg, new_arg):\n                     '`' + new_arg + '`. Stick with the latter!')\n \n \n+def convert_legacy_kwargs(layer_name,\n+                          args,\n+                          kwargs,\n+                          conversions,\n+                          converted=None):\n+    converted = converted or []\n+    for old_arg, new_arg in conversions:\n+        if old_arg in kwargs:\n+            if new_arg in kwargs:\n+                raise_duplicate_arg_error(old_arg, new_arg)\n+            arg_value = kwargs.pop(old_arg)\n+            kwargs[new_arg] = arg_value\n+            converted.append((new_arg, arg_value))\n+\n+    if converted:\n+        signature = '`' + layer_name + '('\n+        for value in args:\n+            if isinstance(value, six.string_types):\n+                signature += '\"' + value + '\"'\n+            else:\n+                signature += str(value)\n+            signature += ', '\n+        for i, (name, value) in enumerate(converted):\n+            signature += name + '='\n+            if isinstance(value, six.string_types):\n+                signature += '\"' + value + '\"'\n+            else:\n+                signature += str(value)\n+            if i < len(converted) - 1:\n+                signature += ', '\n+        signature += ')`'\n+        warnings.warn('Update your `' + layer_name + '` layer call '\n+                      'to the Keras 2 API: ' + signature)\n+    return kwargs\n+\n+\n def legacy_dense_support(func):\n     \"\"\"Function wrapper to convert the `Dense` constructor from Keras 1 to 2.\n \n@@ -40,8 +76,6 @@ def legacy_dense_support(func):\n             output_dim = kwargs.pop('output_dim')\n             args = (args[0], output_dim)\n \n-        converted = []\n-\n         # Remaining kwargs.\n         conversions = [\n             ('init', 'kernel_initializer'),\n@@ -51,27 +85,10 @@ def legacy_dense_support(func):\n             ('b_constraint', 'bias_constraint'),\n             ('bias', 'use_bias'),\n         ]\n-\n-        for old_arg, new_arg in conversions:\n-            if old_arg in kwargs:\n-                if new_arg in kwargs:\n-                    raise_duplicate_arg_error(old_arg, new_arg)\n-                arg_value = kwargs.pop(old_arg)\n-                kwargs[new_arg] = arg_value\n-                converted.append((new_arg, arg_value))\n-\n-        if converted:\n-            signature = '`Dense(' + str(args[1])\n-            for name, value in converted:\n-                signature += ', ' + name + '='\n-                if isinstance(value, six.string_types):\n-                    signature += ('\"' + value + '\"')\n-                else:\n-                    signature += str(value)\n-            signature += ')`'\n-            warnings.warn('Update your `Dense` layer call '\n-                          'to the Keras 2 API: ' + signature)\n-\n+        kwargs = convert_legacy_kwargs('Dense',\n+                                       args,\n+                                       kwargs,\n+                                       conversions)\n         return func(*args, **kwargs)\n     return wrapper\n \n@@ -103,15 +120,16 @@ def legacy_dropout_support(func):\n                 raise_duplicate_arg_error('p', 'rate')\n             rate = kwargs.pop('p')\n             args = (args[0], rate)\n-            signature = '`Dropout(rate=' + str(args[1])\n+            signature = '`Dropout(' + str(args[1])\n             for kwarg in kwargs:\n-                signature += ', ' + kwarg + \"=\"\n+                signature += ', ' + kwarg + '='\n                 if isinstance(kwargs[kwarg], six.string_types):\n                     signature += ('\"' + kwargs[kwarg] + '\"')\n                 else:\n                     signature += str(kwargs[kwarg])\n             signature += ')`'\n-            warnings.warn('Update your `Dropout` layer call to Keras 2 API: ' + signature)\n+            warnings.warn('Update your `Dropout` layer call '\n+                          'to the Keras 2 API: ' + signature)\n \n         return func(*args, **kwargs)\n     return wrapper\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
