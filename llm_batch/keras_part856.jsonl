{"custom_id": "keras#cfa32a3d3ccf1cef0a454362287f6f9fe00a4058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 75 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 81 | Churn Cumulative: 2271 | Contributors (this commit): 30 | Commits (past 90d): 2 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,22 +29,20 @@ import torch  # noqa: F401\n package = \"keras\"\n build_directory = \"tmp_build_dir\"\n dist_directory = \"dist\"\n-to_copy = [\"setup.py\", \"README.md\"]\n+to_copy = [\"pyproject.toml\", \"README.md\"]\n \n \n def export_version_string(version, is_nightly=False, rc_index=None):\n     \"\"\"Export Version and Package Name.\"\"\"\n     if is_nightly:\n         date = datetime.datetime.now()\n-        version += f\".dev{date.strftime('%Y%m%d%H')}\"\n-        # Replaces `name=\"keras\"` string in `setup.py` with `keras-nightly`\n-        with open(\"setup.py\") as f:\n-            setup_contents = f.read()\n-        with open(\"setup.py\", \"w\") as f:\n-            setup_contents = setup_contents.replace(\n+        version += f\".dev{date:%Y%m%d%H}\"\n+        # Update `name = \"keras\"` with \"keras-nightly\"\n+        pyproj_pth = pathlib.Path(\"pyproject.toml\")\n+        pyproj_str = pyproj_pth.read_text().replace(\n             'name = \"keras\"', 'name = \"keras-nightly\"'\n         )\n-            f.write(setup_contents)\n+        pyproj_pth.write_text(pyproj_str)\n     elif rc_index is not None:\n         version += \"rc\" + str(rc_index)\n \n\n@@ -1,67 +0,0 @@\n-\"\"\"Setup script.\"\"\"\n-\n-import os\n-import pathlib\n-\n-from setuptools import find_packages\n-from setuptools import setup\n-\n-\n-def read(rel_path):\n-    here = os.path.abspath(os.path.dirname(__file__))\n-    with open(os.path.join(here, rel_path)) as fp:\n-        return fp.read()\n-\n-\n-def get_version(rel_path):\n-    for line in read(rel_path).splitlines():\n-        if line.startswith(\"__version__\"):\n-            delim = '\"' if '\"' in line else \"'\"\n-            return line.split(delim)[1]\n-    raise RuntimeError(\"Unable to find version string.\")\n-\n-\n-HERE = pathlib.Path(__file__).parent\n-README = (HERE / \"README.md\").read_text()\n-VERSION = get_version(\"keras/src/version.py\")\n-\n-setup(\n-    name=\"keras\",\n-    description=\"Multi-backend Keras.\",\n-    long_description_content_type=\"text/markdown\",\n-    long_description=README,\n-    version=VERSION,\n-    url=\"https://github.com/keras-team/keras\",\n-    author=\"Keras team\",\n-    author_email=\"keras-users@googlegroups.com\",\n-    license=\"Apache License 2.0\",\n-    install_requires=[\n-        \"absl-py\",\n-        \"numpy\",\n-        \"rich\",\n-        \"namex\",\n-        \"h5py\",\n-        \"optree\",\n-        \"ml-dtypes\",\n-        \"packaging\",\n-    ],\n-    # Supported Python versions\n-    python_requires=\">=3.9\",\n-    classifiers=[\n-        \"Development Status :: 4 - Beta\",\n-        \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.9\",\n-        \"Programming Language :: Python :: 3.10\",\n-        \"Programming Language :: Python :: 3.11\",\n-        \"Programming Language :: Python :: 3 :: Only\",\n-        \"Operating System :: Unix\",\n-        \"Operating System :: MacOS\",\n-        \"Intended Audience :: Science/Research\",\n-        \"Topic :: Scientific/Engineering\",\n-        \"Topic :: Software Development\",\n-    ],\n-    packages=find_packages(\n-        include=(\"keras\", \"keras.*\"),\n-        exclude=(\"*_test.py\", \"benchmarks\"),\n-    ),\n-)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#392340095855f3d1b28db4ebe06743683943df96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 40 | Churn Cumulative: 179 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.9615384615384616\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import copy\n+\n from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.layers.merging.base_merge import Merge\n@@ -50,15 +52,15 @@ class Concatenate(Merge):\n             return\n \n         reduced_inputs_shapes = [list(shape) for shape in input_shape]\n+        reduced_inputs_shapes_copy = copy.copy(reduced_inputs_shapes)\n         shape_set = set()\n-\n-        for i in range(len(reduced_inputs_shapes)):\n+        for i in range(len(reduced_inputs_shapes_copy)):\n             # Convert self.axis to positive axis for each input\n             # in case self.axis is a negative number\n-            concat_axis = self.axis % len(reduced_inputs_shapes[i])\n+            concat_axis = self.axis % len(reduced_inputs_shapes_copy[i])\n             #  Skip batch axis.\n             for axis, axis_value in enumerate(\n-                reduced_inputs_shapes[i][1:], start=1\n+                reduced_inputs_shapes_copy, start=1\n             ):\n                 # Remove squeezable axes (axes with value of 1)\n                 # if not in the axis that will be used for concatenation\n\n@@ -5,6 +5,7 @@ from absl.testing import parameterized\n from keras.src import backend\n from keras.src import layers\n from keras.src import models\n+from keras.src import ops\n from keras.src import testing\n \n \n@@ -339,6 +340,35 @@ class MergingLayersTest(testing.TestCase):\n         )\n         self.assertAllClose(output._keras_mask, [[1, 1, 1, 1]])\n \n+    def test_concatenate_errors(self):\n+        # This should work\n+        x1 = np.ones((1, 1, 1, 1, 5))\n+        x2 = np.ones((1, 1, 1, 1, 4))\n+        out = layers.Concatenate(axis=-1)([x1, x2])\n+        self.assertEqual(ops.shape(out), (1, 1, 1, 1, 9))\n+\n+        # This won't\n+        x1 = np.ones((1, 2, 1, 1, 5))\n+        x2 = np.ones((1, 1, 1, 1, 4))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            (\n+                \"requires inputs with matching shapes \"\n+                \"except for the concatenation axis\"\n+            ),\n+        ):\n+            out = layers.Concatenate(axis=-1)([x1, x2])\n+        x1 = np.ones((1, 2, 1, 2, 1))\n+        x2 = np.ones((1, 1, 1, 3, 1))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            (\n+                \"requires inputs with matching shapes \"\n+                \"except for the concatenation axis\"\n+            ),\n+        ):\n+            out = layers.Concatenate(axis=1)([x1, x2])\n+\n     @parameterized.named_parameters(TEST_PARAMETERS)\n     @pytest.mark.skipif(\n         not backend.SUPPORTS_SPARSE_TENSORS,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc1b52e79d8c2284dfb4521762d6065147bfb7e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 38 | Files Changed: 21 | Hunks: 34 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 77 | Churn Cumulative: 7480 | Contributors (this commit): 24 | Commits (past 90d): 94 | Contributors (cumulative): 112 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ import matplotlib.pyplot as plt\n \n **Transfer learning** consists of taking features learned on one problem, and\n leveraging them on a new, similar problem. For instance, features from a model that has\n-learned to identify racoons may be useful to kick-start a model meant to identify\n+learned to identify raccoons may be useful to kick-start a model meant to identify\n  tanukis.\n \n Transfer learning is usually done for tasks where your dataset has too little data to\n\n@@ -357,7 +357,7 @@ def ConvNeXt(\n             won't be used.\n         default_size: Default input image size.\n         name: An optional name for the model.\n-        include_preprocessing: boolean denoting whther to\n+        include_preprocessing: boolean denoting whether to\n             include preprocessing in the model.\n             When `weights=\"imagenet\"` this should always be `True`.\n             But for other models (e.g., randomly initialized) you should set it\n\n@@ -614,7 +614,7 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     batch_size, max_label_length = target.shape\n     log_epsilon = -1e5\n \n-    # Ensure that the dtype promotion behavior matchs that of `tf.nn.ctc_loss`\n+    # Ensure that the dtype promotion behavior matches that of `tf.nn.ctc_loss`\n     dtype = backend.result_type(output.dtype, \"float32\")\n     output = cast(output, dtype)\n \n\n@@ -647,7 +647,7 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     batch_size, max_label_length = target.shape\n     log_epsilon = -1e5\n \n-    # Ensure that the dtype promotion behavior matchs that of `tf.nn.ctc_loss`\n+    # Ensure that the dtype promotion behavior matches that of `tf.nn.ctc_loss`\n     dtype = backend.result_type(output.dtype, \"float32\")\n     output = output.astype(dtype)\n \n\n@@ -296,7 +296,7 @@ def norm(x, ord=None, axis=None, keepdims=False):\n         dtype = dtypes.result_type(x.dtype, float)\n     x = cast(x, dtype)\n \n-    # Fast path to utilze `tf.linalg.norm`\n+    # Fast path to utilize `tf.linalg.norm`\n     if (num_axes == 1 and ord in (\"euclidean\", 1, 2, float(\"inf\"))) or (\n         num_axes == 2 and ord in (\"euclidean\", \"fro\", 1, 2, float(\"inf\"))\n     ):\n\n@@ -323,7 +323,7 @@ def istft(\n             )\n \n     if sequence_length == fft_length and center is True and win is not None:\n-        # can be falled back to torch.istft\n+        # can be fallen back to torch.istft\n         need_unpack = False\n         *batch_shape, num_sequences, fft_unique_bins = complex_input.shape\n         if len(complex_input.shape) > 3:\n\n@@ -758,7 +758,7 @@ def ctc_loss(\n     target_length = convert_to_tensor(target_length)\n     output_length = convert_to_tensor(output_length)\n \n-    # Ensure that the dtype promotion behavior matchs that of `tf.nn.ctc_loss`\n+    # Ensure that the dtype promotion behavior matches that of `tf.nn.ctc_loss`\n     dtype = backend.result_type(output.dtype, \"float32\")\n     output = cast(output, dtype)\n \n\n@@ -401,7 +401,8 @@ class ModelCheckpointTest(testing.TestCase):\n         self.assertTrue(os.path.exists(filepath))\n         os.remove(filepath)\n \n-        # Case 13: ModelCheckpoint doesnt save model if loss was minimum earlier\n+        # Case 13: ModelCheckpoint doesn't save model if loss was minimum\n+        # earlier\n         mode = \"min\"\n         monitor = \"val_loss\"\n         initial_value_threshold = 0\n@@ -426,7 +427,7 @@ class ModelCheckpointTest(testing.TestCase):\n         )\n         self.assertFalse(os.path.exists(filepath))\n \n-        # Case 14: ModelCheckpoint doesnt save model if loss was min earlier in\n+        # Case 14: ModelCheckpoint doesn't save model if loss was min earlier in\n         # auto mode\n         mode = \"auto\"\n         monitor = \"val_loss\"\n\n@@ -167,7 +167,7 @@ class STFTInitializer(Initializer):\n     `hamming` windowing functions. This layer supports periodic windows and\n     scaling-based normalization.\n \n-    This is primarly intended for use in the `STFTSpectrogram` layer.\n+    This is primarily intended for use in the `STFTSpectrogram` layer.\n \n     Examples:\n \n\n@@ -642,7 +642,7 @@ class MultiHeadAttention(Layer):\n \n \n def _index_to_einsum_variable(i):\n-    \"\"\"Coverts an index to a einsum variable name.\n+    \"\"\"Converts an index to a einsum variable name.\n \n     We simply map indices to lowercase characters, e.g. 0 -> 'a', 1 -> 'b'.\n     \"\"\"\n\n@@ -115,7 +115,7 @@ class STFTSpectrogram(layers.Layer):\n         `(batch_size, input_channels, time_length)` if\n         `data_format==\"channels_first\"`, where `time_length` is the length of\n         the input signal, and `input_channels` is the number of input channels.\n-        The same kernels are applied to each channel independetly.\n+        The same kernels are applied to each channel independently.\n \n     Output shape:\n         If `data_format==\"channels_first\" and not expand_dims`, a 3D tensor:\n@@ -150,14 +150,14 @@ class STFTSpectrogram(layers.Layer):\n         ):\n             raise ValueError(\n                 \"`frame_step` should be a positive integer not greater than \"\n-                f\"`frame_length`. Recieved frame_step={frame_step}, \"\n+                f\"`frame_length`. Received frame_step={frame_step}, \"\n                 f\"frame_length={frame_length}\"\n             )\n \n         if fft_length is not None and fft_length < frame_length:\n             raise ValueError(\n                 \"`fft_length` should be not less than `frame_length`. \"\n-                f\"Recieved fft_length={fft_length}, frame_length={frame_length}\"\n+                f\"Received fft_length={fft_length}, frame_length={frame_length}\"\n             )\n \n         if fft_length is not None and (fft_length & -fft_length) != fft_length:\n\n@@ -16,7 +16,7 @@ from keras.src.saving import serialization_lib\n # on exact weight ordering for each layer, so we need\n # to test across all types of layers.\n \n-# TODO: reenable tests after tf_keras is available.\n+# TODO: re-enable tests after tf_keras is available.\n tf_keras = None\n \n \n\n@@ -466,7 +466,7 @@ def solve(a, b):\n \n     Args:\n         a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\n-        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` representing the\n         right-hand side or \"dependent variable\" matrix.\n \n     Returns:\n@@ -514,7 +514,7 @@ def solve_triangular(a, b, lower=False):\n \n     Args:\n         a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\n-        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` representing the\n         right-hand side or \"dependent variable\" matrix.\n \n     Returns:\n\n@@ -890,7 +890,7 @@ class Argmin(Operation):\n \n @keras_export([\"keras.ops.argmin\", \"keras.ops.numpy.argmin\"])\n def argmin(x, axis=None, keepdims=False):\n-    \"\"\"Returns the indices of the minium values along an axis.\n+    \"\"\"Returns the indices of the minimum values along an axis.\n \n     Args:\n         x: Input tensor.\n@@ -1057,7 +1057,7 @@ def average(x, axis=None, weights=None):\n         axis: Integer along which to average `x`. The default, `axis=None`,\n             will average over all of the elements of the input tensor. If axis\n             is negative it counts from the last to the first axis.\n-        weights: Tensor of wieghts associated with the values in `x`. Each\n+        weights: Tensor of weights associated with the values in `x`. Each\n             value in `x` contributes to the average according to its\n             associated weight. The weights array can either be 1-D (in which\n             case its length must be the size of a along the given axis) or of\n@@ -2870,12 +2870,12 @@ class GetItem(Operation):\n             remaining_key = key.copy()\n         else:\n             raise ValueError(\n-                f\"Unsupported key type for array slice. Recieved: `{key}`\"\n+                f\"Unsupported key type for array slice. Received: `{key}`\"\n             )\n         num_ellipses = remaining_key.count(Ellipsis)\n         if num_ellipses > 1:\n             raise ValueError(\n-                f\"Slice should only have one ellipsis. Recieved: `{key}`\"\n+                f\"Slice should only have one ellipsis. Received: `{key}`\"\n             )\n         elif num_ellipses == 0:\n             # Add an implicit final ellipsis.\n@@ -4214,7 +4214,7 @@ def not_equal(x1, x2):\n         x2: Second input tensor.\n \n     Returns:\n-        Output tensor, element-wise comparsion of `x1` and `x2`.\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n     \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return NotEqual().symbolic_call(x1, x2)\n@@ -4533,9 +4533,9 @@ def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n \n     Returns:\n         The quantile(s). If `q` is a single probability and `axis=None`, then\n-        the result is a scalar. If multiple probabilies levels are given, first\n-        axis of the result corresponds to the quantiles. The other axes are the\n-        axes that remain after the reduction of `x`.\n+        the result is a scalar. If multiple probabilities levels are given,\n+        first axis of the result corresponds to the quantiles. The other axes\n+        are the axes that remain after the reduction of `x`.\n     \"\"\"\n     if any_symbolic_tensors((x, q)):\n         return Quantile(\n\n@@ -3295,7 +3295,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertTrue(backend.is_tensor(knp.array(x)))\n         self.assertTrue(backend.is_tensor(knp.Array()(x)))\n \n-        # Check dtype convertion.\n+        # Check dtype conversion.\n         x = [[1, 0, 1], [1, 1, 0]]\n         output = knp.array(x, dtype=\"int32\")\n         self.assertEqual(standardize_dtype(output.dtype), \"int32\")\n@@ -3746,7 +3746,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.isfinite(x), np.isfinite(x))\n         self.assertAllClose(knp.Isfinite()(x), np.isfinite(x))\n \n-    # TODO: fix and reenable\n+    # TODO: fix and re-enable\n     def DISABLED_test_isinf(self):\n         x = np.array([[1, 2, np.inf], [np.nan, np.nan, np.nan]])\n         self.assertAllClose(knp.isinf(x), np.isinf(x))\n@@ -3992,8 +3992,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         # 5D (pad arbitrary dimensions)\n         if backend.backend() == \"torch\" and mode != \"constant\":\n             self.skipTest(\n-                \"reflect and symmetric padding for arbitary dimensions are not \"\n-                \"supported by torch\"\n+                \"reflect and symmetric padding for arbitrary dimensions \"\n+                \"are not supported by torch\"\n             )\n         x = np.ones([2, 3, 4, 5, 6], dtype=dtype)\n         pad_width = ((1, 1), (2, 1), (3, 2), (4, 3), (5, 4))\n\n@@ -15,7 +15,7 @@ class AdamW(adam.Adam):\n \n     According to\n     [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n-    the underying Adam method is \"*computationally\n+    the underlying Adam method is \"*computationally\n     efficient, has little memory requirement, invariant to diagonal rescaling of\n     gradients, and is well suited for problems that are large in terms of\n     data/parameters*\".\n\n@@ -468,7 +468,7 @@ class BaseOptimizer(KerasSaveable):\n             grads = self._clip_gradients(grads)\n             self._apply_weight_decay(trainable_variables)\n \n-            # Run udpate step.\n+            # Run update step.\n             self._backend_update_step(\n                 grads, trainable_variables, self.learning_rate\n             )\n@@ -766,7 +766,7 @@ class BaseOptimizer(KerasSaveable):\n         \"\"\"\n         if hasattr(self, \"_built\") and self._built:\n             raise ValueError(\n-                \"`exclude_from_weight_decay()` can only be configued before \"\n+                \"`exclude_from_weight_decay()` can only be configured before \"\n                 \"the optimizer is built.\"\n             )\n \n\n@@ -273,7 +273,7 @@ def binomial(shape, counts, probabilities, dtype=None, seed=None):\n def beta(shape, alpha, beta, dtype=None, seed=None):\n     \"\"\"Draw samples from a Beta distribution.\n \n-    The values are drawm from a Beta distribution parametrized\n+    The values are drawn from a Beta distribution parametrized\n     by alpha and beta.\n \n     Args:\n\n@@ -279,13 +279,13 @@ class KerasFileEditor:\n                     count += 1\n                 return count\n \n-            occurences = count_occurences(self.weights_dict, source_name)\n-            if occurences > 1:\n+            occurrences = count_occurences(self.weights_dict, source_name)\n+            if occurrences > 1:\n                 raise ValueError(\n                     f\"Name '{source_name}' occurs more than once in the model; \"\n                     \"try passing a complete path\"\n                 )\n-            if occurences == 0:\n+            if occurrences == 0:\n                 raise ValueError(\n                     f\"Source name '{source_name}' does not appear in the \"\n                     \"model. Use `editor.weights_summary()` \"\n\n@@ -586,7 +586,7 @@ class CompileLoss(losses_module.Loss):\n                     raise ValueError(\n                         f\"`loss_weights` must match the number of losses, \"\n                         f\"got {len(tree.flatten(loss))} losses \"\n-                        f\"and {len(loss_weights)} weigths.\"\n+                        f\"and {len(loss_weights)} weights.\"\n                     )\n                 loss_weights = tree.pack_sequence_as(loss, flat_loss_weights)\n             loss = tree.map_structure(\n\n@@ -192,7 +192,7 @@ class JaxLayer(Layer):\n         call_fn: The function to call the model. See description above for the\n             list of arguments it takes and the outputs it returns.\n         init_fn: the function to call to initialize the model. See description\n-            above for the list of arguments it takes and the ouputs it returns.\n+            above for the list of arguments it takes and the outputs it returns.\n             If `None`, then `params` and/or `state` must be provided.\n       params: A `PyTree` containing all the model trainable parameters. This\n             allows passing trained parameters or controlling the initialization.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57caffe2fe7d2a5758cb604c95521cfe266a330c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 912 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1799,7 +1799,8 @@ def _quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n         nan_batch_members = tf.reshape(\n             nan_batch_members, shape=right_rank_matched_shape\n         )\n-        gathered_y = tf.where(nan_batch_members, float(\"NaN\"), gathered_y)\n+        nan_value = tf.constant(float(\"NaN\"), dtype=x.dtype)\n+        gathered_y = tf.where(nan_batch_members, nan_value, gathered_y)\n \n     # Expand dimensions if requested\n     if keepdims:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9ea1a013c29e24001dc6cb7b10d9f740545fe58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 145 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,14 +98,16 @@ class SavingTest(testing.TestCase):\n         model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"))\n         model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n         model.fit(np.array([[1]]), np.array([[1]]), verbose=0)\n-        model.save(\"model.keras\")\n-        model.save_weights(\"model.weights.h5\")\n+        model_fpath = os.path.join(self.get_temp_dir(), \"model.keras\")\n+        weights_fpath = os.path.join(self.get_temp_dir(), \"model.weights.h5\")\n+        model.save(model_fpath)\n+        model.save_weights(weights_fpath)\n \n-        model_editor = KerasFileEditor(\"model.keras\")\n+        model_editor = KerasFileEditor(model_fpath)\n         self.assertEqual(\n             len(keras.src.tree.flatten(model_editor.weights_dict)), 8\n         )\n-        model_weights_editor = KerasFileEditor(\"model.weights.h5\")\n+        model_weights_editor = KerasFileEditor(weights_fpath)\n         self.assertEqual(\n             len(keras.src.tree.flatten(model_weights_editor.weights_dict)), 8\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0adc924e17ce0b1a6dbe769d594f66fbe79e988a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 31 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,10 +91,18 @@ def set_logging_verbosity(level):\n \n def print_msg(message, line_break=True):\n     \"\"\"Print the message to absl logging or stdout.\"\"\"\n+    message = str(message)\n     if is_interactive_logging_enabled():\n-        if line_break:\n-            sys.stdout.write(message + \"\\n\")\n-        else:\n+        message = message + \"\\n\" if line_break else message\n+        try:\n+            sys.stdout.write(message)\n+        except UnicodeEncodeError:\n+            # If the encoding differs from UTF-8, `sys.stdout.write` may fail.\n+            # To address this, replace special unicode characters in the\n+            # message, and then encode and decode using the target encoding.\n+            message = _replace_special_unicode_character(message)\n+            message_bytes = message.encode(sys.stdout.encoding, errors=\"ignore\")\n+            message = message_bytes.decode(sys.stdout.encoding)\n             sys.stdout.write(message)\n         sys.stdout.flush()\n     else:\n@@ -123,3 +131,8 @@ def ask_to_proceed_with_overwrite(filepath):\n         return False\n     print_msg(\"[TIP] Next time specify overwrite=True!\")\n     return True\n+\n+\n+def _replace_special_unicode_character(message):\n+    message = str(message).replace(\"━\", \"=\")  # Fall back to Keras2 behavior.\n+    return message\n\n@@ -1,3 +1,5 @@\n+import sys\n+import tempfile\n from unittest.mock import patch\n \n from keras.src.testing import test_case\n@@ -55,3 +57,13 @@ class TestIoUtils(test_case.TestCase):\n     @patch(\"builtins.input\", side_effect=[\"invalid\", \"n\"])\n     def test_ask_to_proceed_with_overwrite_invalid_then_no(self, _):\n         self.assertFalse(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n+\n+    def test_print_msg_with_different_encoding(self):\n+        # https://github.com/keras-team/keras/issues/19386\n+        io_utils.enable_interactive_logging()\n+        self.assertTrue(io_utils.is_interactive_logging_enabled())\n+        ori_stdout = sys.stdout\n+        with tempfile.TemporaryFile(mode=\"w\", encoding=\"cp1251\") as tmp:\n+            sys.stdout = tmp\n+            io_utils.print_msg(\"━\")\n+        sys.stdout = ori_stdout\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#192b7b25c012dfc9ae567ab1ac12269aab8392f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 6 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1317 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import os\n-\n # DO NOT EDIT. Generated by api_gen.sh\n from keras.api import DTypePolicy\n from keras.api import FloatDTypePolicy\n@@ -54,6 +52,8 @@ from keras.api import visualization\n \n # END DO NOT EDIT.\n \n+import os  # isort: skip\n+\n # Add everything in /api/ to the module search path.\n __path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n \n\n@@ -74,7 +74,7 @@ class TensorBoard(Callback):\n             Batch-level summary writing is also available via `train_step`\n             override. Please see\n             [TensorBoard Scalars tutorial](\n-                https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging)  # noqa: E501\n+                https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging)\n             for more details.\n         profile_batch: (Not supported at this time)\n             Profile the batch(es) to sample compute characteristics.\n@@ -152,7 +152,7 @@ class TensorBoard(Callback):\n         log_dir='./logs', profile_batch=(10,20))\n     model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n     ```\n-    \"\"\"\n+    \"\"\"  # noqa: E501\n \n     def __init__(\n         self,\n\n@@ -192,7 +192,7 @@ class FunctionalTest(testing.TestCase):\n         x = layers.Dense(5)(inputs)\n         output_a = layers.Dense(4)(x)\n         output_b = layers.Dense(5)(x)\n-        if dict == out_type:\n+        if out_type is dict:\n             outputs = {\"a\": output_a, \"b\": output_b}\n         else:\n             outputs = out_type([output_a, output_b])\n\n@@ -767,11 +767,11 @@ def inject_functional_model_class(cls):\n     \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n     from keras.src.models import functional\n \n-    if cls == Model:\n+    if cls is Model:\n         return functional.Functional\n     # In case there is any multiple inheritance, we stop injecting the\n     # class if keras model is not in its class hierarchy.\n-    if cls == object:\n+    if cls is object:\n         return object\n \n     cls.__bases__ = tuple(\n\n@@ -408,7 +408,7 @@ def convert_to_sliceable(arrays, target_backend=None):\n         # Step 2. Normalize floats to floatx.\n         def is_non_floatx_float(dtype):\n             return (\n-                not dtype == object\n+                dtype is not object\n                 and backend.is_float_dtype(dtype)\n                 and not backend.standardize_dtype(dtype) == backend.floatx()\n             )\n\n@@ -103,7 +103,7 @@ def pad_sequences(\n     is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n         dtype, np.str_\n     )\n-    if isinstance(value, str) and dtype != object and not is_dtype_str:\n+    if isinstance(value, str) and dtype is not object and not is_dtype_str:\n         raise ValueError(\n             f\"`dtype` {dtype} is not compatible with `value`'s type: \"\n             f\"{type(value)}\\nYou should set `dtype=object` for variable length \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c052cead4cce39e59a239a64d7894559b6821338", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 713 | Contributors (this commit): 9 | Commits (past 90d): 14 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -125,7 +125,7 @@ def list_summaries(logdir):\n class TestTensorBoardV2(testing.TestCase):\n     def _get_log_dirs(self):\n         logdir = os.path.join(\n-            self.get_temp_dir(), str(random.randint(1, 1e7)), \"tb\"\n+            self.get_temp_dir(), str(random.randint(1, int(1e7))), \"tb\"\n         )\n         train_dir = os.path.join(logdir, \"train\")\n         validation_dir = os.path.join(logdir, \"validation\")\n\n@@ -221,7 +221,7 @@ class Functional(Function, Model):\n             model_inputs_struct = tree.map_structure(\n                 lambda x: x.name, self._inputs_struct\n             )\n-            inputs_struct = tree.map_structure(lambda x: \"*\", inputs)\n+            inputs_struct = tree.map_structure(lambda x: f\"type({x})\", inputs)\n             warnings.warn(\n                 \"The structure of `inputs` doesn't match the expected \"\n                 f\"structure: {model_inputs_struct}. \"\n\n@@ -644,7 +644,11 @@ class OrderedEnqueuer(PyDatasetEnqueuer):\n                 if inputs is not None:\n                     yield inputs\n             except queue.Empty:\n-                pass\n+                warnings.warn(\n+                    \"Generator ran out of batches before reaching `num_batches`\"\n+                )\n+                self.stop()\n+                return\n             except Exception as e:\n                 self.stop(drain_queue_and_join=True)\n                 raise e\n@@ -653,7 +657,7 @@ class OrderedEnqueuer(PyDatasetEnqueuer):\n         # which may happen before the first `on_epoch_begin`. But it's not ok to\n         # poll after `on_epoch_end`.\n         raise ValueError(\n-            \"Iterator called after `on_epoch_end` and before `on_epoch_begin`.\"\n+            \"Iterator called after `on_epoch_end` or before `on_epoch_begin`.\"\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e488c6e87af10c25cf75f89c7b97a56cf20c813f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 147 | Files Changed: 68 | Hunks: 127 | Methods Changed: 90 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 225 | Churn Cumulative: 14741 | Contributors (this commit): 36 | Commits (past 90d): 202 | Contributors (cumulative): 290 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,9 +47,7 @@ class MyDropout(layers.Layer):\n \n     def call(self, inputs):\n         # Use `keras.random` for random ops.\n-        return keras.random.dropout(\n-            inputs, self.rate, seed=self.seed_generator\n-        )\n+        return keras.random.dropout(inputs, self.rate, seed=self.seed_generator)\n \n \n class MyModel(Model):\n\n@@ -27,9 +27,9 @@ Classic MNIST, loaded using tf.data\n \n BATCH_SIZE = 192\n \n-(x_train, train_labels), (\n-    x_eval,\n-    eval_labels,\n+(\n+    (x_train, train_labels),\n+    (x_eval, eval_labels),\n ) = keras.datasets.mnist.load_data()\n x_train = np.expand_dims(x_train, axis=-1).astype(\n     np.float32\n\n@@ -643,7 +643,7 @@ class VariationalAutoEncoder(keras.Model):\n         intermediate_dim=64,\n         latent_dim=32,\n         name=\"autoencoder\",\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(name=name, **kwargs)\n         self.original_dim = original_dim\n\n@@ -3,7 +3,6 @@ from keras.src.datasets import boston_housing\n \n \n class BostonHousingTest(testing.TestCase):\n-\n     def test_load_data(self):\n         (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n         self.assertEqual(x_train.shape[1], 13)\n\n@@ -3,7 +3,6 @@ from keras.src.datasets import california_housing\n \n \n class CaliforniaHousingTest(testing.TestCase):\n-\n     def test_load_data_large(self):\n         (x_train, y_train), (x_test, y_test) = california_housing.load_data(\n             version=\"large\"\n\n@@ -41,7 +41,6 @@ def get_edge_dict(dot):\n \n \n class ModelVisualizationTest(testing.TestCase):\n-\n     def test_plot_sequential_model(self):\n         model = keras.Sequential(\n             [\n\n@@ -6,7 +6,6 @@ from keras.src.backend.common.masking import set_keras_mask\n \n \n class MaskingTest(testing.TestCase):\n-\n     def test_mask_on_eager_tensor(self):\n         x = ops.zeros((2, 3))\n         self.assertIsNone(get_keras_mask(x))\n@@ -25,7 +24,6 @@ class MaskingTest(testing.TestCase):\n         self.assertIsNone(get_keras_mask(x))\n \n     def test_mask_on_tracer_tensor(self):\n-\n         def fn(x):\n             self.assertIsNone(get_keras_mask(x))\n \n\n@@ -8,7 +8,6 @@ from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n \n class TestSymbolicScope(testing.TestCase):\n     def test_basic_flow(self):\n-\n         # Define a function that behaves differently according to\n         # `in_symbolic_scope`.\n         def compute_loss(y, y_pred):\n\n@@ -153,7 +153,6 @@ def compute_output_spec(fn, *args, **kwargs):\n             return x\n \n         def wrapped_fn(*args, **kwargs):\n-\n             # Turn inputs that are sparse to BCOO tensors\n             def to_bcoo_if_sparse(x, maybe_symbolic_x):\n                 if (\n\n@@ -13,7 +13,6 @@ from keras.src.optimizers import base_optimizer\n \n \n class JaxOptimizer(base_optimizer.BaseOptimizer):\n-\n     def _backend_apply_gradients(self, grads, trainable_variables):\n         if self.gradient_accumulation_steps:\n             is_update_step = (\n\n@@ -354,10 +354,9 @@ class JAXTrainer(base_trainer.Trainer):\n             # Create the validation data using the training data. Only supported\n             # for TF/numpy/jax arrays.\n             (\n-                x,\n-                y,\n-                sample_weight,\n-            ), validation_data = array_slicing.train_validation_split(\n+                (x, y, sample_weight),\n+                validation_data,\n+            ) = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -499,7 +499,6 @@ def associative_scan(f, elems, reverse=False, axis=0):\n             )\n \n         def _recursive_case():\n-\n             odd_elems = _scan(reduced_elems)\n \n             def _even_length_case():\n\n@@ -18,7 +18,6 @@ from keras.src.optimizers import base_optimizer\n \n \n class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n-\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._distribution_strategy = tf.distribute.get_strategy()\n\n@@ -150,22 +150,18 @@ class SavedModelTest(testing.TestCase):\n         named_product(struct_type=[\"tuple\", \"array\", \"dict\"])\n     )\n     def test_model_with_input_structure(self, struct_type):\n-\n         class TupleModel(models.Model):\n-\n             def call(self, inputs):\n                 x, y = inputs\n                 return x + ops.mean(y, axis=1)\n \n         class ArrayModel(models.Model):\n-\n             def call(self, inputs):\n                 x = inputs[0]\n                 y = inputs[1]\n                 return x + ops.mean(y, axis=1)\n \n         class DictModel(models.Model):\n-\n             def call(self, inputs):\n                 x = inputs[\"x\"]\n                 y = inputs[\"y\"]\n\n@@ -99,7 +99,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return y_pred\n \n     def _make_function(self, step_function):\n-\n         @tf.autograph.experimental.do_not_convert\n         def one_step_on_data(data):\n             \"\"\"Runs a single training step on a batch of data.\"\"\"\n@@ -271,10 +270,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             # Create the validation data using the training data. Only supported\n             # for TF/numpy/jax arrays.\n             (\n-                x,\n-                y,\n-                sample_weight,\n-            ), validation_data = array_slicing.train_validation_split(\n+                (x, y, sample_weight),\n+                validation_data,\n+            ) = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -195,10 +195,9 @@ class TorchTrainer(base_trainer.Trainer):\n             # for TF/numpy/jax arrays.\n             # TODO: Support torch tensors for validation data.\n             (\n-                x,\n-                y,\n-                sample_weight,\n-            ), validation_data = array_slicing.train_validation_split(\n+                (x, y, sample_weight),\n+                validation_data,\n+            ) = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -124,8 +124,9 @@ def load_data(\n         xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n \n     idx = int(len(xs) * (1 - test_split))\n-    x_train, y_train = np.array(xs[:idx], dtype=\"object\"), np.array(\n-        labels[:idx]\n+    x_train, y_train = (\n+        np.array(xs[:idx], dtype=\"object\"),\n+        np.array(labels[:idx]),\n     )\n     x_test, y_test = np.array(xs[idx:], dtype=\"object\"), np.array(labels[idx:])\n \n\n@@ -166,7 +166,6 @@ class ExportArchive:\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n             if backend.backend() == \"jax\":\n-\n                 trainable_variables = tree.flatten(resource.trainable_variables)\n                 non_trainable_variables = tree.flatten(\n                     resource.non_trainable_variables\n@@ -328,7 +327,6 @@ class ExportArchive:\n                     fn, input_signature=input_signature, autograph=False\n                 )\n             else:  # JAX backend\n-\n                 # 1. Create a stateless wrapper for `fn`\n                 # 2. jax2tf the stateless wrapper\n                 # 3. Create a stateful function that binds the variables with\n\n@@ -74,7 +74,6 @@ class ExportArchiveTest(testing.TestCase):\n         named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n     )\n     def test_model_with_rng_export(self, model_type):\n-\n         class RandomLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n@@ -104,7 +103,6 @@ class ExportArchiveTest(testing.TestCase):\n         named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n     )\n     def test_model_with_non_trainable_state_export(self, model_type):\n-\n         class StateLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n@@ -151,22 +149,18 @@ class ExportArchiveTest(testing.TestCase):\n         named_product(struct_type=[\"tuple\", \"array\", \"dict\"])\n     )\n     def test_model_with_input_structure(self, struct_type):\n-\n         class TupleModel(models.Model):\n-\n             def call(self, inputs):\n                 x, y = inputs\n                 return ops.add(x, y)\n \n         class ArrayModel(models.Model):\n-\n             def call(self, inputs):\n                 x = inputs[0]\n                 y = inputs[1]\n                 return ops.add(x, y)\n \n         class DictModel(models.Model):\n-\n             def call(self, inputs):\n                 x = inputs[\"x\"]\n                 y = inputs[\"y\"]\n@@ -214,7 +208,6 @@ class ExportArchiveTest(testing.TestCase):\n         export_lib.export_model(revived_model, self.get_temp_dir())\n \n     def test_model_with_multiple_inputs(self):\n-\n         class TwoInputsModel(models.Model):\n             def call(self, x, y):\n                 return x + y\n@@ -302,7 +295,6 @@ class ExportArchiveTest(testing.TestCase):\n         named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n     )\n     def test_low_level_model_export_with_dynamic_dims(self, model_type):\n-\n         class ReductionLayer(layers.Layer):\n             def call(self, inputs):\n                 return ops.max(inputs, axis=1)\n@@ -382,7 +374,6 @@ class ExportArchiveTest(testing.TestCase):\n         reason=\"This test is only for the JAX backend.\",\n     )\n     def test_low_level_model_export_with_jax2tf_polymorphic_shapes(self):\n-\n         class SquareLayer(layers.Layer):\n             def call(self, inputs):\n                 return ops.matmul(inputs, inputs)\n\n@@ -37,7 +37,7 @@ class PReLU(Layer):\n         alpha_regularizer=None,\n         alpha_constraint=None,\n         shared_axes=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n\n@@ -110,7 +110,7 @@ class Conv1D(BaseConv):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=1,\n@@ -130,7 +130,7 @@ class Conv1D(BaseConv):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n \n     def _compute_causal_padding(self):\n\n@@ -108,7 +108,7 @@ class Conv1DTranspose(BaseConvTranspose):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=1,\n@@ -127,5 +127,5 @@ class Conv1DTranspose(BaseConvTranspose):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -104,7 +104,7 @@ class Conv2D(BaseConv):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=2,\n@@ -124,5 +124,5 @@ class Conv2D(BaseConv):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -110,7 +110,7 @@ class Conv2DTranspose(BaseConvTranspose):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=2,\n@@ -129,5 +129,5 @@ class Conv2DTranspose(BaseConvTranspose):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -110,7 +110,7 @@ class Conv3D(BaseConv):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=3,\n@@ -130,5 +130,5 @@ class Conv3D(BaseConv):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -115,7 +115,7 @@ class Conv3DTranspose(BaseConvTranspose):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=3,\n@@ -134,5 +134,5 @@ class Conv3DTranspose(BaseConvTranspose):\n             activity_regularizer=activity_regularizer,\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -717,7 +717,6 @@ class ConvBasicTest(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_lora_weight_name(self):\n-\n         class MyModel(models.Model):\n             def __init__(self):\n                 super().__init__(name=\"mymodel\")\n\n@@ -114,7 +114,7 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n         activity_regularizer=None,\n         depthwise_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=1,\n@@ -133,5 +133,5 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n             activity_regularizer=activity_regularizer,\n             depthwise_constraint=depthwise_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -115,7 +115,7 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n         activity_regularizer=None,\n         depthwise_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=2,\n@@ -134,5 +134,5 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n             activity_regularizer=activity_regularizer,\n             depthwise_constraint=depthwise_constraint,\n             bias_constraint=bias_constraint,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -273,7 +273,6 @@ class DenseTest(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_lora_weight_name(self):\n-\n         class MyModel(models.Model):\n             def __init__(self):\n                 super().__init__(name=\"mymodel\")\n\n@@ -877,7 +877,6 @@ def _analyze_split_string(\n \n \n def _analyze_quantization_info(equation, input_shape):\n-\n     def get_specs(equation, input_shape):\n         possible_labels = string.ascii_letters\n         dot_replaced_string = re.sub(r\"\\.\\.\\.\", \"0\", equation)\n\n@@ -15,7 +15,6 @@ from keras.src.backend.common import global_state\n \n \n class LayerTest(testing.TestCase):\n-\n     def test_compute_output_spec(self):\n         # Test that implementing compute_output_shape\n         # is enough to make compute_output_spec work.\n@@ -1196,7 +1195,6 @@ class LayerTest(testing.TestCase):\n                 return self.post_build_modify_layer(input)\n \n         class PostBuildModifyLayer(layers.Layer):\n-\n             def call(self, input):\n                 return self.var + input\n \n@@ -1330,7 +1328,6 @@ class LayerTest(testing.TestCase):\n         self.assertListEqual(layer1_names, layer2_names)\n \n     def test_complex_dtype_support(self):\n-\n         class MyDenseLayer(layers.Layer):\n             def __init__(self, num_outputs):\n                 super(MyDenseLayer, self).__init__()\n\n@@ -78,7 +78,7 @@ class AveragePooling1D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -95,7 +95,7 @@ class AveragePooling2D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -71,7 +71,7 @@ class AveragePooling3D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -79,7 +79,7 @@ class MaxPooling1D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -95,7 +95,7 @@ class MaxPooling2D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -71,7 +71,7 @@ class MaxPooling3D(BasePooling):\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             pool_size,\n\n@@ -6,7 +6,6 @@ from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n \n \n class BaseImagePreprocessingLayer(TFDataLayer):\n-\n     _USE_BASE_FACTOR = True\n     _FACTOR_BOUNDS = (-1, 1)\n \n\n@@ -48,7 +48,7 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         mode=HORIZONTAL_AND_VERTICAL,\n         seed=None,\n         data_format=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(data_format=data_format, **kwargs)\n         self.seed = seed\n\n@@ -125,7 +125,6 @@ class Resizing(BaseImagePreprocessingLayer):\n         return labels\n \n     def get_random_transformation(self, data, training=True, seed=None):\n-\n         if isinstance(data, dict):\n             input_shape = self.backend.shape(data[\"images\"])\n         else:\n\n@@ -11,7 +11,6 @@ from keras.src import testing\n \n \n class TestSpectrogram(testing.TestCase):\n-\n     DTYPE = \"float32\" if backend.backend() == \"torch\" else \"float64\"\n \n     @staticmethod\n@@ -106,7 +105,6 @@ class TestSpectrogram(testing.TestCase):\n     )\n     @pytest.mark.requires_trainable_backend\n     def test_spectrogram_channels_first(self):\n-\n         rnd = np.random.RandomState(41)\n         audio = rnd.uniform(-1, 1, size=(3, 16000, 7))\n \n\n@@ -106,8 +106,8 @@ class UpSampling2dTest(testing.TestCase):\n     def test_upsampling_2d_correctness(self):\n         input_shape = (2, 2, 1, 3)\n         x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-        expected_output = np.array(\n         # fmt: off\n+        expected_output = np.array(\n             [[[[ 0.,  1.,  2.],\n                [ 0.,  1.,  2.]],\n               [[ 3.,  4.,  5.],\n@@ -116,8 +116,8 @@ class UpSampling2dTest(testing.TestCase):\n                [ 6.,  7.,  8.]],\n               [[ 9., 10., 11.],\n                [ 9., 10., 11.]]]]\n-            # fmt: on\n         )\n+        # fmt: on\n         if backend.config.image_data_format() == \"channels_first\":\n             expected_output = expected_output.transpose((0, 3, 1, 2))\n             x = x.transpose((0, 3, 1, 2))\n\n@@ -149,7 +149,7 @@ class ConvLSTM1D(ConvLSTM):\n         return_state=False,\n         go_backwards=False,\n         stateful=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=1,\n@@ -180,5 +180,5 @@ class ConvLSTM1D(ConvLSTM):\n             dropout=dropout,\n             recurrent_dropout=recurrent_dropout,\n             seed=seed,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -149,7 +149,7 @@ class ConvLSTM2D(ConvLSTM):\n         return_state=False,\n         go_backwards=False,\n         stateful=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=2,\n@@ -180,5 +180,5 @@ class ConvLSTM2D(ConvLSTM):\n             dropout=dropout,\n             recurrent_dropout=recurrent_dropout,\n             seed=seed,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -148,7 +148,7 @@ class ConvLSTM3D(ConvLSTM):\n         return_state=False,\n         go_backwards=False,\n         stateful=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(\n             rank=3,\n@@ -179,5 +179,5 @@ class ConvLSTM3D(ConvLSTM):\n             dropout=dropout,\n             recurrent_dropout=recurrent_dropout,\n             seed=seed,\n-            **kwargs\n+            **kwargs,\n         )\n\n@@ -91,7 +91,7 @@ class Tokenizer:\n         char_level=False,\n         oov_token=None,\n         analyzer=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         # Legacy support\n         if \"nb_words\" in kwargs:\n\n@@ -76,7 +76,6 @@ def get_subclassed_model():\n \n @pytest.mark.requires_trainable_backend\n class CloneModelTest(testing.TestCase):\n-\n     def assert_models_equal(self, model1, model2, ref_input):\n         result1 = model1(ref_input)\n         result2 = model2(ref_input)\n\n@@ -121,7 +121,6 @@ def _get_model_multi_outputs_dict_with_single_tensor():\n \n \n def _get_model_with_custom_compute_loss():\n-\n     class MyModel(Model):\n         def __init__(self):\n             inputs = Input(shape=(3,), name=\"inputs\")\n@@ -166,7 +165,6 @@ def _get_variable_value_by_path(variables, path):\n \n @pytest.mark.requires_trainable_backend\n class ModelTest(testing.TestCase):\n-\n     def test_functional_rerouting(self):\n         model = _get_model()\n         self.assertIsInstance(model, Functional)\n@@ -1000,9 +998,8 @@ class ModelTest(testing.TestCase):\n                 structure,\n                 y_pred,\n             )\n-            flat_y_pred, flat_y_true = tree.flatten(y_pred), tree.flatten(\n-                y_true\n-            )\n+            flat_y_pred = tree.flatten(y_pred)\n+            flat_y_true = tree.flatten(y_true)\n             diff = 0\n             for y_p, y_t in zip(flat_y_pred, flat_y_true):\n                 diff += losses.mean_absolute_error(y_t, y_p)\n\n@@ -818,7 +818,6 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         reason=f\"{backend.backend()} doesn't support `custom_gradient`.\",\n     )\n     def test_custom_gradient(self):\n-\n         # function to test custom_gradient on\n         @ops.custom_gradient\n         def log1pexp(x):\n\n@@ -657,7 +657,6 @@ class ImageOpsCorrectnessTest(testing.TestCase):\n                     [255, 255, 255, 255],\n                 ]\n                 if \"torch\" == backend.backend()\n-                else\n                 # Resize without `round` and `saturate_cast` - differences in\n                 # 16 points\n                 # [\n@@ -669,7 +668,7 @@ class ImageOpsCorrectnessTest(testing.TestCase):\n                 #\n                 # Resize with `round` and `saturate_cast` - differences in\n                 # 8 points\n-                [\n+                else [\n                     [0, 0, 0, 0],\n                     [53, 53, 53, 54],\n                     [201, 202, 202, 202],\n\n@@ -47,7 +47,6 @@ def _cholesky(x):\n \n \n class Det(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -84,7 +83,6 @@ def _det(x):\n \n \n class Eig(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -124,7 +122,6 @@ def _eig(x):\n \n \n class Eigh(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -165,7 +162,6 @@ def _eigh(x):\n \n \n class Inv(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -202,7 +198,6 @@ def _inv(x):\n \n \n class LuFactor(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -445,7 +440,6 @@ def qr(x, mode=\"reduced\"):\n \n \n class Solve(Operation):\n-\n     def __init__(self):\n         super().__init__()\n \n@@ -490,7 +484,6 @@ def _solve(a, b):\n \n \n class SolveTriangular(Operation):\n-\n     def __init__(self, lower=False):\n         super().__init__()\n         self.lower = lower\n@@ -538,7 +531,6 @@ def _solve_triangular(a, b, lower=False):\n \n \n class SVD(Operation):\n-\n     def __init__(self, full_matrices=True, compute_uv=True):\n         super().__init__()\n         self.full_matrices = full_matrices\n\n@@ -330,7 +330,6 @@ class LinalgOpsStaticShapeTest(testing.TestCase):\n \n \n class LinalgOpsCorrectnessTest(testing.TestCase):\n-\n     def test_cholesky(self):\n         x = np.random.rand(4, 3, 3).astype(\"float32\")\n         with self.assertRaises(ValueError):\n\n@@ -43,7 +43,6 @@ class SegmentReduction(Operation):\n \n \n class SegmentSum(SegmentReduction):\n-\n     def call(self, data, segment_ids):\n         _segment_reduce_validation(data, segment_ids)\n         return backend.math.segment_sum(\n@@ -90,7 +89,6 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n \n \n class SegmentMax(SegmentReduction):\n-\n     def call(self, data, segment_ids):\n         _segment_reduce_validation(data, segment_ids)\n         return backend.math.segment_max(\n\n@@ -145,7 +145,6 @@ def _max_reduce(left, right):\n \n \n class MathOpsDynamicShapeTest(testing.TestCase):\n-\n     @parameterized.parameters([(kmath.segment_sum,), (kmath.segment_max,)])\n     def test_segment_reduce(self, segment_reduce_op):\n         # 1D case\n@@ -418,7 +417,6 @@ class MathOpsStaticShapeTest(testing.TestCase):\n \n \n class MathOpsCorrectnessTest(testing.TestCase):\n-\n     def run_segment_reduce_test(\n         self,\n         segment_reduce_op,\n@@ -1345,7 +1343,6 @@ class ISTFTTest(testing.TestCase):\n \n \n class TestMathErrors(testing.TestCase):\n-\n     @parameterized.parameters([(kmath.segment_sum,), (kmath.segment_max,)])\n     @pytest.mark.skipif(\n         backend.backend() != \"jax\", reason=\"Testing Jax errors only\"\n\n@@ -206,21 +206,29 @@ class OptimizerSparseTest(testing.TestCase):\n         # patch \"_apply_weight_decay\" to exclude this special case.\n         # patch the optimizer \"assign\" methods to detect sparse updates.\n         # patch the tf.Variable \"assign\" methods to detect direct assign calls.\n-        with mock.patch.object(\n+        with (\n+            mock.patch.object(\n                 optimizer_to_patch, \"_apply_weight_decay\", autospec=True\n-        ), mock.patch.object(\n+            ),\n+            mock.patch.object(\n                 optimizer_to_patch, \"assign\", autospec=True\n-        ) as optimizer_assign, mock.patch.object(\n+            ) as optimizer_assign,\n+            mock.patch.object(\n                 optimizer_to_patch, \"assign_add\", autospec=True\n-        ) as optimizer_assign_add, mock.patch.object(\n+            ) as optimizer_assign_add,\n+            mock.patch.object(\n                 optimizer_to_patch, \"assign_sub\", autospec=True\n-        ) as optimizer_assign_sub, mock.patch.object(\n+            ) as optimizer_assign_sub,\n+            mock.patch.object(\n                 variable_class, \"assign\", autospec=True\n-        ) as variable_assign, mock.patch.object(\n+            ) as variable_assign,\n+            mock.patch.object(\n                 variable_class, \"assign_add\", autospec=True\n-        ) as variable_assign_add, mock.patch.object(\n+            ) as variable_assign_add,\n+            mock.patch.object(\n                 variable_class, \"assign_sub\", autospec=True\n-        ) as variable_assign_sub:\n+            ) as variable_assign_sub,\n+        ):\n             optimizer_assign.side_effect = mock_optimizer_assign\n             optimizer_assign_add.side_effect = mock_optimizer_assign\n             optimizer_assign_sub.side_effect = mock_optimizer_assign\n\n@@ -552,7 +552,6 @@ class KerasFileEditor:\n         self._print_weights_structure(self.weights_dict, prefix=\" \" * 2)\n \n     def _weights_summary_interactive(self):\n-\n         def _generate_html_weights(dictionary, margin_left=0, font_size=1):\n             html = \"\"\n             for key, value in dictionary.items():\n\n@@ -26,7 +26,6 @@ def get_target_model():\n \n \n class SavingTest(testing.TestCase):\n-\n     def test_basics(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n \n\n@@ -352,7 +352,7 @@ class MyDense(keras.layers.Layer):\n         *,\n         kernel_regularizer=None,\n         kernel_initializer=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self._units = units\n@@ -364,7 +364,7 @@ class MyDense(keras.layers.Layer):\n             units=self._units,\n             kernel_initializer=self._kernel_initializer,\n             kernel_regularizer=self._kernel_regularizer,\n-            **super().get_config()\n+            **super().get_config(),\n         )\n \n     def build(self, input_shape):\n\n@@ -20,9 +20,8 @@ class TestCompileMetrics(testing.TestCase):\n             weighted_metrics=[metrics_module.MeanSquaredError()],\n         )\n         # Test symbolic build\n-        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n-            (3, 4)\n-        )\n+        y_true = backend.KerasTensor((3, 4))\n+        y_pred = backend.KerasTensor((3, 4))\n         compile_metrics.build(y_true, y_pred)\n         # Test eager build\n         y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n@@ -243,9 +242,8 @@ class TestCompileLoss(testing.TestCase):\n             loss=losses_module.MeanSquaredError(),\n         )\n         # Test symbolic build\n-        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n-            (3, 4)\n-        )\n+        y_true = backend.KerasTensor((3, 4))\n+        y_pred = backend.KerasTensor((3, 4))\n         compile_loss.build(y_true, y_pred)\n         # Test eager build\n         y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n@@ -258,9 +256,8 @@ class TestCompileLoss(testing.TestCase):\n         compile_loss = CompileLoss(loss=\"crossentropy\")\n \n         # Test symbolic build\n-        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n-            (3, 4)\n-        )\n+        y_true = backend.KerasTensor((3, 4))\n+        y_pred = backend.KerasTensor((3, 4))\n         compile_loss.build(y_true, y_pred)\n         # Test eager build\n         y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n\n@@ -198,7 +198,6 @@ class ArrayDataAdapter(DataAdapter):\n             )\n \n             def grab_batch(i, data):\n-\n                 def grab_one(x):\n                     if isinstance(x, array_slicing.TensorflowSparseWrapper):\n                         return array_slicing.slice_tensorflow_sparse_wrapper(\n\n@@ -24,7 +24,7 @@ class ExamplePyDataset(py_dataset_adapter.PyDataset):\n         batch_size=32,\n         delay=0,\n         infinite=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.x, self.y = x_set, y_set\n@@ -80,7 +80,6 @@ class DictPyDataset(py_dataset_adapter.PyDataset):\n \n \n class ExceptionPyDataset(py_dataset_adapter.PyDataset):\n-\n     @property\n     def num_batches(self):\n         return 4\n@@ -285,7 +284,6 @@ class PyDatasetAdapterTest(testing.TestCase):\n             self.assertEqual(tuple(by.shape), (4, 2))\n \n     def test_with_different_shapes(self):\n-\n         class TestPyDataset(py_dataset_adapter.PyDataset):\n             @property\n             def num_batches(self):\n\n@@ -57,7 +57,6 @@ class TestTorchDataLoaderAdapter(testing.TestCase):\n         named_product(batch_size=[None, 3], implements_len=[True, False])\n     )\n     def test_dataloader_iterable_dataset(self, batch_size, implements_len):\n-\n         class TestIterableDataset(torch.utils.data.IterableDataset):\n             def __init__(self):\n                 self.x = torch.normal(2, 3, size=(16, 4))\n\n@@ -1999,7 +1999,6 @@ class TestTrainer(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_callbacks_can_update_state_at_batch_boundary(self):\n-\n         class CounterModel(keras.Model):\n             def __init__(self):\n                 super().__init__()\n@@ -2176,7 +2175,6 @@ class TestTrainer(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_compute_loss_no_training_backwards_compatibility(self):\n-\n         class MyModel(keras.Model):\n             def __init__(self):\n                 super().__init__()\n\n@@ -198,7 +198,6 @@ def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n \n \n def lists_to_tuples(structure):\n-\n     def sequence_fn(instance, args):\n         if isinstance(instance, list):\n             return tuple(args)\n@@ -210,7 +209,6 @@ def lists_to_tuples(structure):\n \n \n def map_shape_structure(func, structure):\n-\n     def is_shape_tuple(x):\n         return isinstance(x, (list, tuple)) and all(\n             isinstance(e, (int, type(None))) for e in x\n\n@@ -38,7 +38,6 @@ if optree.available:\n \n @parameterized.named_parameters(TEST_CASES)\n class TreeTest(testing.TestCase):\n-\n     def test_is_nested(self, tree_impl, is_optree):\n         self.assertFalse(tree_impl.is_nested(\"1234\"))\n         self.assertFalse(tree_impl.is_nested(b\"1234\"))\n\n@@ -11,7 +11,6 @@ from keras.src.utils.module_utils import tensorflow as tf\n \n \n class MyTorchDataset(TorchDataset):\n-\n     def __init__(self, x, y):\n         self.x = x\n         self.y = y\n\n@@ -207,7 +207,6 @@ class TestJaxLayer(testing.TestCase):\n             return count\n \n         def verify_weights_and_params(layer):\n-\n             self.assertEqual(trainable_weights, len(layer.trainable_weights))\n             self.assertEqual(\n                 trainable_params,\n@@ -329,7 +328,6 @@ class TestJaxLayer(testing.TestCase):\n \n         # test subclass model building without a build method\n         class TestModel(models.Model):\n-\n             def __init__(self, layer):\n                 super().__init__()\n                 self._layer = layer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
