{"custom_id": "scrapy#e3b15252c80ca3d0872f3068c382a0a3e7cc9db6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 40 | Churn Cumulative: 719 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 29 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -50,7 +50,7 @@ class FilteringLinkExtractor(object):\n     _csstranslator = HTMLTranslator()\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n-                 restrict_xpaths, canonicalize, deny_extensions, restrict_css):\n+                 restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n \n         self.link_extractor = link_extractor\n \n@@ -70,6 +70,8 @@ class FilteringLinkExtractor(object):\n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n         self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n+        self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x)\n+                              for x in arg_to_iter(restrict_text)]\n \n     def _link_allowed(self, link):\n         if not _is_valid_url(link.url):\n@@ -85,6 +87,8 @@ class FilteringLinkExtractor(object):\n             return False\n         if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n             return False\n+        if self.restrict_text and not _matches(link.text, self.restrict_text):\n+            return False\n         return True\n \n     def matches(self, url):\n\n@@ -97,7 +97,7 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                  tags=('a', 'area'), attrs=('href',), canonicalize=False,\n                  unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n-                 strip=True):\n+                 strip=True, restrict_text=None):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n@@ -113,7 +113,8 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n         super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n                                                 allow_domains=allow_domains, deny_domains=deny_domains,\n                                                 restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n-            canonicalize=canonicalize, deny_extensions=deny_extensions)\n+                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n+                                                restrict_text=restrict_text)\n \n     def extract_links(self, response):\n         base_url = get_base_url(response)\n\n@@ -113,7 +113,7 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                  tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True,\n                  process_value=None, deny_extensions=None, restrict_css=(),\n-                 strip=True):\n+                 strip=True, restrict_text=()):\n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n@@ -133,7 +133,8 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n         super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n                                                 allow_domains=allow_domains, deny_domains=deny_domains,\n                                                 restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n-            canonicalize=canonicalize, deny_extensions=deny_extensions)\n+                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n+                                                restrict_text=restrict_text)\n \n     def extract_links(self, response):\n         base_url = None\n\n@@ -479,6 +479,30 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n             Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n         ])\n \n+    def test_link_restrict_text(self):\n+        html = b\"\"\"\n+        <a href=\"http://example.org/item1.html\">Pic of a cat</a>\n+        <a href=\"http://example.org/item2.html\">Pic of a dog</a>\n+        <a href=\"http://example.org/item3.html\">Pic of a cow</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        # Simple text inclusion test\n+        lx = self.extractor_cls(restrict_text='dog')\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item2.html', text=u'Pic of a dog', nofollow=False),\n+        ])\n+        # Unique regex test\n+        lx = self.extractor_cls(restrict_text=r'of.*dog')\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item2.html', text=u'Pic of a dog', nofollow=False),\n+        ])\n+        # Multiple regex test\n+        lx = self.extractor_cls(restrict_text=[r'of.*dog', r'of.*cat'])\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item1.html', text=u'Pic of a cat', nofollow=False),\n+            Link(url='http://example.org/item2.html', text=u'Pic of a dog', nofollow=False),\n+        ])\n+\n     @pytest.mark.xfail\n     def test_restrict_xpaths_with_html_entities(self):\n         super(LxmlLinkExtractorTestCase, self).test_restrict_xpaths_with_html_entities()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#858f5be74728209d8ef71794296814abca4c1c93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 10 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 26 | Churn Cumulative: 4512 | Contributors (this commit): 37 | Commits (past 90d): 32 | Contributors (cumulative): 93 | DMM Complexity: None\n\nDIFF:\n@@ -99,7 +99,7 @@ def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n \n-    # --- backwards compatibility for scrapy.conf.settings singleton ---\n+    # --- backward compatibility for scrapy.conf.settings singleton ---\n     if settings is None and 'scrapy.conf' in sys.modules:\n         from scrapy import conf\n         if hasattr(conf, 'settings'):\n@@ -116,7 +116,7 @@ def execute(argv=None, settings=None):\n             settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n \n-    # --- backwards compatibility for scrapy.conf.settings singleton ---\n+    # --- backward compatibility for scrapy.conf.settings singleton ---\n     import warnings\n     from scrapy.exceptions import ScrapyDeprecationWarning\n     with warnings.catch_warnings():\n\n@@ -1,4 +1,4 @@\n-# This module is kept for backwards compatibility, so users can import\n+# This module is kept for backward compatibility, so users can import\n # scrapy.conf.settings and get the settings they expect\n \n import sys\n\n@@ -3,7 +3,7 @@ from .http10 import HTTP10DownloadHandler\n from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n \n \n-# backwards compatibility\n+# backward compatibility\n class HttpDownloadHandler(HTTP10DownloadHandler):\n \n     def __init__(self, *args, **kwargs):\n\n@@ -94,7 +94,7 @@ class FileFeedStorage(object):\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, access_key=None, secret_key=None):\n-        # BEGIN Backwards compatibility for initialising without keys (and\n+        # BEGIN Backward compatibility for initialising without keys (and\n         # without using from_crawler)\n         no_defaults = access_key is None and secret_key is None\n         if no_defaults:\n@@ -111,7 +111,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 )\n                 access_key = settings['AWS_ACCESS_KEY_ID']\n                 secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        # END Backwards compatibility\n+        # END Backward compatibility\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n\n@@ -17,7 +17,7 @@ warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n               ScrapyDeprecationWarning, stacklevel=2)\n \n \n-# Imports and level_names variable kept for backwards-compatibility\n+# Imports and level_names variable kept for backward-compatibility\n \n DEBUG = logging.DEBUG\n INFO = logging.INFO\n\n@@ -20,7 +20,7 @@ item_scraped = object()\n item_dropped = object()\n item_error = object()\n \n-# for backwards compatibility\n+# for backward compatibility\n stats_spider_opened = spider_opened\n stats_spider_closing = spider_closed\n stats_spider_closed = spider_closed\n\n@@ -42,14 +42,14 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n                 raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                  'a real number or None instead'.format(value, name))\n \n-    # BEGIN Backwards compatibility for old (base, custom) call signature\n+    # BEGIN Backward compatibility for old (base, custom) call signature\n     if isinstance(custom, (list, tuple)):\n         _check_components(custom)\n         return type(custom)(convert(c) for c in custom)\n \n     if custom is not None:\n         compdict.update(custom)\n-    # END Backwards compatibility\n+    # END Backward compatibility\n \n     _validate_values(compdict)\n     compdict = without_none_values(_map_keys(compdict))\n\n@@ -50,7 +50,7 @@ class DummyDH(object):\n \n \n class DummyLazyDH(object):\n-    # Default is lazy for backwards compatibility\n+    # Default is lazy for backward compatibility\n \n     def __init__(self, crawler):\n         pass\n\n@@ -161,7 +161,7 @@ class S3FeedStorageTest(unittest.TestCase):\n                                 aws_credentials['AWS_SECRET_ACCESS_KEY'])\n         self.assertEqual(storage.access_key, 'uri_key')\n         self.assertEqual(storage.secret_key, 'uri_secret')\n-        # Backwards compatibility for initialising without settings\n+        # Backward compatibility for initialising without settings\n         with warnings.catch_warnings(record=True) as w:\n             storage = S3FeedStorage('s3://mybucket/export.csv')\n             self.assertEqual(storage.access_key, 'conf_key')\n\n@@ -11,7 +11,7 @@ class BuildComponentListTest(unittest.TestCase):\n         self.assertEqual(build_component_list(d, convert=lambda x: x),\n                          ['one', 'four', 'three'])\n \n-    def test_backwards_compatible_build_dict(self):\n+    def test_backward_compatible_build_dict(self):\n         base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n         custom = {'two': None, 'three': 8, 'four': 4}\n         self.assertEqual(build_component_list(base, custom,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75d6f56c8a731ea4e1c06814a59a0b51741d04a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 35 | Files Changed: 9 | Hunks: 26 | Methods Changed: 14 | Complexity Δ (Sum/Max): -5/2 | Churn Δ: 70 | Churn Cumulative: 4694 | Contributors (this commit): 42 | Commits (past 90d): 10 | Contributors (cumulative): 89 | DMM Complexity: None\n\nDIFF:\n@@ -153,7 +153,7 @@ class CrawlerRunner(object):\n         It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n         keeping track of it so it can be stopped later.\n \n-        If `crawler_or_spidercls` isn't a :class:`~scrapy.crawler.Crawler`\n+        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n         instance, this method will try to create one using this parameter as\n         the spider class given to it.\n \n@@ -188,10 +188,10 @@ class CrawlerRunner(object):\n         \"\"\"\n         Return a :class:`~scrapy.crawler.Crawler` object.\n \n-        * If `crawler_or_spidercls` is a Crawler, it is returned as-is.\n-        * If `crawler_or_spidercls` is a Spider subclass, a new Crawler\n+        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n+        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n           is constructed for it.\n-        * If `crawler_or_spidercls` is a string, this function finds\n+        * If ``crawler_or_spidercls`` is a string, this function finds\n           a spider with this name in a Scrapy project (using spider loader),\n           then creates a Crawler instance for it.\n         \"\"\"\n@@ -273,7 +273,7 @@ class CrawlerProcess(CrawlerRunner):\n         :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n         on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n \n-        If `stop_after_crawl` is True, the reactor will be stopped after all\n+        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n         crawlers have finished, using :meth:`join`.\n \n         :param boolean stop_after_crawl: stop or not the reactor when all\n\n@@ -13,21 +13,21 @@ CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(ref\n class LogFormatter(object):\n     \"\"\"Class for generating log messages for different actions.\n \n-    All methods must return a dictionary listing the parameters `level`, `msg`\n-    and `args` which are going to be used for constructing the log message when\n-    calling logging.log.\n+    All methods must return a dictionary listing the parameters ``level``,\n+    ``msg`` and ``args`` which are going to be used for constructing the log\n+    message when calling logging.log.\n \n     Dictionary keys for the method outputs:\n-        * `level` should be the log level for that action, you can use those\n+        * ``level`` should be the log level for that action, you can use those\n         from the python logging library: logging.DEBUG, logging.INFO,\n         logging.WARNING, logging.ERROR and logging.CRITICAL.\n \n-        * `msg` should be a string that can contain different formatting\n-        placeholders. This string, formatted with the provided `args`, is going\n-        to be the log message for that action.\n+        * ``msg`` should be a string that can contain different formatting\n+        placeholders. This string, formatted with the provided ``args``, is\n+        going to be the log message for that action.\n \n-        * `args` should be a tuple or dict with the formatting placeholders for\n-        `msg`.  The final log message is computed as output['msg'] %\n+        * ``args`` should be a tuple or dict with the formatting placeholders\n+        for ``msg``.  The final log message is computed as output['msg'] %\n         output['args'].\n     \"\"\"\n \n\n@@ -255,13 +255,13 @@ class FilesPipeline(MediaPipeline):\n     doing stat of the files and determining if file is new, uptodate or\n     expired.\n \n-    `new` files are those that pipeline never processed and needs to be\n+    ``new`` files are those that pipeline never processed and needs to be\n         downloaded from supplier site the first time.\n \n-    `uptodate` files are the ones that the pipeline processed and are still\n+    ``uptodate`` files are the ones that the pipeline processed and are still\n         valid files.\n \n-    `expired` files are those that pipeline already processed but the last\n+    ``expired`` files are those that pipeline already processed but the last\n         modification was made long time ago, so a reprocessing is recommended to\n         refresh it in case of change.\n \n\n@@ -2,7 +2,7 @@ from ftplib import error_perm\n from posixpath import dirname\n \n def ftp_makedirs_cwd(ftp, path, first_call=True):\n-    \"\"\"Set the current directory of the FTP connection given in the `ftp`\n+    \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n     argument (as a ftplib.FTP object), creating all parent directories if they\n     don't exist. The ftplib.FTP object must be already connected and logged in.\n     \"\"\"\n\n@@ -32,7 +32,7 @@ class TopLevelFormatter(logging.Filter):\n \n     Since it can't be set for just one logger (it won't propagate for its\n     children), it's going to be set in the root handler, with a parametrized\n-    `loggers` list where it should act.\n+    ``loggers`` list where it should act.\n     \"\"\"\n \n     def __init__(self, loggers=None):\n\n@@ -97,8 +97,8 @@ def unicode_to_str(text, encoding=None, errors='strict'):\n \n \n def to_unicode(text, encoding=None, errors='strict'):\n-    \"\"\"Return the unicode representation of a bytes object `text`. If `text`\n-    is already an unicode object, return it as-is.\"\"\"\n+    \"\"\"Return the unicode representation of a bytes object ``text``. If\n+    ``text`` is already an unicode object, return it as-is.\"\"\"\n     if isinstance(text, six.text_type):\n         return text\n     if not isinstance(text, (bytes, six.text_type)):\n@@ -110,7 +110,7 @@ def to_unicode(text, encoding=None, errors='strict'):\n \n \n def to_bytes(text, encoding=None, errors='strict'):\n-    \"\"\"Return the binary representation of `text`. If `text`\n+    \"\"\"Return the binary representation of ``text``. If ``text``\n     is already a bytes object, return it as-is.\"\"\"\n     if isinstance(text, bytes):\n         return text\n@@ -123,7 +123,7 @@ def to_bytes(text, encoding=None, errors='strict'):\n \n \n def to_native_str(text, encoding=None, errors='strict'):\n-    \"\"\" Return str representation of `text`\n+    \"\"\" Return str representation of ``text``\n     (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n     if six.PY2:\n         return to_bytes(text, encoding, errors)\n@@ -189,7 +189,7 @@ def isbinarytext(text):\n \n \n def binary_is_text(data):\n-    \"\"\" Returns `True` if the given ``data`` argument (a ``bytes`` object)\n+    \"\"\" Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n     does not contain unprintable control characters.\n     \"\"\"\n     if not isinstance(data, bytes):\n@@ -314,7 +314,7 @@ class WeakKeyCache(object):\n @deprecated\n def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     \"\"\"Return a (new) dict with unicode keys (and values when \"keys_only\" is\n-    False) of the given dict converted to strings. `dct_or_tuples` can be a\n+    False) of the given dict converted to strings. ``dct_or_tuples`` can be a\n     dict or a list of tuples, like any dict constructor supports.\n     \"\"\"\n     d = {}\n@@ -357,10 +357,10 @@ def retry_on_eintr(function, *args, **kw):\n \n \n def without_none_values(iterable):\n-    \"\"\"Return a copy of `iterable` with all `None` entries removed.\n+    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n-    If `iterable` is a mapping, return a dictionary where all pairs that have\n-    value `None` have been removed.\n+    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n+    value ``None`` have been removed.\n     \"\"\"\n     try:\n         return {k: v for k, v in six.iteritems(iterable) if v is not None}\n\n@@ -109,12 +109,12 @@ def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=\n \n     \"\"\"Strip URL string from some of its components:\n \n-    - `strip_credentials` removes \"user:password@\"\n-    - `strip_default_port` removes \":80\" (resp. \":443\", \":21\")\n+    - ``strip_credentials`` removes \"user:password@\"\n+    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n       from http:// (resp. https://, ftp://) URLs\n-    - `origin_only` replaces path component with \"/\", also dropping\n+    - ``origin_only`` replaces path component with \"/\", also dropping\n       query and fragment components ; it also strips credentials\n-    - `strip_fragment` drops any #fragment component\n+    - ``strip_fragment`` drops any #fragment component\n     \"\"\"\n \n     parsed_url = urlparse(url)\n\n@@ -16,7 +16,7 @@ _DATABASES = collections.defaultdict(DummyDB)\n def open(file, flag='r', mode=0o666):\n     \"\"\"Open or create a dummy database compatible.\n \n-    Arguments `flag` and `mode` are ignored.\n+    Arguments ``flag`` and ``mode`` are ignored.\n     \"\"\"\n     # return same instance for same file argument\n     return _DATABASES[file]\n\n@@ -61,7 +61,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_fetch_redirect_follow_302(self):\n-        \"\"\"Test that calling `fetch(url)` follows HTTP redirects by default.\"\"\"\n+        \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n         url = self.url('/redirect-no-meta-refresh')\n         code = \"fetch('{0}')\"\n         errcode, out, errout = yield self.execute(['-c', code.format(url)])\n@@ -71,7 +71,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_fetch_redirect_not_follow_302(self):\n-        \"\"\"Test that calling `fetch(url, redirect=False)` disables automatic redirects.\"\"\"\n+        \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n         url = self.url('/redirect-no-meta-refresh')\n         code = \"fetch('{0}', redirect=False)\"\n         errcode, out, errout = yield self.execute(['-c', code.format(url)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6eca6f92c6ae48bb136311308e77e82d7659cf43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 939 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,11 +34,8 @@ class FormRequest(Request):\n                 self._set_body(querystr)\n             else:\n                 url_split = urlsplit(self.url)\n-                formdata_key_list = []\n-                for k in querystr.split('&'):\n-                    formdata_key_list.append(k.split('=')[0])\n-                items = []\n-                items += [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_key_list]\n+                formdata_key_list = list(dict(parse_qsl(querystr)).keys())\n+                items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_key_list]\n                 query_str = _urlencode(items, self.encoding)\n                 self._set_url(urljoin(self.url,'?'+ (query_str + '&' if query_str else '') + querystr + ('#'+ url_split.fragment if url_split.fragment else '')))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d75b61b96aad172e21c7d3aeba6f48419a63c72d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1081 | Contributors (this commit): 17 | Commits (past 90d): 14 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -289,6 +289,12 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n \n+        #GET duplicates are preserved\n+        formdata=(('foo', 'bar'), ('foo', 'baz'))\n+        fs = _qs(self.request_class('http://example.com/?foo=1&foo=2&a=1&a=2', method='GET', formdata=data))\n+        self.assertEqual(set(fs[b'foo']), {b'bar', b'baz'})\n+        self.assertEqual(set(fs[b'a']), {b'1', b'2'})\n+\n     def test_default_encoding_bytes(self):\n         # using default encoding (utf-8)\n         data = {b'one': b'two', b'price': b'\\xc2\\xa3 100'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fdf03a6d0daae79b9a3b421d19102873d5a75f46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1087 | Contributors (this commit): 17 | Commits (past 90d): 15 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -289,10 +289,10 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'a'], [b'1'])\n         self.assertEqual(fs[b'b'], [b'2'])\n \n-        #GET duplicates are preserved\n-        formdata=(('foo', 'bar'), ('foo', 'baz'))\n+        #Duplicate GET arguments are preserved\n+        formdata={'foo' : 'bar'}\n         fs = _qs(self.request_class('http://example.com/?foo=1&foo=2&a=1&a=2', method='GET', formdata=data))\n-        self.assertEqual(set(fs[b'foo']), {b'bar', b'baz'})\n+        self.assertEqual(fs[b'foo'], [b'bar'])\n         self.assertEqual(set(fs[b'a']), {b'1', b'2'})\n \n     def test_default_encoding_bytes(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8831fafabc699bd2ab48e3e185712fbcd4f7cc7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1089 | Contributors (this commit): 17 | Commits (past 90d): 16 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'b'], [b'2'])\n \n         #Duplicate GET arguments are preserved\n-        formdata={'foo' : 'bar'}\n+        data={'foo' : 'bar'}\n         fs = _qs(self.request_class('http://example.com/?foo=1&foo=2&a=1&a=2', method='GET', formdata=data))\n         self.assertEqual(fs[b'foo'], [b'bar'])\n         self.assertEqual(set(fs[b'a']), {b'1', b'2'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7da460b7935940c93d5454019ea99f9f117b3e7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 947 | Contributors (this commit): 24 | Commits (past 90d): 8 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import six\n-from six.moves.urllib.parse import urljoin, urlencode, urlsplit, parse_qsl\n+from six.moves.urllib.parse import urljoin, urlencode, urlsplit, parse_qsl, urlunsplit\n \n import lxml.html\n from parsel.selector import create_root_node\n@@ -34,10 +34,10 @@ class FormRequest(Request):\n                 self._set_body(querystr)\n             else:\n                 url_split = urlsplit(self.url)\n-                formdata_key_list = list(dict(parse_qsl(querystr)).keys())\n-                items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_key_list]\n+                formdata_keys = set(dict(parse_qsl(querystr)).keys())\n+                items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_keys]\n                 query_str = _urlencode(items, self.encoding)\n-                self._set_url(urljoin(self.url,'?'+ (query_str + '&' if query_str else '') + querystr + ('#'+ url_split.fragment if url_split.fragment else '')))\n+                self._set_url(urlunsplit((url_split.scheme, url_split.netloc, url_split.path, (query_str + '&' + querystr) if query_str else querystr, url_split.fragment)))\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7bf3abbd03a10124ed648d45abbb6c8eb3218b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 950 | Contributors (this commit): 24 | Commits (past 90d): 9 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -37,7 +37,8 @@ class FormRequest(Request):\n                 formdata_keys = set(dict(parse_qsl(querystr)).keys())\n                 items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_keys]\n                 query_str = _urlencode(items, self.encoding)\n-                self._set_url(urlunsplit((url_split.scheme, url_split.netloc, url_split.path, (query_str + '&' + querystr) if query_str else querystr, url_split.fragment)))\n+                query = (query_str + '&' + querystr) if query_str else querystr\n+                self._set_url(urlunsplit(url_split._replace(query = query)))\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#924b67437b92f14601816d02c5d153e7281da6d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 201 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -13,25 +13,29 @@ CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(ref\n class LogFormatter(object):\n     \"\"\"Class for generating log messages for different actions.\n \n-    All methods must return a dictionary listing the parameters `level`, `msg`\n-    and `args` which are going to be used for constructing the log message when\n-    calling logging.log.\n+    All methods must return a dictionary listing the parameters ``level``, ``msg``\n+    and ``args`` which are going to be used for constructing the log message when\n+    calling ``logging.log``.\n \n     Dictionary keys for the method outputs:\n-        * `level` should be the log level for that action, you can use those\n-        from the python logging library: logging.DEBUG, logging.INFO,\n-        logging.WARNING, logging.ERROR and logging.CRITICAL.\n \n-        * `msg` should be a string that can contain different formatting\n-        placeholders. This string, formatted with the provided `args`, is going\n-        to be the log message for that action.\n+        *   ``level`` is the log level for that action, you can use those from the \n+            `python logging library <https://docs.python.org/3/library/logging.html>`_ :\n+            ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``\n+            and ``logging.CRITICAL``.\n+\n+        *   ``msg`` should be a string that can contain different formatting placeholders. This string, formatted\n+            with the provided ``args``, is going to be the long message for that action.\n+\n+        *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``. The final log message is\n+            computed as ``msg % args``.\n \n-        * `args` should be a tuple or dict with the formatting placeholders for\n-        `msg`.  The final log message is computed as output['msg'] %\n-        output['args'].\n     \"\"\"\n \n     def crawled(self, request, response, spider):\n+        \"\"\"\n+        ``crawled`` is called to log message when the crawler finds a webpage.\n+        \"\"\"\n         request_flags = ' %s' % str(request.flags) if request.flags else ''\n         response_flags = ' %s' % str(response.flags) if response.flags else ''\n         return {\n@@ -49,6 +53,9 @@ class LogFormatter(object):\n         }\n \n     def scraped(self, item, response, spider):\n+        \"\"\"\n+        ``scraped`` is called to log message when an item is scraped by a spider.\n+        \"\"\"\n         if isinstance(response, Failure):\n             src = response.getErrorMessage()\n         else:\n@@ -63,6 +70,9 @@ class LogFormatter(object):\n         }\n \n     def dropped(self, item, exception, response, spider):\n+        \"\"\"\n+        ``dropped`` is called to log message when an item is dropped while it is passing through the item pipeline. \n+        \"\"\"\n         return {\n             'level': logging.WARNING,\n             'msg': DROPPEDMSG,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ef38d925e0ded380a7cebabe3aab2340d4f3d37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 61 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -7,9 +7,7 @@ RETRY_TIMES - how many times to retry a failed page\n RETRY_HTTP_CODES - which HTTP response codes to retry\n \n Failed pages are collected on the scraping process and rescheduled at the end,\n-once the spider has finished crawling all regular (non failed) pages. Once\n-there is no more failed pages to retry this middleware sends a signal\n-(retry_complete), so other extensions could connect to that signal.\n+once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n import logging\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82049e9c41f878d84f0fe10f827c6fe2a33f7ba6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 227 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -30,12 +30,24 @@ class LogFormatter(object):\n         *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``. The final log message is\n             computed as ``msg % args``.\n \n+    Here is an example on how to create a custom log formatter to lower the severity level of the log message\n+    when an item is dropped from the pipeline::\n+\n+            class PoliteLogFormatter(logformatter.LogFormatter):\n+                def dropped(self, item, exception, response, spider):\n+                    return {\n+                        'level': logging.INFO, # lowering the level from logging.WARNING\n+                        'msg': u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\",\n+                        'args': {\n+                            'exception': exception,\n+                            'item': item,\n+                        }\n+                    }\n+\n     \"\"\"\n \n     def crawled(self, request, response, spider):\n-        \"\"\"\n-        ``crawled`` is called to log message when the crawler finds a webpage.\n-        \"\"\"\n+        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n         request_flags = ' %s' % str(request.flags) if request.flags else ''\n         response_flags = ' %s' % str(response.flags) if response.flags else ''\n         return {\n@@ -53,9 +65,7 @@ class LogFormatter(object):\n         }\n \n     def scraped(self, item, response, spider):\n-        \"\"\"\n-        ``scraped`` is called to log message when an item is scraped by a spider.\n-        \"\"\"\n+        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n         if isinstance(response, Failure):\n             src = response.getErrorMessage()\n         else:\n@@ -70,9 +80,7 @@ class LogFormatter(object):\n         }\n \n     def dropped(self, item, exception, response, spider):\n-        \"\"\"\n-        ``dropped`` is called to log message when an item is dropped while it is passing through the item pipeline. \n-        \"\"\"\n+        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n         return {\n             'level': logging.WARNING,\n             'msg': DROPPEDMSG,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9cd4ee03aa41e27bea0408b10970ec5bedf35d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 242 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -23,15 +23,14 @@ class LogFormatter(object):\n         `python logging library <https://docs.python.org/3/library/logging.html>`_ :\n         ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR``\n         and ``logging.CRITICAL``.\n+    *   ``msg`` should be a string that can contain different formatting placeholders.\n+        This string, formatted with the provided ``args``, is going to be the long message\n+        for that action.\n+    *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n+        The final log message is computed as ``msg % args``.\n \n-        *   ``msg`` should be a string that can contain different formatting placeholders. This string, formatted\n-            with the provided ``args``, is going to be the long message for that action.\n-\n-        *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``. The final log message is\n-            computed as ``msg % args``.\n-\n-    Here is an example on how to create a custom log formatter to lower the severity level of the log message\n-    when an item is dropped from the pipeline::\n+    Here is an example on how to create a custom log formatter to lower the severity level of\n+    the log message when an item is dropped from the pipeline::\n \n             class PoliteLogFormatter(logformatter.LogFormatter):\n                 def dropped(self, item, exception, response, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35f7595dbebe7012c8f66336b76625b089a1fad3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 960 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -28,16 +28,16 @@ class FormRequest(Request):\n \n         if formdata:\n             items = formdata.items() if isinstance(formdata, dict) else formdata\n-            querystr = _urlencode(items, self.encoding)\n+            form_query_str = _urlencode(items, self.encoding)\n             if self.method == 'POST':\n                 self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n-                self._set_body(querystr)\n+                self._set_body(form_query_str)\n             else:\n                 url_split = urlsplit(self.url)\n-                formdata_keys = set(dict(parse_qsl(querystr)).keys())\n+                formdata_keys = set(dict(parse_qsl(form_query_str)).keys())\n                 items = [(k, v) for k, v in parse_qsl(url_split.query) if k not in formdata_keys]\n-                query_str = _urlencode(items, self.encoding)\n-                query = (query_str + '&' + querystr) if query_str else querystr\n+                url_query_str = _urlencode(items, self.encoding)\n+                query = (url_query_str + '&' + form_query_str) if url_query_str else form_query_str\n                 self._set_url(urlunsplit(url_split._replace(query = query)))\n \n     @classmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43fd6229684b3ccca564524fc92faf009a8c4c97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 36 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,12 +24,23 @@ class Rule(object):\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n-        self.process_request = process_request\n+        self.process_request_function = process_request\n         if follow is None:\n             self.follow = False if callback else True\n         else:\n             self.follow = follow\n \n+    def process_request(self, request, response):\n+        \"\"\"\n+        Wrapper around the request processing function to maintain backward compatibility\n+        with functions that do not take a Response object as parameter.\n+        \"\"\"\n+        argcount = self.process_request_function.__code__.co_argcount\n+        if getattr(self.process_request_function, '__self__', None):\n+            argcount = argcount - 1\n+        args = [request] if argcount == 1 else [request, response]\n+        return self.process_request_function(*args)\n+\n \n class CrawlSpider(Spider):\n \n@@ -65,7 +76,7 @@ class CrawlSpider(Spider):\n             for link in links:\n                 seen.add(link)\n                 r = self._build_request(n, link)\n-                yield rule.process_request(r)\n+                yield rule.process_request(r, response)\n \n     def _response_downloaded(self, response):\n         rule = self._rules[response.meta['rule']]\n@@ -93,7 +104,7 @@ class CrawlSpider(Spider):\n         for rule in self._rules:\n             rule.callback = get_method(rule.callback)\n             rule.process_links = get_method(rule.process_links)\n-            rule.process_request = get_method(rule.process_request)\n+            rule.process_request_function = get_method(rule.process_request_function)\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22fda61d62a2b230b0e8588eabb0d71cb77141b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 98 | Churn Cumulative: 469 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -263,6 +263,104 @@ class CrawlSpiderTest(SpiderTest):\n                            'http://example.org/about.html',\n                            'http://example.org/nofollow.html'])\n \n+    def test_process_request(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+\n+        def process_request_change_domain(request):\n+            return request.replace(url=request.url.replace('.org', '.com'))\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(LinkExtractor(), process_request=process_request_change_domain),\n+            )\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEqual([r.url for r in output],\n+                         ['http://example.com/somepage/item/12.html',\n+                          'http://example.com/about.html',\n+                          'http://example.com/nofollow.html'])\n+\n+    def test_process_request_with_response(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+\n+        def process_request_meta_response_class(request, response):\n+            request.meta['response_class'] = response.__class__.__name__\n+            return request\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(LinkExtractor(), process_request=process_request_meta_response_class),\n+            )\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEqual([r.url for r in output],\n+                         ['http://example.org/somepage/item/12.html',\n+                          'http://example.org/about.html',\n+                          'http://example.org/nofollow.html'])\n+        self.assertEqual([r.meta['response_class'] for r in output],\n+                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])\n+\n+    def test_process_request_instance_method(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(LinkExtractor(), process_request='process_request_upper'),\n+            )\n+\n+            def process_request_upper(self, request):\n+                return request.replace(url=request.url.upper())\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEqual([r.url for r in output],\n+                         ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n+                          'http://EXAMPLE.ORG/ABOUT.HTML',\n+                          'http://EXAMPLE.ORG/NOFOLLOW.HTML'])\n+\n+    def test_process_request_instance_method_with_response(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(LinkExtractor(), process_request='process_request_meta_response_class'),\n+            )\n+\n+            def process_request_meta_response_class(self, request, response):\n+                request.meta['response_class'] = response.__class__.__name__\n+                return request\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEqual([r.url for r in output],\n+                         ['http://example.org/somepage/item/12.html',\n+                          'http://example.org/about.html',\n+                          'http://example.org/nofollow.html'])\n+        self.assertEqual([r.meta['response_class'] for r in output],\n+                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])\n+\n     def test_follow_links_attribute_population(self):\n         crawler = get_crawler()\n         spider = self.spider_class.from_crawler(crawler, 'example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
