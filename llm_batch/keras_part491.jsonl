{"custom_id": "keras#f9fcf9bd3be19fed53354fb93634fb4695f10683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 38 | Churn Cumulative: 4673 | Contributors (this commit): 42 | Commits (past 90d): 10 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -914,13 +914,14 @@ def warmstart_embedding_matrix(\n             indices_new_vocabulary.append(int(index))\n \n     # update embedding matrix\n+    if indices_base_vocabulary:\n         values_to_update = tf.gather(base_embeddings, indices_base_vocabulary)\n-    warmstarted_embedding_matrix = tf.tensor_scatter_nd_update(\n+        new_embedding = tf.tensor_scatter_nd_update(\n             new_embedding,\n             tf.expand_dims(indices_new_vocabulary, axis=1),\n             values_to_update,\n         )\n-    return warmstarted_embedding_matrix\n+    return new_embedding\n \n \n def convert_vocab_to_list(vocab):\n\n@@ -731,6 +731,39 @@ class LayerUtilsTest(tf.test.TestCase):\n             vectorized_vocab_base[1],\n         )\n \n+    def test_warmstart_with_nothing_in_common(self):\n+        vocab_base = np.array([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = np.array([\"d\", \"e\", \"f\", \"g\", \"h\"])\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        vectorized_vocab_new = np.random.rand(len(vocab_new), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=keras.initializers.Constant(\n+                vectorized_vocab_new\n+            ),\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix,\n+            vectorized_vocab_new,\n+        )\n+\n+    def test_warmstart_with_new_vocab_smaller(self):\n+        vocab_base = np.array([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = np.array([\"d\", \"e\", \"f\", \"a\"])\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=\"uniform\",\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[3],\n+            vectorized_vocab_base[1],\n+        )\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#218c1b45ef5821ecd53b1cb8143d81ce4f362302", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 39 | Churn Cumulative: 11728 | Contributors (this commit): 37 | Commits (past 90d): 30 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3404,35 +3404,16 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def _get_state(self):\n         \"\"\"Experimental method for getting the state of this layer object.\"\"\"\n         result = {}\n-        for child_attr, child_obj in self.__dict__.items():\n-            # TODO(rchao): Store non-variable states in the dict as well.\n-            if isinstance(child_obj, tf.Variable):\n-                result[child_attr] = child_obj.numpy()\n-            elif isinstance(child_obj, (list, tuple)):\n-                for k, contained_obj in enumerate(child_obj):\n-                    if isinstance(contained_obj, tf.Variable):\n-                        result[f\"{child_attr}-{k}\"] = contained_obj.numpy()\n-            elif isinstance(child_obj, dict):\n-                for k, v in child_obj.items():\n-                    if isinstance(v, tf.Variable):\n-                        result[f\"{child_attr}-{k}\"] = v.numpy()\n+        all_vars = self._trainable_weights + self._non_trainable_weights\n+        for i, v in enumerate(all_vars):\n+            result[str(i)] = v\n         return result\n \n     def _set_state(self, state):\n         \"\"\"Experimental method for setting the state of this layer object.\"\"\"\n-        for child_attr, child_obj in self.__dict__.items():\n-            # TODO(rchao): Retrieve non-variable states from the dict as well.\n-            # TODO(rchao): Give a warning for mismatches.\n-            if isinstance(child_obj, tf.Variable):\n-                child_obj.assign(state[child_attr])\n-            elif isinstance(child_obj, (list, tuple)):\n-                for k, contained_obj in enumerate(child_obj):\n-                    if isinstance(contained_obj, tf.Variable):\n-                        contained_obj.assign(state[f\"{child_attr}-{k}\"])\n-            elif isinstance(child_obj, dict):\n-                for k, v in child_obj.items():\n-                    if isinstance(v, tf.Variable):\n-                        child_obj[k].assign(state[f\"{child_attr}-{k}\"])\n+        all_vars = self._trainable_weights + self._non_trainable_weights\n+        for i, v in enumerate(all_vars):\n+            v.assign(state[str(i)])\n \n     def _save_state(self, dirpath):\n         filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n\n@@ -396,10 +396,10 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         input_layer = model.layers[0]\n         dense_layer = model.layers[1]\n         self.assertEmpty(input_layer._get_state().keys())\n-        self.assertIn(\"kernel\", dense_layer._get_state().keys())\n-        self.assertIn(\"bias\", dense_layer._get_state().keys())\n-        self.assertEqual(dense_layer._get_state()[\"kernel\"].shape, (4, 2))\n-        self.assertEqual(dense_layer._get_state()[\"bias\"].shape, (2,))\n+        self.assertIn(\"0\", dense_layer._get_state().keys())\n+        self.assertIn(\"1\", dense_layer._get_state().keys())\n+        self.assertEqual(dense_layer._get_state()[\"0\"].shape, (4, 2))\n+        self.assertEqual(dense_layer._get_state()[\"1\"].shape, (2,))\n \n     @tf.__internal__.distribute.combinations.generate(\n         tf.__internal__.test.combinations.combine(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40c54a663bc159b5629206580fa15d2c0af343fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10868 | Contributors (this commit): 22 | Commits (past 90d): 20 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -1818,7 +1818,7 @@ def identity(x, name=None):\n # tf.random.Generator to generate random numbers.\n # The legacy behavior is to use TF's legacy stateful RNG ops like\n # tf.random.uniform.\n-_USE_GENERATOR_FOR_RNG = False\n+_USE_GENERATOR_FOR_RNG = True\n \n # The global generator to create the seed when initializing the\n # tf.random.Genrator used by RandomGenerator. When tf.random.Generator becomes\n\n@@ -44,7 +44,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     `trainable` does not affect the layer's behavior, as Dropout does\n     not have any variables/weights that can be frozen during training.)\n \n-    >>> tf.random.set_seed(0)\n+    >>> tf.keras.utils.set_random_seed(0)\n     >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n     >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n     >>> print(data)\n@@ -57,7 +57,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     >>> print(outputs)\n     tf.Tensor(\n     [[ 0.    1.25]\n-     [ 2.5   3.75]\n+     [ 0.    3.75]\n      [ 5.    6.25]\n      [ 7.5   8.75]\n      [10.    0.  ]], shape=(5, 2), dtype=float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9de9148614562785b4e58e53cb2cd60052301729", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 10773 | Contributors (this commit): 35 | Commits (past 90d): 16 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -794,13 +794,14 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 textwrap.dedent(\n                     f\"\"\"\n           Layer {self.__class__.__name__} has arguments {extra_args}\n-          in `__init__` and therefore must override `get_config()`.\n+          in `__init__()` and therefore must override `get_config()` in\n+          order to be serializable.\n \n           Example:\n \n           class CustomLayer(keras.layers.Layer):\n-              def __init__(self, arg1, arg2):\n-                  super().__init__()\n+              def __init__(self, arg1, arg2, **kwargs):\n+                  super().__init__(**kwargs)\n                   self.arg1 = arg1\n                   self.arg2 = arg2\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78688e7bc057ecb8a9ddcfdfbe157737aadf81a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 788 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -418,7 +418,7 @@ class BatchNormalizationV2Test(test_combinations.TestCase):\n         norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n         _ = norm(np.ones((1, 28, 28)))\n \n-        with self.assertRaisesRegex(Exception, \"requested shape requires\"):\n+        with self.assertRaisesRegex(Exception, \"Reshape\"):\n             norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n             _ = norm(np.ones((1, 28, 28)), training=True)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42339994bfeb72d128f5fa08ad20b61dd51a2825", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 48 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 55 | Churn Cumulative: 9415 | Contributors (this commit): 114 | Commits (past 90d): 15 | Contributors (cumulative): 114 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# flake8: noqa\n # Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -2349,12 +2350,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         write_steps_per_second: whether to log the training steps per second\n           into Tensorboard. This supports both epoch and batch frequency\n           logging.\n-        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n-          writes the losses and metrics to TensorBoard after each batch. The\n-          same applies for `'epoch'`. If using an integer, let's say `1000`, the\n-          callback will write the metrics and losses to TensorBoard every 1000\n-          batches. Note that writing too frequently to TensorBoard can slow down\n-          your training.\n+        update_freq: **disabled**\n+\n+          Warning: Batch-level summary writing using `update_freq` is\n+          currently unsupported. A suggested workaround is shown in the\n+          [TensorBoard Scalars tutorial](https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging). # pylint: disable=protected-access\n+\n         profile_batch: Profile the batch(es) to sample compute characteristics.\n           profile_batch must be a non-negative integer or a tuple of integers.\n           A pair of positive integers signify a range of batches to profile.\n@@ -2376,48 +2377,6 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     # Then run the tensorboard command to view the visualizations.\n     ```\n \n-    Custom batch-level summaries in a subclassed Model:\n-\n-    ```python\n-    class MyModel(tf.keras.Model):\n-\n-      def build(self, _):\n-        self.dense = tf.keras.layers.Dense(10)\n-\n-      def call(self, x):\n-        outputs = self.dense(x)\n-        tf.summary.histogram('outputs', outputs)\n-        return outputs\n-\n-    model = MyModel()\n-    model.compile('sgd', 'mse')\n-\n-    # Make sure to set `update_freq=N` to log a batch-level summary every N\n-    # batches.  In addition to any `tf.summary` contained in `Model.call`,\n-    # metrics added in `Model.compile` will be logged every N batches.\n-    tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n-    model.fit(x_train, y_train, callbacks=[tb_callback])\n-    ```\n-\n-    Custom batch-level summaries in a Functional API Model:\n-\n-    ```python\n-    def my_summary(x):\n-      tf.summary.histogram('x', x)\n-      return x\n-\n-    inputs = tf.keras.Input(10)\n-    x = tf.keras.layers.Dense(10)(inputs)\n-    outputs = tf.keras.layers.Lambda(my_summary)(x)\n-    model = tf.keras.Model(inputs, outputs)\n-    model.compile('sgd', 'mse')\n-\n-    # Make sure to set `update_freq=N` to log a batch-level summary every N\n-    # batches. In addition to any `tf.summary` contained in `Model.call`,\n-    # metrics added in `Model.compile` will be logged every N batches.\n-    tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n-    model.fit(x_train, y_train, callbacks=[tb_callback])\n-    ```\n \n     Profiling:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd588d3c696125e633a964ad7a09d2213afd2c51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 40 | Files Changed: 3 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/5 | Churn Δ: 59 | Churn Cumulative: 4662 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -878,9 +878,8 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n                         )\n                     )\n                     and (\n-                        mask is None\n-                        or gru_lstm_utils.is_cudnn_supported_inputs(\n-                            mask, self.time_major\n+                        gru_lstm_utils.is_cudnn_supported_inputs(\n+                            mask, self.time_major, sequence_lengths\n                         )\n                     )\n                 )\n@@ -1215,20 +1214,6 @@ def gru_with_backend_selection(\n         return_sequences,\n     ):\n         \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n-        if mask is None:\n-            return gpu_gru(\n-                inputs=inputs,\n-                init_h=init_h,\n-                kernel=kernel,\n-                recurrent_kernel=recurrent_kernel,\n-                bias=bias,\n-                mask=mask,\n-                time_major=time_major,\n-                go_backwards=go_backwards,\n-                sequence_lengths=sequence_lengths,\n-                return_sequences=return_sequences,\n-            )\n-\n         def cudnn_gru_fn():\n             return gpu_gru(\n                 inputs=inputs,\n@@ -1259,7 +1244,7 @@ def gru_with_backend_selection(\n             )\n \n         return tf.cond(\n-            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n+            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major, sequence_lengths),\n             true_fn=cudnn_gru_fn,\n             false_fn=standard_gru_fn,\n         )\n\n@@ -168,7 +168,19 @@ def has_fully_masked_sequence(mask):\n     return tf.reduce_any(tf.reduce_all(tf.logical_not(mask), axis=1))\n \n \n-def is_cudnn_supported_inputs(mask, time_major):\n+def is_cudnn_supported_inputs(mask, time_major, sequence_lengths):\n+    if tf.sysconfig.get_build_info()['is_rocm_build']:\n+       if not time_major:\n+          return tf.constant(False)\n+       if mask!=None:\n+          return tf.reduce_all(mask)\n+       elif sequence_lengths!=None:\n+          return tf.math.equal(tf.reduce_min(sequence_lengths), tf.reduce_max(sequence_lengths))\n+       else:\n+          return tf.constant(True)\n+\n+    if mask==None:\n+        return tf.constant(True)\n     if time_major:\n         mask = tf.transpose(mask)\n \n\n@@ -720,11 +720,8 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n                                 and tf.config.list_logical_devices(\"GPU\")\n                             )\n                         )\n-                        and (\n-                            mask is None\n-                            or gru_lstm_utils.is_cudnn_supported_inputs(\n-                                mask, self.time_major\n-                            )\n+                        and gru_lstm_utils.is_cudnn_supported_inputs(\n+                                mask, self.time_major, row_lengths\n                             )\n                     )\n                     # Under eager context, check the device placement and prefer\n@@ -1256,21 +1253,6 @@ def lstm_with_backend_selection(\n         return_sequences,\n     ):\n         \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n-        if mask is None:\n-            return gpu_lstm(\n-                inputs=inputs,\n-                init_h=init_h,\n-                init_c=init_c,\n-                kernel=kernel,\n-                recurrent_kernel=recurrent_kernel,\n-                bias=bias,\n-                mask=mask,\n-                time_major=time_major,\n-                go_backwards=go_backwards,\n-                sequence_lengths=sequence_lengths,\n-                return_sequences=return_sequences,\n-            )\n-\n         def cudnn_lstm_fn():\n             return gpu_lstm(\n                 inputs=inputs,\n@@ -1303,7 +1285,7 @@ def lstm_with_backend_selection(\n             )\n \n         return tf.cond(\n-            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n+            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major, sequence_lengths),\n             true_fn=cudnn_lstm_fn,\n             false_fn=stardard_lstm_fn,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c06aa015e900a2029b5b379f374e5d4dc615fcbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 96 | Files Changed: 63 | Hunks: 54 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1164/225 | Churn Δ: 198 | Churn Cumulative: 79438 | Contributors (this commit): 190 | Commits (past 90d): 157 | Contributors (cumulative): 509 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras.distribute import saved_model_test_base as test_base\n-from keras.saving import save\n+from keras.saving.legacy import save\n from keras.testing_infra import test_utils\n \n \n\n@@ -23,7 +23,7 @@ tf.saved_model.save().\n import tensorflow.compat.v2 as tf\n \n from keras.distribute import saved_model_test_base as test_base\n-from keras.saving import save\n+from keras.saving.legacy import save\n from keras.testing_infra import test_utils\n \n _DEFAULT_FUNCTION_KEY = \"serving_default\"\n\n@@ -41,7 +41,7 @@ from keras.engine import node as node_module\n from keras.mixed_precision import autocast_variable\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import object_identity\n\n@@ -611,7 +611,7 @@ def is_subclassed(layer):\n \n def from_saved_model(layer):\n     \"\"\"Returns whether the layer is loaded from a SavedModel.\"\"\"\n-    return layer.__module__.find(\"keras.saving.saved_model\") != -1\n+    return layer.__module__.find(\"keras.saving.legacy.saved_model\") != -1\n \n \n def check_graph_consistency(tensor=None, method=\"add_loss\", force_raise=False):\n\n@@ -33,7 +33,7 @@ from keras.engine import input_spec\n from keras.mixed_precision import autocast_variable\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import object_identity\n\n@@ -22,7 +22,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import losses as losses_mod\n from keras import metrics as metrics_mod\n-from keras.saving.experimental import saving_lib\n+from keras.saving.legacy.experimental import saving_lib\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n\n@@ -33,8 +33,8 @@ from keras.engine import input_spec\n from keras.engine import node as node_module\n from keras.engine import training as training_lib\n from keras.engine import training_utils\n-from keras.saving.saved_model import json_utils\n-from keras.saving.saved_model import network_serialization\n+from keras.saving.legacy.saved_model import json_utils\n+from keras.saving.legacy.saved_model import network_serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n\n@@ -28,7 +28,7 @@ from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.engine import sequential\n from keras.engine import training as training_lib\n-from keras.saving import save\n+from keras.saving.legacy import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import layer_utils\n\n@@ -22,7 +22,7 @@ from keras.distribute import distributed_training_utils\n from keras.engine import base_layer\n from keras.engine import keras_tensor\n from keras.engine import node as node_module\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n \n\n@@ -20,7 +20,7 @@ from keras import backend\n from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.layers import core\n-from keras.saving import model_config\n+from keras.saving.legacy import model_config\n from keras.testing_infra import test_combinations\n \n # isort: off\n\n@@ -25,7 +25,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.engine import base_layer_utils\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.saved_model import json_utils\n from keras.utils import tf_utils\n \n _CONSTANT_VALUE = \"_CONSTANT_VALUE\"\n\n@@ -25,8 +25,8 @@ from keras.engine import functional\n from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_utils\n-from keras.saving.experimental import saving_lib\n-from keras.saving.saved_model import model_serialization\n+from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n\n@@ -39,13 +39,13 @@ from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_experimental import (\n     optimizer as optimizer_experimental,\n )\n-from keras.saving import hdf5_format\n from keras.saving import pickle_utils\n-from keras.saving import save\n-from keras.saving import saving_utils\n-from keras.saving.experimental import saving_lib\n-from keras.saving.saved_model import json_utils\n-from keras.saving.saved_model import model_serialization\n+from keras.saving.legacy import hdf5_format\n+from keras.saving.legacy import save\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.legacy.saved_model import json_utils\n+from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n\n@@ -36,8 +36,8 @@ from keras.engine import training_utils_v1\n from keras.mixed_precision import loss_scale_optimizer\n from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_v2 import optimizer_v2\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import model_serialization\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import data_utils\n from keras.utils import layer_utils\n from keras.utils import losses_utils\n\n@@ -24,7 +24,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.feature_column import base_feature_layer as kfc\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.saved_model import json_utils\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n\n@@ -24,7 +24,7 @@ from absl.testing import parameterized\n \n import keras\n from keras.feature_column import sequence_feature_column as ksfc\n-from keras.saving import model_config\n+from keras.saving.legacy import model_config\n from keras.testing_infra import test_combinations\n \n \n\n@@ -29,7 +29,7 @@ from keras.engine import base_layer_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.layers import kernelized as kernel_layers\n-from keras.saving import save\n+from keras.saving.legacy import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import kernelized_utils\n\n@@ -24,7 +24,7 @@ from keras import backend\n from keras.engine import base_layer_utils\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n \n\n@@ -22,7 +22,7 @@ from keras import backend\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.layers.preprocessing import string_lookup\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n \n\n@@ -26,7 +26,7 @@ from keras.engine.input_spec import InputSpec\n from keras.layers.rnn import rnn_utils\n from keras.layers.rnn.dropout_rnn_cell_mixin import DropoutRNNCellMixin\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import layer_serialization\n from keras.utils import generic_utils\n \n # isort: off\n\n@@ -510,7 +510,7 @@ class CuDNNV1OnlyTest(test_combinations.TestCase):\n \n         def assert_not_compatible(src, dest, message):\n             with self.assertRaises(ValueError) as ex:\n-                keras.saving.hdf5_format.preprocess_weights_for_loading(\n+                keras.saving.legacy.hdf5_format.preprocess_weights_for_loading(\n                     dest, get_layer_weights(src)\n                 )\n             self.assertIn(message, str(ex.exception))\n\n@@ -50,7 +50,7 @@ from keras.layers.preprocessing import text_vectorization\n from keras.layers.rnn import cell_wrappers\n from keras.layers.rnn import gru\n from keras.layers.rnn import lstm\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.saved_model import json_utils\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n \n\n@@ -23,7 +23,7 @@ from absl.testing import parameterized\n import keras\n from keras.engine import keras_tensor\n from keras.optimizers.optimizer_v2 import adam\n-from keras.saving import model_config\n+from keras.saving.legacy import model_config\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -22,7 +22,7 @@ import functools\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.saving.experimental import saving_lib\n+from keras.saving.legacy.experimental import saving_lib\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import deserialize_keras_object\n\n@@ -30,7 +30,7 @@ from keras.dtensor import utils as dtensor_utils\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import keras_tensor\n-from keras.saving.saved_model import metric_serialization\n+from keras.saving.legacy.saved_model import metric_serialization\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import metrics_utils\n\n@@ -43,7 +43,7 @@ from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.saving import object_registration\n-from keras.saving import save\n+from keras.saving.legacy import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -29,8 +29,8 @@ from keras.models.cloning import clone_and_build_model\n from keras.models.cloning import clone_model\n from keras.models.cloning import share_weights\n from keras.models.sharpness_aware_minimization import SharpnessAwareMinimization\n-from keras.saving.model_config import model_from_config\n-from keras.saving.model_config import model_from_json\n-from keras.saving.model_config import model_from_yaml\n-from keras.saving.save import load_model\n-from keras.saving.save import save_model\n+from keras.saving.legacy.model_config import model_from_config\n+from keras.saving.legacy.model_config import model_from_json\n+from keras.saving.legacy.model_config import model_from_yaml\n+from keras.saving.legacy.save import load_model\n+from keras.saving.legacy.save import save_model\n\n@@ -27,8 +27,12 @@ import keras\n from keras import losses\n from keras.engine import base_layer\n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.saving.experimental.serialization_lib import deserialize_keras_object\n-from keras.saving.experimental.serialization_lib import serialize_keras_object\n+from keras.saving.legacy.experimental.serialization_lib import (\n+    deserialize_keras_object,\n+)\n+from keras.saving.legacy.experimental.serialization_lib import (\n+    serialize_keras_object,\n+)\n from keras.utils import io_utils\n \n # isort: off\n\n@@ -27,8 +27,8 @@ import keras\n from keras import backend\n from keras.optimizers.optimizer_experimental import adam\n from keras.saving import object_registration\n-from keras.saving.experimental import saving_lib\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.legacy.saved_model import json_utils\n from keras.testing_infra import test_utils\n from keras.utils import io_utils\n \n\n\n@@ -21,7 +21,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.saving.experimental import serialization_lib\n+from keras.saving.legacy.experimental import serialization_lib\n from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n\n@@ -26,9 +26,9 @@ from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_experimental import (\n     optimizer as optimizer_experimental,\n )\n-from keras.saving import model_config as model_config_lib\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy import model_config as model_config_lib\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import json_utils\n from keras.utils.generic_utils import LazyLoader\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n \n\n\n\n\n@@ -16,12 +16,12 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving import hdf5_format\n from keras.saving import object_registration\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import load as saved_model_load\n-from keras.saving.saved_model import load_context\n-from keras.saving.saved_model import save as saved_model_save\n+from keras.saving.legacy import hdf5_format\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import load as saved_model_load\n+from keras.saving.legacy.saved_model import load_context\n+from keras.saving.legacy.saved_model import save as saved_model_save\n from keras.utils import generic_utils\n from keras.utils import traceback_utils\n from keras.utils.io_utils import path_to_string\n\n@@ -35,9 +35,9 @@ from keras.feature_column import sequence_feature_column as ksfc\n from keras.layers import core\n from keras.optimizers import optimizer_v1\n from keras.premade_models.linear import LinearModel\n-from keras.saving import model_config\n from keras.saving import object_registration\n-from keras.saving import save\n+from keras.saving.legacy import model_config\n+from keras.saving.legacy import save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n\n@@ -25,7 +25,7 @@ from absl.testing import parameterized\n import keras\n from keras.engine import training\n from keras.optimizers import optimizer_v1\n-from keras.saving import hdf5_format\n+from keras.saving.legacy import hdf5_format\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -21,8 +21,8 @@ from __future__ import print_function\n \n import abc\n \n-from keras.saving.saved_model import json_utils\n-from keras.saving.saved_model import utils\n+from keras.saving.legacy.saved_model import json_utils\n+from keras.saving.legacy.saved_model import utils\n \n \n class SavedModelSaver(object, metaclass=abc.ABCMeta):\n\n\n\n\n\n@@ -19,7 +19,7 @@ import enum\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.saved_model import json_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -17,10 +17,10 @@\n import tensorflow.compat.v2 as tf\n \n from keras.mixed_precision import policy\n-from keras.saving.saved_model import base_serialization\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import save_impl\n-from keras.saving.saved_model import serialized_attributes\n+from keras.saving.legacy.saved_model import base_serialization\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import save_impl\n+from keras.saving.legacy.saved_model import serialized_attributes\n from keras.utils import generic_utils\n \n \n\n@@ -29,11 +29,13 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import object_registration\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import json_utils\n-from keras.saving.saved_model import utils\n-from keras.saving.saved_model.serialized_attributes import CommonEndpoints\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import json_utils\n+from keras.saving.legacy.saved_model import utils\n+from keras.saving.legacy.saved_model.serialized_attributes import (\n+    CommonEndpoints,\n+)\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import metrics_utils\n\n\n@@ -17,8 +17,8 @@\n import tensorflow.compat.v2 as tf\n \n from keras.saving import object_registration\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import layer_serialization\n \n \n class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n\n@@ -14,10 +14,10 @@\n # ==============================================================================\n \"\"\"Classes and functions implementing to Model SavedModel serialization.\"\"\"\n \n-from keras.saving import saving_utils\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import layer_serialization\n-from keras.saving.saved_model import save_impl\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import layer_serialization\n+from keras.saving.legacy.saved_model import save_impl\n \n \n class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n\n@@ -14,8 +14,8 @@\n # ==============================================================================\n \"\"\"Classes and functions implementing to Network SavedModel serialization.\"\"\"\n \n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import model_serialization\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import model_serialization\n \n \n # FunctionalModel serialization is pretty much the same as Model serialization.\n\n\n@@ -28,7 +28,7 @@ from absl.testing import parameterized\n \n import keras\n from keras import backend\n-from keras.saving.saved_model import load as keras_load\n+from keras.saving.legacy.saved_model import load as keras_load\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import CustomObjectScope\n\n@@ -23,10 +23,10 @@ from keras import backend\n from keras.layers import serialization\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import save_impl\n-from keras.saving.saved_model import utils\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import save_impl\n+from keras.saving.legacy.saved_model import utils\n from keras.utils.generic_utils import LazyLoader\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n \n\n@@ -29,11 +29,11 @@ from keras import backend\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n from keras.mixed_precision import autocast_variable\n-from keras.saving import saving_utils\n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import load as keras_load\n-from keras.saving.saved_model import serialized_attributes\n-from keras.saving.saved_model import utils\n+from keras.saving.legacy import saving_utils\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import load as keras_load\n+from keras.saving.legacy.saved_model import serialized_attributes\n+from keras.saving.legacy.saved_model import utils\n from keras.utils import layer_utils\n from keras.utils import tf_contextlib\n from keras.utils import tf_utils\n\n@@ -37,9 +37,9 @@ from keras.feature_column.dense_features import DenseFeatures\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import object_registration\n-from keras.saving.saved_model import json_utils\n-from keras.saving.saved_model import load as keras_load\n-from keras.saving.saved_model import save_impl as keras_save\n+from keras.saving.legacy.saved_model import json_utils\n+from keras.saving.legacy.saved_model import load as keras_load\n+from keras.saving.legacy.saved_model import save_impl as keras_save\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import control_flow_util\n\n@@ -17,9 +17,9 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving.saved_model import constants\n-from keras.saving.saved_model import order_preserving_set as ops\n-from keras.saving.saved_model import save_impl\n+from keras.saving.legacy.saved_model import constants\n+from keras.saving.legacy.saved_model import order_preserving_set as ops\n+from keras.saving.legacy.saved_model import save_impl\n from keras.utils.generic_utils import LazyLoader\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n\n\n\n@@ -24,7 +24,7 @@ from keras import backend\n from keras.engine import sequential\n from keras.feature_column import dense_features\n from keras.optimizers.optimizer_v2 import gradient_descent\n-from keras.saving import saving_utils\n+from keras.saving.legacy import saving_utils\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n@@ -18,7 +18,7 @@ import tempfile\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving.experimental import saving_lib\n+from keras.saving.legacy.experimental import saving_lib\n \n \n def deserialize_model_from_bytecode(serialized_model):\n\n@@ -22,7 +22,7 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n-from keras.saving.saved_model import json_utils\n+from keras.saving.legacy.saved_model import json_utils\n from keras.testing_infra import test_combinations\n \n \n\n@@ -27,7 +27,7 @@ PIP_EXCLUDED_FILES = frozenset(\n         \"keras/api/create_python_api_wrapper.py\",\n         \"keras/applications/efficientnet_weight_update_util.py\",\n         \"keras/distribute/tpu_strategy_test_utils.py\",\n-        \"keras/saving/saved_model/create_test_saved_model.py\",\n+        \"keras/saving/legacy/saved_model/create_test_saved_model.py\",\n         \"keras/tools/pip_package/setup.py\",\n         \"keras/tools/pip_package/create_pip_helper.py\",\n     ]\n\n@@ -189,7 +189,7 @@ class DistributeKplTestUtils(tf.test.TestCase):\n         )\n \n         # Test the saved_model.\n-        loaded_serving_fn = keras.saving.save.load_model(\n+        loaded_serving_fn = keras.saving.legacy.save.load_model(\n             saved_model_dir\n         ).signatures[\"serving_default\"]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaaea7fd451adb82149603efa4478a820651be79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 13 | Files Changed: 9 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): -48/0 | Churn Δ: 22 | Churn Cumulative: 34721 | Contributors (this commit): 160 | Commits (past 90d): 97 | Contributors (cumulative): 218 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import losses as losses_mod\n from keras import metrics as metrics_mod\n-from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.experimental import saving_lib\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n\n@@ -25,7 +25,7 @@ from keras.engine import functional\n from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_utils\n-from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.experimental import saving_lib\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n\n@@ -40,10 +40,10 @@ from keras.optimizers.optimizer_experimental import (\n     optimizer as optimizer_experimental,\n )\n from keras.saving import pickle_utils\n+from keras.saving.experimental import saving_lib\n from keras.saving.legacy import hdf5_format\n from keras.saving.legacy import save\n from keras.saving.legacy import saving_utils\n-from keras.saving.legacy.experimental import saving_lib\n from keras.saving.legacy.saved_model import json_utils\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n\n@@ -22,7 +22,7 @@ import functools\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.experimental import saving_lib\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import deserialize_keras_object\n\n@@ -27,12 +27,8 @@ import keras\n from keras import losses\n from keras.engine import base_layer\n from keras.optimizers.optimizer_experimental import optimizer\n-from keras.saving.legacy.experimental.serialization_lib import (\n-    deserialize_keras_object,\n-)\n-from keras.saving.legacy.experimental.serialization_lib import (\n-    serialize_keras_object,\n-)\n+from keras.saving.experimental.serialization_lib import deserialize_keras_object\n+from keras.saving.experimental.serialization_lib import serialize_keras_object\n from keras.utils import io_utils\n \n # isort: off\n\n@@ -27,7 +27,7 @@ import keras\n from keras import backend\n from keras.optimizers.optimizer_experimental import adam\n from keras.saving import object_registration\n-from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.experimental import saving_lib\n from keras.saving.legacy.saved_model import json_utils\n from keras.testing_infra import test_utils\n from keras.utils import io_utils\n\n\n@@ -21,7 +21,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.saving.legacy.experimental import serialization_lib\n+from keras.saving.experimental import serialization_lib\n from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n\n@@ -18,7 +18,7 @@ import tempfile\n \n import tensorflow.compat.v2 as tf\n \n-from keras.saving.legacy.experimental import saving_lib\n+from keras.saving.experimental import saving_lib\n \n \n def deserialize_model_from_bytecode(serialized_model):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fba19769b0747248411b2cb9837e67f364c7846b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2569 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -806,7 +806,7 @@ class BNTest(tf.test.TestCase):\n         )\n \n         updates = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n-        all_vars = dict([(v.name, v) for v in tf.compat.v1.global_variables()])\n+        all_vars = {v.name: v for v in tf.compat.v1.global_variables()}\n         moving_mean = all_vars[\"bn/moving_mean:0\"]\n         moving_variance = all_vars[\"bn/moving_variance:0\"]\n         beta = all_vars[\"bn/beta:0\"]\n@@ -873,7 +873,7 @@ class BNTest(tf.test.TestCase):\n         updates = tf.compat.v1.get_collection(\n             tf.compat.v1.GraphKeys.UPDATE_OPS\n         )[-2:]\n-        all_vars = dict([(v.name, v) for v in tf.compat.v1.global_variables()])\n+        all_vars = {v.name: v for v in tf.compat.v1.global_variables()}\n         moving_mean = all_vars[\"bn/moving_mean:0\"]\n         moving_variance = all_vars[\"bn/moving_variance:0\"]\n         beta = all_vars[\"bn/beta:0\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf6ab7ea647d59282f326359e5b4907c31ef4c8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 23 | Files Changed: 4 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 26 | Churn Cumulative: 13195 | Contributors (this commit): 27 | Commits (past 90d): 66 | Contributors (cumulative): 50 | DMM Complexity: 0.875\n\nDIFF:\n@@ -1818,7 +1818,7 @@ def identity(x, name=None):\n # tf.random.Generator to generate random numbers.\n # The legacy behavior is to use TF's legacy stateful RNG ops like\n # tf.random.uniform.\n-_USE_GENERATOR_FOR_RNG = True\n+_USE_GENERATOR_FOR_RNG = False\n \n # The global generator to create the seed when initializing the\n # tf.random.Genrator used by RandomGenerator. When tf.random.Generator becomes\n\n@@ -44,7 +44,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     `trainable` does not affect the layer's behavior, as Dropout does\n     not have any variables/weights that can be frozen during training.)\n \n-    >>> tf.keras.utils.set_random_seed(0)\n+    >>> tf.random.set_seed(0)\n     >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n     >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n     >>> print(data)\n@@ -57,7 +57,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     >>> print(outputs)\n     tf.Tensor(\n     [[ 0.    1.25]\n-     [ 0.    3.75]\n+     [ 2.5   3.75]\n      [ 5.    6.25]\n      [ 7.5   8.75]\n      [10.    0.  ]], shape=(5, 2), dtype=float32)\n\n@@ -1092,18 +1092,8 @@ class Optimizer(_BaseOptimizer):\n \n         # Get the distributed variable if it exists.\n         # TODO(b/197554203): replace _distributed_container() with a public api.\n-        # TODO(b/246438937): Remove the first branch after tf-nightly is\n-        # updated.\n         if hasattr(variable, \"_distributed_container\"):\n             variable = variable._distributed_container()\n-        else:\n-            try:\n-                if hasattr(variable, \"handle\") and hasattr(\n-                    variable.handle, \"_distributed_container\"\n-                ):\n-                    variable = variable.handle._distributed_container()\n-            except ValueError:\n-                pass\n         return super()._var_key(variable)\n \n     def aggregate_gradients(self, grads_and_vars):\n\n@@ -1652,18 +1652,8 @@ def _var_key(var):\n     \"\"\"\n \n     # Get the distributed variable if it exists.\n-    # TODO(b/246438937): Remove the first branch after tf-nightly is\n-    # updated.\n     if hasattr(var, \"_distributed_container\"):\n         var = var._distributed_container()\n-    else:\n-        try:\n-            if hasattr(var, \"handle\") and hasattr(\n-                var.handle, \"_distributed_container\"\n-            ):\n-                var = var.handle._distributed_container()\n-        except ValueError:\n-            pass\n     if getattr(var, \"_in_graph_mode\", False):\n         return var._shared_name\n     return var._unique_id\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
