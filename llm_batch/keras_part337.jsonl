{"custom_id": "keras#9df106c6248cb8a723d153d6f1b05210162e435f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 28 | Churn Cumulative: 1783 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -14,14 +14,16 @@\n # ==============================================================================\n \"\"\"Normalization preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n-\n-import numpy as np\n from keras import backend\n from keras.engine import base_preprocessing_layer\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n+# pylint: disable=g-classes-have-attributes\n+\n+\n \n @keras_export('keras.layers.experimental.preprocessing.Normalization')\n class Normalization(base_preprocessing_layer.PreprocessingLayer):\n@@ -110,6 +112,12 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   def build(self, input_shape):\n     super().build(input_shape)\n \n+    if (isinstance(input_shape, (list, tuple)) and\n+        all(isinstance(shape, tf.TensorShape) for shape in input_shape)):\n+      raise ValueError('Normalization only accepts a single input. If you are '\n+                       'passing a python list or tuple as a single input, '\n+                       'please convert to a numpy array or `tf.Tensor`.')\n+\n     input_shape = tf.TensorShape(input_shape).as_list()\n     if len(input_shape) == 1:\n       input_shape = input_shape + [1]\n\n@@ -164,6 +164,18 @@ class NormalizationTest(keras_parameterized.TestCase,\n     with self.assertRaisesRegex(ValueError, r\"in the range\"):\n       layer.build([None, 2, 3])\n \n+  def test_list_input(self):\n+    with self.assertRaisesRegex(\n+        ValueError, (\"Normalization only accepts a single input. If you are \"\n+                     \"passing a python list or tuple as a single input, \"\n+                     \"please convert to a numpy array or `tf.Tensor`.\")):\n+      normalization.Normalization()([1, 2, 3])\n+\n+  def test_scalar_input(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"axis.*values must be in the range\"):\n+      normalization.Normalization()(1)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class NormalizationAdaptTest(keras_parameterized.TestCase,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d04a210d30850ffaa70a71bf3674f9fdd12da203", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 15144 | Contributors (this commit): 101 | Commits (past 90d): 14 | Contributors (cumulative): 101 | DMM Complexity: None\n\nDIFF:\n@@ -461,19 +461,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               **kwargs):\n     \"\"\"Configures the model for training.\n \n+    Example:\n+\n+    ```python\n+    model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),\n+                  loss=tf.keras.losses.BinaryCrossentropy(),\n+                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n+                           tf.keras.metrics.FalseNegatives()])\n+    ```\n+\n     Args:\n         optimizer: String (name of optimizer) or optimizer instance. See\n           `tf.keras.optimizers`.\n-        loss: String (name of objective function), objective function or\n-          `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n+        loss: Loss function. Maybe be a string (name of loss function), or\n+          a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n           function is any callable with the signature `loss = fn(y_true,\n-          y_pred)`, where y_true = ground truth values with shape =\n-          `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n-          categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n-          y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n-          returns a weighted loss float tensor. If a custom `Loss` instance is\n-          used and reduction is set to `None`, return value has the shape\n-          `[batch_size, d0, .. dN-1]` i.e. per-sample or per-timestep loss\n+          y_pred)`, where `y_true` are the ground truth values, and\n+          `y_pred` are the model's predictions.\n+          `y_true` should have shape\n+          `(batch_size, d0, .. dN)` (except in the case of\n+          sparse loss functions such as\n+          sparse categorical crossentropy which expects integer arrays of shape\n+          `(batch_size, d0, .. dN-1)`).\n+          `y_pred` should have shape `(batch_size, d0, .. dN)`.\n+          The loss function should return a float tensor.\n+          If a custom `Loss` instance is\n+          used and reduction is set to `None`, return value has shape\n+          `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n           values; otherwise, it is a scalar. If the model has multiple outputs,\n           you can use a different loss on each output by passing a dictionary\n           or a list of losses. The loss value that will be minimized by the\n@@ -1918,6 +1932,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       outputs = self.predict_function(iterator)\n     return tf_utils.sync_to_numpy_or_python_type(outputs)\n \n+  @doc_controls.do_not_generate_docs\n   def fit_generator(self,\n                     generator,\n                     steps_per_epoch=None,\n@@ -1958,6 +1973,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         shuffle=shuffle,\n         initial_epoch=initial_epoch)\n \n+  @doc_controls.do_not_generate_docs\n   def evaluate_generator(self,\n                          generator,\n                          steps=None,\n@@ -1986,6 +2002,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         verbose=verbose,\n         callbacks=callbacks)\n \n+  @doc_controls.do_not_generate_docs\n   def predict_generator(self,\n                         generator,\n                         steps=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9e7b83c70e2117f40465b79e2e8201cea46d36a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 37/19 | Churn Δ: 159 | Churn Cumulative: 1199 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: 0.6744186046511628\n\nDIFF:\n@@ -20,6 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import os\n import sys\n+import re\n from keras.utils.io_utils import path_to_string\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -64,6 +65,30 @@ def add_edge(dot, src, dst):\n     dot.add_edge(pydot.Edge(src, dst))\n \n \n+def get_layer_index_bound_by_layer_name(model, layer_names):\n+  \"\"\"Return specific range of layers to plot, mainly for sub-graph plot models.\n+\n+  Args:\n+    model: tf.keras.Model\n+    layer_names: unique name of layer of the model, type(str)\n+\n+  Returns:\n+    retun the index value of layer based on its unique name (layer_names)\n+  \"\"\"\n+  lower_index = []\n+  upper_index = []\n+  for idx, layer in enumerate(model.layers):\n+    if re.match(layer_names[0], layer.name):\n+      lower_index.append(idx)\n+    if re.match(layer_names[1], layer.name):\n+      upper_index.append(idx)\n+  if len(lower_index) == 0 or len(upper_index) == 0:\n+    raise ValueError('Passed layer_range does not match to model layers')\n+  if min(lower_index) > max(upper_index):\n+    return [min(upper_index), max(lower_index)]\n+  return [min(lower_index), max(upper_index)]\n+\n+\n @keras_export('keras.utils.model_to_dot')\n def model_to_dot(model,\n                  show_shapes=False,\n@@ -72,7 +97,8 @@ def model_to_dot(model,\n                  rankdir='TB',\n                  expand_nested=False,\n                  dpi=96,\n-                 subgraph=False):\n+                 subgraph=False,\n+                 layer_range=None):\n   \"\"\"Convert a Keras model to dot format.\n \n   Args:\n@@ -87,6 +113,15 @@ def model_to_dot(model,\n     expand_nested: whether to expand nested models into clusters.\n     dpi: Dots per inch.\n     subgraph: whether to return a `pydot.Cluster` instance.\n+    layer_range: input of `list` containing two `str` items, which is the\n+        starting layer name and ending layer name (both inclusive) indicating\n+        the range of layers for which the `pydot.Dot` will be generated. It\n+        also accepts regex patterns instead of exact name. In such case, start\n+        predicate will be the first element it matches to `layer_range[0]`\n+        and the end predicate will be the last element it matches to\n+        `layer_range[1]`. By default `None` which considers all layers of\n+        model. Note that you must pass range such that the resultant subgraph\n+        must be complete.\n \n   Returns:\n     A `pydot.Dot` instance representing the Keras model or\n@@ -125,6 +160,17 @@ def model_to_dot(model,\n     dot.set('dpi', dpi)\n     dot.set_node_defaults(shape='record')\n \n+  if layer_range:\n+    if len(layer_range) != 2:\n+      raise ValueError('layer_range must be of shape (2,)')\n+    if (not isinstance(layer_range[0], str) or\n+        not isinstance(layer_range[1], str)):\n+      raise ValueError('layer_range should contain string type only')\n+    layer_range = get_layer_index_bound_by_layer_name(model, layer_range)\n+    if layer_range[0] < 0 or layer_range[1] > len(model.layers):\n+      raise ValueError('Both values in layer_range should be in',\n+                       'range (%d, %d)' % (0, len(model.layers)))\n+\n   sub_n_first_node = {}\n   sub_n_last_node = {}\n   sub_w_first_node = {}\n@@ -142,6 +188,9 @@ def model_to_dot(model,\n \n   # Create graph nodes.\n   for i, layer in enumerate(layers):\n+    if (layer_range) and (i < layer_range[0] or i > layer_range[1]):\n+      continue\n+\n     layer_id = str(id(layer))\n \n     # Append a wrapped layer's label to node's label, if it exists.\n@@ -228,7 +277,9 @@ def model_to_dot(model,\n       dot.add_node(node)\n \n   # Connect nodes with edges.\n-  for layer in layers:\n+  for i, layer in enumerate(layers):\n+    if (layer_range) and (i <= layer_range[0] or i > layer_range[1]):\n+      continue\n     layer_id = str(id(layer))\n     for i, node in enumerate(layer._inbound_nodes):\n       node_key = layer.name + '_ib-' + str(i)\n@@ -284,7 +335,8 @@ def plot_model(model,\n                show_layer_names=True,\n                rankdir='TB',\n                expand_nested=False,\n-               dpi=96):\n+               dpi=96,\n+               layer_range=None):\n   \"\"\"Converts a Keras model to dot format and save to a file.\n \n   Example:\n@@ -315,6 +367,15 @@ def plot_model(model,\n         'LR' creates a horizontal plot.\n     expand_nested: Whether to expand nested models into clusters.\n     dpi: Dots per inch.\n+    layer_range: input of `list` containing two `str` items, which is the\n+        starting layer name and ending layer name (both inclusive) indicating\n+        the range of layers for which the plot will be generated. It also\n+        accepts regex patterns instead of exact name. In such case, start\n+        predicate will be the first element it matches to `layer_range[0]`\n+        and the end predicate will be the last element it matches to\n+        `layer_range[1]`. By default `None` which considers all layers of\n+        model. Note that you must pass range such that the resultant subgraph\n+        must be complete.\n \n   Returns:\n     A Jupyter notebook Image object if Jupyter is installed.\n@@ -327,7 +388,8 @@ def plot_model(model,\n       show_layer_names=show_layer_names,\n       rankdir=rankdir,\n       expand_nested=expand_nested,\n-      dpi=dpi)\n+      dpi=dpi,\n+      layer_range=layer_range)\n   to_file = path_to_string(to_file)\n   if dot is None:\n     return\n\n@@ -16,11 +16,14 @@\n \n import tensorflow.compat.v2 as tf\n \n+from absl.testing import parameterized\n+\n import keras\n+from keras.applications import efficientnet\n from keras.utils import vis_utils\n \n \n-class ModelToDotFormatTest(tf.test.TestCase):\n+class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_plot_model_cnn(self):\n     model = keras.Sequential()\n@@ -102,6 +105,90 @@ class ModelToDotFormatTest(tf.test.TestCase):\n     except ImportError:\n       pass\n \n+  @parameterized.parameters(\n+      {'layer_range': ['block1a_project_conv', 'block1a_activation']},\n+      {'layer_range': ['block1a_activation', 'block1a_project_conv']},\n+      {'layer_range': [r'block*', 'block2a_se_excite']},\n+      {'layer_range': [r'block\\da_activation', r'block\\da_project_bn']})\n+  def test_dot_layer_range(self, layer_range):\n+    model = efficientnet.EfficientNetB0()\n+    layer_ids_from_model = get_layer_ids_from_model(model, layer_range)\n+    try:\n+      dot = vis_utils.model_to_dot(model, layer_range=layer_range)\n+      dot_edges = dot.get_edges()\n+      layer_ids_from_dot = get_layer_ids_from_dot(dot_edges)\n+      self.assertAllEqual(\n+          sorted(layer_ids_from_model), sorted(layer_ids_from_dot))\n+    except ImportError:\n+      pass\n+\n+  @parameterized.parameters(\n+      {'layer_range': ['block1a_project_conv', 'block1a_activation']},\n+      {'layer_range': ['block1a_activation', 'block1a_project_conv']},\n+      {'layer_range': [r'block*', 'block2a_se_excite']},\n+      {'layer_range': [r'block\\da_activation', r'block\\da_project_bn']})\n+  def test_plot_layer_range(self, layer_range):\n+    model = efficientnet.EfficientNetB0()\n+    effnet_subplot = 'model_effnet.png'\n+    try:\n+      vis_utils.plot_model(\n+          model, to_file=effnet_subplot, layer_range=layer_range)\n+      self.assertTrue(tf.io.gfile.exists(effnet_subplot))\n+    except ImportError:\n+      pass\n+    finally:\n+      if tf.io.gfile.exists(effnet_subplot):\n+        tf.io.gfile.remove(effnet_subplot)\n+\n+  @parameterized.parameters(\n+      {'layer_range': ['block1a_se_squeeze', 'block2a_project_conv']},\n+      {'layer_range': [r'block\\da_se_reshape', r'block*']})\n+  def test_layer_range_assertion_fail(self, layer_range):\n+    model = efficientnet.EfficientNetB0()\n+    try:\n+      with self.assertRaises(AssertionError):\n+        vis_utils.model_to_dot(model, layer_range=layer_range)\n+      with self.assertRaises(AssertionError):\n+        vis_utils.plot_model(model, layer_range=layer_range)\n+    except ImportError:\n+      pass\n+\n+  @parameterized.parameters(\n+      {'layer_range': ['block1a_activation']},\n+      {'layer_range': []},\n+      {'layer_range': ['input', 'block1a_activation', 'block1a_project_conv']},\n+      {'layer_range': [9, 'block1a_activation']},\n+      {'layer_range': [29, 9]},\n+      {'layer_range': ['block8a_se_reshape', 'block*']})\n+  def test_layer_range_value_fail(self, layer_range):\n+    model = efficientnet.EfficientNetB0()\n+    try:\n+      with self.assertRaises(ValueError):\n+        vis_utils.model_to_dot(model, layer_range=layer_range)\n+      with self.assertRaises(ValueError):\n+        vis_utils.plot_model(model, layer_range=layer_range)\n+    except ImportError:\n+      pass\n+\n+\n+def get_layer_ids_from_model(model, layer_range):\n+  layer_range = vis_utils.get_layer_index_bound_by_layer_name(\n+      model, layer_range)\n+  layer_ids_from_model = []\n+  for i, layer in enumerate(model.layers):\n+    if i >= layer_range[0] and i <= layer_range[1]:\n+      layer_ids_from_model.append(str(id(layer)))\n+  return layer_ids_from_model\n+\n+\n+def get_layer_ids_from_dot(dot_edges):\n+  layer_ids_from_dot = []\n+  for edge in dot_edges:\n+    for pt in edge.obj_dict['points']:\n+      if pt not in layer_ids_from_dot:\n+        layer_ids_from_dot.append(pt)\n+  return layer_ids_from_dot\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee65bec7221187ebdc85187ba555e8ed603c40f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 22 | Churn Cumulative: 4373 | Contributors (this commit): 26 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -598,8 +598,9 @@ class Bidirectional(Wrapper):\n \n       kwargs['initial_state'] = initial_state\n       additional_inputs += initial_state\n-      state_specs = [InputSpec(shape=backend.int_shape(state))\n-                     for state in initial_state]\n+      state_specs = tf.nest.map_structure(\n+          lambda state: InputSpec(shape=backend.int_shape(state)),\n+          initial_state)\n       self.forward_layer.state_spec = state_specs[:num_states // 2]\n       self.backward_layer.state_spec = state_specs[num_states // 2:]\n       additional_specs += state_specs\n@@ -616,8 +617,9 @@ class Bidirectional(Wrapper):\n       self.forward_layer._num_constants = self._num_constants\n       self.backward_layer._num_constants = self._num_constants\n \n-    is_keras_tensor = backend.is_keras_tensor(additional_inputs[0])\n-    for tensor in additional_inputs:\n+    is_keras_tensor = backend.is_keras_tensor(\n+        tf.nest.flatten(additional_inputs)[0])\n+    for tensor in tf.nest.flatten(additional_inputs):\n       if backend.is_keras_tensor(tensor) != is_keras_tensor:\n         raise ValueError('The initial state of a Bidirectional'\n                          ' layer cannot be specified with a mix of'\n\n@@ -1302,6 +1302,18 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n       y_merged = convert_ragged_tensor_value(y_merged)\n       self.assertAllClose(y_merged.flat_values, y_expected.flat_values)\n \n+  def test_Bidirectional_nested_state_reuse(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest('Only test eager mode.')\n+    x = tf.random.normal([4, 8, 16])\n+    layer = keras.layers.Bidirectional(\n+        keras.layers.RNN([keras.layers.LSTMCell(5),\n+                          keras.layers.LSTMCell(5)],\n+                         return_sequences=True,\n+                         return_state=True))\n+    y = layer(x)\n+    self.assertAllClose(layer([x] + y[1:]), layer(x, initial_state=y[1:]))\n+\n   def test_full_input_spec(self):\n     # See https://github.com/tensorflow/tensorflow/issues/38403\n     inputs = keras.layers.Input(batch_shape=(1, 1, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fcb659aec519a562cda62b87d428116fafc9c495", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 214 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -111,7 +111,7 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n       {'layer_range': [r'block*', 'block2a_se_excite']},\n       {'layer_range': [r'block\\da_activation', r'block\\da_project_bn']})\n   def test_dot_layer_range(self, layer_range):\n-    model = efficientnet.EfficientNetB0()\n+    model = efficientnet.EfficientNetB0(weights=None)\n     layer_ids_from_model = get_layer_ids_from_model(model, layer_range)\n     try:\n       dot = vis_utils.model_to_dot(model, layer_range=layer_range)\n@@ -128,7 +128,7 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n       {'layer_range': [r'block*', 'block2a_se_excite']},\n       {'layer_range': [r'block\\da_activation', r'block\\da_project_bn']})\n   def test_plot_layer_range(self, layer_range):\n-    model = efficientnet.EfficientNetB0()\n+    model = efficientnet.EfficientNetB0(weights=None)\n     effnet_subplot = 'model_effnet.png'\n     try:\n       vis_utils.plot_model(\n@@ -144,7 +144,7 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n       {'layer_range': ['block1a_se_squeeze', 'block2a_project_conv']},\n       {'layer_range': [r'block\\da_se_reshape', r'block*']})\n   def test_layer_range_assertion_fail(self, layer_range):\n-    model = efficientnet.EfficientNetB0()\n+    model = efficientnet.EfficientNetB0(weights=None)\n     try:\n       with self.assertRaises(AssertionError):\n         vis_utils.model_to_dot(model, layer_range=layer_range)\n@@ -161,7 +161,7 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n       {'layer_range': [29, 9]},\n       {'layer_range': ['block8a_se_reshape', 'block*']})\n   def test_layer_range_value_fail(self, layer_range):\n-    model = efficientnet.EfficientNetB0()\n+    model = efficientnet.EfficientNetB0(weights=None)\n     try:\n       with self.assertRaises(ValueError):\n         vis_utils.model_to_dot(model, layer_range=layer_range)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1af2956a8c854b3f4c1b11f33868740192121e32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 31 | Churn Cumulative: 942 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,22 +143,13 @@ class Hashing(base_layer.Layer):\n     else:\n       self.salt = _DEFAULT_SALT_KEY\n \n-  def _preprocess_single_input(self, inp):\n+  def _preprocess_input(self, inp):\n     if isinstance(inp, (list, tuple, np.ndarray)):\n       inp = tf.convert_to_tensor(inp)\n     return inp\n \n-  def _preprocess_inputs(self, inputs):\n-    if isinstance(inputs, (tuple, list)):\n-      # If any of them is tensor or ndarray, then treat as list\n-      if any(\n-          tf.is_tensor(inp) or isinstance(inp, np.ndarray)\n-          for inp in inputs):\n-        return [self._preprocess_single_input(inp) for inp in inputs]\n-    return self._preprocess_single_input(inputs)\n-\n   def call(self, inputs):\n-    inputs = self._preprocess_inputs(inputs)\n+    inputs = self._preprocess_input(inputs)\n     if isinstance(inputs, tf.SparseTensor):\n       return tf.SparseTensor(\n           indices=inputs.indices,\n\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"Tests for hashing layer.\"\"\"\n \n+from absl.testing import parameterized\n+\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n@@ -268,6 +270,22 @@ class HashingTest(keras_parameterized.TestCase):\n     layer_1 = hashing.Hashing.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  @parameterized.named_parameters(\n+      ('list_input', [1, 2, 3], [1, 1, 1]),\n+      ('list_input_2d', [[1], [2], [3]], [[1], [1], [1]]),\n+      ('tf_constant_2d', [\n+          tf.constant(x) for x in [['aa', 'bb'], ['bb', 'cc'], ['cc', 'dd']]\n+      ], [[0, 0], [0, 0], [0, 0]]),\n+      ('tf_constant_3d', [\n+          tf.constant(x)\n+          for x in [[['aa'], ['bb']], [['bb'], ['cc']], [['cc'], ['dd']]]\n+      ], [[[0], [0]], [[0], [0]], [[0], [0]]]),\n+  )\n+  def test_hash_list_input(self, input_data, expected):\n+    layer = hashing.Hashing(num_bins=2)\n+    out_data = layer(input_data)\n+    self.assertAllEqual(expected, out_data.numpy().tolist())\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8068535b0ce98bc2b90fb43891ef51c30d96f3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 78 | Files Changed: 3 | Hunks: 30 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 177 | Churn Cumulative: 2854 | Contributors (this commit): 4 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -195,36 +195,34 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n \n     `tf.keras.Model` example with multiple adapts:\n \n-    >>> layer = tf.keras.layers.experimental.preprocessing.Normalization()\n+    >>> layer = tf.keras.layers.experimental.preprocessing.Normalization(\n+    ...     axis=None)\n     >>> layer.adapt([0, 2])\n     >>> model = tf.keras.Sequential(layer)\n     >>> model.predict([0, 1, 2])\n-    array([[-1.],\n-           [ 0.],\n-           [ 1.]], dtype=float32)\n+    array([-1.,  0.,  1.], dtype=float32)\n     >>> layer.adapt([-1, 1])\n     >>> model.compile() # This is needed to re-compile model.predict!\n     >>> model.predict([0, 1, 2])\n-    array([[0.],\n-           [1.],\n-           [2.]], dtype=float32)\n+    array([0., 1., 2.], dtype=float32)\n \n     `tf.data.Dataset` example with multiple adapts:\n \n-    >>> layer = tf.keras.layers.experimental.preprocessing.Normalization()\n+    >>> layer = tf.keras.layers.experimental.preprocessing.Normalization(\n+    ...     axis=None)\n     >>> layer.adapt([0, 2])\n     >>> input_ds = tf.data.Dataset.range(3)\n     >>> normalized_ds = input_ds.map(layer)\n     >>> list(normalized_ds.as_numpy_iterator())\n-    [array([[-1.]], dtype=float32),\n-     array([[0.]], dtype=float32),\n-     array([[1.]], dtype=float32)]\n+    [array([-1.], dtype=float32),\n+     array([0.], dtype=float32),\n+     array([1.], dtype=float32)]\n     >>> layer.adapt([-1, 1])\n     >>> normalized_ds = input_ds.map(layer) # Re-map over the input dataset.\n     >>> list(normalized_ds.as_numpy_iterator())\n-    [array([[0.]], dtype=float32),\n-     array([[1.]], dtype=float32),\n-     array([[2.]], dtype=float32)]\n+    [array([0.], dtype=float32),\n+     array([1.], dtype=float32),\n+     array([2.], dtype=float32)]\n \n     Arguments:\n         data: The data to train on. It can be passed either as a tf.data\n\n@@ -24,7 +24,6 @@ from tensorflow.python.util.tf_export import keras_export\n # pylint: disable=g-classes-have-attributes\n \n \n-\n @keras_export('keras.layers.experimental.preprocessing.Normalization')\n class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Feature-wise normalization of the data.\n@@ -38,14 +37,16 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     or `predict`.\n \n   Args:\n-      axis: Integer or tuple of integers, the axis or axes that should be\n-        \"kept\". These axes are not be summed over when calculating the\n-        normalization statistics. By default the last axis, the `features` axis\n-        is kept and any `space` or `time` axes are summed. Each element in the\n-        the axes that are kept is normalized independently. If `axis` is set to\n-        'None', the layer will perform scalar normalization (dividing the input\n-        by a single scalar value). The `batch` axis, 0, is always summed over\n-        (`axis=0` is not allowed).\n+      axis: Integer, tuple of integers, or None. The axis or axes that should\n+        have a separate mean and variance for each index in the shape. For\n+        example, if shape is (None, 5) and axis=1, the layer will track 5\n+        separate mean and variance values for the last axis. If `axis` is set to\n+        `None`, the layer will normalize all elements in the input by a scalar\n+        mean and variance. Defaults to -1, where the last axis of the input is\n+        assumed to be a feature dimension and is normalized per index. Note that\n+        in the specific case of batched scalar inputs where the only axis is the\n+        batch axis, the default will normalize each index in the batch\n+        separately. In this case, consider passing `axis=None`.\n       mean: The mean value(s) to use during normalization. The passed value(s)\n         will be broadcast to the shape of the kept axes above; if the value(s)\n         cannot be broadcast, an error will be raised when this layer's build()\n@@ -57,21 +58,33 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n   Examples:\n \n-  Calculate the mean and variance by analyzing the dataset in `adapt`.\n+  Calculate a global mean and variance by analyzing the dataset in `adapt`.\n \n-  >>> adapt_data = np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32)\n-  >>> input_data = np.array([[1.], [2.], [3.]], np.float32)\n-  >>> layer = Normalization()\n+  >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n+  >>> input_data = np.array([1., 2., 3.], dtype='float32')\n+  >>> layer = Normalization(axis=None)\n   >>> layer.adapt(adapt_data)\n   >>> layer(input_data)\n-  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n-  array([[-1.4142135 ],\n-         [-0.70710677],\n-         [ 0.        ]], dtype=float32)>\n+  <tf.Tensor: shape=(3,), dtype=float32, numpy=\n+  array([-1.4142135, -0.70710677, 0.], dtype=float32)>\n+\n+  Calculate a mean and variance for each index on the last axis.\n+\n+  >>> adapt_data = np.array([[0., 7., 4.],\n+  ...                        [2., 9., 6.],\n+  ...                        [0., 7., 4.],\n+  ...                        [2., 9., 6.]], dtype='float32')\n+  >>> input_data = np.array([[0., 7., 4.]], dtype='float32')\n+  >>> layer = Normalization(axis=-1)\n+  >>> layer.adapt(adapt_data)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n+  array([0., 0., 0.], dtype=float32)>\n+\n \n   Pass the mean and variance directly.\n \n-  >>> input_data = np.array([[1.], [2.], [3.]], np.float32)\n+  >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n   >>> layer = Normalization(mean=3., variance=2.)\n   >>> layer(input_data)\n   <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n@@ -91,8 +104,6 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n       axis = (axis,)\n     else:\n       axis = tuple(axis)\n-    if 0 in axis:\n-      raise ValueError('The argument \\'axis\\' may not be 0.')\n     self.axis = axis\n \n     # Set `mean` and `variance` if passed.\n@@ -119,18 +130,22 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n                        'please convert to a numpy array or `tf.Tensor`.')\n \n     input_shape = tf.TensorShape(input_shape).as_list()\n-    if len(input_shape) == 1:\n-      input_shape = input_shape + [1]\n     ndim = len(input_shape)\n \n-    if any(a < 1 - ndim or a >= ndim for a in self.axis):\n-      raise ValueError('All `axis` values must be in the range '\n-                       '[1 - ndim, ndim - 1]. Found '\n-                       'ndim: `{}`, axis: {}'.format(ndim, self.axis))\n+    if any(a < -ndim or a >= ndim for a in self.axis):\n+      raise ValueError('All `axis` values must be in the range [-ndim, ndim). '\n+                       'Found ndim: `{}`, axis: {}'.format(ndim, self.axis))\n \n     # Axes to be kept, replacing negative values with positive equivalents.\n     # Sorted to avoid transposing axes.\n     self._keep_axis = sorted([d if d >= 0 else d + ndim for d in self.axis])\n+    # All axes to be kept should have known shape.\n+    for d in self._keep_axis:\n+      if input_shape[d] is None:\n+        raise ValueError(\n+            'All `axis` values to be kept must have known shape. Got axis: {}, '\n+            'input shape: {}, with unknown axis at index: {}'.format(\n+                self.axis, input_shape, d))\n     # Axes to be reduced.\n     self._reduce_axis = [d for d in range(ndim) if d not in self._keep_axis]\n     # 1 if an axis should be reduced, 0 otherwise.\n@@ -185,11 +200,13 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n     data = self._standardize_inputs(data)\n     data = tf.cast(data, self.adapt_mean.dtype)\n-    batch_mean, batch_variance = tf.nn.moments(\n-        data, axes=self._reduce_axis)\n+    batch_mean, batch_variance = tf.nn.moments(data, axes=self._reduce_axis)\n     batch_shape = tf.compat.v1.shape(data, out_type=self.count.dtype)\n+    if self._reduce_axis:\n       batch_reduce_shape = tf.compat.v1.gather(batch_shape, self._reduce_axis)\n       batch_count = tf.reduce_prod(batch_reduce_shape)\n+    else:\n+      batch_count = 1\n \n     total_count = batch_count + self.count\n     batch_weight = (\n@@ -227,8 +244,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n     total_count = tf.reduce_sum(layer_counts)\n     layer_weightings = (\n-        tf.cast(layer_counts, self.dtype) /\n-        tf.cast(total_count, self.dtype))\n+        tf.cast(layer_counts, self.dtype) / tf.cast(total_count, self.dtype))\n     layer_weightings = tf.reshape(\n         layer_weightings,\n         shape=[len(layers)] + [1] * self.adapt_mean.shape.rank)\n@@ -259,8 +275,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     # proper broadcast shape and dtype each time `finalize_state` is called.\n     self.mean = tf.reshape(self.adapt_mean, self._broadcast_shape)\n     self.mean = tf.cast(self.mean, self.compute_dtype)\n-    self.variance = tf.reshape(self.adapt_variance,\n-                                      self._broadcast_shape)\n+    self.variance = tf.reshape(self.adapt_variance, self._broadcast_shape)\n     self.variance = tf.cast(self.variance, self.compute_dtype)\n \n   def call(self, inputs):\n@@ -288,10 +303,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n   def _standardize_inputs(self, inputs):\n     inputs = tf.convert_to_tensor(inputs)\n-    if inputs.shape.rank == 0:\n-      inputs = tf.reshape(inputs, [1, 1])\n-    elif inputs.shape.rank == 1:\n-      inputs = tf.compat.v1.expand_dims(inputs, 1)\n+    if inputs.dtype != self.dtype:\n+      inputs = tf.cast(inputs, self.dtype)\n     return inputs\n \n   def _convert_to_list(self, inputs):\n\n@@ -135,6 +135,19 @@ class NormalizationTest(keras_parameterized.TestCase,\n     self.assertAllClose(output, expected_output)\n     self.assertAllClose(layer.get_weights(), [])\n \n+  def test_1d_data(self):\n+    data = np.array([0., 2., 0., 2.])\n+    layer = normalization.Normalization(mean=1.0, variance=1.0)\n+    output = layer(data)\n+    self.assertListEqual(output.shape.as_list(), [4])\n+    self.assertAllClose(output, [-1, 1, -1, 1])\n+\n+  def test_0d_data(self):\n+    layer = normalization.Normalization(axis=None, mean=1.0, variance=1.0)\n+    output = layer(0.)\n+    self.assertListEqual(output.shape.as_list(), [])\n+    self.assertAllClose(output, -1)\n+\n   def test_broadcasting_during_direct_setting_with_variables_fails(self):\n     with self.assertRaisesRegex(ValueError, \"passing a Variable\"):\n       _ = normalization.Normalization(\n@@ -142,26 +155,22 @@ class NormalizationTest(keras_parameterized.TestCase,\n           mean=tf.Variable([1.0]),\n           variance=tf.Variable([2.0]))\n \n-  @parameterized.parameters(\n-      {\"axis\": 0},\n-      {\"axis\": (-1, 0)},\n-  )\n-  def test_zeros_fail_init(self, axis):\n-    with self.assertRaisesRegex(ValueError,\n-                                \"The argument 'axis' may not be 0.\"):\n-      normalization.Normalization(axis=axis)\n+  def test_keeping_an_unknown_axis_fails(self):\n+    layer = normalization.Normalization(axis=-1)\n+    with self.assertRaisesRegex(ValueError, \"axis.*must have known shape\"):\n+      layer.build([None])\n \n   @parameterized.parameters(\n       # Out of bounds\n       {\"axis\": 3},\n-      {\"axis\": -3},\n+      {\"axis\": -4},\n       # In a tuple\n       {\"axis\": (1, 3)},\n-      {\"axis\": (1, -3)},\n+      {\"axis\": (1, -4)},\n   )\n   def test_bad_axis_fail_build(self, axis):\n     layer = normalization.Normalization(axis=axis)\n-    with self.assertRaisesRegex(ValueError, r\"in the range\"):\n+    with self.assertRaisesRegex(ValueError, \"in the range\"):\n       layer.build([None, 2, 3])\n \n   def test_list_input(self):\n@@ -215,25 +224,26 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n-  def test_1d_data(self):\n-    data = [0, 2, 0, 2]\n+  def test_1d_unbatched_adapt(self):\n+    ds = tf.data.Dataset.from_tensor_slices([\n+        [2., 0., 2., 0.],\n+        [0., 2., 0., 2.],\n+    ])\n     layer = normalization.Normalization(axis=-1)\n-    layer.adapt(data)\n-    output = layer(data)\n-    self.assertListEqual(output.shape.as_list(), [4, 1])\n-    if tf.executing_eagerly():\n-      self.assertAllClose(output.numpy(), [[-1], [1], [-1], [1]])\n+    layer.adapt(ds)\n+    output_ds = ds.map(layer)\n+    self.assertAllClose(\n+        list(output_ds.as_numpy_iterator()), [\n+            [1., -1., 1., -1.],\n+            [-1., 1., -1., 1.],\n+        ])\n \n-  def test_0d_data(self):\n-    if not tf.executing_eagerly():\n-      self.skipTest(\"Only supported in TF2.\")\n-\n-    data = [0, 2, 0, 2]\n-    layer = normalization.Normalization(axis=-1)\n-    layer.adapt(data)\n-    output = layer(0.)\n-    self.assertListEqual(output.shape.as_list(), [1, 1])\n-    self.assertAllClose(output.numpy(), [[-1]])\n+  def test_0d_unbatched_adapt(self):\n+    ds = tf.data.Dataset.from_tensor_slices([2., 0., 2., 0.])\n+    layer = normalization.Normalization(axis=None)\n+    layer.adapt(ds)\n+    output_ds = ds.map(layer)\n+    self.assertAllClose(list(output_ds.as_numpy_iterator()), [1., -1., 1., -1.])\n \n   @parameterized.parameters(\n       # Results should be identical no matter how the axes are specified (3d).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bc12fd701ead9974bab6770a01b08d6834adb1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 108 | Churn Cumulative: 8911 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 15 | DMM Complexity: 0.8194444444444444\n\nDIFF:\n@@ -77,6 +77,8 @@ keras_api_gauge = tf.__internal__.monitoring.BoolGauge('/tensorflow/api/oss-kera\n keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/premade_models', 'premade keras model usage', 'type')\n \n+_is_name_scope_on_model_declaration_enabled = False\n+\n \n @keras_export('keras.layers.Layer')\n class Layer(tf.Module, version_utils.LayerVersionSelector):\n@@ -428,6 +430,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # a list with one element.\n     self._preserve_input_structure_in_config = False\n \n+    # Save outer name scope at layer declaration so that it is preserved at\n+    # the actual layer construction.\n+    self._outer_name_scope = tf.get_current_name_scope()\n+\n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n   def build(self, input_shape):\n@@ -2393,6 +2399,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     if not tf.__internal__.tf2.enabled():\n       return self.name\n     name_scope = self.name\n+    if _is_name_scope_on_model_declaration_enabled and self._outer_name_scope:\n+      name_scope = self._outer_name_scope + '/' + name_scope\n     current_name_scope = tf.__internal__.get_name_scope()\n     if current_name_scope:\n       name_scope = current_name_scope + '/' + name_scope\n@@ -3248,6 +3256,35 @@ def _convert_numpy_or_python_types(x):\n   return x\n \n \n+@keras_export(\n+    'keras.__internal__.apply_name_scope_on_model_declaration', v1=[])\n+def _apply_name_scope_on_model_declaration(enable):\n+  \"\"\"Apply `with tf.name_scope(...)` on model declaration.\n+\n+  ```python\n+  tf.keras.__internal__.apply_name_scope_on_model_declaration(True)\n+\n+  inputs = input_layer.Input((3,))\n+  with tf.name_scope('MyScope'):\n+    outputs = layers.Dense(10, name='MyDense')(inputs)\n+  model = tf.keras.Model(inputs, outputs)\n+\n+  # with `tf.keras.__internal__.apply_name_scope_on_model_declaration(True)`,\n+  # The name of the dense layer is \"model/MyScope/MyDense/*\", and without,\n+  # \"model/MyDense/*\"\n+  ```\n+\n+  Args:\n+    enable: Enables if `True`, disables if `False`.\n+  \"\"\"\n+  if not isinstance(enable, bool):\n+    raise TypeError(\n+        '`enable` argument must be `True` or `False`, got {}'.format(enable))\n+\n+  global _is_name_scope_on_model_declaration_enabled\n+  _is_name_scope_on_model_declaration_enabled = enable\n+\n+\n # Avoid breaking users who directly import this symbol from this file.\n # TODO(fchollet): remove this.\n InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -1314,6 +1314,77 @@ class NameScopingTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.bias.name, 'MyName3/bias:0')\n     self.assertEqual(layer.kernel.name, 'MyName3/kernel:0')\n \n+  @testing_utils.run_v2_only\n+  def test_apply_name_scope_on_model_declaration(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest('`apply_name_scope_on_model_declaration` API is supported'\n+                    ' only for V2 eager')\n+\n+    base_layer._apply_name_scope_on_model_declaration(True)\n+\n+    inputs = input_layer.Input((3,))\n+    x = layers.Dense(10, name='Dense1')(inputs)\n+    with tf.name_scope('outer'):\n+      x = layers.Dense(10, name='Dense2')(x)\n+      with tf.name_scope('inner'):\n+        x = layers.Dense(10, name='Dense3')(x)\n+      x = layers.Dense(10, name='Dense4')(x)\n+    outputs = layers.Dense(10, name='Dense5')(x)\n+\n+    model = training_lib.Model(inputs, outputs)\n+    node_names = self._get_model_node_names(model, np.random.random((1, 3)),\n+                                            'call_scope')\n+    self.assertListEqual(node_names, [\n+        'call_scope/Const',\n+        'call_scope/model/Cast',\n+        'call_scope/model/Dense1/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/Dense1/MatMul/ReadVariableOp',\n+        'call_scope/model/Dense1/MatMul',\n+        'call_scope/model/Dense1/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/Dense1/BiasAdd/ReadVariableOp',\n+        'call_scope/model/Dense1/BiasAdd',\n+        'call_scope/model/outer/Dense2/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/Dense2/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/Dense2/MatMul',\n+        'call_scope/model/outer/Dense2/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/Dense2/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/Dense2/BiasAdd',\n+        'call_scope/model/outer/inner/Dense3/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/inner/Dense3/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/inner/Dense3/MatMul',\n+        'call_scope/model/outer/inner/Dense3/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/inner/Dense3/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/inner/Dense3/BiasAdd',\n+        'call_scope/model/outer/Dense4/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/Dense4/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/Dense4/MatMul',\n+        'call_scope/model/outer/Dense4/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/Dense4/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/Dense4/BiasAdd',\n+        'call_scope/model/Dense5/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/Dense5/MatMul/ReadVariableOp',\n+        'call_scope/model/Dense5/MatMul',\n+        'call_scope/model/Dense5/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/Dense5/BiasAdd/ReadVariableOp',\n+        'call_scope/model/Dense5/BiasAdd',\n+        'Identity',\n+        'NoOp'\n+    ])\n+    base_layer._apply_name_scope_on_model_declaration(False)\n+\n+  def _get_model_node_names(self, model, inputs, call_name_scope):\n+    \"\"\"Returns a list of model's node names.\"\"\"\n+\n+    @tf.function()\n+    def wrapper():\n+      with tf.name_scope(call_name_scope):\n+        return model(inputs)\n+\n+    return [\n+        node.name\n+        for node in wrapper.get_concrete_function().graph.as_graph_def().node\n+    ]\n+\n \n @combinations.generate(combinations.keras_mode_combinations(mode=['eager']))\n class AutographControlFlowTest(keras_parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c567184255468a1d0ca0b6b284e20cdeb7561574", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 67 | Churn Cumulative: 10210 | Contributors (this commit): 91 | Commits (past 90d): 22 | Contributors (cumulative): 94 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1359,7 +1359,7 @@ class ModelCheckpoint(Callback):\n \n   def on_train_batch_end(self, batch, logs=None):\n     if self._should_save_on_batch(batch):\n-      self._save_model(epoch=self._current_epoch, logs=logs)\n+      self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n \n   def on_epoch_begin(self, epoch, logs=None):\n     self._current_epoch = epoch\n@@ -1368,7 +1368,7 @@ class ModelCheckpoint(Callback):\n     self.epochs_since_last_save += 1\n     # pylint: disable=protected-access\n     if self.save_freq == 'epoch':\n-      self._save_model(epoch=epoch, logs=logs)\n+      self._save_model(epoch=epoch, batch=None, logs=logs)\n \n   def _should_save_on_batch(self, batch):\n     \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n@@ -1387,11 +1387,13 @@ class ModelCheckpoint(Callback):\n       return True\n     return False\n \n-  def _save_model(self, epoch, logs):\n+  def _save_model(self, epoch, batch, logs):\n     \"\"\"Saves the model.\n \n     Args:\n         epoch: the epoch this iteration is in.\n+        batch: the batch this iteration is in. `None` if the `save_freq`\n+          is set to `epoch`.\n         logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n     \"\"\"\n     logs = logs or {}\n@@ -1401,7 +1403,7 @@ class ModelCheckpoint(Callback):\n       # Block only when saving interval is reached.\n       logs = tf_utils.sync_to_numpy_or_python_type(logs)\n       self.epochs_since_last_save = 0\n-      filepath = self._get_file_path(epoch, logs)\n+      filepath = self._get_file_path(epoch, batch, logs)\n \n       try:\n         if self.save_best_only:\n@@ -1448,14 +1450,18 @@ class ModelCheckpoint(Callback):\n         # Re-throw the error for any other causes.\n         raise e\n \n-  def _get_file_path(self, epoch, logs):\n+  def _get_file_path(self, epoch, batch, logs):\n     \"\"\"Returns the file path for checkpoint.\"\"\"\n     # pylint: disable=protected-access\n     try:\n-      # `filepath` may contain placeholders such as `{epoch:02d}` and\n-      # `{mape:.2f}`. A mismatch between logged metrics and the path's\n+      # `filepath` may contain placeholders such as `{epoch:02d}`,`{batch:02d}`\n+      # and `{mape:.2f}`. A mismatch between logged metrics and the path's\n       # placeholders can cause formatting to fail.\n+      if batch is None or 'batch' in logs:\n         file_path = self.filepath.format(epoch=epoch + 1, **logs)\n+      else:\n+        file_path = self.filepath.format(\n+            epoch=epoch + 1, batch=batch + 1, **logs)\n     except KeyError as e:\n       raise KeyError('Failed to format this callback filepath: \"{}\". '\n                      'Reason: {}'.format(self.filepath, e))\n\n@@ -767,6 +767,53 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n         mode=mode,\n         options=tf.saved_model.SaveOptions())\n \n+    # Case 11: `ModelCheckpoint` save model with batch number in filename.\n+    filepath = os.path.join(temp_dir,\n+                            'checkpoint.epoch{epoch:02d}batch{batch:02d}.h5')\n+    cbks = [\n+        keras.callbacks.ModelCheckpoint(filepath, monitor=monitor, save_freq=1)\n+    ]\n+    assert not os.path.exists(filepath.format(epoch=1, batch=1))\n+    assert not os.path.exists(filepath.format(epoch=1, batch=2))\n+    assert not os.path.exists(filepath.format(epoch=2, batch=1))\n+    assert not os.path.exists(filepath.format(epoch=2, batch=2))\n+    assert not os.path.exists(filepath.format(epoch=3, batch=1))\n+    assert not os.path.exists(filepath.format(epoch=3, batch=2))\n+    assert not os.path.exists(filepath.format(epoch=4, batch=1))\n+    assert not os.path.exists(filepath.format(epoch=4, batch=2))\n+    assert not os.path.exists(filepath.format(epoch=5, batch=1))\n+    assert not os.path.exists(filepath.format(epoch=5, batch=2))\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=5,\n+        validation_data=(x_test, y_test),\n+        callbacks=cbks,\n+        epochs=5,\n+        verbose=1)\n+\n+    assert os.path.exists(filepath.format(epoch=1, batch=1))\n+    assert os.path.exists(filepath.format(epoch=1, batch=2))\n+    assert os.path.exists(filepath.format(epoch=2, batch=1))\n+    assert os.path.exists(filepath.format(epoch=2, batch=2))\n+    assert os.path.exists(filepath.format(epoch=3, batch=1))\n+    assert os.path.exists(filepath.format(epoch=3, batch=2))\n+    assert os.path.exists(filepath.format(epoch=4, batch=1))\n+    assert os.path.exists(filepath.format(epoch=4, batch=2))\n+    assert os.path.exists(filepath.format(epoch=5, batch=1))\n+    assert os.path.exists(filepath.format(epoch=5, batch=2))\n+\n+    os.remove(filepath.format(epoch=1, batch=1))\n+    os.remove(filepath.format(epoch=1, batch=2))\n+    os.remove(filepath.format(epoch=2, batch=1))\n+    os.remove(filepath.format(epoch=2, batch=2))\n+    os.remove(filepath.format(epoch=3, batch=1))\n+    os.remove(filepath.format(epoch=3, batch=2))\n+    os.remove(filepath.format(epoch=4, batch=1))\n+    os.remove(filepath.format(epoch=4, batch=2))\n+    os.remove(filepath.format(epoch=5, batch=1))\n+    os.remove(filepath.format(epoch=5, batch=2))\n+\n   @testing_utils.run_v2_only\n   def test_ModelCheckpoint_subclass_save_weights_false(self):\n     model = testing_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
