{"custom_id": "scrapy#a6ff165332faa8450236528f453c73cefceb13b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 562 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ class MediaPipeline(object):\n             lst.append(dfd)\n \n         dlst = defer.DeferredList(lst, consumeErrors=False)\n-        dlst.addBoth(lambda _: self.item_completed(item, info))\n+        dlst.addBoth(self.item_completed, item, info)\n         return dlst\n \n     def _enqueue(self, request, info):\n@@ -183,7 +183,7 @@ class MediaPipeline(object):\n         return value of this method isn't important and is recommended to return None.\n         \"\"\"\n \n-    def item_completed(self, item, info):\n+    def item_completed(self, results, item, info):\n         \"\"\" Method called when all media requests for a single item has returned a result or failure.\n \n         The return value of this method is used as output of pipeline stage.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0938984de0a339d4a6073cf1dafc4ac94d960e29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 574 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -65,14 +65,11 @@ class MediaPipeline(object):\n         # add to pending list for this request, and wait for result like the others.\n         info.waiting.setdefault(fp, []).append(wad)\n \n-        # if request is already downloading, just wait.\n-        if fp in info.downloading:\n-            return wad # break\n+        # if request is not downloading, download it.\n+        if fp not in info.downloading:\n+            self._download(request, info, fp)\n \n-        # if not, this is the first time for request, try to download it.\n-        dfd = self._download(request, info, fp)\n-        dfd.addCallback(lambda _: wad)\n-        return dfd\n+        return wad\n \n     def _download(self, request, info, fp):\n         def _bugtrap(_failure):\n@@ -109,7 +106,6 @@ class MediaPipeline(object):\n         # defer pre-download request processing\n         dfd = mustbe_deferred(self.media_to_download, request, info)\n         dfd.addCallback(_evaluated)\n-        return dfd\n \n     ### Overradiable Interface\n     def download(self, request, info):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b56a5d933e0d4e0359a8743eeb55c3468a01b9ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 33 | Churn Cumulative: 187 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from twisted.python.failure import Failure\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n-from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n@@ -18,16 +18,13 @@ def _isiterable(possible_iterator):\n         return None\n \n class SpiderMiddlewareManager(object):\n-    def __init__(self, callback=None, errback=None):\n+    def __init__(self):\n         self.loaded = False\n         self.spider_middleware = []\n         self.result_middleware = []\n         self.exception_middleware = []\n         self.filter_middleware = []\n         self.domaininfo = {}\n-\n-        self.callback = callback or self._callback\n-        self.errback = errback or self._errback\n         self.load()\n \n     def _add_middleware(self, mw):\n@@ -94,29 +91,5 @@ class SpiderMiddlewareManager(object):\n         return dfd\n \n     def call(self, request, response, spider):\n-        if isinstance(response, Exception):\n-            return self.errback(request, Failure(response), spider)\n-        return self.callback(request, response, spider)\n-\n-    def _callback(self, request, response, spider):\n-        request.deferred.callback(response)\n+        defer_result(response).chainDeferred(request.deferred)\n         return request.deferred\n-\n-    def _errback(self, request, failure, spider):\n-        request.deferred.errback(failure)\n-        return request.deferred\n-\n-\n-class DummyMiddleware(object):\n-    def process_scrape(self, response, spider):\n-        pass\n-\n-    def process_result(self, response, result, spider):\n-        return result\n-\n-    def process_exception(self, response, exception, spider):\n-        pass\n-\n-    def filter(self, item):\n-        return True\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98bc8ec3e17e9a11b954034d2f8051b7dce50041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 191 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,8 +23,6 @@ class SpiderMiddlewareManager(object):\n         self.spider_middleware = []\n         self.result_middleware = []\n         self.exception_middleware = []\n-        self.filter_middleware = []\n-        self.domaininfo = {}\n         self.load()\n \n     def _add_middleware(self, mw):\n@@ -34,8 +32,6 @@ class SpiderMiddlewareManager(object):\n             self.result_middleware.insert(0, mw.process_result)\n         if hasattr(mw, 'process_exception'):\n             self.exception_middleware.insert(0, mw.process_exception)\n-        if hasattr(mw, 'filter'):\n-            self.filter_middleware.insert(0, mw.filter)\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4434590b0c037b794751f2cf5d58e01cd18a9e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 100 | Churn Cumulative: 927 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,22 +20,60 @@ from scrapy.contrib.pipeline.media import MediaPipeline\n IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n \n class NoimagesDrop(DropItem):\n-    pass\n+    \"\"\"Product with no images exception\"\"\"\n \n class ImageException(Exception):\n     \"\"\"General image error exception\"\"\"\n \n \n-class ImagesPipeline(MediaPipeline):\n+class BaseImagesPipeline(MediaPipeline):\n     MEDIA_TYPE = 'image'\n-    THUMBS = None\n-#     THUMBS = (\n-#             (\"50\", (50, 50)),\n-#             (\"110\", (110, 110)),\n-#             (\"270\", (270, 270))\n-#     )\n+\n+    def media_downloaded(self, response, request, info):\n+        mtype = self.MEDIA_TYPE\n+        referer = request.headers.get('Referer')\n+\n+        if not response or not response.body.to_string():\n+            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: no-content' \\\n+                    % (mtype, request, referer)\n+            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            raise ImageException(msg)\n+\n+        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n+        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n+        log.msg(msg, level=log.DEBUG, domain=info.domain)\n+        self.inc_stats(info.domain, status)\n+        return self.image_downloaded(response, request, info)\n+\n+    def media_failed(self, failure, request, info):\n+        referer = request.headers.get('Referer')\n+        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n+        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n+                % (self.MEDIA_TYPE, request, referer, errmsg)\n+        log.msg(msg, level=log.WARNING, domain=info.domain)\n+        raise ImageException(msg)\n+\n+    def inc_stats(self, domain, status):\n+        stats.incpath('%s/image_count' % domain)\n+        stats.incpath('%s/image_status_count/%s' % (domain, status))\n+\n+    def image_downloaded(self, response, request, info):\n+        raise NotImplementedError\n+\n+\n+class ImagesPipeline(BaseImagesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n+    THUMBS = (\n+#             ('50', (50, 50)),\n+#             ('110', (110, 110)),\n+#             ('270', (270, 270))\n+     )\n+\n+    class DomainInfo(BaseImagesPipeline.DomainInfo):\n+        def __init__(self, domain):\n+            super(ImagesPipeline.DomainInfo, self).__init__(domain)\n+            self.created_directories = set()\n \n     def __init__(self):\n         if not settings['IMAGES_DIR']:\n@@ -46,7 +84,7 @@ class ImagesPipeline(MediaPipeline):\n \n         self.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n         self.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n-        MediaPipeline.__init__(self)\n+        super(ImagesPipeline, self).__init__()\n \n     def media_to_download(self, request, info):\n         relative, absolute = self._get_paths(request)\n@@ -57,31 +95,7 @@ class ImagesPipeline(MediaPipeline):\n                     (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n             return relative\n \n-    def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        referer = request.headers.get('Referer')\n-\n-        if not response or not response.body.to_string():\n-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n-\n-        result = self.save_image(response, request, info) # save and thumbs response\n-\n-        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n-        log.msg(msg, level=log.DEBUG, domain=info.domain)\n-        self.inc_stats(info.domain, status)\n-        return result\n-\n-    def media_failed(self, failure, request, info):\n-        referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n-        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n-        log.msg(msg, level=log.WARNING, domain=info.domain)\n-        raise ImageException(msg)\n-\n-    def save_image(self, response, request, info):\n+    def image_downloaded(self, response, request, info):\n         mtype = self.MEDIA_TYPE\n         relpath, abspath = self._get_paths(request)\n         dirname = os.path.dirname(abspath)\n@@ -94,7 +108,8 @@ class ImagesPipeline(MediaPipeline):\n             raise ex\n         except Exception, ex:\n             referer = request.headers.get('Referer')\n-            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' % (mtype, request, referer, ex)\n+            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' \\\n+                    % (mtype, request, referer, ex)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n \n@@ -106,17 +121,12 @@ class ImagesPipeline(MediaPipeline):\n         return relative, absolute\n \n     def mkdir(self, dirname, info=None):\n-        already_created = info.extra.setdefault('created_directories', set()) if info else set()\n+        already_created = info.created_directories if info else set()\n         if dirname not in already_created:\n             if not os.path.exists(dirname):\n                 os.makedirs(dirname)\n             already_created.add(dirname)\n \n-    def inc_stats(self, domain, status):\n-        stats.incpath('%s/image_count' % domain)\n-        stats.incpath('%s/image_status_count/%s' % (domain, status))\n-\n-\n \n def should_download(path):\n     \"\"\"Should the image downloader download the image to the location specified\n\n@@ -6,6 +6,8 @@ from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n \n \n+class MediaPipeline(object):\n+\n     class DomainInfo(object):\n         def __init__(self, domain):\n             self.domain = domain\n@@ -13,15 +15,12 @@ class DomainInfo(object):\n             self.downloading = {}\n             self.downloaded = {}\n             self.waiting = {}\n-        self.extra = {}\n \n-\n-class MediaPipeline(object):\n     def __init__(self):\n         self.domaininfo = {}\n \n     def open_domain(self, domain):\n-        self.domaininfo[domain] = DomainInfo(domain)\n+        self.domaininfo[domain] = self.DomainInfo(domain)\n \n     def close_domain(self, domain):\n         del self.domaininfo[domain]\n@@ -98,7 +97,6 @@ class MediaPipeline(object):\n             info.downloading[fp] = (request, dwld) # fill downloading state data\n             dwld.addBoth(_downloaded) # append post-download hook\n             dwld.addErrback(_bugtrap) # catch media_downloaded and media_failed unhandled errors\n-            # WARNING: dont be tempted to return 'dwld' here.\n \n         # declare request in downloading state (None is used as place holder)\n         info.downloading[fp] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2142d4f7e02b387301e6c39ed82daeac0a230b3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 71 | Files Changed: 2 | Hunks: 27 | Methods Changed: 19 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 137 | Churn Cumulative: 295 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,25 +1,24 @@\n import time\n+import hmac\n+import base64\n import hashlib\n import rfc822\n from cStringIO import StringIO\n \n import Image\n-import boto\n \n from scrapy import log\n+from scrapy.http import Request\n from scrapy.stats import stats\n-from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException\n+from scrapy.core.exceptions import DropItem, NotConfigured, HttpException\n from scrapy.contrib.pipeline.media import MediaPipeline\n+from scrapy.contrib.aws import canonical_string\n from scrapy.conf import settings\n \n-class NoimagesDrop(DropItem):\n-    \"\"\"Product with no images exception\"\"\"\n+from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n \n-class ImageException(Exception):\n-    \"\"\"General image error exception\"\"\"\n \n-class S3ImagesPipeline(MediaPipeline):\n+class S3ImagesPipeline(BaseImagesPipeline):\n     MEDIA_TYPE = 'image'\n     THUMBS = (\n         (\"50\", (50, 50)),\n@@ -31,55 +30,32 @@ class S3ImagesPipeline(MediaPipeline):\n         if not settings['S3_IMAGES']:\n             raise NotConfigured\n \n-        # days to wait before redownloading images\n-        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n-        \n         self.bucket_name = settings['S3_BUCKET']\n         self.prefix = settings['S3_PREFIX']\n-        access_key = settings['AWS_ACCESS_KEY_ID']\n-        secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        conn = boto.connect_s3(access_key, secret_key)\n-        self.bucket = conn.get_bucket(self.bucket_name)\n-\n+        self.access_key = settings['AWS_ACCESS_KEY_ID']\n+        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n+        self._hmac = hmac.new(settings['AWS_SECRET_ACCESS_KEY'], digestmod=hashlib.sha1)\n         MediaPipeline.__init__(self)\n \n-    def media_to_download(self, request, info):\n-        key = self.s3_image_key(request.url)\n-        if not self.s3_should_download(request.url):\n-            self.inc_stats(info.domain, 'uptodate')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n-                    (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n-            return key\n+    def s3request(self, key, method, body=None, headers=None):\n+        url = 'http://%s.s3.amazonaws.com/%s' % (self.bucket_name, key)\n+        req = Request(url, method=method, body=body, headers=headers)\n \n-    def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        referer = request.headers.get('Referer')\n+        if not (headers and 'Date' in headers):\n+            req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n \n-        if not response or not response.body.to_string():\n-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n+        fullkey = '/%s/%s' % (self.bucket_name, key)\n+        c_string = canonical_string(method, fullkey, req.headers)\n+        _hmac = self._hmac.copy()\n+        _hmac.update(c_string)\n+        b64_hmac = base64.encodestring(_hmac.digest()).strip()\n+        req.headers['Authorization'] = \"AWS %s:%s\" % (self.access_key, b64_hmac)\n+        return req\n \n-        result = self.save_image(response, request, info) # save and thumbs response\n-\n-        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n-        log.msg(msg, level=log.DEBUG, domain=info.domain)\n-        self.inc_stats(info.domain, status)\n-        return result\n-\n-    def media_failed(self, failure, request, info):\n-        referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n-        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n-        log.msg(msg, level=log.WARNING, domain=info.domain)\n-        raise ImageException(msg)\n-\n-    def save_image(self, response, request, info):\n+    def image_downloaded(self, response, request, info):\n         try:\n             key = self.s3_image_key(request.url)\n-            self.s3_store_image(response, request.url)\n+            self.s3_store_image(response, request.url, info)\n         except ImageException, ex:\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n@@ -89,10 +65,6 @@ class S3ImagesPipeline(MediaPipeline):\n \n         return key # success value sent as input result for item_media_downloaded\n \n-    def inc_stats(self, domain, status):\n-        stats.incpath('%s/image_count' % domain)\n-        stats.incpath('%s/image_status_count/%s' % (domain, status))\n-\n     def s3_image_key(self, url):\n         \"\"\"Return the relative path on the target filesystem for an image to be\n         downloaded to.\n@@ -107,48 +79,70 @@ class S3ImagesPipeline(MediaPipeline):\n         image_guid = hashlib.sha1(url).hexdigest()\n         return '%s/thumbs/%s/%s.jpg' % (self.prefix, thumb_id, image_guid)\n \n-    def s3_should_download(self, url):\n+    def media_to_download(self, request, info):\n         \"\"\"Return if the image should be downloaded by checking if it's already in\n         the S3 storage and not too old\"\"\"\n-        key = self.s3_image_key(url)\n-        k = self.bucket.get_key(key)\n-        if k is None:\n+\n+        def _on200(response):\n+            if 'Last-Modified' not in response.headers:\n                 return True\n-        modified_tuple = rfc822.parsedate_tz(k.last_modified)\n+\n+            last_modified = response.headers['Last-Modified'][0]\n+            modified_tuple = rfc822.parsedate_tz(last_modified)\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n             age_seconds = time.time() - modified_stamp\n             age_days = age_seconds / 60 / 60 / 24\n             return age_days > self.image_refresh_days\n \n-    def s3_store_image(self, response, url):\n+        def _non200(_failure):\n+            return True\n+\n+        def _evaluate(should):\n+            if not should:\n+                self.inc_stats(info.domain, 'uptodate')\n+                referer = request.headers.get('Referer')\n+                log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n+                        (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n+                return key\n+\n+        key = self.s3_image_key(request.url)\n+        req = self.s3request(key, method='HEAD')\n+        dfd = self.download(req, info)\n+        dfd.addCallbacks(_on200, _non200)\n+        dfd.addCallback(_evaluate)\n+        dfd.addErrback(log.err, 'S3ImagesPipeline.media_to_download')\n+        return dfd\n+\n+    def s3_store_image(self, response, url, info):\n         \"\"\"Upload image to S3 storage\"\"\"\n         buf = StringIO(response.body.to_string())\n         image = Image.open(buf)\n         key = self.s3_image_key(url)\n-        self._s3_put_image(image, key)\n-        self.s3_store_thumbnails(image, url)\n+        self._s3_put_image(image, key, info)\n+        self.s3_store_thumbnails(image, url, info)\n \n-    def s3_store_thumbnails(self, image, url):\n+    def s3_store_thumbnails(self, image, url, info):\n         \"\"\"Upload image thumbnails to S3 storage\"\"\"\n         for thumb_id, size in self.THUMBS or []:\n             thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n             thumb.thumbnail(size, Image.ANTIALIAS)\n             key = self.s3_thumb_key(url, thumb_id)\n-            self._s3_put_image(thumb, key)\n+            self._s3_put_image(thumb, key, info)\n \n-    def s3_public_url(self, key):\n-        return \"http://%s.s3.amazonaws.com/%s\" % (self.bucket_name, key)\n-\n-    def _s3_put_image(self, image, key):\n+    def _s3_put_image(self, image, key, info):\n         buf = StringIO()\n         try:\n             image.save(buf, 'JPEG')\n         except Exception, ex:\n             raise ImageException(\"Cannot process image. Error: %s\" % ex)\n-\n         buf.seek(0)\n-        k = self.bucket.new_key(key)\n-        k.content_type = 'image/jpeg'\n-        k.set_contents_from_file(buf, policy='public-read')\n-        log.msg(\"Uploaded to S3: %s\" % self.s3_public_url(key), level=log.DEBUG)\n+\n+        headers = {\n+                'Content-Type': 'image/jpeg',\n+                'X-Amz-Acl': 'public-read',\n+                }\n+\n+        req = self.s3request(key, method='PUT', body=buf.read(), headers=headers)\n+        return self.download(req, info)\n+\n \n\n@@ -55,3 +55,4 @@ def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n     message = message + '\\n' + format_exc()\n     msg(message, level, component, domain)\n \n+err = log.err\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cbffced5947a57f93d25165545d9cde94ef092a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 89 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.41818181818181815\n\nDIFF:\n@@ -0,0 +1,89 @@\n+import re\n+import time\n+import hmac\n+import base64\n+import hashlib\n+\n+\n+METADATA_PREFIX = 'x-amz-meta-'\n+AMAZON_HEADER_PREFIX = 'x-amz-'\n+\n+\n+# generates the aws canonical string for the given parameters\n+def canonical_string(method, path, headers, expires=None):\n+    interesting_headers = {}\n+    for key in headers:\n+        lk = key.lower()\n+        if lk in ['content-md5', 'content-type', 'date'] or lk.startswith(AMAZON_HEADER_PREFIX):\n+            interesting_headers[lk] = headers[key].strip()\n+\n+    # these keys get empty strings if they don't exist\n+    if not interesting_headers.has_key('content-type'):\n+        interesting_headers['content-type'] = ''\n+    if not interesting_headers.has_key('content-md5'):\n+        interesting_headers['content-md5'] = ''\n+\n+    # just in case someone used this.  it's not necessary in this lib.\n+    if interesting_headers.has_key('x-amz-date'):\n+        interesting_headers['date'] = ''\n+\n+    # if you're using expires for query string auth, then it trumps date\n+    # (and x-amz-date)\n+    if expires:\n+        interesting_headers['date'] = str(expires)\n+\n+    sorted_header_keys = interesting_headers.keys()\n+    sorted_header_keys.sort()\n+\n+    buf = \"%s\\n\" % method\n+    for key in sorted_header_keys:\n+        if key.startswith(AMAZON_HEADER_PREFIX):\n+            buf += \"%s:%s\\n\" % (key, interesting_headers[key])\n+        else:\n+            buf += \"%s\\n\" % interesting_headers[key]\n+\n+    # don't include anything after the first ? in the resource...\n+    buf += \"%s\" % path.split('?')[0]\n+\n+    # ...unless there is an acl or torrent parameter\n+    if re.search(\"[&?]acl($|=|&)\", path):\n+        buf += \"?acl\"\n+    elif re.search(\"[&?]logging($|=|&)\", path):\n+        buf += \"?logging\"\n+    elif re.search(\"[&?]torrent($|=|&)\", path):\n+        buf += \"?torrent\"\n+    elif re.search(\"[&?]location($|=|&)\", path):\n+        buf += \"?location\"\n+\n+    return buf\n+\n+\n+def merge_meta(headers, metadata):\n+    final_headers = headers.copy()\n+    for k in metadata.keys():\n+        if k.lower() in ['content-md5', 'content-type', 'date']:\n+            final_headers[k] = metadata[k]\n+        else:\n+            final_headers[METADATA_PREFIX + k] = metadata[k]\n+\n+    return final_headers\n+\n+def get_aws_metadata(headers):\n+    metadata = {}\n+    for hkey in headers.keys():\n+        if hkey.lower().startswith(METADATA_PREFIX):\n+            metadata[hkey[len(METADATA_PREFIX):]] = headers[hkey]\n+            del headers[hkey]\n+    return metadata\n+\n+\n+def add_aws_auth_header(headers, method, path):\n+    if not headers.has_key('Date'):\n+        headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n+\n+    c_string = canonical_string(method, path, headers)\n+    _hmac = hmac.new(settings.AWS_SECRET_ACCESS_KEY, digestmod=hashlib.sha1)\n+    _hmac.update(c_string)\n+    b64_hmac = base64.encodestring(_hmac.digest()).strip()\n+    headers['Authorization'] = \"AWS %s:%s\" % (settings.AWS_ACCESS_KEY_ID, b64_hmac)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f9eba8a9764ecd6e09d653e1c555eb0600ac4b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 29 | Methods Changed: 20 | Complexity Δ (Sum/Max): 21/13 | Churn Δ: 100 | Churn Cumulative: 1984 | Contributors (this commit): 2 | Commits (past 90d): 49 | Contributors (cumulative): 6 | DMM Complexity: 0.6176470588235294\n\nDIFF:\n@@ -1,5 +1,6 @@\n import re\n from scrapy.utils.markup import replace_tags, remove_entities\n+from scrapy.item.adaptors import adaptize\n \n def remove_tags(value):\n     \"\"\"\n@@ -34,10 +35,10 @@ class Unquote(object):\n     Input: iterable with strings\n     Output: list of strings\n     \"\"\"\n-    def __init__(self, keep=['lt', 'amp']):\n+    def __init__(self, keep=None):\n+        self.keep = ['amp', 'lt'] if keep is None else keep\n+\n+    def __call__(self, value, keep=None):\n+        if keep is not None:\n             self.keep = keep\n-\n-    def __call__(self, value):\n         return [ remove_entities(v, keep=self.keep) for v in value ]\n-\n-\n\n@@ -5,6 +5,7 @@ from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.python import flatten\n+from scrapy.item.adaptors import adaptize\n \n def to_unicode(value):\n     \"\"\"\n@@ -72,10 +73,10 @@ def canonicalize_urls(value):\n     Output: list of unicodes(urls)\n     \"\"\"\n     if hasattr(value, '__iter__'):\n-        return [canonicalize_url(url) for url in value]\n+        return [canonicalize_url(str(url)) for url in value]\n     elif isinstance(value, basestring):\n-        return canonicalize_url(value)\n-    return u''\n+        return canonicalize_url(str(value))\n+    return ''\n \n class Delist(object):\n     \"\"\"\n@@ -88,7 +89,9 @@ class Delist(object):\n     def __init__(self, delimiter=' '):\n         self.delimiter = delimiter\n     \n-    def __call__(self, value):\n+    def __call__(self, value, delimiter=None):\n+        if delimiter is not None:\n+            self.delimiter = delimiter\n         return self.delimiter.join(value)\n \n class Regex(object):\n\n@@ -1,29 +1,45 @@\n+import inspect\n+\n from traceback import format_exc\n from scrapy.conf import settings\n \n-class AdaptorDict(dict):\n+class AdaptorPipe(list):\n     \"\"\"\n     Class that represents an item's attribute pipeline.\n \n-    This class contains a dictionary of attributes, matched with a list\n-    of adaptors to be run for filtering the input before storing.\n+    This class is itself a list, filled by adaptors to be run\n+    in order to filter the input.\n     \"\"\"\n+    def __init__(self, adaptors):\n+        super(AdaptorPipe, self).__init__([adaptor for adaptor in adaptors if callable(adaptor)])\n \n-    def execute(self, attrname, value, debug=False):\n+    def __call__(self, value, **kwargs):\n         \"\"\"\n-        Execute pipeline for attribute name \"attrname\" and value \"value\".\n+        Execute the adaptor pipeline for this attribute.\n         \"\"\"\n-        debug = debug or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n+        debug = kwargs.pop('debug', all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE']))\n \n-        for adaptor in self.get(attrname, []):\n+        for adaptor in self:\n+            if inspect.isfunction(adaptor):\n+                func_args, _, _ ,_ = inspect.getargspec(adaptor)\n+            else:\n+                func_args = []\n+\n+            name = getattr(adaptor, 'func_name', None)\n+            if not name:\n                 name = adaptor.__class__.__name__ if hasattr(adaptor, '__class__') else adaptor.__name__\n+\n             try:\n                 if debug:\n                     print \"  %07s | input >\" % name, repr(value)\n+\n+                if 'adaptor_args' in func_args:\n+                    value = adaptor(value, kwargs)\n+                else:\n                     value = adaptor(value)\n+\n                 if debug:\n                     print \"  %07s | output >\" % name, repr(value)\n-       \n             except Exception:\n                 print \"Error in '%s' adaptor. Traceback text:\" % name\n                 print format_exc()\n@@ -31,6 +47,29 @@ class AdaptorDict(dict):\n \n         return value\n \n+    def __add__(self, other):\n+        if isinstance(other, list):\n+            return AdaptorPipe(super(AdaptorPipe, self).__add__(other))\n+        elif callable(other):\n+            return AdaptorPipe(self + [other])\n \n-    def __repr__(self):\n-        return \"<AdaptorDict [ %d attributes ] >\" % len(self.keys())\n+def adaptize(adaptor, my_args=None):\n+    \"\"\"\n+    This decorator helps you add to your pipelines adaptors that are able\n+    to receive extra keyword arguments from the spiders.\n+\n+    If my_args is None, it'll try to guess the arguments your adaptor receives\n+    by instropection, and if it can't guess them, it will act as if it were a normal\n+    adaptor.\n+    To do the last, you may also call adaptize with my_args = [], or @adaptize([])\n+    \"\"\"\n+    if my_args is None:\n+        if inspect.isfunction(adaptor):\n+            my_args, _, _, _ = inspect.getargspec(adaptor)\n+        else:\n+            my_args = []\n+\n+    def _adaptor(value, **adaptor_args):\n+        kwargs = dict((key, val) for key, val in adaptor_args.items() if key in my_args)\n+        return adaptor(value, **kwargs)\n+    return _adaptor\n\n@@ -1,4 +1,4 @@\n-from scrapy.item.adaptors import AdaptorDict\n+from scrapy.item.adaptors import AdaptorPipe\n from scrapy.conf import settings\n \n class ScrapedItem(object):\n@@ -8,22 +8,25 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def set_adaptors(self, adaptors_dict, **kwargs):\n+    def set_adaptors(self, adaptors_dict):\n         \"\"\"\n         Set the adaptors to use for this item. Receives a dict of the adaptors\n         desired for each attribute and returns the item itself.\n         \"\"\"\n-        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))\n+        _adaptors_dict = dict(item for item in adaptors_dict.items() if isinstance(item[1], AdaptorPipe))\n+        self.__dict__['_adaptors_dict'] = _adaptors_dict\n         return self\n     \n-    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):\n+    def set_attrib_adaptors(self, attrib, pipe):\n         \"\"\"\n         Set the adaptors (from a list or tuple) to be used for a specific attribute.\n         \"\"\"\n-        self._adaptors_dict[attrib] = adaptors\n+        self._adaptors_dict[attrib] = AdaptorPipe(pipe) if hasattr(pipe, '__iter__') else None\n \n-    def attribute(self, attrname, value, override=False, add=False, debug=False):\n-        val = self._adaptors_dict.execute(attrname, value, debug)\n+    def attribute(self, attrname, value, **kwargs):\n+        pipe = self._adaptors_dict.get(attrname)\n+        if pipe:\n+            val = pipe(value, **kwargs)\n             if val or val is False:\n                 curr_val = getattr(self, attrname, None)\n                 if not curr_val:\n@@ -36,6 +39,8 @@ class ScrapedItem(object):\n                         newval.extend(curr_val)\n                         newval.extend(val)\n                         setattr(self, attrname, newval)\n+        elif value:\n+            setattr(self, attrname, value)\n \n     def __sub__(self, other):\n         raise NotImplementedError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82cf911192c5c36bea5d545b3bf0200cebdfa88a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 502 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,11 +18,22 @@ class ScrapedItem(object):\n         return self\n     \n     def set_attrib_adaptors(self, attrib, pipe):\n-        \"\"\"\n-        Set the adaptors (from a list or tuple) to be used for a specific attribute.\n-        \"\"\"\n+        \"\"\" Set the adaptors (from a list or tuple) to be used for a specific attribute. \"\"\"\n         self._adaptors_dict[attrib] = AdaptorPipe(pipe) if hasattr(pipe, '__iter__') else None\n \n+    def add_adaptor(self, attrib, adaptor, position=None):\n+        \"\"\"\n+        Add an adaptor for the specified attribute at the given position.\n+        If position = None, then the adaptor is appended at the end of the pipeline.\n+        \"\"\"\n+        if callable(adaptor):\n+            pipe = self._adaptors_dict.get(attrib, [])\n+            if position is None:\n+                pipe = pipe + [adaptor]\n+            else:\n+                pipe.insert(position, adaptor)\n+            self.set_attrib_adaptors(attrib, pipe)\n+\n     def attribute(self, attrname, value, **kwargs):\n         pipe = self._adaptors_dict.get(attrname)\n         if pipe:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f01a70ee7ba240319b36eaaed42542a25e59424", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -37,8 +37,10 @@ class Command(ScrapyCommand):\n                 else:\n                     result = spider.parse(response)\n \n-                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n                 links = [i for i in result if isinstance(i, Request)]\n+                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n+                for item in items:\n+                    del item._adaptors_dict\n \n                 display.nocolour = opts.nocolour\n                 if not opts.noitems:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9e8fa00d9b555930ca7ef7f93b8b76b39a83c18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,6 +40,7 @@ class Command(ScrapyCommand):\n                 links = [i for i in result if isinstance(i, Request)]\n                 items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n                 for item in items:\n+                    if hasattr(item, '_adaptors_dict'):\n                         del item._adaptors_dict\n \n                 display.nocolour = opts.nocolour\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68e497e53257074ccb7573fc544839c82e093568", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1763 | Contributors (this commit): 2 | Commits (past 90d): 40 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -69,7 +69,7 @@ def adaptize(adaptor, my_args=None):\n         else:\n             my_args = []\n \n-    def _adaptor(value, **adaptor_args):\n+    def _adaptor(value, adaptor_args):\n         kwargs = dict((key, val) for key, val in adaptor_args.items() if key in my_args)\n         return adaptor(value, **kwargs)\n     return _adaptor\n\n@@ -35,6 +35,8 @@ class ScrapedItem(object):\n             self.set_attrib_adaptors(attrib, pipe)\n \n     def attribute(self, attrname, value, **kwargs):\n+        override = kwargs.pop('override', False)\n+        add = kwargs.pop('add', False)\n         pipe = self._adaptors_dict.get(attrname)\n         if pipe:\n             val = pipe(value, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8903d5877880389e312beba0bca638432df2edf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,8 +40,7 @@ class Command(ScrapyCommand):\n                 links = [i for i in result if isinstance(i, Request)]\n                 items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n                 for item in items:\n-                    if hasattr(item, '_adaptors_dict'):\n-                        del item._adaptors_dict\n+                    item.__dict__.pop('_adaptors_dict', None)\n \n                 display.nocolour = opts.nocolour\n                 if not opts.noitems:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5028146e8c2dff20174089f73d54ee2d85e52fca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 112 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -77,6 +77,9 @@ class Link(object):\n     Link objects represent an extracted link by the LinkExtractor.\n     At the moment, it contains just the url and link text.\n     \"\"\"\n+\n+    __slots__ = 'url', 'text'\n+\n     def __init__(self, url, text=''):\n         self.url = url\n         self.text = text\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f558a49d2cdc7f4e100225b7d68c982308c6e1e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 142 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ Adaptors related with extraction of data\n \"\"\"\n \n import urlparse\n+import re\n from scrapy.http import Response\n from scrapy.utils.python import flatten\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n@@ -51,7 +52,19 @@ class ExtractImages(object):\n     Output: list of unicodes\n     \"\"\"\n     def __init__(self, response=None, base_url=None):\n-        self.base_url = response.url if response else base_url\n+        BASETAG_RE = re.compile(r'<base\\s+href\\s*=\\s*[\\\"\\']\\s*([^\\\"\\'\\s]+)\\s*[\\\"\\']', re.I)\n+\n+        if response:\n+            match = BASETAG_RE.search(response.body.to_string()[0:4096])\n+            if match:\n+                self.base_url = match.group(1)\n+            else:\n+                self.base_url = response.url\n+        else:\n+            self.base_url = base_url\n+\n+        if not self.base_url:\n+            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n \n     def extract_from_xpath(self, selector):\n         ret = []\n@@ -76,9 +89,6 @@ class ExtractImages(object):\n         return ret\n \n     def __call__(self, locations):\n-        if not self.base_url:\n-            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n-        \n         if isinstance(locations, basestring):\n             locations = [locations]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0fbb7579f651cb2f68d7847d3c48bbe765284364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 83 | Churn Cumulative: 83 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.6470588235294118\n\nDIFF:\n@@ -0,0 +1,83 @@\n+\"\"\"\n+This module contains the basic crawling spider that you can use to inherit your\n+spider from.\n+\"\"\"\n+\n+from scrapy.http import Request\n+from scrapy.spider import BaseSpider\n+from scrapy.item import ScrapedItem\n+\n+class CrawlSpider(BaseSpider):\n+    \"\"\"\n+    This is the base class for crawling spiders. It is based on a list of\n+    crawling rules (stored in the \"rules\" attribute) which specify how links\n+    are extracted and followed, and how pages are processed (by using a\n+    callback function).\n+\n+    For more info about rules see the Rule class.\n+    \"\"\"\n+\n+    def parse(self, response):\n+        \"\"\"Method called by the framework core for all the start_urls. Do not\n+        override this function, override parse_start_url instead.\n+        \"\"\"\n+        if response.url in self.start_urls:\n+            return self._parse_wrapper(response, self.parse_start_url, cb_kwargs={}, follow=True) \n+        else:\n+            return self.parse_url(response)\n+\n+    def parse_start_url(self, response):\n+        \"\"\"Callback function for processing start_urls. It must return a list\n+        of ScrapedItems and/or Requests.\n+        \"\"\"\n+        return []\n+\n+    def _requests_to_follow(self, response):\n+        requests = []\n+        seen = set()\n+        for rule in self.rules:\n+            callback = rule.callback if callable(rule.callback) else getattr(self, rule.callback, None)\n+            links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]\n+            seen.union(links)\n+            for link in links:\n+                r = Request(url=link.url, link_text=link.text)\n+                r.append_callback(self._parse_wrapper, callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n+                requests.append(r)\n+        return requests\n+\n+    def _parse_wrapper(self, response, callback, cb_kwargs, follow):\n+        res = []\n+        if follow:\n+            res.extend(self._requests_to_follow(response))\n+        if callback:\n+            res.extend(callback(response, **cb_kwargs) or ())\n+        return res\n+\n+\n+class Rule(object):\n+    \"\"\"\n+    A rule for crawling, which receives the following constructor arguments:\n+\n+    link_extractor (required)\n+       A LinkExtractor which defines the policy for extracting links\n+    callback (optional)\n+       A function to use to process the page once it has been downloaded. If\n+       callback is omitted the page is not procesed, just crawled. If callback\n+       is a string (instead of callable) a method of the spider class with that\n+       name is used as the callback function\n+    cb_kwargs (optional)\n+       A dict specifying keyword arguments to pass to the callback function\n+    follow (optional)\n+       If True, links will be followed from the pages crawled by this rule.\n+       It defaults to True when no callback is specified or False when a\n+       callback is specified\n+    \"\"\"\n+    \n+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None):\n+        self.link_extractor = link_extractor\n+        self.callback = callback\n+        self.cb_kwargs = cb_kwargs or {}\n+        if follow is None:\n+            self.follow = False if callback else True\n+        else:\n+            self.follow = follow \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#072fed8d4acc21c08bd28e7e1397f2f9d7a7c961", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 41 | Files Changed: 3 | Hunks: 14 | Methods Changed: 15 | Complexity Δ (Sum/Max): -1/8 | Churn Δ: 169 | Churn Cumulative: 548 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import time\n import hmac\n import base64\n import hashlib\n+from urlparse import urlsplit\n \n \n METADATA_PREFIX = 'x-amz-meta-'\n@@ -14,17 +15,15 @@ def canonical_string(method, path, headers, expires=None):\n     interesting_headers = {}\n     for key in headers:\n         lk = key.lower()\n-        if lk in ['content-md5', 'content-type', 'date'] or lk.startswith(AMAZON_HEADER_PREFIX):\n+        if lk in set('content-md5', 'content-type', 'date') or lk.startswith(AMAZON_HEADER_PREFIX):\n             interesting_headers[lk] = headers[key].strip()\n \n     # these keys get empty strings if they don't exist\n-    if not interesting_headers.has_key('content-type'):\n-        interesting_headers['content-type'] = ''\n-    if not interesting_headers.has_key('content-md5'):\n-        interesting_headers['content-md5'] = ''\n+    interesting_headers.setdefault('content-type', '')\n+    interesting_headers.setdefault('content-md5', '')\n \n     # just in case someone used this.  it's not necessary in this lib.\n-    if interesting_headers.has_key('x-amz-date'):\n+    if 'x-amz-date' in interesting_headers:\n         interesting_headers['date'] = ''\n \n     # if you're using expires for query string auth, then it trumps date\n@@ -58,32 +57,20 @@ def canonical_string(method, path, headers, expires=None):\n     return buf\n \n \n-def merge_meta(headers, metadata):\n-    final_headers = headers.copy()\n-    for k in metadata.keys():\n-        if k.lower() in ['content-md5', 'content-type', 'date']:\n-            final_headers[k] = metadata[k]\n-        else:\n-            final_headers[METADATA_PREFIX + k] = metadata[k]\n \n-    return final_headers\n+def sign_request(req, accesskey, secretkey):\n+    if 'Date' not in req.headers:\n+        req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n \n-def get_aws_metadata(headers):\n-    metadata = {}\n-    for hkey in headers.keys():\n-        if hkey.lower().startswith(METADATA_PREFIX):\n-            metadata[hkey[len(METADATA_PREFIX):]] = headers[hkey]\n-            del headers[hkey]\n-    return metadata\n+    parsed = urlsplit(req.url)\n+    bucket = parsed.hostname.replace('.s3.amazonaws.com','')\n+    key = '%s?%s' % (parsed.path, parsed.query) if parsed.query else parsed.path\n+    fqkey = '/%s%s' % (bucket, key)\n \n-\n-def add_aws_auth_header(headers, method, path):\n-    if not headers.has_key('Date'):\n-        headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n-\n-    c_string = canonical_string(method, path, headers)\n-    _hmac = hmac.new(settings.AWS_SECRET_ACCESS_KEY, digestmod=hashlib.sha1)\n+    c_string = canonical_string(req.method, fqkey, req.headers)\n+    _hmac = hmac.new(secretkey, digestmod=hashlib.sha1)\n     _hmac.update(c_string)\n     b64_hmac = base64.encodestring(_hmac.digest()).strip()\n-    headers['Authorization'] = \"AWS %s:%s\" % (settings.AWS_ACCESS_KEY_ID, b64_hmac)\n+    req.headers['Authorization'] = \"AWS %s:%s\" % (accesskey, b64_hmac)\n+\n \n\n@@ -12,7 +12,7 @@ from scrapy.http import Request\n from scrapy.stats import stats\n from scrapy.core.exceptions import DropItem, NotConfigured, HttpException\n from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.contrib.aws import canonical_string\n+from scrapy.contrib.aws import canonical_string, sign_request\n from scrapy.conf import settings\n \n from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n@@ -33,23 +33,14 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         self.bucket_name = settings['S3_BUCKET']\n         self.prefix = settings['S3_PREFIX']\n         self.access_key = settings['AWS_ACCESS_KEY_ID']\n+        self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n         self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n-        self._hmac = hmac.new(settings['AWS_SECRET_ACCESS_KEY'], digestmod=hashlib.sha1)\n         MediaPipeline.__init__(self)\n \n     def s3request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s' % (self.bucket_name, key)\n         req = Request(url, method=method, body=body, headers=headers)\n-\n-        if not (headers and 'Date' in headers):\n-            req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n-\n-        fullkey = '/%s/%s' % (self.bucket_name, key)\n-        c_string = canonical_string(method, fullkey, req.headers)\n-        _hmac = self._hmac.copy()\n-        _hmac.update(c_string)\n-        b64_hmac = base64.encodestring(_hmac.digest()).strip()\n-        req.headers['Authorization'] = \"AWS %s:%s\" % (self.access_key, b64_hmac)\n+        sign_request(req, self.access_key, self.secret_keself.secret_keyy)\n         return req\n \n     def image_downloaded(self, response, request, info):\n\n@@ -0,0 +1,109 @@\n+from unittest import TestCase, main\n+from scrapy.contrib import aws\n+from scrapy.http import Request\n+\n+# keys are provided by amazon developer guide at\n+# http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n+# and the tests described here are the examples from that manual\n+\n+AWS_ACCESS_KEY_ID = '0PN5J17HBGZHT7JJ3X82'\n+AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n+\n+\n+class ScrapyAWSTest(TestCase):\n+    def test_cannonical_string1(self):\n+        cs = aws.canonical_string('GET', '/johnsmith/photos/puppy.jpg', {\n+            'Host': 'johnsmith.s3.amazonaws.com',\n+            'Date': 'Tue, 27 Mar 2007 19:36:42 +0000',\n+            })\n+        self.assertEqual(cs, \\\n+                '''GET\\n\\n\\nTue, 27 Mar 2007 19:36:42 +0000\\n/johnsmith/photos/puppy.jpg''')\n+\n+    def test_cannonical_string2(self):\n+        cs = aws.canonical_string('PUT', '/johnsmith/photos/puppy.jpg', {\n+            'Content-Type': 'image/jpeg',\n+            'Host': 'johnsmith.s3.amazonaws.com',\n+            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n+            'Content-Length': '94328',\n+            })\n+        self.assertEqual(cs, \\\n+                '''PUT\\n\\nimage/jpeg\\nTue, 27 Mar 2007 21:15:45 +0000\\n/johnsmith/photos/puppy.jpg''')\n+\n+    def test_request_signing1(self):\n+        # gets an object from the johnsmith bucket.\n+        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', headers={\n+            'Date': 'Tue, 27 Mar 2007 19:36:42 +0000',\n+            })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n+\n+    def test_request_signing2(self):\n+        # puts an object into the johnsmith bucket.\n+        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', method='PUT', headers={\n+            'Content-Type': 'image/jpeg',\n+            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n+            'Content-Length': '94328',\n+            })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n+\n+    def test_request_signing3(self):\n+        # lists the content of the johnsmith bucket.\n+        req = Request('http://johnsmith.s3.amazonaws.com/?prefix=photos&max-keys=50&marker=puppy', \\\n+                method='GET', headers={\n+                    'User-Agent': 'Mozilla/5.0',\n+                    'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n+                    })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n+\n+    def test_request_signing4(self):\n+        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n+        req = Request('http://johnsmith.s3.amazonaws.com/?acl', \\\n+                method='GET', headers={\n+                    'Date': 'Tue, 27 Mar 2007 19:44:46 +0000',\n+                    })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n+\n+    def test_request_signing5(self):\n+        # deletes an object from the 'johnsmith' bucket using the path-style and Date alternative.\n+        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', \\\n+                method='DELETE', headers={\n+                    'Date': 'Tue, 27 Mar 2007 21:20:27 +0000',\n+                    'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n+                    })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n+\n+    def test_request_signing6(self):\n+        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n+        req = Request('http://static.johnsmith.net:8080/db-backup.dat.gz', \\\n+                method='PUT', headers={\n+                    'User-Agent': 'curl/7.15.5',\n+                    'Host': 'static.johnsmith.net:8080',\n+                    'Date': 'Tue, 27 Mar 2007 21:06:08 +0000',\n+                    'x-amz-acl': 'public-read',\n+                    'content-type': 'application/x-download',\n+                    'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',\n+                    'X-Amz-Meta-ReviewedBy': 'joe@johnsmith.net,jane@johnsmith.net',\n+                    'X-Amz-Meta-FileChecksum': '0x02661779',\n+                    'X-Amz-Meta-ChecksumAlgorithm': 'crc32',\n+                    'Content-Disposition': 'attachment; filename=database.dat',\n+                    'Content-Encoding': 'gzip',\n+                    'Content-Length': '5913339',\n+                    })\n+        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n+        self.assertEqual(req.headers['Authorization'], \\\n+                'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n+\n+\n+\n+\n+if __name__ == '__main__':\n+    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
