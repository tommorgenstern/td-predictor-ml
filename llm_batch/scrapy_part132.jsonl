{"custom_id": "scrapy#c35a7519c0ebc18fe79da0d0e50940334ec4870a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,8 +27,10 @@ class S3DownloadHandler(object):\n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n         scheme = 'https' if request.meta.get('is_secure') else 'http'\n-        url = '%s://%s.s3.amazonaws.com%s' % (scheme, p.hostname, p.path)\n+        bucket = p.hostname\n+        path = p.path + '?' + p.query if p.query else p.path\n+        url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n         httpreq = request.replace(url=url)\n         self.conn.add_aws_auth_header(httpreq.headers, httpreq.method, \\\n-                '%s/%s' % (p.hostname, p.path))\n+                '%s/%s' % (bucket, path))\n         return self._download_http(httpreq, spider)\n\n@@ -4,5 +4,9 @@ import unittest\n \n class UrlparseTestCase(unittest.TestCase):\n \n-    def test_s3_netloc(self):\n-        self.assertEqual(urlparse('s3://bucket/key').netloc, 'bucket')\n+    def test_s3_url(self):\n+        p = urlparse('s3://bucket/key/name?param=value')\n+        self.assertEquals(p.scheme, 's3')\n+        self.assertEquals(p.hostname, 'bucket')\n+        self.assertEquals(p.path, '/key/name')\n+        self.assertEquals(p.query, 'param=value')\n\n@@ -1,5 +1,7 @@\n-from urlparse import urlparse, uses_netloc\n+from urlparse import urlparse, uses_netloc, uses_query\n \n # workaround for http://bugs.python.org/issue7904\n if urlparse('s3://bucket/key').netloc != 'bucket':\n     uses_netloc.append('s3')\n+if urlparse('s3://bucket/key?key=value').query != 'key=value':\n+    uses_query.append('s3')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f03af7874d3cada97f2ef4d6f4e3575317f4d326", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 238 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class Scheduler(object):\n         return cls(dupefilter, job_dir(settings), dqclass)\n \n     def has_pending_requests(self):\n-        return self.dqs or self.mqs\n+        return len(self.dqs) or len(self.mqs)\n \n     def open(self, spider):\n         self.spider = spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#984be354611109700b2cb75c6537efc48edf3e50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,14 +61,19 @@ class TelnetConsole(protocol.ServerFactory):\n \n     def _get_telnet_vars(self):\n         # Note: if you add entries here also update topics/telnetconsole.rst\n+        slots = self.crawler.engine.slots\n+        if len(slots) == 1:\n+            spider, slot = slots.items()[0]\n         telnet_vars = {\n             'engine': self.crawler.engine,\n+            'spider': spider,\n+            'slot': slot,\n             'manager': self.crawler,\n             'extensions': self.crawler.extensions,\n             'stats': stats,\n             'spiders': self.crawler.spiders,\n             'settings': self.crawler.settings,\n-            'est': lambda x: print_engine_status(self.crawler),\n+            'est': lambda: print_engine_status(self.crawler.engine),\n             'p': pprint.pprint,\n             'prefs': print_live_refs,\n             'hpy': hpy,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c64123cc63fe3067d28a576f879a0debd04d55ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 5 | Churn Cumulative: 301 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,7 +26,7 @@ class Scheduler(object):\n         return cls(dupefilter, job_dir(settings), dqclass)\n \n     def has_pending_requests(self):\n-        return len(self.dqs) or len(self.mqs)\n+        return len(self) > 0\n \n     def open(self, spider):\n         self.spider = spider\n\n@@ -53,6 +53,3 @@ class PriorityQueue(object):\n \n     def __len__(self):\n         return sum(len(x) for x in self.queues.values()) if self.queues else 0\n-\n-    def __nonzero__(self):\n-        return bool(self.queues)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#436ad6393005179d71a4be358286b27e9ed17d81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 37 | Churn Cumulative: 85 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,29 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.conf import settings\n from .http import HttpDownloadHandler\n \n+try:\n+    from boto.s3.connection import S3Connection\n+except ImportError:\n+    S3Connection = object\n+\n+class _v19_S3Connection(S3Connection):\n+    \"\"\"A dummy S3Connection wrapper that doesn't do any syncronous download\"\"\"\n+    def _mexe(self, method, bucket, key, headers, *args, **kwargs):\n+        return headers\n+\n+class _v20_S3Connection(S3Connection):\n+    \"\"\"A dummy S3Connection wrapper that doesn't do any syncronous download\"\"\"\n+    def _mexe(self, http_request, *args):\n+        http_request.authorize(connection=self)\n+        return http_request.headers\n+\n+try:\n+    import boto.auth\n+except ImportError:\n+    _S3Connection = _v19_S3Connection\n+else:\n+    _S3Connection = _v20_S3Connection\n+\n \n class S3DownloadHandler(object):\n \n@@ -17,9 +40,8 @@ class S3DownloadHandler(object):\n         if not aws_secret_access_key:\n             aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n \n-        from boto import connect_s3\n         try:\n-            self.conn = connect_s3(aws_access_key_id, aws_secret_access_key)\n+            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)\n         except Exception, ex:\n             raise NotConfigured(str(ex))\n         self._download_http = httpdownloadhandler().download_request\n@@ -30,7 +52,12 @@ class S3DownloadHandler(object):\n         bucket = p.hostname\n         path = p.path + '?' + p.query if p.query else p.path\n         url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n-        httpreq = request.replace(url=url)\n-        self.conn.add_aws_auth_header(httpreq.headers, httpreq.method, \\\n-                '%s/%s' % (bucket, path))\n+        signed_headers = self.conn.make_request(\n+                method=request.method,\n+                bucket=bucket,\n+                key=p.path,\n+                query_args=p.query,\n+                headers=request.headers,\n+                data=request.body)\n+        httpreq = request.replace(url=url, headers=signed_headers)\n         return self._download_http(httpreq, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2108517ce02038556b3629482275dbd791e64a65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 123 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,8 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         if len(args) < 1:\n             raise UsageError()\n+        elif len(args) > 1:\n+            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n         for spname in args:\n             spider = self.crawler.spiders.create(spname, **opts.spargs)\n             self.crawler.crawl(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#061132ce88c6d54fd6014ded09b83a95ad78e7ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 398 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,14 +17,18 @@ class MediaPipeline(object):\n             self.downloaded = {}\n             self.waiting = defaultdict(list)\n \n-    def __init__(self, download_func=None, crawler=None):\n+    def __init__(self, download_func=None):\n         self.spiderinfo = {}\n         self.download_func = download_func\n-        self.crawler = crawler\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler=crawler)\n+        try:\n+            pipe = cls.from_settings(crawler.settings)\n+        except AttributeError:\n+            pipe = cls()\n+        pipe.crawler = crawler\n+        return pipe\n \n     def open_spider(self, spider):\n         self.spiderinfo[spider] = self.SpiderInfo(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#415933a1f6b68d6deba825a3c16c8b43085a4abc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ def start_python_console(namespace=None, noipython=False):\n                 raise ImportError\n             import IPython\n             try:\n-                shell = IPython.embed(user_ns=namespace)\n+                IPython.embed(user_ns=namespace)\n             except AttributeError:\n                 shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n                 shell()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09af0866c7aae482628df144ecc50f38b9606586", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 34 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ def start_python_console(namespace=None, noipython=False):\n                 raise ImportError\n             import IPython\n             try:\n-                shell = IPython.embed(user_ns=namespace)\n+                IPython.embed(user_ns=namespace)\n             except AttributeError:\n                 shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n                 shell()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19e6da59d82c96e1b804438496354c9a5f6c687b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 53 | Churn Cumulative: 214 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,13 @@\n+from scrapy.utils.http import decode_chunked_transfer\n+\n+\n+class ChunkedTransferMiddleware(object):\n+    \"\"\"This middleware adds support for chunked transfer encoding, as\n+    documented in: http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n+    \"\"\"\n+\n+    def process_response(self, request, response, spider):\n+        if response.headers.get('Transfer-Encoding') == 'chunked':\n+            body = decode_chunked_transfer(response.body)\n+            return response.replace(body=body)\n+        return response\n\n@@ -81,6 +81,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n     'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n+    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n     'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n     # Downloader side\n\n@@ -0,0 +1,20 @@\n+import unittest\n+\n+from scrapy.utils.http import decode_chunked_transfer\n+\n+class ChunkedTest(unittest.TestCase):\n+\n+    def test_decode_chunked_transfer(self):\n+        \"\"\"Example taken from: http://en.wikipedia.org/wiki/Chunked_transfer_encoding\"\"\"\n+        chunked_body = \"25\\r\\n\" + \"This is the data in the first chunk\\r\\n\\r\\n\"\n+        chunked_body += \"1C\\r\\n\" + \"and this is the second one\\r\\n\\r\\n\"\n+        chunked_body += \"3\\r\\n\" + \"con\\r\\n\"\n+        chunked_body += \"8\\r\\n\" + \"sequence\\r\\n\"\n+        chunked_body += \"0\\r\\n\\r\\n\"\n+        body = decode_chunked_transfer(chunked_body)\n+        self.assertEqual(body, \\\n+            \"This is the data in the first chunk\\r\\n\" +\n+            \"and this is the second one\\r\\n\" +\n+            \"consequence\")\n+\n+\n\n@@ -5,3 +5,22 @@ For new code, always import from w3lib.http instead of this module\n \"\"\"\n \n from w3lib.http import *\n+\n+def decode_chunked_transfer(chunked_body):\n+    \"\"\"Parsed body received with chunked transfer encoding, and return the\n+    decoded body.\n+\n+    For more info see:\n+    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n+\n+    \"\"\"\n+    body, h, t = '', '', chunked_body\n+    while t:\n+        h, t = t.split('\\r\\n', 1)\n+        if h == '0':\n+            break\n+        size = int(h, 16)\n+        body += t[:size]\n+        t = t[size+2:]\n+    return body\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee40aa1223bf1fe54d90d3e47f6ba0308a78f8c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 516 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,4 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n \n class Command(ScrapyCommand):\n \n@@ -11,7 +9,5 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])\n-        spiders = spman_cls.from_settings(settings)\n-        for s in spiders.list():\n+        for s in self.crawler.spiders.list():\n             print s\n\n@@ -33,7 +33,7 @@ class Crawler(object):\n         self.configured = True\n         self.extensions = ExtensionManager.from_crawler(self)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-        self.spiders = spman_cls.from_settings(self.settings)\n+        self.spiders = spman_cls.from_crawler(self)\n         self.engine = ExecutionEngine(self, self._spider_closed)\n \n     def crawl(self, spider, requests=None):\n\n@@ -5,7 +5,7 @@ spiders\n \n from zope.interface import implements\n \n-from scrapy import log, signals\n+from scrapy import signals\n from scrapy.interfaces import ISpiderManager\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n@@ -32,6 +32,10 @@ class SpiderManager(object):\n     def from_settings(cls, settings):\n         return cls(settings.getlist('SPIDER_MODULES'))\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls.from_settings(crawler.settings)\n+\n     def create(self, spider_name, **spider_kwargs):\n         try:\n             return self._spiders[spider_name](**spider_kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d97e73a24f2b8b47da81303d55386135f4fca2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 355 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class DepthMiddleware(object):\n                 depth = response.request.meta['depth'] + 1\n                 request.meta['depth'] = depth\n                 if self.prio:\n-                    request.priority += depth * self.prio\n+                    request.priority -= depth * self.prio\n                 if self.maxdepth and depth > self.maxdepth:\n                     log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), \\\n                         level=log.DEBUG, spider=spider)\n\n@@ -58,7 +58,7 @@ class Scheduler(object):\n             return\n         try:\n             reqd = request_to_dict(request, self.spider)\n-            self.dqs.push(reqd, request.priority)\n+            self.dqs.push(reqd, -request.priority)\n         except ValueError: # non serializable request\n             return\n         else:\n@@ -67,7 +67,7 @@ class Scheduler(object):\n \n     def _mqpush(self, request):\n         stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n-        self.mqs.push(request, request.priority)\n+        self.mqs.push(request, -request.priority)\n \n     def _dqpop(self):\n         if self.dqs:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#669b98c4fc107960ee98ecbfe61c5cab10867810", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 296 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ class Scheduler(object):\n             prios = self.dqs.close()\n             with open(join(self.dqdir, 'active.json'), 'w') as f:\n                 json.dump(prios, f)\n-        return self.df.close()\n+        return self.df.close(reason)\n \n     def enqueue_request(self, request):\n         if not request.dont_filter and self.df.request_seen(request):\n\n@@ -15,7 +15,7 @@ class BaseDupeFilter(object):\n     def open(self):  # can return deferred\n         pass\n \n-    def close(self): # can return a deferred\n+    def close(self, reason): # can return a deferred\n         pass\n \n \n@@ -41,6 +41,6 @@ class RFPDupeFilter(BaseDupeFilter):\n         if self.file:\n             self.file.write(fp + os.linesep)\n \n-    def close(self):\n+    def close(self, reason):\n         if self.file:\n             self.file.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47cae5fa35492ea3d66c9db9d19607b65c7cd961", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 94 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,4 +20,4 @@ class RFPDupeFilterTest(unittest.TestCase):\n         assert not filter.request_seen(r2)\n         assert filter.request_seen(r3)\n \n-        filter.close()\n+        filter.close('finished')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce0850485332eca7d728c78bb23ca116f099b437", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,9 +2,6 @@ from zope.interface import Interface\n \n class ISpiderManager(Interface):\n \n-    def from_settings(settings):\n-        \"\"\"Class method to instantiate from settings\"\"\"\n-\n     def create(spider_name, **spider_args):\n         \"\"\"Returns a new Spider instance for the given spider name, and using\n         the given spider arguments. If the spider name is not found, it must\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#075a2d62d32fff5c4ac34445901776512c9ca37a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 13 | Churn Cumulative: 343 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,12 @@ class UtilsTest(unittest.TestCase):\n         self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n         assert all(isinstance(x, str) for x in cargs), cargs\n \n+    def test_get_crawl_args_with_settings(self):\n+        msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1', 'settings': {'ONE': 'two'}}\n+        cargs = get_crawl_args(msg)\n+        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '--set', 'ONE=two'])\n+        assert all(isinstance(x, str) for x in cargs), cargs\n+\n class GetSpiderListTest(unittest.TestCase):\n \n     def test_get_spider_list(self):\n\n@@ -40,9 +40,13 @@ def get_crawl_args(message):\n     msg = message.copy()\n     args = [unicode_to_str(msg['_spider'])]\n     del msg['_project'], msg['_spider']\n+    settings = msg.pop('settings', {})\n     for k, v in stringify_dict(msg, keys_only=False).items():\n         args += ['-a']\n         args += ['%s=%s' % (k, v)]\n+    for k, v in stringify_dict(settings, keys_only=False).items():\n+        args += ['--set']\n+        args += ['%s=%s' % (k, v)]\n     return args\n \n def get_spider_list(project, runner=None):\n\n@@ -23,9 +23,12 @@ class WsResource(JsonResource):\n class Schedule(WsResource):\n \n     def render_POST(self, txrequest):\n+        settings = txrequest.args.pop('setting', [])\n+        settings = dict(x.split('=', 1) for x in settings)\n         args = dict((k, v[0]) for k, v in txrequest.args.items())\n         project = args.pop('project')\n         spider = args.pop('spider')\n+        args['settings'] = settings\n         jobid = uuid.uuid1().hex\n         args['_job'] = jobid\n         self.root.scheduler.schedule(project, spider, **args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1aff779dac765baeab051bfac55030450df6fd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 75 | Files Changed: 3 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 75 | Churn Cumulative: 234 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,61 +0,0 @@\n-from zope.interface import Interface, implements\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n-from scrapy.utils.sqlite import JsonSqliteDict\n-from scrapy.utils.project import sqlite_db\n-from scrapy import signals\n-\n-class ISpiderContextStorage(Interface):\n-\n-    def get(spider):\n-        \"\"\"Get the context for the given spider, or None if the spider has no\n-        context stored.\"\"\"\n-\n-    def put(spider, context):\n-        \"\"\"Store the context for the given spider\"\"\"\n-\n-\n-class SqliteSpiderContextStorage(object):\n-\n-    implements(ISpiderContextStorage)\n-    sqlite_dict_class = JsonSqliteDict\n-\n-    def __init__(self, database=None, table='contexts'):\n-        self.d = self.sqlite_dict_class(database, table)\n-\n-    @classmethod\n-    def from_settings(cls, settings):\n-        return cls(sqlite_db(settings['SQLITE_DB']))\n-\n-    def get(self, spider):\n-        if spider.name in self.d:\n-            return self.d[spider.name]\n-\n-    def put(self, spider, context):\n-        self.d[spider.name] = context\n-\n-\n-class SpiderContext(object):\n-\n-    def __init__(self, storage):\n-        dispatcher.connect(self._spider_opened, signals.spider_opened)\n-        dispatcher.connect(self._spider_closed, signals.spider_closed)\n-        self.storage = storage\n-\n-    @classmethod\n-    def from_settings(cls, settings):\n-        if not settings.getbool('SPIDER_CONTEXT_ENABLED'):\n-            raise NotConfigured\n-        stcls = load_object(settings['SPIDER_CONTEXT_STORAGE_CLASS'])\n-        storage = stcls.from_settings(settings)\n-        return cls(storage)\n-\n-    def _spider_opened(self, spider):\n-        spider.context = self.storage.get(spider) or {}\n-\n-    def _spider_closed(self, spider):\n-        if spider.context:\n-            self.storage.put(spider, spider.context)\n-\n\n@@ -143,7 +143,6 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closespider.CloseSpider': 0,\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n-    'scrapy.contrib.spidercontext.SpiderContext': 0,\n     'scrapy.contrib.logstats.LogStats': 0,\n }\n \n@@ -244,10 +243,6 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n-SPIDER_CONTEXT_ENABLED = True\n-SPIDER_CONTEXT_STORAGE_CLASS = 'scrapy.contrib.spidercontext.SqliteSpiderContextStorage'\n-\n-\n SQLITE_DB = 'scrapy.db'\n \n STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n\n@@ -1,9 +0,0 @@\n-from twisted.trial import unittest\n-from zope.interface.verify import verifyObject\n-\n-from scrapy.contrib.spidercontext import ISpiderContextStorage, SqliteSpiderContextStorage\n-\n-class SqliteSpiderContextStorageTest(unittest.TestCase):\n-\n-    def test_interface(self):\n-        verifyObject(ISpiderContextStorage, SqliteSpiderContextStorage())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91b9d89ffd9e687da4a92e33822eec4ac32797ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/14 | Churn Δ: 33 | Churn Cumulative: 255 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 0.8125\n\nDIFF:\n@@ -40,24 +40,3 @@ def data_path(path):\n     otherwise return the path unmodified\n     \"\"\"\n     return path if isabs(path) else join(project_data_dir(), path)\n-\n-def sqlite_db(path, nonwritable_fallback=True):\n-    \"\"\"Get the SQLite database to use. If path is relative, returns the given\n-    path inside the project data dir, otherwise returns the path unmodified. If\n-    not inside a project returns :memory: to use an in-memory database.\n-\n-    If nonwritable_fallback is True, and the path is not writable it issues a\n-    warning and returns :memory:\n-    \"\"\"\n-    if not inside_project() or path == ':memory:':\n-        db = ':memory:'\n-    else:\n-        try:\n-            db = data_path(path)\n-        except NotConfigured:\n-            db = ':memory:'\n-        else:\n-            if not is_writable(db) and nonwritable_fallback:\n-                warnings.warn(\"%r is not writable - using in-memory SQLite instead\" % db)\n-                db = ':memory:'\n-    return db\n\n@@ -49,9 +49,6 @@ class IPoller(Interface):\n \n class ISpiderQueue(Interface):\n \n-    def from_settings(settings):\n-        \"\"\"Class method to instantiate from settings\"\"\"\n-\n     def add(name, **spider_args):\n         \"\"\"Add a spider to the queue given its name a some spider arguments.\n \n\n@@ -1,8 +1,7 @@\n from zope.interface import implements\n \n from scrapyd.interfaces import ISpiderQueue\n-from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n-from scrapy.utils.project import sqlite_db\n+from scrapyd.sqlite import JsonSqlitePriorityQueue\n \n \n class SqliteSpiderQueue(object):\n@@ -12,10 +11,6 @@ class SqliteSpiderQueue(object):\n     def __init__(self, database=None, table='spider_queue'):\n         self.q = JsonSqlitePriorityQueue(database, table)\n \n-    @classmethod\n-    def from_settings(cls, settings):\n-        return cls(sqlite_db(settings['SQLITE_DB']))\n-\n     def add(self, name, **spider_args):\n         d = spider_args.copy()\n         d['name'] = name\n\n\n@@ -3,7 +3,7 @@ from datetime import datetime\n from decimal import Decimal\n \n from scrapy.http import Request\n-from scrapy.utils.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \\\n+from scrapyd.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \\\n     PickleSqlitePriorityQueue, SqliteDict, JsonSqliteDict, PickleSqliteDict\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75284015b54a14cd7f3378ff214d7f5f1a3cffcb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 44 | Files Changed: 14 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 128 | Churn Cumulative: 1651 | Contributors (this commit): 7 | Commits (past 90d): 42 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,8 +5,6 @@ from scrapy.utils.datatypes import CaselessDict\n class Headers(CaselessDict):\n     \"\"\"Case insensitive http headers dictionary\"\"\"\n \n-    __slots__ = ['encoding']\n-\n     def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n         super(Headers, self).__init__(seq)\n\n@@ -16,10 +16,6 @@ from scrapy.http.common import deprecated_setter\n \n class Request(object_ref):\n \n-    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', \\\n-        'dont_filter', 'headers', 'cookies', 'callback', 'errback', 'priority', \\\n-        '__weakref__']\n-\n     def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n                  cookies=None, meta=None, encoding='utf-8', priority=0,\n                  dont_filter=False, errback=None):\n\n@@ -22,8 +22,6 @@ def _unicode_to_str(string, encoding):\n \n class FormRequest(Request):\n \n-    __slots__ = ()\n-\n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n         super(FormRequest, self).__init__(*args, **kwargs)\n\n@@ -15,8 +15,6 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n class XmlRpcRequest(Request):\n \n-    __slots__ = ()\n-\n     def __init__(self, *args, **kwargs):\n         encoding = kwargs.get('encoding', None)\n         if 'body' not in kwargs and 'params' in kwargs:\n\n@@ -13,9 +13,6 @@ from scrapy.http.common import deprecated_setter\n \n class Response(object_ref):\n \n-    __slots__ = ['_url', 'headers', 'status', '_body', 'request', \\\n-        'flags', '__weakref__']\n-\n     def __init__(self, url, status=200, headers=None, body='', flags=None, request=None):\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n\n@@ -12,8 +12,6 @@ from scrapy.utils.python import memoizemethod_noargs\n \n class HtmlResponse(TextResponse):\n \n-    __slots__ = ()\n-\n     _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n \n     _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n\n@@ -24,8 +24,6 @@ class TextResponse(Response):\n     _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    __slots__ = ['_encoding', '_cached_benc', '_cached_ubody']\n-\n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop('encoding', None)\n         self._cached_benc = None\n\n@@ -12,8 +12,6 @@ from scrapy.utils.python import memoizemethod_noargs\n \n class XmlResponse(TextResponse):\n \n-    __slots__ = ()\n-\n     _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n     _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n     XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n\n@@ -223,7 +223,7 @@ RETRY_PRIORITY_ADJUST = -1\n ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_DISK_QUEUE = 'scrapy.squeue.MarshalDiskQueue'\n+SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleDiskQueue'\n \n SELECTORS_BACKEND = None # possible values: libxml2, lxml\n \n\n@@ -2,10 +2,22 @@\n Scheduler disk-based queues\n \"\"\"\n \n-import marshal\n+import marshal, cPickle as pickle\n \n from scrapy.utils.queue import DiskQueue\n \n+\n+class PickleDiskQueue(DiskQueue):\n+\n+    def push(self, obj):\n+        super(PickleDiskQueue, self).push(pickle.dumps(obj))\n+\n+    def pop(self):\n+        s = super(PickleDiskQueue, self).pop()\n+        if s:\n+            return pickle.loads(s)\n+\n+\n class MarshalDiskQueue(DiskQueue):\n \n     def push(self, obj):\n\n@@ -119,9 +119,3 @@ class HeadersTest(unittest.TestCase):\n         h1.setlistdefault('header2', ['value2', 'value3'])\n         self.assertEqual(h1.getlist('header1'), ['value1'])\n         self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])\n-\n-    def test_slots(self):\n-        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n-        x = Headers({})\n-        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n-            x.__class__.__name__\n\n@@ -1,5 +1,4 @@\n import cgi\n-import weakref\n import unittest\n import xmlrpclib\n from inspect import getargspec\n@@ -169,13 +168,6 @@ class RequestTest(unittest.TestCase):\n         r = self.request_class(\"http://www.example.com\", method=u\"POST\")\n         assert isinstance(r.method, str)\n \n-    def test_weakref_slots(self):\n-        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n-        x = self.request_class('http://www.example.com')\n-        weakref.ref(x)\n-        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n-            x.__class__.__name__\n-\n \n class FormRequestTest(RequestTest):\n \n\n@@ -1,5 +1,4 @@\n import unittest\n-import weakref\n \n from scrapy.http import Request, Response, TextResponse, HtmlResponse, XmlResponse, Headers\n from scrapy.utils.encoding import resolve_encoding\n@@ -92,13 +91,6 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual(r4.body, '')\n         self.assertEqual(r4.flags, [])\n \n-    def test_weakref_slots(self):\n-        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n-        x = self.response_class('http://www.example.com')\n-        weakref.ref(x)\n-        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n-            x.__class__.__name__\n-\n     def _assert_response_values(self, response, encoding, body):\n         if isinstance(body, unicode):\n             body_unicode = body\n\n@@ -1,6 +1,18 @@\n from scrapy.tests import test_utils_queue as t\n-from scrapy.squeue import MarshalDiskQueue\n+from scrapy.squeue import MarshalDiskQueue, PickleDiskQueue\n+from scrapy.item import Item, Field\n+from scrapy.http import Request\n+from scrapy.contrib.loader import ItemLoader\n \n+class TestItem(Item):\n+    name = Field()\n+\n+def test_processor(x):\n+    return x + x\n+\n+class TestLoader(ItemLoader):\n+    default_item_class = TestItem\n+    name_out = staticmethod(test_processor)\n \n class MarshalDiskQueueTest(t.DiskQueueTest):\n \n@@ -29,3 +41,60 @@ class ChunkSize3MarshalDiskQueueTest(MarshalDiskQueueTest):\n \n class ChunkSize4MarshalDiskQueueTest(MarshalDiskQueueTest):\n     chunksize = 4\n+\n+\n+class PickleDiskQueueTest(t.DiskQueueTest):\n+\n+    chunksize = 100000\n+\n+    def queue(self):\n+        return PickleDiskQueue(self.qdir, chunksize=self.chunksize)\n+\n+    def test_serialize(self):\n+        q = self.queue()\n+        q.push('a')\n+        q.push(123)\n+        q.push({'a': 'dict'})\n+        self.assertEqual(q.pop(), 'a')\n+        self.assertEqual(q.pop(), 123)\n+        self.assertEqual(q.pop(), {'a': 'dict'})\n+\n+    def test_serialize_item(self):\n+        q = self.queue()\n+        i = TestItem(name='foo')\n+        q.push(i)\n+        i2 = q.pop()\n+        assert isinstance(i2, TestItem)\n+        self.assertEqual(i, i2)\n+\n+    def test_serialize_loader(self):\n+        q = self.queue()\n+        l = TestLoader()\n+        q.push(l)\n+        l2 = q.pop()\n+        assert isinstance(l2, TestLoader)\n+        assert l2.default_item_class is TestItem\n+        self.assertEqual(l2.name_out('x'), 'xx')\n+\n+    def test_serialize_request_recursive(self):\n+        q = self.queue()\n+        r = Request('http://www.example.com')\n+        r.meta['request'] = r\n+        q.push(r)\n+        r2 = q.pop()\n+        assert isinstance(r2, Request)\n+        self.assertEqual(r.url, r2.url)\n+        assert r2.meta['request'] is r2\n+\n+class ChunkSize1PickleDiskQueueTest(PickleDiskQueueTest):\n+    chunksize = 1\n+\n+class ChunkSize2PickleDiskQueueTest(PickleDiskQueueTest):\n+    chunksize = 2\n+\n+class ChunkSize3PickleDiskQueueTest(PickleDiskQueueTest):\n+    chunksize = 3\n+\n+class ChunkSize4PickleDiskQueueTest(PickleDiskQueueTest):\n+    chunksize = 4\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#edefb8ac69570fc98a104f730289202589b7a3ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -77,7 +77,7 @@ class ScrapyCommand(object):\n             help=\"write lsprof profiling stats to FILE\")\n         group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        group.add_option(\"--set\", dest=\"set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        group.add_option(\"-s\", \"--set\", dest=\"set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set/override setting (may be repeated)\")\n         parser.add_option_group(group)\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46edfd4a9d47f765310555220e21c92f32d33a61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 70 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -64,20 +64,20 @@ class ScrapyCommand(object):\n         Populate option parse with options available for this command\n         \"\"\"\n         group = OptionGroup(parser, \"Global Options\")\n-        group.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n+        group.add_option(\"--logfile\", metavar=\"FILE\", \\\n             help=\"log file. if omitted stderr will be used\")\n-        group.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n+        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", \\\n             default=None, \\\n             help=\"log level (default: %s)\" % settings['LOGLEVEL'])\n-        group.add_option(\"--nolog\", action=\"store_true\", dest=\"nolog\", \\\n+        group.add_option(\"--nolog\", action=\"store_true\", \\\n             help=\"disable logging completely\")\n-        group.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n+        group.add_option(\"--profile\", metavar=\"FILE\", default=None, \\\n             help=\"write python cProfile stats to FILE\")\n-        group.add_option(\"--lsprof\", dest=\"lsprof\", metavar=\"FILE\", default=None, \\\n+        group.add_option(\"--lsprof\", metavar=\"FILE\", default=None, \\\n             help=\"write lsprof profiling stats to FILE\")\n-        group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n+        group.add_option(\"--pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        group.add_option(\"-s\", \"--set\", dest=\"set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set/override setting (may be repeated)\")\n         parser.add_option_group(group)\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76af0cdd44ccb23802c2d80a7ceaf5248d9c6385", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 294 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -85,7 +85,7 @@ class ScrapyCommand(object):\n         try:\n             settings.overrides.update(arglist_to_dict(opts.set))\n         except ValueError:\n-            raise UsageError(\"Invalid --set value, use --set NAME=VALUE\", print_help=False)\n+            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n             settings.overrides['LOG_ENABLED'] = True\n\n@@ -22,7 +22,7 @@ class CmdlineTest(unittest.TestCase):\n             'default + loaded + started')\n \n     def test_override_settings_using_set_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--set', 'TEST1=override'), \\\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s', 'TEST1=override'), \\\n             'override + loaded + started')\n \n     def test_override_settings_using_envvar(self):\n\n@@ -25,7 +25,7 @@ class UtilsTest(unittest.TestCase):\n     def test_get_crawl_args_with_settings(self):\n         msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1', 'settings': {'ONE': 'two'}}\n         cargs = get_crawl_args(msg)\n-        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '--set', 'ONE=two'])\n+        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '-s', 'ONE=two'])\n         assert all(isinstance(x, str) for x in cargs), cargs\n \n class GetSpiderListTest(unittest.TestCase):\n\n@@ -45,7 +45,7 @@ def get_crawl_args(message):\n         args += ['-a']\n         args += ['%s=%s' % (k, v)]\n     for k, v in stringify_dict(settings, keys_only=False).items():\n-        args += ['--set']\n+        args += ['-s']\n         args += ['%s=%s' % (k, v)]\n     return args\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#725362fdeb4aa7dad5302a8307697d325ab6c1e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 113 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,22 +43,13 @@ class ChunkSize4MarshalDiskQueueTest(MarshalDiskQueueTest):\n     chunksize = 4\n \n \n-class PickleDiskQueueTest(t.DiskQueueTest):\n+class PickleDiskQueueTest(MarshalDiskQueueTest):\n \n     chunksize = 100000\n \n     def queue(self):\n         return PickleDiskQueue(self.qdir, chunksize=self.chunksize)\n \n-    def test_serialize(self):\n-        q = self.queue()\n-        q.push('a')\n-        q.push(123)\n-        q.push({'a': 'dict'})\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(q.pop(), 123)\n-        self.assertEqual(q.pop(), {'a': 'dict'})\n-\n     def test_serialize_item(self):\n         q = self.queue()\n         i = TestItem(name='foo')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#789e1493e98ecae9df4a5ae0baf8885b77c3cc50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 36 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,8 @@ from scrapy.utils.queue import DiskQueue\n class PickleDiskQueue(DiskQueue):\n \n     def push(self, obj):\n-        super(PickleDiskQueue, self).push(pickle.dumps(obj))\n+        s = pickle.dumps(obj, protocol=2)\n+        super(PickleDiskQueue, self).push(s)\n \n     def pop(self):\n         s = super(PickleDiskQueue, self).pop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cca0b910007846a4eeb147c374b2baeb31ea188e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 20 | Churn Cumulative: 493 | Contributors (this commit): 4 | Commits (past 90d): 39 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,17 +13,19 @@ from scrapy import log\n \n class Scheduler(object):\n \n-    def __init__(self, dupefilter, jobdir=None, dqclass=None):\n+    def __init__(self, dupefilter, jobdir=None, dqclass=None, logunser=False):\n         self.df = dupefilter\n         self.dqdir = join(jobdir, 'requests.queue') if jobdir else None\n         self.dqclass = dqclass\n+        self.logunser = logunser\n \n     @classmethod\n     def from_settings(cls, settings):\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n         dupefilter = dupefilter_cls.from_settings(settings)\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n-        return cls(dupefilter, job_dir(settings), dqclass)\n+        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')\n+        return cls(dupefilter, job_dir(settings), dqclass, logunser)\n \n     def has_pending_requests(self):\n         return len(self) > 0\n@@ -59,7 +61,10 @@ class Scheduler(object):\n         try:\n             reqd = request_to_dict(request, self.spider)\n             self.dqs.push(reqd, -request.priority)\n-        except ValueError: # non serializable request\n+        except ValueError, e: # non serializable request\n+            if self.logunser:\n+                log.msg(\"Unable to serialize request: %s - reason: %s\" % \\\n+                    (request, str(e)), level=log.ERROR, spider=self.spider)\n             return\n         else:\n             stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n\n@@ -188,6 +188,8 @@ LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n \n+LOG_UNSERIALIZABLE_REQUESTS = False\n+\n LOGSTATS_INTERVAL = 60.0\n \n MAIL_DEBUG = False\n\n@@ -10,7 +10,10 @@ from scrapy.utils.queue import DiskQueue\n class PickleDiskQueue(DiskQueue):\n \n     def push(self, obj):\n+        try:\n             s = pickle.dumps(obj, protocol=2)\n+        except pickle.PicklingError, e:\n+            raise ValueError(str(e))\n         super(PickleDiskQueue, self).push(s)\n \n     def pop(self):\n\n@@ -30,6 +30,10 @@ class MarshalDiskQueueTest(t.DiskQueueTest):\n         self.assertEqual(q.pop(), 123)\n         self.assertEqual(q.pop(), {'a': 'dict'})\n \n+    def test_nonserializable_object(self):\n+        q = self.queue()\n+        self.assertRaises(ValueError, q.push, lambda x: x)\n+\n class ChunkSize1MarshalDiskQueueTest(MarshalDiskQueueTest):\n     chunksize = 1\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
