{"custom_id": "keras#ff2b386abd920955132a66b8fe3fca1d120c9d18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 21/10 | Churn Δ: 133 | Churn Cumulative: 3725 | Contributors (this commit): 12 | Commits (past 90d): 117 | Contributors (cumulative): 26 | DMM Complexity: 0.547945205479452\n\nDIFF:\n@@ -487,7 +487,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n-            batch_outputs, np.concatenate, outputs\n+            batch_outputs, potentially_ragged_concat, outputs\n         )\n \n     def train_on_batch(\n@@ -826,3 +826,49 @@ def _is_tpu_strategy_class(clz):\n     if is_tpu_strat(clz):\n         return True\n     return any(map(_is_tpu_strategy_class, clz.__bases__))\n+\n+\n+def potentially_ragged_concat(tensors):\n+    \"\"\"Concats `Tensor`s along their first dimension.\n+\n+    Args:\n+        tensors: List of `Tensor`s.\n+\n+    Returns:\n+        Concatenation of the inputs along the first dimension -- of type\n+        `Tensor` if all input shapes are compatible, or `RaggedTensor`\n+        if not.\n+    \"\"\"\n+    if len(tensors) == 1:\n+        return tensors[0]\n+    if isinstance(tensors[0], tf.SparseTensor):\n+        return tf.sparse.concat(axis=0, sp_inputs=tensors)\n+    elif isinstance(tensors[0], tf.RaggedTensor):\n+        return tf.concat(tensors, axis=0)\n+    elif not tf.__internal__.tf2.enabled():\n+        return tf.concat(tensors, axis=0)\n+\n+    non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n+    constant_dims = tf.math.reduce_all(\n+        non_batch_shapes == non_batch_shapes[:1], axis=0\n+    )\n+    if tf.math.reduce_all(constant_dims).numpy().item():\n+        # All non-batch dims are constant\n+        if _is_scalar(tensors[0]):\n+            return tf.stack(tensors, axis=0)\n+        else:\n+            return tf.concat(tensors, axis=0)\n+\n+    # First, identify constant inner dimensions by finding the\n+    # rightmost dimension that is not constant\n+    constant_inner_dimensions = (\n+        constant_dims.numpy().tolist()[::-1].index(False)\n+    )\n+    # If there are constant inner dimensions, define a constant inner shape\n+    if constant_inner_dimensions == 0:\n+        constant_inner_shape = None\n+    else:\n+        constant_inner_shape = tensors[0].shape[-constant_inner_dimensions:]\n+    return tf.ragged.constant(\n+        [tensor.numpy() for tensor in tensors], inner_shape=constant_inner_shape\n+    ).merge_dims(0, 1)\n\n@@ -295,6 +295,10 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy(dtype=dtype)\n+        if isinstance(x, (tf.Tensor, tf.Variable)):\n+            x = x.numpy()\n+        if isinstance(x, tf.RaggedTensor):\n+            return tf.cast(x, dtype=dtype)\n         if not isinstance(x, np.ndarray):\n             # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n             # `torch.Tensor`, as well as any other tensor-like object that has\n@@ -303,9 +307,10 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.array(x, dtype=dtype)\n             else:\n                 raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, jax.np.ndarray, \"\n-                    \"torch.Tensor, Pandas Dataframe, or Pandas Series. \"\n-                    f\"Received invalid input: {x} (of type {type(x)})\"\n+                    \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n+                    \"jax.np.ndarray, torch.Tensor, Pandas Dataframe, or \"\n+                    \"Pandas Series. Received invalid input: \"\n+                    f\"{x} (of type {type(x)})\"\n                 )\n         if x.dtype == object:\n             return x\n\n@@ -14,7 +14,7 @@ except ImportError:\n # Leave jax, tf, and torch arrays off this list. Instead we will use\n # `__array__` to detect these types. Doing so allows us to avoid importing a\n # backend framework we are not currently using just to do type-checking.\n-ARRAY_TYPES = (np.ndarray,)\n+ARRAY_TYPES = (np.ndarray, tf.RaggedTensor)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n\n@@ -450,3 +450,75 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         x = np.ones((16, 2))\n         y = np.zeros((16, 1))\n         model.fit(x, y, batch_size=4)\n+\n+    def get_layer(self):\n+        class ExampleLayer(keras_core.Layer):\n+            def call(self, x):\n+                return x * 2\n+\n+        return ExampleLayer\n+\n+    def get_model(self):\n+        class ExampleModel(keras_core.Model):\n+            def call(self, x):\n+                return x * 2\n+\n+        return ExampleModel\n+\n+    def get_functional(self):\n+        ExampleLayer = self.get_layer()\n+\n+        class ExampleFunctional(keras_core.Functional):\n+            def __init__(self, input_shape=(None,)):\n+                inputs = keras_core.Input(input_shape)\n+                outputs = ExampleLayer()(inputs)\n+                super().__init__(inputs=inputs, outputs=outputs)\n+\n+        return ExampleFunctional\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"model\",\n+                \"model_class\": \"get_model\",\n+            },\n+            {\n+                \"testcase_name\": \"layer\",\n+                \"model_class\": \"get_layer\",\n+            },\n+            {\n+                \"testcase_name\": \"functional\",\n+                \"model_class\": \"get_functional\",\n+            },\n+        ]\n+    )\n+    @pytest.mark.skipif(\n+        keras_core.backend.backend() != \"tensorflow\",\n+        reason=\"Only tensorflow supports raggeds\",\n+    )\n+    def test_trainer_with_raggeds(self, model_class):\n+        import tensorflow as tf\n+\n+        def loss_fn(y, y_pred, sample_weight=None):\n+            return 0\n+\n+        model = getattr(self, model_class)()()\n+        x = tf.ragged.constant([[1], [2, 3]])\n+\n+        # test forward pass\n+        y = model(x)\n+        self.assertEqual(type(y), tf.RaggedTensor)\n+\n+        # test training\n+        if model_class in [\"get_model\", \"get_functional\"]:\n+            model.compile(optimizer=\"adam\", loss=loss_fn)\n+            model.fit(x, x)\n+            y = model.predict(x)\n+            self.assertEqual(type(y), tf.RaggedTensor)\n+\n+        # test if everything works with the sequential model\n+        model = keras_core.Sequential([model])\n+        model.compile(optimizer=\"adam\", loss=loss_fn)\n+        model.fit(x, x)\n+        y = model.predict(x)\n+        self.assertEqual(type(y), tf.RaggedTensor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#710dc765212973820f53ce302ab1410718279f5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 994 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -130,7 +130,7 @@ for epoch in range(epochs):\n \n         # Run one step of gradient descent by updating\n         # the value of the variables to minimize the loss.\n-        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+        optimizer.apply(grads, model.trainable_weights)\n \n         # Log every 100 batches.\n         if step % 100 == 0:\n@@ -184,7 +184,7 @@ for epoch in range(epochs):\n             logits = model(x_batch_train, training=True)\n             loss_value = loss_fn(y_batch_train, logits)\n         grads = tape.gradient(loss_value, model.trainable_weights)\n-        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+        optimizer.apply(grads, model.trainable_weights)\n \n         # Update training metric.\n         train_acc_metric.update_state(y_batch_train, logits)\n@@ -236,7 +236,7 @@ def train_step(x, y):\n         logits = model(x, training=True)\n         loss_value = loss_fn(y, logits)\n     grads = tape.gradient(loss_value, model.trainable_weights)\n-    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+    optimizer.apply(grads, model.trainable_weights)\n     train_acc_metric.update_state(y, logits)\n     return loss_value\n \n@@ -340,7 +340,7 @@ def train_step(x, y):\n         # Add any extra losses created during the forward pass.\n         loss_value += sum(model.losses)\n     grads = tape.gradient(loss_value, model.trainable_weights)\n-    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+    optimizer.apply(grads, model.trainable_weights)\n     train_acc_metric.update_state(y, logits)\n     return loss_value\n \n@@ -465,7 +465,7 @@ def train_step(real_images):\n         predictions = discriminator(combined_images)\n         d_loss = loss_fn(labels, predictions)\n     grads = tape.gradient(d_loss, discriminator.trainable_weights)\n-    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n+    d_optimizer.apply(grads, discriminator.trainable_weights)\n \n     # Sample random points in the latent space\n     random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n@@ -478,7 +478,7 @@ def train_step(real_images):\n         predictions = discriminator(generator(random_latent_vectors))\n         g_loss = loss_fn(misleading_labels, predictions)\n     grads = tape.gradient(g_loss, generator.trainable_weights)\n-    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n+    g_optimizer.apply(grads, generator.trainable_weights)\n     return d_loss, g_loss, generated_images\n \n \n\n@@ -160,7 +160,7 @@ Important differences:\n \n - You retrieve the gradients for the variables via `v.value.grad`,\n called on each trainable variable.\n-- You update your variables via `optimizer.apply_gradients()`, which must be\n+- You update your variables via `optimizer.apply()`, which must be\n called in a `torch.no_grad()` scope.\n \n **Also, a big gotcha:** while all NumPy/TensorFlow/JAX/Keras APIs\n@@ -192,7 +192,7 @@ for epoch in range(epochs):\n \n         # Update weights\n         with torch.no_grad():\n-            optimizer.apply_gradients(zip(gradients, trainable_weights))\n+            optimizer.apply(gradients, trainable_weights)\n \n         # Log every 100 batches.\n         if step % 100 == 0:\n@@ -253,7 +253,7 @@ for epoch in range(epochs):\n \n         # Update weights\n         with torch.no_grad():\n-            optimizer.apply_gradients(zip(gradients, trainable_weights))\n+            optimizer.apply(gradients, trainable_weights)\n \n         # Update training metric.\n         train_acc_metric.update_state(targets, logits)\n@@ -352,7 +352,7 @@ for epoch in range(epochs):\n \n         # Update weights\n         with torch.no_grad():\n-            optimizer.apply_gradients(zip(gradients, trainable_weights))\n+            optimizer.apply(gradients, trainable_weights)\n \n         # Update training metric.\n         train_acc_metric.update_state(targets, logits)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bd040c3cfca1b98923555ba055f542d74b08e3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 16 | Methods Changed: 17 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 93 | Churn Cumulative: 40093 | Contributors (this commit): 151 | Commits (past 90d): 16 | Contributors (cumulative): 170 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -319,7 +319,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\n \n         self._steps_per_execution = None\n-        self._enable_tune_steps_per_execution = False\n+        self._steps_per_execution_tuner = None\n+        self._autotune_steps_per_execution = False\n \n         self._layout_map = layout_map_lib.get_current_layout_map()\n \n@@ -803,12 +804,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             )\n \n             if steps_per_execution == \"auto\":\n+                if self._steps_per_execution is None:\n                     self._configure_steps_per_execution(1)\n                 self._steps_per_execution_tuner = (\n                     steps_per_execution_tuning.StepsPerExecutionTuner(\n                         self.optimizer, self._steps_per_execution\n                     )\n                 )\n+                self._autotune_steps_per_execution = True\n             else:\n                 self._configure_steps_per_execution(steps_per_execution or 1)\n \n@@ -1006,12 +1009,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._run_eagerly = value\n \n     @property\n-    def enable_tune_steps_per_execution(self):\n-        return self._enable_tune_steps_per_execution\n+    def autotune_steps_per_execution(self):\n+        \"\"\"Settable property to enable tuning for steps_per_execution\"\"\"\n+        return self._autotune_steps_per_execution\n \n-    @enable_tune_steps_per_execution.setter\n-    def enable_tune_steps_per_execution(self, value):\n-        self._enable_tune_steps_per_execution = value\n+    @autotune_steps_per_execution.setter\n+    def autotune_steps_per_execution(self, value):\n+        self._autotune_steps_per_execution = value\n+        if value and self._steps_per_execution_tuner is None:\n+            if self._steps_per_execution is None:\n+                self._configure_steps_per_execution(1)\n+            self._steps_per_execution_tuner = (\n+                steps_per_execution_tuning.StepsPerExecutionTuner(\n+                    self.optimizer, self._steps_per_execution\n+                )\n+            )\n+\n+    @property\n+    def steps_per_execution(self):\n+        \"\"\"Settable `steps_per_execution variable. Requires a compiled model.\"\"\"\n+        return self._steps_per_execution\n+\n+    @steps_per_execution.setter\n+    def steps_per_execution(self, value):\n+        if self._steps_per_execution is None:\n+            self._configure_steps_per_execution(value)\n+        else:\n+            self._steps_per_execution.assign(value)\n \n     @property\n     def jit_compile(self):\n@@ -1376,7 +1400,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def train_function(iterator):\n@@ -1759,7 +1783,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._train_counter.assign(0)\n             callbacks.on_train_begin()\n             training_logs = None\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             # Handle fault-tolerance for multi-worker.\n             # TODO(omalleyt): Fix the ordering issues that mean this has to\n@@ -1867,7 +1891,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # If eval data_handler exists, delete it after all epochs are done.\n             if getattr(self, \"_eval_data_handler\", None) is not None:\n                 del self._eval_data_handler\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_train_end(logs=training_logs)\n             return self.history\n@@ -2041,7 +2065,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def test_function(iterator):\n@@ -2263,7 +2287,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             test_function_runner = self._get_test_function_runner(callbacks)\n             self._test_counter.assign(0)\n             callbacks.on_test_begin()\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             for (\n                 _,\n@@ -2289,7 +2313,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 logs = self._aggregate_exact_metrics(logs)\n             else:\n                 logs = self._validate_and_get_metrics_result(logs)\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_test_end(logs=logs)\n \n@@ -2415,7 +2439,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.enable_tune_steps_per_execution\n+            and not self.autotune_steps_per_execution\n         ):\n \n             def predict_function(iterator):\n@@ -2628,7 +2652,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self.predict_function = self.make_predict_function()\n             self._predict_counter.assign(0)\n             callbacks.on_predict_begin()\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             batch_outputs = None\n             for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n@@ -2668,7 +2692,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     \"information of where went wrong, or file a \"\n                     \"issue/bug to `tf.keras`.\"\n                 )\n-            if self.enable_tune_steps_per_execution:\n+            if self.autotune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_predict_end()\n         all_outputs = tf.__internal__.nest.map_structure_up_to(\n\n@@ -2472,9 +2472,44 @@ class TestAutotuneSPE(test_combinations.TestCase):\n         x, y = np.ones((10, 1)), np.ones((10, 1))\n         model.fit(x, y, epochs=2)\n         model.evaluate(x, y)\n-        model.enable_tune_steps_per_execution = False\n+        model.autotune_steps_per_execution = False\n         model.predict(x)\n-        assert model.enable_tune_steps_per_execution == False\n+        assert model.autotune_steps_per_execution == False\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_spe_tune_set_after_compile(self):\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=5,\n+        )\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        assert model._steps_per_execution_tuner is None\n+        model.autotune_steps_per_execution = True\n+        model.fit(x, y, epochs=2)\n+        assert model.steps_per_execution.numpy().item() == 5\n+        assert model._steps_per_execution_tuner\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_spe_tune_set_before_compile(self):\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        model.steps_per_execution = 5\n+        model.compile(\n+            \"sgd\",\n+            loss=\"mse\",\n+            run_eagerly=False,\n+            jit_compile=True,\n+            steps_per_execution=\"auto\",\n+        )\n+        assert model.steps_per_execution.numpy().item() == 5\n+        assert model._steps_per_execution_tuner\n+\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n \n \n class TestExceptionsAndWarnings(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#402dcdcb2a2df441cdc34c2079dfdb12e7740d10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 821 | Contributors (this commit): 6 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -554,7 +554,7 @@ class R2Score(reduction_metrics.Metric):\n \n     def reset_state(self):\n         for v in self.variables:\n-            v.assign(ops.zeros(v.shape))\n+            v.assign(ops.zeros(v.shape, dtype=v.dtype))\n \n     def get_config(self):\n         config = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a578fee1098e3443618a9939c3ce136ba27f1cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 81 | Churn Cumulative: 1969 | Contributors (this commit): 8 | Commits (past 90d): 57 | Contributors (cumulative): 11 | DMM Complexity: 0.6923076923076923\n\nDIFF:\n@@ -176,45 +176,62 @@ def name_scope(name):\n \n # Shape / dtype inference util\n def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n-\n     def has_none_shape(x):\n+        \"\"\"Check for if a `KerasTensor` has dynamic shape.\"\"\"\n         if isinstance(x, KerasTensor):\n             return None in x.shape\n         return False\n \n-        none_in_shape = any(map(has_none_shape, nest.flatten((args, kwargs))))\n-\n     def convert_keras_tensor_to_torch(x, fill_value=None):\n+        \"\"\"Convert `KerasTensor`s to `torch.Tensor`s.\"\"\"\n         if isinstance(x, KerasTensor):\n             shape = list(x.shape)\n             if fill_value:\n                 for i, e in enumerate(shape):\n                     if e is None:\n                         shape[i] = fill_value\n-                return torch.empty(\n+            return torch.ones(\n                 size=shape,\n                 dtype=TORCH_DTYPES[x.dtype],\n                 device=get_device(),\n             )\n         return x\n \n+    def convert_torch_to_keras_tensor(x):\n+        \"\"\"Convert `torch.Tensor`s to `KerasTensor`s.\"\"\"\n+        if is_tensor(x):\n+            return KerasTensor(x.shape, standardize_dtype(x.dtype))\n+        return x\n+\n+    def symbolic_call(fn, args, kwargs, fill_value):\n+        \"\"\"Call `fn` to infer output shape and dtype.\"\"\"\n+        try:\n+            # First try instantiating all tensors on the `\"meta\"` device,\n+            # which  should give a \"zero flop\" way to trace shape, but does\n+            # not have universal support with torch operations.\n             with device_scope(\"meta\"):\n-            args_1, kwargs_1 = nest.map_structure(\n-                lambda x: convert_keras_tensor_to_torch(x, fill_value=83),\n+                args, kwargs = nest.map_structure(\n+                    lambda x: convert_keras_tensor_to_torch(x, fill_value),\n                     (args, kwargs),\n                 )\n-            outputs_1 = fn(*args_1, **kwargs_1)\n+                return fn(*args, **kwargs)\n+        except:\n+            # If the `\"meta\"` device placement fails, fall back to tracing\n+            # eagerly with tensors on the default device. This will be\n+            # more robust, but more expensive.\n+            args, kwargs = nest.map_structure(\n+                lambda x: convert_keras_tensor_to_torch(x, fill_value),\n+                (args, kwargs),\n+            )\n+            return fn(*args, **kwargs)\n \n-        outputs = outputs_1\n+    with StatelessScope():\n+        outputs = symbolic_call(fn, args, kwargs, fill_value=83)\n \n+        none_in_shape = any(map(has_none_shape, nest.flatten((args, kwargs))))\n         if none_in_shape:\n-            with device_scope(\"meta\"):\n-                args_2, kwargs_2 = nest.map_structure(\n-                    lambda x: convert_keras_tensor_to_torch(x, fill_value=89),\n-                    (args, kwargs),\n-                )\n-                outputs_2 = fn(*args_2, **kwargs_2)\n+            outputs_1 = outputs\n+            outputs_2 = symbolic_call(fn, args, kwargs, fill_value=89)\n \n             flat_out_1 = nest.flatten(outputs_1)\n             flat_out_2 = nest.flatten(outputs_2)\n@@ -228,11 +245,6 @@ def compute_output_spec(fn, *args, **kwargs):\n                 flat_out.append(KerasTensor(shape, standardize_dtype(x1.dtype)))\n             outputs = nest.pack_sequence_as(outputs_1, flat_out)\n \n-        def convert_torch_to_keras_tensor(x):\n-            if is_tensor(x):\n-                return KerasTensor(x.shape, standardize_dtype(x.dtype))\n-            return x\n-\n         output_spec = nest.map_structure(convert_torch_to_keras_tensor, outputs)\n     return output_spec\n \n\n@@ -7,7 +7,9 @@ import torch\n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n-from keras_core.backend.torch.core import device_scope\n+from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.torch.core import is_tensor\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n@@ -150,20 +152,29 @@ class TorchTrainer(base_trainer.Trainer):\n             and not self._compile_metrics.built\n         )\n         if model_unbuilt or compile_metrics_unbuilt:\n-            # Build the model on one batch of data.\n+            # Create symbolic tensors matching an input batch.\n+\n+            def to_symbolic_input(v):\n+                if is_tensor(v):\n+                    return KerasTensor(v.shape, standardize_dtype(v.dtype))\n+                return v\n+\n+            data_batch = tf.nest.map_structure(to_symbolic_input, data_batch)\n             (\n                 x,\n                 y,\n                 sample_weight,\n             ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n-            # Build model\n-            with backend.StatelessScope():\n-                with device_scope(\"meta\"):\n-                    y_pred = self(x)\n+            # Build all model state with `backend.compute_output_spec`.\n+            y_pred = backend.compute_output_spec(self, x)\n             if compile_metrics_unbuilt:\n-                        # Build metrics\n-                        self.compute_metrics(\n-                            x, y, y_pred, sample_weight=sample_weight\n+                # Build all metric state with `backend.compute_output_spec`.\n+                backend.compute_output_spec(\n+                    self.compute_metrics,\n+                    x,\n+                    y,\n+                    y_pred,\n+                    sample_weight=sample_weight,\n                 )\n         if self.optimizer is not None and not self.optimizer.built:\n             # Build optimizer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dae8a9b93f0d853e8f89a1d9e36a2bfd0d09f722", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 3334 | Contributors (this commit): 11 | Commits (past 90d): 83 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -171,10 +171,10 @@ class JAXTrainer(base_trainer.Trainer):\n             kwargs[\"training\"] = False\n \n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        outputs, _ = self.stateless_call(\n+        outputs, non_trainable_variables = self.stateless_call(\n             trainable_variables, non_trainable_variables, x, **kwargs\n         )\n-        return outputs\n+        return outputs, (trainable_variables, non_trainable_variables)\n \n     def make_train_function(self, force=False):\n         if self.train_function is not None and not force:\n@@ -243,9 +243,10 @@ class JAXTrainer(base_trainer.Trainer):\n             return self.predict_step(state, data)\n \n         def multi_predict_steps(state, data):\n-            outputs = one_predict_step(state, data[:1])\n+            outputs, state = one_predict_step(state, data[:1])\n+\n             for single_step_data in data[1:]:\n-                step_outputs = one_predict_step(\n+                step_outputs, state = one_predict_step(\n                     state,\n                     [single_step_data],\n                 )\n@@ -254,7 +255,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     outputs,\n                     step_outputs,\n                 )\n-            return outputs\n+            return outputs, state\n \n         if self.steps_per_execution > 1:\n             predict_step = multi_predict_steps\n@@ -597,12 +598,11 @@ class JAXTrainer(base_trainer.Trainer):\n \n         trainable_variables = self.trainable_variables\n         non_trainable_variables = self.non_trainable_variables\n+        state = (trainable_variables, non_trainable_variables)\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs = self.predict_function(\n-                [trainable_variables, non_trainable_variables], x\n-            )\n+            batch_outputs, state = self.predict_function(state, x)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n@@ -718,9 +718,8 @@ class JAXTrainer(base_trainer.Trainer):\n         self.make_predict_function()\n         trainable_variables = self.trainable_variables\n         non_trainable_variables = self.non_trainable_variables\n-        batch_outputs = self.predict_function(\n-            [trainable_variables, non_trainable_variables], [x]\n-        )\n+        state = (trainable_variables, non_trainable_variables)\n+        batch_outputs, state = self.predict_function(state, [x])\n         batch_outputs = tf.nest.map_structure(\n             lambda x: np.array(x), batch_outputs\n         )\n\n@@ -522,3 +522,17 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         model.fit(x, x)\n         y = model.predict(x)\n         self.assertEqual(type(y), tf.RaggedTensor)\n+\n+    def test_predict_dropout(self):\n+        # Test that `predict` with a dropout op\n+        # has nondeterministic behavior across batches.\n+\n+        inputs = layers.Input((20,))\n+        outputs = layers.Dropout(0.5, seed=1337)(inputs, training=True)\n+        model = keras_core.Model(inputs, outputs)\n+        out1 = model.predict(np.ones((4, 20)), batch_size=2)\n+        self.assertGreater(5, np.sum(np.abs(out1[:2, :] - out1[2:4, :])))\n+\n+        out2 = model.predict_on_batch(np.ones((2, 20)))\n+        out3 = model.predict_on_batch(np.ones((2, 20)))\n+        self.assertGreater(5, np.sum(np.abs(out2 - out3)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eee3ca3c9ab0e13189693c7866c3bf64f8b4656b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 34 | Churn Cumulative: 862 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ python3 -m model_benchmark.bert_benchmark \\\n \n import time\n \n+import keras_core as keras\n import keras_nlp\n import numpy as np\n import tensorflow as tf\n@@ -21,7 +22,6 @@ from absl import flags\n from absl import logging\n from model_benchmark.benchmark_utils import BenchmarkMetricsCallback\n \n-import keras_core as keras\n \n flags.DEFINE_string(\"model_size\", \"small\", \"The size of model to benchmark.\")\n flags.DEFINE_string(\n\n@@ -1,6 +1,8 @@\n import jax.numpy as jnp\n \n+from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import convert_to_tensor\n+from keras_core.backend import config\n \n \n def add(x1, x2):\n@@ -43,8 +45,14 @@ def multiply(x1, x2):\n \n \n def mean(x, axis=None, keepdims=False):\n-    return jnp.mean(x, axis=axis, keepdims=keepdims)\n-\n+    # `jnp.mean` does not handle low precision (e.g., float16) overflow\n+    # correctly, so we compute with float32 and cast back to the original type.\n+    outputs = jnp.mean(x, axis=axis, keepdims=keepdims, dtype=jnp.float32)\n+    dtype = getattr(x, \"dtype\", None)\n+    if hasattr(dtype, \"name\") and \"float\" in dtype.name:\n+        return cast(outputs, dtype)\n+    else:\n+        return cast(outputs, config.floatx())\n \n def max(x, axis=None, keepdims=False, initial=None):\n     return jnp.max(x, axis=axis, keepdims=keepdims, initial=initial)\n@@ -547,7 +555,14 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    return jnp.var(x, axis=axis, keepdims=keepdims)\n+    # `jnp.var` does not handle low precision (e.g., float16) overflow\n+    # correctly, so we compute with float32 and cast back to the original type.\n+    outputs = jnp.var(x, axis=axis, keepdims=keepdims, dtype=jnp.float32)\n+    dtype = getattr(x, \"dtype\", None)\n+    if hasattr(dtype, \"name\") and \"float\" in dtype.name:\n+        return cast(outputs, dtype)\n+    else:\n+        return cast(outputs, config.floatx())\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -4,6 +4,13 @@ from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n \n \n+def _large_negative_number(dtype):\n+    \"\"\"Return a Large negative number based on dtype.\"\"\"\n+    if backend.standardize_dtype(dtype) == \"float16\":\n+        return -3e4\n+    return -1e9\n+\n+\n @keras_core_export(\"keras_core.layers.Softmax\")\n class Softmax(Layer):\n     \"\"\"Softmax activation layer.\n@@ -43,7 +50,9 @@ class Softmax(Layer):\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n-            adder = (1.0 - backend.cast(mask, inputs.dtype)) * (-1e9)\n+            adder = (\n+                1.0 - backend.cast(mask, inputs.dtype)\n+            ) * _large_negative_number(inputs.dtype)\n             inputs += adder\n         if isinstance(self.axis, (tuple, list)):\n             if len(self.axis) > 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#468523a43ebf7d38c9a99181466e2c65e8f2f521", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 766 | Contributors (this commit): 6 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,6 @@ python3 -m model_benchmark.bert_benchmark \\\n \n import time\n \n-import keras_core as keras\n import keras_nlp\n import numpy as np\n import tensorflow as tf\n@@ -22,6 +21,7 @@ from absl import flags\n from absl import logging\n from model_benchmark.benchmark_utils import BenchmarkMetricsCallback\n \n+import keras_core as keras\n \n flags.DEFINE_string(\"model_size\", \"small\", \"The size of model to benchmark.\")\n flags.DEFINE_string(\n\n@@ -1,8 +1,8 @@\n import jax.numpy as jnp\n \n+from keras_core.backend import config\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import convert_to_tensor\n-from keras_core.backend import config\n \n \n def add(x1, x2):\n@@ -54,6 +54,7 @@ def mean(x, axis=None, keepdims=False):\n     else:\n         return cast(outputs, config.floatx())\n \n+\n def max(x, axis=None, keepdims=False, initial=None):\n     return jnp.max(x, axis=axis, keepdims=keepdims, initial=initial)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e39c842dbde35cd53b411a58ffce083985654930", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 24 | Churn Cumulative: 3122 | Contributors (this commit): 11 | Commits (past 90d): 72 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,7 +118,7 @@ def decode_predictions(preds, top=5):\n     \"\"\"Decodes the prediction of an ImageNet model.\n \n     Args:\n-      preds: Numpy array encoding a batch of predictions.\n+        preds: NumPy array encoding a batch of predictions.\n         top: Integer, how many top-guesses to return. Defaults to 5.\n \n     Returns:\n@@ -137,7 +137,7 @@ def decode_predictions(preds, top=5):\n             \"`decode_predictions` expects \"\n             \"a batch of predictions \"\n             \"(i.e. a 2D array of shape (samples, 1000)). \"\n-            \"Found array with shape: \" + str(preds.shape)\n+            f\"Received array with shape: {preds.shape}\"\n         )\n     if CLASS_INDEX is None:\n         fpath = file_utils.get_file(\n@@ -149,6 +149,7 @@ def decode_predictions(preds, top=5):\n         with open(fpath) as f:\n             CLASS_INDEX = json.load(f)\n     results = []\n+    preds = ops.convert_to_numpy(preds)\n     for pred in preds:\n         top_indices = pred.argsort()[-top:][::-1]\n         result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n\n@@ -44,6 +44,7 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns -inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n+    if tf.executing_eagerly():\n         size_x = size(x)\n         tf.assert_greater(\n             size_x,\n@@ -362,6 +363,7 @@ def min(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n+    if tf.executing_eagerly():\n         size_x = size(x)\n         tf.assert_greater(\n             size_x,\n\n@@ -486,9 +486,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 outputs = append_to_outputs(batch_outputs, outputs)\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n-        return tf.__internal__.nest.map_structure_up_to(\n+        outputs = tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, potentially_ragged_concat, outputs\n         )\n+        return tf.nest.map_structure(convert_to_np_if_not_ragged, outputs)\n \n     def train_on_batch(\n         self,\n@@ -544,7 +545,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.make_predict_function()\n         batch_outputs = self.predict_function((x,))\n         batch_outputs = tf.nest.map_structure(\n-            lambda x: np.array(x), batch_outputs\n+            convert_to_np_if_not_ragged, batch_outputs\n         )\n         return batch_outputs\n \n@@ -828,6 +829,12 @@ def _is_tpu_strategy_class(clz):\n     return any(map(_is_tpu_strategy_class, clz.__bases__))\n \n \n+def convert_to_np_if_not_ragged(x):\n+    if isinstance(x, tf.RaggedTensor):\n+        return x\n+    return x.numpy()\n+\n+\n def potentially_ragged_concat(tensors):\n     \"\"\"Concats `Tensor`s along their first dimension.\n \n@@ -836,17 +843,15 @@ def potentially_ragged_concat(tensors):\n \n     Returns:\n         Concatenation of the inputs along the first dimension -- of type\n-        `Tensor` if all input shapes are compatible, or `RaggedTensor`\n+        `np.ndarray` if all input shapes are compatible, or `tf.RaggedTensor`\n         if not.\n     \"\"\"\n     if len(tensors) == 1:\n         return tensors[0]\n-    if isinstance(tensors[0], tf.SparseTensor):\n+    elif isinstance(tensors[0], tf.SparseTensor):\n         return tf.sparse.concat(axis=0, sp_inputs=tensors)\n     elif isinstance(tensors[0], tf.RaggedTensor):\n         return tf.concat(tensors, axis=0)\n-    elif not tf.__internal__.tf2.enabled():\n-        return tf.concat(tensors, axis=0)\n \n     non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n     constant_dims = tf.math.reduce_all(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0efc1da260b79cbca5d2e0a23e2d0bad96c1047", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 76 | Files Changed: 2 | Hunks: 16 | Methods Changed: 17 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 93 | Churn Cumulative: 40186 | Contributors (this commit): 151 | Commits (past 90d): 18 | Contributors (cumulative): 171 | DMM Complexity: 0.0625\n\nDIFF:\n@@ -319,8 +319,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\n \n         self._steps_per_execution = None\n-        self._steps_per_execution_tuner = None\n-        self._autotune_steps_per_execution = False\n+        self._enable_tune_steps_per_execution = False\n \n         self._layout_map = layout_map_lib.get_current_layout_map()\n \n@@ -804,14 +803,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             )\n \n             if steps_per_execution == \"auto\":\n-                if self._steps_per_execution is None:\n                 self._configure_steps_per_execution(1)\n                 self._steps_per_execution_tuner = (\n                     steps_per_execution_tuning.StepsPerExecutionTuner(\n                         self.optimizer, self._steps_per_execution\n                     )\n                 )\n-                self._autotune_steps_per_execution = True\n             else:\n                 self._configure_steps_per_execution(steps_per_execution or 1)\n \n@@ -1009,33 +1006,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._run_eagerly = value\n \n     @property\n-    def autotune_steps_per_execution(self):\n-        \"\"\"Settable property to enable tuning for steps_per_execution\"\"\"\n-        return self._autotune_steps_per_execution\n+    def enable_tune_steps_per_execution(self):\n+        return self._enable_tune_steps_per_execution\n \n-    @autotune_steps_per_execution.setter\n-    def autotune_steps_per_execution(self, value):\n-        self._autotune_steps_per_execution = value\n-        if value and self._steps_per_execution_tuner is None:\n-            if self._steps_per_execution is None:\n-                self._configure_steps_per_execution(1)\n-            self._steps_per_execution_tuner = (\n-                steps_per_execution_tuning.StepsPerExecutionTuner(\n-                    self.optimizer, self._steps_per_execution\n-                )\n-            )\n-\n-    @property\n-    def steps_per_execution(self):\n-        \"\"\"Settable `steps_per_execution variable. Requires a compiled model.\"\"\"\n-        return self._steps_per_execution\n-\n-    @steps_per_execution.setter\n-    def steps_per_execution(self, value):\n-        if self._steps_per_execution is None:\n-            self._configure_steps_per_execution(value)\n-        else:\n-            self._steps_per_execution.assign(value)\n+    @enable_tune_steps_per_execution.setter\n+    def enable_tune_steps_per_execution(self, value):\n+        self._enable_tune_steps_per_execution = value\n \n     @property\n     def jit_compile(self):\n@@ -1400,7 +1376,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.autotune_steps_per_execution\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def train_function(iterator):\n@@ -1783,7 +1759,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._train_counter.assign(0)\n             callbacks.on_train_begin()\n             training_logs = None\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             # Handle fault-tolerance for multi-worker.\n             # TODO(omalleyt): Fix the ordering issues that mean this has to\n@@ -1891,7 +1867,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # If eval data_handler exists, delete it after all epochs are done.\n             if getattr(self, \"_eval_data_handler\", None) is not None:\n                 del self._eval_data_handler\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_train_end(logs=training_logs)\n             return self.history\n@@ -2065,7 +2041,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.autotune_steps_per_execution\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def test_function(iterator):\n@@ -2287,7 +2263,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             test_function_runner = self._get_test_function_runner(callbacks)\n             self._test_counter.assign(0)\n             callbacks.on_test_begin()\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             for (\n                 _,\n@@ -2313,7 +2289,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 logs = self._aggregate_exact_metrics(logs)\n             else:\n                 logs = self._validate_and_get_metrics_result(logs)\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_test_end(logs=logs)\n \n@@ -2439,7 +2415,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if (\n             self._steps_per_execution is None\n             or self._steps_per_execution.numpy().item() == 1\n-            and not self.autotune_steps_per_execution\n+            and not self.enable_tune_steps_per_execution\n         ):\n \n             def predict_function(iterator):\n@@ -2652,7 +2628,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self.predict_function = self.make_predict_function()\n             self._predict_counter.assign(0)\n             callbacks.on_predict_begin()\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.start()\n             batch_outputs = None\n             for _, iterator in data_handler.enumerate_epochs():  # Single epoch.\n@@ -2692,7 +2668,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     \"information of where went wrong, or file a \"\n                     \"issue/bug to `tf.keras`.\"\n                 )\n-            if self.autotune_steps_per_execution:\n+            if self.enable_tune_steps_per_execution:\n                 self._steps_per_execution_tuner.stop()\n             callbacks.on_predict_end()\n         all_outputs = tf.__internal__.nest.map_structure_up_to(\n\n@@ -2472,44 +2472,9 @@ class TestAutotuneSPE(test_combinations.TestCase):\n         x, y = np.ones((10, 1)), np.ones((10, 1))\n         model.fit(x, y, epochs=2)\n         model.evaluate(x, y)\n-        model.autotune_steps_per_execution = False\n+        model.enable_tune_steps_per_execution = False\n         model.predict(x)\n-        assert model.autotune_steps_per_execution == False\n-\n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_spe_tune_set_after_compile(self):\n-        model = sequential.Sequential([layers_module.Dense(1)])\n-        model.compile(\n-            \"sgd\",\n-            loss=\"mse\",\n-            run_eagerly=False,\n-            jit_compile=True,\n-            steps_per_execution=5,\n-        )\n-        x, y = np.ones((10, 1)), np.ones((10, 1))\n-        model.fit(x, y, epochs=2)\n-        assert model._steps_per_execution_tuner is None\n-        model.autotune_steps_per_execution = True\n-        model.fit(x, y, epochs=2)\n-        assert model.steps_per_execution.numpy().item() == 5\n-        assert model._steps_per_execution_tuner\n-\n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_spe_tune_set_before_compile(self):\n-        model = sequential.Sequential([layers_module.Dense(1)])\n-        model.steps_per_execution = 5\n-        model.compile(\n-            \"sgd\",\n-            loss=\"mse\",\n-            run_eagerly=False,\n-            jit_compile=True,\n-            steps_per_execution=\"auto\",\n-        )\n-        assert model.steps_per_execution.numpy().item() == 5\n-        assert model._steps_per_execution_tuner\n-\n-        x, y = np.ones((10, 1)), np.ones((10, 1))\n-        model.fit(x, y, epochs=2)\n+        assert model.enable_tune_steps_per_execution == False\n \n \n class TestExceptionsAndWarnings(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
