{"custom_id": "keras#81822d3e52fa1e213d6be5851ce21f6aef34e8c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 50 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,7 +21,7 @@ def test_softmax():\n         e = numpy.exp(values - m)\n         dist = list(e / numpy.sum(e))\n \n-        return [dist]\n+        return dist\n \n     x = T.vector()\n     exp = s(x)\n@@ -31,4 +31,16 @@ def test_softmax():\n     result = f(test_values)\n     expected = softmax(test_values)\n \n-    list_assert_equal(result[0], expected[0])\n+    print(str(result))\n+    print(str(expected))\n+\n+    list_assert_equal(result, expected)\n+\n+def test_linear():\n+\n+    from keras.activations import linear as l\n+\n+    xs = [1, 5, True, None, 'foo']\n+\n+    for x in xs:\n+        assert x == l(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3d90020fc1f93afdd27539553144dc432a31709", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 20 | Churn Cumulative: 2214 | Contributors (this commit): 18 | Commits (past 90d): 99 | Contributors (cumulative): 25 | DMM Complexity: 0.07692307692307693\n\nDIFF:\n@@ -91,8 +91,7 @@ class Model(object):\n \n         if hasattr(self, 'cost_updates'):\n             for u in self.loss_updates:\n-                if u.modifies_loss:\n-                    train_loss = u(train_loss)\n+                train_loss = u.update_loss(train_loss)\n \n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n \n\n@@ -29,7 +29,7 @@ class Optimizer(object):\n \n         new_grads = []\n         for p, g, r in zip(params, grads, regularizers):\n-            g = r(g, p)\n+            g = r.update_gradient(g, p)\n             new_grads.append(g)\n \n         return new_grads\n\n@@ -25,9 +25,6 @@ class WeightsL1(Regularizer):\n         gradient += T.sgn(params) * self.l\n         return gradient\n \n-    def __call__(self, gradient, params):\n-        return self.update_gradient(gradient, params)\n-\n \n class WeightsL2(Regularizer):\n     def __init__(self, l=0.01):\n@@ -40,9 +37,6 @@ class WeightsL2(Regularizer):\n         gradient += params * self.l\n         return gradient\n \n-    def __call__(self, gradient, params):\n-        return self.update_gradient(gradient, params)\n-\n \n class WeightsL1L2(Regularizer):\n     def __init__(self, l1=0.01, l2=0.01):\n@@ -57,8 +51,6 @@ class WeightsL1L2(Regularizer):\n         gradient += T.sgn(params) * self.l1\n         return gradient\n \n-    def __call__(self, gradient, params):\n-        return self.update_gradient(gradient, params)\n \n class Identity(Regularizer):\n     def __init__(self):\n@@ -69,9 +61,6 @@ class Identity(Regularizer):\n     def update_gradient(self, gradient, params):\n         return gradient\n \n-    def __call__(self, gradient, params):\n-        return self.update_gradient(gradient, params)\n-\n \n class ActivityL1(Regularizer):\n     def __init__(self, l = 0.01):\n@@ -87,8 +76,6 @@ class ActivityL1(Regularizer):\n     def update_loss(self, loss):\n         return loss + self.l * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n \n-    def __call__(self, loss):\n-        return self.update_loss(loss)\n \n class ActivityL2(Regularizer):\n     def __init__(self, l = 0.01):\n@@ -104,8 +91,6 @@ class ActivityL2(Regularizer):\n     def update_loss(self, loss):\n         return loss + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n \n-    def __call__(self, loss):\n-        return self.update_loss(loss)\n \n #old style variables for backwards compatibility\n l1 = weights_l1 = WeightsL1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#15055483ff7046970fd9ca6cf6db108585a11fc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 63 | Churn Cumulative: 147 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,9 @@ def hard_sigmoid(x):\n     return T.nnet.hard_sigmoid(x)\n \n def linear(x):\n+    '''\n+    The function returns the variable that is passed in, so all types work\n+    '''\n     return x\n \n from .utils.generic_utils import get_from_module\n\n@@ -1,3 +1,5 @@\n+import math\n+\n import keras\n import theano\n import theano.tensor as T\n@@ -5,10 +7,21 @@ import theano.tensor as T\n import numpy\n \n def list_assert_equal(a, b, round_to=7):\n+    '''\n+    This will do a pairwise, rounded equality test across two lists of\n+    numbers.\n+    '''\n     pairs = zip(a, b)\n     for i, j in pairs:\n         assert round(i, round_to) == round(j, round_to)\n \n+def get_standard_values():\n+    '''\n+    These are just a set of floats used for testing the activation\n+    functions, and are useful in multiple tests.\n+    '''\n+\n+    return [0,0.1,0.5,0.9,1.0]\n \n def test_softmax():\n \n@@ -26,7 +39,7 @@ def test_softmax():\n     x = T.vector()\n     exp = s(x)\n     f = theano.function([x], exp)\n-    test_values=[0,0.1,0.5,0.9,1.0]\n+    test_values=get_standard_values()\n \n     result = f(test_values)\n     expected = softmax(test_values)\n@@ -36,7 +49,52 @@ def test_softmax():\n \n     list_assert_equal(result, expected)\n \n+def test_relu():\n+    '''\n+    Relu implementation doesn't depend on the value being\n+    a theano variable. Testing ints, floats and theano tensors.\n+    '''\n+\n+    from keras.activations import relu as r\n+\n+    assert r(5) == 5\n+    assert r(-5) == 0\n+    assert r(-0.1) == 0\n+    assert r(0.1) == 0.1\n+\n+    x = T.vector()\n+    exp = r(x)\n+    f = theano.function([x], exp)\n+\n+    test_values = get_standard_values()\n+    result = f(test_values)\n+\n+    list_assert_equal(result, test_values) # because no negatives in test values\n+\n+\n+def test_tanh():\n+\n+    from keras.activations import tanh as t\n+    test_values = get_standard_values()\n+\n+    x = T.vector()\n+    exp = t(x)\n+    f = theano.function([x], exp)\n+\n+    result = f(test_values)\n+    expected = [math.tanh(v) for v in test_values]\n+\n+    print(result)\n+    print(expected)\n+\n+    list_assert_equal(result, expected)\n+\n+\n def test_linear():\n+    '''\n+    This function does no input validation, it just returns the thing\n+    that was passed in.\n+    '''\n \n     from keras.activations import linear as l\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d7085d52361b476db3a36c329d5081ecac82965", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 22 | Churn Cumulative: 298 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,9 +3,6 @@ import theano.tensor as T\n \n \n class Regularizer(object):\n-    def __init__(self):\n-        self.modifies_gradient = False\n-        self.modifies_loss = False\n \n     def update_gradient(self, gradient, params):\n         raise NotImplementedError\n@@ -16,10 +13,7 @@ class Regularizer(object):\n \n class WeightsL1(Regularizer):\n     def __init__(self, l=0.01):\n-        super(Regularizer, self).__init__()\n-\n         self.l = l\n-        self.modifies_gradient = True\n \n     def update_gradient(self, gradient, params):\n         gradient += T.sgn(params) * self.l\n@@ -28,10 +22,7 @@ class WeightsL1(Regularizer):\n \n class WeightsL2(Regularizer):\n     def __init__(self, l=0.01):\n-        super(Regularizer, self).__init__()\n-\n         self.l = l\n-        self.modifies_gradient = True\n \n     def update_gradient(self, gradient, params):\n         gradient += params * self.l\n@@ -40,11 +31,8 @@ class WeightsL2(Regularizer):\n \n class WeightsL1L2(Regularizer):\n     def __init__(self, l1=0.01, l2=0.01):\n-        super(Regularizer, self).__init__()\n-\n         self.l1 = l1\n         self.l2 = l2\n-        self.modifies_gradient = True\n \n     def update_gradient(self, gradient, params):\n         gradient += params * self.l2\n@@ -53,10 +41,6 @@ class WeightsL1L2(Regularizer):\n \n \n class Identity(Regularizer):\n-    def __init__(self):\n-        super(Regularizer, self).__init__()\n-\n-        self.modifies_gradient = True\n \n     def update_gradient(self, gradient, params):\n         return gradient\n@@ -64,11 +48,8 @@ class Identity(Regularizer):\n \n class ActivityL1(Regularizer):\n     def __init__(self, l = 0.01):\n-        super(Regularizer, self).__init__()\n-\n         self.l = l\n         self.layer = None\n-        self.modifies_loss = True\n \n     def set_layer(self, layer):\n         self.layer = layer\n@@ -79,11 +60,8 @@ class ActivityL1(Regularizer):\n \n class ActivityL2(Regularizer):\n     def __init__(self, l = 0.01):\n-        super(Regularizer, self).__init__()\n-\n         self.l = l\n         self.layer = None\n-        self.modifies_loss = True\n \n     def set_layer(self, layer):\n         self.layer = layer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aaceb11b9e86e67dc291339dc918f0a9b4b9bd13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 8 | Churn Cumulative: 1534 | Contributors (this commit): 17 | Commits (past 90d): 72 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -402,6 +402,10 @@ class Autoencoder(Sequential):\n         super(Autoencoder,self).fit(X,X,**kwargs)\n \n     def freeze_encoder(self):\n-        def zero_grad(g, p):\n-            return 0.\n+        class ZeroGrad(regularizers.Regularizer):\n+\n+            def update_gradient(self, gradient, params):\n+                return 0.*gradient\n+\n+        zero_grad = ZeroGrad()\n         self.encoder.regularizers = [zero_grad for r in self.encoder.regularizers]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad99af7be7a97407954d00a76fb9d46ce0932dfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 6 | Churn Cumulative: 752 | Contributors (this commit): 6 | Commits (past 90d): 32 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -27,9 +27,9 @@ class BaseRecurrent(MaskedLayer):\n \n         if pad > 0:\n             # left-pad in time with 0\n-            padding = alloc_zeros_matrix(pad, mask.shape[1], 1).astype('uint8')\n-            return T.concatenate([padding, mask], axis=0)\n-        return mask\n+            padding = alloc_zeros_matrix(pad, mask.shape[1], 1)\n+            mask = T.concatenate([padding, mask], axis=0)\n+        return mask.astype('int8')\n \n \n class SimpleRNN(BaseRecurrent):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da20a4ccd2cf5d9033358b55589a2f75a83d1dea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 31 | Churn Cumulative: 1565 | Contributors (this commit): 17 | Commits (past 90d): 73 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -93,7 +93,7 @@ class Model(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        if hasattr(self, 'cost_updates'):\n+        if hasattr(self, 'loss_update'):\n             for u in self.loss_updates:\n                 train_loss = u.update_loss(train_loss)\n \n@@ -380,32 +380,3 @@ class Sequential(Model, containers.Sequential):\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n         f.close()\n-\n-\n-class Autoencoder(Sequential):\n-\n-    def __init__(self, encoder, decoder):\n-        super(Autoencoder,self).__init__()\n-        self.encoder = encoder\n-        self.decoder = decoder\n-\n-        self.add(Merge([self.encoder]))\n-        self.add(Merge([self.decoder]))\n-\n-    def train(self, X, **kwargs):\n-        super(Autoencoder,self).train(X,X,**kwargs)\n-\n-    def test(self, X, **kwargs):\n-        super(Autoencoder,self).test(X,X,**kwargs)\n-\n-    def fit(self, X, **kwargs):\n-        super(Autoencoder,self).fit(X,X,**kwargs)\n-\n-    def freeze_encoder(self):\n-        class ZeroGrad(regularizers.Regularizer):\n-\n-            def update_gradient(self, gradient, params):\n-                return 0.*gradient\n-\n-        zero_grad = ZeroGrad()\n-        self.encoder.regularizers = [zero_grad for r in self.encoder.regularizers]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71787762db4f1a9efa0c74799538a2f4630c6dac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 64 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 64 | Churn Cumulative: 1906 | Contributors (this commit): 18 | Commits (past 90d): 80 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -9,8 +9,6 @@ from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n-from .layers.core import Merge\n-\n import time, copy\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n\n@@ -1,62 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential, Autoencoder\n-from keras.layers.core import Dense, Dropout, Activation, Layer, Merge\n-from keras.optimizers import RMSprop\n-import theano.tensor as T\n-import numpy as np\n-\n-from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n-srng = RandomStreams()\n-\n-\n-(X_train, y_train), (_, _) = mnist.load_data()\n-\n-X_train = X_train.reshape((X_train.shape[0],-1)).astype(float)/255.\n-\n-encoder = Sequential()\n-decoder = Sequential()\n-\n-n_hidden = 64\n-\n-layer_enc = Dense(X_train.shape[1],n_hidden,W_regularizer=None)\n-layer_dec = Dense(n_hidden,X_train.shape[1])\n-\n-# Tie the weights of encoder and decoder\n-layer_dec.params.remove(layer_dec.W)\n-layer_dec.W = T.transpose(layer_enc.W)\n-\n-encoder.add(layer_enc)\n-encoder.add(Dropout(p=0.3))\n-encoder.add(Activation('relu'))\n-encoder.add(Activation('softmax'))\n-\n-decoder.add(layer_dec)\n-decoder.add(Activation('relu'))\n-\n-autoencoder = Autoencoder(encoder,decoder)\n-\n-rms = RMSprop()\n-autoencoder.compile(loss='mean_squared_error', optimizer=rms)\n-\n-autoencoder.fit(X_train, verbose=1, nb_epoch=10)\n-\n-autoencoder.freeze_encoder()\n-\n-testnet = Sequential()\n-testnet.add(Merge([autoencoder.encoder]))\n-testnet.add(Dense(n_hidden,10))\n-testnet.add(Activation('sigmoid'))\n-\n-W_pretrain = layer_enc.W.get_value()\n-\n-y_train_full = np.zeros((y_train.shape[0],10))\n-for n in range(len(y_train)):\n-    y_train_full[n,y_train[n]] = 1.\n-testnet.compile(loss='mean_squared_error', optimizer=rms)\n-testnet.fit(X_train,y_train_full,nb_epoch=10)\n-\n-W_posttrain = layer_enc.W.get_value()\n-\n-assert(np.abs(W_posttrain-W_pretrain).max() < 1e-9)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d57206dfd9d5896b5be95aff495577e51f0785be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 1570 | Contributors (this commit): 17 | Commits (past 90d): 75 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,7 +47,8 @@ def weighted_obj_fn(fn):\n         masked_y_true = y_true[weights.nonzero()[:-1]]\n         masked_y_pred = y_pred[weights.nonzero()[:-1]]\n         masked_weights = weights[weights.nonzero()]\n-        return (masked_weights * fn(masked_y_true, masked_y_pred)).mean()\n+        obj_output = fn(masked_y_true, masked_y_pred)\n+        return (masked_weights.flatten() * obj_output.flatten()).mean()\n     return weighted\n \n class Model(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95a363f3d83a2bfd3bb8c6af6006947ec8af8b20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 5 | Churn Cumulative: 1056 | Contributors (this commit): 11 | Commits (past 90d): 52 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -80,6 +80,8 @@ class Merge(object):\n         ''' Merge the output of a list of models into a single tensor.\n             mode: {'sum', 'concat'}\n         '''\n+        if len(models) < 2:\n+            raise Exception(\"Please specify two or more input models to merge\")\n         self.mode = mode\n         self.models = models\n         self.params = []\n@@ -136,9 +138,6 @@ class Merge(object):\n             \"models\":[m.get_config() for m in self.models],\n             \"mode\":self.mode}\n \n-    def connect(self, node, which=0):\n-        self.models[which].layers[0].previous = node\n-\n \n class Dropout(Layer):\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ff19e2a62cee44491a9a293434f47267870acf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1064 | Contributors (this commit): 11 | Commits (past 90d): 53 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -312,13 +312,7 @@ class TimeDistributedDense(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-\n-        def act_func(X):\n-            return self.activation(T.dot(X, self.W) + self.b)\n-\n-        output, _ = theano.scan(fn = act_func,\n-                                sequences = X.dimshuffle(1,0,2),\n-                                outputs_info=None)\n+        output = self.activation(T.dot(X.dimshuffle(1,0,2), self.W) + self.b)\n         return output.dimshuffle(1,0,2)\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d1976c46dcd8046af45dea0e38967dc90a7104c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 124 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,22 +21,24 @@ from keras.datasets import imdb\n \n     - RNNs are tricky. Choice of batch size is important, \n     choice of loss and optimizer is critical, etc. \n-    Most configurations won't converge.\n+    Some configurations won't converge.\n \n-    - LSTM loss decrease during training can be quite different \n-    from what you see with CNNs/MLPs/etc. It's more or less a sigmoid\n-    instead of an inverse exponential.\n+    - LSTM loss decrease patterns during training can be quite different \n+    from what you see with CNNs/MLPs/etc. \n \n     GPU command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n \n     250s/epoch on GPU (GT 650M), vs. 400s/epoch on CPU (2.4Ghz Core i7). \n+    Increasing the batch_size increases the GPU speedup.\n '''\n \n max_features = 20000\n maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n batch_size = 16\n \n+np.random.seed(1337)\n+\n print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n print(len(X_train), 'train sequences')\n@@ -50,7 +52,7 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 256))\n+model.add(Embedding(max_features, 256, mask_zero=True))\n model.add(LSTM(256, 128)) # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(128, 1))\n@@ -60,7 +62,7 @@ model.add(Activation('sigmoid'))\n model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n \n print(\"Train...\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=5, validation_split=0.1, show_accuracy=True)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_split=0.1, show_accuracy=True)\n score = model.evaluate(X_test, y_test, batch_size=batch_size)\n print('Test score:', score)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45e444cc63f9ebe9417a05c970596dbb041c5115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 17 | Methods Changed: 11 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 29 | Churn Cumulative: 2351 | Contributors (this commit): 20 | Commits (past 90d): 109 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,10 +9,10 @@ from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n from ..layers.core import Layer, MaskedLayer\n from six.moves import range\n \n-class BaseRecurrent(MaskedLayer):\n+class Recurrent(MaskedLayer):\n     def get_output_mask(self, train=None):\n         if self.return_sequences:\n-            return super(BaseRecurrent, self).get_output_mask(train)\n+            return super(Recurrent, self).get_output_mask(train)\n         else:\n             return None\n \n@@ -20,6 +20,7 @@ class BaseRecurrent(MaskedLayer):\n         mask = self.get_input_mask(train)\n         if mask is None:\n             mask = T.ones_like(X.sum(axis=-1)) # is there a better way to do this without a sum?\n+\n         # mask is (nb_samples, time)\n         mask = T.shape_padright(mask) # (nb_samples, time, 1)\n         mask = T.addbroadcast(mask, -1) # (time, nb_samples, 1) matrix.\n@@ -32,7 +33,7 @@ class BaseRecurrent(MaskedLayer):\n         return mask.astype('int8')\n \n \n-class SimpleRNN(BaseRecurrent):\n+class SimpleRNN(Recurrent):\n     '''\n         Fully connected RNN where output is to fed back to input.\n \n@@ -43,6 +44,7 @@ class SimpleRNN(BaseRecurrent):\n     def __init__(self, input_dim, output_dim, \n         init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n         truncate_gradient=-1, return_sequences=False):\n+\n         super(SimpleRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -70,12 +72,10 @@ class SimpleRNN(BaseRecurrent):\n         return self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n \n     def get_output(self, train):\n-        X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n+        X = self.get_input(train) # shape: (nb_samples, time (padded with zeros), input_dim)\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n-\n-\n         x = T.dot(X, self.W) + self.b\n         \n         # scan = theano symbolic loop.\n@@ -105,7 +105,7 @@ class SimpleRNN(BaseRecurrent):\n             \"return_sequences\":self.return_sequences}\n \n \n-class SimpleDeepRNN(BaseRecurrent):\n+class SimpleDeepRNN(Recurrent):\n     '''\n         Fully connected RNN where the output of multiple timesteps \n         (up to \"depth\" steps in the past) is fed back to the input:\n@@ -119,6 +119,7 @@ class SimpleDeepRNN(BaseRecurrent):\n         init='glorot_uniform', inner_init='orthogonal', \n         activation='sigmoid', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n+\n         super(SimpleDeepRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -191,7 +192,7 @@ class SimpleDeepRNN(BaseRecurrent):\n             \"return_sequences\":self.return_sequences}\n \n \n-class GRU(BaseRecurrent):\n+class GRU(Recurrent):\n     '''\n         Gated Recurrent Unit - Cho et al. 2014\n \n@@ -260,7 +261,6 @@ class GRU(BaseRecurrent):\n         r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n         hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n         h_t = z * h_mask_tm1 + (1 - z) * hh_t\n-        #return theano.printing.Print(\"h_t\")(h_t)\n         return h_t\n \n     def get_output(self, train):\n@@ -296,7 +296,7 @@ class GRU(BaseRecurrent):\n \n \n \n-class LSTM(BaseRecurrent):\n+class LSTM(Recurrent):\n     '''\n         Acts as a spatiotemporal projection,\n         turning a sequence of vectors into a single vector.\n@@ -481,7 +481,7 @@ class JZS1(Layer):\n             self.set_weights(weights)\n \n     def _step(self, \n-        xz_t, xr_t, xh_t, \n+        xz_t, xr_t, xh_t, mask_tm1,\n         h_tm1, \n         u_r, u_h):\n         z = self.inner_activation(xz_t)\n@@ -493,13 +493,14 @@ class JZS1(Layer):\n     def get_output(self, train):\n         X = self.get_input(train) \n         X = X.dimshuffle((1, 0, 2)) \n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n \n         x_z = T.dot(X, self.W_z) + self.b_z\n         x_r = T.dot(X, self.W_r) + self.b_r\n         x_h = T.tanh(T.dot(X, self.Pmat)) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n-            sequences=[x_z, x_r, x_h], \n+            sequences=[x_z, x_r, x_h, padded_mask], \n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient\n\n@@ -41,7 +41,7 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n-def weighted_obj_fn(fn):\n+def weighted_objective(fn):\n     def weighted(y_true, y_pred, weights):\n         # it's important that 0 * Inf == 0, not NaN, so I need to mask first\n         masked_y_true = y_true[weights.nonzero()[:-1]]\n@@ -55,7 +55,7 @@ class Model(object):\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n-        self.loss = weighted_obj_fn(objectives.get(loss))\n+        self.loss = weighted_objective(objectives.get(loss))\n \n         # input of model \n         self.X_train = self.get_input(train=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99c20e25c9a6d02b6bde0b7fb2cdb98ef588e055", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 411 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from __future__ import print_function\n import numpy as np\n+np.random.seed(1337) # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.optimizers import SGD, RMSprop, Adagrad\n@@ -28,16 +29,11 @@ from keras.datasets import imdb\n \n     GPU command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n-\n-    250s/epoch on GPU (GT 650M), vs. 400s/epoch on CPU (2.4Ghz Core i7). \n-    Increasing the batch_size increases the GPU speedup.\n '''\n \n max_features = 20000\n maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n-batch_size = 16\n-\n-np.random.seed(1337)\n+batch_size = 32\n \n print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n@@ -52,7 +48,7 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 256, mask_zero=True))\n+model.add(Embedding(max_features, 256))\n model.add(LSTM(256, 128)) # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(128, 1))\n\n@@ -1,11 +1,13 @@\n from __future__ import absolute_import\n from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n-import numpy as np\n \n '''\n     Train a simple convnet on the MNIST dataset.\n\n@@ -1,11 +1,13 @@\n from __future__ import absolute_import\n from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n from keras.optimizers import SGD, Adam, RMSprop\n from keras.utils import np_utils\n-import numpy as np\n \n '''\n     Train a simple deep NN on the MNIST dataset.\n@@ -18,7 +20,7 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 20\n \n-np.random.seed(1337) # for reproducibility\n+\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from __future__ import print_function\n import numpy as np\n+np.random.seed(1337) # for reproducibility\n \n from keras.datasets import reuters\n from keras.models import Sequential\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea0b7d263c54757ae95282185557ecf0b7d809e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 22/11 | Churn Δ: 67 | Churn Cumulative: 141 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,8 +4,11 @@ import gzip\n from .data_utils import get_file\n import random\n from six.moves import zip\n+import numpy as np\n+\n+def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, \n+    start_char=1, oov_char=2, index_from=3):\n \n-def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/imdb.pkl\")\n \n     if path.endswith(\".gz\"):\n@@ -16,10 +19,15 @@ def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_spli\n     X, labels = six.moves.cPickle.load(f)\n     f.close()\n \n-    random.seed(seed)\n-    random.shuffle(X)\n-    random.seed(seed)\n-    random.shuffle(labels)\n+    np.random.seed(seed)\n+    np.random.shuffle(X)\n+    np.random.seed(seed)\n+    np.random.shuffle(labels)\n+\n+    if start_char is not None:\n+        X = [[start_char] + [w + index_from for w in x] for x in X]\n+    elif index_from:\n+        X = [[w + index_from for w in x] for x in X]\n \n     if maxlen:\n         new_X = []\n@@ -34,7 +42,20 @@ def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_spli\n     if not nb_words:\n         nb_words = max([max(x) for x in X])\n \n-    X = [[0 if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+    # by convention, use 2 as OOV word\n+    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n+    if oov_char is not None:\n+        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+    else:\n+        nX = []\n+        for x in X:\n+            nx = []\n+            for w in x:\n+                if (w >= nb_words or w < skip_top):\n+                    nx.append(w)\n+            nX.append(nx)\n+        X = nX\n+\n     X_train = X[:int(len(X)*(1-test_split))]\n     y_train = labels[:int(len(X)*(1-test_split))]\n \n\n@@ -7,6 +7,7 @@ import random\n import os\n import six.moves.cPickle\n from six.moves import zip\n+import numpy as np\n \n def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'), min_samples_per_topic=15):\n     import re\n@@ -79,16 +80,24 @@ def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'),\n \n \n \n-def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113):\n+def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113,\n+    start_char=1, oov_char=2, index_from=3):\n+\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n     f = open(path, 'rb')\n \n     X, labels = six.moves.cPickle.load(f)\n     f.close()\n-    random.seed(seed)\n-    random.shuffle(X)\n-    random.seed(seed)\n-    random.shuffle(labels)\n+\n+    np.random.seed(seed)\n+    np.random.shuffle(X)\n+    np.random.seed(seed)\n+    np.random.shuffle(labels)\n+\n+    if start_char is not None:\n+        X = [[start_char] + [w + index_from for w in x] for x in X]\n+    elif index_from:\n+        X = [[w + index_from for w in x] for x in X]\n \n     if maxlen:\n         new_X = []\n@@ -103,7 +112,20 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_s\n     if not nb_words:\n         nb_words = max([max(x) for x in X])\n \n-    X = [[0 if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+    # by convention, use 2 as OOV word\n+    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n+    if oov_char is not None:\n+        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n+    else:\n+        nX = []\n+        for x in X:\n+            nx = []\n+            for w in x:\n+                if (w >= nb_words or w < skip_top):\n+                    nx.append(w)\n+            nX.append(nx)\n+        X = nX\n+\n     X_train = X[:int(len(X)*(1-test_split))]\n     y_train = labels[:int(len(X)*(1-test_split))]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c8e0d43f35c0581f0d2ac8c225d25d57490629e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 128 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import numpy as np\n import random\n from six.moves import range\n \n-def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre'):\n+def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', value=0.):\n     \"\"\"\n         Pad each sequence to the same length: \n         the length of the longuest sequence.\n@@ -20,7 +20,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre'):\n     if maxlen is None:\n         maxlen = np.max(lengths)\n \n-    x = np.zeros((nb_samples, maxlen)).astype(dtype)\n+    x = (np.ones((nb_samples, maxlen)) * value).astype(dtype)\n     for idx, s in enumerate(sequences):\n         if padding == 'post':\n             x[idx, :lengths[idx]] = s[:maxlen]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b6f56a0407bd73fe4b0193191885113da1e3e5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 820 | Contributors (this commit): 6 | Commits (past 90d): 34 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -416,7 +416,7 @@ class LSTM(Recurrent):\n \n \n \n-class JZS1(Layer):\n+class JZS1(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -484,16 +484,17 @@ class JZS1(Layer):\n         xz_t, xr_t, xh_t, mask_tm1,\n         h_tm1, \n         u_r, u_h):\n+        h_mask_tm1 = mask_tm1 * h_tm1\n         z = self.inner_activation(xz_t)\n-        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n-        h_t = hh_t * z + h_tm1 * (1 - z)\n+        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n+        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n     def get_output(self, train):\n         X = self.get_input(train) \n-        X = X.dimshuffle((1, 0, 2)) \n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n+        X = X.dimshuffle((1, 0, 2))\n \n         x_z = T.dot(X, self.W_z) + self.b_z\n         x_r = T.dot(X, self.W_r) + self.b_r\n@@ -522,7 +523,7 @@ class JZS1(Layer):\n \n \n \n-class JZS2(Layer):\n+class JZS2(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -588,17 +589,19 @@ class JZS2(Layer):\n             self.set_weights(weights)\n \n     def _step(self, \n-        xz_t, xr_t, xh_t, \n+        xz_t, xr_t, xh_t, mask_tm1,\n         h_tm1, \n         u_z, u_r, u_h):\n-        z = self.inner_activation(xz_t + T.dot(h_tm1, u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n-        h_t = hh_t * z + h_tm1 * (1 - z)\n+        h_mask_tm1 = mask_tm1 * h_tm1\n+        z = self.inner_activation(xz_t + T.dot(h_mask_tm1, u_z))\n+        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n+        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n     def get_output(self, train):\n         X = self.get_input(train)\n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n \n         x_z = T.dot(X, self.W_z) + self.b_z\n@@ -606,7 +609,7 @@ class JZS2(Layer):\n         x_h = T.dot(X, self.W_h) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n-            sequences=[x_z, x_r, x_h], \n+            sequences=[x_z, x_r, x_h, padded_mask],\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient\n@@ -628,7 +631,7 @@ class JZS2(Layer):\n \n \n \n-class JZS3(Layer):\n+class JZS3(Recurrent):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -687,17 +690,19 @@ class JZS3(Layer):\n             self.set_weights(weights)\n \n     def _step(self, \n-        xz_t, xr_t, xh_t, \n+        xz_t, xr_t, xh_t, mask_tm1,\n         h_tm1, \n         u_z, u_r, u_h):\n-        z = self.inner_activation(xz_t + T.dot(T.tanh(h_tm1), u_z))\n-        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n-        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n-        h_t = hh_t * z + h_tm1 * (1 - z)\n+        h_mask_tm1 = mask_tm1 * h_tm1\n+        z = self.inner_activation(xz_t + T.dot(T.tanh(h_mask_tm1), u_z))\n+        r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))\n+        h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n     def get_output(self, train):\n         X = self.get_input(train)\n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n \n         x_z = T.dot(X, self.W_z) + self.b_z\n@@ -705,7 +710,7 @@ class JZS3(Layer):\n         x_h = T.dot(X, self.W_h) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n-            sequences=[x_z, x_r, x_h], \n+            sequences=[x_z, x_r, x_h, padded_mask],\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d267f5ba3c24781dd190aeb633f93558c0100f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 49 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 49 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,49 +0,0 @@\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Flatten, ActivityRegularization\n-from keras.utils import np_utils\n-from keras.regularizers import activity_l1\n-\n-import numpy as np\n-\n-np.random.seed(1337)\n-\n-max_train_samples = 128*8\n-max_test_samples = 1000\n-\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape((-1, 28*28))\n-X_test = X_test.reshape((-1, 28*28))\n-\n-nb_classes = len(np.unique(y_train))\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-model_noreg = Sequential()\n-model_noreg.add(Flatten())\n-model_noreg.add(Dense(28*28, 20, activation='sigmoid'))\n-model_noreg.add(Dense(20, 10, activation='sigmoid'))\n-\n-model_noreg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-model_noreg.fit(X_train, Y_train)\n-\n-score_noreg = model_noreg.evaluate(X_test, Y_test)\n-score_train_noreg = model_noreg.evaluate(X_train, Y_train)\n-\n-model_reg = Sequential()\n-model_reg.add(Flatten())\n-model_reg.add(Dense(28*28, 20, activation='sigmoid'))\n-model_reg.add(ActivityRegularization(activity_l1(0.1)))\n-model_reg.add(Dense(20, 10, activation='sigmoid'))\n-\n-model_reg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-model_reg.fit(X_train, Y_train)\n-\n-score_reg = model_reg.evaluate(X_test, Y_test)\n-score_train_reg = model_reg.evaluate(X_train, Y_train)\n-\n-print\n-print\n-print \"Overfitting without regularisation: %f - %f = %f\" % ( score_noreg , score_train_noreg , score_noreg-score_train_noreg)\n-print \"Overfitting with regularisation: %f - %f = %f\" % ( score_reg , score_train_reg , score_reg-score_train_reg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f12e0119bc8c5c987c741e39024bb87234c8a76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 28 | Churn Cumulative: 50 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,20 +3,30 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n-def maxnorm(m=2):\n-    def maxnorm_wrap(p):\n+class Constraint(object):\n+    def __call__(self, p):\n+        return p\n+\n+class MaxNorm(Constraint):\n+    def __init__(self, m=2):\n+        self.m = m\n+\n+    def __call__(self, p):\n         norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n-        desired = T.clip(norms, 0, m)\n+        desired = T.clip(norms, 0, self.m)\n         p = p * (desired / (1e-7 + norms))\n         return p\n-    return maxnorm_wrap\n \n-def nonneg(p):\n+class NonNeg(Constraint):\n+    def __call__(self, p):\n         p *= T.ge(p, 0)\n         return p\n \n-def identity(g):\n-    return g\n-\n-def unitnorm(e):\n+def UnitNorm(Constraint):\n+    def __call__(self, p):\n         return e / T.sqrt(T.sum(e**2, axis=-1, keepdims=True))\n+\n+identity = Constraint\n+maxnorm = MaxNorm\n+nonneg = NonNeg\n+unitnorm = UnitNorm\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9addefa289ca397598fde4b0bc697eb22119e5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 16 | Churn Cumulative: 314 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,14 +1,12 @@\n from __future__ import absolute_import\n import theano.tensor as T\n \n-\n class Regularizer(object):\n-\n     def update_gradient(self, gradient, params):\n-        raise NotImplementedError\n+        return gradient\n \n     def update_loss(self, loss):\n-        raise NotImplementedError\n+        return loss\n \n \n class WeightsL1(Regularizer):\n@@ -40,12 +38,6 @@ class WeightsL1L2(Regularizer):\n         return gradient\n \n \n-class Identity(Regularizer):\n-\n-    def update_gradient(self, gradient, params):\n-        return gradient\n-\n-\n class ActivityL1(Regularizer):\n     def __init__(self, l=0.01):\n         self.l = l\n@@ -70,11 +62,11 @@ class ActivityL2(Regularizer):\n         return loss + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n \n \n-#old style variables for backwards compatibility\n+identity = Regularizer\n+\n l1 = weights_l1 = WeightsL1\n l2 = weights_l2 = WeightsL2\n l1l2 = weights_l1l2 = WeightsL1L2\n-identity = Identity()\n \n activity_l1 = ActivityL1\n activity_l2 = ActivityL2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#499a05003bbfe050cd8a089a4e1452299dc317b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ def create_model(weight_reg=None, activity_reg=None):\n \n class TestRegularizers(unittest.TestCase):\n     def test_W_reg(self):\n-        for reg in [regularizers.identity, regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n+        for reg in [regularizers.identity(), regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n             model = create_model(weight_reg=reg)\n             model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n             model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n\n@@ -35,7 +35,7 @@ model = Sequential()\n model.add(Dense(784, 20, W_constraint=maxnorm(1)))\n model.add(Activation('relu'))\n model.add(Dropout(0.1))\n-model.add(Dense(20, 20, W_constraint=nonneg))\n+model.add(Dense(20, 20, W_constraint=nonneg()))\n model.add(Activation('relu'))\n model.add(Dropout(0.1))\n model.add(Dense(20, 10, W_constraint=maxnorm(1)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
