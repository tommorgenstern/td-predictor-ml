{"custom_id": "keras#9d104c87dd49771892ba22febcedddeeee2d1baf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 945 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -357,7 +357,7 @@ class StringLookup(index_lookup.IndexLookup):\n     During `adapt()`, the layer will build a vocabulary of all string tokens\n     seen in the dataset, sorted by occurance count, with ties broken by sort\n     order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n     adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n     tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n     will also learn the document frequencies of each token in the input dataset.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e9af6bf4c3d376fd2533aa4bf6edef0dbd972bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1110 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -393,7 +393,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n     During `adapt()`, the layer will build a vocabulary of all integer tokens\n     seen in the dataset, sorted by occurance count, with ties broken by sort\n     order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n     adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n     tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n     will also learn the document frequencies of each token in the input dataset.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1eefff16929c0ebf7fcf3f8e40313b51ffc2886", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3309 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -347,7 +347,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"Gets the current size of the layer's vocabulary.\n \n     Returns:\n-      The integer size of the voculary, including optional mask and oov indices.\n+      The integer size of the vocabulary, including optional mask and oov indices.\n     \"\"\"\n     return int(self.lookup_table.size().numpy()) + self._token_start_index()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce154ce4bcc7c4890fddae91037a9c98825f8a23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 132 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n-# Required by third_party/py/tensorflow_gnn/graph/keras/keras_tensors.py\n+# Required by third_party/py/tensorflow_gnn/keras/keras_tensors.py\n from keras.layers.core.tf_op_layer import _delegate_method\n from keras.layers.core.tf_op_layer import _delegate_property\n from keras.layers.core.tf_op_layer import ClassMethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8036b06d827b7ebd3f53140a5f7237d9152f446", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 230 | Lines Deleted: 32 | Files Changed: 5 | Hunks: 35 | Methods Changed: 28 | Complexity Δ (Sum/Max): 33/22 | Churn Δ: 262 | Churn Cumulative: 30750 | Contributors (this commit): 216 | Commits (past 90d): 45 | Contributors (cumulative): 270 | DMM Complexity: 0.5612903225806452\n\nDIFF:\n@@ -1664,9 +1664,13 @@ class BackupAndRestore(Callback):\n         cannot be reused elsewhere to store other files, e.g. by\n         BackupAndRestore callback of another training, or by another callback\n         (ModelCheckpoint) of the same training.\n+      save_freq: `'epoch'` or integer. When set to `'epoch'`\n+        the callback saves the checkpoint at the end of each epoch.\n+        When set to an integer, the callback saves the checkpoint every\n+        `save_freq` batches.\n   \"\"\"\n \n-  def __init__(self, backup_dir):\n+  def __init__(self, backup_dir, save_freq='epoch'):\n     super().__init__()\n     self.backup_dir = backup_dir\n     self._supports_tf_logs = True\n@@ -1675,6 +1679,9 @@ class BackupAndRestore(Callback):\n         tf.distribute.MultiWorkerMirroredStrategy,\n         tf.distribute.experimental.TPUStrategy, tf.distribute.TPUStrategy,\n         tf.distribute.experimental.ParameterServerStrategy)\n+    self.save_freq = save_freq\n+    self._batches_count = 0\n+    self._current_epoch = 0\n \n     if not tf.executing_eagerly():\n       if tf.inside_function():\n@@ -1703,10 +1710,14 @@ class BackupAndRestore(Callback):\n           'Currently BackupAndRestore callback only supports empty strategy, '\n           'MirroredStrategy, MultiWorkerMirroredStrategy and TPUStrategy.')\n     self.model._training_state = (\n-        worker_training_state.WorkerTrainingState(self.model, self.backup_dir))\n+        worker_training_state.WorkerTrainingState(self.model, self.backup_dir,\n+                                                  self.save_freq))\n     self._training_state = self.model._training_state\n     self._training_state.restore()\n \n+  def _implements_train_batch_hooks(self):\n+    return self.save_freq != 'epoch'\n+\n   def on_train_end(self, logs=None):\n     # pylint: disable=protected-access\n     # On exit of training, delete the training state backup file that was saved\n@@ -1717,9 +1728,20 @@ class BackupAndRestore(Callback):\n     del self._training_state\n     del self.model._training_state\n \n+  def on_train_batch_end(self, batch, logs=None):\n+    if self.save_freq != 'epoch':\n+      self._batches_count += 1\n+      if self._batches_count >= self.save_freq:\n+        self._batches_count = 0\n+        self._training_state.back_up(epoch=self._current_epoch, batch=batch)\n+\n+  def on_epoch_begin(self, epoch, logs=None):\n+    self._current_epoch = epoch\n+\n   def on_epoch_end(self, epoch, logs=None):\n     # Back up the model and current epoch for possible future recovery.\n-    self._training_state.back_up(epoch)\n+    if self.save_freq == 'epoch':\n+      self._training_state.back_up(epoch=epoch)\n \n \n @keras_export('keras.callbacks.experimental.BackupAndRestore', v1=[])\n\n@@ -30,6 +30,7 @@ from unittest import mock\n from absl.testing import parameterized\n import keras\n from keras.callbacks import BackupAndRestore\n+from keras.callbacks import Callback\n from keras.callbacks import BackupAndRestoreExperimental\n from keras.engine import sequential\n from keras.layers import Activation\n@@ -330,7 +331,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n \n   def test_backup_restore_train_counter(self):\n     if not tf.compat.v1.executing_eagerly():\n-      self.skipTest('BackupAndRestore only available when execution is enabled')\n+      self.skipTest(\n+          'BackupAndRestore only available when eager execution is enabled')\n     model = keras.Sequential([keras.layers.Dense(1)])\n     model.compile('sgd', 'mse')\n     cbk = BackupAndRestore(self.get_temp_dir())\n@@ -392,6 +394,85 @@ class KerasCallbacksTest(test_combinations.TestCase):\n       model.fit(\n           dataset, epochs=20, steps_per_epoch=5, callbacks=[backup_callback])\n \n+  def _test_backup_and_restore_callback_at_steps(self, cls, epoch_int,\n+                                                 steps_int, mode):\n+    if not tf.compat.v1.executing_eagerly():\n+      self.skipTest(\n+          'BackupAndRestore only available when eager execution is enabled')\n+\n+    class InterruptingCallback(keras.callbacks.Callback):\n+      \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+      batch_count = 0\n+\n+      def on_epoch_end(self, epoch, log=None):\n+        if epoch == epoch_int:\n+          raise RuntimeError('EpochInterruption')\n+\n+      def on_batch_end(self, batch, logs=None):\n+        self.batch_count += 1\n+        if self.batch_count == steps_int:\n+          raise RuntimeError('StepsInterruption')\n+\n+    class VerifyRestore(Callback):\n+      \"\"\"Verify if the training restored to the correct epoch and step.\"\"\"\n+\n+      def __init__(self, initial_epoch, initial_step):\n+        super(VerifyRestore, self).__init__()\n+        self.initial_epoch = initial_epoch\n+        self.initial_step = initial_step\n+        self._current_epoch = 0\n+\n+      def on_epoch_begin(self, epoch, logs=None):\n+        self._current_epoch = epoch\n+        if epoch < self.initial_epoch:\n+          raise ValueError(\n+              'Training did not restore at epoch (%d) and step (%d)' %\n+              (self.initial_epoch, self.initial_step))\n+\n+      def on_batch_begin(self, batch, logs=None):\n+        if (batch <= self.initial_step and\n+            self._current_epoch < self.initial_epoch):\n+          raise ValueError(\n+              'Training did not restore at Epoch (%d) and step (%d)' %\n+              (self.initial_epoch, self.initial_step))\n+\n+    model = keras.Sequential([keras.layers.Dense(10)])\n+    optimizer = gradient_descent.SGD()\n+    model.compile(optimizer, loss='mse')\n+\n+    x = tf.random.uniform((24, 10))\n+    y = tf.random.uniform((24,))\n+    dataset = tf.data.Dataset.from_tensor_slices((x, y)).repeat().batch(2)\n+    save_freq_arg = 'epoch' if mode == 'epoch' else 7\n+    backup_callback = cls(\n+        backup_dir=self.get_temp_dir(), save_freq=save_freq_arg)\n+    # epoch where the restore should resume from\n+    init_epoch = epoch_int if save_freq_arg == 'epoch' else int(\n+        ((steps_int // 7) * 7) // 5)\n+    # step from where the restore should resume from\n+    init_step = 0 if save_freq_arg == 'epoch' else int(((\n+        (steps_int // 7) * 7) % 5) - 1)\n+    # callback to verify accurate training state restore\n+    verify_restore_callback = VerifyRestore(\n+        initial_epoch=init_epoch, initial_step=init_step)\n+    try:\n+      model.fit(\n+          dataset,\n+          epochs=20,\n+          steps_per_epoch=5,\n+          callbacks=[backup_callback, InterruptingCallback()])\n+    except RuntimeError as e:\n+      if str(e) == 'EpochInterruption':\n+        logging.warning('***Handling interruption at epoch***')\n+      elif str(e) == 'StepsInterruption':\n+        logging.warning('***Handling interruption at Nth step***')\n+      # This continues at the epoch and step where it left off.\n+      model.fit(\n+          dataset,\n+          epochs=20,\n+          steps_per_epoch=5,\n+          callbacks=[backup_callback, verify_restore_callback])\n+\n   def test_experimental_backup_and_restore(self):\n     \"\"\"Ensure the legacy endpoint of `BackupAndRestore` gives warning.\"\"\"\n \n@@ -426,6 +507,58 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     warning_msg = ('***Handling interruption***')\n     self.assertIn(warning_msg, '\\n'.join(warning_messages))\n \n+  def test_backup_and_restore_steps(self):\n+    \"\"\"Ensure the public endpoint of `BackupAndRestore` is working.\"\"\"\n+\n+    warning_messages = []\n+\n+    def warning(msg):\n+      warning_messages.append(msg)\n+\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      # interrupt at steps before 1 epoch\n+      self._test_backup_and_restore_callback_at_steps(\n+          BackupAndRestore, epoch_int=20, steps_int=3, mode='batch')\n+    warning_msg = ('`tf.keras.callbacks.experimental.BackupAndRestore` '\n+                   'endpoint is deprecated')\n+    self.assertNotIn(warning_msg, '\\n'.join(warning_messages))\n+    warning_msg = ('***Handling interruption at Nth step***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n+    # interrupt at steps after 1 epoch\n+    warning_messages = []\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      self._test_backup_and_restore_callback_at_steps(\n+          BackupAndRestore, epoch_int=20, steps_int=8, mode='batch')\n+    warning_msg = ('***Handling interruption at Nth step***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n+    # interrupt at epoch before steps\n+    warning_messages = []\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      self._test_backup_and_restore_callback_at_steps(\n+          BackupAndRestore, epoch_int=1, steps_int=12, mode='epoch')\n+    warning_msg = ('***Handling interruption at epoch***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n+  def test_backup_and_restore_steps_last_batch(self):\n+    \"\"\"Ensure the public endpoint of `BackupAndRestore` is working.\"\"\"\n+\n+    warning_messages = []\n+\n+    def warning(msg):\n+      warning_messages.append(msg)\n+\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      # interrupt at last step in 7th epoch\n+      self._test_backup_and_restore_callback_at_steps(\n+          BackupAndRestore, epoch_int=20, steps_int=35, mode='batch')\n+    warning_msg = ('`tf.keras.callbacks.experimental.BackupAndRestore` '\n+                   'endpoint is deprecated')\n+    self.assertNotIn(warning_msg, '\\n'.join(warning_messages))\n+    warning_msg = ('***Handling interruption at Nth step***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n   @test_combinations.run_all_keras_modes\n   def test_callback_warning(self):\n \n\n@@ -27,6 +27,9 @@ CKPT_SAVED_EPOCH = '_ckpt_saved_epoch'\n \n CKPT_SAVED_EPOCH_UNUSED_VALUE = -1\n \n+CKPT_SAVED_BATCH = '_ckpt_saved_batch'\n+\n+CKPT_SAVED_BATCH_UNUSED_VALUE = -1\n \n class WorkerTrainingState:\n   \"\"\"Training state management class.\n@@ -36,23 +39,28 @@ class WorkerTrainingState:\n   for fault-tolerance, also known as preemption-recovery purpose.\n   \"\"\"\n \n-  def __init__(self, model, checkpoint_dir):\n+  def __init__(self, model, checkpoint_dir, save_freq='epoch'):\n     self._model = model\n-\n-    # The epoch at which the checkpoint is saved. Used for fault-tolerance.\n-    # GPU device only has int64 dtype registered VarHandleOp.\n+    self._save_freq = save_freq\n+    # The batch and epoch at which the checkpoint is saved. Used for\n+    # fault-tolerance. GPU device only has int64 dtype registered VarHandleOp.\n     self._ckpt_saved_epoch = tf.Variable(\n         initial_value=tf.constant(\n             CKPT_SAVED_EPOCH_UNUSED_VALUE, dtype=tf.int64),\n         name='ckpt_saved_epoch')\n-\n+    self._ckpt_saved_batch = tf.Variable(\n+        initial_value=tf.constant(\n+            CKPT_SAVED_BATCH_UNUSED_VALUE, dtype=tf.int64),\n+        name='ckpt_saved_batch')\n     # Variable initialization.\n     backend.set_value(self._ckpt_saved_epoch, CKPT_SAVED_EPOCH_UNUSED_VALUE)\n-\n-    # _ckpt_saved_epoch gets tracked and is included in the checkpoint file\n-    # when backing up.\n+    backend.set_value(self._ckpt_saved_batch, CKPT_SAVED_BATCH_UNUSED_VALUE)\n+    # _ckpt_saved_epoch  and _ckpt_saved_batch gets tracked and is included in\n+    # the checkpoint file when backing up.\n     checkpoint = tf.train.Checkpoint(\n-        model=self._model, ckpt_saved_epoch=self._ckpt_saved_epoch,\n+        model=self._model,\n+        ckpt_saved_epoch=self._ckpt_saved_epoch,\n+        ckpt_saved_batch=self._ckpt_saved_batch,\n         train_counter=self._model._train_counter)\n \n     # If this is single-worker training, checkpoint_dir are the same for\n@@ -78,14 +86,18 @@ class WorkerTrainingState:\n       self.write_checkpoint_manager = tf.train.CheckpointManager(\n           checkpoint, directory=write_checkpoint_dir, max_to_keep=1)\n \n-  def back_up(self, epoch):\n+  def back_up(self, epoch, batch=0):\n     \"\"\"Back up the current state of training into a checkpoint file.\n \n     Args:\n       epoch: The current epoch information to be saved.\n+      batch: The current batch(step) information to be saved.\n     \"\"\"\n+\n     backend.set_value(self._ckpt_saved_epoch, epoch)\n-    # Save the model plus CKPT_SAVED_EPOCH variable.\n+    backend.set_value(self._ckpt_saved_batch, batch)\n+\n+    # Save the model plus CKPT_SAVED_BATCH variable.\n     if self.write_checkpoint_manager.save():\n       distributed_file_utils.remove_temp_dirpath(\n           self.write_checkpoint_manager.directory,\n@@ -112,7 +124,10 @@ class WorkerTrainingState:\n       except tf.errors.NotFoundError:\n         pass\n \n-  def maybe_load_initial_epoch_from_ckpt(self, initial_epoch, mode):\n+  def maybe_load_initial_counters_from_ckpt(self,\n+                                            steps_per_epoch,\n+                                            initial_epoch,\n+                                            mode):\n     \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n \n     When `_ckpt_saved_epoch` attribute exists and is not\n@@ -122,18 +137,36 @@ class WorkerTrainingState:\n     unfinished training from certain epoch.\n \n     Args:\n+      steps_per_epoch: The number of steps per epoch value.\n       initial_epoch: The original initial_epoch user passes in in `fit()`.\n       mode: The mode for running `model.fit()`.\n \n     Returns:\n       If the training is recovering from previous failure under multi-worker\n-      training setting, return the epoch the training is supposed to continue\n-      at. Otherwise, return the `initial_epoch` the user passes in.\n+      training setting, return the (epoch, step) the training is supposed to\n+      continue at. Otherwise, return the `initial_epoch, initial_step` the user\n+      passes in.\n     \"\"\"\n \n+    initial_step = 0\n     epoch = backend.eval(self._ckpt_saved_epoch)\n-    if mode == mode_keys.ModeKeys.TRAIN and epoch >= 0:\n+    batch = backend.eval(self._ckpt_saved_batch)\n+    if mode == mode_keys.ModeKeys.TRAIN:\n+      if self._save_freq == 'epoch':\n+        if epoch >= 0:\n           # The most recently saved epoch is one epoch prior to the epoch it\n           # failed at, so return the value of 'self._ckpt_saved_epoch' plus one.\n-      return epoch + 1\n-    return initial_epoch\n+          initial_epoch = epoch + 1\n+      else:\n+        if batch >= 0 and epoch >= 0:\n+          # If the checkpoint was last saved at last batch of the epoch, return\n+          # the next epoch number and batch=0\n+          if batch == steps_per_epoch - 1:\n+            initial_epoch = epoch + 1\n+            initial_step = 0\n+          else:\n+            # If the checkpoint was not last saved at last batch of the epoch,\n+            # return the same epoch and next batch number\n+            initial_epoch = epoch\n+            initial_step = batch + 1\n+    return (initial_epoch, initial_step)\n\n@@ -1246,6 +1246,7 @@ class DataHandler:\n   def steps(self):\n     \"\"\"Yields steps for the current epoch.\"\"\"\n     self._current_step = self._initial_step\n+    self._initial_step = 0\n     # `self._inferred_steps` can be changed by `catch_stop_iteration`.\n     while (self._inferred_steps is None or\n            self._current_step < self._inferred_steps):\n\n@@ -1404,14 +1404,15 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       # Handle fault-tolerance for multi-worker.\n       # TODO(omalleyt): Fix the ordering issues that mean this has to\n       # happen after `callbacks.on_train_begin`.\n-      data_handler._initial_epoch = (  # pylint: disable=protected-access\n-          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n+      steps_per_epoch_inferred = steps_per_epoch or data_handler.inferred_steps\n+      data_handler._initial_epoch, data_handler._initial_step = (  # pylint: disable=protected-access\n+          self._maybe_load_initial_counters_from_ckpt(steps_per_epoch_inferred,\n+                                                      initial_epoch))  # pylint: disable=protected-access\n       logs = None\n       for epoch, iterator in data_handler.enumerate_epochs():\n         self.reset_metrics()\n         callbacks.on_epoch_begin(epoch)\n         with data_handler.catch_stop_iteration():\n-          data_handler._initial_step = self._maybe_load_initial_step_from_ckpt()  # pylint: disable=protected-access\n           for step in data_handler.steps():\n             with tf.profiler.experimental.Trace(\n                 'train',\n@@ -3171,25 +3172,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               'distribution strategy scope.'\n           )\n \n-  def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch):\n+  def _maybe_load_initial_counters_from_ckpt(self,\n+                                             steps_per_epoch,\n+                                             initial_epoch):\n     \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n \n     Refer to tensorflow/python/keras/distribute/worker_training_state.py\n     for more information.\n \n     Args:\n-      initial_epoch: The original initial_epoch user passes in in `fit()`.\n+      steps_per_epoch: The number of step per epoch.\n+      initial_epoch: The original initial_epoch user passes in `fit()`.\n+      mode: The mode for running `model.fit()`.\n+      initial_step: The original initial_step user passes in `fit()`.\n \n     Returns:\n       If the training is recovering from previous failure under multi-worker\n-      training setting, return the epoch the training is supposed to continue\n-      at. Otherwise, return the `initial_epoch` the user passes in.\n+      training setting, return the (epoch, step) the training is supposed to\n+      continue at. Otherwise, return the `initial_epoch, initial_step` the user\n+      passes in.\n     \"\"\"\n+    initial_step = 0\n     if self._training_state is not None:\n-      return self._training_state.maybe_load_initial_epoch_from_ckpt(\n-          initial_epoch, mode=ModeKeys.TRAIN)\n-\n-    return initial_epoch\n+      return self._training_state.maybe_load_initial_counters_from_ckpt(\n+          steps_per_epoch,\n+          initial_epoch,\n+          mode=ModeKeys.TRAIN)\n+    return (initial_epoch, initial_step)\n \n   def _maybe_load_initial_step_from_ckpt(self):\n     if getattr(self, '_callback_step', 0) > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c19fe0e336456e123bbb747ffcc6d2b911326af4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 60 | Churn Cumulative: 1913 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -140,20 +140,20 @@ class CheckpointingTests(test_combinations.TestCase):\n     named_variables = {v.name: v for v in named_variables}\n     self.assertEqual(len(expected_checkpoint_names),\n                      len(named_variables.keys()))\n-    # Check that we've mapped to the right variable objects (not exhaustive)\n-    self.assertEqual(\n-        \"global_step\",\n-        named_variables[\"step\" + suffix].full_name)\n-    self.assertEqual(\n-        \"my_model/dense_1/kernel\",\n-        named_variables[\"model/_second/kernel\" + suffix].full_name)\n-    self.assertEqual(\n-        \"my_model/dense/kernel\",\n-        named_variables[\"model/_named_dense/kernel\" + suffix].full_name)\n-    self.assertEqual(\"Adam/beta_1\",\n-                     named_variables[\"optimizer/beta_1\" + suffix].full_name)\n-    self.assertEqual(\"Adam/beta_2\",\n-                     named_variables[\"optimizer/beta_2\" + suffix].full_name)\n+    # Check that we've created the right full_names of objects (not exhaustive)\n+    expected_names = {\n+        \"step\" + suffix: \"global_step\",\n+        \"model/_second/kernel\" + suffix: \"my_model/dense_1/kernel\",\n+        \"model/_named_dense/kernel\" + suffix: \"my_model/dense/kernel\",\n+        \"optimizer/beta_1\" + suffix: \"Adam/beta_1\",\n+        \"optimizer/beta_2\" + suffix: \"Adam/beta_2\",\n+    }\n+    for nodes in serialized_graph.nodes:\n+      for attribute in nodes.attributes:\n+        expected_name = expected_names.pop(attribute.checkpoint_key, None)\n+        if expected_name is not None:\n+          self.assertEqual(expected_name, attribute.full_name)\n+    self.assertEmpty(expected_names)\n     # Spot check the generated protocol buffers.\n     self.assertEqual(\"optimizer\",\n                      serialized_graph.nodes[0].children[1].local_name)\n\n@@ -107,22 +107,21 @@ class CheckpointingTests(test_combinations.TestCase):\n     named_variables = {v.name: v for v in named_variables}\n     self.assertEqual(len(expected_checkpoint_names),\n                      len(named_variables.keys()))\n-    # Check that we've mapped to the right variable objects (not exhaustive)\n-    self.assertEqual(\n-        \"global_step\",\n-        named_variables[\"optimizer_step\" + suffix].full_name)\n-    self.assertEqual(\n-        \"my_model/dense_1/kernel\",\n-        named_variables[\"model/_second/kernel\" + suffix].full_name)\n-    self.assertEqual(\n-        \"my_model/dense/kernel\",\n-        named_variables[\"model/_named_dense/kernel\" + suffix].full_name)\n-    self.assertEqual(\n-        \"beta1_power\",\n-        named_variables[\"optimizer/beta1_power\" + suffix].full_name)\n-    self.assertEqual(\n-        \"beta2_power\",\n-        named_variables[\"optimizer/beta2_power\" + suffix].full_name)\n+    # Check that we've created the right full_names of objects (not exhaustive)\n+    expected_names = {\n+        \"optimizer_step\" + suffix: \"global_step\",\n+        \"model/_second/kernel\" + suffix: \"my_model/dense_1/kernel\",\n+        \"model/_named_dense/kernel\" + suffix: \"my_model/dense/kernel\",\n+        \"optimizer/beta1_power\" + suffix: \"beta1_power\",\n+        \"optimizer/beta2_power\" + suffix: \"beta2_power\",\n+    }\n+    for nodes in serialized_graph.nodes:\n+      for attribute in nodes.attributes:\n+        expected_name = expected_names.pop(attribute.checkpoint_key, None)\n+        if expected_name is not None:\n+          self.assertEqual(expected_name, attribute.full_name)\n+    self.assertEmpty(expected_names)\n+\n     # Spot check the generated protocol buffers.\n     self.assertEqual(\"optimizer\",\n                      serialized_graph.nodes[0].children[1].local_name)\n@@ -138,6 +137,7 @@ class CheckpointingTests(test_combinations.TestCase):\n         serialized_graph.nodes[optimizer_node.slot_variables[0]\n                                .original_variable_node_id]\n         .attributes[0].full_name)\n+\n     # We strip off the :0 suffix, as variable.name-based saving does.\n     self.assertEqual(\n         \"my_model/dense/kernel/Adam\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb071fce0807e25ce50277c573ab4361ee4accc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 3312 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -349,6 +349,9 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     Returns:\n       The integer size of the voculary, including optional mask and oov indices.\n     \"\"\"\n+    if tf.executing_eagerly():\n+      return int(self.lookup_table.size().numpy()) + self._token_start_index()\n+    else:\n       return self.lookup_table.size() + self._token_start_index()\n \n   def vocab_size(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ad5ace3866d12a1d3aa829c21f4b7ede00164fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 361 | Lines Deleted: 69 | Files Changed: 4 | Hunks: 35 | Methods Changed: 18 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 430 | Churn Cumulative: 18564 | Contributors (this commit): 32 | Commits (past 90d): 24 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5424,6 +5424,8 @@ def binary_crossentropy(target, output, from_logits=False):\n def binary_focal_crossentropy(\n     target,\n     output,\n+    apply_class_balancing=False,\n+    alpha=0.25,\n     gamma=2.0,\n     from_logits=False,\n ):\n@@ -5435,14 +5437,24 @@ def binary_focal_crossentropy(\n \n   `focal_factor = (1 - output) ** gamma` for class 1\n   `focal_factor = output ** gamma` for class 0\n-  where `gamma` is a focusing parameter. When `gamma` = 0, this function is\n-  equivalent to the binary crossentropy.\n+  where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+  effect on the binary crossentropy.\n+\n+  If `apply_class_balancing == True`, this function also takes into account a\n+  weight balancing factor for the binary classes 0 and 1 as follows:\n+\n+  `weight = alpha` for class 1 (`target == 1`)\n+  `weight = 1 - alpha` for class 0\n+  where `alpha` is a float in the range of `[0, 1]`.\n \n   Args:\n     target: A tensor with the same shape as `output`.\n     output: A tensor.\n-    gamma: A focusing parameter used to compute the focal factor, default is 2.0\n-      as mentioned in reference.\n+    apply_class_balancing: A bool, whether to apply weight balancing on the\n+      binary classes 0 and 1.\n+    alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n+      in the reference. The weight for class 0 is `1.0 - alpha`.\n+    gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n     from_logits: Whether `output` is expected to be a logits tensor. By default,\n       we consider that `output` encodes a probability distribution.\n \n@@ -5454,7 +5466,7 @@ def binary_focal_crossentropy(\n       lambda: sigmoid(output),\n       lambda: output,\n   )\n-  p_t = (target * sigmoidal) + ((1 - target) * (1 - sigmoidal))\n+  p_t = target * sigmoidal + (1 - target) * (1 - sigmoidal)\n   # Calculate focal factor\n   focal_factor = tf.pow(1.0 - p_t, gamma)\n   # Binary crossentropy\n@@ -5463,60 +5475,13 @@ def binary_focal_crossentropy(\n       output=output,\n       from_logits=from_logits,\n   )\n-  return focal_factor * bce\n+  focal_bce = focal_factor * bce\n \n-\n-@keras_export('keras.backend.binary_weighted_focal_crossentropy')\n-@tf.__internal__.dispatch.add_dispatch_support\n-@doc_controls.do_not_generate_docs\n-def binary_weighted_focal_crossentropy(\n-    target,\n-    output,\n-    alpha=0.25,\n-    gamma=2.0,\n-    from_logits=False,\n-):\n-  \"\"\"Binary weighted focal crossentropy between an output tensor and a target.\n-\n-  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-  helps to apply a focal factor to down-weight easy examples and focus more on\n-  hard examples. By default, the focal tensor is computed as follows:\n-\n-  `focal_factor = (1 - output)**gamma` for class 1\n-  `focal_factor = output**gamma` for class 0\n-  where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n-  effect on the binary crossentropy.\n-\n-  This function also takes into account a weight balancing factor for the binary\n-  classes 0 and 1 as follows:\n-\n-  `weight = alpha` for class 1 (`target` = 1)\n-  `weight = 1 - alpha` for class 0\n-  where `alpha` is a float in the range of [0, 1].\n-\n-  Args:\n-    target: A tensor with the same shape as `output`.\n-    output: A tensor.\n-    alpha: A weight balancing factor for class 1, default is 0.25 as mentioned\n-    in reference. The weight for class 0 is 1.0 - `alpha`.\n-    gamma: A focusing parameter, default is 2.0 as mentioned in reference.\n-    from_logits: Whether `output` is expected to be a logits tensor. By default,\n-      we consider that `output` encodes a probability distribution.\n-\n-  Returns:\n-    A tensor.\n-  \"\"\"\n-  # Balancing weight for the binary classes\n+  if apply_class_balancing:\n     weight = target * alpha + (1 - target) * (1 - alpha)\n+    focal_bce = weight * focal_bce\n \n-  # Binary focal crossentropy\n-  bfce = binary_focal_crossentropy(\n-      target=target,\n-      output=output,\n-      gamma=gamma,\n-      from_logits=from_logits,\n-  )\n-  return weight * bfce\n+  return focal_bce\n \n \n @keras_export('keras.backend.sigmoid')\n\n@@ -1873,7 +1873,12 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     logits = backend.constant([[8., 1., 1.]])\n     p = backend.sigmoid(logits)\n     p = tf.identity(tf.identity(p))\n-    result = self.evaluate(backend.binary_weighted_focal_crossentropy(t, p))\n+    result = self.evaluate(\n+        backend.binary_focal_crossentropy(\n+            target=t,\n+            output=p,\n+            apply_class_balancing=True,\n+        ))\n     self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n \n   @test_combinations.generate(\n@@ -1882,9 +1887,10 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n     result = self.evaluate(\n-        backend.binary_weighted_focal_crossentropy(\n+        backend.binary_focal_crossentropy(\n             target=t,\n             output=logits,\n+            apply_class_balancing=True,\n             from_logits=True,\n         ))\n     self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n\n@@ -18,6 +18,7 @@\n \n import abc\n import functools\n+\n from keras import backend\n from keras.saving.experimental import saving_lib\n from keras.utils import generic_utils\n@@ -659,6 +660,12 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n   >>> loss(y_true, y_pred).numpy()\n   0.691\n \n+  >>> # Apply class weight\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+  ...     apply_class_balancing=True, gamma=2, from_logits=True)\n+  >>> loss(y_true, y_pred).numpy()\n+  0.51\n+\n   >>> # Example 2: (batch_size = 2, number of samples = 4)\n   >>> y_true = [[0, 1], [0, 0]]\n   >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n@@ -667,23 +674,56 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n   >>> loss(y_true, y_pred).numpy()\n   0.647\n \n-  >>> # Using 'sample_weight' attribute\n+  >>> # Apply class weight\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+  ...     apply_class_balancing=True, gamma=3, from_logits=True)\n+  >>> loss(y_true, y_pred).numpy()\n+  0.482\n+\n+  >>> # Using 'sample_weight' attribute with focal effect\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3, from_logits=True)\n   >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n   0.133\n \n+  >>> # Apply class weight\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+  ...     apply_class_balancing=True, gamma=3, from_logits=True)\n+  >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n+  0.097\n+\n   >>> # Using 'sum' reduction` type.\n   >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4, from_logits=True,\n   ...     reduction=tf.keras.losses.Reduction.SUM)\n   >>> loss(y_true, y_pred).numpy()\n   1.222\n \n+  >>> # Apply class weight\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+  ...     apply_class_balancing=True, gamma=4, from_logits=True,\n+  ...     reduction=tf.keras.losses.Reduction.SUM)\n+  >>> loss(y_true, y_pred).numpy()\n+  0.914\n+\n   >>> # Using 'none' reduction type.\n   >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=5, from_logits=True,\n   ...     reduction=tf.keras.losses.Reduction.NONE)\n   >>> loss(y_true, y_pred).numpy()\n   array([0.0017 1.1561], dtype=float32)\n \n+  >>> # Apply class weight\n+  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n+  ...     apply_class_balancing=True, gamma=5, from_logits=True,\n+  ...     reduction=tf.keras.losses.Reduction.NONE)\n+  >>> loss(y_true, y_pred).numpy()\n+  array([0.0004 0.8670], dtype=float32)\n+\n+\n   Args:\n+    apply_class_balancing: A bool, whether to apply weight balancing on the\n+      binary classes 0 and 1.\n+    alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n+      in reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+      The weight for class 0 is `1.0 - alpha`.\n     gamma: A focusing parameter used to compute the focal factor, default is\n       `2.0` as mentioned in the reference\n       [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n@@ -711,6 +751,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n \n   def __init__(\n       self,\n+      apply_class_balancing=False,\n+      alpha=0.25,\n       gamma=2.0,\n       from_logits=False,\n       label_smoothing=0.,\n@@ -721,6 +763,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n     super().__init__(\n         binary_focal_crossentropy,\n+        apply_class_balancing=apply_class_balancing,\n+        alpha=alpha,\n         gamma=gamma,\n         name=name,\n         reduction=reduction,\n@@ -728,10 +772,14 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         label_smoothing=label_smoothing,\n         axis=axis)\n     self.from_logits = from_logits\n+    self.apply_class_balancing = apply_class_balancing\n+    self.alpha = alpha\n     self.gamma = gamma\n \n   def get_config(self):\n     config = {\n+        'apply_class_balancing': self.apply_class_balancing,\n+        'alpha': self.alpha,\n         'gamma': self.gamma,\n     }\n     base_config = super().get_config()\n@@ -2000,6 +2048,8 @@ def _ragged_tensor_binary_crossentropy(y_true,\n def binary_focal_crossentropy(\n     y_true,\n     y_pred,\n+    apply_class_balancing=False,\n+    alpha=0.25,\n     gamma=2.0,\n     from_logits=False,\n     label_smoothing=0.,\n@@ -2013,8 +2063,15 @@ def binary_focal_crossentropy(\n \n   `focal_factor = (1 - output)**gamma` for class 1\n   `focal_factor = output**gamma` for class 0\n-  where `gamma` is a focusing parameter. When `gamma` = 0, this function is\n-  equivalent to the binary crossentropy loss.\n+  where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+  effect on the binary crossentropy loss.\n+\n+  If `apply_class_balancing == True`, this function also takes into account a\n+  weight balancing factor for the binary classes 0 and 1 as follows:\n+\n+  `weight = alpha` for class 1 (`target == 1`)\n+  `weight = 1 - alpha` for class 0\n+  where `alpha` is a float in the range of `[0, 1]`.\n \n   Standalone usage:\n \n@@ -2028,6 +2085,10 @@ def binary_focal_crossentropy(\n   Args:\n     y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n     y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n+    apply_class_balancing: A bool, whether to apply weight balancing on the\n+      binary classes 0 and 1.\n+    alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n+    in the reference. The weight for class 0 is `1.0 - alpha`.\n     gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n     from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n       we assume that `y_pred` encodes a probability distribution.\n@@ -2053,6 +2114,8 @@ def binary_focal_crossentropy(\n       backend.binary_focal_crossentropy(\n           target=y_true,\n           output=y_pred,\n+          apply_class_balancing=apply_class_balancing,\n+          alpha=alpha,\n           gamma=gamma,\n           from_logits=from_logits,\n       ),\n@@ -2064,6 +2127,8 @@ def binary_focal_crossentropy(\n def _ragged_tensor_binary_focal_crossentropy(\n     y_true,\n     y_pred,\n+    apply_class_balancing=False,\n+    alpha=0.25,\n     gamma=2.0,\n     from_logits=False,\n     label_smoothing=0.,\n@@ -2082,8 +2147,12 @@ def _ragged_tensor_binary_focal_crossentropy(\n   Args:\n     y_true: Tensor of one-hot true targets.\n     y_pred: Tensor of predicted targets.\n-    gamma: A focusing parameter, default is `2.0` as mentioned in the reference\n-      [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+    apply_class_balancing: A bool, whether to apply weight balancing on the\n+      binary classes 0 and 1.\n+    alpha: A weight balancing factor for class 1, default is `0.25` as mentioned\n+      in the reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+      The weight for class 0 is `1.0 - alpha`.\n+    gamma: A focusing parameter, default is `2.0` as mentioned in the reference.\n     from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n       we assume that `y_pred` encodes a probability distribution.\n     label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels. For\n@@ -2096,6 +2165,8 @@ def _ragged_tensor_binary_focal_crossentropy(\n   \"\"\"\n   fn = functools.partial(\n       binary_focal_crossentropy,\n+      apply_class_balancing=apply_class_balancing,\n+      alpha=alpha,\n       gamma=gamma,\n       from_logits=from_logits,\n       label_smoothing=label_smoothing,\n\n@@ -14,17 +14,16 @@\n # ==============================================================================\n \"\"\"Tests for Keras loss functions.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n-\n-from tensorflow.python.autograph.impl import api as autograph\n from keras import activations\n from keras import backend\n-from keras.testing_infra import test_combinations\n from keras import losses\n+from keras.testing_infra import test_combinations\n from keras.utils import losses_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.autograph.impl import api as autograph\n \n ALL_LOSSES = [\n     losses.mean_squared_error, losses.mean_absolute_error,\n@@ -1138,6 +1137,257 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.18166, 3)\n \n \n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class BinaryWeightedFocalCrossentropyTest(tf.test.TestCase):\n+\n+  def test_config(self):\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.1,\n+        gamma=1.5,\n+        name='bfce_0',\n+    )\n+    self.assertTrue(obj.apply_class_balancing)\n+    self.assertEqual(obj.name, 'bfce_0')\n+    self.assertAlmostEqual(obj.alpha, 0.1)\n+    self.assertAlmostEqual(obj.gamma, 1.5)\n+\n+    obj_2 = losses.BinaryFocalCrossentropy.from_config(obj.get_config())\n+    self.assertTrue(obj_2.apply_class_balancing)\n+    self.assertEqual(obj_2.name, 'bfce_0')\n+    self.assertAlmostEqual(obj_2.alpha, 0.1)\n+    self.assertAlmostEqual(obj_2.gamma, 1.5)\n+\n+  def test_all_correct_unweighted(self):\n+    y_true = tf.constant([\n+        [1, 0, 0],\n+        [0, 1, 0],\n+        [0, 0, 1],\n+    ], dtype=tf.float32)\n+    obj = losses.BinaryFocalCrossentropy(apply_class_balancing=True, gamma=1.5)\n+    loss = obj(y_true, y_true)\n+    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+    # Test with logits.\n+    logits = tf.constant([\n+        [100.0, -100.0, -100.0],\n+        [-100.0, 100.0, -100.0],\n+        [-100.0, -100.0, 100.0],\n+    ])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.3,\n+        gamma=2.0,\n+        from_logits=True,\n+    )\n+    loss = obj(y_true, logits)\n+    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+  def test_unweighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.4,\n+        gamma=2.0,\n+    )\n+    loss = obj(y_true, y_pred)\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.4, 0.6], [0.4, 0.6]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n+    # weightedfocalLoss = alpha_weight focal bceLoss\n+    #                   = [[0.0004, 0.618], [0.0128, 0.0054]]\n+    # Reduced loss = (0.0004 + 0.618 + 0.0128 + 0.0054) / 4 = 0.15915\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.15915, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.3,\n+        gamma=3.0,\n+        from_logits=True,\n+    )\n+    loss = obj(y_true, logits)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.3, 0.3, 0.7], [0.7, 0.3, 0.7]]\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # bceLoss = -log(p_t)\n+    #         = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]]\n+\n+    # weightedfocalLoss = alpha_weight focal bceLoss\n+    # = [[0.00036, 0.68229, 1.7598], [0.00000014, 0.00099, 0.000000007]]\n+    # Reduced loss = 0.40724\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.40724, 3)\n+\n+  def test_scalar_weighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.6,\n+        gamma=2.0,\n+    )\n+    loss = obj(y_true, y_pred, sample_weight=1.23)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.6, 0.4], [0.6, 0.4]]\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n+    # weightedfocalLoss = alpha_weight focal bceLoss\n+    #           = [[0.0006, 0.412], [0.0192, 0.0036]] * sample_weight\n+    # Reduced loss = (0.0006 + 0.412 + 0.0192 + 0.0036) * 1.23 / 4 = 0.13388\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.13388, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.2,\n+        gamma=3.0,\n+        from_logits=True,\n+    )\n+    loss = obj(y_true, logits, sample_weight=3.21)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.2, 0.2, 0.8], [0.8, 0.2, 0.8]]\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+\n+    # weightedfocalLoss = alpha_weight * focal * bceLoss =\n+    # [[0.00024, 0.45486, 2.0112], [0.00000016, 0.00066, 0.000000008]] * 3.21\n+    # Reduced loss = 0.41116 * 3.21 = 1.32\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 1.32, 3)\n+\n+  def test_sample_weighted(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.1,\n+        gamma=2.0,\n+    )\n+    loss = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.1, 0.9], [0.1, 0.9]]\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    #         = [[0.105, 1.609] ,[0.357, 0.223]] * sample_weight\n+    # focalLoss = alpha_weight * focal * bceLoss\n+    #           = [[0.0001, 0.927], [0.0032, 0.0081]] * sample_weight\n+    #           = [[0.00012, 1.1124], [0.01088, 0.02754]]\n+    # Reduced loss = (0.00012 + 1.1124 + 0.01088 + 0.02754) / 4 = 0.2877\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.2877, 3)\n+\n+    # Test with logits.\n+    y_true = tf.constant([[1, 1, 0], [0, 1, 0]], dtype=tf.float32)\n+    logits = tf.constant([[1.5, -2.7, 2.9], [-3.8, 1.2, -4.5]])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.2,\n+        gamma=3.0,\n+        from_logits=True,\n+    )\n+    loss = obj(y_true, logits, sample_weight=sample_weight)\n+\n+    # sigmoidal = sigmoid(logits)\n+    #           = [[0.8176, 0.063, 0.9478], [0.0219, 0.7685, 0.011]]\n+    # p_t = y_true sigmoidal + (1 - y_true) (1 - sigmoidal)\n+    #     = [[0.8176, 0.063, 0.0522], [0.9781, 0.7685, 0.989]]\n+    # focal = (1 - p_t) ** gamma\n+    #       = [[0.006, 0.823, 0.851], [0.00001, 0.0124, 0.000001]]\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.2, 0.2, 0.8], [0.8, 0.2, 0.8]]\n+\n+    # bceLoss = -log(p_t) * sample_weight\n+    # = [[0.2014, 2.7646 , 2.9527], [0.0221, 0.2633, 0.01106]] * sample_weight\n+\n+    # focalLoss = alpha_weight * focal * bceLoss =\n+    # [[0.00024, 0.45486, 2.0112], [1.6e-7, 6.6e-4, 8e-9]] * sample_weight\n+    # focalLoss = [[0.000288, 0.5458, 2.41344], [5.44e-7, 2.444e-3, 2.72e-8]]\n+    # Reduced loss = 0.49366\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.49366, 3)\n+\n+  def test_no_reduction(self):\n+    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+    y_pred = np.asarray([0.9, 0.8, 0.7, 0.2], dtype=np.float32).reshape([2, 2])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.6,\n+        gamma=2.0,\n+        reduction=losses_utils.ReductionV2.NONE,\n+    )\n+    loss = obj(y_true, y_pred)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.6, 0.4], [0.6, 0.4]]\n+\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2], [0.7, 0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64], [0.09, 0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609] ,[0.357, 0.223]]\n+    # focalLoss = alpha_weight focal bceLoss\n+    #           = [[0.0006, 0.412], [0.0192, 0.0036]]\n+    # Reduced loss = [(0.0006 + 0.412) / 2, (0.0192 + 0.0036) / 2]\n+\n+    self.assertAllClose(self.evaluate(loss), (0.2063, 0.0114), 3)\n+\n+  def test_ragged_tensors(self):\n+    y_true = tf.ragged.constant([[1, 0, 1], [0]])\n+    y_pred = tf.ragged.constant([[0.9, 0.8, 0.7], [0.2]])\n+    obj = losses.BinaryFocalCrossentropy(\n+        apply_class_balancing=True,\n+        alpha=0.1,\n+        gamma=2.0,\n+    )\n+    loss = obj(y_true, y_pred)\n+\n+    # alpha_weight = alpha y_true + (1 - alpha) (1 - y_true)\n+    #              = [[0.1, 0.9, 0.1], [0.9]]\n+    # p_t = y_true y_pred + (1 - y_true) (1 - y_pred) = [[0.9, 0.2, 0.7], [0.8]]\n+    # focal = (1 - p_t) ** gamma = [[0.01, 0.64, 0.09], [0.04]]\n+\n+    # bceLoss = -log(p_t) = [[0.105, 1.609, 0.357], [0.223]]\n+    # focalLoss = alpha_weight focal bceLoss\n+    #           = [[0.0001, 0.927, 0.0032], [0.0081]]\n+    # Reduced loss = ((0.0001 + 0.927 + 0.0032) / 3 + 0.0081) / 2 = 0.1591\n+\n+    self.assertAlmostEqual(self.evaluate(loss), 0.1591, 3)\n+\n+\n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CategoricalCrossentropyTest(tf.test.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
