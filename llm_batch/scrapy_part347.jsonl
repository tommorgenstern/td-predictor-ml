{"custom_id": "scrapy#eecc035f4c2ed69cf9dbceae009c583d849b9f2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 1173 | Contributors (this commit): 31 | Commits (past 90d): 13 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,11 +9,11 @@ import logging\n import mimetypes\n import os\n import time\n-from os import PathLike\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n+from os import PathLike\n from pathlib import Path\n from typing import DefaultDict, Optional, Set, Union\n from urllib.parse import urlparse\n@@ -54,12 +54,14 @@ class FSFilesStore:\n         self._mkdir(Path(self.basedir))\n         self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n \n-    def persist_file(self, path: str, buf, info, meta=None, headers=None):\n+    def persist_file(\n+        self, path: Union[str, PathLike], buf, info, meta=None, headers=None\n+    ):\n         absolute_path = self._get_filesystem_path(path)\n         self._mkdir(absolute_path.parent, info)\n         absolute_path.write_bytes(buf.getvalue())\n \n-    def stat_file(self, path: str, info):\n+    def stat_file(self, path: Union[str, PathLike], info):\n         absolute_path = self._get_filesystem_path(path)\n         try:\n             last_modified = absolute_path.stat().st_mtime\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80a86de5071aef1ac43277c273beace6bd426e80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 81 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,11 +20,11 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n         \"DOWNLOAD_SLOTS\": {\n             'quotes.toscrape.com': {\n                 'concurrency': 1,\n-                'delay': 1.5,\n+                'delay': 2,\n                 'randomize_delay': False\n             },\n             'books.toscrape.com': {\n-                'delay': 2,\n+                'delay': 3,\n                 'randomize_delay': False\n             }\n         }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa9897282fc571bbac64eaba06433362ad233530", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 844 | Contributors (this commit): 22 | Commits (past 90d): 9 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -288,7 +288,7 @@ def deferred_from_coro(o) -> Any:\n         if not is_asyncio_reactor_installed():\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n-            return ensureDeferred(o)\n+            return ensureDeferred(cast(Coroutine[Deferred, Any, Any], o))\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n         event_loop = _get_asyncio_event_loop()\n         return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7daa2624d698a2fcf64923b4d1c51d859673006", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 6 | Churn Cumulative: 1242 | Contributors (this commit): 37 | Commits (past 90d): 6 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -198,12 +198,6 @@ def _select_value(ele: SelectElement, n: str, v: str):\n         # And for select tags without options\n         o = ele.value_options\n         return (n, o[0]) if o else (None, None)\n-    if v is not None and multiple:\n-        # This is a workaround to bug in lxml fixed 2.3.1\n-        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n-        selected_options = ele.xpath(\".//option[@selected]\")\n-        values = [(o.get(\"value\") or o.text or \"\").strip() for o in selected_options]\n-        return n, values\n     return n, v\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1717aa48c42fdeb9c45739a82b00b9cadf7b58c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 2313 | Contributors (this commit): 52 | Commits (past 90d): 11 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,10 +5,17 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union\n+from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union, cast\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n-from lxml.html import FormElement, HtmlElement, HTMLParser, SelectElement\n+from lxml.html import (\n+    FormElement,\n+    HTMLParser,\n+    InputElement,\n+    MultipleSelectOptions,\n+    SelectElement,\n+    TextareaElement,\n+)\n from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n@@ -19,7 +26,8 @@ from scrapy.utils.response import get_base_url\n \n FormRequestTypeVar = TypeVar(\"FormRequestTypeVar\", bound=\"FormRequest\")\n \n-FormdataType = Optional[Union[dict, List[Tuple[str, str]]]]\n+FormdataKVType = Tuple[str, Union[str, Iterable[str]]]\n+FormdataType = Optional[Union[dict, List[FormdataKVType]]]\n \n \n class FormRequest(Request):\n@@ -79,6 +87,7 @@ class FormRequest(Request):\n \n \n def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n+    assert form.base_url is not None  # typing\n     if url is None:\n         action = form.get(\"action\")\n         if action is None:\n@@ -87,11 +96,11 @@ def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n     return urljoin(form.base_url, url)\n \n \n-def _urlencode(seq: Iterable, enc: str) -> str:\n+def _urlencode(seq: Iterable[FormdataKVType], enc: str) -> str:\n     values = [\n         (to_bytes(k, enc), to_bytes(v, enc))\n         for k, vs in seq\n-        for v in (vs if is_listlike(vs) else [vs])\n+        for v in (cast(Iterable[str], vs) if is_listlike(vs) else [cast(str, vs)])\n     ]\n     return urlencode(values, doseq=True)\n \n@@ -147,7 +156,7 @@ def _get_inputs(\n     formdata: FormdataType,\n     dont_click: bool,\n     clickdata: Optional[dict],\n-) -> List[Tuple[str, str]]:\n+) -> List[FormdataKVType]:\n     \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n     try:\n         formdata_keys = dict(formdata or ()).keys()\n@@ -165,7 +174,7 @@ def _get_inputs(\n         '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n         namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n     )\n-    values = [\n+    values: List[FormdataKVType] = [\n         (k, \"\" if v is None else v)\n         for k, v in (_value(e) for e in inputs)\n         if k and k not in formdata_keys\n@@ -183,15 +192,19 @@ def _get_inputs(\n     return values\n \n \n-def _value(ele: HtmlElement):\n+def _value(\n+    ele: Union[InputElement, SelectElement, TextareaElement]\n+) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n     n = ele.name\n     v = ele.value\n     if ele.tag == \"select\":\n-        return _select_value(ele, n, v)\n+        return _select_value(cast(SelectElement, ele), n, v)\n     return n, v\n \n \n-def _select_value(ele: SelectElement, n: str, v: str):\n+def _select_value(\n+    ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]\n+) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n\n@@ -8,7 +8,7 @@ import sys\n import weakref\n from functools import partial, wraps\n from itertools import chain\n-from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n+from typing import Any, AsyncGenerator, AsyncIterable, Iterable, Union\n \n from scrapy.utils.asyncgen import as_async_generator\n \n@@ -45,7 +45,7 @@ def iflatten(x):\n             yield el\n \n \n-def is_listlike(x):\n+def is_listlike(x: Any) -> bool:\n     \"\"\"\n     >>> is_listlike(\"foo\")\n     False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60bf56b715e443951125a10ff91ad1699270a82c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 97 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,3 +8,12 @@ def is_botocore_available():\n         return True\n     except ImportError:\n         return False\n+\n+\n+def is_boto3_available():\n+    try:\n+        import boto3  # noqa: F401\n+\n+        return True\n+    except ImportError:\n+        return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01ad49515d31e3053949d20e923dc61a87147eab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1555 | Contributors (this commit): 37 | Commits (past 90d): 5 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -173,7 +173,27 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n+        if is_boto3_available():\n+            import boto3.session\n+            session = boto3.session.Session()\n+\n+            self.s3_client = session.client(\n+                \"s3\",\n+                aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key,\n+                aws_session_token=self.session_token,\n+                endpoint_url=self.endpoint_url,\n+            )\n+        else:\n+            warnings.warn(\n+                \"Botocore usage is deprecated for S3FeedStorage, \"\n+                \"please use boto3 to avoid problems\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n             import botocore.session\n+            session = botocore.get_session()\n \n         session = botocore.session.get_session()\n             self.s3_client = session.create_client(\n@@ -183,6 +203,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 aws_session_token=self.session_token,\n                 endpoint_url=self.endpoint_url,\n             )\n+\n         if feed_options and feed_options.get(\"overwrite\", True) is False:\n             logger.warning(\n                 \"S3 does not support appending to files. To \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1a8baa1fa0c6210e5c1351ab5a4063b8dacaa7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1557 | Contributors (this commit): 37 | Commits (past 90d): 6 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n+from scrapy.utils.boto import is_boto3_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n@@ -195,7 +196,6 @@ class S3FeedStorage(BlockingFeedStorage):\n             import botocore.session\n             session = botocore.get_session()\n \n-        session = botocore.session.get_session()\n             self.s3_client = session.create_client(\n                 \"s3\",\n                 aws_access_key_id=self.access_key,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59ba3c4e4cabd661810e0ac1b9963040d2e42f31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 1570 | Contributors (this commit): 37 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -174,7 +174,9 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n-        if is_boto3_available():\n+        self._using_boto3 = is_boto3_available()\n+\n+        if self._using_boto3:\n             import boto3.session\n             session = boto3.session.Session()\n \n@@ -187,8 +189,8 @@ class S3FeedStorage(BlockingFeedStorage):\n             )\n         else:\n             warnings.warn(\n-                \"Botocore usage is deprecated for S3FeedStorage, \"\n-                \"please use boto3 to avoid problems\",\n+                \"`botocore` usage has been deprecated for S3 feed \"\n+                \"export, please use `boto3` to avoid problems\",\n                 category=ScrapyDeprecationWarning,\n                 stacklevel=2,\n             )\n@@ -227,6 +229,11 @@ class S3FeedStorage(BlockingFeedStorage):\n     def _store_in_thread(self, file):\n         file.seek(0)\n         kwargs = {\"ACL\": self.acl} if self.acl else {}\n+        if self._using_boto3:\n+            self.s3_client.upload_fileobj(\n+                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n+            )\n+        else:\n             self.s3_client.put_object(\n                 Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f45a7d3f3c90ad27b2b13bd1ee038ce2fe9c02f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1571 | Contributors (this commit): 37 | Commits (past 90d): 8 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -192,7 +192,6 @@ class S3FeedStorage(BlockingFeedStorage):\n                 \"`botocore` usage has been deprecated for S3 feed \"\n                 \"export, please use `boto3` to avoid problems\",\n                 category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n             )\n \n             import botocore.session\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98a5958687b4fae213bf053e560bd18a69312f3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 939 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -102,13 +102,13 @@ class Downloader:\n         key = self._get_slot_key(request, spider)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n-            conc = slot_settings.get(\n-                'concurrency', (\n-                    self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n-                )\n-            )\n+            conc = (\n+                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency)\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n-            delay = slot_settings.get('delay', delay)\n+            conc, delay = (\n+                slot_settings.get('concurrency', conc),\n+                slot_settings.get('delay', delay)\n+            )\n             randomize_delay = slot_settings.get('randomize_delay', self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb0cca471d20f00a1cc0980b84fe6266772faaa3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1576 | Contributors (this commit): 38 | Commits (past 90d): 9 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -21,8 +21,7 @@ from zope.interface import Interface, implementer\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n-from scrapy.utils.boto import is_botocore_available\n-from scrapy.utils.boto import is_boto3_available\n+from scrapy.utils.boto import is_boto3_available, is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n@@ -178,6 +177,7 @@ class S3FeedStorage(BlockingFeedStorage):\n \n         if self._using_boto3:\n             import boto3.session\n+\n             session = boto3.session.Session()\n \n             self.s3_client = session.client(\n@@ -195,6 +195,7 @@ class S3FeedStorage(BlockingFeedStorage):\n             )\n \n             import botocore.session\n+\n             session = botocore.get_session()\n \n             self.s3_client = session.create_client(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#045092e8d753b9689b24db7d30bcbd7f976fb517", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 14 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 1066 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,13 +103,14 @@ class Downloader:\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n             conc = (\n-                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency)\n+                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            )\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n             conc, delay = (\n-                slot_settings.get('concurrency', conc),\n-                slot_settings.get('delay', delay)\n+                slot_settings.get(\"concurrency\", conc),\n+                slot_settings.get(\"delay\", delay),\n             )\n-            randomize_delay = slot_settings.get('randomize_delay', self.randomize_delay)\n+            randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n \n\n@@ -1,31 +1,31 @@\n import time\n \n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n from scrapy.crawler import CrawlerRunner\n from scrapy.http import Request\n \n from tests.mockserver import MockServer\n from tests.spiders import MetaSpider\n \n-from twisted.internet import defer\n-from twisted.trial.unittest import TestCase\n-\n \n class DownloaderSlotsSettingsTestSpider(MetaSpider):\n \n-    name = 'downloader_slots'\n+    name = \"downloader_slots\"\n \n     custom_settings = {\n         \"DOWNLOAD_DELAY\": 1,\n         \"RANDOMIZE_DOWNLOAD_DELAY\": False,\n         \"DOWNLOAD_SLOTS\": {\n-            'quotes.toscrape.com': {\n-                'concurrency': 1,\n-                'delay': 2,\n-                'randomize_delay': False\n+            \"quotes.toscrape.com\": {\n+                \"concurrency\": 1,\n+                \"delay\": 2,\n+                \"randomize_delay\": False\n             },\n-            'books.toscrape.com': {\n-                'delay': 3,\n-                'randomize_delay': False\n+            \"books.toscrape.com\": {\n+                \"delay\": 3,\n+                \"randomize_delay\": False\n             }\n         }\n     }\n@@ -33,21 +33,21 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n     def start_requests(self):\n         self.times = {None: []}\n \n-        slots = list(self.custom_settings.get('DOWNLOAD_SLOTS', {}).keys()) + [None]\n+        slots = list(self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}).keys()) + [None]\n \n         for slot in slots:\n             url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n             self.times[slot] = []\n-            yield Request(url, callback=self.parse, meta={'download_slot': slot})\n+            yield Request(url, callback=self.parse, meta={\"download_slot\": slot})\n \n     def parse(self, response):\n-        slot = response.meta.get('download_slot', None)\n+        slot = response.meta.get(\"download_slot\", None)\n         self.times[slot].append(time.time())\n         url = self.mockserver.url(f\"/?downloader_slot={slot}&req=2\")\n-        yield Request(url, callback=self.not_parse, meta={'download_slot': slot})\n+        yield Request(url, callback=self.not_parse, meta={\"download_slot\": slot})\n \n     def not_parse(self, response):\n-        slot = response.meta.get('download_slot', None)\n+        slot = response.meta.get(\"download_slot\", None)\n         self.times[slot].append(time.time())\n \n \n@@ -70,6 +70,9 @@ class CrawlTestCase(TestCase):\n         tolerance = 0.3\n \n         delays_real = {k: v[1] - v[0] for k, v in times.items()}\n-        error_delta = {k: 1 - min(delays_real[k], v.delay) / max(delays_real[k], v.delay)  for k, v in slots.items()}\n+        error_delta = {\n+            k: 1 - min(delays_real[k], v.delay) / max(delays_real[k], v.delay)\n+            for k, v in slots.items()\n+        }\n \n         self.assertTrue(max(list(error_delta.values())) < tolerance)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd5524eb986c0ef1d6ce6ec2887ed8aabe55f7c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 127 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,13 +21,10 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n             \"quotes.toscrape.com\": {\n                 \"concurrency\": 1,\n                 \"delay\": 2,\n-                \"randomize_delay\": False\n+                \"randomize_delay\": False,\n+            },\n+            \"books.toscrape.com\": {\"delay\": 3, \"randomize_delay\": False},\n         },\n-            \"books.toscrape.com\": {\n-                \"delay\": 3,\n-                \"randomize_delay\": False\n-            }\n-        }\n     }\n \n     def start_requests(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#864eee66c77ceff21ba0c8611aa84402f300fc27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 130 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -5,13 +5,11 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.crawler import CrawlerRunner\n from scrapy.http import Request\n-\n from tests.mockserver import MockServer\n from tests.spiders import MetaSpider\n \n \n class DownloaderSlotsSettingsTestSpider(MetaSpider):\n-\n     name = \"downloader_slots\"\n \n     custom_settings = {\n@@ -49,7 +47,6 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n \n \n class CrawlTestCase(TestCase):\n-\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45b9dbae40de00fc7d4498e871538c074c64be8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1578 | Contributors (this commit): 38 | Commits (past 90d): 10 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -196,7 +196,7 @@ class S3FeedStorage(BlockingFeedStorage):\n \n             import botocore.session\n \n-            session = botocore.get_session()\n+            session = botocore.session.get_session()\n \n             self.s3_client = session.create_client(\n                 \"s3\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#733309affa0c01239841899a790ec4734ea173dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1280 | Contributors (this commit): 37 | Commits (past 90d): 8 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,7 +58,7 @@ class FormRequest(Request):\n         response: TextResponse,\n         formname: Optional[str] = None,\n         formid: Optional[str] = None,\n-        formnumber: Optional[int] = 0,\n+        formnumber: int = 0,\n         formdata: FormdataType = None,\n         clickdata: Optional[dict] = None,\n         dont_click: bool = False,\n@@ -109,7 +109,7 @@ def _get_form(\n     response: TextResponse,\n     formname: Optional[str],\n     formid: Optional[str],\n-    formnumber: Optional[int],\n+    formnumber: int,\n     formxpath: Optional[str],\n ) -> FormElement:\n     \"\"\"Find the wanted form element within the given response.\"\"\"\n@@ -142,7 +142,6 @@ def _get_form(\n         raise ValueError(f\"No <form> element found with {formxpath}\")\n \n     # If we get here, it means that either formname was None or invalid\n-    if formnumber is not None:\n     try:\n         form = forms[formnumber]\n     except IndexError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d20f2788822eabc513a4c55a341cfe65b6bce37d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 467 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ def get_meta_refresh(\n     \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n-        # a w3lib typing bug here\n+        # a w3lib typing bug here, fixed in https://github.com/scrapy/w3lib/pull/211\n         _metaref_cache[response] = html.get_meta_refresh(  # type: ignore[assignment]\n             text, response.url, response.encoding, ignore_tags=ignore_tags\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3dd9d71c328fea0f4b2ab4bc668e65ecd1eb1d8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 353 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,12 @@ from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n from twisted.internet.endpoints import HostnameEndpoint\n from twisted.python.failure import Failure\n-from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _StandardEndpointFactory\n+from twisted.web.client import (\n+    URI,\n+    BrowserLikePolicyForHTTPS,\n+    ResponseFailed,\n+    _StandardEndpointFactory,\n+)\n from twisted.web.error import SchemeNotSupported\n \n from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\n@@ -83,8 +88,7 @@ class H2ConnectionPool:\n         pending_requests = self._pending_requests.pop(key, None)\n         while pending_requests:\n             d = pending_requests.popleft()\n-            # TODO: this is incorrect, errback takes a single exception\n-            d.errback(errors)  # type: ignore[arg-type]\n+            d.errback(ResponseFailed(errors))\n \n     def close_connections(self) -> None:\n         \"\"\"Close all the HTTP/2 connections and remove them from pool\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68ba25cb69cb70c0144c9473c1ada46051c7af5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 256 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with Path(\"tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n+with Path(r\"C:/Users/georg/PycharmProjects/scrapy/tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n     for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != \"#\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be52fe4f67647cea9670197feebbcdb05af6cf96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 258 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with Path(r\"C:/Users/georg/PycharmProjects/scrapy/tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n+with Path(\"tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n     for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != \"#\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8aca47e25dd406a39efebc28019db0226b6ec9aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 20 | Churn Cumulative: 2120 | Contributors (this commit): 24 | Commits (past 90d): 5 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -293,6 +293,13 @@ class BaseSettings(MutableMapping):\n         else:\n             self.attributes[name].set(value, priority)\n \n+    def setdefault(self, name, default=None, priority=\"project\"):\n+        if name not in self:\n+            self.set(name, default, priority)\n+            return default\n+\n+        return self.attributes[name].value\n+\n     def setdict(self, values, priority=\"project\"):\n         self.update(values, priority)\n \n\n@@ -65,6 +65,19 @@ class BaseSettingsTest(unittest.TestCase):\n     def setUp(self):\n         self.settings = BaseSettings()\n \n+    def test_setdefault_not_existing_value(self):\n+        settings = BaseSettings()\n+        value = settings.setdefault(\"TEST_OPTION\", \"value\")\n+        self.assertEqual(settings[\"TEST_OPTION\"], \"value\")\n+        self.assertEqual(value, \"value\")\n+        self.assertIsNotNone(value)\n+\n+    def test_setdefault_existing_value(self):\n+        settings = BaseSettings({\"TEST_OPTION\": \"value\"})\n+        value = settings.setdefault(\"TEST_OPTION\", None)\n+        self.assertEqual(settings[\"TEST_OPTION\"], \"value\")\n+        self.assertEqual(value, \"value\")\n+\n     def test_set_new_attribute(self):\n         self.settings.set(\"TEST_OPTION\", \"value\", 0)\n         self.assertIn(\"TEST_OPTION\", self.settings.attributes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d311779887d5c8a34c8062eadd3155cb0ef4dc72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 51 | Files Changed: 3 | Hunks: 27 | Methods Changed: 13 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 119 | Churn Cumulative: 9520 | Contributors (this commit): 84 | Commits (past 90d): 21 | Contributors (cumulative): 137 | DMM Complexity: 0.8421052631578947\n\nDIFF:\n@@ -274,8 +274,6 @@ class FTPFeedStorage(BlockingFeedStorage):\n class _FeedSlot:\n     def __init__(\n         self,\n-        file,\n-        exporter,\n         storage,\n         uri,\n         format,\n@@ -283,9 +281,14 @@ class _FeedSlot:\n         batch_id,\n         uri_template,\n         filter,\n+        feed_options,\n+        spider,\n+        exporters,\n+        settings,\n+        crawler,\n     ):\n-        self.file = file\n-        self.exporter = exporter\n+        self.file = None\n+        self.exporter = None\n         self.storage = storage\n         # feed params\n         self.batch_id = batch_id\n@@ -294,15 +297,44 @@ class _FeedSlot:\n         self.uri_template = uri_template\n         self.uri = uri\n         self.filter = filter\n+        # exporter params\n+        self.feed_options = feed_options\n+        self.spider = spider\n+        self.exporters = exporters\n+        self.settings = settings\n+        self.crawler = crawler\n         # flags\n         self.itemcount = 0\n         self._exporting = False\n+        self._fileloaded = False\n \n     def start_exporting(self):\n+        if not self._fileloaded:\n+            self.file = self.storage.open(self.spider)\n+            if \"postprocessing\" in self.feed_options:\n+                self.file = PostProcessingManager(\n+                    self.feed_options[\"postprocessing\"], self.file, self.feed_options\n+                )\n+            self.exporter = self._get_exporter(\n+                file=self.file,\n+                format=self.feed_options[\"format\"],\n+                fields_to_export=self.feed_options[\"fields\"],\n+                encoding=self.feed_options[\"encoding\"],\n+                indent=self.feed_options[\"indent\"],\n+                **self.feed_options[\"item_export_kwargs\"],\n+            )\n+            self._fileloaded = True\n+\n         if not self._exporting:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n+    def _get_instance(self, objcls, *args, **kwargs):\n+        return create_instance(objcls, self.settings, self.crawler, *args, **kwargs)\n+\n+    def _get_exporter(self, file, format, *args, **kwargs):\n+        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+\n     def finish_exporting(self):\n         if self._exporting:\n             self.exporter.finish_exporting()\n@@ -379,15 +411,22 @@ class FeedExporter:\n         deferred_list = []\n         for slot in self.slots:\n             d = self._close_slot(slot, spider)\n+            if d:\n                 deferred_list.append(d)\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+        if slot.itemcount:\n+            # Nomal case\n             slot.finish_exporting()\n-        if not slot.itemcount and not slot.store_empty:\n-            # We need to call slot.storage.store nonetheless to get the file\n-            # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, slot.file)\n+        elif slot.store_empty and slot.batch_id == 1:\n+            # Need Store Empty\n+            slot.start_exporting()\n+            slot.finish_exporting()\n+        else:\n+            # In this case, the file is not stored, so no processing is required.\n+            return None\n+\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n \n@@ -423,23 +462,7 @@ class FeedExporter:\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n-        file = storage.open(spider)\n-        if \"postprocessing\" in feed_options:\n-            file = PostProcessingManager(\n-                feed_options[\"postprocessing\"], file, feed_options\n-            )\n-\n-        exporter = self._get_exporter(\n-            file=file,\n-            format=feed_options[\"format\"],\n-            fields_to_export=feed_options[\"fields\"],\n-            encoding=feed_options[\"encoding\"],\n-            indent=feed_options[\"indent\"],\n-            **feed_options[\"item_export_kwargs\"],\n-        )\n         slot = _FeedSlot(\n-            file=file,\n-            exporter=exporter,\n             storage=storage,\n             uri=uri,\n             format=feed_options[\"format\"],\n@@ -447,9 +470,12 @@ class FeedExporter:\n             batch_id=batch_id,\n             uri_template=uri_template,\n             filter=self.filters[uri_template],\n+            feed_options=feed_options,\n+            spider=spider,\n+            exporters=self.exporters,\n+            settings=self.settings,\n+            crawler=getattr(self, \"crawler\", None),\n         )\n-        if slot.store_empty:\n-            slot.start_exporting()\n         return slot\n \n     def item_scraped(self, item, spider):\n@@ -533,14 +559,6 @@ class FeedExporter:\n         else:\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n \n-    def _get_instance(self, objcls, *args, **kwargs):\n-        return create_instance(\n-            objcls, self.settings, getattr(self, \"crawler\", None), *args, **kwargs\n-        )\n-\n-    def _get_exporter(self, file, format, *args, **kwargs):\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n-\n     def _get_storage(self, uri, feed_options):\n         \"\"\"Fork of create_instance specific to feed storage classes\n \n\n@@ -141,7 +141,7 @@ EXTENSIONS_BASE = {\n FEED_TEMPDIR = None\n FEEDS = {}\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n-FEED_STORE_EMPTY = False\n+FEED_STORE_EMPTY = True\n FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n\n@@ -725,10 +725,9 @@ class FeedExportTest(FeedExportTestBase):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[feed_options[\"format\"]] = Path(file_path).read_bytes()\n+                content[feed_options[\"format\"]] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -945,9 +944,10 @@ class FeedExportTest(FeedExportTestBase):\n                 \"FEEDS\": {\n                     self._random_temp_filename(): {\"format\": fmt},\n                 },\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(b\"\", data[fmt])\n+            self.assertEqual(None, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_start_finish_exporting_items(self):\n@@ -1057,7 +1057,6 @@ class FeedExportTest(FeedExportTestBase):\n                 self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n             \"FEED_STORAGES\": {\"file\": LogOnStoreFileStorage},\n-            \"FEED_STORE_EMPTY\": False,\n         }\n \n         with LogCapture() as log:\n@@ -1680,10 +1679,9 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[str(file_path)] = Path(file_path).read_bytes()\n+                content[str(file_path)] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -2184,6 +2182,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n             for path, feed in FEEDS.items():\n                 dir_name = Path(path).parent\n+                if not dir_name.exists():\n+                    content[feed[\"format\"]] = []\n+                    continue\n                 for file in sorted(dir_name.iterdir()):\n                     content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n@@ -2367,10 +2368,11 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     / self._file_mark: {\"format\": fmt},\n                 },\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n-            self.assertEqual(b\"\", data[fmt][0])\n+            self.assertEqual(0, len(data[fmt]))\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -2484,9 +2486,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n-    @pytest.mark.skipif(\n-        sys.platform == \"win32\", reason=\"Odd behaviour on file creation/output\"\n-    )\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n@@ -2508,7 +2507,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         data = yield self.exported_data(items, settings)\n-        self.assertEqual(len(items), len([_ for _ in data[\"json\"] if _]))\n+        self.assertEqual(len(items), len(data[\"json\"]))\n \n     @defer.inlineCallbacks\n     def test_stats_batch_file_success(self):\n@@ -2595,7 +2594,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             crawler = get_crawler(TestSpider, settings)\n             yield crawler.crawl()\n \n-        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items) + 1)\n+        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items))\n         for stub in CustomS3FeedStorage.stubs[:-1]:\n             stub.assert_no_pending_responses()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#afafc2781af71e0f32c79ede44be3155709dbd36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 130 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 138 | Churn Cumulative: 1709 | Contributors (this commit): 28 | Commits (past 90d): 4 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import inspect\n import json\n import logging\n from typing import Dict\n@@ -10,7 +11,11 @@ from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n from scrapy.http import Request\n from scrapy.utils import display\n-from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n+from scrapy.utils.asyncgen import collect_asyncgen\n+from scrapy.utils.defer import aiter_errback, deferred_from_coro\n+from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.spider import spidercls_for_request\n \n logger = logging.getLogger(__name__)\n \n@@ -108,6 +113,25 @@ class Command(BaseRunSpiderCommand):\n             max_requests = max(self.requests)\n         return max(max_items, max_requests)\n \n+    def handle_exception(self, _failure):\n+        logger.error(\n+            \"An error is caught while iterating the async iterable\",\n+            exc_info=failure_to_exc_info(_failure),\n+        )\n+\n+    def iterate_spider_output(self, result):\n+        if inspect.isasyncgen(result):\n+            d = deferred_from_coro(\n+                collect_asyncgen(aiter_errback(result, self.handle_exception))\n+            )\n+            d.addCallback(self.iterate_spider_output)\n+            return d\n+        if inspect.iscoroutine(result):\n+            d = deferred_from_coro(result)\n+            d.addCallback(self.iterate_spider_output)\n+            return d\n+        return arg_to_iter(deferred_from_coro(result))\n+\n     def add_items(self, lvl, new_items):\n         old_items = self.items.get(lvl, [])\n         self.items[lvl] = old_items + new_items\n@@ -165,7 +189,7 @@ class Command(BaseRunSpiderCommand):\n \n     def run_callback(self, response, callback, cb_kwargs=None):\n         cb_kwargs = cb_kwargs or {}\n-        d = maybeDeferred(iterate_spider_output, callback(response, **cb_kwargs))\n+        d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n         return d\n \n     def get_callback_from_rules(self, spider, response):\n\n@@ -30,14 +30,53 @@ import scrapy\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import CrawlSpider, Rule\n from scrapy.utils.test import get_from_asyncio_queue\n+import asyncio\n \n-class AsyncDefAsyncioSpider(scrapy.Spider):\n \n-    name = 'asyncdef{self.spider_name}'\n+class AsyncDefAsyncioReturnSpider(scrapy.Spider):\n+    name = \"asyncdef_asyncio_return\"\n \n     async def parse(self, response):\n+        await asyncio.sleep(0.2)\n         status = await get_from_asyncio_queue(response.status)\n-        return [scrapy.Item(), dict(foo='bar')]\n+        self.logger.info(f\"Got response {{status}}\")\n+        return [{{'id': 1}}, {{'id': 2}}]\n+\n+class AsyncDefAsyncioReturnSingleElementSpider(scrapy.Spider):\n+    name = \"asyncdef_asyncio_return_single_element\"\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.1)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.info(f\"Got response {{status}}\")\n+        return {{'foo': 42}}\n+\n+class AsyncDefAsyncioGenLoopSpider(scrapy.Spider):\n+    name = \"asyncdef_asyncio_gen_loop\"\n+\n+    async def parse(self, response):\n+        for i in range(10):\n+            await asyncio.sleep(0.1)\n+            yield {{'foo': i}}\n+        self.logger.info(f\"Got response {{response.status}}\")\n+\n+class AsyncDefAsyncioSpider(scrapy.Spider):\n+    name = \"asyncdef_asyncio\"\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.debug(f\"Got response {{status}}\")\n+\n+class AsyncDefAsyncioGenExcSpider(scrapy.Spider):\n+    name = \"asyncdef_asyncio_gen_exc\"\n+\n+    async def parse(self, response):\n+        for i in range(10):\n+            await asyncio.sleep(0.1)\n+            yield {{'foo': i}}\n+            if i > 5:\n+                raise ValueError(\"Stopping the processing\")\n \n class MySpider(scrapy.Spider):\n     name = '{self.spider_name}'\n@@ -213,17 +252,76 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n-    def test_asyncio_parse_items(self):\n+    def test_async_def_asyncio_parse_items_list(self):\n         status, out, stderr = yield self.execute(\n             [\n                 \"--spider\",\n-                \"asyncdef\" + self.spider_name,\n+                \"asyncdef_asyncio_return\",\n                 \"-c\",\n                 \"parse\",\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n+        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n+        self.assertIn(\"{'id': 1}\", _textmode(out))\n+        self.assertIn(\"{'id': 2}\", _textmode(out))\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse_items_single_element(self):\n+        status, out, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                \"asyncdef_asyncio_return_single_element\",\n+                \"-c\",\n+                \"parse\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n+        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n+        self.assertIn(\"{'foo': 42}\", _textmode(out))\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse_loop(self):\n+        status, out, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                \"asyncdef_asyncio_gen_loop\",\n+                \"-c\",\n+                \"parse\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n+        self.assertIn(\"INFO: Got response 200\", _textmode(stderr))\n+        for i in range(10):\n+            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse_exc(self):\n+        status, out, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                \"asyncdef_asyncio_gen_exc\",\n+                \"-c\",\n+                \"parse\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n+        self.assertIn(\"ValueError\", _textmode(stderr))\n+        for i in range(7):\n+            self.assertIn(f\"{{'foo': {i}}}\", _textmode(out))\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse(self):\n+        _, _, stderr = yield self.execute(\n+            [\n+                \"--spider\",\n+                \"asyncdef_asyncio\",\n+                \"-c\",\n+                \"parse\",\n+                self.url(\"/html\"),\n+            ]\n+        )\n+        self.assertIn(\"DEBUG: Got response 200\", _textmode(stderr))\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
