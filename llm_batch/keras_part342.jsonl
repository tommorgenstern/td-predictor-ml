{"custom_id": "keras#8351b87a132d5f4f072d95c0bf289082f3958baa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 536 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -271,16 +271,26 @@ class HashingTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   @parameterized.named_parameters(\n-      ('list_input', [1, 2, 3], [1, 1, 1]),\n-      ('list_input_2d', [[1], [2], [3]], [[1], [1], [1]]),\n-      # TODO(b/191397026): Reenable after fix.\n-      # ('tf_constant_2d', [\n-      #     tf.constant(x) for x in [['aa', 'bb'], ['bb', 'cc'], ['cc', 'dd']]\n-      # ], [[0, 0], [0, 0], [0, 0]]),\n-      # ('tf_constant_3d', [\n-      #     tf.constant(x)\n-      #     for x in [[['aa'], ['bb']], [['bb'], ['cc']], [['cc'], ['dd']]]\n-      # ], [[[0], [0]], [[0], [0]], [[0], [0]]]),\n+      (\n+          'list_input',\n+          [1, 2, 3],\n+          [1, 1, 1],\n+      ),\n+      (\n+          'list_input_2d',\n+          [[1], [2], [3]],\n+          [[1], [1], [1]],\n+      ),\n+      (\n+          'list_input_2d_multiple',\n+          [[1, 2], [2, 3], [3, 4]],\n+          [[1, 1], [1, 1], [1, 1]],\n+      ),\n+      (\n+          'list_input_3d',\n+          [[[1], [2]], [[2], [3]], [[3], [4]]],\n+          [[[1], [1]], [[1], [1]], [[1], [1]]],\n+      ),\n   )\n   def test_hash_list_input(self, input_data, expected):\n     layer = hashing.Hashing(num_bins=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4244fba008d686f644b2ea3711e1897edf32432b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 9102 | Contributors (this commit): 21 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -53,6 +53,30 @@ from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n \n+_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n+\n+For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n+different.\n+\n+Args:\n+  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n+    shape = `[batch_size, d0, .. dN-1, 1]`.\n+  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n+  sample_weight: Optional `sample_weight` acts as a\n+    coefficient for the metric. If a scalar is provided, then the metric is\n+    simply scaled by the given value. If `sample_weight` is a tensor of size\n+    `[batch_size]`, then the metric for each sample of the batch is rescaled\n+    by the corresponding element in the `sample_weight` vector. If the shape\n+    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n+    to this shape), then each metric element of `y_pred` is scaled by the\n+    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n+    functions reduce by 1 dimension, usually the last axis (-1)).\n+\n+Returns:\n+  Update op.\n+\"\"\"\n+\n+\n @keras_export('keras.metrics.Metric')\n class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n   \"\"\"Encapsulates metric logic and state.\n@@ -871,6 +895,9 @@ class SparseCategoricalAccuracy(MeanMetricWrapper):\n         sparse_categorical_accuracy, name, dtype=dtype)\n \n \n+SparseCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+\n+\n @keras_export('keras.metrics.TopKCategoricalAccuracy')\n class TopKCategoricalAccuracy(MeanMetricWrapper):\n   \"\"\"Computes how often targets are in the top `K` predictions.\n@@ -948,6 +975,9 @@ class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n         sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n \n \n+SparseTopKCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+\n+\n class _ConfusionMatrixConditionCount(Metric):\n   \"\"\"Calculates the number of the given confusion matrix condition.\n \n@@ -3334,6 +3364,9 @@ class SparseCategoricalCrossentropy(MeanMetricWrapper):\n         axis=axis)\n \n \n+SparseCategoricalCrossentropy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+\n+\n class SumOverBatchSize(Reduce):\n   \"\"\"Computes the weighted sum over batch size of the given values.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72c161e3ca71d7c502de8e59f6c5aaa30807cf46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 784 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -218,7 +218,7 @@ def image_dataset_from_directory(directory,\n   if shuffle:\n     # Shuffle locally at each iteration\n     dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.batch(batch_size)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n   # Include file paths for images as attribute.\n\n@@ -165,7 +165,7 @@ def text_dataset_from_directory(directory,\n   if shuffle:\n     # Shuffle locally at each iteration\n     dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.batch(batch_size)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n   return dataset\n\n@@ -218,7 +218,7 @@ def timeseries_dataset_from_array(\n   if shuffle:\n     # Shuffle locally at each iteration\n     dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.batch(batch_size)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n   return dataset\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88cabd5cba016580d1137e856fe276620525e6fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 385 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -348,7 +348,7 @@ def model_to_estimator_v2(keras_model=None,\n   \"\"\"\n \n   try:\n-    from tensorflow_estimator.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top\n+    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top\n   except ImportError:\n     raise NotImplementedError(\n         'tf.keras.estimator.model_to_estimator function not available in your '\n@@ -364,4 +364,4 @@ def model_to_estimator_v2(keras_model=None,\n       use_v2_estimator=True,\n       metric_names_map=metric_names_map,\n       export_outputs=export_outputs)\n-# LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)\n+# LINT.ThenChange(//tensorflow_estimator/python/estimator/keras_lib.py)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e24363d8fa26b82e0d9853706bc6d9c2a5955c5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 371 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -17,6 +17,7 @@\n \n import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n _PRINT_EVAL_STEP_EVERY_SEC = 60.0\n _ITERATIONS_UNINITIALIZED = -1\n@@ -42,6 +43,7 @@ def list_checkpoint_attributes(ckpt_dir_or_file):\n   return {name.split('/')[0] for name in variable_map.keys()}\n \n \n+@keras_export('keras.experimental.SidecarEvaluator', v1=[])\n class SidecarEvaluator(object):\n   \"\"\"A class designed for a dedicated evaluator task.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#48141423b186e6f33357a3fc8fafc5c3034b3890", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 4198 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -282,9 +282,14 @@ class RandomCrop(base_layer.Layer):\n       outputs = tf.slice(resized_inputs, bbox_begin, bbox_size)\n       return outputs\n \n+    input_shape = inputs.shape.as_list()\n+\n+    if self.height > input_shape[H_AXIS] or self.width > input_shape[W_AXIS]:\n+      output = resize_and_center_cropped_inputs()\n+    else:\n       output = control_flow_util.smart_cond(training, random_cropped_inputs,\n                                             resize_and_center_cropped_inputs)\n-    input_shape = inputs.shape.as_list()\n+\n     if unbatched:\n       output_shape = [self.height, self.width, input_shape[-1]]\n     else:\n\n@@ -299,16 +299,6 @@ class RandomCropTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n-  @parameterized.named_parameters(('random_crop_5_by_12', 5, 12),\n-                                  ('random_crop_10_by_8', 10, 8),\n-                                  ('random_crop_10_by_12', 10, 12))\n-  def test_invalid_random_crop(self, expected_height, expected_width):\n-    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n-    # is enabled.\n-    with self.assertRaises((tf.errors.InvalidArgumentError, tf.errors.InternalError)):\n-      with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n-        self._run_test(expected_height, expected_width)\n-\n   def test_training_with_mock(self):\n     np.random.seed(1337)\n     height, width = 3, 4\n@@ -389,6 +379,16 @@ class RandomCropTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=1)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n+  def test_input_smaller_than_crop_box(self):\n+    np.random.seed(1337)\n+    height, width = 10, 8\n+    inp = np.random.random((12, 3, 3, 3))\n+    with testing_utils.use_gpu():\n+      layer = image_preprocessing.RandomCrop(height, width)\n+      actual_output = layer(inp)\n+      expected_output_shape = (12, 10, 8, 3)\n+      self.assertEqual(expected_output_shape, actual_output.shape)\n+\n \n class RescalingTest(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16608a98ec2cf02476393f221c86383f10b518ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 387 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -160,7 +160,7 @@ def model_to_estimator(\n   \"\"\"\n \n   try:\n-    from tensorflow_estimator.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top\n+    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top\n   except ImportError:\n     raise NotImplementedError(\n         'tf.keras.estimator.model_to_estimator function not available in your '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73967df0f4dbd3d43d8d8d958fd0d016251d7059", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 1606 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -628,9 +628,7 @@ def update_confusion_matrix_variables(variables_to_update,\n     y_true = y_true[..., class_id]\n     y_pred = y_pred[..., class_id]\n \n-  if thresholds_distributed_evenly and tf.compat.forward_compatible(2021, 6, 8):\n-    # The new approach will take effect after 2021/6/8, to give enough time\n-    # for Brella release to pick up the new op tf.math.cumsum with float32.\n+  if thresholds_distributed_evenly:\n     return _update_confusion_matrix_variables_optimized(\n         variables_to_update, y_true, y_pred, thresholds,\n         multi_label=multi_label, sample_weights=sample_weight,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f224f33778e153ceda864a258939d53781b44b2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 45 | Churn Cumulative: 6553 | Contributors (this commit): 4 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -217,7 +217,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       self._value_dtype = tf.as_dtype(self.dtype)\n       mask_key = 0\n       mask_value = mask_token\n-      default_value = self.oov_token\n+      self._default_value = self.oov_token\n     else:\n       self._key_dtype = tf.as_dtype(self.dtype)\n       self._value_dtype = tf.int64\n@@ -228,18 +228,17 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       if self.num_oov_indices == 0:\n         # If there are no OOV indices, we map OOV tokens to -1 and error out\n         # during call if we find a negative index.\n-        default_value = -1\n+        self._default_value = -1\n       elif self.num_oov_indices == 1:\n         # If there is only one OOV index, we can set that index as the default\n         # value of the index_lookup table.\n-        default_value = self._oov_start_index()\n+        self._default_value = self._oov_start_index()\n       else:\n         # If we hav multiple OOV values, we need to do a further hashing step;\n         # to make this easier, we set the OOV value to -1. (This lets us do a\n         # vectorized add and cast to boolean to determine locations where we\n         # need to do extra hashing.)\n-        default_value = -1\n-    self._default_value = tf.convert_to_tensor(default_value, self._value_dtype)\n+        self._default_value = -1\n     if self.mask_token is not None:\n       self._mask_key = tf.convert_to_tensor(mask_key, self._key_dtype)\n       self._mask_value = tf.convert_to_tensor(mask_value, self._value_dtype)\n@@ -704,11 +703,12 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"Encode the lookup result to the final output depending on output_mode.\"\"\"\n \n   def _uninitialized_lookup_table(self):\n-    initializer = NullInitializer(self._key_dtype, self._value_dtype)\n     with tf.init_scope():\n+      initializer = NullInitializer(self._key_dtype, self._value_dtype)\n       return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n   def _lookup_table_from_tokens(self, tokens):\n+    with tf.init_scope():\n       token_start = self._token_start_index()\n       token_end = token_start + tf.size(tokens)\n       indices = tf.range(token_start, token_end, dtype=tf.int64)\n@@ -716,7 +716,6 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       initializer = tf.lookup.KeyValueTensorInitializer(keys, values,\n                                                         self._key_dtype,\n                                                         self._value_dtype)\n-    with tf.init_scope():\n       return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n   def _lookup_table_from_file(self, filename):\n@@ -726,6 +725,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     else:\n       key_index = tf.lookup.TextFileIndex.WHOLE_LINE\n       value_index = tf.lookup.TextFileIndex.LINE_NUMBER\n+    with tf.init_scope():\n       initializer = tf.lookup.TextFileInitializer(\n           filename=filename,\n           key_dtype=self._key_dtype,\n@@ -733,7 +733,6 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n           value_dtype=self._value_dtype,\n           value_index=value_index,\n           value_index_offset=self._token_start_index())\n-    with tf.init_scope():\n       return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n   def _standardize_inputs(self, inputs, dtype):\n\n@@ -1448,6 +1448,36 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_dataset_map_output(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=None,\n+        num_oov_indices=0,\n+        mask_token=None,\n+        oov_token=\"[OOV]\",\n+        vocabulary=vocab_data,\n+        dtype=tf.string)\n+    ds = tf.data.Dataset.from_tensor_slices([[\"earth\"], [\"wind\"], [\"and\"]])\n+    ds = ds.map(layer)\n+    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])\n+\n+  def test_dataset_map_output_layer_created_in_function(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+\n+    def apply_lookup(data):\n+      layer = index_lookup.IndexLookup(\n+          max_tokens=None,\n+          num_oov_indices=0,\n+          mask_token=None,\n+          oov_token=\"[OOV]\",\n+          vocabulary=vocab_data,\n+          dtype=tf.string)\n+      return layer(data)\n+\n+    ds = tf.data.Dataset.from_tensor_slices([[\"earth\"], [\"wind\"], [\"and\"]])\n+    ds = ds.map(apply_lookup)\n+    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1abd9a1044dc5643a54a8a228de92852e4e488dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 92 | Files Changed: 6 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 101 | Churn Cumulative: 7634 | Contributors (this commit): 5 | Commits (past 90d): 63 | Contributors (cumulative): 24 | DMM Complexity: 0.06\n\nDIFF:\n@@ -50,16 +50,11 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n \n   The `PreprocessingLayer` class is the base class you would subclass to\n   implement your own preprocessing layers.\n-\n-  Attributes:\n-    streaming: Whether a layer can be adapted multiple times without resetting\n-      the state of the layer.\n   \"\"\"\n   _must_restore_from_config = True\n \n-  def __init__(self, streaming=True, **kwargs):\n+  def __init__(self, **kwargs):\n     super(PreprocessingLayer, self).__init__(**kwargs)\n-    self._streaming = streaming\n     self._is_compiled = False\n     self._is_adapted = False\n \n@@ -69,11 +64,6 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n \n     self._adapt_function = None\n \n-  @property\n-  def streaming(self):\n-    \"\"\"Whether `adapt` can be called twice without resetting the state.\"\"\"\n-    return self._streaming\n-\n   @property\n   def is_adapted(self):\n     \"\"\"Whether the layer has been fit to data already.\"\"\"\n@@ -163,7 +153,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n \n     self._is_compiled = True\n \n-  def adapt(self, data, batch_size=None, steps=None, reset_state=True):\n+  def adapt(self, data, batch_size=None, steps=None):\n     \"\"\"Fits the state of the preprocessing layer to the data being passed.\n \n     After calling `adapt` on a layer, a preprocessing layer's state will not\n@@ -232,21 +222,13 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n             the input dataset is exhausted. When passing an infinitely\n             repeating dataset, you must specify the `steps` argument. This\n             argument is not supported with array inputs.\n-        reset_state: Optional argument specifying whether to clear the state of\n-          the layer at the start of the call to `adapt`, or whether to start\n-          from the existing state. This argument may not be relevant to all\n-          preprocessing layers: a subclass of PreprocessingLayer may choose to\n-          throw if 'reset_state' is set to False.\n     \"\"\"\n     _disallow_inside_tf_function('adapt')\n     if not version_utils.should_use_v2():\n       raise RuntimeError('`adapt` is only supported in tensorflow v2.')  # pylint: disable=g-doc-exception\n-    if not self.streaming and self._is_adapted and not reset_state:\n-      raise ValueError('{} does not supporting calling `adapt` twice without '\n-                       'resetting the state.'.format(self.__class__.__name__))\n     if not self._is_compiled:\n       self.compile()  # Compile with defaults.\n-    if self.built and reset_state:\n+    if self.built:\n       self.reset_state()\n     data_handler = data_adapter.DataHandler(\n         data,\n@@ -345,11 +327,9 @@ class CombinerPreprocessingLayer(PreprocessingLayer):\n     super(CombinerPreprocessingLayer, self).compile(\n         run_eagerly=run_eagerly, steps_per_execution=steps_per_execution)\n \n-  def adapt(self, data, batch_size=None, steps=None, reset_state=True):\n-    if not reset_state:\n-      self._adapt_accumulator = self._combiner.restore(self._restore_updates())\n+  def adapt(self, data, batch_size=None, steps=None):\n     super(CombinerPreprocessingLayer, self).adapt(\n-        data, batch_size=batch_size, steps=steps, reset_state=reset_state)\n+        data, batch_size=batch_size, steps=steps)\n \n   def _add_state_variable(self,\n                           name,\n\n@@ -248,44 +248,6 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n \n-  def test_further_tuning(self):\n-    \"\"\"Test that models can be tuned with multiple calls to 'adapt'.\"\"\"\n-\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-\n-    layer = AddingPreprocessingLayer()\n-    layer.adapt(input_dataset)\n-\n-    input_data = keras.Input(shape=(1,))\n-    output = layer(input_data)\n-    model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n-\n-    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n-\n-    layer.adapt(np.array([1, 2]), reset_state=False)\n-    self.assertAllEqual([[19], [20], [21]], model.predict([1., 2., 3.]))\n-\n-  def test_further_tuning_post_injection(self):\n-    \"\"\"Test that models can be tuned with multiple calls to 'adapt'.\"\"\"\n-\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-\n-    layer = AddingPreprocessingLayer()\n-\n-    input_data = keras.Input(shape=(1,))\n-    output = layer(input_data)\n-    model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n-\n-    combiner = layer._combiner\n-    updates = combiner.extract(combiner.compute(input_dataset))\n-    layer._set_state_variables(updates)\n-    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n-\n-    layer.adapt(np.array([1, 2]), reset_state=False)\n-    self.assertAllEqual([[19], [20], [21]], model.predict([1., 2., 3.]))\n-\n   def test_weight_based_state_transfer(self):\n     \"\"\"Test that preproc layers can transfer state via get/set weights..\"\"\"\n \n@@ -311,31 +273,6 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     model_2.set_weights(weights)\n     self.assertAllEqual([[16], [17], [18]], model_2.predict([1., 2., 3.]))\n \n-  def test_weight_based_state_transfer_with_further_tuning(self):\n-    \"\"\"Test that transferred state can be used to further tune a model..\"\"\"\n-\n-    def get_model():\n-      input_data = keras.Input(shape=(1,))\n-      layer = AddingPreprocessingLayer()\n-      output = layer(input_data)\n-      model = keras.Model(input_data, output)\n-      model._run_eagerly = testing_utils.should_run_eagerly()\n-      return (model, layer)\n-\n-    input_dataset = np.array([1, 2, 3, 4, 5])\n-    model, layer = get_model()\n-    layer.adapt(input_dataset)\n-    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n-\n-    # Transfer state from model to model_2 via get/set weights.\n-    weights = model.get_weights()\n-    model_2, layer_2 = get_model()\n-    model_2.set_weights(weights)\n-\n-    # Further adapt this layer based on the transferred weights.\n-    layer_2.adapt(np.array([1, 2]), reset_state=False)\n-    self.assertAllEqual([[19], [20], [21]], model_2.predict([1., 2., 3.]))\n-\n   def test_loading_without_providing_class_fails(self):\n     input_data = keras.Input(shape=(1,))\n     layer = AddingPreprocessingLayer()\n\n@@ -181,7 +181,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       elif bin_boundaries is None:\n         bin_boundaries = kwargs[\"bins\"]\n       del kwargs[\"bins\"]\n-    super().__init__(streaming=True, **kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"Discretization\").set(\n         True)\n     if num_bins is not None and num_bins < 0:\n\n@@ -210,7 +210,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     kwargs.pop(\"vocabulary_size\", None)\n     kwargs.pop(\"has_static_table\", None)\n \n-    super().__init__(streaming=False, **kwargs)\n+    super().__init__(**kwargs)\n \n     if invert:\n       self._key_dtype = tf.int64\n\n@@ -93,7 +93,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"\n \n   def __init__(self, axis=-1, mean=None, variance=None, **kwargs):\n-    super().__init__(streaming=True, **kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n \n     # Standardize `axis` to a tuple.\n\n@@ -304,7 +304,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     # Drop deprecated config options.\n     kwargs.pop(\"vocabulary_size\", None)\n \n-    super().__init__(streaming=False, **kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"TextVectorization\").set(\n         True)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#faca712c3ee7553647326de41432298130578946", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 536 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class Dense(keras_layers.Dense, base.Layer):\n   @compatibility(TF2)\n   This API is not compatible with eager execution or `tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Dense`.\n@@ -204,7 +204,7 @@ def dense(\n   @compatibility(TF2)\n   This API is not compatible with eager execution or `tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Dense`.\n@@ -276,7 +276,7 @@ class Dropout(keras_layers.Dropout, base.Layer):\n   @compatibility(TF2)\n   This API is not compatible with eager execution or `tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Dropout`.\n@@ -357,7 +357,7 @@ def dropout(inputs,\n   @compatibility(TF2)\n   This API is not compatible with eager execution or `tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Dropout`.\n@@ -414,7 +414,7 @@ class Flatten(keras_layers.Flatten, base.Layer):\n   @compatibility(TF2)\n   This API is not compatible with eager execution or `tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Flatten`.\n@@ -472,7 +472,7 @@ def flatten(inputs, name=None, data_format='channels_last'):\n   @compatibility(TF2)\n   This API is not compatible with eager execution or`tf.function`.\n \n-  Please refer to [migration guide]\n+  Please refer to [tf.layers section of the migration guide]\n   (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n   for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n   layer is `tf.keras.layers.Flatten`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0baa84bcfa5cd9e0b1be43d056d3676b49911028", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2942 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,6 @@\n # pylint: disable=g-classes-have-attributes\n \n import collections\n-import os\n \n from keras import backend\n from keras.engine import base_layer_utils\n@@ -372,7 +371,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n                        \"TF_IDF. output_mode is {}.\".format(self.output_mode))\n \n     if isinstance(vocabulary, str):\n-      if not os.path.exists(vocabulary):\n+      if not tf.io.gfile.exists(vocabulary):\n         raise ValueError(\n             \"Vocabulary file {} does not exist.\".format(vocabulary))\n       if self.output_mode == TF_IDF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d2663ec63a49549cb9da3efd5ff94df4949d0ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 80 | Files Changed: 12 | Hunks: 76 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 189 | Churn Cumulative: 10554 | Contributors (this commit): 6 | Commits (past 90d): 102 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -14,20 +14,20 @@\n # ==============================================================================\n \"\"\"Contains the base ProcessingLayer and a subclass that uses Combiners.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import abc\n import collections\n \n-import numpy as np\n-\n-from tensorflow.python.eager import context\n from keras import backend\n from keras.engine import data_adapter\n from keras.engine.base_layer import Layer\n from keras.utils import tf_utils\n from keras.utils import version_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.eager import context\n from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls\n \n \n keras_kpl_gauge = tf.__internal__.monitoring.BoolGauge(\n@@ -69,6 +69,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     \"\"\"Whether the layer has been fit to data already.\"\"\"\n     return self._is_adapted\n \n+  @doc_controls.do_not_generate_docs\n   def update_state(self, data):\n     \"\"\"Accumulates statistics for the preprocessing layer.\n \n@@ -77,10 +78,12 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     \"\"\"\n     raise NotImplementedError\n \n+  @doc_controls.do_not_generate_docs\n   def reset_state(self):  # pylint: disable=method-hidden\n     \"\"\"Resets the statistics of the preprocessing layer.\"\"\"\n     raise NotImplementedError\n \n+  @doc_controls.do_not_generate_docs\n   def finalize_state(self):\n     \"\"\"Finalize the statistics for the preprocessing layer.\n \n@@ -90,6 +93,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     \"\"\"\n     pass\n \n+  @doc_controls.do_not_generate_docs\n   def make_adapt_function(self):\n     \"\"\"Creates a function to execute one step of `adapt`.\n \n\n@@ -87,11 +87,11 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n     # will not be changed in a training step.\n     if use_adapt:\n       feature_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               num_oov_indices=1))\n       feature_lookup_layer.adapt(FEATURE_VOCAB)\n       label_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               num_oov_indices=0, mask_token=None))\n       label_lookup_layer.adapt(LABEL_VOCAB)\n     else:\n@@ -99,10 +99,10 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n       shuffled_vocab = FEATURE_VOCAB.copy()\n       random.shuffle(shuffled_vocab)\n       feature_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               vocabulary=shuffled_vocab, num_oov_indices=1))\n       label_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))\n \n     raw_feature_input = tf.keras.Input(\n@@ -122,7 +122,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n   def define_reverse_lookup_layer(self):\n     # Only needed for serving.\n     label_inverse_lookup_layer = (\n-        tf.keras.layers.experimental.preprocessing.StringLookup(\n+        tf.keras.layers.StringLookup(\n             num_oov_indices=0,\n             mask_token=None,\n             vocabulary=LABEL_VOCAB,\n@@ -270,7 +270,7 @@ class KPLCreatedInDatasetsFromFunctionTest(tf.test.TestCase,\n \n       def dataset_fn(input_context):\n         del input_context\n-        lookup_layer = tf.keras.layers.experimental.preprocessing.StringLookup(\n+        lookup_layer = tf.keras.layers.StringLookup(\n             num_oov_indices=1, vocabulary=filepath)\n         x = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                       [\"fire\", \"and\", \"earth\", \"michigan\"]])\n\n@@ -17,7 +17,7 @@\n import os\n \n import tensorflow as tf\n-preprocessing = tf.keras.layers.experimental.preprocessing\n+preprocessing = tf.keras.layers\n \n BATCH_SIZE = 64\n DS_SIZE = BATCH_SIZE * 16\n\n@@ -56,19 +56,19 @@ class TpuStrategyTest(tf.test.TestCase):\n   def define_kpls_for_training(self, use_adapt):\n     if use_adapt:\n       feature_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               num_oov_indices=1))\n       feature_lookup_layer.adapt(FEATURE_VOCAB)\n       label_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               num_oov_indices=0, mask_token=None))\n       label_lookup_layer.adapt(LABEL_VOCAB)\n     else:\n       feature_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               vocabulary=FEATURE_VOCAB, num_oov_indices=1))\n       label_lookup_layer = (\n-          tf.keras.layers.experimental.preprocessing.StringLookup(\n+          tf.keras.layers.StringLookup(\n               vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))\n \n     raw_feature_input = tf.keras.layers.Input(\n@@ -87,7 +87,7 @@ class TpuStrategyTest(tf.test.TestCase):\n   def define_inverse_lookup_layer(self):\n     # Only needed for serving.\n     label_inverse_lookup_layer = (\n-        tf.keras.layers.experimental.preprocessing.StringLookup(\n+        tf.keras.layers.StringLookup(\n             num_oov_indices=0,\n             mask_token=None,\n             vocabulary=LABEL_VOCAB,\n\n@@ -32,7 +32,8 @@ MULTI_HOT = \"multi_hot\"\n COUNT = \"count\"\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.CategoryEncoding\")\n+@keras_export(\"keras.layers.CategoryEncoding\",\n+              \"keras.layers.experimental.preprocessing.CategoryEncoding\")\n class CategoryEncoding(base_layer.Layer):\n   \"\"\"Category encoding layer.\n \n@@ -40,13 +41,13 @@ class CategoryEncoding(base_layer.Layer):\n   when the total number of tokens are known in advance. It accepts integer\n   values as inputs, and it outputs a dense representation of those\n   inputs. For integer inputs where the total number of tokens is not known,\n-  use instead `tf.keras.layers.experimental.preprocessing.IntegerLookup`.\n+  use instead `tf.keras.layers.IntegerLookup`.\n \n   Examples:\n \n   **One-hot encoding data**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n+  >>> layer = tf.keras.layers.CategoryEncoding(\n   ...           num_tokens=4, output_mode=\"one_hot\")\n   >>> layer([3, 2, 0, 1])\n   <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n@@ -57,7 +58,7 @@ class CategoryEncoding(base_layer.Layer):\n \n   **Multi-hot encoding data**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n+  >>> layer = tf.keras.layers.CategoryEncoding(\n   ...           num_tokens=4, output_mode=\"multi_hot\")\n   >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]])\n   <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n@@ -68,7 +69,7 @@ class CategoryEncoding(base_layer.Layer):\n \n   **Using weighted inputs in `\"count\"` mode**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n+  >>> layer = tf.keras.layers.CategoryEncoding(\n   ...           num_tokens=4, output_mode=\"count\")\n   >>> count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])\n   >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)\n\n@@ -118,7 +118,8 @@ def get_bin_boundaries(summary, num_bins):\n   return compress(summary, 1.0 / num_bins)[0, :-1]\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.Discretization\")\n+@keras_export(\"keras.layers.Discretization\",\n+              \"keras.layers.experimental.preprocessing.Discretization\")\n class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Buckets data into discrete ranges.\n \n@@ -148,8 +149,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n \n   Bucketize float values based on provided buckets.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n-  >>> layer = tf.keras.layers.experimental.preprocessing.Discretization(\n-  ...          bin_boundaries=[0., 1., 2.])\n+  >>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n   array([[0, 2, 3, 1],\n@@ -157,8 +157,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n \n   Bucketize float values based on a number of buckets to compute.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n-  >>> layer = tf.keras.layers.experimental.preprocessing.Discretization(\n-  ...          num_bins=4, epsilon=0.01)\n+  >>> layer = tf.keras.layers.Discretization(num_bins=4, epsilon=0.01)\n   >>> layer.adapt(input)\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n\n@@ -27,7 +27,8 @@ from tensorflow.python.util.tf_export import keras_export\n _DEFAULT_SALT_KEY = [0xDECAFCAFFE, 0xDECAFCAFFE]\n \n \n-@keras_export('keras.layers.experimental.preprocessing.Hashing')\n+@keras_export('keras.layers.Hashing',\n+              'keras.layers.experimental.preprocessing.Hashing')\n class Hashing(base_layer.Layer):\n   \"\"\"Implements categorical feature hashing, also known as \"hashing trick\".\n \n@@ -48,7 +49,7 @@ class Hashing(base_layer.Layer):\n \n   **Example (FarmHash64)**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)\n+  >>> layer = tf.keras.layers.Hashing(num_bins=3)\n   >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n   >>> layer(inp)\n   <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n@@ -60,8 +61,7 @@ class Hashing(base_layer.Layer):\n \n   **Example (FarmHash64) with a mask value**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n-  ...    mask_value='')\n+  >>> layer = tf.keras.layers.Hashing(num_bins=3, mask_value='')\n   >>> inp = [['A'], ['B'], [''], ['C'], ['D']]\n   >>> layer(inp)\n   <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n@@ -73,8 +73,7 @@ class Hashing(base_layer.Layer):\n \n   **Example (SipHash64)**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n-  ...    salt=[133, 137])\n+  >>> layer = tf.keras.layers.Hashing(num_bins=3, salt=[133, 137])\n   >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n   >>> layer(inp)\n   <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n@@ -86,8 +85,7 @@ class Hashing(base_layer.Layer):\n \n   **Example (Siphash64 with a single integer, same as `salt=[133, 133]`)**\n \n-  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n-  ...    salt=133)\n+  >>> layer = tf.keras.layers.Hashing(num_bins=3, salt=133)\n   >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n   >>> layer(inp)\n   <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n\n@@ -53,7 +53,8 @@ def check_fill_mode_and_interpolation(fill_mode, interpolation):\n                               '`bilinear` are supported.'.format(interpolation))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.Resizing')\n+@keras_export('keras.layers.Resizing',\n+              'keras.layers.experimental.preprocessing.Resizing')\n class Resizing(base_layer.Layer):\n   \"\"\"Image resizing layer.\n \n@@ -118,7 +119,8 @@ class Resizing(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.CenterCrop')\n+@keras_export('keras.layers.CenterCrop',\n+              'keras.layers.experimental.preprocessing.CenterCrop')\n class CenterCrop(base_layer.Layer):\n   \"\"\"Crop the central portion of the images to target height and width.\n \n@@ -190,7 +192,8 @@ class CenterCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomCrop')\n+@keras_export('keras.layers.RandomCrop',\n+              'keras.layers.experimental.preprocessing.RandomCrop')\n class RandomCrop(base_layer.Layer):\n   \"\"\"Randomly crop the images to target height and width.\n \n@@ -313,7 +316,8 @@ class RandomCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.Rescaling')\n+@keras_export('keras.layers.Rescaling',\n+              'keras.layers.experimental.preprocessing.Rescaling')\n class Rescaling(base_layer.Layer):\n   \"\"\"Multiply inputs by `scale` and adds `offset`.\n \n@@ -367,7 +371,8 @@ VERTICAL = 'vertical'\n HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomFlip')\n+@keras_export('keras.layers.RandomFlip',\n+              'keras.layers.experimental.preprocessing.RandomFlip')\n class RandomFlip(base_layer.Layer):\n   \"\"\"Randomly flip each image horizontally and vertically.\n \n@@ -447,7 +452,8 @@ class RandomFlip(base_layer.Layer):\n \n \n # TODO(tanzheny): Add examples, here and everywhere.\n-@keras_export('keras.layers.experimental.preprocessing.RandomTranslation')\n+@keras_export('keras.layers.RandomTranslation',\n+              'keras.layers.experimental.preprocessing.RandomTranslation')\n class RandomTranslation(base_layer.Layer):\n   \"\"\"Randomly translate each image during training.\n \n@@ -767,7 +773,8 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n         axis=1)\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomRotation')\n+@keras_export('keras.layers.RandomRotation',\n+              'keras.layers.experimental.preprocessing.RandomRotation')\n class RandomRotation(base_layer.Layer):\n   \"\"\"Randomly rotate each image.\n \n@@ -887,7 +894,8 @@ class RandomRotation(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomZoom')\n+@keras_export('keras.layers.RandomZoom',\n+              'keras.layers.experimental.preprocessing.RandomZoom')\n class RandomZoom(base_layer.Layer):\n   \"\"\"Randomly zoom each image during training.\n \n@@ -926,7 +934,7 @@ class RandomZoom(base_layer.Layer):\n   Example:\n \n   >>> input_img = np.random.random((32, 224, 224, 3))\n-  >>> layer = tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2)\n+  >>> layer = tf.keras.layers.RandomZoom(.5, .2)\n   >>> out_img = layer(input_img)\n   >>> out_img.shape\n   TensorShape([32, 224, 224, 3])\n@@ -1087,7 +1095,8 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n         axis=1)\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomContrast')\n+@keras_export('keras.layers.RandomContrast',\n+              'keras.layers.experimental.preprocessing.RandomContrast')\n class RandomContrast(base_layer.Layer):\n   \"\"\"Adjust the contrast of an image or images by a random factor.\n \n@@ -1156,7 +1165,8 @@ class RandomContrast(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomHeight')\n+@keras_export('keras.layers.RandomHeight',\n+              'keras.layers.experimental.preprocessing.RandomHeight')\n class RandomHeight(base_layer.Layer):\n   \"\"\"Randomly vary the height of a batch of images during training.\n \n@@ -1255,7 +1265,8 @@ class RandomHeight(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.experimental.preprocessing.RandomWidth')\n+@keras_export('keras.layers.RandomWidth',\n+              'keras.layers.experimental.preprocessing.RandomWidth')\n class RandomWidth(base_layer.Layer):\n   \"\"\"Randomly vary the width of a batch of images during training.\n \n\n@@ -24,7 +24,10 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.IntegerLookup\", v1=[])\n+@keras_export(\n+    \"keras.layers.IntegerLookup\",\n+    \"keras.layers.experimental.preprocessing.IntegerLookup\",\n+    v1=[])\n class IntegerLookup(index_lookup.IndexLookup):\n   \"\"\"Reindex integer inputs to be in a contiguous range, via a dict lookup.\n \n@@ -117,7 +120,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])  # Note OOV tokens\n-  >>> layer = IntegerLookup(vocabulary=vocab)\n+  >>> layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[1, 3, 4],\n@@ -129,7 +132,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   the dataset.\n \n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n-  >>> layer = IntegerLookup()\n+  >>> layer = tf.keras.layers.IntegerLookup()\n   >>> layer.adapt(data)\n   >>> layer.get_vocabulary()\n   [-1, 42, 1138, 1000, 36, 12]\n@@ -139,7 +142,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   by inverse sort order.\n \n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n-  >>> layer = IntegerLookup()\n+  >>> layer = tf.keras.layers.IntegerLookup()\n   >>> layer.adapt(data)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n@@ -156,7 +159,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42], [37, 1000, 36]])\n-  >>> layer = IntegerLookup(vocabulary=vocab, num_oov_indices=2)\n+  >>> layer = tf.keras.layers.IntegerLookup(vocabulary=vocab, num_oov_indices=2)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[2, 4, 5],\n@@ -174,7 +177,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([12, 36, 1138, 42, 7]) # Note OOV tokens\n-  >>> layer = IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n+  >>> layer = tf.keras.layers.IntegerLookup(\n+  ...     vocabulary=vocab, output_mode='one_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 0., 0.],\n@@ -190,7 +194,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(vocabulary=vocab, output_mode='multi_hot')\n+  >>> layer = tf.keras.layers.IntegerLookup(\n+  ...     vocabulary=vocab, output_mode='multi_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 1.],\n@@ -203,7 +208,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(vocabulary=vocab, output_mode='count')\n+  >>> layer = tf.keras.layers.IntegerLookup(\n+  ...     vocabulary=vocab, output_mode='count')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 2.],\n@@ -222,7 +228,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> vocab = [12, 36, 1138, 42]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(output_mode='tf_idf')\n+  >>> layer = tf.keras.layers.IntegerLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -235,7 +241,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> vocab = [-1, 12, 36, 1138, 42]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(output_mode='tf_idf')\n+  >>> layer = tf.keras.layers.IntegerLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -254,7 +260,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n-  >>> layer = IntegerLookup(vocabulary=vocab, invert=True)\n+  >>> layer = tf.keras.layers.IntegerLookup(vocabulary=vocab, invert=True)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[  12, 1138,   42],\n@@ -270,8 +276,9 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n-  >>> layer = IntegerLookup(vocabulary=vocab)\n-  >>> i_layer = IntegerLookup(vocabulary=layer.get_vocabulary(), invert=True)\n+  >>> layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)\n+  >>> i_layer = tf.keras.layers.IntegerLookup(\n+  ...     vocabulary=layer.get_vocabulary(), invert=True)\n   >>> int_data = layer(data)\n   >>> i_layer(int_data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n\n@@ -24,7 +24,8 @@ from tensorflow.python.util.tf_export import keras_export\n # pylint: disable=g-classes-have-attributes\n \n \n-@keras_export('keras.layers.experimental.preprocessing.Normalization')\n+@keras_export('keras.layers.Normalization',\n+              'keras.layers.experimental.preprocessing.Normalization')\n class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Feature-wise normalization of the data.\n \n@@ -62,7 +63,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n   >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n   >>> input_data = np.array([1., 2., 3.], dtype='float32')\n-  >>> layer = Normalization(axis=None)\n+  >>> layer = tf.keras.layers.Normalization(axis=None)\n   >>> layer.adapt(adapt_data)\n   >>> layer(input_data)\n   <tf.Tensor: shape=(3,), dtype=float32, numpy=\n@@ -75,7 +76,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   ...                        [0., 7., 4.],\n   ...                        [2., 9., 6.]], dtype='float32')\n   >>> input_data = np.array([[0., 7., 4.]], dtype='float32')\n-  >>> layer = Normalization(axis=-1)\n+  >>> layer = tf.keras.layers.Normalization(axis=-1)\n   >>> layer.adapt(adapt_data)\n   >>> layer(input_data)\n   <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n@@ -84,7 +85,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   Pass the mean and variance directly.\n \n   >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n-  >>> layer = Normalization(mean=3., variance=2.)\n+  >>> layer = tf.keras.layers.Normalization(mean=3., variance=2.)\n   >>> layer(input_data)\n   <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n   array([[-1.4142135 ],\n\n@@ -23,7 +23,10 @@ from keras.layers.preprocessing import index_lookup\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.StringLookup\", v1=[])\n+@keras_export(\n+    \"keras.layers.StringLookup\",\n+    \"keras.layers.experimental.preprocessing.StringLookup\",\n+    v1=[])\n class StringLookup(index_lookup.IndexLookup):\n   \"\"\"Maps strings from a vocabulary to integer indices.\n \n@@ -113,7 +116,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n-  >>> layer = StringLookup(vocabulary=vocab)\n+  >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[1, 3, 4],\n@@ -125,7 +128,7 @@ class StringLookup(index_lookup.IndexLookup):\n   the dataset.\n \n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n-  >>> layer = StringLookup()\n+  >>> layer = tf.keras.layers.StringLookup()\n   >>> layer.adapt(data)\n   >>> layer.get_vocabulary()\n   ['[UNK]', 'd', 'z', 'c', 'b', 'a']\n@@ -135,7 +138,7 @@ class StringLookup(index_lookup.IndexLookup):\n   (`\"d\"`, which has 2 occurrences, is first) then by inverse sort order.\n \n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n-  >>> layer = StringLookup()\n+  >>> layer = tf.keras.layers.StringLookup()\n   >>> layer.adapt(data)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n@@ -151,7 +154,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"m\", \"z\", \"b\"]])\n-  >>> layer = StringLookup(vocabulary=vocab, num_oov_indices=2)\n+  >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab, num_oov_indices=2)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[2, 4, 5],\n@@ -169,7 +172,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"z\"])\n-  >>> layer = StringLookup(vocabulary=vocab, output_mode='one_hot')\n+  >>> layer = tf.keras.layers.StringLookup(\n+  ...     vocabulary=vocab, output_mode='one_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 0., 0.],\n@@ -185,7 +189,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(vocabulary=vocab, output_mode='multi_hot')\n+  >>> layer = tf.keras.layers.StringLookup(\n+  ...     vocabulary=vocab, output_mode='multi_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 1.],\n@@ -198,7 +203,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(vocabulary=vocab, output_mode='count')\n+  >>> layer = tf.keras.layers.StringLookup(\n+  ...     vocabulary=vocab, output_mode='count')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 2.],\n@@ -217,7 +223,7 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode=\"tf_idf\")\n+  >>> layer = tf.keras.layers.StringLookup(output_mode=\"tf_idf\")\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -230,7 +236,7 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode=\"tf_idf\")\n+  >>> layer = tf.keras.layers.StringLookup(output_mode=\"tf_idf\")\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -249,7 +255,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n-  >>> layer = StringLookup(vocabulary=vocab, invert=True)\n+  >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=string, numpy=\n   array([[b'a', b'c', b'd'],\n@@ -265,8 +271,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n-  >>> layer = StringLookup(vocabulary=vocab)\n-  >>> i_layer = StringLookup(vocabulary=vocab, invert=True)\n+  >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab)\n+  >>> i_layer = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)\n   >>> int_data = layer(data)\n   >>> i_layer(int_data)\n   <tf.Tensor: shape=(2, 3), dtype=string, numpy=\n\n@@ -58,7 +58,9 @@ _ACCUMULATOR_NUM_DOCUMENTS = \"num_documents\"\n \n \n @keras_export(\n-    \"keras.layers.experimental.preprocessing.TextVectorization\", v1=[])\n+    \"keras.layers.TextVectorization\",\n+    \"keras.layers.experimental.preprocessing.TextVectorization\",\n+    v1=[])\n class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Text vectorization layer.\n \n@@ -165,7 +167,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   >>> max_len = 4  # Sequence length to pad the outputs to.\n   >>>\n   >>> # Create the layer.\n-  >>> vectorize_layer = TextVectorization(\n+  >>> vectorize_layer = tf.keras.layers.TextVectorization(\n   ...  max_tokens=max_features,\n   ...  output_mode='int',\n   ...  output_sequence_length=max_len)\n@@ -206,7 +208,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   >>> # Create the layer, passing the vocab directly. You can also pass the\n   >>> # vocabulary arg a path to a file containing one vocabulary word per\n   >>> # line.\n-  >>> vectorize_layer = TextVectorization(\n+  >>> vectorize_layer = tf.keras.layers.TextVectorization(\n   ...  max_tokens=max_features,\n   ...  output_mode='int',\n   ...  output_sequence_length=max_len,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
