{"custom_id": "scrapy#d015329d759dda72586b856ee92d787dc730f06e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 20 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 638 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: 0.32432432432432434\n\nDIFF:\n@@ -1,8 +1,11 @@\n+from __future__ import annotations\n+\n import logging\n import sys\n import warnings\n from logging.config import dictConfig\n-from typing import Tuple\n+from types import TracebackType\n+from typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union, cast\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -12,13 +15,25 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.settings import Settings\n from scrapy.utils.versions import scrapy_components_versions\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n-def failure_to_exc_info(failure: Failure):\n+def failure_to_exc_info(\n+    failure: Failure,\n+) -> Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:\n     \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n-        return (failure.type, failure.value, failure.getTracebackObject())\n+        assert failure.type\n+        assert failure.value\n+        return (\n+            failure.type,\n+            failure.value,\n+            cast(Optional[TracebackType], failure.getTracebackObject()),\n+        )\n+    return None\n \n \n class TopLevelFormatter(logging.Filter):\n@@ -33,10 +48,10 @@ class TopLevelFormatter(logging.Filter):\n     ``loggers`` list where it should act.\n     \"\"\"\n \n-    def __init__(self, loggers=None):\n-        self.loggers = loggers or []\n+    def __init__(self, loggers: Optional[List[str]] = None):\n+        self.loggers: List[str] = loggers or []\n \n-    def filter(self, record):\n+    def filter(self, record: logging.LogRecord) -> bool:\n         if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n             record.name = record.name.split(\".\", 1)[0]\n         return True\n@@ -62,7 +77,9 @@ DEFAULT_LOGGING = {\n }\n \n \n-def configure_logging(settings=None, install_root_handler=True):\n+def configure_logging(\n+    settings: Union[Settings, dict, None] = None, install_root_handler: bool = True\n+) -> None:\n     \"\"\"\n     Initialize logging defaults for Scrapy.\n \n@@ -99,13 +116,13 @@ def configure_logging(settings=None, install_root_handler=True):\n         settings = Settings(settings)\n \n     if settings.getbool(\"LOG_STDOUT\"):\n-        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))\n+        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))  # type: ignore[assignment]\n \n     if install_root_handler:\n         install_scrapy_root_handler(settings)\n \n \n-def install_scrapy_root_handler(settings):\n+def install_scrapy_root_handler(settings: Settings) -> None:\n     global _scrapy_root_handler\n \n     if (\n@@ -118,16 +135,17 @@ def install_scrapy_root_handler(settings):\n     logging.root.addHandler(_scrapy_root_handler)\n \n \n-def get_scrapy_root_handler():\n+def get_scrapy_root_handler() -> Optional[logging.Handler]:\n     return _scrapy_root_handler\n \n \n-_scrapy_root_handler = None\n+_scrapy_root_handler: Optional[logging.Handler] = None\n \n \n-def _get_handler(settings):\n+def _get_handler(settings: Settings) -> logging.Handler:\n     \"\"\"Return a log handler object according to settings\"\"\"\n     filename = settings.get(\"LOG_FILE\")\n+    handler: logging.Handler\n     if filename:\n         mode = \"a\" if settings.getbool(\"LOG_FILE_APPEND\") else \"w\"\n         encoding = settings.get(\"LOG_ENCODING\")\n@@ -181,16 +199,16 @@ class StreamLogger:\n         https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n     \"\"\"\n \n-    def __init__(self, logger, log_level=logging.INFO):\n-        self.logger = logger\n-        self.log_level = log_level\n-        self.linebuf = \"\"\n+    def __init__(self, logger: logging.Logger, log_level: int = logging.INFO):\n+        self.logger: logging.Logger = logger\n+        self.log_level: int = log_level\n+        self.linebuf: str = \"\"\n \n-    def write(self, buf):\n+    def write(self, buf: str) -> None:\n         for line in buf.rstrip().splitlines():\n             self.logger.log(self.log_level, line.rstrip())\n \n-    def flush(self):\n+    def flush(self) -> None:\n         for h in self.logger.handlers:\n             h.flush()\n \n@@ -198,11 +216,11 @@ class StreamLogger:\n class LogCounterHandler(logging.Handler):\n     \"\"\"Record log levels count into a crawler stats\"\"\"\n \n-    def __init__(self, crawler, *args, **kwargs):\n+    def __init__(self, crawler: Crawler, *args: Any, **kwargs: Any):\n         super().__init__(*args, **kwargs)\n-        self.crawler = crawler\n+        self.crawler: Crawler = crawler\n \n-    def emit(self, record):\n+    def emit(self, record: logging.LogRecord) -> None:\n         sname = f\"log_count/{record.levelname}\"\n         self.crawler.stats.inc_value(sname)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c43798cb9bee99ccf96047f3dfcc6debb65973d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 19 | Methods Changed: 17 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 66 | Churn Cumulative: 1577 | Contributors (this commit): 38 | Commits (past 90d): 7 | Contributors (cumulative): 61 | DMM Complexity: 0.22727272727272727\n\nDIFF:\n@@ -1,11 +1,12 @@\n import traceback\n import warnings\n from collections import defaultdict\n+from types import ModuleType\n from typing import DefaultDict, Dict, List, Tuple, Type\n \n from zope.interface import implementer\n \n-from scrapy import Spider\n+from scrapy import Request, Spider\n from scrapy.interfaces import ISpiderLoader\n from scrapy.settings import BaseSettings\n from scrapy.utils.misc import walk_modules\n@@ -45,12 +46,12 @@ class SpiderLoader:\n                 category=UserWarning,\n             )\n \n-    def _load_spiders(self, module):\n+    def _load_spiders(self, module: ModuleType) -> None:\n         for spcls in iter_spider_classes(module):\n             self._found[spcls.name].append((module.__name__, spcls.__name__))\n             self._spiders[spcls.name] = spcls\n \n-    def _load_all_spiders(self):\n+    def _load_all_spiders(self) -> None:\n         for name in self.spider_modules:\n             try:\n                 for module in walk_modules(name):\n@@ -81,7 +82,7 @@ class SpiderLoader:\n         except KeyError:\n             raise KeyError(f\"Spider not found: {spider_name}\")\n \n-    def find_by_request(self, request):\n+    def find_by_request(self, request: Request) -> List[str]:\n         \"\"\"\n         Return the list of spider names that can handle the given request.\n         \"\"\"\n\n@@ -82,7 +82,7 @@ class Spider(object_ref):\n         settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n \n     @classmethod\n-    def handles_request(cls, request):\n+    def handles_request(cls, request: Request) -> bool:\n         return url_is_from_spider(request.url, cls)\n \n     @staticmethod\n\n@@ -5,11 +5,13 @@ import asyncio\n import inspect\n from asyncio import Future\n from functools import wraps\n+from types import CoroutineType\n from typing import (\n     Any,\n     AsyncGenerator,\n     AsyncIterable,\n     AsyncIterator,\n+    Awaitable,\n     Callable,\n     Coroutine,\n     Dict,\n@@ -19,8 +21,10 @@ from typing import (\n     List,\n     Optional,\n     Tuple,\n+    TypeVar,\n     Union,\n     cast,\n+    overload,\n )\n \n from twisted.internet import defer\n@@ -186,9 +190,7 @@ class _AsyncCooperatorAdapter(Iterator):\n     def _call_anext(self) -> None:\n         # This starts waiting for the next result from aiterator.\n         # If aiterator is exhausted, _errback will be called.\n-        self.anext_deferred = cast(\n-            Deferred, deferred_from_coro(self.aiterator.__anext__())\n-        )\n+        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n         self.anext_deferred.addCallbacks(self._callback, self._errback)\n \n     def __next__(self) -> Deferred:\n@@ -297,7 +299,21 @@ async def aiter_errback(\n             errback(failure.Failure(), *a, **kw)\n \n \n-def deferred_from_coro(o: Any) -> Any:\n+_CT = TypeVar(\"_CT\", bound=Union[Awaitable, CoroutineType, Future])\n+_T = TypeVar(\"_T\")\n+\n+\n+@overload\n+def deferred_from_coro(o: _CT) -> Deferred:\n+    ...\n+\n+\n+@overload\n+def deferred_from_coro(o: _T) -> _T:\n+    ...\n+\n+\n+def deferred_from_coro(o: _T) -> Union[Deferred, _T]:\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, Deferred):\n         return o\n\n@@ -1,14 +1,33 @@\n+from __future__ import annotations\n+\n import inspect\n import logging\n+from types import ModuleType\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AsyncIterable,\n+    Generator,\n+    Iterable,\n+    Optional,\n+    Type,\n+    Union,\n+)\n \n+from twisted.internet.defer import Deferred\n+\n+from scrapy import Request\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n \n+if TYPE_CHECKING:\n+    from scrapy.spiderloader import SpiderLoader\n+\n logger = logging.getLogger(__name__)\n \n \n-def iterate_spider_output(result):\n+def iterate_spider_output(result: Any) -> Union[Iterable, AsyncIterable, Deferred]:\n     if inspect.isasyncgen(result):\n         return result\n     if inspect.iscoroutine(result):\n@@ -18,7 +37,7 @@ def iterate_spider_output(result):\n     return arg_to_iter(deferred_from_coro(result))\n \n \n-def iter_spider_classes(module):\n+def iter_spider_classes(module: ModuleType) -> Generator[Type[Spider], Any, None]:\n     \"\"\"Return an iterator over all spider classes defined in the given module\n     that can be instantiated (i.e. which have name)\n     \"\"\"\n@@ -37,8 +56,12 @@ def iter_spider_classes(module):\n \n \n def spidercls_for_request(\n-    spider_loader, request, default_spidercls=None, log_none=False, log_multiple=False\n-):\n+    spider_loader: SpiderLoader,\n+    request: Request,\n+    default_spidercls: Optional[Type[Spider]] = None,\n+    log_none: bool = False,\n+    log_multiple: bool = False,\n+) -> Optional[Type[Spider]]:\n     \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1f87e4f088c0757dd833d2a0841be0b830deb57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 34 | Files Changed: 4 | Hunks: 34 | Methods Changed: 29 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 137 | Churn Cumulative: 3116 | Contributors (this commit): 39 | Commits (past 90d): 5 | Contributors (cumulative): 82 | DMM Complexity: 0.29310344827586204\n\nDIFF:\n@@ -7,7 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n import json\n from contextlib import suppress\n-from typing import Generator, Tuple\n+from typing import Generator, Optional, Tuple\n from urllib.parse import urljoin\n \n import parsel\n@@ -37,7 +37,7 @@ class TextResponse(Response):\n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop(\"encoding\", None)\n         self._cached_benc = None\n-        self._cached_ubody = None\n+        self._cached_ubody: Optional[str] = None\n         self._cached_selector = None\n         super().__init__(*args, **kwargs)\n \n@@ -82,7 +82,7 @@ class TextResponse(Response):\n         return self._cached_decoded_json\n \n     @property\n-    def text(self):\n+    def text(self) -> str:\n         \"\"\"Body as unicode\"\"\"\n         # access self.encoding before _cached_ubody to make sure\n         # _body_inferred_encoding is called\n\n@@ -1,10 +1,11 @@\n \"\"\"\n XPath selectors based on lxml\n \"\"\"\n+from typing import Any, Optional, Type, Union\n \n from parsel import Selector as _ParselSelector\n \n-from scrapy.http import HtmlResponse, XmlResponse\n+from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n \n@@ -13,14 +14,14 @@ __all__ = [\"Selector\", \"SelectorList\"]\n _NOT_SET = object()\n \n \n-def _st(response, st):\n+def _st(response: Optional[TextResponse], st: Optional[str]) -> str:\n     if st is None:\n         return \"xml\" if isinstance(response, XmlResponse) else \"html\"\n     return st\n \n \n-def _response_from_text(text, st):\n-    rt = XmlResponse if st == \"xml\" else HtmlResponse\n+def _response_from_text(text: Union[str, bytes], st: Optional[str]) -> TextResponse:\n+    rt: Type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n     return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n \n \n@@ -65,7 +66,14 @@ class Selector(_ParselSelector, object_ref):\n     __slots__ = [\"response\"]\n     selectorlist_cls = SelectorList\n \n-    def __init__(self, response=None, text=None, type=None, root=_NOT_SET, **kwargs):\n+    def __init__(\n+        self,\n+        response: Optional[TextResponse] = None,\n+        text: Optional[str] = None,\n+        type: Optional[str] = None,\n+        root: Optional[Any] = _NOT_SET,\n+        **kwargs: Any,\n+    ):\n         if response is not None and text is not None:\n             raise ValueError(\n                 f\"{self.__class__.__name__}.__init__() received \"\n\n@@ -1,16 +1,37 @@\n+from __future__ import annotations\n+\n import csv\n import logging\n import re\n from io import StringIO\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Generator,\n+    Iterable,\n+    List,\n+    Literal,\n+    Optional,\n+    Union,\n+    cast,\n+    overload,\n+)\n \n from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n \n+if TYPE_CHECKING:\n+    from lxml._types import SupportsReadClose\n+\n logger = logging.getLogger(__name__)\n \n \n-def xmliter(obj, nodename):\n+def xmliter(\n+    obj: Union[Response, str, bytes], nodename: str\n+) -> Generator[Selector, Any, None]:\n     \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n        given the name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -27,20 +48,22 @@ def xmliter(obj, nodename):\n     NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n     text = _body_or_str(obj)\n \n-    document_header = re.search(DOCUMENT_HEADER_RE, text)\n-    document_header = document_header.group().strip() if document_header else \"\"\n+    document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n+    document_header = (\n+        document_header_match.group().strip() if document_header_match else \"\"\n+    )\n     header_end_idx = re_rsearch(HEADER_END_RE, text)\n     header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n-    namespaces = {}\n+    namespaces: Dict[str, str] = {}\n     if header_end:\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n+            assert header_end_idx\n             tag = re.search(\n                 rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n             )\n             if tag:\n-                namespaces.update(\n-                    reversed(x) for x in re.findall(NAMESPACE_RE, tag.group())\n-                )\n+                for x in re.findall(NAMESPACE_RE, tag.group()):\n+                    namespaces[x[1]] = x[0]\n \n     r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n     for match in r.finditer(text):\n@@ -54,12 +77,19 @@ def xmliter(obj, nodename):\n         yield Selector(text=nodetext, type=\"xml\")\n \n \n-def xmliter_lxml(obj, nodename, namespace=None, prefix=\"x\"):\n+def xmliter_lxml(\n+    obj: Union[TextResponse, str, bytes],\n+    nodename: str,\n+    namespace: Optional[str] = None,\n+    prefix: str = \"x\",\n+) -> Generator[Selector, Any, None]:\n     from lxml import etree\n \n     reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n-    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n+    iterable = etree.iterparse(\n+        cast(SupportsReadClose[bytes], reader), tag=tag, encoding=reader.encoding\n+    )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     for _, node in iterable:\n         nodetext = etree.tostring(node, encoding=\"unicode\")\n@@ -71,30 +101,39 @@ def xmliter_lxml(obj, nodename, namespace=None, prefix=\"x\"):\n \n \n class _StreamReader:\n-    def __init__(self, obj):\n-        self._ptr = 0\n-        if isinstance(obj, Response):\n+    def __init__(self, obj: Union[TextResponse, str, bytes]):\n+        self._ptr: int = 0\n+        self._text: Union[str, bytes]\n+        if isinstance(obj, TextResponse):\n             self._text, self.encoding = obj.body, obj.encoding\n         else:\n             self._text, self.encoding = obj, \"utf-8\"\n-        self._is_unicode = isinstance(self._text, str)\n+        self._is_unicode: bool = isinstance(self._text, str)\n \n-    def read(self, n=65535):\n-        self.read = self._read_unicode if self._is_unicode else self._read_string\n+    def read(self, n: int = 65535) -> bytes:\n+        self.read: Callable[[int], bytes] = (  # type: ignore[method-assign]\n+            self._read_unicode if self._is_unicode else self._read_string\n+        )\n         return self.read(n).lstrip()\n \n-    def _read_string(self, n=65535):\n+    def _read_string(self, n: int = 65535) -> bytes:\n         s, e = self._ptr, self._ptr + n\n         self._ptr = e\n-        return self._text[s:e]\n+        return cast(bytes, self._text)[s:e]\n \n-    def _read_unicode(self, n=65535):\n+    def _read_unicode(self, n: int = 65535) -> bytes:\n         s, e = self._ptr, self._ptr + n\n         self._ptr = e\n-        return self._text[s:e].encode(\"utf-8\")\n+        return cast(str, self._text)[s:e].encode(\"utf-8\")\n \n \n-def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n+def csviter(\n+    obj: Union[Response, str, bytes],\n+    delimiter: Optional[str] = None,\n+    headers: Optional[List[str]] = None,\n+    encoding: Optional[str] = None,\n+    quotechar: Optional[str] = None,\n+) -> Generator[Dict[str, str], Any, None]:\n     \"\"\"Returns an iterator of dictionaries from the given csv object\n \n     obj can be:\n@@ -112,12 +151,12 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n \n     encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or \"utf-8\"\n \n-    def row_to_unicode(row_):\n+    def row_to_unicode(row_: Iterable) -> List[str]:\n         return [to_unicode(field, encoding) for field in row_]\n \n     lines = StringIO(_body_or_str(obj, unicode=True))\n \n-    kwargs = {}\n+    kwargs: Dict[str, Any] = {}\n     if delimiter:\n         kwargs[\"delimiter\"] = delimiter\n     if quotechar:\n@@ -147,7 +186,24 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n         yield dict(zip(headers, row))\n \n \n-def _body_or_str(obj, unicode=True):\n+@overload\n+def _body_or_str(obj: Union[Response, str, bytes]) -> str:\n+    ...\n+\n+\n+@overload\n+def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[True]) -> str:\n+    ...\n+\n+\n+@overload\n+def _body_or_str(obj: Union[Response, str, bytes], unicode: Literal[False]) -> bytes:\n+    ...\n+\n+\n+def _body_or_str(\n+    obj: Union[Response, str, bytes], unicode: bool = True\n+) -> Union[str, bytes]:\n     expected_types = (Response, str, bytes)\n     if not isinstance(obj, expected_types):\n         expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n@@ -156,10 +212,10 @@ def _body_or_str(obj, unicode=True):\n         )\n     if isinstance(obj, Response):\n         if not unicode:\n-            return obj.body\n+            return cast(bytes, obj.body)\n         if isinstance(obj, TextResponse):\n             return obj.text\n-        return obj.body.decode(\"utf-8\")\n+        return cast(bytes, obj.body).decode(\"utf-8\")\n     if isinstance(obj, str):\n         return obj if unicode else obj.encode(\"utf-8\")\n     return obj.decode(\"utf-8\") if unicode else obj\n\n@@ -1,13 +1,18 @@\n+from typing import Callable, Iterable, Union\n+\n from pytest import mark\n from twisted.trial import unittest\n \n+from scrapy import Selector\n from scrapy.http import Response, TextResponse, XmlResponse\n from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\n from tests import get_testdata\n \n \n class XmliterTestCase(unittest.TestCase):\n-    xmliter = staticmethod(xmliter)\n+    xmliter: Callable[\n+        [Union[TextResponse, str, bytes], str], Iterable[Selector]\n+    ] = staticmethod(xmliter)\n \n     def test_xmliter(self):\n         body = b\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9fe662d856a6b2496379585143aa2f4d023039f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 128 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,13 @@\n+from __future__ import annotations\n+\n import os\n import sys\n+from typing import Iterable, Optional, Tuple, cast\n \n-from twisted.internet import defer, protocol\n+from twisted.internet.defer import Deferred\n+from twisted.internet.error import ProcessTerminated\n+from twisted.internet.protocol import ProcessProtocol\n+from twisted.python.failure import Failure\n \n \n class ProcessTest:\n@@ -9,7 +15,12 @@ class ProcessTest:\n     prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n     cwd = os.getcwd()  # trial chdirs to temp dir\n \n-    def execute(self, args, check_code=True, settings=None):\n+    def execute(\n+        self,\n+        args: Iterable[str],\n+        check_code: bool = True,\n+        settings: Optional[str] = None,\n+    ) -> Deferred:\n         from twisted.internet import reactor\n \n         env = os.environ.copy()\n@@ -21,29 +32,31 @@ class ProcessTest:\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n-    def _process_finished(self, pp, cmd, check_code):\n+    def _process_finished(\n+        self, pp: TestProcessProtocol, cmd: str, check_code: bool\n+    ) -> Tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\n-            msg += f\"\\n>>> stdout <<<\\n{pp.out}\"\n+            msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n             msg += \"\\n\"\n-            msg += f\"\\n>>> stderr <<<\\n{pp.err}\"\n+            msg += f\"\\n>>> stderr <<<\\n{pp.err.decode()}\"\n             raise RuntimeError(msg)\n-        return pp.exitcode, pp.out, pp.err\n+        return cast(int, pp.exitcode), pp.out, pp.err\n \n \n-class TestProcessProtocol(protocol.ProcessProtocol):\n-    def __init__(self):\n-        self.deferred = defer.Deferred()\n-        self.out = b\"\"\n-        self.err = b\"\"\n-        self.exitcode = None\n+class TestProcessProtocol(ProcessProtocol):\n+    def __init__(self) -> None:\n+        self.deferred: Deferred = Deferred()\n+        self.out: bytes = b\"\"\n+        self.err: bytes = b\"\"\n+        self.exitcode: Optional[int] = None\n \n-    def outReceived(self, data):\n+    def outReceived(self, data: bytes) -> None:\n         self.out += data\n \n-    def errReceived(self, data):\n+    def errReceived(self, data: bytes) -> None:\n         self.err += data\n \n-    def processEnded(self, status):\n-        self.exitcode = status.value.exitCode\n+    def processEnded(self, status: Failure) -> None:\n+        self.exitcode = cast(ProcessTerminated, status.value).exitCode\n         self.deferred.callback(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66bad1150cf0a72b39d1af8a63d7b4eb7c1f42fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 276 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 0.8\n\nDIFF:\n@@ -23,7 +23,10 @@ logger = logging.getLogger(__name__)\n \n \n def send_catch_log(\n-    signal=Any, sender=Anonymous, *arguments, **named\n+    signal: TypingAny = Any,\n+    sender: TypingAny = Anonymous,\n+    *arguments: TypingAny,\n+    **named: TypingAny\n ) -> List[Tuple[TypingAny, TypingAny]]:\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n@@ -65,13 +68,18 @@ def send_catch_log(\n     return responses\n \n \n-def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n+def send_catch_log_deferred(\n+    signal: TypingAny = Any,\n+    sender: TypingAny = Anonymous,\n+    *arguments: TypingAny,\n+    **named: TypingAny\n+) -> Deferred:\n     \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n     Returns a deferred that gets fired once all signal handlers deferreds were\n     fired.\n     \"\"\"\n \n-    def logerror(failure, recv):\n+    def logerror(failure: Failure, recv: Any) -> Failure:\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\n                 \"Error caught on signal handler: %(receiver)s\",\n@@ -96,7 +104,7 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n     return d\n \n \n-def disconnect_all(signal=Any, sender=Any):\n+def disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n     \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n     tests\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#518e56046e45d38c505db9e2bc00677ad08ac58c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 28 | Churn Cumulative: 1084 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,8 @@ import re\n import sys\n from functools import wraps\n from inspect import getmembers\n-from typing import Dict\n+from types import CoroutineType\n+from typing import AsyncGenerator, Dict\n from unittest import TestCase\n \n from scrapy.http import Request\n@@ -37,7 +38,10 @@ class Contract:\n                 else:\n                     results.addSuccess(self.testcase_pre)\n                 finally:\n-                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n+                    cb_result = cb(response, **cb_kwargs)\n+                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                        raise TypeError(\"Contracts don't support async callbacks\")\n+                    return list(iterate_spider_output(cb_result))\n \n             request.callback = wrapper\n \n@@ -49,7 +53,10 @@ class Contract:\n \n             @wraps(cb)\n             def wrapper(response, **cb_kwargs):\n-                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n+                cb_result = cb(response, **cb_kwargs)\n+                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                    raise TypeError(\"Contracts don't support async callbacks\")\n+                output = list(iterate_spider_output(cb_result))\n                 try:\n                     results.startTest(self.testcase_post)\n                     self.post_process(output)\n\n@@ -63,6 +63,13 @@ class TestSpider(Spider):\n         \"\"\"\n         return Request(\"http://scrapy.org\", callback=self.returns_item)\n \n+    async def returns_request_async(self, response):\n+        \"\"\"async method which returns request\n+        @url http://scrapy.org\n+        @returns requests 1\n+        \"\"\"\n+        return Request(\"http://scrapy.org\", callback=self.returns_item)\n+\n     def returns_item(self, response):\n         \"\"\"method which returns item\n         @url http://scrapy.org\n@@ -337,6 +344,14 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response)\n         self.should_fail()\n \n+    def test_returns_async(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        request = self.conman.from_method(spider.returns_request_async, self.results)\n+        request.callback(response)\n+        self.should_error()\n+\n     def test_scrapes(self):\n         spider = TestSpider()\n         response = ResponseMock()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2adec629b63e9d7735efd58b4353dcfe7ab2863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 17 | Files Changed: 7 | Hunks: 30 | Methods Changed: 18 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 108 | Churn Cumulative: 2396 | Contributors (this commit): 43 | Commits (past 90d): 11 | Contributors (cumulative): 99 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ Base class for Scrapy commands\n import argparse\n import os\n from pathlib import Path\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, List, Optional\n \n from twisted.python import failure\n \n@@ -116,7 +116,7 @@ class ScrapyCommand:\n         if opts.pdb:\n             failure.startDebugMode()\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         \"\"\"\n         Entry point for running commands\n         \"\"\"\n\n@@ -1,7 +1,10 @@\n import sys\n+from argparse import Namespace\n+from typing import List, Type\n \n from w3lib.url import is_url\n \n+from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.http import Request\n@@ -57,7 +60,7 @@ class Command(ScrapyCommand):\n     def _print_bytes(self, bytes_):\n         sys.stdout.buffer.write(bytes_ + b\"\\n\")\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: Namespace) -> None:\n         if len(args) != 1 or not is_url(args[0]):\n             raise UsageError()\n         request = Request(\n@@ -73,7 +76,8 @@ class Command(ScrapyCommand):\n         else:\n             request.meta[\"handle_httpstatus_all\"] = True\n \n-        spidercls = DefaultSpider\n+        spidercls: Type[Spider] = DefaultSpider\n+        assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n         if opts.spider:\n             spidercls = spider_loader.load(opts.spider)\n\n@@ -3,8 +3,11 @@ Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n \"\"\"\n+from argparse import Namespace\n from threading import Thread\n+from typing import List, Type\n \n+from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.shell import Shell\n@@ -54,15 +57,16 @@ class Command(ScrapyCommand):\n         \"\"\"\n         pass\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: Namespace) -> None:\n         url = args[0] if args else None\n         if url:\n             # first argument may be a local file\n             url = guess_scheme(url)\n \n+        assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n \n-        spidercls = DefaultSpider\n+        spidercls: Type[Spider] = DefaultSpider\n         if opts.spider:\n             spidercls = spider_loader.load(opts.spider)\n         elif url:\n\n@@ -4,10 +4,11 @@ discovering (through HTTP headers) to base Response class.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+from __future__ import annotations\n \n import json\n from contextlib import suppress\n-from typing import Generator, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Generator, Optional, Tuple\n from urllib.parse import urljoin\n \n import parsel\n@@ -25,6 +26,9 @@ from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n+if TYPE_CHECKING:\n+    from scrapy.selector import Selector\n+\n _NONE = object()\n \n \n@@ -34,11 +38,11 @@ class TextResponse(Response):\n \n     attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n \n-    def __init__(self, *args, **kwargs):\n+    def __init__(self, *args: Any, **kwargs: Any):\n         self._encoding = kwargs.pop(\"encoding\", None)\n-        self._cached_benc = None\n+        self._cached_benc: Optional[str] = None\n         self._cached_ubody: Optional[str] = None\n-        self._cached_selector = None\n+        self._cached_selector: Optional[Selector] = None\n         super().__init__(*args, **kwargs)\n \n     def _set_url(self, url):\n\n@@ -27,7 +27,7 @@ class SpiderLoader:\n         self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n         self._load_all_spiders()\n \n-    def _check_name_duplicates(self):\n+    def _check_name_duplicates(self) -> None:\n         dupes = []\n         for name, locations in self._found.items():\n             dupes.extend(\n\n@@ -1,7 +1,13 @@\n import signal\n-from typing import Callable\n+from types import FrameType\n+from typing import Any, Callable, Dict, Optional, Union\n \n-signal_names = {}\n+# copy of _HANDLER from typeshed/stdlib/signal.pyi\n+SignalHandlerT = Union[\n+    Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None\n+]\n+\n+signal_names: Dict[int, str] = {}\n for signame in dir(signal):\n     if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n         signum = getattr(signal, signame)\n@@ -9,7 +15,9 @@ for signame in dir(signal):\n             signal_names[signum] = signame\n \n \n-def install_shutdown_handlers(function: Callable, override_sigint: bool = True) -> None:\n+def install_shutdown_handlers(\n+    function: SignalHandlerT, override_sigint: bool = True\n+) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n     signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n     SIGINT handler won't be install if there is already a handler in place\n\n@@ -2,16 +2,19 @@ from __future__ import annotations\n \n import inspect\n import logging\n-from types import ModuleType\n+from types import CoroutineType, ModuleType\n from typing import (\n     TYPE_CHECKING,\n     Any,\n-    AsyncIterable,\n+    AsyncGenerator,\n     Generator,\n     Iterable,\n+    Literal,\n     Optional,\n     Type,\n+    TypeVar,\n     Union,\n+    overload,\n )\n \n from twisted.internet.defer import Deferred\n@@ -26,8 +29,26 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n \n-def iterate_spider_output(result: Any) -> Union[Iterable, AsyncIterable, Deferred]:\n+\n+# https://stackoverflow.com/questions/60222982\n+@overload\n+def iterate_spider_output(result: AsyncGenerator) -> AsyncGenerator:  # type: ignore[misc]\n+    ...\n+\n+\n+@overload\n+def iterate_spider_output(result: CoroutineType) -> Deferred:\n+    ...\n+\n+\n+@overload\n+def iterate_spider_output(result: _T) -> Iterable:\n+    ...\n+\n+\n+def iterate_spider_output(result: Any) -> Union[Iterable, AsyncGenerator, Deferred]:\n     if inspect.isasyncgen(result):\n         return result\n     if inspect.iscoroutine(result):\n@@ -55,6 +76,39 @@ def iter_spider_classes(module: ModuleType) -> Generator[Type[Spider], Any, None\n             yield obj\n \n \n+@overload\n+def spidercls_for_request(\n+    spider_loader: SpiderLoader,\n+    request: Request,\n+    default_spidercls: Type[Spider],\n+    log_none: bool = ...,\n+    log_multiple: bool = ...,\n+) -> Type[Spider]:\n+    ...\n+\n+\n+@overload\n+def spidercls_for_request(\n+    spider_loader: SpiderLoader,\n+    request: Request,\n+    default_spidercls: Literal[None],\n+    log_none: bool = ...,\n+    log_multiple: bool = ...,\n+) -> Optional[Type[Spider]]:\n+    ...\n+\n+\n+@overload\n+def spidercls_for_request(\n+    spider_loader: SpiderLoader,\n+    request: Request,\n+    *,\n+    log_none: bool = ...,\n+    log_multiple: bool = ...,\n+) -> Optional[Type[Spider]]:\n+    ...\n+\n+\n def spidercls_for_request(\n     spider_loader: SpiderLoader,\n     request: Request,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f5f593e5f5e2b0c216e9d6fd41f4260f70c74d34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 470 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -42,8 +42,7 @@ def get_meta_refresh(\n     \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n-        # a w3lib typing bug here, fixed in https://github.com/scrapy/w3lib/pull/211\n-        _metaref_cache[response] = html.get_meta_refresh(  # type: ignore[assignment]\n+        _metaref_cache[response] = html.get_meta_refresh(\n             text, response.url, response.encoding, ignore_tags=ignore_tags\n         )\n     return _metaref_cache[response]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#471281d29e5c7b8e293f59ac3c3330ecda687d19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 1497 | Contributors (this commit): 29 | Commits (past 90d): 5 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-from __future__ import annotations\n-\n import csv\n import logging\n import re\n@@ -78,7 +76,7 @@ def xmliter(\n \n \n def xmliter_lxml(\n-    obj: Union[TextResponse, str, bytes],\n+    obj: Union[Response, str, bytes],\n     nodename: str,\n     namespace: Optional[str] = None,\n     prefix: str = \"x\",\n@@ -87,8 +85,9 @@ def xmliter_lxml(\n \n     reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n+    # technically, etree.iterparse only needs .read() AFAICS, but this is how it's typed\n     iterable = etree.iterparse(\n-        cast(SupportsReadClose[bytes], reader), tag=tag, encoding=reader.encoding\n+        cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     for _, node in iterable:\n@@ -101,11 +100,13 @@ def xmliter_lxml(\n \n \n class _StreamReader:\n-    def __init__(self, obj: Union[TextResponse, str, bytes]):\n+    def __init__(self, obj: Union[Response, str, bytes]):\n         self._ptr: int = 0\n         self._text: Union[str, bytes]\n         if isinstance(obj, TextResponse):\n             self._text, self.encoding = obj.body, obj.encoding\n+        elif isinstance(obj, Response):\n+            self._text, self.encoding = obj.body, \"utf-8\"\n         else:\n             self._text, self.encoding = obj, \"utf-8\"\n         self._is_unicode: bool = isinstance(self._text, str)\n\n@@ -1,18 +1,13 @@\n-from typing import Callable, Iterable, Union\n-\n from pytest import mark\n from twisted.trial import unittest\n \n-from scrapy import Selector\n from scrapy.http import Response, TextResponse, XmlResponse\n from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\n from tests import get_testdata\n \n \n class XmliterTestCase(unittest.TestCase):\n-    xmliter: Callable[\n-        [Union[TextResponse, str, bytes], str], Iterable[Selector]\n-    ] = staticmethod(xmliter)\n+    xmliter = staticmethod(xmliter)\n \n     def test_xmliter(self):\n         body = b\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#644a71bfd40be3ce8a465ad49891d34c47516f56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 7275 | Contributors (this commit): 45 | Commits (past 90d): 5 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -2978,8 +2978,8 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     def test_init(self):\n         settings_dict = {\n-            \"FEED_URI\": \"file:///tmp/foobar\",\n-            \"FEED_STORAGES\": {\"file\": FTPFeedStorageWithoutFeedOptions},\n+            \"FEED_URI\": \"ftp://localhost/foo\",\n+            \"FEED_STORAGES\": {\"ftp\": FTPFeedStorageWithoutFeedOptions},\n         }\n         with pytest.warns(\n             ScrapyDeprecationWarning,\n@@ -3000,8 +3000,8 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     def test_from_crawler(self):\n         settings_dict = {\n-            \"FEED_URI\": \"file:///tmp/foobar\",\n-            \"FEED_STORAGES\": {\"file\": FTPFeedStorageWithoutFeedOptionsWithFromCrawler},\n+            \"FEED_URI\": \"ftp://localhost/foo\",\n+            \"FEED_STORAGES\": {\"ftp\": FTPFeedStorageWithoutFeedOptionsWithFromCrawler},\n         }\n         with pytest.warns(\n             ScrapyDeprecationWarning,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23af21491d526072873ab7ec3d1429560861964f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 644 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -122,6 +122,9 @@ def configure_logging(\n         install_scrapy_root_handler(settings)\n \n \n+_scrapy_root_handler: Optional[logging.Handler] = None\n+\n+\n def install_scrapy_root_handler(settings: Settings) -> None:\n     global _scrapy_root_handler\n \n@@ -139,9 +142,6 @@ def get_scrapy_root_handler() -> Optional[logging.Handler]:\n     return _scrapy_root_handler\n \n \n-_scrapy_root_handler: Optional[logging.Handler] = None\n-\n-\n def _get_handler(settings: Settings) -> logging.Handler:\n     \"\"\"Return a log handler object according to settings\"\"\"\n     filename = settings.get(\"LOG_FILE\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53539483c32dec537064628c0c6e407eb8ce1c2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 608 | Contributors (this commit): 24 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -110,12 +110,17 @@ class _StreamReader:\n         else:\n             self._text, self.encoding = obj, \"utf-8\"\n         self._is_unicode: bool = isinstance(self._text, str)\n+        self._is_first_read: bool = True\n \n     def read(self, n: int = 65535) -> bytes:\n-        self.read: Callable[[int], bytes] = (  # type: ignore[method-assign]\n+        method: Callable[[int], bytes] = (\n             self._read_unicode if self._is_unicode else self._read_string\n         )\n-        return self.read(n).lstrip()\n+        result = method(n)\n+        if self._is_first_read:\n+            self._is_first_read = False\n+            result = result.lstrip()\n+        return result\n \n     def _read_string(self, n: int = 65535) -> bytes:\n         s, e = self._ptr, self._ptr + n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8050257c1495fe405767dcd37c4db0f6f29c38aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2411 | Contributors (this commit): 53 | Commits (past 90d): 13 | Contributors (cumulative): 66 | DMM Complexity: 0.0\n\nDIFF:\n@@ -290,7 +290,8 @@ class FTPFeedStorage(BlockingFeedStorage):\n         feed_options: Optional[Dict[str, Any]] = None,\n     ):\n         u = urlparse(uri)\n-        assert u.hostname\n+        if not u.hostname:\n+            raise ValueError(f\"Got a storage URI without a hostname: {uri}\")\n         self.host: str = u.hostname\n         self.port: int = int(u.port or \"21\")\n         self.username: str = u.username or \"\"\n\n@@ -85,7 +85,6 @@ def xmliter_lxml(\n \n     reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n-    # technically, etree.iterparse only needs .read() AFAICS, but this is how it's typed\n     iterable = etree.iterparse(\n         cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#084a9ba0768c34e2c0b82e13c7182da33460a19b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 52 | Files Changed: 5 | Hunks: 57 | Methods Changed: 56 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 166 | Churn Cumulative: 3376 | Contributors (this commit): 44 | Commits (past 90d): 15 | Contributors (cumulative): 85 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,9 +4,14 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Set, Type, Union\n+from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union\n \n-from twisted.internet import defer\n+from twisted.internet.defer import (\n+    Deferred,\n+    DeferredList,\n+    inlineCallbacks,\n+    maybeDeferred,\n+)\n from zope.interface.exceptions import DoesNotImplement\n \n try:\n@@ -24,7 +29,7 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n from scrapy.logformatter import LogFormatter\n-from scrapy.settings import Settings, overridden_settings\n+from scrapy.settings import BaseSettings, Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n from scrapy.spiderloader import SpiderLoader\n from scrapy.statscollectors import StatsCollector\n@@ -123,8 +128,8 @@ class Crawler:\n         self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n-    @defer.inlineCallbacks\n-    def crawl(self, *args, **kwargs):\n+    @inlineCallbacks\n+    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n         if self.crawling:\n             raise RuntimeError(\"Crawling already taking place\")\n         self.crawling = True\n@@ -134,26 +139,27 @@ class Crawler:\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n             yield self.engine.open_spider(self.spider, start_requests)\n-            yield defer.maybeDeferred(self.engine.start)\n+            yield maybeDeferred(self.engine.start)\n         except Exception:\n             self.crawling = False\n             if self.engine is not None:\n                 yield self.engine.close()\n             raise\n \n-    def _create_spider(self, *args, **kwargs):\n+    def _create_spider(self, *args: Any, **kwargs: Any) -> Spider:\n         return self.spidercls.from_crawler(self, *args, **kwargs)\n \n-    def _create_engine(self):\n+    def _create_engine(self) -> ExecutionEngine:\n         return ExecutionEngine(self, lambda _: self.stop())\n \n-    @defer.inlineCallbacks\n-    def stop(self):\n+    @inlineCallbacks\n+    def stop(self) -> Generator[Deferred, Any, None]:\n         \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n         fired when the crawler is stopped.\"\"\"\n         if self.crawling:\n             self.crawling = False\n-            yield defer.maybeDeferred(self.engine.stop)\n+            assert self.engine\n+            yield maybeDeferred(self.engine.stop)\n \n \n class CrawlerRunner:\n@@ -176,10 +182,10 @@ class CrawlerRunner:\n     )\n \n     @staticmethod\n-    def _get_spider_loader(settings) -> SpiderLoader:\n+    def _get_spider_loader(settings: BaseSettings) -> SpiderLoader:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n         cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n-        loader_cls = load_object(cls_path)\n+        loader_cls: Type[SpiderLoader] = load_object(cls_path)\n         excs = (\n             (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n         )\n@@ -201,11 +207,11 @@ class CrawlerRunner:\n         self.settings = settings\n         self.spider_loader = self._get_spider_loader(settings)\n         self._crawlers: Set[Crawler] = set()\n-        self._active: Set[defer.Deferred] = set()\n+        self._active: Set[Deferred] = set()\n         self.bootstrap_failed = False\n \n     @property\n-    def spiders(self):\n+    def spiders(self) -> SpiderLoader:\n         warnings.warn(\n             \"CrawlerRunner.spiders attribute is renamed to \"\n             \"CrawlerRunner.spider_loader.\",\n@@ -214,7 +220,12 @@ class CrawlerRunner:\n         )\n         return self.spider_loader\n \n-    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n+    def crawl(\n+        self,\n+        crawler_or_spidercls: Union[Type[Spider], str, Crawler],\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Deferred:\n         \"\"\"\n         Run a crawler with the provided arguments.\n \n@@ -244,12 +255,12 @@ class CrawlerRunner:\n         crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n-    def _crawl(self, crawler, *args, **kwargs):\n+    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred:\n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n         self._active.add(d)\n \n-        def _done(result):\n+        def _done(result: Any) -> Any:\n             self.crawlers.discard(crawler)\n             self._active.discard(d)\n             self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n@@ -284,16 +295,16 @@ class CrawlerRunner:\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n-    def stop(self):\n+    def stop(self) -> Deferred:\n         \"\"\"\n         Stops simultaneously all the crawling jobs taking place.\n \n         Returns a deferred that is fired when they all have ended.\n         \"\"\"\n-        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n+        return DeferredList([c.stop() for c in list(self.crawlers)])\n \n-    @defer.inlineCallbacks\n-    def join(self):\n+    @inlineCallbacks\n+    def join(self) -> Generator[Deferred, Any, None]:\n         \"\"\"\n         join()\n \n@@ -301,7 +312,7 @@ class CrawlerRunner:\n         completed their executions.\n         \"\"\"\n         while self._active:\n-            yield defer.DeferredList(self._active)\n+            yield DeferredList(self._active)\n \n \n class CrawlerProcess(CrawlerRunner):\n@@ -328,13 +339,17 @@ class CrawlerProcess(CrawlerRunner):\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    def __init__(self, settings=None, install_root_handler=True):\n+    def __init__(\n+        self,\n+        settings: Union[Dict[str, Any], Settings, None] = None,\n+        install_root_handler: bool = True,\n+    ):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n         self._initialized_reactor = False\n \n-    def _signal_shutdown(self, signum, _):\n+    def _signal_shutdown(self, signum: int, _: Any) -> None:\n         from twisted.internet import reactor\n \n         install_shutdown_handlers(self._signal_kill)\n@@ -345,7 +360,7 @@ class CrawlerProcess(CrawlerRunner):\n         )\n         reactor.callFromThread(self._graceful_stop_reactor)\n \n-    def _signal_kill(self, signum, _):\n+    def _signal_kill(self, signum: int, _: Any) -> None:\n         from twisted.internet import reactor\n \n         install_shutdown_handlers(signal.SIG_IGN)\n@@ -355,14 +370,16 @@ class CrawlerProcess(CrawlerRunner):\n         )\n         reactor.callFromThread(self._stop_reactor)\n \n-    def _create_crawler(self, spidercls):\n+    def _create_crawler(self, spidercls: Union[Type[Spider], str]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         init_reactor = not self._initialized_reactor\n         self._initialized_reactor = True\n         return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n-    def start(self, stop_after_crawl=True, install_signal_handlers=True):\n+    def start(\n+        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n+    ) -> None:\n         \"\"\"\n         This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n         size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n@@ -396,12 +413,12 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n-    def _graceful_stop_reactor(self):\n+    def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\n         d.addBoth(self._stop_reactor)\n         return d\n \n-    def _stop_reactor(self, _=None):\n+    def _stop_reactor(self, _: Any = None) -> None:\n         from twisted.internet import reactor\n \n         try:\n\n@@ -1,6 +1,8 @@\n+from __future__ import annotations\n+\n import logging\n import os\n-from typing import Any, Dict, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n \n from twisted.python.failure import Failure\n \n@@ -8,6 +10,13 @@ from scrapy import Request, Spider\n from scrapy.http import Response\n from scrapy.utils.request import referer_str\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n+\n SCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n CRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n@@ -161,5 +170,5 @@ class LogFormatter:\n         }\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n\n@@ -1,7 +1,21 @@\n+from __future__ import annotations\n+\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Any, Callable, Deque, Dict, Iterable, List, Tuple, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Deque,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet.defer import Deferred\n \n@@ -11,6 +25,13 @@ from scrapy.settings import Settings\n from scrapy.utils.defer import process_chain, process_parallel\n from scrapy.utils.misc import create_instance, load_object\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -34,7 +55,9 @@ class MiddlewareManager:\n         raise NotImplementedError\n \n     @classmethod\n-    def from_settings(cls, settings: Settings, crawler=None):\n+    def from_settings(\n+        cls, settings: Settings, crawler: Optional[Crawler] = None\n+    ) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -63,7 +86,7 @@ class MiddlewareManager:\n         return cls(*middlewares)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls.from_settings(crawler.settings, crawler)\n \n     def _add_middleware(self, mw: Any) -> None:\n\n@@ -1,8 +1,10 @@\n+from __future__ import annotations\n+\n import traceback\n import warnings\n from collections import defaultdict\n from types import ModuleType\n-from typing import DefaultDict, Dict, List, Tuple, Type\n+from typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n \n from zope.interface import implementer\n \n@@ -12,6 +14,10 @@ from scrapy.settings import BaseSettings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n @implementer(ISpiderLoader)\n class SpiderLoader:\n@@ -69,7 +75,7 @@ class SpiderLoader:\n         self._check_name_duplicates()\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(settings)\n \n     def load(self, spider_name: str) -> Type[Spider]:\n\n@@ -6,15 +6,21 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Union, cast\n+\n+from twisted.internet.defer import Deferred\n \n from scrapy import signals\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n     from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n \n \n class Spider(object_ref):\n@@ -25,21 +31,21 @@ class Spider(object_ref):\n     name: str\n     custom_settings: Optional[dict] = None\n \n-    def __init__(self, name=None, **kwargs):\n+    def __init__(self, name: Optional[str] = None, **kwargs: Any):\n         if name is not None:\n             self.name = name\n         elif not getattr(self, \"name\", None):\n             raise ValueError(f\"{type(self).__name__} must have a name\")\n         self.__dict__.update(kwargs)\n         if not hasattr(self, \"start_urls\"):\n-            self.start_urls = []\n+            self.start_urls: List[str] = []\n \n     @property\n-    def logger(self):\n+    def logger(self) -> logging.LoggerAdapter:\n         logger = logging.getLogger(self.name)\n         return logging.LoggerAdapter(logger, {\"spider\": self})\n \n-    def log(self, message, level=logging.DEBUG, **kw):\n+    def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n         \"\"\"Log the given message at the given log level\n \n         This helper wraps a log call to the logger within the spider, but you\n@@ -49,17 +55,17 @@ class Spider(object_ref):\n         self.logger.log(level, message, **kw)\n \n     @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs):\n+    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n         spider = cls(*args, **kwargs)\n         spider._set_crawler(crawler)\n         return spider\n \n-    def _set_crawler(self, crawler: Crawler):\n+    def _set_crawler(self, crawler: Crawler) -> None:\n         self.crawler = crawler\n         self.settings = crawler.settings\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n-    def start_requests(self):\n+    def start_requests(self) -> Iterable[Request]:\n         if not self.start_urls and hasattr(self, \"start_url\"):\n             raise AttributeError(\n                 \"Crawling could not start: 'start_urls' not found \"\n@@ -69,16 +75,16 @@ class Spider(object_ref):\n         for url in self.start_urls:\n             yield Request(url, dont_filter=True)\n \n-    def _parse(self, response, **kwargs):\n+    def _parse(self, response: Response, **kwargs: Any) -> Any:\n         return self.parse(response, **kwargs)\n \n-    def parse(self, response, **kwargs):\n+    def parse(self, response: Response, **kwargs: Any) -> Any:\n         raise NotImplementedError(\n             f\"{self.__class__.__name__}.parse callback is not defined\"\n         )\n \n     @classmethod\n-    def update_settings(cls, settings):\n+    def update_settings(cls, settings: BaseSettings) -> None:\n         settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n \n     @classmethod\n@@ -86,12 +92,13 @@ class Spider(object_ref):\n         return url_is_from_spider(request.url, cls)\n \n     @staticmethod\n-    def close(spider, reason):\n+    def close(spider: Spider, reason: str) -> Union[Deferred, None]:\n         closed = getattr(spider, \"closed\", None)\n         if callable(closed):\n-            return closed(reason)\n+            return cast(Union[Deferred, None], closed(reason))\n+        return None\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
