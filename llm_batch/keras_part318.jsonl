{"custom_id": "keras#2112489ae10a60ccabe68960c7d20db9d5ba03a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 4 | Files Changed: 5 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 47 | Churn Cumulative: 15046 | Contributors (this commit): 35 | Commits (past 90d): 23 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1450,10 +1450,7 @@ def ndim(x):\n   2\n \n   \"\"\"\n-  dims = x.shape._dims\n-  if dims is not None:\n-    return len(dims)\n-  return None\n+  return x.shape.rank\n \n \n @keras_export('keras.backend.dtype')\n\n@@ -70,6 +70,9 @@ class LeakyReLU(Layer):\n \n   def __init__(self, alpha=0.3, **kwargs):\n     super(LeakyReLU, self).__init__(**kwargs)\n+    if alpha is None:\n+      raise ValueError('alpha of leaky Relu layer '\n+                       'cannot be None. Required a float')\n     self.supports_masking = True\n     self.alpha = K.cast_to_floatx(alpha)\n \n@@ -205,6 +208,8 @@ class ELU(Layer):\n \n   def __init__(self, alpha=1.0, **kwargs):\n     super(ELU, self).__init__(**kwargs)\n+    if alpha is None:\n+      raise ValueError('alpha of ELU layer ' 'cannot be None. Required a float')\n     self.supports_masking = True\n     self.alpha = K.cast_to_floatx(alpha)\n \n\n@@ -108,6 +108,26 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(np.ones((10, 10)), np.ones((10, 1)), batch_size=2)\n \n+  def test_leaky_relu_with_invalid_alpha(self):\n+    # Test case for GitHub issue 46993.\n+    with self.assertRaisesRegex(ValueError,\n+                                'alpha of leaky Relu layer cannot be None'):\n+      testing_utils.layer_test(\n+          keras.layers.LeakyReLU,\n+          kwargs={'alpha': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+  def test_leaky_elu_with_invalid_alpha(self):\n+    # Test case for GitHub issue 46993.\n+    with self.assertRaisesRegex(ValueError,\n+                                'alpha of ELU layer cannot be None'):\n+      testing_utils.layer_test(\n+          keras.layers.ELU,\n+          kwargs={'alpha': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -1282,6 +1282,12 @@ def mean_absolute_error(y_true, y_pred):\n   return K.mean(tf.abs(y_pred - y_true), axis=-1)\n \n \n+@dispatch.dispatch_for_types(mean_absolute_error, tf.RaggedTensor)\n+def _ragged_tensor_mae(y_true, y_pred):\n+  \"\"\" RaggedTensor adapter for mean_absolute_error\"\"\"\n+  return _ragged_tensor_apply_loss(mean_absolute_error, y_true, y_pred)\n+\n+\n @keras_export('keras.metrics.mean_absolute_percentage_error',\n               'keras.metrics.mape', 'keras.metrics.MAPE',\n               'keras.losses.mean_absolute_percentage_error',\n\n@@ -493,6 +493,17 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n     loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 25.29999, 3)\n \n+  def test_ragged_tensor(self):\n+    mae_obj = losses.MeanAbsoluteError()\n+    y_true = tf.ragged.constant([[1, 9, 2], [-5, -2]],\n+                                         dtype=tf.float32)\n+    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]],\n+                                         dtype=tf.float32)\n+    # loss = [14/3, 16/2]\n+    sample_weight = tf.constant([1.2, 1.0], shape=(2, 1))\n+    loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+    self.assertAlmostEqual(self.evaluate(loss), 6.8, 5)\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc3c84a3a722239faf06e2d73a330bc66b306b2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 184 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 190 | Churn Cumulative: 8460 | Contributors (this commit): 89 | Commits (past 90d): 16 | Contributors (cumulative): 94 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1610,9 +1610,6 @@ class BackupAndRestore(Callback):\n     # restore checkpoint at on_train_begin().\n     self._chief_worker_only = False\n \n-  def set_model(self, model):\n-    self.model = model\n-\n   def on_train_begin(self, logs=None):\n     # TrainingState is used to manage the training state needed for\n     # failure-recovery of a worker in training.\n\n@@ -21,9 +21,13 @@ from __future__ import print_function\n import tensorflow as tf\n \n import numpy as np\n+\n+import keras\n from tensorflow.python.eager import backprop\n from keras.engine import training as keras_training\n from keras.layers import core as keras_core\n+from keras.optimizer_v2 import rmsprop\n+from keras.utils import kpl_test_utils\n from tensorflow.python.training import optimizer as optimizer_lib\n \n \n@@ -77,6 +81,51 @@ class MirroredStrategyDefunTest(tf.test.TestCase):\n       self.assertAllEqual(0.5 * np.ones([10, 1]), updated_var_values[0])\n       self.assertAllEqual([0.5], updated_var_values[1])\n \n+  def testTrainAndServeWithKPL(self, distribution):\n+    use_adapt = False\n+    test_utils_obj = kpl_test_utils.DistributeKplTestUtils()\n+    with distribution.scope():\n+      feature_mapper, label_mapper = test_utils_obj.define_kpls_for_training(\n+          use_adapt)\n+      model = test_utils_obj.define_model()\n+      optimizer = rmsprop.RMSprop(learning_rate=0.1)\n+      accuracy = keras.metrics.Accuracy()\n+\n+      def dataset_fn(_):\n+        return test_utils_obj.dataset_fn(feature_mapper, label_mapper)\n+\n+      @tf.function\n+      def train_step(iterator):\n+        \"\"\"The step function for one training step.\"\"\"\n+\n+        def step_fn(inputs):\n+          \"\"\"The computation to run on each TPU device.\"\"\"\n+          features, labels = inputs\n+          with tf.GradientTape() as tape:\n+            pred = model(features, training=True)\n+            loss = keras.losses.binary_crossentropy(labels, pred)\n+            loss = tf.nn.compute_average_loss(loss)\n+          grads = tape.gradient(loss, model.trainable_variables)\n+          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n+\n+          actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n+          accuracy.update_state(labels, actual_pred)\n+\n+        distribution.run(step_fn, args=(next(iterator),))\n+\n+      distributed_dataset = distribution.distribute_datasets_from_function(\n+          dataset_fn)\n+      distributed_iterator = iter(distributed_dataset)\n+      num_epochs = 4\n+      num_steps = 7\n+      for _ in range(num_epochs):\n+        accuracy.reset_states()\n+        for _ in range(num_steps):\n+          train_step(distributed_iterator)\n+\n+      self.assertGreater(accuracy.result().numpy(), 0.5)\n+      self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)\n+\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -194,8 +194,11 @@ class CenterCropTest(keras_parameterized.TestCase):\n                                   ('center_crop_10_by_8', 10, 8),\n                                   ('center_crop_10_by_12', 10, 12))\n   def test_invalid_center_crop(self, expected_height, expected_width):\n-    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n-                                r'assertion failed'):\n+    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n+    # is enabled.\n+    with self.assertRaisesRegex(\n+        (tf.errors.InvalidArgumentError, tf.errors.InternalError),\n+        r'assertion failed|error: \\'tf.Slice\\' op'):\n       self._run_test(expected_height, expected_width)\n \n   def test_config_with_custom_name(self):\n@@ -227,7 +230,9 @@ class RandomCropTest(keras_parameterized.TestCase):\n                                   ('random_crop_10_by_8', 10, 8),\n                                   ('random_crop_10_by_12', 10, 12))\n   def test_invalid_random_crop(self, expected_height, expected_width):\n-    with self.assertRaises(tf.errors.InvalidArgumentError):\n+    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n+    # is enabled.\n+    with self.assertRaises((tf.errors.InvalidArgumentError, tf.errors.InternalError)):\n       with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n         self._run_test(expected_height, expected_width)\n \n\n@@ -0,0 +1,127 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test related utilities for KPL + tf.distribute.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+import random\n+\n+import keras\n+from keras.layers.preprocessing import string_lookup\n+\n+\n+class DistributeKplTestUtils:\n+  \"\"\"Utils for test of tf.distribute + KPL.\"\"\"\n+  FEATURE_VOCAB = [\n+      \"avenger\", \"ironman\", \"batman\", \"hulk\", \"spiderman\", \"kingkong\",\n+      \"wonder_woman\"\n+  ]\n+  LABEL_VOCAB = [\"yes\", \"no\"]\n+\n+  def define_kpls_for_training(self, use_adapt):\n+    \"\"\"Function that defines KPL used for unit tests of tf.distribute.\n+\n+    Args:\n+      use_adapt: if adapt will be called. False means there will be precomputed\n+        statistics.\n+\n+    Returns:\n+      feature_mapper: a simple keras model with one keras StringLookup layer\n+      which maps feature to index.\n+      label_mapper: similar to feature_mapper, but maps label to index.\n+\n+    \"\"\"\n+    if use_adapt:\n+      feature_lookup_layer = (\n+          string_lookup.StringLookup(\n+              num_oov_indices=1))\n+      feature_lookup_layer.adapt(self.FEATURE_VOCAB)\n+      label_lookup_layer = (\n+          string_lookup.StringLookup(\n+              num_oov_indices=0, mask_token=None))\n+      label_lookup_layer.adapt(self.LABEL_VOCAB)\n+    else:\n+      feature_lookup_layer = (\n+          string_lookup.StringLookup(\n+              vocabulary=self.FEATURE_VOCAB, num_oov_indices=1))\n+      label_lookup_layer = (\n+          string_lookup.StringLookup(\n+              vocabulary=self.LABEL_VOCAB, num_oov_indices=0, mask_token=None))\n+\n+    raw_feature_input = keras.layers.Input(\n+        shape=(3,), dtype=tf.string, name=\"feature\", ragged=True)\n+    feature_id_input = feature_lookup_layer(raw_feature_input)\n+    feature_mapper = keras.Model({\"features\": raw_feature_input},\n+                                 feature_id_input)\n+\n+    raw_label_input = keras.layers.Input(\n+        shape=(1,), dtype=tf.string, name=\"label\")\n+    label_id_input = label_lookup_layer(raw_label_input)\n+    label_mapper = keras.Model({\"label\": raw_label_input}, label_id_input)\n+\n+    return feature_mapper, label_mapper\n+\n+  def dataset_fn(self, feature_mapper, label_mapper):\n+    \"\"\"Function that generates dataset for test of tf.distribute + KPL.\n+\n+    Args:\n+      feature_mapper: a simple keras model with one keras StringLookup layer\n+        which maps feature to index.\n+      label_mapper: similar to feature_mapper, but maps label to index.\n+\n+    Returns:\n+      Generated dataset for test of tf.distribute + KPL.\n+\n+    \"\"\"\n+\n+    def feature_and_label_gen():\n+      # Generator of dataset.\n+      while True:\n+        features = random.sample(self.FEATURE_VOCAB, 3)\n+        label = [\"yes\"] if self.FEATURE_VOCAB[0] in features else [\"no\"]\n+        yield {\"features\": features, \"label\": label}\n+\n+    raw_dataset = tf.data.Dataset.from_generator(\n+        feature_and_label_gen,\n+        output_signature={\n+            \"features\": tf.TensorSpec([3], tf.string),\n+            \"label\": tf.TensorSpec([1], tf.string)\n+        }).shuffle(100).batch(32)\n+\n+    train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda\n+        {\n+            \"features\": feature_mapper(x[\"features\"])\n+        }, label_mapper(x[\"label\"])))\n+    return train_dataset\n+\n+  def define_model(self):\n+    \"\"\"A simple model for test of tf.distribute + KPL.\"\"\"\n+    # Create the model. The input needs to be compatible with KPLs.\n+    model_input = keras.layers.Input(\n+        shape=(3,), dtype=tf.int64, name=\"model_input\")\n+\n+    # input_dim includes a mask token and an oov token.\n+    emb_output = keras.layers.Embedding(\n+        input_dim=len(self.FEATURE_VOCAB) + 2, output_dim=20)(\n+            model_input)\n+    emb_output = tf.reduce_mean(emb_output, axis=1)\n+    dense_output = keras.layers.Dense(\n+        units=1, activation=\"sigmoid\")(\n+            emb_output)\n+    model = keras.Model({\"features\": model_input}, dense_output)\n+    return model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89a31e4858592f707b5dea5f095539b661159b0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 768 | Lines Deleted: 1019 | Files Changed: 28 | Hunks: 227 | Methods Changed: 172 | Complexity Δ (Sum/Max): -38/16 | Churn Δ: 1787 | Churn Cumulative: 44096 | Contributors (this commit): 89 | Commits (past 90d): 87 | Contributors (cumulative): 158 | DMM Complexity: 0.4307692307692308\n\nDIFF:\n@@ -790,8 +790,14 @@ class TrackableWeightHandler(object):\n \n     # TODO(b/141682913): Figure out why this is private and fix it.\n     saveables = trackable._gather_saveables_for_checkpoint().values()  # pylint: disable=protected-access\n-    if len(saveables) != 1:\n-      raise ValueError('Only Trackables with one Saveable are supported.')\n+    # 'Saveables' won't exist when we're passed a legacy TF1 table like\n+    # a StaticHashTable.\n+    if not saveables:\n+      self._num_tensors = 0\n+      self._setter = lambda weights: None\n+      self._getter = lambda: []\n+\n+    elif len(saveables) == 1:\n       saveable = list(saveables)[0]\n \n       if tf.compat.v1.executing_eagerly_outside_functions():\n@@ -816,9 +822,14 @@ class TrackableWeightHandler(object):\n           tensor = spec.tensor\n           self._placeholder_tensors.append(\n               tf.compat.v1.placeholder(tensor.dtype, tensor.shape))\n-      self._assign_op = self._saveable.restore(self._placeholder_tensors, None)\n+        self._assign_op = self._saveable.restore(self._placeholder_tensors,\n+                                                 None)\n         self._setter = self._set_weights_v1\n         self._getter = lambda: [spec.tensor for spec in self._saveable.specs]\n+    else:\n+      raise ValueError('Only Trackables with one Saveable are supported. '\n+                       'The Trackable %s has %d Saveables.' %\n+                       (trackable, len(saveables)))\n \n   @property\n   def num_tensors(self):\n\n@@ -417,7 +417,7 @@ def run_all_keras_modes(test_or_class=None,\n def _v1_session_test(f, test_or_class, config, *args, **kwargs):\n   with tf.compat.v1.get_default_graph().as_default():\n     with testing_utils.run_eagerly_scope(False):\n-      with test_or_class.test_session(use_gpu=True, config=config):\n+      with test_or_class.test_session(config=config):\n         f(test_or_class, *args, **kwargs)\n \n \n\n@@ -46,9 +46,6 @@ from keras.layers.preprocessing.image_preprocessing import Rescaling\n \n # Preprocessing layers.\n if tf.__internal__.tf2.enabled():\n-  from keras.layers.preprocessing.category_encoding import CategoryEncoding\n-  from keras.layers.preprocessing.category_encoding_v1 import CategoryEncoding as CategoryEncodingV1\n-  CategoryEncodingV2 = CategoryEncoding\n   from keras.layers.preprocessing.integer_lookup import IntegerLookup\n   from keras.layers.preprocessing.integer_lookup_v1 import IntegerLookup as IntegerLookupV1\n   IntegerLookupV2 = IntegerLookup\n@@ -65,9 +62,6 @@ else:\n   from keras.layers.preprocessing.integer_lookup_v1 import IntegerLookup\n   from keras.layers.preprocessing.integer_lookup import IntegerLookup as IntegerLookupV2\n   IntegerLookupV1 = IntegerLookup\n-  from keras.layers.preprocessing.category_encoding_v1 import CategoryEncoding\n-  from keras.layers.preprocessing.category_encoding import CategoryEncoding as CategoryEncodingV2\n-  CategoryEncodingV1 = CategoryEncoding\n   from keras.layers.preprocessing.normalization_v1 import Normalization\n   from keras.layers.preprocessing.normalization import Normalization as NormalizationV2\n   NormalizationV1 = Normalization\n@@ -78,6 +72,7 @@ else:\n   from keras.layers.preprocessing.text_vectorization import TextVectorization as TextVectorizationV2\n   TextVectorizationV1 = TextVectorization\n from keras.layers.preprocessing.category_crossing import CategoryCrossing\n+from keras.layers.preprocessing.category_encoding import CategoryEncoding\n from keras.layers.preprocessing.discretization import Discretization\n from keras.layers.preprocessing.hashing import Hashing\n \n\n@@ -89,9 +89,10 @@ class Conv(Layer):\n     activation: Activation function to use.\n       If you don't specify anything, no activation is applied.\n     use_bias: Boolean, whether the layer uses a bias.\n-    kernel_initializer: An initializer for the convolution kernel.\n+    kernel_initializer: An initializer for the convolution kernel. If None, the \n+      default initializer (glorot_uniform) will be used. \n     bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer will be used.\n+      initializer (zeros) will be used.\n     kernel_regularizer: Optional regularizer for the convolution kernel.\n     bias_regularizer: Optional regularizer for the bias vector.\n     activity_regularizer: Optional regularizer function for the output.\n@@ -446,9 +447,9 @@ class Conv1D(Conv):\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     kernel_initializer: Initializer for the `kernel` weights matrix (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'glorot_uniform'.\n     bias_initializer: Initializer for the bias vector (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to\n       the `kernel` weights matrix (see `keras.regularizers`).\n     bias_regularizer: Regularizer function applied to the bias vector (\n@@ -602,9 +603,9 @@ class Conv2D(Conv):\n       activation is applied (see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     kernel_initializer: Initializer for the `kernel` weights matrix (see\n-      `keras.initializers`).\n+      `keras.initializers`). Defaults to 'glorot_uniform'.\n     bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`).\n+      `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to the `kernel` weights\n       matrix (see `keras.regularizers`). \n     bias_regularizer: Regularizer function applied to the bias vector (see\n@@ -747,9 +748,9 @@ class Conv3D(Conv):\n       activation is applied (see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     kernel_initializer: Initializer for the `kernel` weights matrix (see\n-      `keras.initializers`).\n+      `keras.initializers`). Defaults to 'glorot_uniform'.\n     bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`).\n+      `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to the `kernel` weights\n       matrix (see `keras.regularizers`).\n     bias_regularizer: Regularizer function applied to the bias vector (see\n@@ -868,9 +869,9 @@ class Conv1DTranspose(Conv1D):\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     kernel_initializer: Initializer for the `kernel` weights matrix (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'glorot_uniform'.\n     bias_initializer: Initializer for the bias vector (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to\n       the `kernel` weights matrix (see `keras.regularizers`).\n     bias_regularizer: Regularizer function applied to the bias vector (\n@@ -1132,9 +1133,9 @@ class Conv2DTranspose(Conv2D):\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     kernel_initializer: Initializer for the `kernel` weights matrix (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'glorot_uniform'.\n     bias_initializer: Initializer for the bias vector (\n-      see `keras.initializers`).\n+      see `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to\n       the `kernel` weights matrix (see `keras.regularizers`).\n     bias_regularizer: Regularizer function applied to the bias vector (\n@@ -1435,8 +1436,10 @@ class Conv3DTranspose(Conv3D):\n       If you don't specify anything, no activation is applied (\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix.\n-    bias_initializer: Initializer for the bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix (\n+      see `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector (\n+      see `keras.initializers`). Defaults to 'zeros'.\n     kernel_regularizer: Regularizer function applied to\n       the `kernel` weights matrix (\n       see `keras.regularizers`).\n@@ -1725,10 +1728,14 @@ class SeparableConv(Conv):\n       If you don't specify anything, no activation is applied (\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias.\n-    depthwise_initializer: An initializer for the depthwise convolution kernel.\n-    pointwise_initializer: An initializer for the pointwise convolution kernel.\n+    depthwise_initializer: An initializer for the depthwise convolution kernel (\n+      see `keras.initializers`). If None, then the default initializer (\n+      'glorot_uniform') will be used.\n+    pointwise_initializer: An initializer for the pointwise convolution kernel (\n+      see `keras.initializers`). If None, then the default initializer \n+      ('glorot_uniform') will be used.\n     bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer will be used.\n+      initializer ('zeros') will be used (see `keras.initializers`).\n     depthwise_regularizer: Optional regularizer for the depthwise\n       convolution kernel.\n     pointwise_regularizer: Optional regularizer for the pointwise\n@@ -1931,11 +1938,13 @@ class SeparableConv1D(SeparableConv):\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias.\n     depthwise_initializer: An initializer for the depthwise convolution kernel (\n-      see `keras.initializers`).\n+      see `keras.initializers`). If None, then the default initializer (\n+      'glorot_uniform') will be used.\n     pointwise_initializer: An initializer for the pointwise convolution kernel (\n-      see `keras.initializers`).\n+      see `keras.initializers`). If None, then the default initializer \n+      ('glorot_uniform') will be used.\n     bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer will be used (see `keras.initializers`).\n+      initializer ('zeros') will be used (see `keras.initializers`).\n     depthwise_regularizer: Optional regularizer for the depthwise\n       convolution kernel (see `keras.regularizers`).\n     pointwise_regularizer: Optional regularizer for the pointwise\n@@ -2123,12 +2132,14 @@ class SeparableConv2D(SeparableConv):\n       If you don't specify anything, no activation is applied (\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n-    depthwise_initializer: Initializer for the depthwise kernel matrix (\n-      see `keras.initializers`).\n-    pointwise_initializer: Initializer for the pointwise kernel matrix (\n-      see `keras.initializers`).\n-    bias_initializer: Initializer for the bias vector (\n-      see `keras.initializers`).\n+    depthwise_initializer: An initializer for the depthwise convolution kernel (\n+      see `keras.initializers`). If None, then the default initializer (\n+      'glorot_uniform') will be used.\n+    pointwise_initializer: An initializer for the pointwise convolution kernel (\n+      see `keras.initializers`). If None, then the default initializer \n+      ('glorot_uniform') will be used.\n+    bias_initializer: An initializer for the bias vector. If None, the default\n+      initializer ('zeros') will be used (see `keras.initializers`).\n     depthwise_regularizer: Regularizer function applied to\n       the depthwise kernel matrix (see `keras.regularizers`).\n     pointwise_regularizer: Regularizer function applied to\n@@ -2287,9 +2298,11 @@ class DepthwiseConv2D(Conv2D):\n       see `keras.activations`).\n     use_bias: Boolean, whether the layer uses a bias vector.\n     depthwise_initializer: Initializer for the depthwise kernel matrix (\n-      see `keras.initializers`).\n+      see `keras.initializers`). If None, the default initializer (\n+      'glorot_uniform') will be used.\n     bias_initializer: Initializer for the bias vector (\n-      see `keras.initializers`).\n+      see `keras.initializers`). If None, the default initializer (\n+      'zeros') will bs used.\n     depthwise_regularizer: Regularizer function applied to\n       the depthwise kernel matrix (see `keras.regularizers`).\n     bias_regularizer: Regularizer function applied to the bias vector (\n\n@@ -37,7 +37,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n     stack_size = 3\n     length = 7\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Conv1D,\n           kwargs=kwargs,\n@@ -49,7 +49,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n     stack_size = 3\n     length = 7\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n \n@@ -107,7 +107,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv1D(**kwargs)\n       layer.build((None, 5, 2))\n       self.assertEqual(len(layer.losses), 2)\n@@ -126,14 +126,14 @@ class Conv1DTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv1D(**kwargs)\n       layer.build((None, 5, 2))\n       self.assertEqual(layer.kernel.constraint, k_constraint)\n       self.assertEqual(layer.bias.constraint, b_constraint)\n \n   def test_conv1d_recreate_conv(self):\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv1D(filters=1,\n                                   kernel_size=3,\n                                   strides=1,\n@@ -146,7 +146,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n       self.assertEqual(outp1_shape, layer(inpt1).shape)\n \n   def test_conv1d_recreate_conv_unknown_dims(self):\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv1D(filters=1,\n                                   kernel_size=3,\n                                   strides=1,\n@@ -179,7 +179,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n       input_data_shape = (num_samples, num_row or 7, num_col or 6, stack_size)\n       input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Conv2D,\n           kwargs=kwargs,\n@@ -200,7 +200,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n       input_data_shape = batch_shape + (num_row or 7, num_col or 6, stack_size)\n       input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n       testing_utils.layer_test(\n@@ -267,7 +267,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv2D(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(len(layer.losses), 2)\n@@ -286,7 +286,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv2D(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(layer.kernel.constraint, k_constraint)\n@@ -308,7 +308,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n     num_col = 6\n     depth = 5\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Conv3D,\n           kwargs=kwargs,\n@@ -326,7 +326,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n     num_col = 6\n     depth = 5\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n \n@@ -382,7 +382,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv3D(**kwargs)\n       layer.build((None, 5, 5, 5, 2))\n       self.assertEqual(len(layer.losses), 2)\n@@ -402,7 +402,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv3D(**kwargs)\n       layer.build((None, 5, 5, 5, 2))\n       self.assertEqual(layer.kernel.constraint, k_constraint)\n@@ -410,7 +410,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n \n   def test_conv3d_dynamic_shape(self):\n     input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       # Won't raise error here.\n       testing_utils.layer_test(\n           keras.layers.Conv3D,\n@@ -559,7 +559,7 @@ class ConvSequentialTest(keras_parameterized.TestCase):\n     kwargs['filters'] = 1\n     kwargs['kernel_size'] = 3\n     kwargs['dilation_rate'] = 2\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = conv_layer_cls(**kwargs)\n       output1 = layer(np.zeros(input_shape1))\n       self.assertEqual(output1.shape, expected_output_shape1)\n@@ -602,7 +602,7 @@ class ConvSequentialTest(keras_parameterized.TestCase):\n                    expected_output_shape1, expected_output_shape2)\n \n   def test_dynamic_shape(self):\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv3D(2, 3)\n       input_shape = (5, None, None, 2)\n       inputs = keras.Input(shape=input_shape)\n@@ -621,7 +621,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n     shape = (num_samples, num_steps, input_dim)\n     inputs = np.ones(shape)\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       # basic test\n       testing_utils.layer_test(\n           keras.layers.ZeroPadding1D,\n@@ -677,7 +677,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n       inputs = np.ones((num_samples, input_num_row, input_num_col, stack_size))\n \n     # basic test\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.ZeroPadding2D,\n           kwargs={\n@@ -694,7 +694,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n           input_shape=inputs.shape)\n \n     # correctness test\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.ZeroPadding2D(\n           padding=(2, 2), data_format=data_format)\n       layer.build(inputs.shape)\n@@ -765,7 +765,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n       inputs = np.ones((num_samples, input_len_dim1, input_len_dim2,\n                         input_len_dim3, stack_size))\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       # basic test\n       testing_utils.layer_test(\n           keras.layers.ZeroPadding3D,\n@@ -782,7 +782,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n           },\n           input_shape=inputs.shape)\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       # correctness test\n       layer = keras.layers.ZeroPadding3D(\n           padding=(2, 2, 2), data_format=data_format)\n@@ -851,7 +851,7 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n class UpSamplingTest(keras_parameterized.TestCase):\n \n   def test_upsampling_1d(self):\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.UpSampling1D, kwargs={'size': 2}, input_shape=(3, 5, 4))\n \n@@ -870,7 +870,7 @@ class UpSamplingTest(keras_parameterized.TestCase):\n                                 stack_size)\n \n       # basic test\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         testing_utils.layer_test(\n             keras.layers.UpSampling2D,\n             kwargs={'size': (2, 2),\n@@ -955,7 +955,7 @@ class UpSamplingTest(keras_parameterized.TestCase):\n                                 input_len_dim3, stack_size)\n \n       # basic test\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         testing_utils.layer_test(\n             keras.layers.UpSampling3D,\n             kwargs={'size': (2, 2, 2),\n@@ -1005,7 +1005,7 @@ class CroppingTest(keras_parameterized.TestCase):\n     input_len_dim1 = 2\n     inputs = np.random.rand(num_samples, time_length, input_len_dim1)\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Cropping1D,\n           kwargs={'cropping': (2, 2)},\n@@ -1031,7 +1031,7 @@ class CroppingTest(keras_parameterized.TestCase):\n       else:\n         inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n                                 stack_size)\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         # basic test\n         testing_utils.layer_test(\n             keras.layers.Cropping2D,\n@@ -1064,7 +1064,7 @@ class CroppingTest(keras_parameterized.TestCase):\n         inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n                                 stack_size)\n       # another correctness test (no cropping)\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         cropping = ((0, 0), (0, 0))\n         layer = keras.layers.Cropping2D(\n             cropping=cropping, data_format=data_format)\n@@ -1100,7 +1100,7 @@ class CroppingTest(keras_parameterized.TestCase):\n           inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n                                   input_len_dim3, stack_size)\n         # basic test\n-        with self.cached_session(use_gpu=True):\n+        with self.cached_session():\n           testing_utils.layer_test(\n               keras.layers.Cropping3D,\n               kwargs={'cropping': cropping,\n@@ -1109,7 +1109,7 @@ class CroppingTest(keras_parameterized.TestCase):\n \n         if len(croppings) == 3 and len(croppings[0]) == 2:\n           # correctness test\n-          with self.cached_session(use_gpu=True):\n+          with self.cached_session():\n             layer = keras.layers.Cropping3D(\n                 cropping=cropping, data_format=data_format)\n             layer.build(inputs.shape)\n@@ -1147,7 +1147,7 @@ class DepthwiseConv2DTest(keras_parameterized.TestCase):\n     num_row = 7\n     num_col = 6\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.DepthwiseConv2D,\n           kwargs=kwargs,\n\n@@ -37,7 +37,7 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n     num_row = 7\n     num_col = 6\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Conv2DTranspose,\n           kwargs=kwargs,\n@@ -68,7 +68,7 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv2DTranspose(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(len(layer.losses), 2)\n@@ -87,7 +87,7 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv2DTranspose(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(layer.kernel.constraint, k_constraint)\n@@ -128,7 +128,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n     num_col = 6\n     depth = 5\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.Conv3DTranspose,\n           kwargs=kwargs,\n@@ -160,7 +160,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv3DTranspose(**kwargs)\n       layer.build((None, 5, 5, 5, 2))\n       self.assertEqual(len(layer.losses), 2)\n@@ -179,7 +179,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.Conv3DTranspose(**kwargs)\n       layer.build((None, 5, 5, 5, 2))\n       self.assertEqual(layer.kernel.constraint, k_constraint)\n@@ -187,7 +187,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n \n   def test_conv3d_transpose_dynamic_shape(self):\n     input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       # Won't raise error here.\n       testing_utils.layer_test(\n           keras.layers.Conv3DTranspose,\n\n@@ -203,7 +203,7 @@ class CuDNNGraphOnlyTest(keras_parameterized.TestCase):\n     units = 2\n     num_samples = 32\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       model = keras.models.Sequential()\n       model.add(\n           keras.layers.Embedding(\n\n@@ -97,7 +97,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes\n   def test_batchnorm_convnet(self):\n     if tf.test.is_gpu_available(cuda_only=True):\n-      with self.session(use_gpu=True):\n+      with self.session():\n         model = keras.models.Sequential()\n         norm = keras.layers.BatchNormalization(\n             axis=1, input_shape=(3, 4, 4), momentum=0.8)\n\n@@ -363,15 +363,11 @@ class MaxPooling2D(Pooling2D):\n   ...                  [9., 10., 11., 12.]])\n   >>> x = tf.reshape(x, [1, 3, 4, 1])\n   >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n-  ...    strides=(1, 1), padding='valid')\n+  ...    strides=(2, 2), padding='valid')\n   >>> max_pool_2d(x)\n-  <tf.Tensor: shape=(1, 2, 3, 1), dtype=float32, numpy=\n+  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n     array([[[[6.],\n-             [ 7.],\n-             [ 8.]],\n-            [[10.],\n-             [11.],\n-             [12.]]]], dtype=float32)>\n+             [8.]]]], dtype=float32)>\n \n   Usage Example:\n \n\n@@ -0,0 +1,141 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Benchmark for Keras text vectorization preprocessing layer's adapt method.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+import os\n+import random\n+import string\n+import time\n+\n+import numpy as np\n+\n+import keras\n+from keras.layers.preprocessing import index_lookup\n+\n+tf.compat.v1.enable_v2_behavior()\n+\n+\n+# word_gen creates random sequences of ASCII letters (both lowercase and upper).\n+# The number of unique strings is ~2,700.\n+def tensor_gen(batch, num_elements):\n+  data = []\n+  for _ in range(batch):\n+    batch_element = []\n+    for _ in range(num_elements - 1):\n+      tok = \"\".join(random.choice(string.ascii_letters) for i in range(2))\n+      batch_element.append(tok)\n+    batch_element.append(\"\")  # Explicitly test the empty string.\n+    data.append(batch_element)\n+  return tf.constant(data)\n+\n+\n+def get_vocab():\n+  vocab = list(\n+      set([a + b for a in string.ascii_letters for b in string.ascii_letters]))  # pylint:disable=g-complex-comprehension\n+  vocab.sort()\n+  return vocab\n+\n+\n+# This class uses TestCase for get_temp_dir().\n+class BenchmarkLookup(tf.test.Benchmark):\n+  \"\"\"Benchmark the index lookup layer's forward pass.\"\"\"\n+\n+  def _write_to_temp_file(self, file_name, vocab_list):\n+    vocab_path = os.path.join(self.get_temp_dir(), file_name + \".txt\")\n+    with tf.io.gfile.GFile(vocab_path, \"w\") as writer:\n+      for vocab in vocab_list:\n+        writer.write(vocab + \"\\n\")\n+      writer.flush()\n+      writer.close()\n+    return vocab_path\n+\n+  def run_numpy_implementation(self, data, vocab):\n+    \"\"\"Test the python implementation.\"\"\"\n+    input_t = keras.Input(shape=(), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        vocabulary=vocab,\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"OOV\",\n+        dtype=tf.string)\n+    out_t = layer(input_t)\n+    model = keras.Model(input_t, out_t)\n+    num_repeats = 5\n+    starts = []\n+    ends = []\n+    _ = model(data)\n+    for _ in range(num_repeats):\n+      starts.append(time.time())\n+      out = model(data)\n+      ends.append(time.time())\n+    avg_time = np.mean(np.array(ends) - np.array(starts))\n+    return avg_time, out\n+\n+  def bm_adapt_implementation(self, num_elements, batch_size):\n+    \"\"\"Test the KPL adapt implementation.\"\"\"\n+    vocab = get_vocab()\n+    vocab_file = self._write_to_temp_file(\"vocab\", vocab)\n+    vocabulary_initializer = tf.lookup.TextFileInitializer(\n+        filename=vocab_file,\n+        key_dtype=tf.string,\n+        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n+        value_dtype=tf.int64,\n+        value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n+        value_index_offset=2)\n+    input_t = keras.Input(shape=(), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        vocabulary=vocabulary_initializer,\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"OOV\",\n+        dtype=tf.string)\n+    out_t = layer(input_t)\n+    model = keras.Model(input_t, out_t)\n+    num_repeats = 5\n+    starts = []\n+    ends = []\n+    data = tensor_gen(batch_size, num_elements)\n+    _ = model(data)\n+    for _ in range(num_repeats):\n+      starts.append(time.time())\n+      _ = model(data)\n+      ends.append(time.time())\n+    avg_time = np.mean(np.array(ends) - np.array(starts))\n+    baseline, _ = self.run_numpy_implementation(data, vocab)\n+    extras = {\n+        \"numpy implementation baseline\": baseline,\n+        \"delta seconds\": (baseline - avg_time),\n+        \"delta percent\": ((baseline - avg_time) / baseline) * 100\n+    }\n+    name = \"index_lookup_forward|%s_elements|batch_%s\" % (num_elements,\n+                                                          batch_size)\n+    self.report_benchmark(\n+        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)\n+\n+  def benchmark_vocab_size_by_batch(self):\n+    for tensor_size in [100, 1000, 10000]:\n+      for batch in [1, 16, 2048]:\n+        self.bm_adapt_implementation(tensor_size, batch)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -20,15 +20,11 @@ from __future__ import print_function\n \n import tensorflow as tf\n \n-import collections\n-import json\n-import numbers\n-\n import numpy as np\n from keras import backend as K\n from keras.engine import base_preprocessing_layer\n-from keras.engine.input_spec import InputSpec\n from keras.utils import layer_utils\n+from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n TFIDF = \"tf-idf\"\n@@ -36,29 +32,27 @@ INT = \"int\"\n BINARY = \"binary\"\n COUNT = \"count\"\n \n-# The string tokens in the extracted vocabulary\n-_NUM_ELEMENTS_NAME = \"num_elements\"\n # The inverse-document-frequency weights\n _IDF_NAME = \"idf\"\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.CategoryEncoding\", v1=[])\n-class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n+@keras_export(\"keras.layers.experimental.preprocessing.CategoryEncoding\")\n+class CategoryEncoding(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Category encoding layer.\n \n-  This layer provides options for condensing data into a categorical encoding.\n-  It accepts integer values as inputs and outputs a dense representation\n-  (one sample = 1-index tensor of float values representing data about the\n-  sample's tokens) of those inputs.\n+  This layer provides options for condensing data into a categorical encoding\n+  when the total number of tokens are known in advance. It accepts integer\n+  values as inputs and outputs a dense representation (one sample = 1-index\n+  tensor of float values representing data about the sample's tokens) of those\n+  inputs. For integer inputs where the total number of tokens is not known, see\n+  `tf.keras.layers.experimental.preprocessing.IntegerLookup`.\n \n   Examples:\n \n-  **Multi-hot encoding data if you know in advance the number of tokens**\n-\n-  In this case, you can pass the `max_tokens` argument to the constructor.\n+  **Multi-hot encoding data**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n-  ...           max_tokens=4, output_mode=\"binary\")\n+  ...           num_tokens=4, output_mode=\"binary\")\n   >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]])\n   <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n     array([[1., 1., 0., 0.],\n@@ -66,20 +60,10 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n            [0., 1., 1., 0.],\n            [0., 1., 0., 1.]], dtype=float32)>\n \n-  **Multi-hot encoding data where the number of tokens is unknown**\n-\n-  In this case, you should `adapt()` the layer on a sample dataset.\n-\n-  ```python\n-  layer = CategoryEncoding(output_mode=\"binary\")\n-  layer.adapt(sample_dataset)  # Indexes the vocabulary of the data\n-  outputs = layer(inputs)\n-  ```\n-\n   **Using weighted inputs in `count` mode**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n-  ...           max_tokens=4, output_mode=\"count\")\n+  ...           num_tokens=4, output_mode=\"count\")\n   >>> count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])\n   >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)\n   <tf.Tensor: shape=(4, 4), dtype=float64, numpy=\n@@ -89,18 +73,17 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n            [0. , 0.2, 0. , 0.4]])>\n \n   Args:\n-    max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary.\n+    num_tokens: The total number of tokens the layer should support. All inputs\n+      to the layer must integers in the range 0 <= value < num_tokens or an\n+      error will be thrown.\n     output_mode: Specification for the output of the layer.\n       Defaults to \"binary\". Values can\n       be \"binary\", \"count\" or \"tf-idf\", configuring the layer as follows:\n         \"binary\": Outputs a single int array per batch, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n+          num_tokens size, containing 1s in all elements where the token mapped\n           to that index exists at least once in the batch item.\n         \"count\": As \"binary\", but the int array contains a count of the number\n           of times the token at that index appeared in the batch item.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n     sparse: Boolean. If true, returns a `SparseTensor` instead of a dense\n       `Tensor`. Defaults to `False`.\n \n@@ -112,59 +95,40 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n   \"\"\"\n \n   def __init__(self,\n-               max_tokens=None,\n+               num_tokens=None,\n                output_mode=BINARY,\n                sparse=False,\n                **kwargs):\n-    # 'output_mode' must be one of (COUNT, BINARY, TFIDF)\n+    # max_tokens is an old name for the num_tokens arg we continue to support\n+    # because of usage.\n+    if \"max_tokens\" in kwargs:\n+      logging.warning(\n+          \"max_tokens is deprecated, please use num_tokens instead.\")\n+      num_tokens = kwargs[\"max_tokens\"]\n+      del kwargs[\"max_tokens\"]\n+\n+    super(CategoryEncoding, self).__init__(**kwargs)\n+\n+    # 'output_mode' must be one of (COUNT, BINARY)\n     layer_utils.validate_string_arg(\n         output_mode,\n-        allowable_strings=(COUNT, BINARY, TFIDF),\n+        allowable_strings=(COUNT, BINARY),\n         layer_name=\"CategoryEncoding\",\n         arg_name=\"output_mode\")\n \n-    # If max_tokens is set, the value must be greater than 1 - otherwise we\n-    # are creating a 0-element vocab, which doesn't make sense.\n-    if max_tokens is not None and max_tokens < 1:\n-      raise ValueError(\"max_tokens must be > 1.\")\n+    if num_tokens is None:\n+      raise ValueError(\"num_tokens must be set to use this layer. If the \"\n+                       \"number of tokens is not known beforehand, use the \"\n+                       \"IntegerLookup layer instead.\")\n+    if num_tokens < 1:\n+      raise ValueError(\"num_tokens must be >= 1.\")\n \n-    # We need to call super() before we call _add_state_variable().\n-    combiner = _CategoryEncodingCombiner(\n-        max_tokens=max_tokens,\n-        compute_idf=output_mode == TFIDF)\n-    super(CategoryEncoding, self).__init__(combiner=combiner, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"CategoryEncoding\").set(True)\n-\n-    self.max_tokens = max_tokens\n+    self.num_tokens = num_tokens\n     self.output_mode = output_mode\n     self.sparse = sparse\n-    self._called = False\n-\n-    if self.output_mode == TFIDF:\n-      # The TF-IDF weight may have a (None,) tensorshape. This creates\n-      # a 1D variable with arbitrary shape, which we can assign any weight to\n-      # so long as it has 1 dimension. In order to properly initialize this\n-      # weight in Keras, we need to provide a custom callable initializer which\n-      # does not depend on the shape of the weight (as all other initializers\n-      # do) since the weight is not known. Hence the lambda shape, dtype: [0].\n-      if max_tokens is None:\n-        initializer = lambda shape, dtype: [0]\n-      else:\n-        initializer = tf.compat.v1.zeros_initializer\n-\n-      # We are adding these here instead of in build() since they do not depend\n-      # on the input shape at all.\n-      self.tf_idf_weights = self._add_state_variable(\n-          name=_IDF_NAME,\n-          shape=tf.TensorShape((max_tokens,)),\n-          dtype=K.floatx(),\n-          initializer=initializer)\n-\n-      self.input_spec = InputSpec(ndim=2)\n \n   def compute_output_shape(self, input_shape):\n-    return tf.TensorShape([input_shape[0], self.max_tokens])\n+    return tf.TensorShape([input_shape[0], self.num_tokens])\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n@@ -175,95 +139,15 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n     else:\n       return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n-  def adapt(self, data, reset_state=True):\n-    \"\"\"Fits the state of the preprocessing layer to the dataset.\n-\n-    Overrides the default adapt method to apply relevant preprocessing to the\n-    inputs before passing to the combiner.\n-\n-    Args:\n-      data: The data to train on. It can be passed either as a tf.data Dataset,\n-        or as a numpy array.\n-      reset_state: Optional argument specifying whether to clear the state of\n-        the layer at the start of the call to `adapt`. This must be True for\n-        this layer, which does not support repeated calls to `adapt`.\n-\n-    Raises:\n-      RuntimeError: if the layer cannot be adapted at this time.\n-    \"\"\"\n-    if not reset_state:\n-      raise ValueError(\"CategoryEncoding does not support streaming adapts.\")\n-\n-    super(CategoryEncoding, self).adapt(data, reset_state)\n-\n-  def _set_state_variables(self, updates):\n-    if not self.built:\n-      raise RuntimeError(\"_set_state_variables() must be called after build().\")\n-    if _NUM_ELEMENTS_NAME in updates:\n-      if self.max_tokens is None:\n-        self.set_num_elements(updates[_NUM_ELEMENTS_NAME])\n-      elif self.max_tokens != updates[_NUM_ELEMENTS_NAME]:\n-        raise RuntimeError(\"Cannot update states if you construct the layer \"\n-                           \"with `max_tokens`={}\".format(self.max_tokens))\n-    if self.output_mode == TFIDF:\n-      self.set_tfidf_data(updates[_IDF_NAME])\n-\n   def get_config(self):\n     config = {\n-        \"max_tokens\": self.max_tokens,\n+        \"num_tokens\": self.num_tokens,\n         \"output_mode\": self.output_mode,\n         \"sparse\": self.sparse,\n     }\n     base_config = super(CategoryEncoding, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  def _convert_to_ndarray(self, x):\n-    if isinstance(x, tf.Tensor):\n-      return x\n-    else:\n-      return np.array(x)\n-\n-  def _convert_to_sparse_inputs(self, inputs):\n-    if isinstance(inputs, tf.SparseTensor):\n-      return inputs\n-    elif isinstance(inputs, tf.RaggedTensor):\n-      return inputs.to_sparse()\n-    else:\n-      indices = tf.where(\n-          tf.greater_equal(inputs, tf.constant(0, inputs.dtype)))\n-      values = tf.compat.v1.gather_nd(inputs, indices)\n-      shape = tf.compat.v1.shape(inputs, out_type=tf.int64)\n-      return tf.SparseTensor(indices, values, shape)\n-\n-  def set_num_elements(self, num_elements):\n-    if self.max_tokens is not None:\n-      raise RuntimeError(\n-          \"In order to dynamically set the number of elements, the \"\n-          \"layer's 'max_tokens' arg must be set to None.\")\n-    if not isinstance(num_elements, numbers.Integral):\n-      raise ValueError(\"num_elements must be a scalar integer.\")\n-    if self._called:\n-      raise RuntimeError(\"num_elements cannot be changed after the layer is \"\n-                         \"called.\")\n-    self.max_tokens = num_elements\n-\n-  def set_tfidf_data(self, tfidf_data):\n-    tfidf_data = self._convert_to_ndarray(tfidf_data)\n-    if self.output_mode != TFIDF:\n-      raise RuntimeError(\n-          \"In order to set TF-IDF data, the output mode must be 'tf-idf'.\")\n-    if tfidf_data.ndim != 1:\n-      raise ValueError(\"TF-IDF data must be a 1-index array.\")\n-    if self.max_tokens is not None:\n-      input_data_length = tfidf_data.shape[0]\n-      if input_data_length > self.max_tokens:\n-        raise ValueError(\"The array provided has %d elements. This layer is \"\n-                         \"configured to only allow %d elements.\" %\n-                         (input_data_length, self.max_tokens))\n-      if input_data_length < self.max_tokens:\n-        tfidf_data = np.resize(tfidf_data, (self.max_tokens,))\n-    K.set_value(self.tf_idf_weights, tfidf_data)\n-\n   def call(self, inputs, count_weights=None):\n     if isinstance(inputs, (list, np.ndarray)):\n       inputs = tf.convert_to_tensor(inputs)\n@@ -273,30 +157,8 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if count_weights is not None and self.output_mode != COUNT:\n       raise ValueError(\"count_weights is not used in `output_mode='tf-idf'`, \"\n                        \"or `output_mode='binary'`. Please pass a single input.\")\n-    self._called = True\n-    if self.max_tokens is None:\n-      raise RuntimeError(\n-          \"If you construct a `CategoryEncoding` layer with \"\n-          \"`max_tokens=None`, you need to call `adapt()` \"\n-          \"on it before using it\")\n-    else:\n-      out_depth = self.max_tokens\n-\n-    if self.output_mode == TFIDF:\n-      # If the input is a sparse tensor, we densify it with the default value of\n-      # -1. Because -1 is ignored by one_hot, this effectively drops the non-set\n-      # positions from the output encoding.\n-      if self.sparse:\n-        raise ValueError(\"`sparse=True` with `output_mode=tfidf` \"\n-                         \"is not supported.\")\n-      if isinstance(inputs, tf.SparseTensor):\n-        inputs = tf.sparse.to_dense(inputs, default_value=-1)\n-      one_hot_data = tf.one_hot(inputs, depth=out_depth)\n-      counts = tf.reduce_sum(one_hot_data, axis=1)\n-      tf_idf_data = tf.multiply(counts, self.tf_idf_weights)\n-      tf_idf_data.set_shape(tf.TensorShape((None, out_depth)))\n-      return tf_idf_data\n \n+    out_depth = self.num_tokens\n     binary_output = (self.output_mode == BINARY)\n     if isinstance(inputs, tf.SparseTensor):\n       max_value = tf.reduce_max(inputs.values)\n@@ -309,191 +171,16 @@ class CategoryEncoding(base_preprocessing_layer.CombinerPreprocessingLayer):\n             tf.cast(out_depth, max_value.dtype), max_value),\n         tf.greater_equal(\n             min_value, tf.cast(0, min_value.dtype)))\n-    tf.Assert(\n-        condition, [\"Input values must be in the range 0 <= values < max_tokens\"\n-                    \" with max_tokens={}\".format(out_depth)])\n+    tf.Assert(condition, [\n+        \"Input values must be in the range 0 <= values < num_tokens\"\n+        \" with num_tokens={}\".format(out_depth)\n+    ])\n     if self.sparse:\n       return sparse_bincount(inputs, out_depth, binary_output, count_weights)\n     else:\n       return dense_bincount(inputs, out_depth, binary_output, count_weights)\n \n \n-class _CategoryEncodingAccumulator(\n-    collections.namedtuple(\"Accumulator\", [\"data\", \"per_doc_count_dict\"])):\n-  pass\n-\n-\n-class _CategoryEncodingCombiner(base_preprocessing_layer.Combiner):\n-  \"\"\"Combiner for the CategoryEncoding preprocessing layer.\n-\n-  This class encapsulates the logic for computing the number of elements in the\n-  input dataset and the document frequency for each element.\n-\n-  Attributes:\n-    compute_max_element: (Optional) If set, this combiner will return the\n-      maximum element in this set as part of its `extract()` call.\n-    compute_idf: (Optional) If set, the inverse document frequency will be\n-      computed for each value.\n-  \"\"\"\n-  # These are indices into the accumulator's `data` array.\n-  MAX_VALUE_IDX = 0\n-  DOC_ID_IDX = 1\n-\n-  def __init__(self, max_tokens=None, compute_idf=False):\n-    self.max_tokens = max_tokens\n-    self._compute_idf = compute_idf\n-\n-  def compute(self, values, accumulator=None):\n-    \"\"\"Computes a step in this computation, returning a new accumulator.\"\"\"\n-    values = base_preprocessing_layer.convert_to_list(values)\n-\n-    if accumulator is None:\n-      accumulator = self._create_accumulator()\n-\n-    # TODO(momernick): Benchmark improvements to this algorithm.\n-    for element in values:\n-      if not isinstance(element, list):\n-        element = [element]\n-      current_doc_id = accumulator.data[self.DOC_ID_IDX]\n-      for value in element:\n-        if self.max_tokens is None:\n-          current_max_value = accumulator.data[self.MAX_VALUE_IDX]\n-          if value > current_max_value:\n-            accumulator.data[self.MAX_VALUE_IDX] = value\n-        if self._compute_idf:\n-          doc_count = accumulator.per_doc_count_dict[value]\n-          if doc_count[\"last_doc_id\"] != current_doc_id:\n-            doc_count[\"count\"] += 1\n-            doc_count[\"last_doc_id\"] = current_doc_id\n-      accumulator.data[self.DOC_ID_IDX] += 1\n-\n-    return accumulator\n-\n-  def merge(self, accumulators):\n-    \"\"\"Merges several accumulators to a single accumulator.\"\"\"\n-    if not accumulators:\n-      return accumulators\n-\n-    base_accumulator = accumulators[0]\n-\n-    for accumulator in accumulators[1:]:\n-      base_accumulator.data[self.DOC_ID_IDX] += accumulator.data[\n-          self.DOC_ID_IDX]\n-      if self.max_tokens is None:\n-        base_accumulator.data[self.MAX_VALUE_IDX] = max(\n-            base_accumulator.data[self.MAX_VALUE_IDX],\n-            accumulator.data[self.MAX_VALUE_IDX])\n-      if self._compute_idf:\n-        for token, value in accumulator.per_doc_count_dict.items():\n-          # Any newly created token counts in 'base_accumulator''s\n-          # per_doc_count_dict will have a last_doc_id of -1. This is always\n-          # less than the next doc id (which are strictly positive), so any\n-          # future occurrences are guaranteed to be counted.\n-          base_accumulator.per_doc_count_dict[token][\"count\"] += value[\"count\"]\n-\n-    return base_accumulator\n-\n-  def _inverse_document_frequency(self, document_counts, num_documents):\n-    \"\"\"Computes the inverse-document-frequency (IDF) component of TFIDF.\n-\n-    Uses the default weighting scheme described in\n-    https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n-\n-    Args:\n-      document_counts: An array of the # of documents each token appears in.\n-      num_documents: An int representing the total number of documents\n-\n-    Returns:\n-      An array of \"inverse document frequency\" weights.\n-    \"\"\"\n-    return np.log(1 + num_documents / (1 + np.array(document_counts)))\n-\n-  def extract(self, accumulator):\n-    \"\"\"Converts an accumulator into a dict of output values.\n-\n-    Args:\n-      accumulator: An accumulator aggregating over the full dataset.\n-\n-    Returns:\n-      A dict of:\n-        \"num_elements\": The number of unique elements in the data set. Only\n-          returned if `compute_max_element` is True.\n-        \"idf\": The inverse-document-frequency for each index, where idf[i] is\n-          the IDF value for index i. Only returned if `compute_idf` is True.\n-    \"\"\"\n-    data, document_counts = accumulator\n-    if data[self.MAX_VALUE_IDX] is not None:\n-      max_element = data[self.MAX_VALUE_IDX] + 1\n-    else:\n-      max_element = self.max_tokens\n-    output_dict = {}\n-    if self.max_tokens is None:\n-      output_dict[_NUM_ELEMENTS_NAME] = max_element\n-\n-    if self._compute_idf:\n-      num_documents = data[self.DOC_ID_IDX]\n-      # Here, we need to get the doc_counts for every token value, including\n-      # values we have not yet seen (and are not in the document_counts dict).\n-      # However, because document_counts is a defaultdict (see below), querying\n-      # the dict directly for those values gives us meaningful counts (of 0).\n-      # However, this also means we can't just extract the values in\n-      # document_counts - we need to do a deliberate indexing using range().\n-      doc_counts = [document_counts[i][\"count\"] for i in range(max_element)]\n-      idf = self._inverse_document_frequency(doc_counts, num_documents)\n-      output_dict[_IDF_NAME] = idf\n-\n-    return output_dict\n-\n-  def restore(self, output):\n-    \"\"\"Creates an accumulator based on 'output'.\"\"\"\n-    raise NotImplementedError(\n-        \"CategoryEncoding does not restore or support streaming updates.\")\n-\n-  def serialize(self, accumulator):\n-    \"\"\"Serializes an accumulator for a remote call.\"\"\"\n-    output_dict = {}\n-    output_dict[\"data\"] = accumulator.data\n-    if self._compute_idf:\n-      output_dict[\"idf_vocab\"] = list(accumulator.per_doc_count_dict.keys())\n-      output_dict[\"idf_counts\"] = [\n-          counter[\"count\"]\n-          for counter in accumulator.per_doc_count_dict.values()\n-      ]\n-    return tf.compat.as_bytes(json.dumps(output_dict))\n-\n-  def deserialize(self, encoded_accumulator):\n-    \"\"\"Deserializes an accumulator received from 'serialize()'.\"\"\"\n-    accumulator_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n-\n-    accumulator = self._create_accumulator()\n-    for i, value in enumerate(accumulator_dict[\"data\"]):\n-      accumulator.data[i] = value\n-\n-    if self._compute_idf:\n-      create_dict = lambda x: {\"count\": x, \"last_doc_id\": -1}\n-      idf_count_dicts = [\n-          create_dict(count) for count in accumulator_dict[\"idf_counts\"]\n-      ]\n-      idf_dict = dict(zip(accumulator_dict[\"idf_vocab\"], idf_count_dicts))\n-      accumulator.per_doc_count_dict.update(idf_dict)\n-\n-    return accumulator\n-\n-  def _create_accumulator(self):\n-    \"\"\"Accumulates a sorted array of vocab tokens and corresponding counts.\"\"\"\n-\n-    if self._compute_idf:\n-      create_default_dict = lambda: {\"count\": 0, \"last_doc_id\": -1}\n-      per_doc_count_dict = collections.defaultdict(create_default_dict)\n-    else:\n-      per_doc_count_dict = None\n-    if self.max_tokens is None:\n-      data = [0, 0]\n-    else:\n-      data = [None, 0]\n-    return _CategoryEncodingAccumulator(data, per_doc_count_dict)\n-\n-\n def sparse_bincount(inputs, out_depth, binary_output, count_weights=None):\n   \"\"\"Apply binary or count encoding to an input and return a sparse tensor.\"\"\"\n   result = tf.sparse.bincount(\n\n@@ -59,13 +59,13 @@ class CategoryEncodingDistributionTest(\n     expected_output = [[0, 1, 1, 1, 0, 0],\n                        [1, 1, 0, 1, 0, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n+    num_tokens = 6\n     tf.config.set_soft_device_placement(True)\n \n     with distribution.scope():\n       input_data = keras.Input(shape=(4,), dtype=tf.int32)\n       layer = category_encoding.CategoryEncoding(\n-          max_tokens=max_tokens, output_mode=category_encoding.BINARY)\n+          num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(inp_dataset)\n\n@@ -29,15 +29,11 @@ from keras import backend\n from keras import keras_parameterized\n from keras.layers import core\n from keras.layers.preprocessing import category_encoding\n-from keras.layers.preprocessing import category_encoding_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n \n \n def get_layer_class():\n-  if tf.executing_eagerly():\n   return category_encoding.CategoryEncoding\n-  else:\n-    return category_encoding_v1.CategoryEncoding\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -54,11 +50,11 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     #  [X, X, X, 2]]\n     expected_indices = [[0, 1], [0, 2], [0, 3], [1, 0], [1, 3]]\n     expected_values = [1, 1, 1, 1, 2]\n-    max_tokens = 6\n+    num_tokens = 6\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT, sparse=True)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)\n     int_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -68,7 +64,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n \n     # Assert sparse output is same as dense output.\n     layer = get_layer_class()(\n-        max_tokens=max_tokens,\n+        num_tokens=num_tokens,\n         output_mode=category_encoding.COUNT,\n         sparse=False)\n     int_data = layer(input_data)\n@@ -86,13 +82,13 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     expected_output = [[0, 1, 1, 1, 0, 0],\n                        [0, 1, 0, 1, 0, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n \n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -110,14 +106,14 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     expected_output = [[0, .1, .2, .3, .4, 0],\n                        [0, .4, 0, .1, .5, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n     weight_data = keras.Input(shape=(None,), dtype=tf.float32, sparse=True)\n \n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT)\n     int_data = layer(input_data, count_weights=weight_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -140,10 +136,10 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     #  [1, X, X, X]]\n     expected_indices = [[0, 0], [1, 2], [2, 1], [3, 0]]\n     expected_values = [1, 1, 2, 1]\n-    max_tokens = 6\n+    num_tokens = 6\n \n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT, sparse=True)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)\n     int_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -153,7 +149,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n \n     # Assert sparse output is same as dense output.\n     layer = get_layer_class()(\n-        max_tokens=max_tokens,\n+        num_tokens=num_tokens,\n         output_mode=category_encoding.COUNT,\n         sparse=False)\n     int_data = layer(input_data)\n@@ -179,10 +175,10 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     #  [1, X, X, X]]\n     expected_indices = [[0, 0], [1, 2], [2, 1], [3, 0]]\n     expected_values = [.1, .2, .7, .2]\n-    max_tokens = 6\n+    num_tokens = 6\n \n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT, sparse=True)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)\n     int_data = layer(input_data, count_weights=weight_data)\n \n     model = keras.Model(inputs=[input_data, weight_data], outputs=int_data)\n@@ -197,13 +193,13 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     expected_output = [[0, 1, 1, 1, 0, 0],\n                        [0, 1, 0, 1, 0, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n \n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n     int_data = layer(input_data)\n \n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n@@ -220,11 +216,11 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     #  [X, X, X, 2]]\n     expected_indices = [[0, 1], [0, 2], [0, 3], [1, 3]]\n     expected_values = [1, 1, 1, 2]\n-    max_tokens = 6\n+    num_tokens = 6\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT, sparse=True)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)\n     int_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -234,7 +230,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n \n     # Assert sparse output is same as dense output.\n     layer = get_layer_class()(\n-        max_tokens=max_tokens,\n+        num_tokens=num_tokens,\n         output_mode=category_encoding.COUNT,\n         sparse=False)\n     int_data = layer(input_data)\n@@ -247,12 +243,11 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n   def test_sparse_output_and_dense_layer(self):\n     input_array = tf.constant([[1, 2, 3], [3, 3, 0]])\n \n-    max_tokens = 4\n+    num_tokens = 4\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     encoding_layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.COUNT,\n-        sparse=True)\n+        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)\n     int_data = encoding_layer(input_data)\n     dense_layer = keras.layers.Dense(units=1)\n     output_data = dense_layer(int_data)\n@@ -262,126 +257,41 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n \n   def test_dense_oov_input(self):\n     input_array = tf.constant([[0, 1, 2], [2, 3, 1]])\n-    max_tokens = 3\n-    expected_output_shape = [None, max_tokens]\n-    encoder_layer = get_layer_class()(max_tokens)\n+    num_tokens = 3\n+    expected_output_shape = [None, num_tokens]\n+    encoder_layer = get_layer_class()(num_tokens)\n     input_data = keras.Input(shape=(3,), dtype=tf.int32)\n     int_data = encoder_layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     with self.assertRaisesRegex(\n         tf.errors.InvalidArgumentError,\n-        \".*must be in the range 0 <= values < max_tokens.*\"):\n+        \".*must be in the range 0 <= values < num_tokens.*\"):\n       _ = model.predict(input_array, steps=1)\n \n   def test_dense_negative(self):\n     input_array = tf.constant([[1, 2, 0], [2, 2, -1]])\n-    max_tokens = 3\n-    expected_output_shape = [None, max_tokens]\n-    encoder_layer = get_layer_class()(max_tokens)\n+    num_tokens = 3\n+    expected_output_shape = [None, num_tokens]\n+    encoder_layer = get_layer_class()(num_tokens)\n     input_data = keras.Input(shape=(3,), dtype=tf.int32)\n     int_data = encoder_layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     with self.assertRaisesRegex(\n         tf.errors.InvalidArgumentError,\n-        \".*must be in the range 0 <= values < max_tokens.*\"):\n+        \".*must be in the range 0 <= values < num_tokens.*\"):\n       _ = model.predict(input_array, steps=1)\n \n-\n-@keras_parameterized.run_all_keras_modes\n-class CategoryEncodingAdaptTest(keras_parameterized.TestCase,\n-                                preprocessing_test_utils.PreprocessingLayerTest\n-                               ):\n-\n-  def test_sparse_adapt(self):\n-    vocab_data = tf.sparse.from_dense(\n-        np.array([[1, 1, 0, 1, 1, 2, 2, 0, 2, 3, 3, 0, 4]], dtype=np.int64))\n-    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)\n-    input_array = tf.sparse.from_dense(\n-        np.array([[1, 2, 3, 0], [0, 3, 1, 0]], dtype=np.int64))\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [0, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.adapt(vocab_dataset)\n-    int_data = layer(input_data)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array, steps=1)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-  def test_ragged_adapt(self):\n-    vocab_data = tf.ragged.constant(\n-        np.array([[1, 1, 0, 1, 1], [2, 2], [0, 2, 3], [0, 4]]))\n-    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)\n-    input_array = tf.ragged.constant([[1, 2, 3], [3, 1]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [0, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n-\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.adapt(vocab_dataset)\n-    int_data = layer(input_data)\n-\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array, steps=1)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-  def test_hard_maximum_set_state_variables_after_build(self):\n-    state_variables = {category_encoding._NUM_ELEMENTS_NAME: 5}\n-    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [1, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n+  def test_legacy_max_tokens_arg(self):\n+    input_array = np.array([[1, 2, 3, 1]])\n+    expected_output = [[0, 1, 1, 1, 0, 0]]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.BINARY)\n-    int_data = layer(input_data)\n-    layer._set_state_variables(state_variables)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-  def test_soft_maximum_set_state_after_build(self):\n-    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [1, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.build(input_data.shape)\n-    layer.set_num_elements(max_tokens)\n+        max_tokens=num_tokens, output_mode=category_encoding.BINARY)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -389,34 +299,6 @@ class CategoryEncodingAdaptTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_set_weights_fails_on_wrong_size_weights(self):\n-    tfidf_data = [.05, .5, .25, .2, .125]\n-    layer = get_layer_class()(max_tokens=6, output_mode=category_encoding.TFIDF)\n-\n-    with self.assertRaisesRegex(ValueError, \".*Layer weight shape.*\"):\n-      layer.set_weights([np.array(tfidf_data)])\n-\n-  def test_set_num_elements_after_call_fails(self):\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.adapt([1, 2])\n-    _ = layer(input_data)\n-    with self.assertRaisesRegex(\n-        RuntimeError, \".*'max_tokens' arg must be set to None.\"):\n-      layer.set_num_elements(5)\n-\n-  def test_set_state_variables_after_call_fails(self):\n-    state_variables = {category_encoding._NUM_ELEMENTS_NAME: 5}\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.adapt([1, 2])\n-    _ = layer(input_data)\n-    with self.assertRaisesRegex(RuntimeError, \"Cannot update states.*\"):\n-      layer._set_state_variables(state_variables)\n-\n \n @keras_parameterized.run_all_keras_modes\n @keras_parameterized.run_all_keras_modes\n@@ -424,19 +306,19 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n-  def test_binary_output_hard_maximum(self):\n+  def test_binary_output(self):\n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n \n     # pyformat: disable\n     expected_output = [[0, 1, 1, 1, 0, 0],\n                        [1, 1, 0, 1, 0, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     layer = get_layer_class()(\n-        max_tokens=max_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -444,39 +326,18 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_binary_output_soft_maximum(self):\n-    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [1, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.BINARY)\n-    layer.set_num_elements(max_tokens)\n-    int_data = layer(input_data)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-  def test_count_output_hard_maximum(self):\n+  def test_count_output(self):\n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n \n     # pyformat: disable\n     expected_output = [[0, 2, 1, 1, 0, 0],\n                        [2, 1, 0, 1, 0, 0]]\n     # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n+    num_tokens = 6\n+    expected_output_shape = [None, num_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(max_tokens=6, output_mode=category_encoding.COUNT)\n+    layer = get_layer_class()(num_tokens=6, output_mode=category_encoding.COUNT)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -484,75 +345,6 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_count_output_soft_maximum(self):\n-    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 2, 1, 1, 0],\n-                       [2, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.COUNT)\n-    layer.set_num_elements(max_tokens)\n-    int_data = layer(input_data)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n-  def test_tfidf_output_hard_maximum(self):\n-    tfidf_data = [.05, .5, .25, .2, .125]\n-    input_array = np.array([[1, 2, 3, 1], [0, 4, 1, 0]])\n-\n-    # pyformat: disable\n-    # pylint: disable=bad-whitespace\n-    expected_output = [[ 0,  1, .25, .2,    0, 0],\n-                       [.1, .5,   0,  0, .125, 0]]\n-    # pylint: enable=bad-whitespace\n-    # pyformat: enable\n-    max_tokens = 6\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(max_tokens=6, output_mode=category_encoding.TFIDF)\n-    layer.set_tfidf_data(tfidf_data)\n-    int_data = layer(input_data)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllClose(expected_output, output_dataset)\n-\n-  def test_tfidf_output_soft_maximum(self):\n-    tfidf_data = [.05, .5, .25, .2, .125]\n-    input_array = np.array([[1, 2, 3, 1], [0, 4, 1, 0]])\n-\n-    # pyformat: disable\n-    # pylint: disable=bad-whitespace\n-    expected_output = [[ 0,  1, .25, .2,    0],\n-                       [.1, .5,   0,  0, .125]]\n-    # pylint: enable=bad-whitespace\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(\n-        max_tokens=None, output_mode=category_encoding.TFIDF)\n-    layer.set_num_elements(max_tokens)\n-    layer.set_tfidf_data(tfidf_data)\n-    int_data = layer(input_data)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllClose(expected_output, output_dataset)\n-\n \n class CategoryEncodingModelBuildingTest(\n     keras_parameterized.TestCase,\n@@ -560,43 +352,23 @@ class CategoryEncodingModelBuildingTest(\n \n   @parameterized.named_parameters(\n       {\n-          \"testcase_name\": \"count_hard_max\",\n-          \"max_tokens\": 5,\n+          \"testcase_name\": \"count_output\",\n+          \"num_tokens\": 5,\n           \"output_mode\": category_encoding.COUNT\n       }, {\n-          \"testcase_name\": \"count_soft_max\",\n-          \"max_tokens\": None,\n-          \"output_mode\": category_encoding.COUNT\n-      }, {\n-          \"testcase_name\": \"binary_hard_max\",\n-          \"max_tokens\": 5,\n+          \"testcase_name\": \"binary_output\",\n+          \"num_tokens\": 5,\n           \"output_mode\": category_encoding.BINARY\n-      }, {\n-          \"testcase_name\": \"binary_soft_max\",\n-          \"max_tokens\": None,\n-          \"output_mode\": category_encoding.BINARY\n-      }, {\n-          \"testcase_name\": \"tfidf_hard_max\",\n-          \"max_tokens\": 5,\n-          \"output_mode\": category_encoding.TFIDF\n-      }, {\n-          \"testcase_name\": \"tfidf_soft_max\",\n-          \"max_tokens\": None,\n-          \"output_mode\": category_encoding.TFIDF\n       })\n-  def test_end_to_end_bagged_modeling(self, output_mode, max_tokens):\n-    tfidf_data = np.array([.03, .5, .25, .2, .125])\n+  def test_end_to_end_bagged_modeling(self, output_mode, num_tokens):\n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n-    layer = get_layer_class()(max_tokens=max_tokens, output_mode=output_mode)\n+    layer = get_layer_class()(num_tokens=num_tokens, output_mode=output_mode)\n \n     weights = []\n-    if max_tokens is None:\n+    if num_tokens is None:\n       layer.set_num_elements(5)\n-    if output_mode == category_encoding.TFIDF:\n-      weights.append(tfidf_data)\n-\n     layer.set_weights(weights)\n \n     int_data = layer(input_data)\n@@ -606,160 +378,5 @@ class CategoryEncodingModelBuildingTest(\n     _ = model.predict(input_array)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class CategoryEncodingCombinerTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def compare_idf_accumulators(self, a, b, msg=None):\n-    if a is None or b is None:\n-      self.assertAllEqual(a, b, msg=msg)\n-\n-    self.assertAllEqual(a.data, b.data, msg=msg)\n-\n-    if a.per_doc_count_dict is not None:\n-\n-      def per_doc_counts(accumulator):\n-        count_values = [\n-            count_dict[\"count\"]\n-            for count_dict in accumulator.per_doc_count_dict.values()\n-        ]\n-        return dict(zip(accumulator.per_doc_count_dict.keys(), count_values))\n-\n-      self.assertAllEqual(per_doc_counts(a), per_doc_counts(b), msg=msg)\n-\n-  compare_accumulators = compare_idf_accumulators\n-\n-  def update_accumulator(self, accumulator, data):\n-    accumulator.data[1] = data[\"num_documents\"]\n-    accumulator.data[0] = data[\"max_element\"]\n-\n-    if \"document_counts\" in data:\n-      create_dict = lambda x: {\"count\": x, \"last_doc_id\": -1}\n-      idf_dict = {}\n-      for i, count in enumerate(data[\"document_counts\"]):\n-        if count > 0:\n-          idf_dict[i] = create_dict(count)\n-\n-      accumulator.per_doc_count_dict.update(idf_dict)\n-\n-    return accumulator\n-\n-  def test_combiner_api_compatibility_int_mode(self):\n-    data = np.array([[1, 2, 3, 4], [1, 2, 3, 0]])\n-    combiner = category_encoding._CategoryEncodingCombiner(compute_idf=False)\n-    expected_accumulator_output = {\n-        \"max_element\": np.array(4),\n-        \"num_documents\": np.array(2),\n-    }\n-    expected_extract_output = {\n-        \"num_elements\": np.array(5),\n-    }\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_serialize_and_deserialize(combiner, data,\n-                                                        expected_accumulator)\n-    self.validate_accumulator_uniqueness(combiner, data)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-  def test_combiner_api_compatibility_tfidf_mode(self):\n-    data = np.array([[1, 2, 3, 4], [1, 2, 3, 0]])\n-    combiner = category_encoding._CategoryEncodingCombiner(compute_idf=True)\n-    expected_accumulator_output = {\n-        \"max_element\": np.array(4),\n-        \"document_counts\": np.array([1, 2, 2, 2, 1]),\n-        \"num_documents\": np.array(2),\n-    }\n-    expected_extract_output = {\n-        \"num_elements\": np.array(5),\n-        \"idf\": np.array([0.693147, 0.510826, 0.510826, 0.510826, 0.693147]),\n-    }\n-\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_serialize_and_deserialize(combiner, data,\n-                                                        expected_accumulator)\n-    self.validate_accumulator_uniqueness(combiner, data)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-  # TODO(askerryryan): Add tests confirming equivalence to behavior of\n-  # existing tf.keras.preprocessing.text.Tokenizer.\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"no_top_k\",\n-          \"data\": np.array([[1, 2], [4, 2], [3], [4, 2]]),\n-          \"expected_accumulator_output\": {\n-              \"max_element\": np.array(4),\n-              \"document_counts\": np.array([0, 1, 3, 1, 2]),\n-              \"num_documents\": np.array(4),\n-          },\n-          \"expected_extract_output\": {\n-              \"num_elements\":\n-                  np.array(5),\n-              \"idf\":\n-                  np.array([1.609438, 1.098612, 0.693147, 1.098612, 0.847298]),\n-          },\n-      }, {\n-          \"testcase_name\": \"single_element_per_row\",\n-          \"data\": np.array([[1], [2], [4], [2], [3]]),\n-          \"expected_accumulator_output\": {\n-              \"max_element\": np.array(4),\n-              \"document_counts\": np.array([0, 1, 2, 1, 1]),\n-              \"num_documents\": np.array(5),\n-          },\n-          \"expected_extract_output\": {\n-              \"num_elements\":\n-                  np.array(5),\n-              \"idf\":\n-                  np.array([1.791759, 1.252763, 0.980829, 1.252763, 1.252763]),\n-          },\n-      })\n-  def test_combiner_computation(self,\n-                                data,\n-                                expected_accumulator_output,\n-                                expected_extract_output,\n-                                compute_idf=True):\n-    combiner = category_encoding._CategoryEncodingCombiner(\n-        compute_idf=compute_idf)\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_computation(combiner, data, expected_accumulator)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-  def test_1d_data(self):\n-    data = [1, 2, 3]\n-    cls = get_layer_class()\n-    layer = cls()\n-    layer.adapt(data)\n-    output = layer(data)\n-    self.assertListEqual(output.shape.as_list(), [3, 4])\n-\n-  def test_no_adapt_exception(self):\n-    cls = get_layer_class()\n-    layer = cls()\n-    with self.assertRaisesRegex(\n-        RuntimeError, r\".*you need to call.*\"):\n-      _ = layer([1, 2, 3])\n-\n-  def test_saving_loading(self):\n-    cls = get_layer_class()\n-    encoder = cls()\n-    encoder.adapt([1, 2, 3])\n-    model = keras.Sequential([encoder])\n-    model.save(\"/tmp/model\", save_format=\"tf\")\n-    loaded_model = keras.models.load_model(\"/tmp/model\")\n-    self.assertAllClose(model.predict([[1]]), loaded_model.predict([[1]]))\n-\n-  def test_serialize(self):\n-    cls = get_layer_class()\n-    encoder = cls()\n-    encoder.adapt([1, 2, 3])\n-    model = keras.Sequential([encoder])\n-    _ = keras.models.clone_model(model)\n-\n-\n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -1,69 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tensorflow V1 version of the text category_encoding preprocessing layer.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.engine import base_preprocessing_layer_v1\n-from keras.layers.preprocessing import category_encoding\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(v1=[\"keras.layers.experimental.preprocessing.CategoryEncoding\"])\n-class CategoryEncoding(category_encoding.CategoryEncoding,\n-                       base_preprocessing_layer_v1.CombinerPreprocessingLayer):\n-  \"\"\"CategoryEncoding layer.\n-\n-  This layer provides options for condensing input data into denser\n-  representations. It accepts either integer values or strings as inputs,\n-  allows users to map those inputs into a contiguous integer space, and\n-  outputs either those integer values (one sample = 1D tensor of integer token\n-  indices) or a dense representation (one sample = 1D tensor of float values\n-  representing data about the sample's tokens).\n-\n-  If desired, the user can call this layer's adapt() method on a dataset.\n-  When this layer is adapted, it will analyze the dataset, determine the\n-  frequency of individual integer or string values, and create a 'vocabulary'\n-  from them. This vocabulary can have unlimited size or be capped, depending\n-  on the configuration options for this layer; if there are more unique\n-  values in the input than the maximum vocabulary size, the most frequent\n-  terms will be used to create the vocabulary.\n-\n-  Attributes:\n-    max_elements: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary.\n-    output_mode: Optional specification for the output of the layer. Values can\n-      be \"int\", \"binary\", \"count\" or \"tf-idf\", configuring the layer as follows:\n-        \"int\": Outputs integer indices, one integer index per split string\n-          token.\n-        \"binary\": Outputs a single int array per batch, of either vocab_size or\n-          max_elements size, containing 1s in all elements where the token\n-          mapped to that index exists at least once in the batch item.\n-        \"count\": As \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n-    output_sequence_length: Only valid in INT mode. If set, the output will have\n-      its time dimension padded or truncated to exactly `output_sequence_length`\n-      values, resulting in a tensor of shape [batch_size,\n-      output_sequence_length] regardless of the input shape.\n-    pad_to_max_elements: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes.\n-      If True, the output will have its feature axis padded to `max_elements`\n-      even if the number of unique values in the vocabulary is less than\n-      max_elements, resulting in a tensor of shape [batch_size, max_elements]\n-      regardless of vocabulary size. Defaults to False.\n-  \"\"\"\n\n@@ -408,7 +408,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n       mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n       with tf.compat.v1.test.mock.patch.object(\n           random_ops, 'random_uniform', return_value=mock_random):\n-        with self.cached_session(use_gpu=True):\n+        with self.cached_session():\n           layer = image_preprocessing.RandomFlip()\n           actual_output = layer(input_images, training=1)\n           self.assertAllClose(expected_output, actual_output)\n@@ -694,7 +694,7 @@ class RandomTransformTest(keras_parameterized.TestCase):\n                                       fill_value=0.0,\n                                       interpolation='bilinear'):\n     inp = np.arange(15).reshape((1, 5, 3, 1)).astype(np.float32)\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       output = image_preprocessing.transform(\n           inp,\n           transform_matrix,\n\n@@ -179,18 +179,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       self._value_dtype = tf.int64\n       oov_value = self._oov_value\n \n-    self._table = lookup_ops.MutableHashTable(\n-        key_dtype=self._key_dtype,\n-        value_dtype=self._value_dtype,\n-        default_value=oov_value,\n-        name=(self._name + \"_index_table\"))\n-    tracked_table = self._add_trackable(self._table, trainable=False)\n-    # This is a workaround for summary() on this layer. Because the table is\n-    # not mutable during training, the effective number of parameters (and so\n-    # the weight shape) is 0; we add this as an attr so that the parameter\n-    # counting code in the Model object doesn't throw an attribute error.\n-    tracked_table.shape = tf.TensorShape((0,))\n-\n     if self.num_oov_indices <= 1:\n       oov_indices = None\n     else:\n@@ -198,11 +186,28 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       oov_end = oov_start + num_oov_indices\n       oov_indices = list(range(oov_start, oov_end))\n \n+    if vocabulary is not None and isinstance(vocabulary,\n+                                             tf.lookup.TextFileInitializer):\n+      self._table = self._static_table_class()(\n+          vocabulary, default_value=oov_value)\n+      self._table_handler = table_utils.TableHandler(\n+          table=self._table,\n+          mask_token=mask_token,\n+          oov_tokens=oov_indices,\n+          use_v1_apis=self._use_v1_apis())\n+      self.max_tokens = (\n+          self._table_handler.vocab_size() + self.num_oov_indices +\n+          (0 if mask_token is None else 1))\n+    else:\n+      self._table = lookup_ops.MutableHashTable(\n+          key_dtype=self._key_dtype,\n+          value_dtype=self._value_dtype,\n+          default_value=oov_value,\n+          name=(self._name + \"_index_table\"))\n       self._table_handler = table_utils.TableHandler(\n           table=self._table,\n           oov_tokens=oov_indices,\n           use_v1_apis=self._use_v1_apis())\n-\n       if vocabulary is not None:\n         self.set_vocabulary(vocabulary)\n \n@@ -227,6 +232,13 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n           dtype=K.floatx(),\n           initializer=initializer)\n \n+    tracked_table = self._add_trackable(self._table, trainable=False)\n+    # This is a workaround for summary() on this layer. Because the table is\n+    # not mutable during training, the effective number of parameters (and so\n+    # the weight shape) is 0; we add this as an attr so that the parameter\n+    # counting code in the Model object doesn't throw an attribute error.\n+    tracked_table.shape = tf.TensorShape((0,))\n+\n   def compute_output_shape(self, input_shape):\n     if self.output_mode != INT:\n       return tf.TensorShape([input_shape[0], self.max_tokens])\n@@ -533,6 +545,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _use_v1_apis(self):\n     return False\n \n+  def _static_table_class(self):\n+    return tf.lookup.StaticHashTable\n+\n \n class _IndexLookupAccumulator(\n     collections.namedtuple(\"Accumulator\",\n\n@@ -696,6 +696,15 @@ class CategoricalEncodingAdaptTest(\n class IndexLookupOutputTest(keras_parameterized.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n+  def _write_to_temp_file(self, file_name, vocab_list):\n+    vocab_path = os.path.join(self.get_temp_dir(), file_name + \".txt\")\n+    with tf.io.gfile.GFile(vocab_path, \"w\") as writer:\n+      for vocab in vocab_list:\n+        writer.write(vocab + \"\\n\")\n+      writer.flush()\n+      writer.close()\n+    return vocab_path\n+\n   def test_int_output(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n@@ -951,7 +960,60 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     layer_output = layer(input_data)\n     self.assertAllEqual(layer_output.shape.as_list(), [16, 2])\n \n+  def test_int_output_file_vocab(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]\n \n+    vocab_file = self._write_to_temp_file(\"temp\", vocab_data)\n+    vocabulary_initializer = tf.lookup.TextFileInitializer(\n+        filename=vocab_file,\n+        key_dtype=tf.string,\n+        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n+        value_dtype=tf.int64,\n+        value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n+        value_index_offset=2)\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        vocabulary=vocabulary_initializer,\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n+  def test_int_output_int_file_vocab(self):\n+    vocab_data = [\"10\", \"20\", \"30\", \"40\"]\n+    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])\n+    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]\n+\n+    vocab_file = self._write_to_temp_file(\"temp\", vocab_data)\n+    vocabulary_initializer = tf.lookup.TextFileInitializer(\n+        filename=vocab_file,\n+        key_dtype=tf.int64,\n+        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n+        value_dtype=tf.int64,\n+        value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n+        value_index_offset=2)\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.int64)\n+    layer = get_layer_class()(\n+        vocabulary=vocabulary_initializer,\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=0,\n+        oov_token=-1,\n+        dtype=tf.int64)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n @keras_parameterized.run_all_keras_modes\n class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n                                 preprocessing_test_utils.PreprocessingLayerTest\n\n@@ -18,6 +18,8 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow as tf\n+\n \n from keras.engine import base_preprocessing_layer_v1\n from keras.layers.preprocessing import index_lookup\n@@ -58,3 +60,6 @@ class IndexLookup(index_lookup.IndexLookup,\n \n   def _use_v1_apis(self):\n     return True\n+\n+  def _static_table_class(self):\n+    return tf.compat.v1.lookup.StaticHashTable\n\n@@ -24,13 +24,30 @@ import os\n import numpy as np\n from keras import backend as K\n from keras.utils import tf_utils\n+from tensorflow.python.ops import lookup_ops\n \n \n class TableHandler(object):\n   \"\"\"Wrapper object that holds a lookup table and provides accessors.\"\"\"\n \n-  def __init__(self, table, oov_tokens=None, use_v1_apis=False):\n+  def __init__(self,\n+               table,\n+               oov_tokens=None,\n+               mask_token=None,\n+               use_v1_apis=False):\n     self.table = table\n+\n+    # If we are using V1 APIs, and the table has an initializer, we need to run\n+    # it. However, not all tables have initializers, so we try-except here.\n+    if use_v1_apis:\n+      try:\n+        K.get_session().run(self.table.initializer)\n+      except AttributeError:\n+        pass\n+\n+    self.mutable = isinstance(table, lookup_ops.MutableHashTable)\n+    self.mask_token = mask_token\n+\n     self.use_v1_apis = use_v1_apis\n     if oov_tokens is None:\n       self.oov_tokens = oov_tokens\n@@ -47,10 +64,17 @@ class TableHandler(object):\n     return self._eval(self.table.size())\n \n   def clear(self):\n+    if not self.mutable:\n+      return RuntimeError(\"Unable to clear a statically-backed table.\")\n+\n     keys, _ = self.table.export()\n     self._run(self.table.remove(keys))\n \n   def insert(self, keys, values):\n+    \"\"\"Insert values into the backed table.\"\"\"\n+    if not self.mutable:\n+      raise RuntimeError(\"Unable to insert into a statically-backed table.\")\n+\n     if len(values) != len(keys):\n       raise RuntimeError(\"Size mismatch between values and key arrays. \"\n                          \"Keys had size %s, values had size %s.\" %\n@@ -81,12 +105,35 @@ class TableHandler(object):\n \n     return tf.compat.v1.where(oov_locations, oov_values, lookups)\n \n+  def _lookup_and_mask(self, inputs):\n+    \"\"\"Return a lookup with any location with the mask_token masked to 0.\"\"\"\n+    lookups = self.table.lookup(inputs)\n+    # If we don't need to handle masking, return the lookup values directly.\n+    if self.mask_token is None:\n+      return lookups\n+\n+    # If we do need to handle masking, increment all the lookup values by 1\n+    # to account for the mask value at location 0. This also increments the\n+    # OOV value, so replace that. (This is inefficient, but we can't adjust\n+    # the table safely, so we don't have a choice.)\n+    oov_locations = tf.equal(lookups, self.table._default_value)  # pylint: disable=protected-access\n+    oov_values = tf.compat.v1.ones_like(\n+        lookups, dtype=self.table._value_dtype) * self.table._default_value  # pylint: disable=protected-access\n+    adjusted_lookups = tf.compat.v1.where(oov_locations, oov_values, lookups)\n+\n+    # Inject 0s wherever the mask token was in the inputs.\n+    mask_locations = tf.equal(inputs, self.mask_token)\n+    return tf.compat.v1.where(\n+        mask_locations,\n+        tf.compat.v1.zeros_like(lookups, dtype=self.table._value_dtype),  # pylint: disable=protected-access\n+        adjusted_lookups)  # pylint: disable=protected-access\n+\n   def _ragged_lookup(self, inputs):\n     \"\"\"Perform a table lookup on a ragged tensor.\"\"\"\n     # The table lookup ops don't natively support ragged tensors, so if we have\n     # a RT we need to use map_flat_values to look up every element.\n     indexed_data = tf.ragged.map_flat_values(\n-        self.table.lookup, inputs)\n+        self._lookup_and_mask, inputs)\n     indexed_data = tf.ragged.map_flat_values(\n         self._replace_oov_buckets, inputs, indexed_data)\n     # table.lookup is not shape-preserving, so we need to set the shape here.\n@@ -98,7 +145,7 @@ class TableHandler(object):\n \n   def _sparse_lookup(self, inputs):\n     \"\"\"Perform a table lookup on a sparse tensor.\"\"\"\n-    values = self.table.lookup(inputs.values)\n+    values = self._lookup_and_mask(inputs.values)\n     values = self._replace_oov_buckets(inputs.values, values)\n     indexed_data = tf.SparseTensor(inputs.indices, values,\n                                               inputs.dense_shape)\n@@ -109,7 +156,7 @@ class TableHandler(object):\n \n   def _tensor_lookup(self, inputs):\n     \"\"\"Perform a table lookup on a tf.tensor.\"\"\"\n-    values = self.table.lookup(inputs)\n+    values = self._lookup_and_mask(inputs)\n     indexed_data = self._replace_oov_buckets(inputs, values)\n     # (b/149446477): output does not preserve input shape.\n     indexed_data.set_shape(inputs.shape)\n\n@@ -40,6 +40,41 @@ def get_table(dtype=tf.string, oov_tokens=None):\n       table, oov_tokens, use_v1_apis=(not tf.executing_eagerly()))\n \n \n+def get_static_table(tmpdir,\n+                     vocab_list,\n+                     mask_token=None,\n+                     dtype=tf.string,\n+                     oov_tokens=None):\n+  vocabulary_file = os.path.join(tmpdir, \"tmp_vocab.txt\")\n+\n+  if dtype == tf.string:\n+    with open(vocabulary_file, \"w\") as f:\n+      f.write(\"\\n\".join(vocab_list) + \"\\n\")\n+  else:\n+    with open(vocabulary_file, \"w\") as f:\n+      f.write(\"\\n\".join([str(v) for v in vocab_list]) + \"\\n\")\n+\n+  offset = ((0 if mask_token is None else 1) +\n+            (len(oov_tokens) if oov_tokens is not None else 0))\n+  init = tf.lookup.TextFileInitializer(\n+      vocabulary_file,\n+      dtype,\n+      tf.lookup.TextFileIndex.WHOLE_LINE,\n+      tf.int64,\n+      tf.lookup.TextFileIndex.LINE_NUMBER,\n+      value_index_offset=offset)\n+  if tf.executing_eagerly():\n+    table = tf.lookup.StaticHashTable(init, default_value=-7)\n+  else:\n+    table = tf.compat.v1.lookup.StaticHashTable(init, default_value=-7)\n+\n+  return table_utils.TableHandler(\n+      table,\n+      oov_tokens,\n+      mask_token=mask_token,\n+      use_v1_apis=(not tf.executing_eagerly()))\n+\n+\n @keras_parameterized.run_all_keras_modes\n class CategoricalEncodingInputTest(\n     keras_parameterized.TestCase,\n@@ -247,6 +282,132 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_data)\n \n \n+@keras_parameterized.run_all_keras_modes\n+class StaticIndexLookupOutputTest(\n+    keras_parameterized.TestCase,\n+    preprocessing_test_utils.PreprocessingLayerTest):\n+\n+  def test_int_output_default_lookup_value(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, -7]]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(),\n+        vocab_list=vocab_data,\n+        mask_token=\"\",\n+        oov_tokens=None)\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_output, output_data)\n+\n+  def test_output_shape(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(), vocab_list=vocab_data, oov_tokens=None)\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(input_array.shape[1:], output_data.shape[1:])\n+\n+  def test_int_output_no_reserved_zero_default_lookup_value(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[0, 1, 2, 3], [3, 2, 0, -7]]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(), vocab_list=vocab_data, oov_tokens=None)\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_output, output_data)\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class CategoricalEncodingStaticInputTest(\n+    keras_parameterized.TestCase,\n+    preprocessing_test_utils.PreprocessingLayerTest):\n+\n+  def test_sparse_string_input(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = tf.SparseTensor(\n+        indices=[[0, 0], [1, 2]],\n+        values=[\"fire\", \"michigan\"],\n+        dense_shape=[3, 4])\n+\n+    expected_indices = [[0, 0], [1, 2]]\n+    expected_values = [5, 1]\n+    expected_dense_shape = [3, 4]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(),\n+        vocab_list=vocab_data,\n+        mask_token=\"\",\n+        oov_tokens=[1])\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_indices, output_data.indices)\n+    self.assertAllEqual(expected_values, output_data.values)\n+    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n+\n+  def test_sparse_int_input(self):\n+    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n+    input_array = tf.SparseTensor(\n+        indices=[[0, 0], [1, 2]],\n+        values=np.array([13, 32], dtype=np.int64),\n+        dense_shape=[3, 4])\n+\n+    expected_indices = [[0, 0], [1, 2]]\n+    expected_values = [5, 1]\n+    expected_dense_shape = [3, 4]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(),\n+        vocab_list=vocab_data,\n+        dtype=tf.int64,\n+        mask_token=0,\n+        oov_tokens=[1])\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_indices, output_data.indices)\n+    self.assertAllEqual(expected_values, output_data.values)\n+    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n+\n+  def test_ragged_string_input(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = tf.ragged.constant(\n+        [[\"earth\", \"wind\", \"fire\"], [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(),\n+        vocab_list=vocab_data,\n+        mask_token=\"\",\n+        oov_tokens=[1])\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_output, output_data)\n+\n+  def test_ragged_int_input(self):\n+    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n+    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],\n+                                              dtype=np.int64)\n+    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n+\n+    table = get_static_table(\n+        tmpdir=self.get_temp_dir(),\n+        vocab_list=vocab_data,\n+        dtype=tf.int64,\n+        mask_token=0,\n+        oov_tokens=[1])\n+    output_data = table.lookup(input_array)\n+\n+    self.assertAllEqual(expected_output, output_data)\n+\n+\n class GetVocabularyFromFileTest(tf.test.TestCase):\n \n   def setUp(self):\n\n@@ -36,7 +36,7 @@ class SeparableConv1DTest(keras_parameterized.TestCase):\n     stack_size = 3\n     length = 7\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.SeparableConv1D,\n           kwargs=kwargs,\n@@ -67,7 +67,7 @@ class SeparableConv1DTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.SeparableConv1D(**kwargs)\n       layer.build((None, 5, 2))\n       self.assertEqual(len(layer.losses), 3)\n@@ -88,7 +88,7 @@ class SeparableConv1DTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.SeparableConv1D(**kwargs)\n       layer.build((None, 5, 2))\n       self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)\n@@ -105,7 +105,7 @@ class SeparableConv2DTest(keras_parameterized.TestCase):\n     num_row = 7\n     num_col = 6\n \n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       testing_utils.layer_test(\n           keras.layers.SeparableConv2D,\n           kwargs=kwargs,\n@@ -139,7 +139,7 @@ class SeparableConv2DTest(keras_parameterized.TestCase):\n         'activity_regularizer': 'l2',\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.SeparableConv2D(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(len(layer.losses), 3)\n@@ -160,7 +160,7 @@ class SeparableConv2DTest(keras_parameterized.TestCase):\n         'bias_constraint': b_constraint,\n         'strides': 1\n     }\n-    with self.cached_session(use_gpu=True):\n+    with self.cached_session():\n       layer = keras.layers.SeparableConv2D(**kwargs)\n       layer.build((None, 5, 5, 2))\n       self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)\n\n@@ -48,7 +48,6 @@ from keras.layers import rnn_cell_wrapper_v2\n from keras.layers import wrappers\n from keras.layers.preprocessing import category_crossing\n from keras.layers.preprocessing import category_encoding\n-from keras.layers.preprocessing import category_encoding_v1\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing import image_preprocessing\n@@ -71,12 +70,11 @@ ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                pooling, image_preprocessing, preprocessing_integer_lookup_v1,\n                preprocessing_normalization_v1, preprocessing_string_lookup_v1,\n                preprocessing_text_vectorization_v1, recurrent, wrappers,\n-               hashing, category_crossing, category_encoding_v1, discretization,\n+               hashing, category_crossing, category_encoding, discretization,\n                multi_head_attention)\n ALL_V2_MODULES = (rnn_cell_wrapper_v2, normalization_v2, recurrent_v2,\n                   preprocessing_integer_lookup, preprocessing_normalization,\n-                  preprocessing_string_lookup, preprocessing_text_vectorization,\n-                  category_encoding)\n+                  preprocessing_string_lookup, preprocessing_text_vectorization)\n # ALL_OBJECTS is meant to be a global mutable. Hence we need to make it\n # thread-local to avoid concurrent mutations.\n LOCAL = threading.local()\n\n@@ -398,7 +398,7 @@ class BNTest(tf.test.TestCase):\n       training = tf.compat.v1.placeholder(dtype='bool')\n       outputs = bn.apply(inputs, training=training)\n \n-      with self.session(use_gpu=True) as sess:\n+      with self.session() as sess:\n         # Test training with placeholder learning phase.\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n         np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])\n@@ -889,7 +889,7 @@ class BNTest(tf.test.TestCase):\n     moving_stddev = 1.\n     renorm_mean = 0.\n     renorm_stddev = 1.\n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -939,7 +939,7 @@ class BNTest(tf.test.TestCase):\n     moving_stddev = 1.\n     renorm_mean = 0.\n     renorm_stddev = 1.\n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for step in range(6):\n         x = np.random.random(shape)\n@@ -993,7 +993,7 @@ class BNTest(tf.test.TestCase):\n \n     moving_mean = 0.\n     moving_variance = 1.\n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -1046,7 +1046,7 @@ class BNTest(tf.test.TestCase):\n     moving_stddev = 1.\n     renorm_mean = 0.\n     renorm_stddev = 1.\n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -1092,7 +1092,7 @@ class BNTest(tf.test.TestCase):\n     self.assertListEqual(\n         out1.shape.as_list(), out2.shape.as_list())\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n \n       x = np.random.random(shape)\n@@ -1114,7 +1114,7 @@ class BNTest(tf.test.TestCase):\n     out = normalization_layers.batch_normalization(\n         inp, virtual_batch_size=2)\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n \n       x = np.random.random(np_shape)\n@@ -1145,7 +1145,7 @@ class BNTest(tf.test.TestCase):\n                     shape[0] // virtual_batch_size,\n                     shape[1]])\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -1198,7 +1198,7 @@ class BNTest(tf.test.TestCase):\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -1252,7 +1252,7 @@ class BNTest(tf.test.TestCase):\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n@@ -1404,7 +1404,7 @@ class BNTest(tf.test.TestCase):\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n-    with self.session(use_gpu=True) as sess:\n+    with self.session() as sess:\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       for _ in range(5):\n         x = np.random.random(shape)\n\n@@ -20,6 +20,7 @@ from __future__ import print_function\n import tensorflow as tf\n \n import abc\n+import functools\n \n import six\n from tensorflow.python.framework import smart_cond\n@@ -1220,7 +1221,31 @@ def _ragged_tensor_apply_loss(loss_fn, y_true, y_pred):\n     (per-batch loss value); a ragged tensor otherwise.\n   \"\"\"\n \n+  def rt_is_equiv_dense(rt):\n+    \"\"\"Returns true if this RaggedTensor has the same row_lenghts across\n+\n+       all ragged dimensions and thus can be converted to a dense tensor\n+       without loss of information.\n+\n+    Args:\n+      rt: RaggedTensor\n+    \"\"\"\n+    return tf.reduce_all([\n+        tf.equal(\n+            tf.math.reduce_variance(tf.cast(row_lens, K.floatx())),\n+            tf.constant([0.])) for row_lens in rt.nested_row_lengths()\n+    ])\n+\n+  def _convert_to_dense(inputs):\n+    return tuple(rt.to_tensor() for rt in inputs)\n+\n   def _wrapper(inputs):\n+    _, y_pred = inputs\n+    if isinstance(y_pred, tf.RaggedTensor):\n+      return tf.compat.v1.cond(\n+          rt_is_equiv_dense(y_pred),\n+          lambda: loss_fn(*_convert_to_dense(inputs)), lambda: loss_fn(*inputs))\n+\n     return loss_fn(*inputs)\n \n   lshape = y_pred.shape.as_list()[1:-1]\n@@ -1593,6 +1618,31 @@ def categorical_crossentropy(y_true,\n   return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n \n \n+@dispatch.dispatch_for_types(categorical_crossentropy,\n+                             tf.RaggedTensor)\n+def _ragged_tensor_categorical_crossentropy(y_true,\n+                                            y_pred,\n+                                            from_logits=False,\n+                                            label_smoothing=0):\n+  \"\"\" Implements support for handling RaggedTensors.\n+\n+      Expected shape: (batch, sequence_len, n_classes) with sequence_len\n+      being variable per batch.\n+      Return shape: (batch, sequence_len).\n+\n+      When used by CategoricalCrossentropy() with the default reduction\n+      (SUM_OVER_BATCH_SIZE), the reduction averages the loss over the\n+      number of elements independent of the batch. E.g. if the RaggedTensor\n+      has 2 batches with [2, 1] values respectivly the resulting loss is\n+      the sum of the individual loss values divided by 3.\n+  \"\"\"\n+  fn = functools.partial(\n+      categorical_crossentropy,\n+      from_logits=from_logits,\n+      label_smoothing=label_smoothing)\n+  return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n+\n+\n @keras_export('keras.metrics.sparse_categorical_crossentropy',\n               'keras.losses.sparse_categorical_crossentropy')\n @tf.__internal__.dispatch.add_dispatch_support\n\n@@ -995,6 +995,26 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     with self.assertRaisesRegex(ValueError, 'Shapes .+ are incompatible'):\n       cce_obj(y_true, y_pred)\n \n+  def test_ragged_tensors(self):\n+    cce_obj = losses.CategoricalCrossentropy()\n+    y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])\n+    y_pred = tf.ragged.constant(\n+        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],\n+        dtype=tf.float32)\n+    # batch losses [[0.1054, 0.8047], [0.0619]]\n+    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))\n+    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+    # sum([0.1054, 0.8047, 0.0619]) / 3\n+    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)\n+\n+    # Test with logits.\n+    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],\n+                                          [[2., 3., 5.]]])\n+    cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n+    # batch losses [[0.0018, 0.0004], [0.1698]]\n+    loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n\n@@ -106,7 +106,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSparse(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -196,7 +196,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def doTestBasic(self, use_callable_params=False):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -254,7 +254,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithAmsgrad(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, v0hat, m1, v1, v1hat = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -346,7 +346,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testBasicWithLearningRateDecay(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -391,7 +391,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testBasicWithLearningRateInverseTimeDecay(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -438,7 +438,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testTensorLearningRate(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -477,7 +477,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSharing(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -558,7 +558,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSparse(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -648,7 +648,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def doTestBasic(self, use_callable_params=False):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -708,7 +708,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithAmsgrad(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with self.cached_session(use_gpu=True):\n+      with self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, v0hat, m1, v1, v1hat = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -802,7 +802,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testBasicWithLearningRateDecay(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -847,7 +847,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testBasicWithLearningRateInverseTimeDecay(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -894,7 +894,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testTensorLearningRate(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -933,7 +933,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSharing(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n\n@@ -74,7 +74,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testResourceSparse(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         zero_slots = lambda: np.zeros((3), dtype=dtype.as_numpy_dtype)  # pylint: disable=cell-var-from-loop\n         m0, v0, m1, v1 = zero_slots(), zero_slots(), zero_slots(), zero_slots()\n@@ -268,7 +268,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testTensorLearningRate(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n@@ -305,7 +305,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSharing(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         # Initialize variables for numpy implementation.\n         m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n\n@@ -29,7 +29,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def doTestFtrlwithoutRegularization(self, use_resource=False):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         if use_resource:\n           var0 = tf.Variable([0.0, 0.0], dtype=dtype)\n           var1 = tf.Variable([0.0, 0.0], dtype=dtype)\n@@ -69,7 +69,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testFtrlwithoutRegularization2(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -99,7 +99,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testMinimizeSparseResourceVariable(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32, tf.float64]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n         x = tf.constant([[4.0], [5.0]], dtype=dtype)\n \n@@ -121,7 +121,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testFtrlWithL1(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -151,7 +151,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testFtrlWithBeta(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -177,7 +177,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testFtrlWithL2_Beta(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -208,7 +208,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testFtrlWithL1_L2(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -245,7 +245,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n     \"\"\"\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([4.0, 3.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -278,7 +278,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n     \"\"\"Tests the new FTRL op with support for l2 shrinkage on sparse grads.\"\"\"\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         var0 = tf.Variable([[1.0], [2.0]], dtype=dtype)\n         var1 = tf.Variable([[4.0], [3.0]], dtype=dtype)\n         grads0 = tf.IndexedSlices(\n@@ -313,7 +313,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n     \"\"\"Verifies that l2 shrinkage in FTRL does not change lr schedule.\"\"\"\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True) as sess:\n+      with tf.Graph().as_default(), self.cached_session() as sess:\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([1.0, 2.0], dtype=dtype)\n         grads0 = tf.constant([0.1, 0.2], dtype=dtype)\n@@ -396,7 +396,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testEquivAdagradwithoutRegularization(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val0, val1 = self.applyOptimizer(\n             ftrl.Ftrl(\n                 3.0,\n@@ -407,7 +407,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n                 l2_regularization_strength=0.0),\n             dtype)\n \n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val2, val3 = self.applyOptimizer(\n             tf.compat.v1.train.AdagradOptimizer(3.0, initial_accumulator_value=0.1), dtype)\n \n@@ -441,7 +441,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testEquivSparseGradientDescentwithoutRegularization(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val0, val1 = self.applyOptimizer(\n             ftrl.Ftrl(\n                 3.0,\n@@ -453,7 +453,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n             dtype,\n             is_sparse=True)\n \n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val2, val3 = self.applyOptimizer(\n             tf.compat.v1.train.GradientDescentOptimizer(3.0),\n             dtype,\n@@ -465,7 +465,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n   def testEquivGradientDescentwithoutRegularization(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for dtype in [tf.half, tf.float32]:\n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val0, val1 = self.applyOptimizer(\n             ftrl.Ftrl(\n                 3.0,\n@@ -476,7 +476,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n                 l2_regularization_strength=0.0),\n             dtype)\n \n-      with tf.Graph().as_default(), self.cached_session(use_gpu=True):\n+      with tf.Graph().as_default(), self.cached_session():\n         val2, val3 = self.applyOptimizer(\n             tf.compat.v1.train.GradientDescentOptimizer(3.0), dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
