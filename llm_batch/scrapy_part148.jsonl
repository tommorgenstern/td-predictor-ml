{"custom_id": "scrapy#fd58de493c3b7044bd3a93040d3e40ae56927113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 605 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,7 @@ class Slot(object):\n         self.queue = deque()\n         self.transferring = set()\n         self.lastseen = 0\n+        self.latercall = None\n \n     def free_transfer_slots(self):\n         return self.concurrency - len(self.transferring)\n@@ -57,9 +58,6 @@ def _get_concurrency_delay(concurrency, spider, settings):\n     if hasattr(spider, 'max_concurrent_requests'):\n         concurrency = spider.max_concurrent_requests\n \n-    if delay > 0:\n-        concurrency = 1 # force concurrency=1 if download delay required\n-\n     return concurrency, delay\n \n \n@@ -87,6 +85,7 @@ class Downloader(object):\n             slot.active.remove(request)\n             if not slot.active: # remove empty slots\n                 self.inactive_slots[key] = self.slots.pop(key)\n+\n             return response\n \n         dlfunc = partial(self._enqueue_request, slot=slot)\n@@ -124,23 +123,28 @@ class Downloader(object):\n         return deferred\n \n     def _process_queue(self, spider, slot):\n+        if slot.latercall and slot.latercall.active():\n+            return\n+\n         # Delay queue processing if a download_delay is configured\n         now = time()\n         delay = slot.download_delay()\n         if delay:\n             penalty = delay - now + slot.lastseen\n-            if penalty > 0 and slot.free_transfer_slots():\n-                d = defer.Deferred()\n-                d.addCallback(self._process_queue, slot)\n-                reactor.callLater(penalty, d.callback, spider)\n+            if penalty > 0:\n+                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n                 return\n-        slot.lastseen = now\n \n         # Process enqueued requests if there are free slots to transfer for this slot\n         while slot.queue and slot.free_transfer_slots() > 0:\n+            slot.lastseen = now\n             request, deferred = slot.queue.popleft()\n             dfd = self._download(slot, request, spider)\n             dfd.chainDeferred(deferred)\n+            # prevent burst if inter-request delays were configured\n+            if delay:\n+                self._process_queue(spider, slot)\n+                break\n \n     def _download(self, slot, request, spider):\n         # The order is very important for the following deferreds. Do not change!\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e676ad31d049a5718c26d12dde3076366b85b786", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 130 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,9 +39,9 @@ class Root(Resource):\n             qps = len(self.tail) / sum(self.tail)\n             print('samplesize={0} concurrent={1} qps={2:0.2f}'.format(len(self.tail), self.concurrent, qps))\n \n-        if 'sleep' in request.args:\n-            sleep = float(request.args['sleep'][0])\n-            reactor.callLater(sleep, self._finish, request)\n+        if 'latency' in request.args:\n+            latency = float(request.args['latency'][0])\n+            reactor.callLater(latency, self._finish, request)\n             return NOT_DONE_YET\n \n         self.concurrent -= 1\n\n@@ -3,7 +3,7 @@ A spider that generate light requests to meassure QPS troughput\n \n usage:\n \n-    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a sleep=0.3\n+    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3\n \n \"\"\"\n \n@@ -16,27 +16,40 @@ class QPSSpider(BaseSpider):\n     name = 'qps'\n     benchurl = 'http://localhost:8880/'\n \n-\n     # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n     max_concurrent_requests = 8\n     # Requests per second goal\n     qps = None # same as: 1 / download_delay\n+    download_delay = None\n     # time in seconds to delay server responses\n-    sleep = None \n+    latency = None\n+    # number of slots to create\n+    slots = 1\n \n     def __init__(self, *a, **kw):\n         super(QPSSpider, self).__init__(*a, **kw)\n         if self.qps is not None:\n             self.qps = float(self.qps)\n             self.download_delay = 1 / self.qps\n+        elif self.download_delay is not None:\n+            self.download_delay = float(self.download_delay)\n \n     def start_requests(self):\n         url = self.benchurl\n-        if self.sleep is not None:\n-            url += '?sleep={0}'.format(self.sleep)\n+        if self.latency is not None:\n+            url += '?latency={0}'.format(self.latency)\n \n+        slots = int(self.slots)\n+        if slots > 1:\n+            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in xrange(slots)]\n+        else:\n+            urls = [url]\n+\n+        idx = 0\n         while True:\n+            url = urls[idx % len(urls)]\n             yield Request(url, dont_filter=True)\n+            idx += 1\n \n     def parse(self, response):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8778af5c5be50a5d746751352f8d710d1f24681c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,8 @@ and subset the given arguments to match only\n those which are acceptable.\n \"\"\"\n \n+import inspect\n+\n def function( receiver ):\n \t\"\"\"Get function-like callable object for given receiver\n \n@@ -14,7 +16,7 @@ def function( receiver ):\n \tIf fromMethod is true, then the callable already\n \thas its first argument bound\n \t\"\"\"\n-\tif hasattr(receiver, '__call__'):\n+\tif inspect.isclass(receiver) and hasattr(receiver, '__call__'):\n \t\t# receiver is a class instance; assume it is callable.\n \t\t# Reassign receiver to the actual method that will be called.\n \t\tif hasattr( receiver.__call__, 'im_func') or hasattr( receiver.__call__, 'im_code'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c505b3367283e8e6f77b3cea00f924ca5e5e2758", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,13 +19,16 @@ def function( receiver ):\n     if inspect.isclass(receiver) and hasattr(receiver, '__call__'):\n         # receiver is a class instance; assume it is callable.\n         # Reassign receiver to the actual method that will be called.\n-\t\tif hasattr( receiver.__call__, 'im_func') or hasattr( receiver.__call__, 'im_code'):\n+        if hasattr(receiver.__call__, 'im_func') or \\\n+           hasattr(receiver.__call__, 'im_code'):\n             receiver = receiver.__call__\n+\n     if hasattr( receiver, 'im_func' ):\n         # an instance-method...\n         return receiver, receiver.im_func.func_code, 1\n     elif not hasattr(receiver, 'func_code'):\n         raise ValueError('unknown reciever type %s %s'%(receiver, type(receiver)))\n+\n     return receiver, receiver.func_code, 0\n \n def robustApply(receiver, *arguments, **named):\n@@ -40,12 +43,13 @@ def robustApply(receiver, *arguments, **named):\n                     name, receiver,\n                 )\n             )\n+\n     if not (codeObject.co_flags & 8):\n         # fc does not have a **kwds type parameter, therefore \n         # remove unacceptable arguments.\n         for arg in named.keys():\n             if arg not in acceptable:\n                 del named[arg]\n+\n     return receiver(*arguments, **named)\n \n-\t\t\t\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85c9881724321e7e58b40e307d992d692a620ded", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 614 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,17 +5,16 @@ from collections import deque\n from functools import partial\n \n from twisted.internet import reactor, defer\n-from twisted.python.failure import Failure\n \n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy import signals\n-from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n \n+\n class Slot(object):\n     \"\"\"Downloader slot\"\"\"\n \n@@ -50,7 +49,7 @@ def _get_concurrency_delay(concurrency, spider, settings):\n     # TODO: remove for Scrapy 0.15\n     c = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n     if c:\n-        warnings.warn(\"CONCURRENT_REQUESTS_PER_SPIDER setting is deprecated, \" \\\n+        warnings.warn(\"CONCURRENT_REQUESTS_PER_SPIDER setting is deprecated, \"\n                       \"use CONCURRENT_REQUESTS_PER_DOMAIN instead\", ScrapyDeprecationWarning)\n         concurrency = c\n     # ----------------------------\n@@ -80,6 +79,7 @@ class Downloader(object):\n \n         self.active.add(request)\n         slot.active.add(request)\n+\n         def _deactivate(response):\n             self.active.remove(request)\n             slot.active.remove(request)\n@@ -157,12 +157,13 @@ class Downloader(object):\n         # following requests (perhaps those which came from the downloader\n         # middleware itself)\n         slot.transferring.add(request)\n+\n         def finish_transferring(_):\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n             return _\n+\n         return dfd.addBoth(finish_transferring)\n \n     def is_idle(self):\n         return not self.slots\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7daf836d5091bf2190dc4711339aed3af5f83a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 88 | Files Changed: 2 | Hunks: 17 | Methods Changed: 20 | Complexity Δ (Sum/Max): -16/0 | Churn Δ: 145 | Churn Cumulative: 1106 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -1,106 +1,69 @@\n+import logging\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.resolver import dnscache\n+\n \n class AutoThrottle(object):\n \n     def __init__(self, crawler):\n-        settings = crawler.settings    \n-        if not settings.getbool('AUTOTHROTTLE_ENABLED'):\n-            raise NotConfigured\n         self.crawler = crawler\n-        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n-        crawler.signals.connect(self.response_received, signal=signals.response_received)\n-        self.START_DELAY = settings.getfloat(\"AUTOTHROTTLE_START_DELAY\", 5.0)\n-        self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n-        self.MAX_CONCURRENCY = self._max_concurency(settings)\n-        self.MIN_DOWNLOAD_DELAY = self._min_download_delay(settings)\n-        self.DEBUG = settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n-        self.last_latencies = [self.START_DELAY]\n-        self.last_lat = self.START_DELAY, 0.0\n+        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):\n+            raise NotConfigured\n \n-    def _min_download_delay(self, settings):\n-        return max(settings.getfloat(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\"),\n-            settings.getfloat(\"DOWNLOAD_DELAY\"))\n-\n-    def _max_concurency(self, settings):\n-        delay = self._min_download_delay(settings)\n-        if delay == 0:\n-            candidates = [\"AUTOTHROTTLE_MAX_CONCURRENCY\",\n-                \"CONCURRENT_REQUESTS_PER_DOMAIN\", \"CONCURRENT_REQUESTS_PER_IP\"]\n-            candidates = [settings.getint(x) for x in candidates]\n-            candidates = [x for x in candidates if x > 0]\n-            if candidates:\n-                return min(candidates)\n-        return 1\n+        self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n+        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n-    def spider_opened(self, spider):\n-        if hasattr(spider, \"download_delay\"):\n-            self.MIN_DOWNLOAD_DELAY = spider.download_delay\n-        spider.download_delay = self.START_DELAY\n-        if hasattr(spider, \"max_concurrent_requests\"):\n-            self.MAX_CONCURRENCY = spider.max_concurrent_requests\n-        # override in order to avoid to initialize slot with concurrency > 1\n-        spider.max_concurrent_requests = 1\n+    def _spider_opened(self, spider):\n+        self.mindelay = self._min_delay(spider)\n+        self.maxdelay = self._max_delay(spider)\n+        spider.download_delay = self._start_delay(spider)\n \n-    def response_received(self, response, spider):\n-        key, slot = self._get_slot(response.request)\n-        latency = response.meta.get('download_latency')\n+    def _min_delay(self, spider):\n+        s = self.crawler.settings\n+        return getattr(spider, 'download_delay', 0.0) or \\\n+            s.getfloat('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY') or \\\n+            s.getfloat('DOWNLOAD_DELAY')\n \n-        if not latency or not slot:\n+    def _max_delay(self, spider):\n+        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY', 60.0)\n+\n+    def _start_delay(self, spider):\n+        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY', 5.0))\n+\n+    def _response_downloaded(self, response, request, spider):\n+        key, slot = self._get_slot(request, spider)\n+        latency = request.meta.get('download_latency')\n+        if latency is None or slot is None:\n             return\n \n+        olddelay = slot.delay\n         self._adjust_delay(slot, latency, response)\n-        self._check_concurrency(slot, latency)\n+        if self.debug:\n+            diff = slot.delay - olddelay\n+            size = len(response.body)\n+            conc = len(slot.transferring)\n+            msg = \"slot: %s | conc:%2d | delay:%5d ms (%+d) | latency:%5d ms | size:%6d bytes\" % \\\n+                  (key, conc, slot.delay * 1000, diff * 1000, latency * 1000, size)\n+            spider.log(msg, level=logging.INFO)\n \n-        if self.DEBUG:\n-            spider.log(\"slot: %s | conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes\" % \\\n-                (key, slot.concurrency, slot.delay*1000, \\\n-                latency*1000, len(response.body)))\n-\n-    def _get_slot(self, request):\n-        downloader = self.crawler.engine.downloader\n-        key = urlparse_cached(request).hostname or ''\n-        if downloader.ip_concurrency:\n-            key = dnscache.get(key, key)\n-        return key, downloader.slots.get(key) or downloader.inactive_slots.get(key)\n-\n-    def _check_concurrency(self, slot, latency):\n-        latencies = self.last_latencies\n-        latencies.append(latency)\n-        if len(latencies) == self.CONCURRENCY_CHECK_PERIOD:\n-            curavg, curdev = avg_stdev(latencies)\n-            preavg, predev = self.last_lat\n-            self.last_lat = curavg, curdev\n-            del latencies[:]\n-            if curavg > preavg + predev:\n-                if slot.concurrency > 1:\n-                    slot.concurrency -= 1\n-            elif slot.concurrency < self.MAX_CONCURRENCY:\n-                slot.concurrency += 1\n+    def _get_slot(self, request, spider):\n+        key = request.meta.get('download_slotkey')\n+        return key, self.crawler.engine.downloader.slots.get(key)\n \n     def _adjust_delay(self, slot, latency, response):\n         \"\"\"Define delay adjustment policy\"\"\"\n-        # if latency is bigger than old delay, then use latency instead of mean. Works better with problematic sites\n-        new_delay = (slot.delay + latency) / 2.0 if latency < slot.delay else latency\n+        # If latency is bigger than old delay, then use latency instead of mean.\n+        # It works better with problematic sites\n+        new_delay = min(max(self.mindelay, latency, (slot.delay + latency) / 2.0), self.maxdelay)\n \n-        if new_delay < self.MIN_DOWNLOAD_DELAY:\n-            new_delay = self.MIN_DOWNLOAD_DELAY\n-\n-        # dont adjust delay if response status != 200 and new delay is smaller than old one,\n-        # as error pages (and redirections) are usually small and so tend to reduce latency, thus provoking a positive feedback\n-        # by reducing delay instead of increase.\n+        # Dont adjust delay if response status != 200 and new delay is smaller\n+        # than old one, as error pages (and redirections) are usually small and\n+        # so tend to reduce latency, thus provoking a positive feedback by\n+        # reducing delay instead of increase.\n         if response.status == 200 or new_delay > slot.delay:\n             slot.delay = new_delay\n-\n-def avg_stdev(lst):\n-    \"\"\"Return average and standard deviation of the given list\"\"\"\n-    avg = sum(lst)/len(lst)\n-    sdsq = sum((x-avg) ** 2 for x in lst)\n-    stdev = (sdsq / (len(lst) -1)) ** 0.5\n-    return avg, stdev\n\n@@ -76,6 +76,7 @@ class Downloader(object):\n \n     def fetch(self, request, spider):\n         key, slot = self._get_slot(request, spider)\n+        request.meta['download_slotkey'] = key\n \n         self.active.add(request)\n         slot.active.add(request)\n@@ -112,12 +113,7 @@ class Downloader(object):\n         return key, self.slots[key]\n \n     def _enqueue_request(self, request, spider, slot):\n-        def _downloaded(response):\n-            self.signals.send_catch_log(signal=signals.response_downloaded, \\\n-                    response=response, request=request, spider=spider)\n-            return response\n-\n-        deferred = defer.Deferred().addCallback(_downloaded)\n+        deferred = defer.Deferred()\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n         return deferred\n@@ -152,7 +148,17 @@ class Downloader(object):\n         # 1. Create the download deferred\n         dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n \n-        # 2. After response arrives,  remove the request from transferring\n+        # 2. Notify response_downloaded listeners about the recent download\n+        # before querying queue for next request\n+        def _downloaded(response):\n+            self.signals.send_catch_log(signal=signals.response_downloaded,\n+                                        response=response,\n+                                        request=request,\n+                                        spider=spider)\n+            return response\n+        dfd.addCallback(_downloaded)\n+\n+        # 3. After response arrives,  remove the request from transferring\n         # state to free up the transferring slot so it can be used by the\n         # following requests (perhaps those which came from the downloader\n         # middleware itself)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb2d87259fb5c3da1cad5cec8de66553b2210608", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 2654 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,6 @@ import random\n import warnings\n from time import time\n from collections import deque\n-from functools import partial\n \n from twisted.internet import reactor, defer\n \n@@ -75,22 +74,12 @@ class Downloader(object):\n         self.inactive_slots = {}\n \n     def fetch(self, request, spider):\n-        key, slot = self._get_slot(request, spider)\n-        request.meta['download_slotkey'] = key\n-\n-        self.active.add(request)\n-        slot.active.add(request)\n-\n         def _deactivate(response):\n             self.active.remove(request)\n-            slot.active.remove(request)\n-            if not slot.active:  # remove empty slots\n-                self.inactive_slots[key] = self.slots.pop(key)\n-\n             return response\n \n-        dlfunc = partial(self._enqueue_request, slot=slot)\n-        dfd = self.middleware.download(dlfunc, request, spider)\n+        self.active.add(request)\n+        dfd = self.middleware.download(self._enqueue_request, request, spider)\n         return dfd.addBoth(_deactivate)\n \n     def needs_backout(self):\n@@ -100,6 +89,7 @@ class Downloader(object):\n         key = urlparse_cached(request).hostname or ''\n         if self.ip_concurrency:\n             key = dnscache.get(key, key)\n+\n         if key not in self.slots:\n             if key in self.inactive_slots:\n                 self.slots[key] = self.inactive_slots.pop(key)\n@@ -112,8 +102,18 @@ class Downloader(object):\n                 self.slots[key] = Slot(concurrency, delay, self.settings)\n         return key, self.slots[key]\n \n-    def _enqueue_request(self, request, spider, slot):\n-        deferred = defer.Deferred()\n+    def _enqueue_request(self, request, spider):\n+        key, slot = self._get_slot(request, spider)\n+        request.meta['download_slotkey'] = key\n+\n+        def _deactivate(response):\n+            slot.active.remove(request)\n+            if not slot.active:  # remove empty slots\n+                self.inactive_slots[key] = self.slots.pop(key)\n+            return response\n+\n+        slot.active.add(request)\n+        deferred = defer.Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n         return deferred\n\n@@ -154,7 +154,7 @@ class ExecutionEngine(object):\n         scraper_idle = spider in self.scraper.slots \\\n             and self.scraper.slots[spider].is_idle()\n         pending = self.slots[spider].scheduler.has_pending_requests()\n-        downloading = bool(self.downloader.slots)\n+        downloading = bool(self.downloader.active)\n         idle = scraper_idle and not (pending or downloading)\n         return idle\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22559833e7ec7717d110a64dbec34df9bb2364f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 1167 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,7 +52,7 @@ class AutoThrottle(object):\n             spider.log(msg, level=logging.INFO)\n \n     def _get_slot(self, request, spider):\n-        key = request.meta.get('download_slotkey')\n+        key = request.meta.get('download_slot')\n         return key, self.crawler.engine.downloader.slots.get(key)\n \n     def _adjust_delay(self, slot, latency, response):\n\n@@ -71,7 +71,6 @@ class Downloader(object):\n         self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n-        self.inactive_slots = {}\n \n     def fetch(self, request, spider):\n         def _deactivate(response):\n@@ -86,30 +85,30 @@ class Downloader(object):\n         return len(self.active) >= self.total_concurrency\n \n     def _get_slot(self, request, spider):\n+        key = self._get_slot_key(request, spider)\n+        if key not in self.slots:\n+            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n+            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n+            self.slots[key] = Slot(conc, delay, self.settings)\n+\n+        return key, self.slots[key]\n+\n+    def _get_slot_key(self, request, spider):\n+        if 'download_slot' in request.meta:\n+            return request.meta['download_slot']\n+\n         key = urlparse_cached(request).hostname or ''\n         if self.ip_concurrency:\n             key = dnscache.get(key, key)\n \n-        if key not in self.slots:\n-            if key in self.inactive_slots:\n-                self.slots[key] = self.inactive_slots.pop(key)\n-            else:\n-                if self.ip_concurrency:\n-                    concurrency = self.ip_concurrency\n-                else:\n-                    concurrency = self.domain_concurrency\n-                concurrency, delay = _get_concurrency_delay(concurrency, spider, self.settings)\n-                self.slots[key] = Slot(concurrency, delay, self.settings)\n-        return key, self.slots[key]\n+        return key\n \n     def _enqueue_request(self, request, spider):\n         key, slot = self._get_slot(request, spider)\n-        request.meta['download_slotkey'] = key\n+        request.meta['download_slot'] = key\n \n         def _deactivate(response):\n             slot.active.remove(request)\n-            if not slot.active:  # remove empty slots\n-                self.inactive_slots[key] = self.slots.pop(key)\n             return response\n \n         slot.active.add(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dabb064293a2f7721bf32f6b40fc693f6517b7ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 430 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,7 @@ class Command(ScrapyCommand):\n                 log.msg(format='Unable to find spider: %(spider)s',\n                         level=log.ERROR, spider=opts.spider)\n         else:\n-            self.spider = create_spider_for_request(self.crawler.spiders, url)\n+            self.spider = create_spider_for_request(self.crawler.spiders, Request(url))\n             if not self.spider:\n                 log.msg(format='Unable to find spider for: %(url)s',\n                         level=log.ERROR, url=url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e396509f5b4a04f2979abe0fe4b6e7ab7cd15678", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 523 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -45,10 +45,10 @@ class HttpCacheMiddleware(object):\n         response = self.storage.retrieve_response(spider, request)\n         if response and self.is_cacheable_response(response):\n             response.flags.append('cached')\n-            self.stats.inc_value('httpcache/hits', spider=spider)\n+            self.stats.inc_value('httpcache/hit', spider=spider)\n             return response\n \n-        self.stats.inc_value('httpcache/misses', spider=spider)\n+        self.stats.inc_value('httpcache/miss', spider=spider)\n         if self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb55f39aedf682b5140c8b208468ddb6b02d3f85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 35 | Churn Cumulative: 725 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,6 +7,7 @@ from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n \n from scrapy import signals\n from scrapy.http import Headers\n+from scrapy.http.request import Request\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n@@ -24,6 +25,7 @@ class HttpCacheMiddleware(object):\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n+        self.use_dummy_cache = settings.getbool('HTTPCACHE_USE_DUMMY')\n         self.stats = stats\n \n     @classmethod\n@@ -43,21 +45,41 @@ class HttpCacheMiddleware(object):\n         if not self.is_cacheable(request):\n             return\n         response = self.storage.retrieve_response(spider, request)\n-        if response and self.is_cacheable_response(response):\n-            response.flags.append('cached')\n-            self.stats.inc_value('httpcache/hit', spider=spider)\n-            return response\n \n+        # Response cached, but stale\n+        if response and type(response) is Request:\n+            # Return None so that Scrapy continues processing\n+            self.stats.inc_value('httpcache/revalidation', spider=spider)\n+            return\n+\n+        if response and self.is_cacheable_response(response):\n+            self.stats.inc_value('httpcache/hit', spider=spider)\n+            if self.use_dummy_cache:\n+                response.flags.append('cached')\n+                return response\n+            else:\n+                # Response cached and fresh\n+                raise IgnoreRequest(\"Ignored request already in cache: %s\" % request)\n+\n+        # Response not cached\n         self.stats.inc_value('httpcache/miss', spider=spider)\n         if self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n         if (self.is_cacheable(request)\n-            and self.is_cacheable_response(response)\n-            and 'cached' not in response.flags):\n+            and self.is_cacheable_response(response)):\n+            if self.use_dummy_cache:\n+                if 'cached' not in response.flags:\n                     self.storage.store_response(spider, request, response)\n                     self.stats.inc_value('httpcache/store', spider=spider)\n+            else:\n+                if response.status != 304:\n+                    self.storage.store_response(spider, request, response)\n+                    self.stats.inc_value('httpcache/store', spider=spider)\n+                else:\n+                    response.flags.append('cached')\n+                    self.stats.inc_value('httpcache/hits', spider=spider)\n         return response\n \n     def is_cacheable_response(self, response):\n\n@@ -134,6 +134,7 @@ FEED_EXPORTERS_BASE = {\n }\n \n HTTPCACHE_ENABLED = False\n+HTTPCACHE_USE_DUMMY = True\n HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e435fb5f9a5542abe3929f13a9b91059f2d6976", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 11 | Churn Cumulative: 568 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,11 +83,16 @@ class HttpCacheMiddleware(object):\n         return response\n \n     def is_cacheable_response(self, response):\n-        return response.status not in self.ignore_http_codes\n+        retval = response.status not in self.ignore_http_codes\n+        if not self.use_dummy_cache and response.headers.has_key('cache-control'):\n+            retval = retval and (response.headers['cache-control'].lower().find('no-store') == -1)\n+        return retval\n \n     def is_cacheable(self, request):\n-        return urlparse_cached(request).scheme not in self.ignore_schemes\n-\n+        retval = urlparse_cached(request).scheme not in self.ignore_schemes\n+        if not self.use_dummy_cache and request.headers.has_key('cache-control'):\n+            retval = retval and (request.headers['cache-control'].lower().find('no-store') == -1)\n+        return retval\n \n class FilesystemCacheStorage(object):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2d3f4dd1b8d1618a727c3670d1252b7b6290b47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 288 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 12 | Methods Changed: 19 | Complexity Δ (Sum/Max): 50/36 | Churn Δ: 288 | Churn Cumulative: 636 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.6010928961748634\n\nDIFF:\n@@ -1,8 +1,12 @@\n import os\n+import calendar\n+import email.utils\n from time import time\n import cPickle as pickle\n \n+from scrapy import log\n from scrapy.http import Headers\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n@@ -59,3 +63,141 @@ class DbmCacheStorage(object):\n \n     def _request_key(self, request):\n         return request_fingerprint(request)\n+\n+\n+class BaseRealCacheStorage(object):\n+    \"\"\"\n+    Most of the code was taken from the httplib2 (MIT License)\n+    https://code.google.com/p/httplib2/source/browse/python2/httplib2/__init__.py\n+    \"\"\"\n+    \n+    def parse_cache_control(self, headers):\n+        retval = {}\n+        if headers.has_key('cache-control'):\n+            parts =  headers['cache-control'].split(',')\n+            parts_with_args = [tuple([x.strip().lower() for x in part.split(\"=\", 1)]) for part in parts if -1 != part.find(\"=\")]\n+            parts_wo_args = [(name.strip().lower(), 1) for name in parts if -1 == name.find(\"=\")]\n+            retval = dict(parts_with_args + parts_wo_args)\n+        return retval\n+    \n+    \n+    def get(self, response_headers, request_headers):\n+        \"\"\"Determine freshness from the Date, Expires and Cache-Control headers.\n+\n+        We don't handle the following:\n+\n+        1. Cache-Control: max-stale\n+        2. Age: headers are not used in the calculations.\n+\n+        Not that this algorithm is simpler than you might think\n+        because we are operating as a private (non-shared) cache.\n+        This lets us ignore 's-maxage'. We can also ignore\n+        'proxy-invalidate' since we aren't a proxy.\n+        We will never return a stale document as\n+        fresh as a design decision, and thus the non-implementation\n+        of 'max-stale'. This also lets us safely ignore 'must-revalidate'\n+        since we operate as if every server has sent 'must-revalidate'.\n+        Since we are private we get to ignore both 'public' and\n+        'private' parameters. We also ignore 'no-transform' since\n+        we don't do any transformations.\n+        The 'no-store' parameter is handled at a higher level.\n+        So the only Cache-Control parameters we look at are:\n+\n+        no-cache\n+        only-if-cached\n+        max-age\n+        min-fresh\n+        \"\"\"\n+\n+        retval = \"STALE\"\n+        cc = self.parse_cache_control(request_headers)\n+        cc_response = self.parse_cache_control(response_headers)\n+        \n+        if request_headers.has_key('pragma') and request_headers['pragma'].lower().find('no-cache') != -1:\n+            retval = \"TRANSPARENT\"\n+            if 'cache-control' not in request_headers:\n+                request_headers['cache-control'] = 'no-cache'\n+        elif cc.has_key('no-cache'):\n+            retval = \"TRANSPARENT\"\n+        elif cc_response.has_key('no-cache'):\n+            retval = \"STALE\"\n+        elif cc.has_key('only-if-cached'):\n+            retval = \"FRESH\"\n+        elif response_headers.has_key('date'):\n+            date = calendar.timegm(email.utils.parsedate_tz(response_headers['date']))\n+            now = time()\n+            current_age = max(0, now - date)\n+            if cc_response.has_key('max-age'):\n+                try:\n+                    freshness_lifetime = int(cc_response['max-age'])\n+                except ValueError:\n+                    freshness_lifetime = 0\n+            elif response_headers.has_key('expires'):\n+                expires = email.utils.parsedate_tz(response_headers['expires'])\n+                if None == expires:\n+                    freshness_lifetime = 0\n+                else:\n+                    freshness_lifetime = max(0, calendar.timegm(expires) - date)\n+            else:\n+                freshness_lifetime = 0\n+            if cc.has_key('max-age'):\n+                try:\n+                    freshness_lifetime = int(cc['max-age'])\n+                except ValueError:\n+                    freshness_lifetime = 0\n+            if cc.has_key('min-fresh'):\n+                try:\n+                    min_fresh = int(cc['min-fresh'])\n+                except ValueError:\n+                    min_fresh = 0\n+                current_age += min_fresh\n+            if freshness_lifetime > current_age:\n+                retval = \"FRESH\"\n+        return retval\n+\n+    def retrieve_cache(self, spider, request, response_headers, response_status, response_url='', response_body=''):\n+        # Determine our course of action:\n+        #   Is the cached entry fresh or stale?\n+        #\n+        # There seems to be three possible answers:\n+        # 1. [FRESH] Return the Response object\n+        # 2. [STALE] Update the Request object with cache validators if available\n+        # 3. [TRANSPARENT] Don't update the Request with cache validators (Cache-Control: no-cache)\n+        entry_disposition = self.get(Headers(response_headers), Headers(request.headers))\n+        \n+        # Per the RFC, requests should not be repeated in these situations\n+        if response_status in [400, 401, 403, 410]:\n+            raise IgnoreRequest(\"Ignored request because cached response status is %d.\" % response_status)\n+        \n+        if entry_disposition == \"FRESH\":\n+            log.msg(\"Cache is FRESH\", level=log.DEBUG, spider=spider)\n+            \n+            headers = Headers(response_headers)\n+            respcls = responsetypes.from_args(headers=headers, url=response_url)\n+            response = respcls(url=response_url, headers=headers, status=response_status, body=response_body)\n+            \n+            return response\n+\n+        new_request = request.copy()\n+        if entry_disposition == \"STALE\":\n+            log.msg(\"Cache is STALE, updating Request object with cache validators\", level=log.DEBUG, spider=spider)\n+            if response_headers.has_key('ETag') and not 'If-None-Match' in request.headers:\n+                new_request.headers['If-None-Match'] = response_headers['ETag']\n+            if response_headers.has_key('Last-Modified') and not 'Last-Modified' in request.headers:\n+                new_request.headers['If-Modified-Since'] = response_headers['Last-Modified']\n+        elif entry_disposition == \"TRANSPARENT\":\n+            log.msg(\"Cache is TRANSPARENT, not adding cache validators to Request object\", level=log.DEBUG, spider=spider)\n+        \n+        return new_request\n+\n+\n+class DbmRealCacheStorage(DbmCacheStorage, BaseRealCacheStorage):\n+    def __init__(self, settings):\n+        super(DbmRealCacheStorage, self).__init__(settings)\n+    \n+    def retrieve_response(self, spider, request):\n+        data = self._read_data(spider, request)\n+        if data is None:\n+            return # not cached\n+        else:\n+            return self.retrieve_cache(spider, request, data['headers'], data['status'], data['url'], data['body'])\n\n@@ -2,6 +2,7 @@ import time\n import tempfile\n import shutil\n import unittest\n+import email.utils\n from contextlib import contextmanager\n \n from scrapy.http import Response, HtmlResponse, Request\n@@ -11,11 +12,17 @@ from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n from scrapy.contrib.downloadermiddleware.httpcache import \\\n     FilesystemCacheStorage, HttpCacheMiddleware\n+from scrapy.contrib.httpcache import DbmRealCacheStorage\n \n \n class HttpCacheMiddlewareTest(unittest.TestCase):\n \n     storage_class = FilesystemCacheStorage\n+    realcache_storage_class = DbmRealCacheStorage\n+\n+    yesterday = email.utils.formatdate(time.time() - 1 * 24 * 60 * 60)\n+    now = email.utils.formatdate()\n+    tomorrow = email.utils.formatdate(time.time() + 1 * 24 * 60 * 60)\n \n     def setUp(self):\n         self.crawler = get_crawler()\n@@ -34,6 +41,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def _get_settings(self, **new_settings):\n         settings = {\n             'HTTPCACHE_ENABLED': True,\n+            'HTTPCACHE_USE_DUMMY': True,\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n@@ -44,7 +52,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     @contextmanager\n     def _storage(self, **new_settings):\n         settings = self._get_settings(**new_settings)\n+        if settings.getbool('HTTPCACHE_USE_DUMMY'):\n             storage = self.storage_class(settings)\n+        else:\n+            storage = self.realcache_storage_class(settings)\n         storage.open_spider(self.spider)\n         try:\n             yield storage\n@@ -171,11 +182,146 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n             self.assertEqualResponse(self.response, response)\n             assert 'cached' in response.flags\n \n+    def test_real_http_cache_middleware_response304_not_cached(self):\n+        # test response is not cached because the status is 304 Not Modified\n+        # (so it should be cached already)\n+        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+            assert mw.process_request(self.request, self.spider) is None\n+            response = Response('http://www.example.com', status=304)\n+            mw.process_response(self.request, response, self.spider)\n+\n+            assert 'cached' in response.flags\n+            assert mw.storage.retrieve_response(self.spider, self.request) is None\n+            assert mw.process_request(self.request, self.spider) is None\n+\n+    def test_real_http_cache_middleware_response_nostore_not_cached(self):\n+        # test response is not cached because of the Cache-Control 'no-store' directive\n+        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n+        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+            assert mw.process_request(self.request, self.spider) is None\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Cache-Control': 'no-store'},\n+                body='test body', status=200)\n+            mw.process_response(self.request, response, self.spider)\n+\n+            assert mw.storage.retrieve_response(self.spider, self.request) is None\n+            assert mw.process_request(self.request, self.spider) is None\n+\n+    def test_real_http_cache_middleware_request_nostore_not_cached(self):\n+        # test response is not cached because of the request's Cache-Control 'no-store' directive\n+        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n+        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+            request = Request('http://www.example.com',\n+                headers={'User-Agent': 'test', 'Cache-Control': 'no-store'})\n+            assert mw.process_request(request, self.spider) is None\n+            mw.process_response(request, self.response, self.spider)\n+\n+            assert mw.storage.retrieve_response(self.spider, request) is None\n+            assert mw.process_request(request, self.spider) is None\n+\n+    def test_real_http_cache_middleware_response_cached_and_fresh(self):\n+        # test response cached and fresh\n+        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:\n+            response = mw.process_response(self.request, self.response, self.spider)\n+            self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n+            assert 'cached' not in response.flags\n+\n+    def test_real_http_cache_middleware_response_cached_and_stale(self):\n+        # test response cached but stale\n+        with self._middleware(HTTPCACHE_USE_DUMMY=False,\n+            HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmRealCacheStorage') as mw:\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Cache-Control': 'no-cache'},\n+                body='test body', status=200)\n+            mw.process_response(self.request, response, self.spider)\n+            assert mw.process_request(self.request, self.spider) is None\n+\n+            response = mw.storage.retrieve_response(self.spider, self.request)\n+            assert isinstance(response, Request)\n+\n+    def test_real_http_cache_storage_response_cached_and_fresh(self):\n+        # test response is cached and is fresh\n+        # (response requested should be same as response received)\n+        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+            assert storage.retrieve_response(self.spider, self.request) is None\n+\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Date': self.yesterday, 'Expires': self.tomorrow},\n+                body='test body', status=200)\n+            storage.store_response(self.spider, self.request, response)\n+            response2 = storage.retrieve_response(self.spider, self.request)\n+            self.assertEqualResponse(response, response2)\n+\n+    def test_real_http_cache_storage_response403_cached_and_further_requests_ignored(self):\n+        # test response is cached but further requests are ignored\n+        # because response status is 403 (as per the RFC)\n+        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+            assert storage.retrieve_response(self.spider, self.request) is None\n+\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Date': self.yesterday, 'Expires': self.tomorrow},\n+                body='test body', status=403)\n+            storage.store_response(self.spider, self.request, response)\n+            self.assertRaises(IgnoreRequest, storage.retrieve_response,\n+                self.spider, self.request)\n+\n+    def test_real_http_cache_storage_response_cached_and_stale(self):\n+        # test response is cached and is stale (no cache validators inserted)\n+        # (request should be same as response received)\n+        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+            assert storage.retrieve_response(self.spider, self.request) is None\n+\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Date': self.now, 'Expires': self.yesterday},\n+                body='test body', status=200)\n+            storage.store_response(self.spider, self.request, response)\n+            response2 = storage.retrieve_response(self.spider, self.request)\n+            assert isinstance(response2, Request)\n+            self.assertEqualRequest(self.request, response2)\n+\n+    def test_real_http_cache_storage_response_cached_and_stale_with_cache_validators(self):\n+        # test response is cached and is stale and cache validators are inserted\n+        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+            assert storage.retrieve_response(self.spider, self.request) is None\n+\n+            response = Response('http://www.example.com', headers=\n+                {'Content-Type': 'text/html', 'Date': self.now, 'Expires': self.yesterday,\n+                'Last-Modified': self.yesterday}, body='test body', status=200)\n+            storage.store_response(self.spider, self.request, response)\n+            response2 = storage.retrieve_response(self.spider, self.request)\n+            assert isinstance(response2, Request)\n+            self.assertEqualRequestButWithCacheValidators(self.request, response2)\n+\n+    def test_real_http_cache_storage_response_cached_and_transparent(self):\n+        # test response is not cached because of the request's Cache-Control 'no-cache' directive\n+        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2\n+        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:\n+            request = Request('http://www.example.com',\n+                headers={'User-Agent': 'test', 'Cache-Control': 'no-cache'})\n+            assert storage.retrieve_response(self.spider, request) is None\n+            storage.store_response(self.spider, request, self.response)\n+            response = storage.retrieve_response(self.spider, request)\n+            assert isinstance(response, Request)\n+            self.assertEqualRequest(request, response)\n+\n     def assertEqualResponse(self, response1, response2):\n         self.assertEqual(response1.url, response2.url)\n         self.assertEqual(response1.status, response2.status)\n         self.assertEqual(response1.headers, response2.headers)\n         self.assertEqual(response1.body, response2.body)\n \n+    def assertEqualRequest(self, request1, request2):\n+        self.assertEqual(request1.url, request2.url)\n+        self.assertEqual(request1.headers, request2.headers)\n+        self.assertEqual(request1.body, request2.body)\n+\n+    def assertEqualRequestButWithCacheValidators(self, request1, request2):\n+        self.assertEqual(request1.url, request2.url)\n+        assert not request1.headers.has_key('If-None-Match')\n+        assert not request1.headers.has_key('If-Modified-Since')\n+        assert (request2.headers.has_key('If-None-Match') or \\\n+            request2.headers.has_key('If-Modified-Since'))\n+        self.assertEqual(request1.body, request2.body)\n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
