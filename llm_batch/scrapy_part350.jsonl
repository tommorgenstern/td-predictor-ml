{"custom_id": "scrapy#ef61fb5698c93178a57a9d4d67760def9a3039d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 7103 | Contributors (this commit): 41 | Commits (past 90d): 10 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -2542,7 +2542,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     def test_batch_path_differ(self):\n         \"\"\"\n         Test that the name of all batch files differ from each other.\n-        So %(batch_time)s replaced with the current date.\n+        So %(batch_id)d replaced with the current id.\n         \"\"\"\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -2552,7 +2552,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = {\n             \"FEEDS\": {\n                 self._random_temp_filename()\n-                / \"%(batch_time)s\": {\n+                / \"%(batch_id)d\": {\n                     \"format\": \"json\",\n                 },\n             },\n@@ -2615,7 +2615,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 return super().open(*args, **kwargs)\n \n         key = \"export.csv\"\n-        uri = f\"s3://{bucket}/{key}/%(batch_time)s.json\"\n+        uri = f\"s3://{bucket}/{key}/%(batch_id)d.json\"\n         batch_item_count = 1\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7ecec18099ace6ba77161302cca85c3e58a2ae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 243 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,8 +10,9 @@ from w3lib.http import basic_auth_header\n \n class DataAction(argparse.Action):\n     def __call__(self, parser, namespace, values, option_string=None):\n-        value = str(values).encode(\"utf-8\").decode(\"utf-8\")\n-        value = value[1::] if re.match(r\"^\\$(.+)\", value) else value\n+        value = str(values)\n+        if value.startswith(\"$\"):\n+            value = value[1:]\n         setattr(namespace, self.dest, value)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f69ba43f8e5b52af51119c682509167f5cf5517f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 244 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import argparse\n-import re\n import warnings\n from http.cookies import SimpleCookie\n from shlex import split\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9af596a6b806a0cd0ba7f0d3bcff9ea6e3a19519", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 9 | Churn Cumulative: 1410 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,6 +142,12 @@ class Response(object_ref):\n         \"\"\"\n         raise NotSupported(\"Response content isn't text\")\n \n+    def jmespath(self, *a, **kw):\n+        \"\"\"Shortcut method implemented only by responses whose content\n+        is text (subclasses of TextResponse).\n+        \"\"\"\n+        raise NotSupported(\"Response content isn't text\")\n+\n     def xpath(self, *a, **kw):\n         \"\"\"Shortcut method implemented only by responses whose content\n         is text (subclasses of TextResponse).\n\n@@ -139,6 +139,9 @@ class TextResponse(Response):\n             self._cached_selector = Selector(self)\n         return self._cached_selector\n \n+    def jmespath(self, query, **kwargs):\n+        return self.selector.jmespath(query, **kwargs)\n+\n     def xpath(self, query, **kwargs):\n         return self.selector.xpath(query, **kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b50c032ee9a75d1c9b42f1126637fdc655b141a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 21 | Methods Changed: 16 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 137 | Churn Cumulative: 8975 | Contributors (this commit): 53 | Commits (past 90d): 22 | Contributors (cumulative): 92 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,10 +11,11 @@ import warnings\n from datetime import datetime\n from pathlib import Path\n from tempfile import NamedTemporaryFile\n-from typing import IO, Any, Callable, Optional, Tuple, Union\n+from typing import IO, Any, Callable, List, Optional, Tuple, Union\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n+from twisted.internet.defer import DeferredList\n from w3lib.url import file_uri_to_path\n from zope.interface import Interface, implementer\n \n@@ -23,6 +24,8 @@ from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n+from scrapy.utils.defer import maybe_deferred_to_future\n+from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n@@ -271,7 +274,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         )\n \n \n-class _FeedSlot:\n+class FeedSlot:\n     def __init__(\n         self,\n         file,\n@@ -309,7 +312,15 @@ class _FeedSlot:\n             self._exporting = False\n \n \n+_FeedSlot = create_deprecated_class(\n+    name=\"_FeedSlot\",\n+    new_class=FeedSlot,\n+)\n+\n+\n class FeedExporter:\n+    _pending_deferreds: List[defer.Deferred] = []\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         exporter = cls(crawler)\n@@ -375,12 +386,18 @@ class FeedExporter:\n                 )\n             )\n \n-    def close_spider(self, spider):\n-        deferred_list = []\n+    async def close_spider(self, spider):\n         for slot in self.slots:\n-            d = self._close_slot(slot, spider)\n-            deferred_list.append(d)\n-        return defer.DeferredList(deferred_list) if deferred_list else None\n+            self._close_slot(slot, spider)\n+\n+        # Await all deferreds\n+        if self._pending_deferreds:\n+            await maybe_deferred_to_future(DeferredList(self._pending_deferreds))\n+\n+        # Send FEED_EXPORTER_CLOSED signal\n+        await maybe_deferred_to_future(\n+            self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n+        )\n \n     def _close_slot(self, slot, spider):\n         def get_file(slot_):\n@@ -404,6 +421,14 @@ class FeedExporter:\n         d.addErrback(\n             self._handle_store_error, logmsg, spider, type(slot.storage).__name__\n         )\n+        self._pending_deferreds.append(d)\n+        d.addCallback(\n+            lambda _: self.crawler.signals.send_catch_log_deferred(\n+                signals.feed_slot_closed, slot=slot\n+            )\n+        )\n+        d.addBoth(lambda _: self._pending_deferreds.remove(d))\n+\n         return d\n \n     def _handle_store_error(self, f, logmsg, spider, slot_type):\n@@ -444,7 +469,7 @@ class FeedExporter:\n             indent=feed_options[\"indent\"],\n             **feed_options[\"item_export_kwargs\"],\n         )\n-        slot = _FeedSlot(\n+        slot = FeedSlot(\n             file=file,\n             exporter=exporter,\n             storage=storage,\n@@ -579,7 +604,7 @@ class FeedExporter:\n         self,\n         spider: Spider,\n         uri_params_function: Optional[Union[str, Callable[[dict, Spider], dict]]],\n-        slot: Optional[_FeedSlot] = None,\n+        slot: Optional[FeedSlot] = None,\n     ) -> dict:\n         params = {}\n         for k in dir(spider):\n\n@@ -22,6 +22,8 @@ bytes_received = object()\n item_scraped = object()\n item_dropped = object()\n item_error = object()\n+feed_slot_closed = object()\n+feed_exporter_closed = object()\n \n # for backward compatibility\n stats_spider_opened = spider_opened\n\n@@ -32,18 +32,19 @@ from zope.interface import implementer\n from zope.interface.verify import verifyObject\n \n import scrapy\n+from scrapy import signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n     BlockingFeedStorage,\n     FeedExporter,\n+    FeedSlot,\n     FileFeedStorage,\n     FTPFeedStorage,\n     GCSFeedStorage,\n     IFeedStorage,\n     S3FeedStorage,\n     StdoutFeedStorage,\n-    _FeedSlot,\n )\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -660,8 +661,8 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         return result\n \n \n-class InstrumentedFeedSlot(_FeedSlot):\n-    \"\"\"Instrumented _FeedSlot subclass for keeping track of calls to\n+class InstrumentedFeedSlot(FeedSlot):\n+    \"\"\"Instrumented FeedSlot subclass for keeping track of calls to\n     start_exporting and finish_exporting.\"\"\"\n \n     def start_exporting(self):\n@@ -964,7 +965,7 @@ class FeedExportTest(FeedExportTestBase):\n         listener = IsExportingListener()\n         InstrumentedFeedSlot.subscribe__listener(listener)\n \n-        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n             _ = yield self.exported_data(items, settings)\n             self.assertFalse(listener.start_without_finish)\n             self.assertFalse(listener.finish_without_start)\n@@ -982,7 +983,7 @@ class FeedExportTest(FeedExportTestBase):\n         listener = IsExportingListener()\n         InstrumentedFeedSlot.subscribe__listener(listener)\n \n-        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n             _ = yield self.exported_data(items, settings)\n             self.assertFalse(listener.start_without_finish)\n             self.assertFalse(listener.finish_without_start)\n@@ -1003,7 +1004,7 @@ class FeedExportTest(FeedExportTestBase):\n         listener = IsExportingListener()\n         InstrumentedFeedSlot.subscribe__listener(listener)\n \n-        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n             _ = yield self.exported_data(items, settings)\n             self.assertFalse(listener.start_without_finish)\n             self.assertFalse(listener.finish_without_start)\n@@ -1022,7 +1023,7 @@ class FeedExportTest(FeedExportTestBase):\n         listener = IsExportingListener()\n         InstrumentedFeedSlot.subscribe__listener(listener)\n \n-        with mock.patch(\"scrapy.extensions.feedexport._FeedSlot\", InstrumentedFeedSlot):\n+        with mock.patch(\"scrapy.extensions.feedexport.FeedSlot\", InstrumentedFeedSlot):\n             _ = yield self.exported_data(items, settings)\n             self.assertFalse(listener.start_without_finish)\n             self.assertFalse(listener.finish_without_start)\n@@ -2651,6 +2652,83 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             stub.assert_no_pending_responses()\n \n \n+# Test that the FeedExporer sends the feed_exporter_closed and feed_slot_closed signals\n+class FeedExporterSignalsTest(unittest.TestCase):\n+    items = [\n+        {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n+        {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux2\"},\n+        {\"foo\": \"bar3\", \"baz\": \"quux3\"},\n+    ]\n+\n+    with tempfile.NamedTemporaryFile(suffix=\"json\") as tmp:\n+        settings = {\n+            \"FEEDS\": {\n+                f\"file:///{tmp.name}\": {\n+                    \"format\": \"json\",\n+                },\n+            },\n+        }\n+\n+    def feed_exporter_closed_signal_handler(self):\n+        self.feed_exporter_closed_received = True\n+\n+    def feed_slot_closed_signal_handler(self, slot):\n+        self.feed_slot_closed_received = True\n+\n+    def feed_exporter_closed_signal_handler_deferred(self):\n+        d = defer.Deferred()\n+        d.addCallback(lambda _: setattr(self, \"feed_exporter_closed_received\", True))\n+        d.callback(None)\n+        return d\n+\n+    def feed_slot_closed_signal_handler_deferred(self, slot):\n+        d = defer.Deferred()\n+        d.addCallback(lambda _: setattr(self, \"feed_slot_closed_received\", True))\n+        d.callback(None)\n+        return d\n+\n+    def run_signaled_feed_exporter(\n+        self, feed_exporter_signal_handler, feed_slot_signal_handler\n+    ):\n+        crawler = get_crawler(settings_dict=self.settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n+        crawler.signals.connect(\n+            feed_exporter_signal_handler,\n+            signal=signals.feed_exporter_closed,\n+        )\n+        crawler.signals.connect(\n+            feed_slot_signal_handler, signal=signals.feed_slot_closed\n+        )\n+        feed_exporter.open_spider(spider)\n+        for item in self.items:\n+            feed_exporter.item_scraped(item, spider)\n+        defer.ensureDeferred(feed_exporter.close_spider(spider))\n+\n+    def test_feed_exporter_signals_sent(self):\n+        self.feed_exporter_closed_received = False\n+        self.feed_slot_closed_received = False\n+\n+        self.run_signaled_feed_exporter(\n+            self.feed_exporter_closed_signal_handler,\n+            self.feed_slot_closed_signal_handler,\n+        )\n+        self.assertTrue(self.feed_slot_closed_received)\n+        self.assertTrue(self.feed_exporter_closed_received)\n+\n+    def test_feed_exporter_signals_sent_deferred(self):\n+        self.feed_exporter_closed_received = False\n+        self.feed_slot_closed_received = False\n+\n+        self.run_signaled_feed_exporter(\n+            self.feed_exporter_closed_signal_handler_deferred,\n+            self.feed_slot_closed_signal_handler_deferred,\n+        )\n+        self.assertTrue(self.feed_slot_closed_received)\n+        self.assertTrue(self.feed_exporter_closed_received)\n+\n+\n class FeedExportInitTest(unittest.TestCase):\n     def test_unsupported_storage(self):\n         settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d29f20fc2021efb80d96389dc02c5a2cac9ef62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 138 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 138 | Churn Cumulative: 1106 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -108,3 +108,141 @@ class SelectorTestCase(unittest.TestCase):\n     def test_selector_bad_args(self):\n         with self.assertRaisesRegex(ValueError, \"received both response and text\"):\n             Selector(TextResponse(url=\"http://example.com\", body=b\"\"), text=\"\")\n+\n+\n+class JMESPathTestCase(unittest.TestCase):\n+    def test_json_has_html(self) -> None:\n+        \"\"\"Sometimes the information is returned in a json wrapper\"\"\"\n+        body = \"\"\"\n+        {\n+            \"content\": [\n+                {\n+                    \"name\": \"A\",\n+                    \"value\": \"a\"\n+                },\n+                {\n+                    \"name\": {\n+                        \"age\": 18\n+                    },\n+                    \"value\": \"b\"\n+                },\n+                {\n+                    \"name\": \"C\",\n+                    \"value\": \"c\"\n+                },\n+                {\n+                    \"name\": \"<a>D</a>\",\n+                    \"value\": \"<div>d</div>\"\n+                }\n+            ],\n+            \"html\": \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\"\n+        }\n+        \"\"\"\n+        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n+        self.assertEqual(\n+            resp.jmespath(\"html\").get(),\n+            \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\",\n+        )\n+        self.assertEqual(\n+            resp.jmespath(\"html\").xpath(\"//div/a/text()\").getall(),\n+            [\"a\", \"b\", \"d\"],\n+        )\n+        self.assertEqual(resp.jmespath(\"html\").css(\"div > b\").getall(), [\"<b>f</b>\"])\n+        self.assertEqual(resp.jmespath(\"content\").jmespath(\"name.age\").get(), \"18\")\n+\n+    def test_html_has_json(self) -> None:\n+        body = \"\"\"\n+        <div>\n+            <h1>Information</h1>\n+            <content>\n+            {\n+              \"user\": [\n+                        {\n+                                  \"name\": \"A\",\n+                                  \"age\": 18\n+                        },\n+                        {\n+                                  \"name\": \"B\",\n+                                  \"age\": 32\n+                        },\n+                        {\n+                                  \"name\": \"C\",\n+                                  \"age\": 22\n+                        },\n+                        {\n+                                  \"name\": \"D\",\n+                                  \"age\": 25\n+                        }\n+              ],\n+              \"total\": 4,\n+              \"status\": \"ok\"\n+            }\n+            </content>\n+        </div>\n+        \"\"\"\n+        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n+        self.assertEqual(\n+            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").getall(),\n+            [\"A\", \"B\", \"C\", \"D\"],\n+        )\n+        self.assertEqual(\n+            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").getall(),\n+            [\"A\", \"B\", \"C\", \"D\"],\n+        )\n+        self.assertEqual(resp.xpath(\"//div/content\").jmespath(\"total\").get(), \"4\")\n+\n+    def test_jmestpath_with_re(self) -> None:\n+        body = \"\"\"\n+            <div>\n+                <h1>Information</h1>\n+                <content>\n+                {\n+                  \"user\": [\n+                            {\n+                                      \"name\": \"A\",\n+                                      \"age\": 18\n+                            },\n+                            {\n+                                      \"name\": \"B\",\n+                                      \"age\": 32\n+                            },\n+                            {\n+                                      \"name\": \"C\",\n+                                      \"age\": 22\n+                            },\n+                            {\n+                                      \"name\": \"D\",\n+                                      \"age\": 25\n+                            }\n+                  ],\n+                  \"total\": 4,\n+                  \"status\": \"ok\"\n+                }\n+                </content>\n+            </div>\n+            \"\"\"\n+        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n+        self.assertEqual(\n+            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n+            [\"A\", \"B\", \"C\", \"D\"],\n+        )\n+        self.assertEqual(\n+            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n+            [\"A\", \"B\", \"C\", \"D\"],\n+        )\n+\n+        self.assertEqual(\n+            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re(r\"(\\d+)\"), []\n+        )\n+\n+        self.assertEqual(\n+            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re_first(r\"(\\d+)\"),\n+            None,\n+        )\n+\n+        self.assertEqual(\n+            resp.xpath(\"//div/content\")\n+            .jmespath(\"user[*].age.to_string(@)\")\n+            .re(r\"(\\d+)\"),\n+            [\"18\", \"32\", \"22\", \"25\"],\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#578606779d0f127a759f8b1623c1e4be341a17db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2114 | Contributors (this commit): 27 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,9 @@ class BaseResponseTest(unittest.TestCase):\n             isinstance(self.response_class(\"http://example.com/\"), self.response_class)\n         )\n         self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n+        self.assertRaises(\n+            TypeError, self.response_class, url=\"http://example.com\", body={}\n+        )\n         # body can be str or None\n         self.assertTrue(\n             isinstance(\n@@ -192,6 +195,7 @@ class BaseResponseTest(unittest.TestCase):\n             self.assertRaisesRegex(AttributeError, msg, getattr, r, \"text\")\n             self.assertRaisesRegex(NotSupported, msg, r.css, \"body\")\n             self.assertRaisesRegex(NotSupported, msg, r.xpath, \"//body\")\n+            self.assertRaisesRegex(NotSupported, msg, r.jmespath, \"body\")\n         else:\n             r.text\n             r.css(\"body\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8acde511a902515c56f7452f950221657953b92e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 1084 | Contributors (this commit): 27 | Commits (past 90d): 5 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -100,11 +100,13 @@ class TextResponse(Response):\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get(b\"Content-Type\", b\"\")\n-        return http_content_type_encoding(to_unicode(content_type))\n+        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_unicode(self.headers.get(b\"Content-Type\", b\"\"))\n+            content_type = to_unicode(\n+                self.headers.get(b\"Content-Type\", b\"\"), encoding=\"latin-1\"\n+            )\n             benc, ubody = html_to_unicode(\n                 content_type,\n                 self.body,\n\n@@ -51,7 +51,9 @@ class ResponseTypes:\n         header\"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\n+        mimetype = (\n+            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n+        )\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b49aa1b019672d4f2ab7a8d75465381cb5705a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2284 | Contributors (this commit): 30 | Commits (past 90d): 4 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -448,6 +448,13 @@ class TextResponseTest(BaseResponseTest):\n             body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n             headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n         )\n+        r9 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\x80\",\n+            headers={\n+                \"Content-type\": [b\"application/x-download; filename=\\x80dummy.txt\"]\n+            },\n+        )\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -458,9 +465,12 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n         self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r9._headers_encoding(), None)\n         self.assertEqual(r8._declared_encoding(), \"utf-8\")\n+        self.assertEqual(r9._declared_encoding(), None)\n         self._assert_response_encoding(r5, \"utf-8\")\n         self._assert_response_encoding(r8, \"utf-8\")\n+        self._assert_response_encoding(r9, \"cp1252\")\n         assert (\n             r4._body_inferred_encoding() is not None\n             and r4._body_inferred_encoding() != \"ascii\"\n@@ -470,6 +480,7 @@ class TextResponseTest(BaseResponseTest):\n         self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n         self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n         self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n+        self._assert_response_values(r9, \"cp1252\", \"€\")\n \n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n         self.assertRaises(\n\n@@ -42,6 +42,7 @@ class ResponseTypesTest(unittest.TestCase):\n             (\"application/octet-stream\", Response),\n             (\"application/x-json; encoding=UTF8;charset=UTF-8\", TextResponse),\n             (\"application/json-amazonui-streaming;charset=UTF-8\", TextResponse),\n+            (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1eb44604853e24f7b526853d5e95414949fd88c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 898 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,7 +140,12 @@ class TextResponse(Response):\n         return self._cached_selector\n \n     def jmespath(self, query, **kwargs):\n-        return self.selector.jmespath(query, **kwargs)\n+        if not hasattr(self.selector, \"jmespath\"):  # type: ignore[attr-defined]\n+            raise AttributeError(\n+                \"Please install parsel >= 1.8.1 to get jmespath support\"\n+            )\n+\n+        return self.selector.jmespath(query, **kwargs)  # type: ignore[attr-defined]\n \n     def xpath(self, query, **kwargs):\n         return self.selector.xpath(query, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a604dfae5c12a55760d0b44d7da06b022cce3615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 1133 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,16 @@\n import weakref\n \n+import packaging.version as version\n+import parsel\n+import pytest\n from twisted.trial import unittest\n \n from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n from scrapy.selector import Selector\n \n+PARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\n+PARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n+\n \n class SelectorTestCase(unittest.TestCase):\n     def test_simple_selection(self):\n@@ -111,8 +117,12 @@ class SelectorTestCase(unittest.TestCase):\n \n \n class JMESPathTestCase(unittest.TestCase):\n+    @pytest.mark.skipif(\n+        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n+    )\n     def test_json_has_html(self) -> None:\n         \"\"\"Sometimes the information is returned in a json wrapper\"\"\"\n+\n         body = \"\"\"\n         {\n             \"content\": [\n@@ -150,6 +160,9 @@ class JMESPathTestCase(unittest.TestCase):\n         self.assertEqual(resp.jmespath(\"html\").css(\"div > b\").getall(), [\"<b>f</b>\"])\n         self.assertEqual(resp.jmespath(\"content\").jmespath(\"name.age\").get(), \"18\")\n \n+    @pytest.mark.skipif(\n+        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n+    )\n     def test_html_has_json(self) -> None:\n         body = \"\"\"\n         <div>\n@@ -191,6 +204,9 @@ class JMESPathTestCase(unittest.TestCase):\n         )\n         self.assertEqual(resp.xpath(\"//div/content\").jmespath(\"total\").get(), \"4\")\n \n+    @pytest.mark.skipif(\n+        not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n+    )\n     def test_jmestpath_with_re(self) -> None:\n         body = \"\"\"\n             <div>\n@@ -246,3 +262,14 @@ class JMESPathTestCase(unittest.TestCase):\n             .re(r\"(\\d+)\"),\n             [\"18\", \"32\", \"22\", \"25\"],\n         )\n+\n+    @pytest.mark.skipif(PARSEL_18_PLUS, reason=\"parsel >= 1.8 supports jmespath\")\n+    def test_jmespath_not_available(my_json_page) -> None:\n+        body = \"\"\"\n+        {\n+            \"website\": {\"name\": \"Example\"}\n+        }\n+        \"\"\"\n+        resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n+        with pytest.raises(AttributeError):\n+            resp.jmespath(\"website.name\").get()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bb99fd2f3a957958ed9ea8fa418b6127a6bebee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1138 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n import weakref\n \n-import packaging.version as version\n import parsel\n import pytest\n+from packaging import version\n from twisted.trial import unittest\n \n from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n@@ -11,6 +11,9 @@ from scrapy.selector import Selector\n PARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\n PARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n \n+print(PARSEL_VERSION)\n+print(PARSEL_18_PLUS)\n+\n \n class SelectorTestCase(unittest.TestCase):\n     def test_simple_selection(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a038faf11c2bc47921f57954a2405279d427f890", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1141 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -11,9 +11,6 @@ from scrapy.selector import Selector\n PARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\n PARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n \n-print(PARSEL_VERSION)\n-print(PARSEL_18_PLUS)\n-\n \n class SelectorTestCase(unittest.TestCase):\n     def test_simple_selection(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d907f9e09284367d555c89d2bea862a310f60a19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 702 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,11 @@ XPath selectors based on lxml\n \n from parsel import Selector as _ParselSelector\n \n+try:\n+    from parsel.selector import _NOT_SET\n+except ImportError:\n+    _NOT_SET = None\n+\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n@@ -63,7 +68,7 @@ class Selector(_ParselSelector, object_ref):\n     __slots__ = [\"response\"]\n     selectorlist_cls = SelectorList\n \n-    def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n+    def __init__(self, response=None, text=None, type=None, root=_NOT_SET, **kwargs):\n         if response is not None and text is not None:\n             raise ValueError(\n                 f\"{self.__class__.__name__}.__init__() received \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7317ff11014c4bf20d4e35193cc96a7151d1d5b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 715 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,17 +4,14 @@ XPath selectors based on lxml\n \n from parsel import Selector as _ParselSelector\n \n-try:\n-    from parsel.selector import _NOT_SET\n-except ImportError:\n-    _NOT_SET = None\n-\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n \n __all__ = [\"Selector\", \"SelectorList\"]\n \n+_NOT_SET = object()\n+\n \n def _st(response, st):\n     if st is None:\n@@ -85,4 +82,8 @@ class Selector(_ParselSelector, object_ref):\n             kwargs.setdefault(\"base_url\", response.url)\n \n         self.response = response\n-        super().__init__(text=text, type=st, root=root, **kwargs)\n+\n+        if root is not _NOT_SET:\n+            kwargs[\"root\"] = root\n+\n+        super().__init__(text=text, type=st, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4596a58a1333b8174b09336ab74c32c2e14c40f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 14 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 521 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"Boto/botocore helpers\"\"\"\n \n \n-def is_botocore_available():\n+def is_botocore_available() -> bool:\n     try:\n         import botocore  # noqa: F401\n \n\n@@ -1,19 +1,21 @@\n import warnings\n from functools import wraps\n+from typing import Any, Callable\n \n from twisted.internet import defer, threads\n+from twisted.internet.defer import Deferred\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-def deprecated(use_instead=None):\n+def deprecated(use_instead: Any = None) -> Callable:\n     \"\"\"This is a decorator which can be used to mark functions\n     as deprecated. It will result in a warning being emitted\n     when the function is used.\"\"\"\n \n-    def deco(func):\n+    def deco(func: Callable) -> Callable:\n         @wraps(func)\n-        def wrapped(*args, **kwargs):\n+        def wrapped(*args: Any, **kwargs: Any) -> Any:\n             message = f\"Call to deprecated function {func.__name__}.\"\n             if use_instead:\n                 message += f\" Use {use_instead} instead.\"\n@@ -28,23 +30,23 @@ def deprecated(use_instead=None):\n     return deco\n \n \n-def defers(func):\n+def defers(func: Callable) -> Callable[..., Deferred]:\n     \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n \n     @wraps(func)\n-    def wrapped(*a, **kw):\n+    def wrapped(*a: Any, **kw: Any) -> Deferred:\n         return defer.maybeDeferred(func, *a, **kw)\n \n     return wrapped\n \n \n-def inthread(func):\n+def inthread(func: Callable) -> Callable[..., Deferred]:\n     \"\"\"Decorator to call a function in a thread and return a deferred with the\n     result\n     \"\"\"\n \n     @wraps(func)\n-    def wrapped(*a, **kw):\n+    def wrapped(*a: Any, **kw: Any) -> Deferred:\n         return threads.deferToThread(func, *a, **kw)\n \n     return wrapped\n\n@@ -1,6 +1,7 @@\n import datetime\n import decimal\n import json\n+from typing import Any\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet import defer\n@@ -12,7 +13,7 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n     DATE_FORMAT = \"%Y-%m-%d\"\n     TIME_FORMAT = \"%H:%M:%S\"\n \n-    def default(self, o):\n+    def default(self, o: Any) -> Any:\n         if isinstance(o, set):\n             return list(o)\n         if isinstance(o, datetime.datetime):\n\n@@ -1,5 +1,6 @@\n import platform\n import sys\n+from typing import List, Tuple\n \n import cryptography\n import cssselect\n@@ -12,7 +13,7 @@ import scrapy\n from scrapy.utils.ssl import get_openssl_version\n \n \n-def scrapy_components_versions():\n+def scrapy_components_versions() -> List[Tuple[str, str]]:\n     lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n     libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0dbc83bd269e256e8247525e4d5a1d07658b635", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 698 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,18 @@ scrapy.http.Request objects\n import hashlib\n import json\n import warnings\n-from typing import Dict, Iterable, List, Optional, Tuple, Union\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Dict,\n+    Generator,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+)\n from urllib.parse import urlunparse\n from weakref import WeakKeyDictionary\n \n@@ -19,11 +30,16 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n _deprecated_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n _deprecated_fingerprint_cache = WeakKeyDictionary()\n \n \n-def _serialize_headers(headers, request):\n+def _serialize_headers(\n+    headers: Iterable[bytes], request: Request\n+) -> Generator[bytes, Any, None]:\n     for header in headers:\n         if header in request.headers:\n             yield header\n@@ -139,7 +155,7 @@ def request_fingerprint(\n     return cache[cache_key]\n \n \n-def _request_fingerprint_as_bytes(*args, **kwargs):\n+def _request_fingerprint_as_bytes(*args: Any, **kwargs: Any) -> bytes:\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"ignore\")\n         return bytes.fromhex(request_fingerprint(*args, **kwargs))\n@@ -231,7 +247,7 @@ class RequestFingerprinter:\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n-    def __init__(self, crawler=None):\n+    def __init__(self, crawler: Optional[\"Crawler\"] = None):\n         if crawler:\n             implementation = crawler.settings.get(\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n@@ -265,7 +281,7 @@ class RequestFingerprinter:\n                 f\"and '2.7'.\"\n             )\n \n-    def fingerprint(self, request: Request):\n+    def fingerprint(self, request: Request) -> bytes:\n         return self._fingerprint(request)\n \n \n@@ -311,7 +327,7 @@ def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:\n     If a spider is given, it will try to resolve the callbacks looking at the\n     spider for methods with the same name.\n     \"\"\"\n-    request_cls = load_object(d[\"_class\"]) if \"_class\" in d else Request\n+    request_cls: Type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n     kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n     if d.get(\"callback\") and spider:\n         kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n@@ -320,7 +336,7 @@ def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:\n     return request_cls(**kwargs)\n \n \n-def _get_method(obj, name):\n+def _get_method(obj: Any, name: Any) -> Any:\n     \"\"\"Helper function for request_from_dict\"\"\"\n     name = str(name)\n     try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f64a7dedca6d4bf33f86c633a0381a8017eb79f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 14 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 1224 | Contributors (this commit): 26 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: 0.775\n\nDIFF:\n@@ -6,6 +6,7 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import re\n+from typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n@@ -15,8 +16,14 @@ from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n \n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    from scrapy import Spider\n \n-def url_is_from_any_domain(url, domains):\n+\n+UrlT = Union[str, bytes, ParseResult]\n+\n+\n+def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n     host = parse_url(url).netloc.lower()\n     if not host:\n@@ -25,29 +32,29 @@ def url_is_from_any_domain(url, domains):\n     return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n \n \n-def url_is_from_spider(url, spider):\n+def url_is_from_spider(url: UrlT, spider: Type[\"Spider\"]) -> bool:\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n     return url_is_from_any_domain(\n         url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n     )\n \n \n-def url_has_any_extension(url, extensions):\n+def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n     \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n     lowercase_path = parse_url(url).path.lower()\n     return any(lowercase_path.endswith(ext) for ext in extensions)\n \n \n-def parse_url(url, encoding=None):\n+def parse_url(url: UrlT, encoding: Optional[str] = None) -> ParseResult:\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n     \"\"\"\n     if isinstance(url, ParseResult):\n         return url\n-    return urlparse(to_unicode(url, encoding))\n+    return cast(ParseResult, urlparse(to_unicode(url, encoding)))\n \n \n-def escape_ajax(url):\n+def escape_ajax(url: str) -> str:\n     \"\"\"\n     Return the crawlable url according to:\n     https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n@@ -76,7 +83,7 @@ def escape_ajax(url):\n     return add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n \n \n-def add_http_if_no_scheme(url):\n+def add_http_if_no_scheme(url: str) -> str:\n     \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n     match = re.match(r\"^\\w+://\", url, flags=re.I)\n     if not match:\n@@ -87,7 +94,7 @@ def add_http_if_no_scheme(url):\n     return url\n \n \n-def _is_posix_path(string):\n+def _is_posix_path(string: str) -> bool:\n     return bool(\n         re.match(\n             r\"\"\"\n@@ -109,7 +116,7 @@ def _is_posix_path(string):\n     )\n \n \n-def _is_windows_path(string):\n+def _is_windows_path(string: str) -> bool:\n     return bool(\n         re.match(\n             r\"\"\"\n@@ -125,11 +132,11 @@ def _is_windows_path(string):\n     )\n \n \n-def _is_filesystem_path(string):\n+def _is_filesystem_path(string: str) -> bool:\n     return _is_posix_path(string) or _is_windows_path(string)\n \n \n-def guess_scheme(url):\n+def guess_scheme(url: str) -> str:\n     \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n     http:// otherwise.\"\"\"\n     if _is_filesystem_path(url):\n@@ -138,12 +145,12 @@ def guess_scheme(url):\n \n \n def strip_url(\n-    url,\n-    strip_credentials=True,\n-    strip_default_port=True,\n-    origin_only=False,\n-    strip_fragment=True,\n-):\n+    url: str,\n+    strip_credentials: bool = True,\n+    strip_default_port: bool = True,\n+    origin_only: bool = False,\n+    strip_fragment: bool = True,\n+) -> str:\n     \"\"\"Strip URL string from some of its components:\n \n     - ``strip_credentials`` removes \"user:password@\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7347d021457866ade62a6ed8a0839766e91032e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 21 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 1864 | Contributors (this commit): 43 | Commits (past 90d): 4 | Contributors (cumulative): 64 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from typing import Any\n+\n from twisted.internet import defer\n from twisted.internet.base import ThreadedResolver\n from twisted.internet.interfaces import (\n@@ -11,7 +13,7 @@ from zope.interface.declarations import implementer, provider\n from scrapy.utils.datatypes import LocalCache\n \n # TODO: cache misses\n-dnscache = LocalCache(10000)\n+dnscache: LocalCache[str, Any] = LocalCache(10000)\n \n \n @implementer(IResolverSimple)\n@@ -36,7 +38,7 @@ class CachingThreadedResolver(ThreadedResolver):\n     def install_on_reactor(self):\n         self.reactor.installResolver(self)\n \n-    def getHostByName(self, name, timeout=None):\n+    def getHostByName(self, name: str, timeout=None):\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n         # in Twisted<=16.6, getHostByName() is always called with\n@@ -110,7 +112,7 @@ class CachingHostnameResolver:\n     def resolveHostName(\n         self,\n         resolutionReceiver,\n-        hostName,\n+        hostName: str,\n         portNumber=0,\n         addressTypes=None,\n         transportSemantics=\"TCP\",\n\n@@ -8,6 +8,10 @@ This module must not depend on any module outside the Standard Library.\n import collections\n import weakref\n from collections.abc import Mapping\n+from typing import Any, Optional, Sequence, TypeVar\n+\n+_KT = TypeVar(\"_KT\")\n+_VT = TypeVar(\"_VT\")\n \n \n class CaselessDict(dict):\n@@ -64,24 +68,24 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n-class LocalCache(collections.OrderedDict):\n+class LocalCache(collections.OrderedDict[_KT, _VT]):\n     \"\"\"Dictionary with a finite number of keys.\n \n     Older items expires first.\n     \"\"\"\n \n-    def __init__(self, limit=None):\n+    def __init__(self, limit: Optional[int] = None):\n         super().__init__()\n-        self.limit = limit\n+        self.limit: Optional[int] = limit\n \n-    def __setitem__(self, key, value):\n+    def __setitem__(self, key: _KT, value: _VT) -> None:\n         if self.limit:\n             while len(self) >= self.limit:\n                 self.popitem(last=False)\n         super().__setitem__(key, value)\n \n \n-class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n+class LocalWeakReferencedCache(weakref.WeakKeyDictionary[_KT, _VT]):\n     \"\"\"\n     A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n     underlying data structure, making it ordered and capable of being size-limited.\n@@ -93,17 +97,17 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n     it cannot be instantiated with an initial dictionary.\n     \"\"\"\n \n-    def __init__(self, limit=None):\n+    def __init__(self, limit: Optional[int] = None):\n         super().__init__()\n-        self.data = LocalCache(limit=limit)\n+        self.data: LocalCache = LocalCache(limit=limit)\n \n-    def __setitem__(self, key, value):\n+    def __setitem__(self, key: _KT, value: _VT) -> None:\n         try:\n             super().__setitem__(key, value)\n         except TypeError:\n             pass  # key is not weak-referenceable, skip caching\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: _KT) -> Optional[_VT]:  # type: ignore[override]\n         try:\n             return super().__getitem__(key)\n         except (TypeError, KeyError):\n@@ -113,8 +117,8 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n class SequenceExclude:\n     \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n \n-    def __init__(self, seq):\n-        self.seq = seq\n+    def __init__(self, seq: Sequence):\n+        self.seq: Sequence = seq\n \n-    def __contains__(self, item):\n+    def __contains__(self, item: Any) -> bool:\n         return item not in self.seq\n\n@@ -214,16 +214,18 @@ def walk_callable(node):\n         yield node\n \n \n-_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n+_generator_callbacks_cache: LocalWeakReferencedCache[\n+    Callable, bool\n+] = LocalWeakReferencedCache(limit=128)\n \n \n-def is_generator_with_return_value(callable):\n+def is_generator_with_return_value(callable: Callable) -> bool:\n     \"\"\"\n     Returns True if a callable is a generator function which includes a\n     'return' statement with a value different than None, False otherwise\n     \"\"\"\n     if callable in _generator_callbacks_cache:\n-        return _generator_callbacks_cache[callable]\n+        return bool(_generator_callbacks_cache[callable])\n \n     def returns_none(return_node):\n         value = return_node.value\n@@ -248,10 +250,10 @@ def is_generator_with_return_value(callable):\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n                 _generator_callbacks_cache[callable] = True\n-                return _generator_callbacks_cache[callable]\n+                return bool(_generator_callbacks_cache[callable])\n \n     _generator_callbacks_cache[callable] = False\n-    return _generator_callbacks_cache[callable]\n+    return bool(_generator_callbacks_cache[callable])\n \n \n def warn_on_generator_with_return_value(spider: \"Spider\", callable: Callable) -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
