{"custom_id": "keras#7de5c53be40ff8f737b13295246ad75fb7bb5d39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1594 | Contributors (this commit): 10 | Commits (past 90d): 42 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -99,7 +99,7 @@ class RandomCrop(TFDataLayer):\n                 ops.random.uniform(\n                     (),\n                     0,\n-                    maxval=input_height - self.height + 1,\n+                    maxval=float(input_height - self.height + 1),\n                     dtype=inputs.dtype,\n                     seed=self.seed_generator,\n                 ),\n@@ -109,7 +109,7 @@ class RandomCrop(TFDataLayer):\n                 ops.random.uniform(\n                     (),\n                     0,\n-                    maxval=input_width - self.width + 1,\n+                    maxval=float(input_width - self.width + 1),\n                     dtype=inputs.dtype,\n                     seed=self.seed_generator,\n                 ),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#336c6a042bf5ca508ea3c3ee0fba2624d7f9c99f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 659 | Lines Deleted: 159 | Files Changed: 13 | Hunks: 45 | Methods Changed: 43 | Complexity Δ (Sum/Max): 35/17 | Churn Δ: 818 | Churn Cumulative: 13254 | Contributors (this commit): 18 | Commits (past 90d): 247 | Contributors (cumulative): 89 | DMM Complexity: 0.7135135135135136\n\nDIFF:\n@@ -0,0 +1,400 @@\n+\"\"\"\n+Title: Compact Convolutional Transformers\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/06/30\n+Last modified: 2023/07/17\n+Description: Compact Convolutional Transformers for efficient image classification.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+As discussed in the [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) paper,\n+a Transformer-based architecture for vision typically requires a larger dataset than\n+usual, as well as a longer pre-training schedule. [ImageNet-1k](http://imagenet.org/)\n+(which has about a million images) is considered to fall under the medium-sized data regime with\n+respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a typical\n+Transformer-based architecture) do not have well-informed inductive biases (such as\n+convolutions for processing images). This begs the question: can't we combine the\n+benefits of convolution and the benefits of Transformers\n+in a single network architecture? These benefits include parameter-efficiency, and\n+self-attention to process long-range and global dependencies (interactions between\n+different regions in an image).\n+\n+In [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704),\n+Hassani et al. present an approach for doing exactly this. They proposed the\n+**Compact Convolutional Transformer** (CCT) architecture. In this example, we will work on an\n+implementation of CCT and we will see how well it performs on the CIFAR-10 dataset.\n+\n+If you are unfamiliar with the concept of self-attention or Transformers, you can read\n+[this chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)\n+from  François Chollet's book *Deep Learning with Python*. This example uses\n+code snippets from another example,\n+[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n+\n+\n+\"\"\"\n+\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+from keras_core import layers\n+import keras_core as keras\n+\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import numpy as np\n+\n+\"\"\"\n+## Hyperparameters and constants\n+\"\"\"\n+\n+positional_emb = True\n+conv_layers = 2\n+projection_dim = 128\n+\n+num_heads = 2\n+transformer_units = [\n+    projection_dim,\n+    projection_dim,\n+]\n+transformer_layers = 2\n+stochastic_depth_rate = 0.1\n+\n+learning_rate = 0.001\n+weight_decay = 0.0001\n+batch_size = 128\n+num_epochs = 30\n+image_size = 32\n+\n+\"\"\"\n+## Load CIFAR-10 dataset\n+\"\"\"\n+\n+num_classes = 10\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\"\"\"\n+## The CCT tokenizer\n+\n+The first recipe introduced by the CCT authors is the tokenizer for processing the\n+images. In a standard ViT, images are organized into uniform *non-overlapping* patches.\n+This eliminates the boundary-level information present in between different patches. This\n+is important for a neural network to effectively exploit the locality information. The\n+figure below presents an illustration of how images are organized into patches.\n+\n+![](https://i.imgur.com/IkBK9oY.png)\n+\n+We already know that convolutions are quite good at exploiting locality information. So,\n+based on this, the authors introduce an all-convolution mini-network to produce image\n+patches.\n+\"\"\"\n+\n+\n+class CCTTokenizer(layers.Layer):\n+    def __init__(\n+        self,\n+        kernel_size=3,\n+        stride=1,\n+        padding=1,\n+        pooling_kernel_size=3,\n+        pooling_stride=2,\n+        num_conv_layers=conv_layers,\n+        num_output_channels=[64, 128],\n+        positional_emb=positional_emb,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        # This is our tokenizer.\n+        self.conv_model = keras.Sequential()\n+        for i in range(num_conv_layers):\n+            self.conv_model.add(\n+                layers.Conv2D(\n+                    num_output_channels[i],\n+                    kernel_size,\n+                    stride,\n+                    padding=\"valid\",\n+                    use_bias=False,\n+                    activation=\"relu\",\n+                    kernel_initializer=\"he_normal\",\n+                )\n+            )\n+            self.conv_model.add(layers.ZeroPadding2D(padding))\n+            self.conv_model.add(\n+                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n+            )\n+\n+        self.positional_emb = positional_emb\n+\n+    def call(self, images):\n+        outputs = self.conv_model(images)\n+        # After passing the images through our mini-network the spatial dimensions\n+        # are flattened to form sequences.\n+        reshaped = tf.reshape(\n+            outputs,\n+            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n+        )\n+        return reshaped\n+\n+    def positional_embedding(self, image_size):\n+        # Positional embeddings are optional in CCT. Here, we calculate\n+        # the number of sequences and initialize an `Embedding` layer to\n+        # compute the positional embeddings later.\n+        if self.positional_emb:\n+            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n+            dummy_outputs = self.call(dummy_inputs)\n+            sequence_length = tf.shape(dummy_outputs)[1]\n+            projection_dim = tf.shape(dummy_outputs)[-1]\n+\n+            embed_layer = layers.Embedding(\n+                input_dim=sequence_length, output_dim=projection_dim\n+            )\n+            return embed_layer, sequence_length\n+        else:\n+            return None\n+\n+\"\"\"\n+## Sequence Pooling\n+Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only\n+the feature map corresponding to the class token is pooled and is then used for the\n+subsequent classification task (or any other downstream task).\n+\"\"\"\n+\n+class SequencePooling(layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.attention = layers.Dense(1)\n+\n+    def call(self, x):\n+        attention_weights = tf.nn.softmax(self.attention(x), axis=1)\n+        weighted_representation = tf.matmul(\n+        attention_weights, x, transpose_a=True\n+    )\n+        return tf.squeeze(weighted_representation, -2)\n+\n+\n+\"\"\"\n+## Stochastic depth for regularization\n+\n+[Stochastic depth](https://arxiv.org/abs/1603.09382) is a regularization technique that\n+randomly drops a set of layers. During inference, the layers are kept as they are. It is\n+very much similar to [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) but only\n+that it operates on a block of layers rather than individual nodes present inside a\n+layer. In CCT, stochastic depth is used just before the residual blocks of a Transformers\n+encoder.\n+\"\"\"\n+\n+\n+# Referred from: github.com:rwightman/pytorch-image-models.\n+class StochasticDepth(layers.Layer):\n+    def __init__(self, drop_prop, **kwargs):\n+        super().__init__(**kwargs)\n+        self.drop_prob = drop_prop\n+\n+    def call(self, x, training=None):\n+        if training:\n+            keep_prob = 1 - self.drop_prob\n+            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n+            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n+            random_tensor = tf.floor(random_tensor)\n+            return (x / keep_prob) * random_tensor\n+        return x\n+\n+\n+\"\"\"\n+## MLP for the Transformers encoder\n+\"\"\"\n+\n+\n+def mlp(x, hidden_units, dropout_rate):\n+    for units in hidden_units:\n+        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dropout(dropout_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Data augmentation\n+\n+In the [original paper](https://arxiv.org/abs/2104.05704), the authors use\n+[AutoAugment](https://arxiv.org/abs/1805.09501) to induce stronger regularization. For\n+this example, we will be using the standard geometric augmentations like random cropping\n+and flipping.\n+\"\"\"\n+\n+# Note the rescaling layer. These layers have pre-defined inference behavior.\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Rescaling(scale=1.0 / 255),\n+        layers.RandomCrop(image_size, image_size),\n+        layers.RandomFlip(\"horizontal\"),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+\n+\"\"\"\n+## The final CCT model\n+\n+In CCT, outputs from the Transformers encoder are weighted and then passed on to the final task-specific layer (in\n+this example, we do classification).\n+\"\"\"\n+\n+\n+\n+\n+def create_cct_model(\n+    image_size=image_size,\n+    input_shape=input_shape,\n+    num_heads=num_heads,\n+    projection_dim=projection_dim,\n+    transformer_units=transformer_units,\n+):\n+    inputs = layers.Input(input_shape)\n+\n+    # Augment data.\n+    augmented = data_augmentation(inputs)\n+\n+    # Encode patches.\n+    cct_tokenizer = CCTTokenizer()\n+    encoded_patches = cct_tokenizer(augmented)\n+\n+    # Apply positional embedding.\n+    if positional_emb:\n+        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n+        positions = tf.range(start=0, limit=seq_length, delta=1)\n+        position_embeddings = pos_embed(positions)\n+        encoded_patches += position_embeddings\n+\n+    # Calculate Stochastic Depth probabilities.\n+    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n+\n+    # Create multiple layers of the Transformer block.\n+    for i in range(transformer_layers):\n+        # Layer normalization 1.\n+        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n+\n+        # Create a multi-head attention layer.\n+        attention_output = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n+        )(x1, x1)\n+\n+        # Skip connection 1.\n+        attention_output = StochasticDepth(dpr[i])(attention_output)\n+        x2 = layers.Add()([attention_output, encoded_patches])\n+\n+        # Layer normalization 2.\n+        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n+\n+        # MLP.\n+        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n+\n+        # Skip connection 2.\n+        x3 = StochasticDepth(dpr[i])(x3)\n+        encoded_patches = layers.Add()([x3, x2])\n+\n+    # Apply sequence pooling.\n+    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n+    weighted_representation = SequencePooling()(representation)\n+\n+    # Classify outputs.\n+    logits = layers.Dense(num_classes)(weighted_representation)\n+    # Create the Keras model.\n+    model = keras.Model(inputs=inputs, outputs=logits)\n+    return model\n+\n+\n+\"\"\"\n+## Model training and evaluation\n+\"\"\"\n+\n+\n+def run_experiment(model):\n+    optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n+\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=keras.losses.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=0.1\n+        ),\n+        metrics=[\n+            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n+            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+        ],\n+    )\n+\n+    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_accuracy\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    history = model.fit(\n+        x=x_train,\n+        y=y_train,\n+        batch_size=batch_size,\n+        epochs=num_epochs,\n+        validation_split=0.1,\n+        callbacks=[checkpoint_callback],\n+    )\n+\n+    model.load_weights(checkpoint_filepath)\n+    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+    return history\n+\n+\n+cct_model = create_cct_model()\n+history = run_experiment(cct_model)\n+\n+\"\"\"\n+Let's now visualize the training progress of the model.\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"], label=\"train_loss\")\n+plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n+plt.xlabel(\"Epochs\")\n+plt.ylabel(\"Loss\")\n+plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n+plt.legend()\n+plt.grid()\n+plt.show()\n+\n+\"\"\"\n+The CCT model we just trained has just **0.4 million** parameters, and it gets us to\n+~78% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n+well. This means we can train this network for longer (perhaps with a bit more\n+regularization) and may obtain even better performance. This performance can further be\n+improved by additional recipes like cosine decay learning rate schedule, other data augmentation\n+techniques like [AutoAugment](https://arxiv.org/abs/1805.09501),\n+[MixUp](https://arxiv.org/abs/1710.09412) or\n+[Cutmix](https://arxiv.org/abs/1905.04899). With these modifications, the authors present\n+95.1% top-1 accuracy on the CIFAR-10 dataset. The authors also present a number of\n+experiments to study how the number of convolution blocks, Transformers layers, etc.\n+affect the final performance of CCTs.\n+\n+For a comparison, a ViT model takes about **4.7 million** parameters and **100\n+epochs** of training to reach a top-1 accuracy of 78.22% on the CIFAR-10 dataset. You can\n+refer to\n+[this notebook](https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb)\n+to know about the experimental setup.\n+\n+The authors also demonstrate the performance of Compact Convolutional Transformers on\n+NLP tasks and they report competitive results there.\n+\"\"\"\n\n\n@@ -0,0 +1 @@\n+from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -0,0 +1,24 @@\n+import torch\n+\n+from keras_core.optimizers.base_optimizer import BaseOptimizer\n+\n+\n+class TorchOptimizer(BaseOptimizer):\n+    def __new__(cls, *args, **kwargs):\n+        # Import locally to avoid circular imports.\n+        from keras_core import optimizers\n+        from keras_core.backend.torch.optimizers import torch_sgd\n+\n+        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n+        if cls in OPTIMIZERS:\n+            return OPTIMIZERS[cls](*args, **kwargs)\n+        return super().__new__(cls)\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+\n+        torch._foreach_mul_(\n+            [v.value for v in variables if self._use_weight_decay(v)],\n+            1 - self.weight_decay * self._get_current_learning_rate(),\n+        )\n\n@@ -0,0 +1,43 @@\n+import torch\n+\n+from keras_core import optimizers\n+\n+\n+class SGD(optimizers.SGD):\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+\n+        self._parallel_update_step(\n+            grads,\n+            [v.value for v in trainable_variables],\n+            self._get_current_learning_rate(),\n+        )\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        if self.momentum != 0:\n+            bufs = [\n+                self.momentums[self._get_variable_index(variable.value)]\n+                for variable in variables\n+            ]\n+\n+            for i in range(len(bufs)):\n+                if bufs[i] is None:\n+                    bufs[i] = torch.clone(grads[i]).detach()\n+\n+            torch._foreach_mul_(bufs, self.momentum)\n+            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n+\n+            if self.nesterov:\n+                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n+                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n+            else:\n+                torch._foreach_add_(variables, bufs)\n+\n+        else:\n+            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -0,0 +1,88 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.backend import KerasTensor\n+from keras_core.layers import InputLayer\n+\n+\n+class InputLayerTest(testing.TestCase):\n+    # Testing happy path for layer without input tensor\n+    def test_input_basic(self):\n+        input_shape = (2, 3)\n+        batch_size = 4\n+        dtype = \"float32\"\n+        ndim = len(tuple((batch_size,) + input_shape))\n+\n+        values = InputLayer(\n+            shape=input_shape, batch_size=batch_size, dtype=dtype\n+        )\n+\n+        self.assertEqual(values.dtype, dtype)\n+        self.assertEqual(values.batch_shape[0], batch_size)\n+        self.assertEqual(values.batch_shape[1:], input_shape)\n+        self.assertEqual(values.trainable, True)\n+        self.assertIsInstance(values.output, KerasTensor)\n+        self.assertEqual(values.output.ndim, ndim)\n+        self.assertEqual(values.output.dtype, dtype)\n+\n+    # Testing shape is not None and batch_shape is not None condition\n+    def test_input_error1(self):\n+        input_shape = (2, 3)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot pass both `shape` and `batch_shape`\"\n+        ):\n+            InputLayer(shape=input_shape, batch_shape=input_shape)\n+\n+    # Testing batch_size is not None and batch_shape is not None\n+    def test_input_error2(self):\n+        input_shape = (2, 3)\n+        batch_size = 4\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"cannot pass both `batch_size` and `batch_shape`\"\n+        ):\n+            InputLayer(batch_size=batch_size, batch_shape=input_shape)\n+\n+    # Testing shape is None and batch_shape is None\n+    def test_input_error3(self):\n+        with self.assertRaisesRegex(ValueError, \"pass a `shape` argument.\"):\n+            InputLayer(shape=None, batch_shape=None)\n+\n+    # Testing Input tensor is not Keras tensor\n+    def test_input_tensor_error(self):\n+        input_shape = (2, 3)\n+        batch_size = 4\n+        input_tensor = np.zeros(input_shape)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `input_tensor` must be a KerasTensor\"\n+        ):\n+            InputLayer(\n+                shape=input_shape,\n+                batch_size=batch_size,\n+                input_tensor=input_tensor,\n+            )\n+\n+    # Testing happy path for layer with input tensor\n+    def testing_input_tensor(self):\n+        input_shape = (2, 3)\n+        batch_size = 4\n+        dtype = \"float32\"\n+        input_tensor = KerasTensor(shape=input_shape, dtype=dtype)\n+\n+        values = InputLayer(\n+            shape=input_shape,\n+            batch_size=batch_size,\n+            input_tensor=input_tensor,\n+            dtype=dtype,\n+        )\n+\n+        self.assertEqual(values.dtype, dtype)\n+        self.assertEqual(values.batch_shape[0], batch_size)\n+        self.assertEqual(values.batch_shape[1:], input_shape)\n+        self.assertEqual(values.trainable, True)\n+        self.assertIsInstance(values.output, KerasTensor)\n+        self.assertEqual(values.output, input_tensor)\n+        self.assertEqual(values.output.ndim, input_tensor.ndim)\n+        self.assertEqual(values.output.dtype, dtype)\n\n@@ -84,8 +84,8 @@ class Layer(BackendLayer, Operation):\n \n     Attributes:\n         name: The name of the layer (string).\n-        dtype: The dtype of the layer's weights.\n-        variable_dtype: Dtype of the layer's variables.\n+        dtype: Dtype of the layer's weights. Alias of `layer.variable_dtype`.\n+        variable_dtype: Dtype of the layer's weights.\n         compute_dtype: The dtype of the layer's computations.\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n@@ -374,21 +374,19 @@ class Layer(BackendLayer, Operation):\n         constraint=None,\n         name=None,\n     ):\n-        # TODO: handle layout\n-        self._check_super_called()\n-        initializer = initializers.get(initializer)\n-        variable = backend.Variable(\n-            initializer=initializer,\n+        \"\"\"Add a weight variable to the layer.\n+\n+        Alias of `add_weight()`.\n+        \"\"\"\n+        return self.add_weight(\n             shape=shape,\n-            dtype=dtype or self.variable_dtype,\n+            initializer=initializer,\n+            dtype=dtype,\n             trainable=trainable,\n+            regularizer=regularizer,\n+            constraint=constraint,\n             name=name,\n         )\n-        # Will be added to layer.losses\n-        variable.regularizer = regularizer\n-        variable.constraint = constraint\n-        self._track_variable(variable)\n-        return variable\n \n     def add_weight(\n         self,\n@@ -402,8 +400,6 @@ class Layer(BackendLayer, Operation):\n     ):\n         \"\"\"Add a weight variable to the layer.\n \n-        Alias of `add_variable()`.\n-\n         Args:\n             shape: Shape tuple for the variable.\n                 Must be fully-defined (no `None` entries).\n@@ -422,15 +418,21 @@ class Layer(BackendLayer, Operation):\n             name: String name of the variable. Useful\n                 for debugging purposes.\n         \"\"\"\n-        return self.add_variable(\n-            shape=shape,\n+        # TODO: handle layout\n+        self._check_super_called()\n+        initializer = initializers.get(initializer)\n+        variable = backend.Variable(\n             initializer=initializer,\n-            dtype=dtype,\n+            shape=shape,\n+            dtype=dtype or self.variable_dtype,\n             trainable=trainable,\n-            regularizer=regularizer,\n-            constraint=constraint,\n             name=name,\n         )\n+        # Will be added to layer.losses\n+        variable.regularizer = regularizer\n+        variable.constraint = constraint\n+        self._track_variable(variable)\n+        return variable\n \n     @property\n     def trainable(self):\n@@ -459,9 +461,13 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def variables(self):\n-        # Return only weights/rng state/metric variables\n-        # of all Layers, recursively.\n-        # Also deduplicate them.\n+        \"\"\"List of all layer state, including metric variables and random seeds.\n+\n+        This extends `layer.weights` to include all state used by the layer\n+        including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n+        # Return all `Variables` associate with the layer including metrics\n+        # and random seeds. Also deduplicate them.\n         variables = []\n         seen_ids = set()\n         for v in self._trainable_variables + self._non_trainable_variables:\n@@ -481,20 +487,32 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_variables(self):\n+        \"\"\"List of all trainable layer state.\n+\n+        This is equivalent to `layer.trainable_weights`.\n+        \"\"\"\n         if not self.trainable:\n             return []\n         return [v for v in self.variables if v.trainable]\n \n     @property\n     def non_trainable_variables(self):\n+        \"\"\"List of all non-trainable layer state.\n+\n+        This extends `layer.non_trainable_weights` to include all state used by\n+        the layer including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n         if not self.trainable:\n             return self.variables\n         return [v for v in self.variables if not v.trainable]\n \n     @property\n     def weights(self):\n-        \"\"\"List of weight variables of the layer.\"\"\"\n-        # Return only \"own weights\" of all Layers, recursively.\n+        \"\"\"List of all weight variables of the layer.\n+\n+        Unlike, `layer.variables` this excludes metric state and random seeds.\n+        \"\"\"\n+        # Return only `Variables` directly owned by layers and sub-layers.\n         # Also deduplicate them.\n         weights = []\n         seen_ids = set()\n@@ -511,10 +529,9 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_weights(self):\n-        \"\"\"List of trainable weight variables of the layer.\n+        \"\"\"List of all trainable weight variables of the layer.\n \n-        These are the weights that get updated by the optimizer\n-        during training.\n+        These are the weights that get updated by the optimizer during training.\n         \"\"\"\n         if not self.trainable:\n             return []\n@@ -522,10 +539,11 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def non_trainable_weights(self):\n-        \"\"\"List of non-trainable weight variables of the layer.\n+        \"\"\"List of all non-trainable weight variables of the layer.\n \n-        Non-trainable weights may include batch normalization statistics,\n-        metric variables, or RNG seed variables.\n+        These are the weights that should not be updated by the optimizer during\n+        training. Unlike, `layer.non_trainable_variables` this excludes metric\n+        state and random seeds.\n         \"\"\"\n         if not self.trainable:\n             return self.weights\n@@ -555,7 +573,7 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def dtype(self):\n-        \"\"\"The dtype of the state (weights) of the layer.\"\"\"\n+        \"\"\"Alias of `layer.variable_dtype`.\"\"\"\n         return self.variable_dtype\n \n     @property\n\n@@ -1,15 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n-from keras_core.utils import dtype_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -53,128 +52,41 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+        if not tf.available:\n+            raise ImportError(\n+                \"Layer RandomCrop requires TensorFlow. \"\n+                \"Install it via `pip install tensorflow`.\"\n+            )\n+\n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            # input_dtype_max = (2 ** dtype_utils.dtype_size(inputs.dtype)) - 1\n-            input_height, input_width = (\n-                input_shape[self.height_axis],\n-                input_shape[self.width_axis],\n-            )\n-\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_height - self.height + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                h_diff.dtype,\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_width - self.width + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                h_diff.dtype,\n-            )\n-            # rands = ops.random.uniform(\n-            #     [2], 0, input_dtype_max, inputs.dtype, seed=self.seed_generator\n-            # )\n-            # original_dtype = h_diff.dtype\n-            # h_start = self.backend.cast(\n-            #     rands[0] % self.backend.cast((h_diff + 1), self.compute_dtype),\n-            #     original_dtype,\n-            # )\n-            # w_start = self.backend.cast(\n-            #     rands[1] % self.backend.cast((w_diff + 1), self.compute_dtype),\n-            #     original_dtype,\n-            # )\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.gru(\n+                out = backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,6 +536,11 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.lstm(\n+                out = backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,6 +516,11 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -5,7 +5,7 @@ from keras_core.backend import KerasTensor\n \n class SymbolicArguments:\n     def __init__(self, *args, **kwargs):\n-        # TODO: validation\n+\n         self.args = tree.map_structure(lambda x: x, args)\n         self.kwargs = tree.map_structure(lambda x: x, kwargs)\n         self._flat_arguments = tree.flatten((self.args, self.kwargs))\n\n@@ -3,9 +3,13 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n+    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n \n-    BackendOptimizer = tf_optimizer.TFOptimizer\n+    BackendOptimizer = TFOptimizer\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.optimizers import TorchOptimizer\n+\n+    BackendOptimizer = TorchOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec3964adb870b48cbae66a173e99f6ef90db2ca1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 214 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -128,8 +128,22 @@ class InitializersTest(testing.TestCase):\n         gain = 2.0\n         seed = 1234\n         initializer = initializers.OrthogonalInitializer(gain=gain, seed=seed)\n-        _ = initializer(shape=shape)\n-        # TODO: test correctness\n+        values = initializer(shape=shape)\n+        self.assertEqual(initializer.seed, seed)\n+        self.assertEqual(initializer.gain, gain)\n+\n+        self.assertEqual(values.shape, shape)\n+        array = np.array(values)\n+        # Making sure that the columns have gain * unit norm value\n+        for column in array.T:\n+            self.assertAlmostEqual(np.linalg.norm(column), gain * 1.0)\n+\n+        # Making sure that each column is orthonormal to the other column\n+        for i in range(array.shape[-1]):\n+            for j in range(i + 1, array.shape[-1]):\n+                self.assertAlmostEqual(\n+                    np.dot(array[..., i], array[..., j]), 0.0\n+                )\n \n         self.run_class_serialization_test(initializer)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fcda274b0099a5d6d3d8c3aea13ab6621defa55c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 511 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 511 | Churn Cumulative: 511 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,511 @@\n+\"\"\"\n+Title: Learning to tokenize in Vision Transformers\n+Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/12/10\n+Last modified: 2023/07/18\n+Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Vision Transformers ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)) and many\n+other Transformer-based architectures ([Liu et al.](https://arxiv.org/abs/2103.14030),\n+[Yuan et al.](https://arxiv.org/abs/2101.11986), etc.) have shown strong results in\n+image recognition. The following provides a brief overview of the components involved in the\n+Vision Transformer architecture for image classification:\n+\n+* Extract small patches from input images.\n+* Linearly project those patches.\n+* Add positional embeddings to these linear projections.\n+* Run these projections through a series of Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n+blocks.\n+* Finally, take the representation from the final Transformer block and add a\n+classification head.\n+\n+If we take 224x224 images and extract 16x16 patches, we get a total of 196 patches (also\n+called tokens) for each image. The number of patches increases as we increase the\n+resolution, leading to higher memory footprint. Could we use a reduced\n+number of patches without having to compromise performance?\n+Ryoo et al. investigate this question in\n+[TokenLearner: Adaptive Space-Time Tokenization for Videos](https://openreview.net/forum?id=z-l1kpDXs88).\n+They introduce a novel module called **TokenLearner** that can help reduce the number\n+of patches used by a Vision Transformer (ViT) in an adaptive manner. With TokenLearner\n+incorporated in the standard ViT architecture, they are able to reduce the amount of\n+compute (measured in FLOPS) used by the model.\n+\n+In this example, we implement the TokenLearner module and demonstrate its\n+performance with a mini ViT and the CIFAR-10 dataset. We make use of the following\n+references:\n+\n+* [Official TokenLearner code](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py)\n+* [Image Classification with ViTs on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n+* [TokenLearner slides from NeurIPS 2021](https://nips.cc/media/neurips-2021/Slides/26578.pdf)\n+\"\"\"\n+\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+from tensorflow import data as tf_data\n+\n+\n+from datetime import datetime\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+import math\n+\n+\"\"\"\n+## Hyperparameters\n+\n+Please feel free to change the hyperparameters and check your results. The best way to\n+develop intuition about the architecture is to experiment with it.\n+\"\"\"\n+\n+# DATA\n+BATCH_SIZE = 256\n+AUTO = tf_data.AUTOTUNE\n+INPUT_SHAPE = (32, 32, 3)\n+NUM_CLASSES = 10\n+\n+# OPTIMIZER\n+LEARNING_RATE = 1e-3\n+WEIGHT_DECAY = 1e-4\n+\n+# TRAINING\n+EPOCHS = 20\n+\n+# AUGMENTATION\n+IMAGE_SIZE = 48  # We will resize input images to this size.\n+PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n+NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n+\n+# ViT ARCHITECTURE\n+LAYER_NORM_EPS = 1e-6\n+PROJECTION_DIM = 128\n+NUM_HEADS = 4\n+NUM_LAYERS = 4\n+MLP_UNITS = [\n+    PROJECTION_DIM * 2,\n+    PROJECTION_DIM,\n+]\n+\n+# TOKENLEARNER\n+NUM_TOKENS = 4\n+\n+\"\"\"\n+## Load and prepare the CIFAR-10 dataset\n+\"\"\"\n+\n+# Load the CIFAR-10 dataset.\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+(x_train, y_train), (x_val, y_val) = (\n+    (x_train[:40000], y_train[:40000]),\n+    (x_train[40000:], y_train[40000:]),\n+)\n+print(f\"Training samples: {len(x_train)}\")\n+print(f\"Validation samples: {len(x_val)}\")\n+print(f\"Testing samples: {len(x_test)}\")\n+\n+# Convert to tf.data.Dataset objects.\n+train_ds = tf_data.Dataset.from_tensor_slices((x_train, y_train))\n+train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)\n+\n+val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val))\n+val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n+\n+test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))\n+test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n+\n+\"\"\"\n+## Data augmentation\n+\n+The augmentation pipeline consists of:\n+\n+- Rescaling\n+- Resizing\n+- Random cropping (fixed-sized or random sized)\n+- Random horizontal flipping\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Rescaling(1 / 255.0),\n+        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n+        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n+        layers.RandomFlip(\"horizontal\"),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+\n+\"\"\"\n+Note that image data augmentation layers do not apply data transformations at inference time.\n+This means that when these layers are called with `training=False` they behave differently. Refer\n+[to the documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) for more\n+details.\n+\"\"\"\n+\n+\"\"\"\n+## Positional embedding module\n+\n+A [Transformer](https://arxiv.org/abs/1706.03762) architecture consists of **multi-head\n+self attention** layers and **fully-connected feed forward** networks (MLP) as the main\n+components. Both these components are _permutation invariant_: they're not aware of\n+feature order.\n+\n+To overcome this problem we inject tokens with positional information. The\n+`position_embedding` function adds this positional information to the linearly projected\n+tokens.\n+\"\"\"\n+\n+\n+def position_embedding(\n+    projected_patches, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n+):\n+    # Build the positions.\n+    positions = ops.arange(start=0, stop=num_patches, step=1)\n+\n+    # Encode the positions with an Embedding layer.\n+    encoded_positions = layers.Embedding(\n+        input_dim=num_patches, output_dim=projection_dim\n+    )(positions)\n+\n+    # Add encoded positions to the projected patches.\n+    return projected_patches + encoded_positions\n+\n+\n+\"\"\"\n+## MLP block for Transformer\n+\n+This serves as the Fully Connected Feed Forward block for our Transformer.\n+\"\"\"\n+\n+\n+def mlp(x, dropout_rate, hidden_units):\n+    # Iterate over the hidden units and\n+    # add Dense => Dropout.\n+    for units in hidden_units:\n+        x = layers.Dense(units, activation=ops.gelu)(x)\n+        x = layers.Dropout(dropout_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## TokenLearner module\n+\n+The following figure presents a pictorial overview of the module\n+([source](https://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html)).\n+\n+![TokenLearner module GIF](https://blogger.googleusercontent.com/img/a/AVvXsEiylT3_nmd9-tzTnz3g3Vb4eTn-L5sOwtGJOad6t2we7FsjXSpbLDpuPrlInAhtE5hGCA_PfYTJtrIOKfLYLYGcYXVh1Ksfh_C1ZC-C8gw6GKtvrQesKoMrEA_LU_Gd5srl5-3iZDgJc1iyCELoXtfuIXKJ2ADDHOBaUjhU8lXTVdr2E7bCVaFgVHHkmA=w640-h208)\n+\n+The TokenLearner module takes as input an image-shaped tensor. It then passes it through\n+multiple single-channel convolutional layers extracting different spatial attention maps\n+focusing on different parts of the input. These attention maps are then element-wise\n+multiplied to the input and result is aggregated with pooling. This pooled output can be\n+trated as a summary of the input and has much lesser number of patches (8, for example)\n+than the original one (196, for example).\n+\n+Using multiple convolution layers helps with expressivity. Imposing a form of spatial\n+attention helps retain relevant information from the inputs. Both of these components are\n+crucial to make TokenLearner work, especially when we are significantly reducing the number of patches.\n+\"\"\"\n+\n+\n+def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n+    # Layer normalize the inputs.\n+    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(inputs)  # (B, H, W, C)\n+\n+    # Applying Conv2D => Reshape => Permute\n+    # The reshape and permute is done to help with the next steps of\n+    # multiplication and Global Average Pooling.\n+    attention_maps = keras.Sequential(\n+        [\n+            # 3 layers of conv with gelu activation as suggested\n+            # in the paper.\n+            layers.Conv2D(\n+                filters=number_of_tokens,\n+                kernel_size=(3, 3),\n+                activation=ops.gelu,\n+                padding=\"same\",\n+                use_bias=False,\n+            ),\n+            layers.Conv2D(\n+                filters=number_of_tokens,\n+                kernel_size=(3, 3),\n+                activation=ops.gelu,\n+                padding=\"same\",\n+                use_bias=False,\n+            ),\n+            layers.Conv2D(\n+                filters=number_of_tokens,\n+                kernel_size=(3, 3),\n+                activation=ops.gelu,\n+                padding=\"same\",\n+                use_bias=False,\n+            ),\n+            # This conv layer will generate the attention maps\n+            layers.Conv2D(\n+                filters=number_of_tokens,\n+                kernel_size=(3, 3),\n+                activation=\"sigmoid\",  # Note sigmoid for [0, 1] output\n+                padding=\"same\",\n+                use_bias=False,\n+            ),\n+            # Reshape and Permute\n+            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n+            layers.Permute((2, 1)),\n+        ]\n+    )(\n+        x\n+    )  # (B, num_of_tokens, H*W)\n+\n+    # Reshape the input to align it with the output of the conv block.\n+    num_filters = inputs.shape[-1]\n+    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n+\n+    # Element-Wise multiplication of the attention maps and the inputs\n+    attended_inputs = (\n+        ops.expand_dims(attention_maps, axis=-1) * inputs\n+    )  # (B, num_tokens, H*W, C)\n+\n+    # Global average pooling the element wise multiplication result.\n+    outputs = ops.mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n+    return outputs\n+\n+\n+\"\"\"\n+## Transformer block\n+\"\"\"\n+\n+\n+def transformer(encoded_patches):\n+    # Layer normalization 1.\n+    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n+\n+    # Multi Head Self Attention layer 1.\n+    attention_output = layers.MultiHeadAttention(\n+        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n+    )(x1, x1)\n+\n+    # Skip connection 1.\n+    x2 = layers.Add()([attention_output, encoded_patches])\n+\n+    # Layer normalization 2.\n+    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n+\n+    # MLP layer 1.\n+    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)\n+\n+    # Skip connection 2.\n+    encoded_patches = layers.Add()([x4, x2])\n+    return encoded_patches\n+\n+\n+\"\"\"\n+## ViT model with the TokenLearner module\n+\"\"\"\n+\n+\n+def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS):\n+    inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n+\n+    # Augment data.\n+    augmented = data_augmentation(inputs)\n+\n+    # Create patches and project the pathces.\n+    projected_patches = layers.Conv2D(\n+        filters=PROJECTION_DIM,\n+        kernel_size=(PATCH_SIZE, PATCH_SIZE),\n+        strides=(PATCH_SIZE, PATCH_SIZE),\n+        padding=\"VALID\",\n+    )(augmented)\n+    _, h, w, c = projected_patches.shape\n+    projected_patches = layers.Reshape((h * w, c))(\n+        projected_patches\n+    )  # (B, number_patches, projection_dim)\n+\n+    # Add positional embeddings to the projected patches.\n+    encoded_patches = position_embedding(\n+        projected_patches\n+    )  # (B, number_patches, projection_dim)\n+    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n+\n+    # Iterate over the number of layers and stack up blocks of\n+    # Transformer.\n+    for i in range(NUM_LAYERS):\n+        # Add a Transformer block.\n+        encoded_patches = transformer(encoded_patches)\n+\n+        # Add TokenLearner layer in the middle of the\n+        # architecture. The paper suggests that anywhere\n+        # between 1/2 or 3/4 will work well.\n+        if use_token_learner and i == NUM_LAYERS // 2:\n+            _, hh, c = encoded_patches.shape\n+            h = int(math.sqrt(hh))\n+            encoded_patches = layers.Reshape((h, h, c))(\n+                encoded_patches\n+            )  # (B, h, h, projection_dim)\n+            encoded_patches = token_learner(\n+                encoded_patches, token_learner_units\n+            )  # (B, num_tokens, c)\n+\n+    # Layer normalization and Global average pooling.\n+    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n+    representation = layers.GlobalAvgPool1D()(representation)\n+\n+    # Classify outputs.\n+    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n+\n+    # Create the Keras model.\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+\"\"\"\n+As shown in the [TokenLearner paper](https://openreview.net/forum?id=z-l1kpDXs88), it is\n+almost always advantageous to include the TokenLearner module in the middle of the\n+network.\n+\"\"\"\n+\n+\"\"\"\n+## Training utility\n+\"\"\"\n+\n+\n+def run_experiment(model):\n+    # Initialize the AdamW optimizer.\n+    optimizer = keras.optimizers.AdamW(\n+        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n+    )\n+\n+    # Compile the model with the optimizer, loss function\n+    # and the metrics.\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=\"sparse_categorical_crossentropy\",\n+        metrics=[\n+            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n+            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+        ],\n+    )\n+\n+    # Define callbacks\n+    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_accuracy\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    # Train the model.\n+    _ = model.fit(\n+        train_ds,\n+        epochs=EPOCHS,\n+        validation_data=val_ds,\n+        callbacks=[checkpoint_callback],\n+    )\n+\n+    model.load_weights(checkpoint_filepath)\n+    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+\n+\"\"\"\n+## Train and evaluate a ViT with TokenLearner\n+\"\"\"\n+\n+vit_token_learner = create_vit_classifier()\n+run_experiment(vit_token_learner)\n+\n+\"\"\"\n+## Results\n+\n+We experimented with and without the TokenLearner inside the mini ViT we implemented\n+(with the same hyperparameters presented in this example). Here are our results:\n+\n+| **TokenLearner** | **# tokens in<br> TokenLearner** | **Top-1 Acc<br>(Averaged across 5 runs)** | **GFLOPs** | **TensorBoard** |\n+|:---:|:---:|:---:|:---:|:---:|\n+| N | - | 56.112% | 0.0184 | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n+| Y | 8 | **56.55%** | **0.0153** | [Link](https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/) |\n+| N | - | 56.37% | 0.0184 | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n+| Y | 4 | **56.4980%** | **0.0147** | [Link](https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/) |\n+| N | - (# Transformer layers: 8) | 55.36% | 0.0359 | [Link](https://tensorboard.dev/experiment/sepBK5zNSaOtdCeEG6SV9w/) |\n+\n+TokenLearner is able to consistently outperform our mini ViT without the module. It is\n+also interesting to notice that it was also able to outperform a deeper version of our\n+mini ViT (with 8 layers). The authors also report similar observations in the paper and\n+they attribute this to the adaptiveness of TokenLearner.\n+\n+One should also note that the FLOPs count **decreases** considerably with the addition of\n+the TokenLearner module. With less FLOPs count the TokenLearner module is able to\n+deliver better results. This aligns very well with the authors' findings.\n+\n+Additionally, the authors [introduced](https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py#L104)\n+a newer version of the TokenLearner for smaller training data regimes. Quoting the authors:\n+\n+> Instead of using 4 conv. layers with small channels to implement spatial attention,\n+  this version uses 2 grouped conv. layers with more channels. It also uses softmax\n+  instead of sigmoid. We confirmed that this version works better when having limited\n+  training data, such as training with ImageNet1K from scratch.\n+\n+We experimented with this module and in the following table we summarize the results:\n+\n+| **# Groups** | **# Tokens** | **Top-1 Acc** | **GFLOPs** | **TensorBoard** |\n+|:---:|:---:|:---:|:---:|:---:|\n+| 4 | 4 | 54.638% | 0.0149 | [Link](https://tensorboard.dev/experiment/KmfkGqAGQjikEw85phySmw/) |\n+| 8 | 8 | 54.898% | 0.0146 | [Link](https://tensorboard.dev/experiment/0PpgYOq9RFWV9njX6NJQ2w/) |\n+| 4 | 8 | 55.196% | 0.0149 | [Link](https://tensorboard.dev/experiment/WUkrHbZASdu3zrfmY4ETZg/) |\n+\n+Please note that we used the same hyperparameters presented in this example. Our\n+implementation is available\n+[in this notebook](https://github.com/ariG23498/TokenLearner/blob/master/TokenLearner-V1.1.ipynb).\n+We acknowledge that the results with this new TokenLearner module are slightly off\n+than expected and this might mitigate with hyperparameter tuning.\n+\n+*Note*: To compute the FLOPs of our models we used\n+[this utility](https://github.com/AdityaKane2001/regnety/blob/main/regnety/utils/model_utils.py#L27)\n+from [this repository](https://github.com/AdityaKane2001/regnety).\n+\"\"\"\n+\n+\"\"\"\n+## Number of parameters\n+\n+You may have noticed that adding the TokenLearner module increases the number of\n+parameters of the base network. But that does not mean it is less efficient as shown by\n+[Dehghani et al.](https://arxiv.org/abs/2110.12894). Similar findings were reported\n+by [Bello et al.](https://arxiv.org/abs/2103.07579) as well. The TokenLearner module\n+helps reducing the FLOPS in the overall network thereby helping to reduce the memory\n+footprint.\n+\"\"\"\n+\n+\"\"\"\n+## Final notes\n+\n+* TokenFuser: The authors of the paper also propose another module named TokenFuser. This\n+module helps in remapping the representation of the TokenLearner output back to its\n+original spatial resolution. To reuse the TokenLearner in the ViT architecture, the\n+TokenFuser is a must. We first learn the tokens from the TokenLearner, build a\n+representation of the tokens from a Transformer layer and then remap the representation\n+into the original spatial resolution, so that it can again be consumed by a TokenLearner.\n+Note here that you can only use the TokenLearner module once in entire ViT model if not\n+paired with the TokenFuser.\n+* Use of these modules for video: The authors also suggest that TokenFuser goes really\n+well with Vision Transformers for Videos ([Arnab et al.](https://arxiv.org/abs/2103.15691)).\n+\n+We are grateful to [JarvisLabs](https://jarvislabs.ai/) and\n+[Google Developers Experts](https://developers.google.com/programs/experts/)\n+program for helping with GPU credits. Also, we are thankful to Michael Ryoo (first\n+author of TokenLearner) for fruitful discussions.\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/🤗%20Model-TokenLearner-black.svg)](https://huggingface.co/keras-io/learning_to_tokenize_in_ViT) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-TokenLearner-black.svg)](https://huggingface.co/spaces/keras-io/token_learner) |\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
