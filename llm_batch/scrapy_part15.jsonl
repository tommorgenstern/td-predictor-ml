{"custom_id": "scrapy#ac4688dabbd41ce952f06446b325ad5fbe651046", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -42,3 +42,6 @@ class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n     pass\n \n+class NotSupported(Exception):\n+    \"\"\"Indicates a feature or method is not supported\"\"\"\n+    pass\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#acebe63d0a243628d1776f703f9de864bb586a55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 195 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,6 +84,9 @@ class XPathSelector(object):\n         \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n \n+    def __nonzero__(self):\n+        return bool(self.extract())\n+\n     def __str__(self):\n         return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8dc46c16ceefa1c70108105674a963486b688c7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 101 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -8,12 +8,6 @@ from scrapy.core.exceptions import UsageError\n \n def _valid_start_urls(obj):\n     \"\"\"Check the start urls specified are valid\"\"\"\n-    # TODO: we should add proper validation here\n-    \n-    ## This commented if is a test i was doing, it also takes in account the spider's base class (if any)\n-    ## it works, but the attribute is checked somewhere else (zope?) and causes to fail\n-    ##if not (obj.start_urls or any([hasattr(base, 'start_urls') for base in list(obj.__class__.__bases__)])):\n-\n     if not obj.start_urls:\n         raise UsageError(\"A start url is required\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24df5d16029d12b5a292c0d69b48682c4b0aac3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 53 | Files Changed: 6 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 105 | Churn Cumulative: 258 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,15 @@\n #!/usr/bin/env python\n-\"\"\"Scrapy admin script\"\"\"\n+\"\"\"Scrapy admin script is used to create new scrapy projects and similar\n+tasks\"\"\"\n \n-import os, stat\n+import os\n+import shutil\n from optparse import OptionParser\n \n import scrapy\n \n usage = \"\"\"\n-Usage: scrapy-admin.py [options] [command]\n+scrapy-admin.py [options] [command]\n  \n Available commands:\n      \n@@ -19,33 +21,19 @@ def main():\n     parser = OptionParser(usage=usage)\n     opts, args = parser.parse_args()\n     \n-    if args:\n-        if args[0] == \"startproject\":\n+    if not args:\n+        parser.print_help()\n \n-            try:\n+    cmd = args[0]\n+    if cmd == \"startproject\":\n+        if len(args) >= 2:\n             project_name = args[1]\n-                \n-            except IndexError:\n-                parser.print_help()\n+            project_tplpath = os.path.join(scrapy.__path__[0], \"conf\", \"project_template\")\n+            shutil.copytree(project_tplpath, project_name)\n         else:\n-                os.mkdir(project_name)\n-                os.mknod(os.path.join(project_name, \"__init__.py\"))\n-                \n-                for subdir in [\"spiders\", \"conf\", \"commands\", \"templates\"]:\n-                    os.mkdir(os.path.join(project_name, subdir))\n-                    os.mknod(os.path.join(project_name, subdir, \"__init__.py\"))\n-                \n-                settings_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"settings.tmpl\"), \"r\").read()\n-                settings = settings_template.replace(\"__project_name__\", project_name)\n-                open(os.path.join(project_name, \"conf\", \"scrapy_settings.py\"), \"w\").write(settings)\n-                \n-                control_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"scrapy-ctl.tmpl\"), \"r\").read()\n-                control = control_template.replace(\"__project_name__\", project_name)\n-                open(os.path.join(project_name, \"scrapy-ctl.py\"), \"w\").write(control)\n-                os.chmod(os.path.join(project_name, \"scrapy-ctl.py\"), stat.S_IRWXU)\n-                \n-            return\n-    parser.print_help()\n+            print \"scrapy-admin.py: missing project name\"\n+    else:\n+        print \"scrapy-admin.py: unknown command: %s\" % cmd\n \n if __name__ == '__main__':\n     main()\n\n\n@@ -0,0 +1,6 @@\n+# Define here the models for your scraped items\n+\n+from scrapy.item import ScrapedItem\n+\n+class MyItem(ScrapedItem):\n+    pass\n\n@@ -0,0 +1,4 @@\n+#!/usr/bin/env python\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n@@ -1,20 +1,20 @@\n-import __project_name__\n+import $project_name\n \n # ---------------------------------------------------------------------------\n-# - Scrapy settings for __project_name__                                    -\n+# - Scrapy settings for $project_name                                    -\n # ---------------------------------------------------------------------------\n \n-BOT_NAME = 'scrapy-bot'\n+BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n DEFAULT_DATA_ENCODING = 'utf-8'\n \n-COMMANDS_MODULE = '__project_name__.commands'\n-COMMANDS_SETTINGS_MODULE = '__project_name__.conf.commands'\n-SPIDER_MODULES = ['__project_name__.spiders']\n-NEWSPIDER_MODULE = '__project_name__.spiders'\n-TEMPLATES_DIR = '%s/templates' % __project_name__.__path__[0]\n-ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % __project_name__.__path__[0]\n+COMMANDS_MODULE = '$project_name.commands'\n+COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'\n+SPIDER_MODULES = ['$project_name.spiders']\n+NEWSPIDER_MODULE = '$project_name.spiders'\n+TEMPLATES_DIR = '%s/templates' % $project_name.__path__[0]\n+ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % $project_name.__path__[0]\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n DOWNLOAD_TIMEOUT = 600\n@@ -44,28 +44,28 @@ EXTENSIONS = (\n \n DOWNLOADER_MIDDLEWARES = (\n     # Engine side\n-    #'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n-    #'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n-    #'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n )\n \n SPIDER_MIDDLEWARES = (\n     # Engine side\n-    #'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n-    #'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n-    #'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n-    #'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n-    #'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n-    #'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n-    #'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n     # Spider side\n )\n \n\n@@ -0,0 +1 @@\n+# Place here all your scrapy spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b2317d9358cbe5c35ee78dc56f2a0aad9ac7ece", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,15 @@\n+\"\"\"\n+This module provides some useful functions for working with\n+scrapy.http.Response objects\n+\"\"\"\n+\n+from scrapy.xpath import XPathSelector\n+\n+def new_response_from_xpaths(response, xpaths):\n+    \"\"\"Return a new response constructed by applying the given xpaths to the\n+    original response body\n+    \"\"\"\n+    xs = XPathSelector(response)\n+    parts = [''.join([n for n in xs.x(x).extract()]) for x in xpaths]\n+    new_body = ''.join(parts)\n+    return response.replace(body=new_body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c68c478ac39b67930758dc4c1d23cae362b8675a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 60 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,60 @@\n+\"\"\"\n+This module contains BasicSpider, a spider class which provides support for\n+basic crawling.\n+\"\"\"\n+\n+from scrapy.http import Request\n+from scrapy.spider import BaseSpider\n+from scrapy.core.exceptions import UsageError\n+\n+class BasicSpider(BaseSpider):\n+    \"\"\"BasicSpider extends BaseSpider by providing support for simple crawling\n+    by following links contained in web pages. \n+    \n+    With BasicSpider you can write a basic spider very easily and quickly. For\n+    more information refer to the Scrapy tutorial\"\"\"\n+\n+    def __init__(self):\n+        super(BaseSpider, self).__init__()\n+        \n+        self._links_callback = []\n+\n+        for attr in dir(self):\n+            if attr.startswith('links_'):\n+                suffix = attr.split('_', 1)[1]\n+                value = getattr(self, attr)\n+                try:\n+                    callback = getattr(self, 'parse_%s' % suffix)\n+                except AttributeError:\n+                    raise UsageError(\"%s defines links_%s but doesn't provide a parse_%s method\" % \\\n+                                     (type(self).__name__, suffix, suffix))\n+                self._links_callback.append((value, callback))\n+\n+    def parse(self, response):\n+        \"\"\"This function is called by the core for all the start_urls. Do not\n+        override this function, override parse_start_url instead.\"\"\"\n+        return self._parse_wrapper(response, self.parse_start_url)\n+\n+    def parse_start_url(self, response):\n+        \"\"\"Callback function for processing start_urls. It must return a list\n+        of ScrapedItems and/or Requests.\"\"\"\n+        return []\n+\n+    def _links_to_follow(self, response):\n+        res = []\n+        links_to_follow = {}\n+        for lx, callback in self._links_callback:\n+            for url, link_text in lx.extract_urls(response).iteritems():\n+                links_to_follow[url] = (callback, link_text)\n+\n+        for url, cb_link in links_to_follow.iteritems():\n+            request = Request(url=url, link_text=link_text)\n+            request.append_callback(self._parse_wrapper, callback)\n+            res.append(request)\n+        return res\n+\n+    def _parse_wrapper(self, response, callback):\n+        res = self._links_to_follow(response)\n+        res += callback(response) or ()\n+        return res\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f80603acd342c8775599fe53c1aed6605efc621", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 76 | Churn Cumulative: 76 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.08695652173913043\n\nDIFF:\n@@ -0,0 +1,76 @@\n+\"\"\"\n+This module provides some LinkExtractors, which extend that base LinkExtractor\n+(scrapy.link.LinkExtractor) with some useful features.\n+\n+\"\"\"\n+\n+import re\n+\n+from scrapy.link import LinkExtractor\n+from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n+from scrapy.utils.response import new_response_from_xpaths\n+from scrapy.utils.misc import dict_updatedefault\n+\n+_re_type = type(re.compile(\"\", 0))\n+\n+_matches = lambda url, regexs: any((r.search(url) for r in regexs))\n+_is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'file'])\n+\n+class RegexLinkExtractor(LinkExtractor):\n+    \"\"\"RegexLinkExtractor implements extends the base LinkExtractor by\n+    providing several mechanisms to extract the links.\n+\n+    It's constructor parameters are:\n+\n+    allow - list of regexes that the (absolute urls) must match to be extracted\n+    deny - ignore urls that match any of these regexes\n+    allow_domains - only extract urls from these domains\n+    deny_domains - ignore urls from these dmoains\n+    tags - look for urls in this tags\n+    attrs - look for urls in this attrs\n+    canonicalize - canonicalize all extracted urls using scrapy.utils.url.canonicalize_url\n+\n+    Both 'allow' and 'deny' arguments can be a list of regexes strings or regex\n+    python objects (already compiled)\n+\n+    Url matching is always performed against the absolute urls, never the\n+    relative urls found in pages.\n+\n+    \"\"\"\n+    \n+    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n+                 tags=('a', 'area'), attrs=('href'), canonicalize=True):\n+        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n+        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n+        self.allow_domains = set(allow_domains)\n+        self.deny_domains = set(deny_domains)\n+        self.restrict_xpaths = restrict_xpaths\n+        self.canonicalize = canonicalize\n+        tag_func = lambda x: x in tags\n+        attr_func = lambda x: x in attrs\n+        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n+\n+    def extract_urls(self, response):\n+        if self.restrict_xpaths:\n+            response = new_response_from_xpaths(response, self.restrict_xpaths)\n+\n+        url_text = LinkExtractor.extract_urls(self, response)\n+        urls = [u for u in url_text.iterkeys() if _is_valid_url(u)]\n+\n+        if self.allow_res:\n+            urls = [u for u in urls if _matches(u, self.allow_res)]\n+        if self.deny_res:\n+            urls = [u for u in urls if not _matches(u, self.deny_res)]\n+        if self.allow_domains:\n+            urls = [u for u in urls if url_is_from_any_domain(u, self.allow_domains)]\n+        if self.deny_domains:\n+            urls = [u for u in urls if not url_is_from_any_domain(u, self.deny_domains)]\n+\n+        res = {}\n+        if self.canonicalize:\n+            for u in urls:\n+                res[canonicalize_url(u)] = url_text[u]\n+        else:\n+            for u in urls:\n+                res[u] = url_text[u]\n+        return res\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c74364809e1958eabb20546070dc17fcf1be947d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,13 +27,6 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual(attrs, \n                          [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n \n-    def test_iterator_utf16(self):\n-        samplefile = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"sample_data\", \"feeds\", \"feed-sample3.xml\")\n-        body = open(samplefile).read()\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", headers={\"Content-type\": \"text/xml; encoding=UTF-16\"}, body=body)\n-        self.assertEqual([x.x(\"@id\").extract() for x in xpathselector_iternodes(response, 'product')],\n-                         [['34017532'], ['34017557'], ['34017563'], ['34018057'], ['34018313'], ['34018599']])\n-\n     def test_iterator_text(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c19316d21bb836485940fbdc8e9646eec7547b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 688 | Contributors (this commit): 2 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,7 @@\n-from __future__ import with_statement\n-\n import re\n import os\n import time\n-import sha\n+import hashlib\n import urllib\n import urlparse\n from cStringIO import StringIO\n@@ -12,7 +10,6 @@ import Image\n \n from scrapy import log\n from scrapy.stats import stats\n-from scrapy.http import Request\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.core.exceptions import HttpException\n from scrapy.conf import settings\n@@ -96,6 +93,7 @@ class ImagesPipeline(MediaPipeline):\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n         except Exception, ex:\n+            referer = request.headers.get('Referer')\n             msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' % (mtype, request, referer, ex)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n@@ -143,7 +141,7 @@ def image_path(url):\n     if os.sep != '/':\n         urlpath.replace('/', os.sep)\n     if query:\n-        img_path = os.path.join(netloc, sha.sha(url).hexdigest())\n+        img_path = os.path.join(netloc, hashlib.sha1(url).hexdigest())\n     else:\n         img_path = os.path.join(netloc, urlpath[1:])\n     return urllib.unquote(img_path)\n\n@@ -3,10 +3,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.http import Request\n from scrapy.spider import spiders\n-from scrapy.conf import settings\n \n \n class DomainInfo(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6cbaab65b871751d793b3445dbd4cd377eca89b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 133 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,12 +6,12 @@ class ScrapedItem(object):\n     'guid' attribute is required, and is an attribute\n     that identifies uniquely the given scraped item.\n     \"\"\"\n-    adaptors_pipe = AdaptorPipe()\n \n-    def __init__(self, adaptors_pipe=None):\n-        \"\"\"If an adaptors_pipe is given, overrides class adaptors_pipe\"\"\"\n-        if adaptors_pipe:\n-            self.adaptors_pipe = adaptors_pipe\n+    def setadaptors(self, adaptors_dict):\n+        \"\"\"Set adaptors to use for this item. Receives a dict of adaptors and\n+        returns the item itself\"\"\"\n+        self.adaptors_dict = AdaptorPipe(adaptors_dict)\n+        return self\n \n     def attribute(self, attrname, value):\n         value = self.adaptors_pipe.execute(attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88597e3a772b57389bd2c706d9469036daacaf25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,10 +7,6 @@ import $project_name\n BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n-DEFAULT_DATA_ENCODING = 'utf-8'\n-\n-COMMANDS_MODULE = '$project_name.commands'\n-COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'\n SPIDER_MODULES = ['$project_name.spiders']\n NEWSPIDER_MODULE = '$project_name.spiders'\n TEMPLATES_DIR = '%s/templates' % $project_name.__path__[0]\n@@ -19,6 +15,10 @@ DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n DOWNLOAD_TIMEOUT = 600\n \n+# uncomment if you want to add your own custom scrapy commands\n+#COMMANDS_MODULE = '$project_name.commands'\n+#COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'\n+\n #Global timeout between sucessive downloads (can be overrided by spider\n #attribute download_timeout\n #DOWNLOAD_TIMEOUT = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a309dfeb7d367de0eedae095989e3e26ba528ee0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 43 | Churn Cumulative: 544 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -164,24 +164,12 @@ class Command(ScrapyCommand):\n         return c, s\n \n     def _item_diff(self, old, new):\n-        olddic = old.__dict__\n-        newdic = new.__dict__\n-        ignored_attrs = set(self.opts.ignores or [])\n-        allattrs = [a for a in olddic.keys() + newdic.keys() if not a.startswith('_') and a not in ignored_attrs]\n-\n-        diff = {}\n-        for attr in sorted(allattrs):\n-            oldval = olddic.get(attr, None)\n-            newval = newdic.get(attr, None)\n-            if oldval != newval:\n-                diff[attr] = {}\n-                diff[attr]['old'] = oldval\n-                diff[attr]['new'] = newval\n+        delta = new - old\n \n         s = \"\"\n-        if diff:\n+        if delta.diff:\n             s += \">>> Item guid=%s name=%s\\n\" % (old.guid, old.name)\n-            s += display.pformat(diff) + \"\\n\"\n+            s += display.pformat(delta.diff) + \"\\n\"\n         return s\n \n     def _format_items(self, items):\n\n@@ -96,6 +96,7 @@ class RobustScrapedItem(ScrapedItem):\n             raise AttributeError(\"Attribute '%s' doesn't exist\" % attr)\n     \n     def __eq__(self, other):\n+        if other:\n             return self.version == other.version\n         \n     def __ne__(self, other):\n\n@@ -17,3 +17,27 @@ class ScrapedItem(object):\n         value = self.adaptors_pipe.execute(attrname, value)\n         if not hasattr(self, attrname):\n             setattr(self, attrname, value)\n+\n+\n+class ItemDelta(object):\n+    \"\"\"\n+    This class represents the difference between\n+    a pair of items.\n+    \"\"\"\n+\n+    def __init__(self, old, new):\n+        self.diff = self.do_diff()\n+\n+    def do_diff(self):\n+        \"\"\"\n+        This method should retreive a dictionary\n+        containing the changes between both items\n+        as in this example:\n+        \n+        >>> delta.do_diff()\n+        >>> {'attrib': {'new': 'New value', 'old': 'Old value'}, # Common attributes\n+             'attrib2': {'new': 'New value 2', 'old': 'Old value 2'},\n+             'attrib3': [{'new': 'New list value', 'old': 'Old list value'}, # List attributes\n+                         {'new': 'New list value 2', 'old': 'Old list value 2'}]}\n+        \"\"\"\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05f4a26ccad042b7842d4a70cf613dee0f46efa4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 238 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,7 @@ def replace_tags(text, token=''):\n \n def remove_comments(text):\n     \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', '', text.decode('utf-8'), re.DOTALL)\n+    return re.sub('<!--.*?-->', u'', text.decode('utf-8'), re.DOTALL)\n       \n def remove_tags(text, which_ones=()):\n     \"\"\" Remove HTML Tags only. \n@@ -77,7 +77,7 @@ def remove_tags(text, which_ones=()):\n     else:\n         reg_exp_remove_tags = '<.*?>'\n     re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub('', text.decode('utf-8'))\n+    return re_tags.sub(u'', text.decode('utf-8'))\n \n def remove_tags_with_content(text, which_ones=()):\n     \"\"\" Remove tags and its content.\n@@ -87,7 +87,7 @@ def remove_tags_with_content(text, which_ones=()):\n     \"\"\"\n     tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n     re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub('', text.decode('utf-8'))\n+    return re_tags_remove.sub(u'', text.decode('utf-8'))\n \n def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n@@ -96,5 +96,5 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub('', text.decode('utf-8'))\n+    return re_escape_chars.sub(u'', text.decode('utf-8'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc782d5e4bc18eec4188bba8a8266f95fa5bbee5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 24 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 19/21 | Churn Δ: 93 | Churn Cumulative: 396 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 6 | DMM Complexity: 0.5882352941176471\n\nDIFF:\n@@ -1 +1 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, ValidationError, ValidationPipeline\n+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n\n@@ -5,6 +5,7 @@ useful in some Scrapy implementations\n \n import hashlib\n \n+from pprint import PrettyPrinter\n from scrapy.item import ScrapedItem\n from scrapy.core.exceptions import UsageError, DropItem\n \n@@ -96,12 +97,15 @@ class RobustScrapedItem(ScrapedItem):\n             raise AttributeError(\"Attribute '%s' doesn't exist\" % attr)\n     \n     def __eq__(self, other):\n-        if other:\n+        if isinstance(other, type(self)):\n             return self.version == other.version\n         \n     def __ne__(self, other):\n         return self.version != other.version\n     \n+    def __sub__(self, other):\n+        return RobustItemDelta(other, self)\n+\n     def __repr__(self):\n         # Generate this format so that it can be deserialized easily:\n         # ClassName({...})\n@@ -139,3 +143,66 @@ class RobustScrapedItem(ScrapedItem):\n         hash_ = hashlib.sha1()\n         hash_.update(\"\".join([\"\".join([n, str(v)]) for n,v in sorted(self.__dict__.iteritems())]))\n         return hash_.hexdigest()\n+\n+\n+class RobustItemDelta(object):\n+    \"\"\"\n+    This class represents the difference between\n+    a pair of RobustScrapedItems.\n+    \"\"\"\n+\n+    def __init__(self, old_item, new_item):\n+        if not isinstance(old_item, RobustScrapedItem) or \\\n+           not isinstance(new_item, RobustScrapedItem):\n+            raise TypeError(\"Both arguments must be RobustScrapedItem instances\")\n+\n+        if old_item.guid != new_item.guid:\n+            raise AttributeError(\"Item GUIDs must be equal in order to create a RobustItemDelta object\")\n+\n+        self.old_item = old_item\n+        self.new_item = new_item\n+        self.diff = self.do_diff()\n+\n+    def do_diff(self):\n+        \"\"\"\n+        This method should retreive a dictionary\n+        containing the changes between both items\n+        as in this example:\n+\n+        >>> delta.do_diff()\n+        >>> {'attrib': {'new': 'New value', 'old': 'Old value'}, # Common attributes\n+             'attrib2': {'new': 'New value 2', 'old': 'Old value 2'},\n+             'attrib3': [{'new': 'New list value', 'old': 'Old list value'}, # List attributes\n+                         {'new': 'New list value 2', 'old': 'Old list value 2'}]}\n+        \"\"\"\n+        \n+        if self.old_item == self.new_item:\n+            return {}\n+\n+        diff = {}\n+        for key, value in self.old_item.__dict__.items():\n+            if key in self.old_item.ATTRIBUTES.keys():\n+                new_value = getattr(self.new_item, key)\n+                if value != new_value:\n+                    diff[key] = {'new': new_value, 'old': value}\n+        for key, value in self.new_item.__dict__.items():\n+            if value and key in self.new_item.ATTRIBUTES.keys():\n+                if not getattr(self.old_item, key):\n+                    diff[key] = {'new': value, 'old': None}\n+        return diff\n+    \n+    def __eq__(self, other):\n+        if isinstance(other, RobustItemDelta):\n+            if other.old_item == self.old_item and \\\n+               other.new_item == self.new_item and \\\n+               other.diff == self.diff:\n+                return True\n+        return False\n+\n+    def __repr__(self):\n+        if self.diff:\n+            pp = PrettyPrinter(indent=3)\n+            return pp.pformat(self.diff)\n+        else:\n+            return 'No differences found between the provided items.'\n+\n\n@@ -19,25 +19,3 @@ class ScrapedItem(object):\n             setattr(self, attrname, value)\n \n                 \n-class ItemDelta(object):\n-    \"\"\"\n-    This class represents the difference between\n-    a pair of items.\n-    \"\"\"\n-\n-    def __init__(self, old, new):\n-        self.diff = self.do_diff()\n-\n-    def do_diff(self):\n-        \"\"\"\n-        This method should retreive a dictionary\n-        containing the changes between both items\n-        as in this example:\n-        \n-        >>> delta.do_diff()\n-        >>> {'attrib': {'new': 'New value', 'old': 'Old value'}, # Common attributes\n-             'attrib2': {'new': 'New value 2', 'old': 'Old value 2'},\n-             'attrib3': [{'new': 'New list value', 'old': 'Old list value'}, # List attributes\n-                         {'new': 'New list value 2', 'old': 'Old list value 2'}]}\n-        \"\"\"\n-        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ae4a19e08438d4787651df43c6c3a4ca45b25d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 81 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,13 +4,14 @@ from time import sleep\n from twisted.trial import unittest\n \n \n-# This is the DB used for testing\n-TEST_DB = \"mysql://root:r00tpass@localhost/scrapingtest\"\n-\n class ProductComparisonTestCase(unittest.TestCase):\n     \"\"\" Test product comparison functions \"\"\"\n     \n     def setUp(self):\n+        TEST_DB = settings.get('TEST_DB')\n+        if not TEST_DB:\n+            raise unittest.SkipTest(\"TEST_DB not configured\")\n+\n         try:\n             import MySQLdb\n             from scrapy.utils.db import mysql_connect\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#498cd3a3568b934d0e2caae5ebf642c82df299d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@ from datetime import datetime, timedelta\n from time import sleep\n \n from twisted.trial import unittest\n+from scrapy.conf import settings\n \n \n class ProductComparisonTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9f39133285f7ff8e8b937c56fc2930fd70b0026", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 94 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,24 +9,24 @@ class ProductComparisonTestCase(unittest.TestCase):\n     \"\"\" Test product comparison functions \"\"\"\n     \n     def setUp(self):\n-        TEST_DB = settings.get('TEST_DB')\n-        if not TEST_DB:\n-            raise unittest.SkipTest(\"TEST_DB not configured\")\n+        self.test_db = settings.get('TEST_SCRAPING_DB')\n+        if not self.test_db:\n+            raise unittest.SkipTest(\"Missing TEST_SCRAPING_DB setting\")\n \n         try:\n             import MySQLdb\n             from scrapy.utils.db import mysql_connect\n-            mysql_connect(TEST_DB)\n+            mysql_connect(self.test_db)\n         except ImportError:\n             raise unittest.SkipTest(\"MySQLdb module not available\")\n         except MySQLdb.OperationalError:\n-            raise unittest.SkipTest(\"Test database not available at: %s\" % TEST_DB)\n+            raise unittest.SkipTest(\"Test database not available at: %s\" % self.test_db)\n \n     def test_domaindatahistory(self):\n         from scrapy.store.db import DomainDataHistory\n         from time import sleep\n \n-        ddh = DomainDataHistory(TEST_DB, 'domain_data_history')\n+        ddh = DomainDataHistory(self.test_db, 'domain_data_history')\n         c = ddh.mysql_conn.cursor()\n         c.execute(\"DELETE FROM domain_data_history\")\n         ddh.mysql_conn.commit()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6f73c3dfa0397a426580ba3fe819fa3266136f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 382 | Files Changed: 8 | Hunks: 8 | Methods Changed: 30 | Complexity Δ (Sum/Max): -71/0 | Churn Δ: 382 | Churn Cumulative: 816 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 21 | DMM Complexity: 0.10945273631840796\n\nDIFF:\n@@ -1,91 +0,0 @@\n-\"\"\"\n-SimpageMiddleware is a middleware for detecting similar page layouts\n-\"\"\"\n-\n-import sys\n-import datetime\n-import pprint\n-import pickle\n-\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.http import Response\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-from scrapy.contrib.rulengine.responseWrapper import ResponseWrapper\n-from scrapy.contrib.rulengine.pipeline import RulesPipeline\n-\n-from .metrics import tagdepth, histogram\n-\n-class SimpagesMiddleware(object):\n-    \n-    metric = tagdepth\n-    threshold = 0.8\n-\n-    def __init__(self):\n-        if not settings.getbool('SIMPAGES_ENABLED'):\n-            raise NotConfigured\n-        repfilename = settings.get('SIMPAGES_REPORT_FILE')\n-        self.reportfile = open(repfilename, \"a\") if repfilename else None        \n-        persistence_filename = open(repfilename + '.pickle', 'w') if repfilename else None\n-        self.persistent_simgroup = pickle.Pickler(persistence_filename) if persistence_filename else None\n-        self.sim_groups = {}\n-        self.last_group = 0\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n-        #Rules \n-        RulesPipeline.loadRules()\n-\n-    def process_response(self, request, response, spider):\n-        if isinstance(response, Response):\n-            group, simrate, simhash = self.get_similarity_group(response)\n-            if group:\n-                wres = ResponseWrapper(response)\n-                rp = RulesPipeline(wres)\n-                rules_info = rp.execute()\n-                self.sim_groups[group]['similar_urls'].append((response.url, simrate, simhash, rules_info))\n-            else:\n-                self.create_similarity_group(response)\n-        return response\n-\n-    def get_similarity_group(self, response):\n-        sh = self.metric.simhash(response, symnumbers=True)\n-        for group, data in self.sim_groups.iteritems():\n-            simrate = self.metric.compare(sh, data['simhash'])\n-            if simrate > self.threshold:\n-                return (group, simrate, data['simhash'])\n-        return (None, 0, set())\n-\n-    def create_similarity_group(self, response):\n-        self.last_group += 1\n-        data = {}\n-        data['simhash'] = self.metric.simhash(response, symnumbers=True)\n-        data['first_url'] = response.url\n-        wres = ResponseWrapper(response)\n-        rp = RulesPipeline(wres)\n-        data['rules_info'] = rp.execute()\n-        data['similar_urls'] = []\n-        self.sim_groups[self.last_group] = data\n-\n-    def get_report(self):\n-        data_hist = dict( [(k, len(v['similar_urls'])) for k, v in self.sim_groups.items()] )\n-        r =  \"Page similarity results\\n\"\n-        r += \"=======================\\n\\n\"\n-        r += \"Datetime : %s\\n\" % datetime.datetime.now()\n-        r += \"Metric   : %s\\n\" % self.metric.__name__\n-        r += \"Threshold: %s\\n\" % self.threshold\n-        r += \"Distribution per group:\\n%s\\n\" % histogram.plot(data_hist)\n-        r += \"Results  :\\n\"\n-        r += pprint.pformat(self.sim_groups)\n-        r += \"\\n\\n\"\n-        return r\n-\n-    def engine_stopped(self):\n-        rep = self.get_report()\n-        if self.reportfile:\n-            self.reportfile.write(rep)\n-        else:\n-            print rep\n-        if self.persistent_simgroup:\n-            self.persistent_simgroup.dump(self.sim_groups)\n\n@@ -1,19 +0,0 @@\n-\"\"\"\n-This module contains several metrics that can be used with the\n-SimpagesMiddleware.\n-\n-A metric must implement two functions:\n-\n-1. simhash(response, *args)\n-\n-Receives a response and returns a simhash of that response. A simhash can be\n-an object of any type and its only purpose is to provide a fast way for\n-comparing the [simhashed] response with another responses (that will also be\n-simhashed).\n-\n-2. compare(simhash1, simhash2)\n-\n-Receives two simhashes and must return a (float) value between 0 and 1,\n-depending on how similar the two simhashes (and, thus, the responses they\n-represent) are. 0 means completely different, 1 means identical.\n-\"\"\"\n\n@@ -1,39 +0,0 @@\n-import math\n-\n-def plot(data):\n-    \"\"\"\n-    data is a dict of tuples of the form: {key: quantity, ...}.\n-    Make the histogram of the data dict.\n-    \"\"\"\n-    maxv = max(data.values())\n-    minv = min(data.values())\n-    step = (maxv - minv) * 0.1 if (maxv - minv) != 0 else 1\n-    s = []\n-    for key, q in data.items():\n-        s1 = \"%s%s: \" % (key, blanks(6 - len(str(key))))\n-        for i in xrange(1, int(math.ceil(q/step)+1)):\n-            s1+= \"=\"\n-        if s1[len(s1)-1] == '=':\n-            s1+= \" \" \n-        s1 += str(q)\n-        s.append(s1)\n-    maxl = len(max(s, key=lambda x:len(x)))\n-    s2 = ''\n-    for i in xrange(1,maxl+1):\n-        s2 += '-'\n-\n-    r = \"\\tgroup | quantities\\n\"\n-    r += \"\\t%s\" % s2\n-    for x in s:\n-        r += \"\\n\\t%s\" % x\n-    return r\n-\n-def blanks(n):\n-    return ''.join([' ' for x in range(1,n+1)])\n-\n-def print_plot(data):\n-    print plot(data)\n-\n-\n-\n-\n\n@@ -1,39 +0,0 @@\n-\"\"\"\n-tagdepth metric\n-\n-Compares pages analyzing a predefined set of (relevant)\n-tags and the depth where they appear in the page markup document. \n-\n-Requires ResponseSoup extension enabled.\n-\"\"\"\n-\n-from __future__ import division\n-\n-from BeautifulSoup import Tag\n-\n-relevant_tags = set(['div', 'table', 'td', 'tr', 'h1','p'])\n-\n-def get_symbol_dict(node, tags=(), depth=1):\n-    symdict = {}\n-    for tag in node:\n-        if isinstance(tag, Tag) and tag.name in tags:\n-            symbol = \"%d%s\" % (depth, str(tag.name))\n-            symdict[symbol] = symdict.setdefault(symbol, 0) + 1\n-            symdict.update(get_symbol_dict(tag, tags, depth+1))\n-    return symdict\n-\n-def simhash(response, symnumbers=False):\n-    soup = response.soup\n-    symdict = get_symbol_dict(soup.find('body'), relevant_tags)\n-    if symnumbers:\n-        s = set([k+str(v) for k,v in symdict.items()])\n-    else:\n-        s = set(symdict.keys())\n-    return s\n-\n-def compare(sh1, sh2):\n-    if sh1 == sh2:\n-        return 1.0\n-    else:\n-        return len(sh1 & sh2) / len(sh1 | sh2)\n-\n\n@@ -1,74 +0,0 @@\n-\"\"\"\n-Here put the rules that the rules engine / pipline will take to process it.\n-\"\"\"\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class\n-from scrapy import log \n-from scrapy.conf import settings\n-\n-class Rule(object):\n-    \"\"\"\n-    Interface of the Rules. \n-    Implement this class to create a new rule.\n-    \"\"\"\n-    def __init__(self, wresponse=None):\n-        self.__responsewrapper = wresponse\n-    \n-    def __getresponsewrapper(self):\n-        return self.__responsewrapper\n-    def __setresponsewrapper(self, wresponse):\n-        self.__responsewrapper = wresponse\n-\n-    responsewrapper = property(__getresponsewrapper, __setresponsewrapper)\n-    \n-    def check(self):\n-        result = 0.0\n-        if self.responsewrapper:\n-            result = self.holds()\n-            if result < 0 or result > 1:\n-                raise ValueError, \"Value must be between 0 and 1.\"\n-        return result\n-\n-    def holds(self):\n-        \"\"\"\n-        User of this class must override this method.\n-        Put here the conditions that must be satisfied by the rule.\n-        The return value must be a number between 0.0 and 1.0.\n-        \"\"\"\n-        return 0.0\n-\n-class RulesManager(object):\n-    \"\"\"\n-    This class contains the RulesManager  which takes care of loading and\n-    keeping track of all enabled rules. It also contains an instantiated\n-    RulesManager (rules) to be used as singleton.\n-    The RulesManager contains the rules classes, not instances of the rules \n-    classes, this approach give us more flexiblility in our Rules Engine.\n-    \"\"\"\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-\n-    def load(self):\n-        \"\"\"\n-        Load enabled extensions in settings module\n-        \"\"\"\n-        \n-        self.loaded = False\n-        self.enabled.clear()\n-        \n-        for extension_path in settings.getlist('SIMPAGES_RULES'):\n-            cls = load_class(extension_path)\n-            self.enabled[cls.__name__] = cls\n-                \n-        self.loaded = True\n-    \n-    def reload(self):\n-        self.load()\n-\n-rules = RulesManager()\n-\n-\n-\n-\n\n@@ -1,9 +0,0 @@\n-\"\"\"\n-Contains exceptions of the simpages stuffs\n-\"\"\"\n-\n-class RulesNotLoaded(Exception):\n-    \"\"\"\n-    Indicate that the rules was not loaded.\n-    \"\"\"\n-    pass\n\n@@ -1,52 +0,0 @@\n-\"\"\"\n-Represent the pipepline of execution to the rules engine.\n-\"\"\"\n-from scrapy import log\n-from scrapy.contrib.rulengine.exceptions import RulesNotLoaded\n-\n-from scrapy.contrib.rulengine import rules\n-\n-class RulesPipeline(object):\n-\n-    rulesLoaded = []\n-    loaded = False\n-    \n-    def __init__(self, wresponse):\n-        \"\"\"\n-        wresponse: is a response wrapper object that contain the response.\n-        \"\"\"\n-        self.__rules = []\n-        self._responsewrapper = wresponse\n-    \n-    @staticmethod\n-    def loadRules():\n-        RulesPipeline.loaded = True\n-        rules.load()\n-        try:\n-            for rulename in rules.enabled.keys():\n-                ldr_msg = 'Loading ... %s' % rulename\n-                ruleClass = rules.enabled[rulename]\n-                RulesPipeline.rulesLoaded.append(ruleClass())\n-                log.msg(ldr_msg)\n-                print ldr_msg\n-        except Exception, e:\n-            RulesPipeline.loaded = False\n-            RulesPipeline.rulesLoaded = []\n-            log.msg(e)\n-            print e\n-\n-    def execute(self):\n-        \"\"\"\n-        Return a dictionary that conatins all the rules executed.\n-        \"\"\"\n-        if RulesPipeline.loaded:\n-            rules_loaded = RulesPipeline.rulesLoaded\n-            info_dict = {}\n-            info_dict['rules_executed'] = {}\n-            for rule in rules_loaded:\n-                rule.responsewrapper = self._responsewrapper\n-                rule_result = rule.check()\n-                info_dict['rules_executed'][rule.__class__.__name__] = rule_result\n-            return info_dict\n-        else:\n-            raise RulesNotLoaded, 'Problems loading the rules.'\n\n@@ -1,59 +0,0 @@\n-\"\"\"\n-This class serve as wrapper of the response object. \n-The object created by this class are not response, but are objects that contains a response a a lot of useful information about the response.\n-\"\"\"\n-\n-import re\n-\n-from BeautifulSoup import Tag\n-\n-def extractText(soup):\n-    text_in_tags = soup.findAll(text=True)\n-    res = []\n-    for tag in text_in_tags:\n-        if isinstance(tag.parent, Tag) and tag.parent.name not in('script'):\n-            res.append(tag)\n-    return res\n-\n-class ResponseWrapper(object):\n-    def __init__(self, response):\n-        self._response = response        \n-        self.__soup_bodytext = extractText(response.soup.body) if response else None\n-        self.__soup_headtext = extractText(response.soup.head) if response else None        \n-        self._soup = None\n-        self._bodytext = None\n-        self._headtext = None\n-        self._cleanbodytext = None\n-    \n-    def __cleantext(self, souptext):\n-        return filter(lambda x:re.search('\\w+', x), souptext)\n-\n-    @property\n-    def soup(self):        \n-        if (self._soup):\n-            self._soup = self._response.soup\n-        return self._response.soup\n-\n-    @property\n-    def bodytext(self):\n-        if not self._bodytext:\n-            self._bodytext = self.__cleantext(self.__soup_bodytext)\n-        return self._bodytext\n-\n-    @property\n-    def headtext(self):\n-        if not self._headtext:\n-            self._headtext = self.__cleantext(self.__soup_headtext)\n-        return self._headtext\n-\n-    @property\n-    def cleanbodytext(self):\n-        if not self._cleanbodytext:\n-            text = ' '.join(self.bodytext)\n-            text = text.lower()\n-            text = text.replace('\\n', '')\n-            text = text.replace('\\t', '')\n-            text = re.sub('&.*?;', '', text)\n-            self._cleanbodytext = text\n-        return self._cleanbodytext\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
