{"custom_id": "scrapy#e42120657339adcad483d252905e3446df609e87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 28 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,12 +22,15 @@ class Article(models.Model):\n                                default=False, help_text=MAIN_HELP_TEXT)\n     position = models.IntegerField(_(\"position\"), core=True, blank=False,\n                                    default=0)\n-\n+    publish = models.BooleanField(_(\"publish\"), core=True, default=False)\n \n     # automatic dates\n     created = models.DateTimeField(core=True, editable=False)\n     updated = models.DateTimeField(core=True, editable=False)\n \n+    def toggle_publish(self):\n+        self.publish = not self.publish\n+        self.save()\n \n     def position_up(self):\n         self.position += 1\n@@ -55,9 +58,19 @@ class Article(models.Model):\n     position_link.short_description = u\"position\"\n     position_link.allow_tags = True\n \n+    def publish_link(self):\n+        img_url = \"/media/img/admin/icon-%s.gif\" % \\\n+                  (self.publish and \"yes\" or \"no\")\n+        html = _('<img alt=\"%s\" src=\"' + img_url + '\"/> ' \\\n+                 '(<a href=\"%s/publish/toggle/\">Toggle</a>)')\n+        return html % (_(str(self.publish)), self.id)\n+    publish_link.short_description = u\"publish\"\n+    publish_link.allow_tags = True\n+\n     class Admin:\n-        list_display = (\"title\", \"main\", \"position_link\", \"updated\")\n-        list_filter = (\"main\", \"created\")\n+        list_display = (\"title\", \"main\", \"position_link\", \"publish_link\",\n+                        \"updated\")\n+        list_filter = (\"main\", \"created\", \"publish\")\n \n     class Meta:\n         verbose_name = _(\"article\")\n\n@@ -39,7 +39,7 @@ class LoadArticlesNode(template.Node):\n         self.var_name = var_name\n \n     def render(self, context):\n-        articles = Article.objects.all()\n+        articles = Article.objects.filter(publish=True)\n         if self.only_main:\n             articles = articles.filter(main=True)\n \n\n@@ -6,4 +6,5 @@ from scrapyorg.article.views import *\n urlpatterns = patterns('',\n     (r\"^(?P<article_id>\\d+)/position/up/$\", position_up),\n     (r\"^(?P<article_id>\\d+)/position/down/$\", position_down),\n+    (r\"^(?P<article_id>\\d+)/publish/toggle/$\", publish_toggle),\n )\n\n@@ -16,3 +16,9 @@ def position_down(request, article_id):\n     article = get_object_or_404(Article, pk=article_id)\n     article.position_down()\n     return HttpResponseRedirect(\"/admin/article/article/\")\n+\n+@staff_member_required\n+def publish_toggle(request, article_id):\n+    article = get_object_or_404(Article, pk=article_id)\n+    article.toggle_publish()\n+    return HttpResponseRedirect(\"/admin/article/article/\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22e3d3a02ebb7e84d3948b003fbbf9e76664edb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -71,6 +71,7 @@ class Article(models.Model):\n         list_display = (\"title\", \"main\", \"position_link\", \"publish_link\",\n                         \"updated\")\n         list_filter = (\"main\", \"created\", \"publish\")\n+        save_on_top = True\n \n     class Meta:\n         verbose_name = _(\"article\")\n\n@@ -44,6 +44,7 @@ class DownloadLink(models.Model):\n     class Admin:\n         list_display = (\"text\", \"address\", \"public_link\", \"created\")\n         list_filter = (\"public\", \"created\")\n+        save_on_top = True\n \n     class Meta:\n         verbose_name = _(\"download link\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd7c80ed0a38bf5eacbf531bbbf3fc646c602a4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 54 | Churn Cumulative: 231 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n+from __future__ import with_statement\n+\n import os\n-import shelve\n+import cPickle as pickle\n import shutil\n import tempfile\n import tarfile\n@@ -25,6 +27,9 @@ class Replay(object):\n         usedir=False)\n         \"\"\"\n \n+\tlog.msg(\"beta state! please don't use now\", log.ERROR)\n+\tsys.exit(1)\n+\n         # XXX: this is ugly, and should be removed. but how?\n         cachemw = 'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware'\n         if not cachemw in settings['DOWNLOADER_MIDDLEWARES']:\n@@ -42,7 +47,8 @@ class Replay(object):\n         self.responses_old = {}\n         self.responses_new = {}\n \n-        self._opendb(repfile, usedir)\n+        #self._opendb(repfile, usedir)\n+        self._load(repfile, usedir)\n \n         settings.overrides['CACHE2_DIR'] = self.cache2dir\n         settings.overrides['CACHE2_IGNORE_MISSING'] = self.playing or self.updating\n@@ -84,7 +90,7 @@ class Replay(object):\n         if self.recording or self.updating:\n             log.msg(\"Replay: recorded in %s: %d/%d scraped/passed items, %d downloaded responses\" % \\\n                 (self.repfile, len(self.scraped_old), len(self.passed_old), len(self.responses_old)))\n-        self._closedb()\n+        self._save()\n         self.cleanup()\n \n     def item_scraped(self, item, spider):\n@@ -107,7 +113,7 @@ class Replay(object):\n         elif key:\n             self.responses_new[key] = response.copy()\n \n-    def _opendb(self, repfile, usedir):\n+    def _load(self, repfile, usedir):\n         if usedir:\n             if os.path.isdir(repfile):\n                 replay_dir = repfile\n@@ -140,30 +146,28 @@ class Replay(object):\n         self.responses_path = os.path.join(replay_dir, \"responses.db\")\n \n         if self.updating:\n-            self.options.update(shelve.open(self.options_path))\n-            #self.responses_old.update(shelve.open(self.responses_path))\n+            with open(self.options_path, 'r') as f:\n+                self.options = pickle.load(f)\n         if self.playing:\n-            self.options.update(shelve.open(self.options_path))\n-            self.responses_old.update(shelve.open(self.responses_path))\n-            self.scraped_old.update(shelve.open(self.scraped_path))\n-            self.passed_old.update(shelve.open(self.passed_path))\n+            with open(self.options_path, 'r') as f:\n+                self.options = pickle.load(f)\n+            with open(self.responses_path, 'r') as f:\n+                self.responses_old = pickle.load(f)\n+            with open(self.scraped_path, 'r') as f:\n+                self.scraped_old = pickle.load(f)\n+            with open(self.passed_path, 'r') as f:\n+                self.passed_old = pickle.load(f)\n \n-    def _persistdb(self, dict_, filename):\n-        d = shelve.open(filename)\n-        d.clear()\n-        d.update(dict_)\n-        d.close()\n-        \n-    def _closedb(self):\n+    def _save(self):\n         if self.recording or self.updating:\n-            d = shelve.open(self.options_path)\n-            for k, v in self.options.iteritems():\n-                d[k] = v\n-            d.close()\n-            self._persistdb(self.options, self.options_path)\n-            self._persistdb(self.scraped_old, self.scraped_path)\n-            self._persistdb(self.passed_old, self.passed_path)\n-            self._persistdb(self.responses_old, self.responses_path)\n+            with open(self.options_path, 'w') as f:\n+                pickle.dump(self.options, f)\n+            with open(self.responses_path, 'w') as f:\n+                pickle.dump(self.responses_old, f)\n+            with open(self.scraped_path, 'w') as f:\n+                pickle.dump(self.scraped_old, f)\n+            with open(self.passed_path, 'w') as f:\n+                pickle.dump(self.passed_old, f)\n \n             if not self.usedir:\n                 if self.recording or self.updating:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e18ecd5ce5a4d7896095c8198b9045439a86d26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 50 | Churn Cumulative: 50 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,50 @@\n+#!/usr/bin/env python\n+\"\"\"Scrapy admin script\"\"\"\n+\n+import os, stat\n+from optparse import OptionParser\n+\n+import scrapy\n+\n+usage = \"\"\"\n+Usage: scrapy-admin.py [options] [command]\n+ \n+Available commands:\n+     \n+    startproject <project_name>\n+      Starts a new project with name 'project_name'\n+\"\"\"\n+\n+def main():\n+    parser = OptionParser(usage = usage)\n+    opts, args = parser.parse_args()\n+    \n+    if args:\n+        if args[0] == \"startproject\":\n+            \n+            try:\n+                project_name = args[1]\n+            except IndexError:\n+                parser.print_help()\n+            else:\n+                os.mkdir(project_name)\n+                os.mknod(os.path.join(project_name, \"__init__.py\"))\n+                \n+                for subdir in [\"spiders\", \"conf\", os.path.join(\"conf\", \"sites\"), \"commands\", \"templates\"]:\n+                    os.mkdir(os.path.join(project_name, subdir))\n+                    os.mknod(os.path.join(project_name, subdir, \"__init__.py\"))\n+                \n+                settings_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"settings.tmpl\"), \"r\").read()\n+                settings = settings_template.replace(\"__project_name__\", project_name)\n+                open(os.path.join(project_name, \"conf\", \"scrapy_settings.py\"), \"w\").write(settings)\n+                \n+                control_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"scrapy-ctl.tmpl\"), \"r\").read()\n+                control = control_template.replace(\"__project_name__\", project_name)\n+                open(os.path.join(project_name, \"scrapy-ctl.py\"), \"w\").write(control)\n+                os.chmod(os.path.join(project_name, \"scrapy-ctl.py\"), stat.S_IRWXU)\n+                \n+            return\n+    parser.print_help()\n+\n+if __name__ == '__main__':\n+    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0bb68f34f6d06981730f65690a665b2c1d9846b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 232 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import with_statement\n \n import os\n+import sys\n import cPickle as pickle\n import shutil\n import tempfile\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c97c495d16011fcd8403a5eb2c912b0583d8bcec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 92 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,13 +20,7 @@ class Settings(object):\n         self.core = self._import('scrapy.conf.core_settings')\n \n     def _import(self, modulepath):\n-        try:\n         return __import__(modulepath, {}, {}, [''])\n-        except ImportError:\n-            import sys\n-            err = \"Error: Can't find %s module in your python path\\n\"\n-            sys.stderr.write(err % modulepath)\n-            sys.exit(1)\n \n     def __getitem__(self, opt_name):\n         if opt_name in self.overrides:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67ee6bff2e03d4ce1ea3f49fd30ab57d2dae7073", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 98 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,13 @@ class Settings(object):\n         self.core = self._import('scrapy.conf.core_settings')\n \n     def _import(self, modulepath):\n+        try:\n             return __import__(modulepath, {}, {}, [''])\n+        except ImportError:\n+            import sys\n+            err = \"Error: Can't find %s module in your python path\\n\"\n+            sys.stderr.write(err % modulepath)\n+            sys.exit(1)\n \n     def __getitem__(self, opt_name):\n         if opt_name in self.overrides:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc00f8cce2453b4edb867184370cb55f3cd2556f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 104 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,13 +20,7 @@ class Settings(object):\n         self.core = self._import('scrapy.conf.core_settings')\n \n     def _import(self, modulepath):\n-        try:\n         return __import__(modulepath, {}, {}, [''])\n-        except ImportError:\n-            import sys\n-            err = \"Error: Can't find %s module in your python path\\n\"\n-            sys.stderr.write(err % modulepath)\n-            sys.exit(1)\n \n     def __getitem__(self, opt_name):\n         if opt_name in self.overrides:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbb5860f492546ba2f489c583f67d4484b5ade30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 236 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n from __future__ import with_statement\n \n import os\n-import sys\n import cPickle as pickle\n import shutil\n import tempfile\n@@ -28,9 +27,6 @@ class Replay(object):\n         usedir=False)\n         \"\"\"\n \n-\tlog.msg(\"beta state! please don't use now\", log.ERROR)\n-\tsys.exit(1)\n-\n         # XXX: this is ugly, and should be removed. but how?\n         cachemw = 'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware'\n         if not cachemw in settings['DOWNLOADER_MIDDLEWARES']:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b851ec5f105ce9623f1a939de0b8c48638ef307b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 50 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 50 | Churn Cumulative: 100 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,50 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"Scrapy admin script\"\"\"\n-\n-import os, stat\n-from optparse import OptionParser\n-\n-import scrapy\n-\n-usage = \"\"\"\n-Usage: scrapy-admin.py [options] [command]\n- \n-Available commands:\n-     \n-    startproject <project_name>\n-      Starts a new project with name 'project_name'\n-\"\"\"\n-\n-def main():\n-    parser = OptionParser(usage = usage)\n-    opts, args = parser.parse_args()\n-    \n-    if args:\n-        if args[0] == \"startproject\":\n-            \n-            try:\n-                project_name = args[1]\n-            except IndexError:\n-                parser.print_help()\n-            else:\n-                os.mkdir(project_name)\n-                os.mknod(os.path.join(project_name, \"__init__.py\"))\n-                \n-                for subdir in [\"spiders\", \"conf\", os.path.join(\"conf\", \"sites\"), \"commands\", \"templates\"]:\n-                    os.mkdir(os.path.join(project_name, subdir))\n-                    os.mknod(os.path.join(project_name, subdir, \"__init__.py\"))\n-                \n-                settings_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"settings.tmpl\"), \"r\").read()\n-                settings = settings_template.replace(\"__project_name__\", project_name)\n-                open(os.path.join(project_name, \"conf\", \"scrapy_settings.py\"), \"w\").write(settings)\n-                \n-                control_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"scrapy-ctl.tmpl\"), \"r\").read()\n-                control = control_template.replace(\"__project_name__\", project_name)\n-                open(os.path.join(project_name, \"scrapy-ctl.py\"), \"w\").write(control)\n-                os.chmod(os.path.join(project_name, \"scrapy-ctl.py\"), stat.S_IRWXU)\n-                \n-            return\n-    parser.print_help()\n-\n-if __name__ == '__main__':\n-    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#137ec64318c6f58aaa66a4677f45133c77a930e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 51 | Churn Cumulative: 151 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,51 @@\n+#!/usr/bin/env python\n+\"\"\"Scrapy admin script\"\"\"\n+\n+import os, stat\n+from optparse import OptionParser\n+\n+import scrapy\n+\n+usage = \"\"\"\n+Usage: scrapy-admin.py [options] [command]\n+ \n+Available commands:\n+     \n+    startproject <project_name>\n+      Starts a new project with name 'project_name'\n+\"\"\"\n+\n+def main():\n+    parser = OptionParser(usage = usage)\n+    opts, args = parser.parse_args()\n+    \n+    if args:\n+        if args[0] == \"startproject\":\n+            \n+            try:\n+                project_name = args[1]\n+                \n+            except IndexError:\n+                parser.print_help()\n+            else:\n+                os.mkdir(project_name)\n+                os.mknod(os.path.join(project_name, \"__init__.py\"))\n+                \n+                for subdir in [\"spiders\", \"conf\", \"commands\"]:\n+                    os.mkdir(os.path.join(project_name, subdir))\n+                    os.mknod(os.path.join(project_name, subdir, \"__init__.py\"))\n+                \n+                settings_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"settings.tmpl\"), \"r\").read()\n+                settings = settings_template.replace(\"__project_name__\", project_name)\n+                open(os.path.join(project_name, \"conf\", \"scrapy_settings.py\"), \"w\").write(settings)\n+                \n+                control_template = open(os.path.join(scrapy.__path__[0], \"templates\", \"scrapy-ctl.tmpl\"), \"r\").read()\n+                control = control_template.replace(\"__project_name__\", project_name)\n+                open(os.path.join(project_name, \"scrapy-ctl.py\"), \"w\").write(control)\n+                os.chmod(os.path.join(project_name, \"scrapy-ctl.py\"), stat.S_IRWXU)\n+                \n+            return\n+    parser.print_help()\n+\n+if __name__ == '__main__':\n+    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e2ddb47a3632435ae44cd451b1ce48818d6f5a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 188 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,6 @@ from twisted.python import failure\n \n from scrapy.core.exceptions import UsageError\n from scrapy.utils.python import flatten\n-from decobot.utils.text_extraction import unquote_html\n \n def dict_updatedefault(D, E, **F):\n     \"\"\"\n@@ -155,6 +154,13 @@ def load_class(class_path):\n \n     return cls\n \n+def unquote_html(s, keep_reserved=False):\n+    \"\"\"Convert a HTML quoted string into normal string (ISO-8859-1).\n+   \n+    Works with &#XX; and with &nbsp; &gt; etc.\n+    \"\"\"\n+    return re.sub(re.compile(r'&(#?)(.+?);', re.U), lambda m: convert_entity(m, keep_reserved), s)\n+\n def extract_regex(regex, text, encoding):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b59f62dc91869d499a859929e44c05df9ddc2017", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 19 | Churn Cumulative: 207 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n import re\n+import htmlentitydefs\n \n from twisted.internet import defer, reactor\n from twisted.python import failure\n@@ -154,6 +155,24 @@ def load_class(class_path):\n \n     return cls\n \n+def convert_entity(m, keep_reserved=False):\n+    \"\"\"\n+    Convert a HTML entity into unicode string\n+    \"\"\"\n+    if m.group(1)=='#':\n+        try:\n+            return unichr(int(m.group(2)))\n+        except ValueError:\n+            return '&#%s;' % m.group(2)\n+    try:\n+        if not (keep_reserved and m.group(2) in ['lt', 'amp']):\n+            return unichr(htmlentitydefs.name2codepoint[m.group(2)])\n+        else:\n+            return '&%s;' % m.group(2)\n+    except KeyError:\n+        return '&%s;' % m.group(2)\n+\n+\n def unquote_html(s, keep_reserved=False):\n     \"\"\"Convert a HTML quoted string into normal string (ISO-8859-1).\n    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86c7f37d6a877509fb145d41138f43f9b0490231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,8 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--nocache\", dest=\"nocache\", action=\"store_true\", help=\"disable HTTP cache\")\n+        parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", help=\"disable scraped item pipeline\")\n         parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9c624dd66a78a6962f4942b89874b48d3a9ebcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 153 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ def main():\n                 os.mkdir(project_name)\n                 os.mknod(os.path.join(project_name, \"__init__.py\"))\n                 \n-                for subdir in [\"spiders\", \"conf\", \"commands\"]:\n+                for subdir in [\"spiders\", \"conf\", \"commands\", \"templates\"]:\n                     os.mkdir(os.path.join(project_name, subdir))\n                     os.mknod(os.path.join(project_name, subdir, \"__init__.py\"))\n                 \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b2e20abfdc142d11a823bfb6967d40e0a10530f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 99 | Churn Cumulative: 99 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.6833333333333333\n\nDIFF:\n@@ -0,0 +1,40 @@\n+import unittest\n+\n+from scrapy.utils.markup import remove_entities, replace_tags\n+\n+class UtilsMarkupTest(unittest.TestCase):\n+\n+    def test_remove_entities(self):\n+        # make sure it always return uncode\n+        assert isinstance(remove_entities('no entities'), unicode)\n+        assert isinstance(remove_entities('Price: &pound;100!'),  unicode)\n+\n+        # regular conversions\n+        self.assertEqual(remove_entities(u'As low as &#163;100!'),\n+                         u'As low as \\xa3100!')\n+        self.assertEqual(remove_entities('As low as &pound;100!'),\n+                         u'As low as \\xa3100!')\n+\n+        # keep some entities\n+        self.assertEqual(remove_entities('<b>Low &lt; High &amp; Medium &pound; six</b>', keep=['lt', 'amp']),\n+                         u'<b>Low &lt; High &amp; Medium \\xa3 six</b>')\n+\n+        # illegal entities\n+        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=False),\n+                         u'a < b &illegal; c &#12345678; six')\n+        self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=True),\n+                         u'a < b  c  six')\n+\n+    def test_remove_tags(self):\n+        # make sure it always return uncode\n+        assert isinstance(replace_tags('no entities'), unicode)\n+\n+        self.assertEqual(replace_tags(u'This text contains <a>some tag</a>'),\n+                         u'This text contains some tag')\n+\n+        self.assertEqual(replace_tags('This text is very im<b>port</b>ant', ' '),\n+                         u'This text is very im port ant')\n+\n+        # multiline tags\n+        self.assertEqual(replace_tags('Click <a class=\"one\"\\r\\n href=\"url\">here</a>'),\n+                         u'Click here')\n\n@@ -0,0 +1,59 @@\n+\"\"\"\n+Functions for dealing with markup text\n+\"\"\"\n+\n+import re\n+import htmlentitydefs\n+\n+_ent_re = re.compile(r'&(#?)(.+?);')\n+_tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n+\n+def remove_entities(text, keep=(), remove_illegal=True):\n+    \"\"\"Remove entities from the given text.\n+\n+    'text' can be a unicode string or a regular string encoded as 'utf-8'\n+\n+    If 'keep' is passed (with a list of entity names) those entities will\n+    be kept (they won't be removed).\n+    \n+    It supports both numeric (&#nnnn;) and named (&nbsp; &gt;) entities.\n+    \n+    If remove_illegal is True, entities that can't be converted are removed.\n+    If remove_illegal is False, entities that can't be converted are kept \"as\n+    is\". For more information see the tests.\n+\n+    Always returns a unicode string (with the entities removed).\n+    \"\"\"\n+\n+    def convert_entity(m):\n+        if m.group(1)=='#':\n+            try:\n+                return unichr(int(m.group(2)))\n+            except ValueError:\n+                if remove_illegal:\n+                    return u''\n+                else:\n+                    return u'&#%s;' % m.group(2)\n+        try:\n+            if m.group(2) in keep:\n+                return '&%s;' % m.group(2)\n+            else:\n+                return unichr(htmlentitydefs.name2codepoint[m.group(2)])\n+        except KeyError:\n+            if remove_illegal:\n+                return u''\n+            else:\n+                return u'&%s;' % m.group(2)\n+\n+    return _ent_re.sub(convert_entity, text.decode('utf-8'))\n+\n+\n+def replace_tags(text, token=''):\n+    \"\"\"Replace all markup tags found in the given text by the given token. By\n+    default token is a null string so it just remove all tags.\n+\n+    'text' can be a unicode string or a regular string encoded as 'utf-8'\n+\n+    Always returns a unicode string.\n+    \"\"\"\n+    return _tag_re.sub(token, text.decode('utf-8'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f86324fac398c8738b739f243b003411c9648b0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 59 | Churn Cumulative: 99 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.9565217391304348\n\nDIFF:\n@@ -1,6 +1,7 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.replay import Replay\n+from scrapy.report import Report\n from scrapy.conf import settings\n \n \n@@ -18,6 +19,8 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n+        parser.add_option(\"--report\", dest=\"doreport\", action='store_true', help=\"generate a report of the scraped products in a text file\")\n+        parser.add_option(\"--report-dropped\", dest=\"doreport_dropped\", action=\"store_true\", help=\"choose whether to report dropped products or not\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -35,6 +38,8 @@ class Command(ScrapyCommand):\n             # disconnecting since pydispatcher uses weak references \n             self.replay = Replay(opts.record or opts.recorddir, mode='record', usedir=bool(opts.recorddir))\n             self.replay.record(args=args, opts=opts.__dict__)\n+        if opts.doreport:\n+            self.report = Report(dropped=opts.doreport_dropped)\n \n     def run(self, args, opts):\n         scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -0,0 +1,54 @@\n+# -*- coding: utf8 -*-\n+from datetime import datetime\n+from pydispatch import dispatcher\n+from scrapy.core import signals\n+\n+class Report(object):\n+    def __init__(self, dropped):\n+        self.domain = ''\n+        self.passed_file = None\n+        self.dropped_file = None\n+        self.dropped = dropped\n+        self.total = { 'passed': 0, 'dropped': 0 }\n+\n+        dispatcher.connect(self.domain_open, signal=signals.domain_open)\n+        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+        if self.dropped:\n+            dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n+\n+    def get_product_text(self, product, dropped=False):\n+        product_text = ''\n+        for attrib, value in product.iteritems():\n+            product_text = '%s%s: %s\\n' % (product_text, attrib, value)\n+        if dropped:\n+            product_text = '%sdropping reason: %s\\n' % (product_text, dropped)\n+        product_text = product_text + '\\n\\n'\n+        return product_text\n+\n+    def domain_open(self, domain, spider):\n+        self.domain = domain\n+        now = datetime.now()\n+        filename = '%s_%s_%s.report' % (self.domain, now.strftime('%Y%m%d'), now.strftime('%H%M'))\n+        self.passed_file = open(filename, 'w')\n+        self.passed_file.write('Scraping results for domain \"%s\"\\n\\n%s%s%s' % (self.domain, '##################################\\n',\n+          '### Products scraped correctly ###\\n', '##################################\\n'))\n+        if self.dropped:\n+            self.dropped_file = open(filename + '.dropped', 'w')\n+            self.dropped_file.write('Scraping results for domain \"%s\"\\n\\n%s%s%s' % (self.domain,\n+              '########################\\n', '### Dropped products ###\\n', '########################\\n'))\n+\n+    def item_passed(self, item, spider):\n+        self.total['passed'] += 1\n+        self.passed_file.write(self.get_product_text(item))\n+\n+    def item_dropped(self, item, spider, response, exception):\n+        self.total['dropped'] += 1\n+        self.dropped_file.write(self.get_product_text(item, exception))\n+\n+    def engine_stopped(self):\n+        self.passed_file.write('\\n--- Total scraped products: %d\\n' % self.total['passed'])\n+        self.passed_file.close()\n+        if self.dropped:\n+            self.dropped_file.write('\\n--- Total dropped products: %d\\n' % self.total['dropped'])\n+            self.dropped_file.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea9571aaa0c4095cbb8cb17d46d1869c91950489", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 66 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,10 +17,16 @@ class Report(object):\n         if self.dropped:\n             dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n \n-    def get_product_text(self, product, dropped=False):\n-        product_text = ''\n+    def get_product_attribs(self, product):\n+        product_attribs = ''\n         for attrib, value in product.iteritems():\n-            product_text = '%s%s: %s\\n' % (product_text, attrib, value)\n+            product_attribs = '%s%s: %s\\n' % (product_attribs, attrib, value)\n+        return product_attribs\n+\n+    def get_product_text(self, product, dropped=False):\n+        product_text = '###Product\\n%s' % self.get_product_attribs(product)\n+        if product.variants:\n+            product_text = '%s\\n##Variants\\n%s' % (product_text, ''.join([self.get_product_attribs(variant) for variant in product.variants]))\n         if dropped:\n             product_text = '%sdropping reason: %s\\n' % (product_text, dropped)\n         product_text = product_text + '\\n\\n'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#640e8b91312f9ef8cabfda34490faba844487ebb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 16 | Churn Cumulative: 127 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n         parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n         parser.add_option(\"--report\", dest=\"doreport\", action='store_true', help=\"generate a report of the scraped products in a text file\")\n-        parser.add_option(\"--report-dropped\", dest=\"doreport_dropped\", action=\"store_true\", help=\"choose whether to report dropped products or not\")\n+        parser.add_option(\"--report-dropped\", dest=\"doreport_dropped\", action=\"store_true\", help=\"generate a report of the dropped products in a text file\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -38,8 +38,8 @@ class Command(ScrapyCommand):\n             # disconnecting since pydispatcher uses weak references \n             self.replay = Replay(opts.record or opts.recorddir, mode='record', usedir=bool(opts.recorddir))\n             self.replay.record(args=args, opts=opts.__dict__)\n-        if opts.doreport:\n-            self.report = Report(dropped=opts.doreport_dropped)\n+        if opts.doreport or opts.doreport_dropped:\n+            self.report = Report(passed=opts.doreport, dropped=opts.doreport_dropped)\n \n     def run(self, args, opts):\n         scrapymanager.runonce(*args, **opts.__dict__)\n\n@@ -4,16 +4,18 @@ from pydispatch import dispatcher\n from scrapy.core import signals\n \n class Report(object):\n-    def __init__(self, dropped):\n+    def __init__(self, passed, dropped):\n         self.domain = ''\n         self.passed_file = None\n         self.dropped_file = None\n+        self.passed = passed\n         self.dropped = dropped\n         self.total = { 'passed': 0, 'dropped': 0 }\n \n         dispatcher.connect(self.domain_open, signal=signals.domain_open)\n-        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+        if self.passed:\n+            dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         if self.dropped:\n             dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n \n@@ -28,7 +30,7 @@ class Report(object):\n         if product.variants:\n             product_text = '%s\\n##Variants\\n%s' % (product_text, ''.join([self.get_product_attribs(variant) for variant in product.variants]))\n         if dropped:\n-            product_text = '%sdropping reason: %s\\n' % (product_text, dropped)\n+            product_text = '%s--- Dropping reason: %s ---\\n' % (product_text, dropped)\n         product_text = product_text + '\\n\\n'\n         return product_text\n \n@@ -36,6 +38,7 @@ class Report(object):\n         self.domain = domain\n         now = datetime.now()\n         filename = '%s_%s_%s.report' % (self.domain, now.strftime('%Y%m%d'), now.strftime('%H%M'))\n+        if self.passed:\n             self.passed_file = open(filename, 'w')\n             self.passed_file.write('Scraping results for domain \"%s\"\\n\\n%s%s%s' % (self.domain, '##################################\\n',\n               '### Products scraped correctly ###\\n', '##################################\\n'))\n@@ -53,6 +56,7 @@ class Report(object):\n         self.dropped_file.write(self.get_product_text(item, exception))\n \n     def engine_stopped(self):\n+        if self.passed:\n             self.passed_file.write('\\n--- Total scraped products: %d\\n' % self.total['passed'])\n             self.passed_file.close()\n         if self.dropped:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#28bb53fa22d79685fdb84226f00a577ae927cf40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 35 | Files Changed: 8 | Hunks: 24 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 72 | Churn Cumulative: 1054 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -33,4 +33,4 @@ class DepthMiddleware(object):\n         if self.stats and response.request.depth == 0: # otherwise we loose stats for depth=0 \n             stats.incpath('%s/request_depth_count/0' % spider.domain_name)\n \n-        return [r for r in result or () if _filter(r)]\n+        return (r for r in result or () if _filter(r))\n\n@@ -9,5 +9,5 @@ class OffsiteMiddleware(object):\n                 log.msg(\"Ignoring link (offsite): %s \" % r.url, level=log.DEBUG, domain=spider.domain_name)\n                 return False\n             return True\n-        return [r for r in result or () if _filter(r)]\n+        return (r for r in result or () if _filter(r))\n \n\n@@ -8,5 +8,5 @@ class CrawlMiddleware(object):\n             if isinstance(r, Request):\n                 r.headers.setdefault('Referer', response.url)\n             return r\n-        return [_set_referer(r) for r in result or ()]\n+        return (_set_referer(r) for r in result or ())\n \n\n@@ -17,5 +17,5 @@ class RestrictMiddleware(object):\n             if isinstance(r, Request) and r.url not in self.allowed_urls:\n                 return False\n             return True\n-        return [r for r in result or () if _filter(r)]\n+        return (r for r in result or () if _filter(r))\n \n\n@@ -19,4 +19,4 @@ class UrlLengthMiddleware(object):\n             else:\n                 return True\n \n-        return [r for r in result or () if _filter(r)]\n+        return (r for r in result or () if _filter(r))\n\n@@ -17,7 +17,7 @@ from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipeline\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred\n+from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n from scrapy.conf import settings\n \n \n@@ -238,8 +238,7 @@ class ExecutionEngine(object):\n                     signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n                 self.next_request(spider)\n \n-            def _onsuccess(result):\n-                for item in result or ():\n+            def _onsuccess_per_item(item):\n                 if isinstance(item, ScrapedItem):\n                     log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.DEBUG, domain=domain)\n                     signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=item, spider=spider, response=response)\n@@ -251,6 +250,13 @@ class ExecutionEngine(object):\n                 else:\n                     log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.TRACE, domain=domain)\n \n+            class _ResultContainer(object):\n+                def append(self, item):\n+                    _onsuccess_per_item(item)\n+\n+            def _onsuccess(result):\n+                return deferred_degenerate(result, _ResultContainer())\n+\n             def _onerror(_failure):\n                 if not isinstance(_failure.value, IgnoreRequest):\n                     referer = None if not isinstance(response, Response) else response.request.headers.get('Referer', None)\n@@ -259,10 +265,13 @@ class ExecutionEngine(object):\n             def _bugtrap(_failure):\n                 log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n \n+\n             scd = self.scrape(request, response, spider)\n             scd.addCallbacks(_onsuccess, _onerror)\n             scd.addErrback(_bugtrap)\n-            scd.addBoth(lambda _:response)\n+\n+            self._scraping[domain].add(response)\n+            scd.addBoth(lambda _: self._scraping[domain].remove(response))\n             return scd\n \n         def _cleanfailure(_failure):\n@@ -277,13 +286,7 @@ class ExecutionEngine(object):\n         return schd\n \n     def scrape(self, request, response, spider):\n-        domain = spider.domain_name\n-        scd = self.spidermiddleware.scrape(request, response, spider)\n-        self._scraping[domain].add(response)\n-        def _remove(_):\n-            self._scraping[domain].remove(response)\n-            return _\n-        return scd.addBoth(_remove)\n+        return self.spidermiddleware.scrape(request, response, spider)\n \n     def schedule(self, request, spider, priority=1, domain_priority=1):\n         domain = spider.domain_name\n\n@@ -2,15 +2,20 @@\n Spider middleware manager\n \"\"\"\n \n-import types\n \n from twisted.python.failure import Failure\n \n from scrapy.core import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class, mustbe_deferred, deferred_degenerate\n+from scrapy.utils.misc import load_class, mustbe_deferred\n from scrapy.conf import settings\n \n+def _isiterable(possible_iterator):\n+    try:\n+        return iter(possible_iterator)\n+    except TypeError:\n+        return None\n+\n class SpiderMiddlewareManager(object):\n     def __init__(self, callback=None, errback=None):\n         self.loaded = False\n@@ -55,8 +60,8 @@ class SpiderMiddlewareManager(object):\n         def process_scrape(response):\n             for method in self.spider_middleware:\n                 result = method(response=response, spider=spider)\n-                assert result is None or isinstance(result, (list, tuple)), \\\n-                    'Middleware %s must returns None, list or tuple, got %s ' % \\\n+                assert result is None or _isiterable(result), \\\n+                    'Middleware %s must returns None or an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n                 if result is not None:\n                     return result\n@@ -66,8 +71,8 @@ class SpiderMiddlewareManager(object):\n         def process_result(result):\n             for method in self.result_middleware:\n                 result = method(response=response, result=result, spider=spider)\n-                assert isinstance(result, (list, tuple)), \\\n-                    'Middleware %s must returns list or tuple, got %s ' % \\\n+                assert _isiterable(result), \\\n+                    'Middleware %s must returns an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n             return result\n \n@@ -75,20 +80,14 @@ class SpiderMiddlewareManager(object):\n             exception = _failure.value\n             for method in self.exception_middleware:\n                 result = method(response=response, exception=exception, spider=spider)\n-                assert result is None or isinstance(result, (list, tuple)), \\\n-                    'Middleware %s must returns None, list or tuple, got %s ' % \\\n+                assert result is None or _isiterable(result), \\\n+                    'Middleware %s must returns None, or an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n                 if result is not None:\n                     return result\n             return _failure\n \n-        def _degenerate(gen):\n-            if isinstance(gen, types.GeneratorType):\n-                return deferred_degenerate(gen)\n-            return gen\n-\n         dfd = mustbe_deferred(process_scrape, response)\n-        dfd.addCallback(_degenerate)\n         dfd.addErrback(process_exception)\n         dfd.addCallback(process_result)\n         return dfd\n\n@@ -111,19 +111,19 @@ def memoize(cache, hash):\n     return decorator\n \n \n-def deferred_degenerate(generator):\n+def deferred_degenerate(generator, container=None, next_delay=0):\n     generator = iter(generator)\n     deferred = defer.Deferred()\n-    result = []\n+    container = container or []\n     def _next():\n         try:\n-            result.append(generator.next())\n+            container.append(generator.next())\n         except StopIteration:\n-            reactor.callLater(0, deferred.callback, result)\n+            reactor.callLater(0, deferred.callback, container)\n         except:\n             reactor.callLater(0, deferred.errback, failure.Failure())\n         else:\n-            reactor.callLater(0, _next)\n+            reactor.callLater(next_delay, _next)\n     _next()\n     return deferred\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
