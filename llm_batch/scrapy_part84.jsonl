{"custom_id": "scrapy#16ada1d5dcb812d2deb449d1163732a64f8c6810", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,9 +31,8 @@ class ItemLoader(object):\n         self._values[field_name] += arg_to_iter(processed_value)\n \n     def replace_value(self, field_name, value):\n-        value = arg_to_iter(value)\n-        processed_value = self._process_input_value(field_name, value)\n-        self._values[field_name] = arg_to_iter(processed_value)\n+        self._values.pop(field_name, None)\n+        self.add_value(field_name, value)\n \n     def load_item(self):\n         item = self.item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb4871b7a74465e4ff41d52e16b38d7bef5b9230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 510 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -311,6 +311,7 @@ class TestXPathItemLoader(XPathItemLoader):\n     name_in = MapCompose(lambda v: v.title())\n \n class XPathItemLoaderTest(unittest.TestCase):\n+    response = HtmlResponse(url=\"\", body='<html><body><div id=\"id\">marta</div><p>paragraph</p></body></html>')\n \n     def test_constructor_errors(self):\n         self.assertRaises(RuntimeError, XPathItemLoader)\n@@ -323,18 +324,32 @@ class XPathItemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_constructor_with_response(self):\n-        response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemLoader(response=response)\n+        l = TestXPathItemLoader(response=self.response)\n         self.assert_(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_add_xpath_re(self):\n-        response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemLoader(response=response)\n+        l = TestXPathItemLoader(response=self.response)\n         l.add_xpath('name', '//div/text()', re='ma')\n         self.assertEqual(l.get_output_value('name'), [u'Ma'])\n \n+    def test_replace_xpath(self):\n+        l = TestXPathItemLoader(response=self.response)\n+        self.assert_(l.selector)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+        l.replace_xpath('name', '//p/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n+\n+    def test_replace_xpath_re(self):\n+        l = TestXPathItemLoader(response=self.response)\n+        self.assert_(l.selector)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+        l.replace_xpath('name', '//div/text()', re='ma')\n+        self.assertEqual(l.get_output_value('name'), [u'Ma'])\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64ad96ba1ead3c670820db828d6155fd3d082ffa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 2 | Methods Changed: 15 | Complexity Δ (Sum/Max): 18/10 | Churn Δ: 101 | Churn Cumulative: 101 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -0,0 +1,34 @@\n+from lxml import html, etree\n+\n+from scrapy.contrib.loader import ItemLoader\n+\n+\n+class LxmlItemLoader(ItemLoader):\n+\n+    def __init__(self, response, item=None, **context):\n+        self.tree = html.fromstring(response.body_as_unicode())\n+        context.update(response=response)\n+        super(LxmlItemLoader, self).__init__(item, **context)\n+\n+    def add_xpath(self, field_name, xpath):\n+        self.add_value(field_name, self._get_xpath(xpath))\n+\n+    def replace_xpath(self, field_name, xpath):\n+        self.replace_value(field_name, self._get_xpath(xpath))\n+\n+    def _get_xpath(self, xpath):\n+        return self._get_values(self.tree.xpath(xpath))\n+\n+    def add_css(self, field_name, css):\n+        self.add_value(field_name, self._get_css(css))\n+\n+    def replace_css(self, field_name, css):\n+        self.replace_value(field_name, self._get_css(css))\n+\n+    def _get_css(self, css):\n+        return self._get_values(self.tree.cssselect(css))\n+\n+    def _get_values(self, elems):\n+        for e in elems:\n+            yield etree.tostring(e) if isinstance(e, etree.ElementBase) else e\n+\n\n@@ -0,0 +1,67 @@\n+from twisted.trial import unittest\n+\n+from scrapy.contrib.loader.processor import MapCompose\n+from scrapy.item import Item, Field\n+from scrapy.http import HtmlResponse\n+\n+try:\n+    import lxml\n+except ImportError:\n+    lxml = False\n+\n+\n+class TestItem(Item):\n+    name = Field()\n+\n+\n+if lxml:\n+    from scrapy.contrib_exp.loader.lxmlloader import LxmlItemLoader\n+\n+    class TestLxmlItemLoader(LxmlItemLoader):\n+        default_item_class = TestItem\n+\n+\n+class LxmlItemLoaderTest(unittest.TestCase):\n+    response = HtmlResponse(url=\"\", body='<html><body><div id=\"id\">marta</div><p>paragraph</p></body></html>')\n+\n+    def setUp(self):\n+        if not lxml:\n+            raise unittest.SkipTest(\"lxml is not available\")\n+\n+    def test_constructor_with_response(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        self.assert_(l.tree)\n+\n+    def test_add_xpath(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        l.add_xpath('name', '//div')\n+        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n+\n+    def test_add_xpath_text(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'marta'])\n+\n+    def test_replace_xpath(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        l.add_xpath('name', '//div/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'marta'])\n+        l.replace_xpath('name', '//p/text()')\n+        self.assertEqual(l.get_output_value('name'), [u'paragraph'])\n+\n+    def test_add_css(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        l.add_css('name', '#id')\n+        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n+\n+    def test_replace_css(self):\n+        l = TestLxmlItemLoader(response=self.response)\n+        l.add_css('name', '#id')\n+        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n+        l.replace_css('name', 'p')\n+        self.assertEqual(l.get_output_value('name'), [u'<p>paragraph</p>'])\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f1bb871988c2729f4cb80662fdc599d78638b8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 18 | Churn Cumulative: 24 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,7 +5,6 @@ import sys\n import pprint as pypprint\n \n from scrapy.item import BaseItem\n-from scrapy.http import Request, Response\n \n nocolour = False\n \n@@ -20,30 +19,25 @@ def colorize(text):\n     except ImportError:\n         return text\n \n-def pformat_dictobj(obj):\n+def _pformat_dictobj(obj):\n     clsname = obj.__class__.__name__\n     return \"%s(%s)\\n\" % (clsname, colorize(pypprint.pformat(obj.__dict__)))\n \n-def pprint_dictobj(obj):\n-    print pformat_dictobj(obj)\n-\n def pformat(obj, *args, **kwargs):\n     \"\"\"\n     Wrapper which autodetects the object type and uses the proper formatting\n     function\n     \"\"\"\n-\n-    if isinstance(obj, (list, tuple)):\n-        return \"\".join([pformat(i, *args, **kwargs) for i in obj])\n-    elif isinstance(obj, (BaseItem, Request, Response)):\n-        return pformat_dictobj(obj)\n+    if isinstance(obj, BaseItem):\n+        return _pformat_dictobj(obj)\n+    elif hasattr(obj, '__iter__'):\n+        return \"\".join(map(pformat, obj))\n     else:\n-        return colorize(pypprint.pformat(obj))\n+        return colorize(pypprint.pformat(repr(obj)))\n \n def pprint(obj, *args, **kwargs):\n     \"\"\"\n     Wrapper which autodetects the object type and uses the proper printing\n     function\n     \"\"\"\n-\n     print pformat(obj, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aacb7965b41b0fa805b1e095cefe8c037a4fc9c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 20 | Churn Cumulative: 1293 | Contributors (this commit): 4 | Commits (past 90d): 31 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -161,6 +161,10 @@ class ExecutionEngine(object):\n         return self.downloader.sites.keys()\n \n     def crawl(self, request, spider):\n+        if not request.deferred.callbacks:\n+            log.msg(\"Unable to crawl Request with no callback: %s\" % request,\n+                level=log.ERROR, domain=spider.domain_name)\n+            return\n         schd = mustbe_deferred(self.schedule, request, spider)\n         # FIXME: we can't log errors because we would be preventing them from\n         # propagating to the request errback. This should be fixed after the\n\n@@ -85,23 +85,9 @@ class SpiderMiddlewareManager(object):\n                 assert _isiterable(result), \\\n                     'Middleware %s must returns an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n-\n-            return self._validate_output(request, result, spider)\n+            return result\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception)\n         dfd.addCallback(process_spider_output)\n         return dfd\n-\n-    def _validate_output(self, request, result, spider):\n-        \"\"\"Every request returned by spiders must be instanciate with a callback\"\"\"\n-        for r in result:\n-            if isinstance(r, Request) and not r.deferred.callbacks:\n-                log.msg('Ignoring %s returned from spider while processing %s: ' \\\n-                        'Request has no callback, try adding callback=self.parse ' \\\n-                        'argument when instanciating Request objects inside your spiders' \\\n-                        % (r, request), level=log.WARNING, domain=spider.domain_name)\n-                continue\n-            yield r\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8aa7d153ae0938539654bdfba3b4edcc23c34d81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 272 | Lines Deleted: 157 | Files Changed: 8 | Hunks: 39 | Methods Changed: 48 | Complexity Δ (Sum/Max): 10/13 | Churn Δ: 429 | Churn Cumulative: 825 | Contributors (this commit): 3 | Commits (past 90d): 46 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,6 +124,13 @@ REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n+REQUEST_HANDLERS = {}\n+REQUEST_HANDLERS_BASE = {\n+    'file': 'scrapy.core.downloader.handlers.file.download_file',\n+    'http': 'scrapy.core.downloader.handlers.http.download_http',\n+    'https': 'scrapy.core.downloader.handlers.http.download_http',\n+}\n+\n REQUESTS_QUEUE_SIZE = 0\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n \n\n@@ -1,90 +0,0 @@\n-\"\"\"\n-Download handlers for different schemes\n-\"\"\"\n-from __future__ import with_statement\n-\n-import urlparse\n-\n-from twisted.internet import reactor\n-try:\n-    from twisted.internet import ssl\n-except ImportError:\n-    pass\n-\n-from scrapy import optional_features\n-from scrapy.core import signals\n-from scrapy.http import Headers\n-from scrapy.core.exceptions import NotSupported\n-from scrapy.utils.defer import defer_succeed\n-from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.signal import send_catch_log\n-from scrapy.utils.misc import load_object\n-from scrapy.core.downloader.responsetypes import responsetypes\n-from scrapy.conf import settings\n-\n-\n-HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n-ssl_supported = 'ssl' in optional_features\n-\n-def download_any(request, spider):\n-    scheme = urlparse_cached(request).scheme\n-    if scheme == 'http':\n-        return download_http(request, spider)\n-    elif scheme == 'https':\n-        if ssl_supported:\n-            return download_https(request, spider)\n-        else:\n-            raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n-    elif scheme == 'file':\n-        return download_file(request, spider)\n-    else:\n-        raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (scheme, request.url))\n-\n-def create_factory(request, spider):\n-    \"\"\"Return HTTPClientFactory for the given Request\"\"\"\n-    url = urlparse.urldefrag(request.url)[0]\n-    timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n-    factory = HTTPClientFactory.from_request(request, timeout)\n-\n-    def _create_response(body):\n-        body = body or ''\n-        status = int(factory.status)\n-        headers = Headers(factory.response_headers)\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n-        r = respcls(url=request.url, status=status, headers=headers, body=body)\n-        send_catch_log(signal=signals.request_uploaded, sender='download_http', \\\n-            request=request, spider=spider)\n-        send_catch_log(signal=signals.response_downloaded, sender='download_http', \\\n-            response=r, spider=spider)\n-        return r\n-\n-    factory.deferred.addCallbacks(_create_response)\n-    return factory\n-\n-def download_http(request, spider):\n-    \"\"\"Return a deferred for the HTTP download\"\"\"\n-    factory = create_factory(request, spider)\n-    url = urlparse_cached(request)\n-    port = url.port\n-    reactor.connectTCP(url.hostname, port or 80, factory)\n-    return factory.deferred\n-\n-def download_https(request, spider):\n-    \"\"\"Return a deferred for the HTTPS download\"\"\"\n-    factory = create_factory(request, spider)\n-    url = urlparse_cached(request)\n-    port = url.port\n-    contextFactory = ssl.ClientContextFactory()\n-    reactor.connectSSL(url.hostname, port or 443, factory, contextFactory)\n-    return factory.deferred\n-\n-def download_file(request, spider) :\n-    \"\"\"Return a deferred for a file download.\"\"\"\n-    filepath = request.url.split(\"file://\")[1]\n-    with open(filepath) as f:\n-        body = f.read()\n-        respcls = responsetypes.from_args(filename=filepath, body=body)\n-        response = respcls(url=request.url, body=body)\n-\n-    return defer_succeed(response)\n\n@@ -0,0 +1,27 @@\n+\"\"\"Download handlers for different schemes\"\"\"\n+\n+from scrapy.core.exceptions import NotSupported\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.conf import settings\n+from scrapy.utils.misc import load_object\n+\n+\n+class RequestHandlers(object):\n+\n+    def __init__(self):\n+        self._handlers = {}\n+        handlers = settings.get('REQUEST_HANDLERS_BASE')\n+        handlers.update(settings.get('REQUEST_HANDLERS', {}))\n+        for scheme, cls in handlers.iteritems():\n+            self._handlers[scheme] = load_object(cls)\n+\n+    def download_request(self, request, spider):\n+        scheme = urlparse_cached(request).scheme\n+        try:\n+            handler = self._handlers[scheme]\n+        except KeyError:\n+            raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (scheme, request.url))\n+        return handler(request, spider)\n+\n+\n+download_any = RequestHandlers().download_request\n\n@@ -0,0 +1,18 @@\n+\"\"\"Download handler for file:// scheme\"\"\"\n+from __future__ import with_statement\n+\n+from twisted.internet import defer\n+from scrapy.core.downloader.responsetypes import responsetypes\n+\n+\n+def download_file(request, spider):\n+    \"\"\"Return a deferred for a file download.\"\"\"\n+    return defer.maybeDeferred(_all_in_one_read_download_file, request, spider)\n+\n+def _all_in_one_read_download_file(request, spider):\n+    filepath = request.url.split(\"file://\")[1]\n+    with open(filepath) as f:\n+        body = f.read()\n+    respcls = responsetypes.from_args(filename=filepath, body=body)\n+    return respcls(url=request.url, body=body)\n+\n\n@@ -0,0 +1,50 @@\n+\"\"\"Download handlers for http and https schemes\"\"\"\n+\n+from twisted.internet import reactor\n+\n+from scrapy.core import signals\n+from scrapy.core.exceptions import NotSupported\n+from scrapy.utils.signal import send_catch_log\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+from scrapy import optional_features\n+\n+ssl_supported = 'ssl' in optional_features\n+if ssl_supported:\n+    from twisted.internet.ssl import ClientContextFactory\n+\n+\n+HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n+\n+def _create_factory(request, spider):\n+    def _download_signals(response):\n+        send_catch_log(signal=signals.request_uploaded, \\\n+                sender='download_http', request=request, spider=spider)\n+        send_catch_log(signal=signals.response_downloaded, \\\n+                sender='download_http', response=response, spider=spider)\n+        return response\n+\n+    timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n+    factory = HTTPClientFactory(request, timeout)\n+    factory.deferred.addCallbacks(_download_signals)\n+    return factory\n+\n+\n+def _connect(factory):\n+    host, port = factory.host, factory.port\n+    if factory.scheme == 'https':\n+        if ssl_supported:\n+            return reactor.connectSSL(host, port, factory, ClientContextFactory())\n+        raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n+    else:\n+        return reactor.connectTCP(host, port, factory)\n+\n+\n+def download_http(request, spider):\n+    \"\"\"Return a deferred for the HTTP download\"\"\"\n+    factory = _create_factory(request, spider)\n+    _connect(factory)\n+    return factory.deferred\n+\n+\n\n@@ -1,12 +1,14 @@\n-from urlparse import urlparse, urlunparse\n+from urlparse import urlparse, urlunparse, urldefrag\n \n from twisted.python import failure\n-from twisted.web.client import HTTPClientFactory, PartialDownloadError\n+from twisted.web.client import PartialDownloadError, HTTPClientFactory\n from twisted.web.http import HTTPClient\n from twisted.internet import defer\n \n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.core.downloader.responsetypes import responsetypes\n+\n \n def _parsed_url_args(parsed):\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n@@ -18,11 +20,13 @@ def _parsed_url_args(parsed):\n         port = 443 if scheme == 'https' else 80\n     return scheme, netloc, host, port, path\n \n+\n def _parse(url):\n     url = url.strip()\n     parsed = urlparse(url)\n     return _parsed_url_args(parsed)\n \n+\n class ScrapyHTTPPageGetter(HTTPClient):\n \n     def connectionMade(self):\n@@ -64,7 +68,9 @@ class ScrapyHTTPPageGetter(HTTPClient):\n \n     def timeout(self):\n         self.transport.loseConnection()\n-        self.factory.noPage(defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % (self.factory.url, self.factory.timeout)))\n+        self.factory.noPage(\\\n+                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % \\\n+                (self.factory.url, self.factory.timeout)))\n \n \n class ScrapyHTTPClientFactory(HTTPClientFactory):\n@@ -74,22 +80,19 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     \"\"\"\n \n     protocol = ScrapyHTTPPageGetter\n-    response_headers = None\n     waiting = 1\n     noisy = False\n \n-    def __init__(self, url, method='GET', body=None, headers=None, timeout=0, parsedurl=None):\n-        self.url = url\n-        self.method = method\n-        self.body = body or None\n-        if parsedurl:\n-            self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsedurl)\n-        else:\n-            self.scheme, self.netloc, self.host, self.port, self.path = _parse(url)\n-\n+    def __init__(self, request, timeout=0):\n+        self.url = urldefrag(request.url)[0]\n+        self.method = request.method\n+        self.body = request.body or None\n+        self.headers = request.headers\n+        self.response_headers = None\n         self.timeout = timeout\n-        self.headers = Headers(headers or {})\n-        self.deferred = defer.Deferred()\n+        self.deferred = defer.Deferred().addCallback(self._build_response)\n+\n+        self._set_connection_attributes(request)\n \n         # set Host header based on url\n         self.headers.setdefault('Host', self.netloc)\n@@ -100,33 +103,15 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n \n-    @classmethod\n-    def from_request(cls, request, timeout):\n-        return cls(request.url,\n-            method=request.method,\n-            body=request.body or None, # see http://dev.scrapy.org/ticket/60\n-            headers=Headers(request.headers or {}),\n-            timeout=timeout,\n-            parsedurl=urlparse_cached(request),\n-            )\n+    def _build_response(self, body):\n+        status = int(self.status)\n+        headers = Headers(self.response_headers)\n+        respcls = responsetypes.from_args(headers=headers, url=self.url)\n+        return respcls(url=self.url, status=status, headers=headers, body=body)\n+\n+    def _set_connection_attributes(self, request):\n+        parsed = urlparse_cached(request)\n+        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n \n     def gotHeaders(self, headers):\n         self.response_headers = headers\n-\n-\n-\n-def getPage(url, contextFactory=None, *args, **kwargs):\n-    \"\"\"\n-    Download a web page as a string.\n-\n-    Download a page. Return a deferred, which will callback with a\n-    page (as a string) or errback with a description of the error.\n-\n-    See HTTPClientFactory to see what extra args can be passed.\n-    \"\"\"\n-    from twisted.web.client import _makeGetterFactory\n-    return _makeGetterFactory(\n-        url,\n-        ScrapyHTTPClientFactory,\n-        contextFactory=contextFactory,\n-        *args, **kwargs).deferred\n\n@@ -0,0 +1,105 @@\n+import os\n+\n+from twisted.trial import unittest\n+from twisted.protocols.policies import WrappingFactory\n+from twisted.python.filepath import FilePath\n+from twisted.internet import reactor, defer\n+from twisted.web import server, static, util, resource\n+from twisted.web.test.test_webclient import ForeverTakingResource, \\\n+        NoLengthResource, HostHeaderResource, \\\n+        PayloadResource, BrokenDownloadResource\n+\n+from scrapy.core.downloader.webclient import PartialDownloadError\n+from scrapy.core.downloader.handlers.file import download_file\n+from scrapy.core.downloader.handlers.http import download_http\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+\n+\n+class FileTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.tmpname = self.mktemp()\n+        fd = open(self.tmpname, 'w')\n+        fd.write('0123456789')\n+        fd.close()\n+\n+    def test_download(self):\n+        def _test(response):\n+            self.assertEquals(response.url, request.url)\n+            self.assertEquals(response.status, 200)\n+            self.assertEquals(response.body, '0123456789')\n+\n+        request = Request('file://%s' % self.tmpname)\n+        return download_file(request, BaseSpider()).addCallback(_test)\n+\n+    def test_non_existent(self):\n+        request = Request('file://%s' % self.mktemp())\n+        d = download_file(request, BaseSpider())\n+        return self.assertFailure(d, IOError)\n+\n+\n+class HttpTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        name = self.mktemp()\n+        os.mkdir(name)\n+        FilePath(name).child(\"file\").setContent(\"0123456789\")\n+        r = static.File(name)\n+        r.putChild(\"redirect\", util.Redirect(\"/file\"))\n+        r.putChild(\"wait\", ForeverTakingResource())\n+        r.putChild(\"nolength\", NoLengthResource())\n+        r.putChild(\"host\", HostHeaderResource())\n+        r.putChild(\"payload\", PayloadResource())\n+        r.putChild(\"broken\", BrokenDownloadResource())\n+        self.site = server.Site(r, timeout=None)\n+        self.wrapper = WrappingFactory(self.site)\n+        self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n+        self.portno = self.port.getHost().port\n+\n+    def tearDown(self):\n+        return self.port.stopListening()\n+\n+    def getURL(self, path):\n+        return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n+\n+    def test_download(self):\n+        request = Request(self.getURL('file'))\n+        d = download_http(request, BaseSpider())\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, \"0123456789\")\n+        return d\n+\n+    def test_redirect_status(self):\n+        request = Request(self.getURL('redirect'))\n+        d = download_http(request, BaseSpider())\n+        d.addCallback(lambda r: r.status)\n+        d.addCallback(self.assertEquals, 302)\n+        return d\n+\n+    def test_timeout_download_from_spider(self):\n+        spider = BaseSpider()\n+        spider.download_timeout = 0.000001\n+        request = Request(self.getURL('wait'))\n+        d = download_http(request, spider)\n+        return self.assertFailure(d, defer.TimeoutError)\n+\n+    def test_host_header(self):\n+        request = Request(self.getURL('host'))\n+        d = download_http(request, BaseSpider())\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, '127.0.0.1:%d' % self.portno)\n+        return d\n+\n+    def test_payload(self):\n+        body = '1'*100 # PayloadResource requires body length to be 100\n+        request = Request(self.getURL('payload'), method='POST', body=body)\n+        d = download_http(request, BaseSpider())\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, body)\n+        return d\n+\n+    def test_broken_download(self):\n+        request = Request(self.getURL('broken'))\n+        d = download_http(request, BaseSpider())\n+        return self.assertFailure(d, PartialDownloadError)\n\n@@ -13,14 +13,27 @@ from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n \n from scrapy.core.downloader import webclient as client\n-from scrapy.http import Headers\n+from scrapy.http import Request, Headers\n+\n+\n+def getPage(url, contextFactory=None, *args, **kwargs):\n+    \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n+    def _clientfactory(*args, **kwargs):\n+        timeout = kwargs.pop('timeout', 0)\n+        f = client.ScrapyHTTPClientFactory(Request(*args, **kwargs), timeout=timeout)\n+        f.deferred.addCallback(lambda r: r.body)\n+        return f\n+\n+    from twisted.web.client import _makeGetterFactory\n+    return _makeGetterFactory(url, _clientfactory,\n+        contextFactory=contextFactory, *args, **kwargs).deferred\n \n \n class ParseUrlTestCase(unittest.TestCase):\n     \"\"\"Test URL parsing facility and defaults values.\"\"\"\n \n     def _parse(self, url):\n-        f = client.ScrapyHTTPClientFactory(url)\n+        f = client.ScrapyHTTPClientFactory(Request(url))\n         return (f.scheme, f.netloc, f.host, f.port, f.path)\n \n     def testParse(self):\n@@ -75,7 +88,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n     def test_earlyHeaders(self):\n         # basic test stolen from twisted HTTPageGetter\n-        factory = client.ScrapyHTTPClientFactory(\n+        factory = client.ScrapyHTTPClientFactory(Request(\n             url='http://foo/bar',\n             body=\"some data\",\n             headers={\n@@ -83,7 +96,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n                 'User-Agent': 'fooble',\n                 'Cookie': 'blah blah',\n                 'Content-Length': '12981',\n-                'Useful': 'value'})\n+                'Useful': 'value'}))\n \n         self._test(factory,\n             \"GET /bar HTTP/1.0\\r\\n\"\n@@ -97,18 +110,18 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             \"some data\")\n \n         # test minimal sent headers\n-        factory = client.ScrapyHTTPClientFactory('http://foo/bar')\n+        factory = client.ScrapyHTTPClientFactory(Request('http://foo/bar'))\n         self._test(factory,\n             \"GET /bar HTTP/1.0\\r\\n\"\n             \"Host: foo\\r\\n\"\n             \"\\r\\n\")\n \n         # test a simple POST with body and content-type\n-        factory = client.ScrapyHTTPClientFactory(\n+        factory = client.ScrapyHTTPClientFactory(Request(\n             method='POST',\n             url='http://foo/bar',\n             body='name=value',\n-            headers={'Content-Type': 'application/x-www-form-urlencoded'})\n+            headers={'Content-Type': 'application/x-www-form-urlencoded'}))\n \n         self._test(factory,\n             \"POST /bar HTTP/1.0\\r\\n\"\n@@ -120,12 +133,12 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             \"name=value\")\n \n         # test with single and multivalued headers\n-        factory = client.ScrapyHTTPClientFactory(\n+        factory = client.ScrapyHTTPClientFactory(Request(\n             url='http://foo/bar',\n             headers={\n                 'X-Meta-Single': 'single',\n                 'X-Meta-Multivalued': ['value1', 'value2'],\n-                })\n+                }))\n \n         self._test(factory,\n             \"GET /bar HTTP/1.0\\r\\n\"\n@@ -136,12 +149,12 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             \"\\r\\n\")\n \n         # same test with single and multivalued headers but using Headers class\n-        factory = client.ScrapyHTTPClientFactory(\n+        factory = client.ScrapyHTTPClientFactory(Request(\n             url='http://foo/bar',\n             headers=Headers({\n                 'X-Meta-Single': 'single',\n                 'X-Meta-Multivalued': ['value1', 'value2'],\n-                }))\n+                })))\n \n         self._test(factory,\n             \"GET /bar HTTP/1.0\\r\\n\"\n@@ -193,11 +206,11 @@ class WebClientTestCase(unittest.TestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        return client.getPage(self.getURL(\"payload\"), body=s).addCallback(self.assertEquals, s)\n+        return getPage(self.getURL(\"payload\"), body=s).addCallback(self.assertEquals, s)\n \n     def testBrokenDownload(self):\n         # test what happens when download gets disconnected in the middle\n-        d = client.getPage(self.getURL(\"broken\"))\n+        d = getPage(self.getURL(\"broken\"))\n         d = self.assertFailure(d, client.PartialDownloadError)\n         d.addCallback(lambda exc: self.assertEquals(exc.response, \"abc\"))\n         return d\n@@ -206,8 +219,8 @@ class WebClientTestCase(unittest.TestCase):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n         return defer.gatherResults([\n-            client.getPage(self.getURL(\"host\")).addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno),\n-            client.getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(self.assertEquals, \"www.example.com\")])\n+            getPage(self.getURL(\"host\")).addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno),\n+            getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(self.assertEquals, \"www.example.com\")])\n \n \n     def test_getPage(self):\n@@ -215,7 +228,7 @@ class WebClientTestCase(unittest.TestCase):\n         L{client.getPage} returns a L{Deferred} which is called back with\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n-        d = client.getPage(self.getURL(\"file\"))\n+        d = getPage(self.getURL(\"file\"))\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n@@ -226,11 +239,11 @@ class WebClientTestCase(unittest.TestCase):\n         the empty string if the method is C{HEAD} and there is a successful\n         response code.\n         \"\"\"\n-        def getPage(method):\n-            return client.getPage(self.getURL(\"file\"), method=method)\n+        def _getPage(method):\n+            return getPage(self.getURL(\"file\"), method=method)\n         return defer.gatherResults([\n-            getPage(\"head\").addCallback(self.assertEqual, \"\"),\n-            getPage(\"HEAD\").addCallback(self.assertEqual, \"\")])\n+            _getPage(\"head\").addCallback(self.assertEqual, \"\"),\n+            _getPage(\"HEAD\").addCallback(self.assertEqual, \"\")])\n \n \n     def test_timeoutNotTriggering(self):\n@@ -239,7 +252,7 @@ class WebClientTestCase(unittest.TestCase):\n         retrieved before the timeout period elapses, the L{Deferred} is\n         called back with the contents of the page.\n         \"\"\"\n-        d = client.getPage(self.getURL(\"host\"), timeout=100)\n+        d = getPage(self.getURL(\"host\"), timeout=100)\n         d.addCallback(self.assertEquals, \"127.0.0.1:%d\" % self.portno)\n         return d\n \n@@ -251,7 +264,7 @@ class WebClientTestCase(unittest.TestCase):\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n         finished = self.assertFailure(\n-            client.getPage(self.getURL(\"wait\"), timeout=0.000001),\n+            getPage(self.getURL(\"wait\"), timeout=0.000001),\n             defer.TimeoutError)\n         def cleanup(passthrough):\n             # Clean up the server which is hanging around not doing\n@@ -266,7 +279,7 @@ class WebClientTestCase(unittest.TestCase):\n         return finished\n \n     def testNotFound(self):\n-        return client.getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n+        return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n         self.assert_('404 - No Such Resource' in pageData)\n@@ -274,7 +287,7 @@ class WebClientTestCase(unittest.TestCase):\n     def testFactoryInfo(self):\n         url = self.getURL('file')\n         scheme, netloc, host, port, path = client._parse(url)\n-        factory = client.ScrapyHTTPClientFactory(url)\n+        factory = client.ScrapyHTTPClientFactory(Request(url))\n         reactor.connectTCP(host, port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n@@ -285,7 +298,7 @@ class WebClientTestCase(unittest.TestCase):\n         self.assertEquals(factory.response_headers['content-length'], '10')\n \n     def testRedirect(self):\n-        return client.getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n+        return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n \n     def _cbRedirect(self, pageData):\n         self.assertEquals(pageData,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc64ca3e1372e40b8b89bbacac23a18661cdcf15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 181 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 22 | Complexity Δ (Sum/Max): 35/14 | Churn Δ: 181 | Churn Cumulative: 638 | Contributors (this commit): 3 | Commits (past 90d): 40 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,6 +60,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n     'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n+    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n     'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n\n@@ -0,0 +1,51 @@\n+import base64\n+from urllib import getproxies, unquote, proxy_bypass\n+from urllib2 import _parse_proxy\n+from urlparse import urlunparse\n+\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.core.exceptions import NotConfigured\n+\n+\n+class HttpProxyMiddleware(object):\n+\n+    def __init__(self):\n+        self.proxies = {}\n+        for type, url in getproxies().items():\n+            self.proxies[type] = self._get_proxy(url, type)\n+\n+        if not self.proxies:\n+            raise NotConfigured\n+\n+    def _get_proxy(self, url, orig_type):\n+        proxy_type, user, password, hostport = _parse_proxy(url)\n+        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n+\n+        if user and password:\n+            user_pass = '%s:%s' % (unquote(user), unquote(password))\n+            creds = base64.b64encode(user_pass).strip()\n+        else:\n+            creds = None\n+\n+        return creds, proxy_url\n+\n+    def process_request(self, request, spider):\n+        # ignore if proxy is already seted\n+        if 'proxy' in request.meta:\n+            return\n+\n+        parsed = urlparse_cached(request)\n+        scheme = parsed.scheme\n+\n+        # 'no_proxy' is only supported by http schemes\n+        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n+            return\n+\n+        if scheme in self.proxies:\n+            self._set_proxy(request, scheme)\n+\n+    def _set_proxy(self, request, scheme):\n+        creds, proxy = self.proxies[scheme]\n+        request.meta['proxy'] = proxy\n+        if creds:\n+            request.headers['Proxy-Authorization'] = 'Basic ' + creds\n\n@@ -112,6 +112,10 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     def _set_connection_attributes(self, request):\n         parsed = urlparse_cached(request)\n         self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n+        proxy = request.meta.get('proxy')\n+        if proxy:\n+            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n+            self.path = self.url\n \n     def gotHeaders(self, headers):\n         self.response_headers = headers\n\n@@ -103,3 +103,47 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('broken'))\n         d = download_http(request, BaseSpider())\n         return self.assertFailure(d, PartialDownloadError)\n+\n+\n+class UriResource(resource.Resource):\n+    \"\"\"Return the full uri that was requested\"\"\"\n+\n+    def getChild(self, path, request):\n+        return self\n+\n+    def render(self, request):\n+        return request.uri\n+\n+\n+class HttpProxyTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        site = server.Site(UriResource(), timeout=None)\n+        wrapper = WrappingFactory(site)\n+        self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n+        self.portno = self.port.getHost().port\n+\n+    def tearDown(self):\n+        return self.port.stopListening()\n+\n+    def getURL(self, path):\n+        return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n+\n+    def test_download_with_proxy(self):\n+        def _test(response):\n+            self.assertEquals(response.status, 200)\n+            self.assertEquals(response.url, request.url)\n+            self.assertEquals(response.body, 'https://example.com')\n+\n+        http_proxy = self.getURL('')\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        return download_http(request, BaseSpider()).addCallback(_test)\n+\n+    def test_download_without_proxy(self):\n+        def _test(response):\n+            self.assertEquals(response.status, 200)\n+            self.assertEquals(response.url, request.url)\n+            self.assertEquals(response.body, '/path/to/resource')\n+\n+        request = Request(self.getURL('path/to/resource'))\n+        return download_http(request, BaseSpider()).addCallback(_test)\n\n@@ -0,0 +1,81 @@\n+import os\n+from unittest import TestCase\n+\n+from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.http import Response, Request\n+from scrapy.spider import BaseSpider\n+from scrapy.conf import settings\n+\n+spider = BaseSpider()\n+\n+class TestDefaultHeadersMiddleware(TestCase):\n+\n+    def setUp(self):\n+        self._oldenv = os.environ.copy()\n+\n+    def tearDown(self):\n+        os.environ = self._oldenv\n+\n+    def test_no_proxies(self):\n+        os.environ = {}\n+        self.assertRaises(NotConfigured, HttpProxyMiddleware)\n+\n+    def test_no_enviroment_proxies(self):\n+        os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n+        mw = HttpProxyMiddleware()\n+\n+        for url in ('http://e.com', 'https://e.com', 'file:///tmp/a'):\n+            req = Request(url)\n+            assert mw.process_request(req, spider) is None\n+            self.assertEquals(req.url, url)\n+            self.assertEquals(req.meta, {})\n+\n+    def test_enviroment_proxies(self):\n+        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n+        os.environ['https_proxy'] = https_proxy = 'http://proxy.for.https:8080'\n+        os.environ.pop('file_proxy', None)\n+        mw = HttpProxyMiddleware()\n+\n+        for url, proxy in [('http://e.com', http_proxy),\n+                ('https://e.com', https_proxy), ('file://tmp/a', None)]:\n+            req = Request(url)\n+            assert mw.process_request(req, spider) is None\n+            self.assertEquals(req.url, url)\n+            self.assertEquals(req.meta.get('proxy'), proxy)\n+\n+    def test_proxy_auth(self):\n+        os.environ['http_proxy'] = 'https://user:pass@proxy:3128'\n+        mw = HttpProxyMiddleware()\n+        req = Request('http://scrapytest.org')\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjpwYXNz')\n+\n+    def test_proxy_already_seted(self):\n+        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n+        mw = HttpProxyMiddleware()\n+        req = Request('http://noproxy.com', meta={'proxy': None})\n+        assert mw.process_request(req, spider) is None\n+        assert 'proxy' in req.meta and req.meta['proxy'] is None\n+\n+\n+    def test_no_proxy(self):\n+        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n+        mw = HttpProxyMiddleware()\n+\n+        os.environ['no_proxy'] = '*'\n+        req = Request('http://noproxy.com')\n+        assert mw.process_request(req, spider) is None\n+        assert 'proxy' not in req.meta\n+\n+        os.environ['no_proxy'] = 'other.com'\n+        req = Request('http://noproxy.com')\n+        assert mw.process_request(req, spider) is None\n+        assert 'proxy' in req.meta\n+\n+        os.environ['no_proxy'] = 'other.com,noproxy.com'\n+        req = Request('http://noproxy.com')\n+        assert mw.process_request(req, spider) is None\n+        assert 'proxy' not in req.meta\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1450af257ab516fa689a71ec0411794dd0abeef2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 47 | Churn Cumulative: 1695 | Contributors (this commit): 4 | Commits (past 90d): 82 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,6 +101,7 @@ ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n ITEM_PIPELINES = []\n \n LOG_ENABLED = True\n+LOG_FORMATTER_CRAWLED = 'scrapy.contrib.logformatter.crawled_logline'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n\n@@ -0,0 +1,7 @@\n+\"\"\"Functions for logging diferent actions\"\"\"\n+\n+def crawled_logline(request, response):\n+    referer = request.headers.get('Referer')\n+    flags = ' %s' % str(response.flags) if response.flags else ''\n+    return \"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n+        request, referer, flags)\n\n@@ -34,6 +34,7 @@ class ExecutionEngine(object):\n         self.paused = False\n         self._next_request_pending = set()\n         self._mainloop_task = task.LoopingCall(self._mainloop)\n+        self._crawled_logline = load_object(settings['LOG_FORMATTER_CRAWLED'])\n \n     def configure(self):\n         \"\"\"\n@@ -198,15 +199,14 @@ class ExecutionEngine(object):\n \n     def download(self, request, spider):\n         domain = spider.domain_name\n-        referer = request.headers.get('Referer')\n \n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(\"Crawled %s (referer: <%s>)\" % (request, referer), \\\n-                    level=log.DEBUG, domain=domain)\n+                log.msg(self._crawled_logline(request, response), \\\n+                    level=log.DEBUG, domain=spider.domain_name)\n                 return response\n             elif isinstance(response, Request):\n                 newrequest = response\n@@ -224,8 +224,8 @@ class ExecutionEngine(object):\n                 errmsg = str(_failure)\n                 level = log.ERROR\n             if errmsg:\n-                log.msg(\"Downloading <%s> (referer: <%s>): %s\" % (request.url, \\\n-                    referer, errmsg), level=level, domain=domain)\n+                log.msg(\"Crawling <%s>: %s\" % (request.url, errmsg), \\\n+                    level=level, domain=domain)\n             return Failure(IgnoreRequest(str(exc)))\n \n         def _on_complete(_):\n\n@@ -85,9 +85,6 @@ class Request(object_ref):\n         return self._encoding\n \n     def __str__(self):\n-        if self.method == 'GET':\n-            return \"<%s>\" % self.url\n-        else:\n         return \"<%s %s>\" % (self.method, self.url)\n \n     def __repr__(self):\n\n@@ -65,9 +65,7 @@ class Response(object_ref):\n         return \"%s(%s)\" % (self.__class__.__name__, args)\n \n     def __str__(self):\n-        flags = \"(%s) \" % \",\".join(self.flags) if self.flags else \"\"\n-        status = \"%d \" % self.status + \" \" if self.status != 200 else \"\"\n-        return \"%s<%s%s>\" % (flags, status, self.url)\n+        return \"<%d %s>\" % (self.status, self.url)\n \n     def copy(self):\n         \"\"\"Return a copy of this Response\"\"\"\n\n@@ -0,0 +1,22 @@\n+import unittest\n+\n+from scrapy.http import Request, Response\n+from scrapy.contrib.logformatter import crawled_logline\n+\n+\n+class LoggingContribTest(unittest.TestCase):\n+\n+    def test_crawled_logline(self):\n+        req = Request(\"http://www.example.com\")\n+        res = Response(\"http://www.example.com\")\n+        self.assertEqual(crawled_logline(req, res),\n+            \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n+\n+        req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n+        res = Response(\"http://www.example.com\", flags=['cached'])\n+        self.assertEqual(crawled_logline(req, res),\n+            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
