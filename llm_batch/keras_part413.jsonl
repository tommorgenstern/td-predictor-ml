{"custom_id": "keras#622d125d5f145dd6809da0646f57c644103e62d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 8 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 29/15 | Churn Δ: 20 | Churn Cumulative: 2453 | Contributors (this commit): 14 | Commits (past 90d): 18 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -20,8 +20,8 @@ from keras.engine import sequential\n from keras.layers import core\n from keras.optimizer_v2 import adagrad\n from keras.optimizer_v2 import gradient_descent\n-from keras.premade import linear\n-from keras.premade import wide_deep\n+from keras.premade_models import linear\n+from keras.premade_models import wide_deep\n from keras.utils import dataset_creator\n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -112,8 +112,8 @@ def populate_deserializable_objects():\n \n   # Prevent circular dependencies.\n   from keras import models  # pylint: disable=g-import-not-at-top\n-  from keras.premade.linear import LinearModel  # pylint: disable=g-import-not-at-top\n-  from keras.premade.wide_deep import WideDeepModel  # pylint: disable=g-import-not-at-top\n+  from keras.premade_models.linear import LinearModel  # pylint: disable=g-import-not-at-top\n+  from keras.premade_models.wide_deep import WideDeepModel  # pylint: disable=g-import-not-at-top\n   from keras.feature_column.sequence_feature_column import SequenceFeatures  # pylint: disable=g-import-not-at-top\n \n   LOCAL.ALL_OBJECTS['Input'] = input_layer.Input\n\n@@ -14,5 +14,5 @@\n # ==============================================================================\n \"\"\"Premade Model API.\"\"\"\n \n-from keras.premade import linear\n-from keras.premade import wide_deep\n+from keras.premade_models import linear\n+from keras.premade_models import wide_deep\n\n\n@@ -26,7 +26,7 @@ from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n from keras.optimizer_v2 import gradient_descent\n-from keras.premade import linear\n+from keras.premade_models import linear\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n\n\n@@ -25,8 +25,8 @@ from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n from keras.optimizer_v2 import gradient_descent\n-from keras.premade import linear\n-from keras.premade import wide_deep\n+from keras.premade_models import linear\n+from keras.premade_models import wide_deep\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n\n@@ -38,7 +38,7 @@ from keras.engine import sequential\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n from keras.layers import core\n-from keras.premade.linear import LinearModel\n+from keras.premade_models.linear import LinearModel\n from keras.saving import model_config\n from keras.saving import save\n from keras.utils import generic_utils\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c06d3d133469eb8adab5ee89a30dd8659865d303", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 842 | Lines Deleted: 569 | Files Changed: 21 | Hunks: 35 | Methods Changed: 66 | Complexity Δ (Sum/Max): 17/12 | Churn Δ: 1411 | Churn Cumulative: 9920 | Contributors (this commit): 61 | Commits (past 90d): 52 | Contributors (cumulative): 113 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.layers import advanced_activations\n+import keras.layers.activation as activation_layers\n from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n from tensorflow.python.util.tf_export import keras_export\n@@ -547,8 +547,8 @@ def deserialize(name, custom_objects=None):\n   globs = globals()\n \n   # only replace missing activations\n-  advanced_activations_globs = advanced_activations.get_globals()\n-  for key, val in advanced_activations_globs.items():\n+  activation_globs = activation_layers.get_globals()\n+  for key, val in activation_globs.items():\n     if key not in globs:\n       globs[key] = val\n \n\n@@ -22,7 +22,7 @@ import numpy as np\n from keras import activations\n from keras import backend\n from keras import combinations\n-from keras.layers import advanced_activations\n+import keras.layers.activation as activation_layers\n from keras.layers import core\n from keras.layers import serialization\n \n@@ -58,7 +58,7 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n       assert fn.__name__ == activation_map[fn_v2_key]\n \n   def test_serialization_with_layers(self):\n-    activation = advanced_activations.LeakyReLU(alpha=0.1)\n+    activation = activation_layers.LeakyReLU(alpha=0.1)\n     layer = core.Dense(3, activation=activation)\n     config = serialization.serialize(layer)\n     # with custom objects\n@@ -255,7 +255,7 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n       activations.get('unknown')\n \n     # The following should be possible but should raise a warning:\n-    activations.get(advanced_activations.LeakyReLU())\n+    activations.get(activation_layers.LeakyReLU())\n \n \n if __name__ == '__main__':\n\n@@ -29,7 +29,7 @@ from keras import activations\n from keras import backend\n from keras import combinations\n from keras.engine import input_layer\n-from keras.layers import advanced_activations\n+from keras.layers import activation\n from keras.layers.normalization import batch_normalization_v1\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n@@ -557,7 +557,7 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n \n     # Test case for GitHub issue 35430, with integer dtype\n     x = input_layer.Input(shape=(), name='x', dtype='int64')\n-    _ = advanced_activations.ReLU(max_value=100., dtype='int64')(x)\n+    _ = activation.ReLU(max_value=100., dtype='int64')(x)\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n@@ -49,13 +49,13 @@ from keras.layers.preprocessing.normalization import Normalization\n from keras.layers.preprocessing.string_lookup import StringLookup\n from keras.layers.preprocessing.text_vectorization import TextVectorization\n \n-# Advanced activations.\n-from keras.layers.advanced_activations import LeakyReLU\n-from keras.layers.advanced_activations import PReLU\n-from keras.layers.advanced_activations import ELU\n-from keras.layers.advanced_activations import ReLU\n-from keras.layers.advanced_activations import ThresholdedReLU\n-from keras.layers.advanced_activations import Softmax\n+# Activations layers.\n+from keras.layers.activation.relu import ReLU\n+from keras.layers.activation.softmax import Softmax\n+from keras.layers.activation.leaky_relu import LeakyReLU\n+from keras.layers.activation.prelu import PReLU\n+from keras.layers.activation.elu import ELU\n+from keras.layers.activation.thresholded_relu import ThresholdedReLU\n \n # Attention layers.\n from keras.layers.attention.multi_head_attention import MultiHeadAttention\n\n@@ -0,0 +1,27 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layers that act as activation functions.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+from keras.layers.activation.relu import ReLU\n+from keras.layers.activation.softmax import Softmax\n+from keras.layers.activation.leaky_relu import LeakyReLU\n+from keras.layers.activation.prelu import PReLU\n+from keras.layers.activation.elu import ELU\n+from keras.layers.activation.thresholded_relu import ThresholdedReLU\n+\n+\n+def get_globals():\n+  return globals()\n\n@@ -0,0 +1,67 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Exponential Linear Unit activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ELU')\n+class ELU(Layer):\n+  \"\"\"Exponential Linear Unit.\n+\n+  It follows:\n+\n+  ```\n+    f(x) =  alpha * (exp(x) - 1.) for x < 0\n+    f(x) = x for x >= 0\n+  ```\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    alpha: Scale for the negative factor.\n+  \"\"\"\n+\n+  def __init__(self, alpha=1.0, **kwargs):\n+    super(ELU, self).__init__(**kwargs)\n+    if alpha is None:\n+      raise ValueError(\n+          'Alpha of an ELU layer cannot be None, expecting a float. '\n+          f'Received: {alpha}')\n+    self.supports_masking = True\n+    self.alpha = backend.cast_to_floatx(alpha)\n+\n+  def call(self, inputs):\n+    return backend.elu(inputs, self.alpha)\n+\n+  def get_config(self):\n+    config = {'alpha': float(self.alpha)}\n+    base_config = super(ELU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,46 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for ELU layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ELUTest(keras_parameterized.TestCase):\n+\n+  def test_elu(self):\n+    for alpha in [0., .5, -1.]:\n+      testing_utils.layer_test(keras.layers.ELU,\n+                               kwargs={'alpha': alpha},\n+                               input_shape=(2, 3, 4),\n+                               supports_masking=True)\n+\n+  def test_elu_with_invalid_alpha(self):\n+    # Test case for GitHub issue 46993.\n+    with self.assertRaisesRegex(\n+        ValueError, 'Alpha of an ELU layer cannot be None, '\n+        'expecting a float. Received: None'):\n+      testing_utils.layer_test(\n+          keras.layers.ELU,\n+          kwargs={'alpha': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,79 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Leaky version of a Rectified Linear Unit activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.LeakyReLU')\n+class LeakyReLU(Layer):\n+  \"\"\"Leaky version of a Rectified Linear Unit.\n+\n+  It allows a small gradient when the unit is not active:\n+\n+  ```\n+    f(x) = alpha * x if x < 0\n+    f(x) = x if x >= 0\n+  ```\n+\n+  Usage:\n+\n+  >>> layer = tf.keras.layers.LeakyReLU()\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [-0.9, -0.3, 0.0, 2.0]\n+  >>> layer = tf.keras.layers.LeakyReLU(alpha=0.1)\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [-0.3, -0.1, 0.0, 2.0]\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the batch axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    alpha: Float >= 0. Negative slope coefficient. Default to 0.3.\n+\n+  \"\"\"\n+\n+  def __init__(self, alpha=0.3, **kwargs):\n+    super(LeakyReLU, self).__init__(**kwargs)\n+    if alpha is None:\n+      raise ValueError(\n+          'The alpha value of a Leaky ReLU layer cannot be None, '\n+          f'Expecting a float. Received: {alpha}')\n+    self.supports_masking = True\n+    self.alpha = backend.cast_to_floatx(alpha)\n+\n+  def call(self, inputs):\n+    return backend.relu(inputs, alpha=self.alpha)\n+\n+  def get_config(self):\n+    config = {'alpha': float(self.alpha)}\n+    base_config = super(LeakyReLU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,46 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for LeakyReLU layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class LeakyReLUTest(keras_parameterized.TestCase):\n+\n+  def test_leaky_relu(self):\n+    for alpha in [0., .5]:\n+      testing_utils.layer_test(keras.layers.LeakyReLU,\n+                               kwargs={'alpha': alpha},\n+                               input_shape=(2, 3, 4),\n+                               supports_masking=True)\n+\n+  def test_leaky_relu_with_invalid_alpha(self):\n+    # Test case for GitHub issue 46993.\n+    with self.assertRaisesRegex(\n+        ValueError, 'The alpha value of a Leaky ReLU layer '\n+        'cannot be None. Expecting a float. Received: None'):\n+      testing_utils.layer_test(\n+          keras.layers.LeakyReLU,\n+          kwargs={'alpha': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,120 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Parametric Rectified Linear Unit activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.PReLU')\n+class PReLU(Layer):\n+  \"\"\"Parametric Rectified Linear Unit.\n+\n+  It follows:\n+\n+  ```\n+    f(x) = alpha * x for x < 0\n+    f(x) = x for x >= 0\n+  ```\n+\n+  where `alpha` is a learned array with the same shape as x.\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    alpha_initializer: Initializer function for the weights.\n+    alpha_regularizer: Regularizer for the weights.\n+    alpha_constraint: Constraint for the weights.\n+    shared_axes: The axes along which to share learnable\n+      parameters for the activation function.\n+      For example, if the incoming feature maps\n+      are from a 2D convolution\n+      with output shape `(batch, height, width, channels)`,\n+      and you wish to share parameters across space\n+      so that each filter only has one set of parameters,\n+      set `shared_axes=[1, 2]`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               alpha_initializer='zeros',\n+               alpha_regularizer=None,\n+               alpha_constraint=None,\n+               shared_axes=None,\n+               **kwargs):\n+    super(PReLU, self).__init__(**kwargs)\n+    self.supports_masking = True\n+    self.alpha_initializer = initializers.get(alpha_initializer)\n+    self.alpha_regularizer = regularizers.get(alpha_regularizer)\n+    self.alpha_constraint = constraints.get(alpha_constraint)\n+    if shared_axes is None:\n+      self.shared_axes = None\n+    elif not isinstance(shared_axes, (list, tuple)):\n+      self.shared_axes = [shared_axes]\n+    else:\n+      self.shared_axes = list(shared_axes)\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    param_shape = list(input_shape[1:])\n+    if self.shared_axes is not None:\n+      for i in self.shared_axes:\n+        param_shape[i - 1] = 1\n+    self.alpha = self.add_weight(\n+        shape=param_shape,\n+        name='alpha',\n+        initializer=self.alpha_initializer,\n+        regularizer=self.alpha_regularizer,\n+        constraint=self.alpha_constraint)\n+    # Set input spec\n+    axes = {}\n+    if self.shared_axes:\n+      for i in range(1, len(input_shape)):\n+        if i not in self.shared_axes:\n+          axes[i] = input_shape[i]\n+    self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\n+    self.built = True\n+\n+  def call(self, inputs):\n+    pos = backend.relu(inputs)\n+    neg = -self.alpha * backend.relu(-inputs)\n+    return pos + neg\n+\n+  def get_config(self):\n+    config = {\n+        'alpha_initializer': initializers.serialize(self.alpha_initializer),\n+        'alpha_regularizer': regularizers.serialize(self.alpha_regularizer),\n+        'alpha_constraint': constraints.serialize(self.alpha_constraint),\n+        'shared_axes': self.shared_axes\n+    }\n+    base_config = super(PReLU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,39 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for PReLU layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class PReLUTest(keras_parameterized.TestCase):\n+\n+  def test_prelu(self):\n+    testing_utils.layer_test(keras.layers.PReLU, kwargs={},\n+                             input_shape=(2, 3, 4),\n+                             supports_masking=True)\n+\n+  def test_prelu_share(self):\n+    testing_utils.layer_test(keras.layers.PReLU,\n+                             kwargs={'shared_axes': 1},\n+                             input_shape=(2, 3, 4),\n+                             supports_masking=True)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,112 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Rectified Linear Unit activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ReLU')\n+class ReLU(Layer):\n+  \"\"\"Rectified Linear Unit activation function.\n+\n+  With default values, it returns element-wise `max(x, 0)`.\n+\n+  Otherwise, it follows:\n+\n+  ```\n+    f(x) = max_value if x >= max_value\n+    f(x) = x if threshold <= x < max_value\n+    f(x) = negative_slope * (x - threshold) otherwise\n+  ```\n+\n+  Usage:\n+\n+  >>> layer = tf.keras.layers.ReLU()\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [0.0, 0.0, 0.0, 2.0]\n+  >>> layer = tf.keras.layers.ReLU(max_value=1.0)\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [0.0, 0.0, 0.0, 1.0]\n+  >>> layer = tf.keras.layers.ReLU(negative_slope=1.0)\n+  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+  >>> list(output.numpy())\n+  [-3.0, -1.0, 0.0, 2.0]\n+  >>> layer = tf.keras.layers.ReLU(threshold=1.5)\n+  >>> output = layer([-3.0, -1.0, 1.0, 2.0])\n+  >>> list(output.numpy())\n+  [0.0, 0.0, 0.0, 2.0]\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the batch axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    max_value: Float >= 0. Maximum activation value. Default to None, which\n+      means unlimited.\n+    negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n+    threshold: Float >= 0. Threshold value for thresholded activation. Default\n+      to 0.\n+  \"\"\"\n+\n+  def __init__(self, max_value=None, negative_slope=0., threshold=0., **kwargs):\n+    super(ReLU, self).__init__(**kwargs)\n+    if max_value is not None and max_value < 0.:\n+      raise ValueError('max_value of a ReLU layer cannot be a negative '\n+                       f'value. Received: {max_value}')\n+    if negative_slope is None or negative_slope < 0.:\n+      raise ValueError('negative_slope of a ReLU layer cannot be a negative '\n+                       f'value. Received: {negative_slope}')\n+    if threshold is None or threshold < 0.:\n+      raise ValueError('threshold of a ReLU layer cannot be a negative '\n+                       f'value. Received: {threshold}')\n+\n+    self.supports_masking = True\n+    if max_value is not None:\n+      max_value = backend.cast_to_floatx(max_value)\n+    self.max_value = max_value\n+    self.negative_slope = backend.cast_to_floatx(negative_slope)\n+    self.threshold = backend.cast_to_floatx(threshold)\n+\n+  def call(self, inputs):\n+    # alpha is used for leaky relu slope in activations instead of\n+    # negative_slope.\n+    return backend.relu(inputs,\n+                        alpha=self.negative_slope,\n+                        max_value=self.max_value,\n+                        threshold=self.threshold)\n+\n+  def get_config(self):\n+    config = {\n+        'max_value': self.max_value,\n+        'negative_slope': self.negative_slope,\n+        'threshold': self.threshold\n+    }\n+    base_config = super(ReLU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -12,56 +12,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for advanced activation layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+\"\"\"Tests for ReLU layer.\"\"\"\n \n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes\n-class AdvancedActivationsTest(keras_parameterized.TestCase):\n-\n-  def test_leaky_relu(self):\n-    for alpha in [0., .5]:\n-      testing_utils.layer_test(keras.layers.LeakyReLU,\n-                               kwargs={'alpha': alpha},\n-                               input_shape=(2, 3, 4),\n-                               supports_masking=True)\n-\n-  def test_prelu(self):\n-    testing_utils.layer_test(keras.layers.PReLU, kwargs={},\n-                             input_shape=(2, 3, 4),\n-                             supports_masking=True)\n-\n-  def test_prelu_share(self):\n-    testing_utils.layer_test(keras.layers.PReLU,\n-                             kwargs={'shared_axes': 1},\n-                             input_shape=(2, 3, 4),\n-                             supports_masking=True)\n-\n-  def test_elu(self):\n-    for alpha in [0., .5, -1.]:\n-      testing_utils.layer_test(keras.layers.ELU,\n-                               kwargs={'alpha': alpha},\n-                               input_shape=(2, 3, 4),\n-                               supports_masking=True)\n-\n-  def test_thresholded_relu(self):\n-    testing_utils.layer_test(keras.layers.ThresholdedReLU,\n-                             kwargs={'theta': 0.5},\n-                             input_shape=(2, 3, 4),\n-                             supports_masking=True)\n-\n-  def test_softmax(self):\n-    testing_utils.layer_test(keras.layers.Softmax,\n-                             kwargs={'axis': 1},\n-                             input_shape=(2, 3, 4),\n-                             supports_masking=True)\n+class ReLUTest(keras_parameterized.TestCase):\n \n   def test_relu(self):\n     testing_utils.layer_test(keras.layers.ReLU,\n@@ -71,12 +32,11 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n     x = keras.backend.ones((3, 4))\n     if not tf.executing_eagerly():\n       # Test that we use `leaky_relu` when appropriate in graph mode.\n-      self.assertTrue(\n-          'LeakyRelu' in keras.layers.ReLU(negative_slope=0.2)(x).name)\n+      self.assertIn('LeakyRelu', keras.layers.ReLU(negative_slope=0.2)(x).name)\n       # Test that we use `relu` when appropriate in graph mode.\n-      self.assertTrue('Relu' in keras.layers.ReLU()(x).name)\n+      self.assertIn('Relu', keras.layers.ReLU()(x).name)\n       # Test that we use `relu6` when appropriate in graph mode.\n-      self.assertTrue('Relu6' in keras.layers.ReLU(max_value=6)(x).name)\n+      self.assertIn('Relu6', keras.layers.ReLU(max_value=6)(x).name)\n \n   def test_relu_with_invalid_max_value(self):\n     with self.assertRaisesRegex(\n@@ -127,7 +87,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n           supports_masking=True)\n \n   @keras_parameterized.run_with_all_model_types\n-  def test_layer_as_activation(self):\n+  def test_relu_layer_as_activation(self):\n     layer = keras.layers.Dense(1, activation=keras.layers.ReLU())\n     model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n@@ -136,47 +96,6 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(np.ones((10, 10)), np.ones((10, 1)), batch_size=2)\n \n-  def test_leaky_relu_with_invalid_alpha(self):\n-    # Test case for GitHub issue 46993.\n-    with self.assertRaisesRegex(\n-        ValueError, 'The alpha value of a Leaky ReLU layer '\n-        'cannot be None. Expecting a float. Received: None'):\n-      testing_utils.layer_test(\n-          keras.layers.LeakyReLU,\n-          kwargs={'alpha': None},\n-          input_shape=(2, 3, 4),\n-          supports_masking=True)\n-\n-  def test_leaky_elu_with_invalid_alpha(self):\n-    # Test case for GitHub issue 46993.\n-    with self.assertRaisesRegex(\n-        ValueError, 'Alpha of an ELU layer cannot be None, '\n-        'expecting a float. Received: None'):\n-      testing_utils.layer_test(\n-          keras.layers.ELU,\n-          kwargs={'alpha': None},\n-          input_shape=(2, 3, 4),\n-          supports_masking=True)\n-\n-  def test_threshold_relu_with_invalid_theta(self):\n-    with self.assertRaisesRegex(\n-        ValueError, 'Theta of a Thresholded ReLU layer cannot '\n-        'be None, expecting a float. Received: None'):\n-      testing_utils.layer_test(\n-          keras.layers.ThresholdedReLU,\n-          kwargs={'theta': None},\n-          input_shape=(2, 3, 4),\n-          supports_masking=True)\n-\n-    with self.assertRaisesRegex(\n-        ValueError, 'The theta value of a Thresholded ReLU '\n-        'layer should be >=0. Received: -10'):\n-      testing_utils.layer_test(\n-          keras.layers.ThresholdedReLU,\n-          kwargs={'theta': -10},\n-          input_shape=(2, 3, 4),\n-          supports_masking=True)\n-\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -0,0 +1,108 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Softmax activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+def _large_compatible_negative(tensor_type):\n+  \"\"\"Large negative number as Tensor.\n+\n+  This function is necessary because the standard value for epsilon\n+  in this module (-1e9) cannot be represented using tf.float16\n+\n+  Args:\n+    tensor_type: a dtype to determine the type.\n+\n+  Returns:\n+    a large negative number.\n+  \"\"\"\n+  if tensor_type == tf.float16:\n+    return tf.float16.min\n+  return -1e9\n+\n+\n+@keras_export('keras.layers.Softmax')\n+class Softmax(Layer):\n+  \"\"\"Softmax activation function.\n+\n+  Example without mask:\n+\n+  >>> inp = np.asarray([1., 2., 1.])\n+  >>> layer = tf.keras.layers.Softmax()\n+  >>> layer(inp).numpy()\n+  array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)\n+  >>> mask = np.asarray([True, False, True], dtype=bool)\n+  >>> layer(inp, mask).numpy()\n+  array([0.5, 0. , 0.5], dtype=float32)\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    axis: Integer, or list of Integers, axis along which the softmax\n+      normalization is applied.\n+  Call arguments:\n+    inputs: The inputs, or logits to the softmax layer.\n+    mask: A boolean mask of the same shape as `inputs`. Defaults to `None`. The\n+      mask specifies 1 to keep and 0 to mask.\n+\n+  Returns:\n+    softmaxed output with the same shape as `inputs`.\n+  \"\"\"\n+\n+  def __init__(self, axis=-1, **kwargs):\n+    super(Softmax, self).__init__(**kwargs)\n+    self.supports_masking = True\n+    self.axis = axis\n+\n+  def call(self, inputs, mask=None):\n+    if mask is not None:\n+      # Since mask is 1.0 for positions we want to keep and 0.0 for\n+      # masked positions, this operation will create a tensor which is 0.0 for\n+      # positions we want to attend and -1e.9 for masked positions.\n+      adder = (1.0 - tf.cast(mask, inputs.dtype)) * (\n+          _large_compatible_negative(inputs.dtype))\n+\n+      # Since we are adding it to the raw scores before the softmax, this is\n+      # effectively the same as removing these entirely.\n+      inputs += adder\n+    if isinstance(self.axis, (tuple, list)):\n+      if len(self.axis) > 1:\n+        return tf.exp(inputs - tf.reduce_logsumexp(\n+            inputs, axis=self.axis, keepdims=True))\n+      else:\n+        return backend.softmax(inputs, axis=self.axis[0])\n+    return backend.softmax(inputs, axis=self.axis)\n+\n+  def get_config(self):\n+    config = {'axis': self.axis}\n+    base_config = super(Softmax, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,34 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Softmax layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class SoftmaxTest(keras_parameterized.TestCase):\n+\n+  def test_softmax(self):\n+    testing_utils.layer_test(keras.layers.Softmax,\n+                             kwargs={'axis': 1},\n+                             input_shape=(2, 3, 4),\n+                             supports_masking=True)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,72 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Thresholded Rectified Linear Unit activation layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ThresholdedReLU')\n+class ThresholdedReLU(Layer):\n+  \"\"\"Thresholded Rectified Linear Unit.\n+\n+  It follows:\n+\n+  ```\n+    f(x) = x for x > theta\n+    f(x) = 0 otherwise`\n+  ```\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as the input.\n+\n+  Args:\n+    theta: Float >= 0. Threshold location of activation.\n+  \"\"\"\n+\n+  def __init__(self, theta=1.0, **kwargs):\n+    super(ThresholdedReLU, self).__init__(**kwargs)\n+    if theta is None:\n+      raise ValueError(\n+          'Theta of a Thresholded ReLU layer cannot be None, expecting a float.'\n+          f' Received: {theta}')\n+    if theta < 0:\n+      raise ValueError('The theta value of a Thresholded ReLU layer '\n+                       f'should be >=0. Received: {theta}')\n+    self.supports_masking = True\n+    self.theta = backend.cast_to_floatx(theta)\n+\n+  def call(self, inputs):\n+    theta = tf.cast(self.theta, inputs.dtype)\n+    return inputs * tf.cast(tf.greater(inputs, theta), inputs.dtype)\n+\n+  def get_config(self):\n+    config = {'theta': float(self.theta)}\n+    base_config = super(ThresholdedReLU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,53 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for ThresholdedReLU layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ThresholdedReLUTest(keras_parameterized.TestCase):\n+\n+  def test_thresholded_relu(self):\n+    testing_utils.layer_test(keras.layers.ThresholdedReLU,\n+                             kwargs={'theta': 0.5},\n+                             input_shape=(2, 3, 4),\n+                             supports_masking=True)\n+\n+  def test_threshold_relu_with_invalid_theta(self):\n+    with self.assertRaisesRegex(\n+        ValueError, 'Theta of a Thresholded ReLU layer cannot '\n+        'be None, expecting a float. Received: None'):\n+      testing_utils.layer_test(\n+          keras.layers.ThresholdedReLU,\n+          kwargs={'theta': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'The theta value of a Thresholded ReLU '\n+        'layer should be >=0. Received: -10'):\n+      testing_utils.layer_test(\n+          keras.layers.ThresholdedReLU,\n+          kwargs={'theta': -10},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -1,449 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Layers that act as activation functions.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-from keras import backend\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine.base_layer import Layer\n-from keras.engine.input_spec import InputSpec\n-from keras.utils import tf_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-def get_globals():\n-  return globals()\n-\n-\n-@keras_export('keras.layers.LeakyReLU')\n-class LeakyReLU(Layer):\n-  \"\"\"Leaky version of a Rectified Linear Unit.\n-\n-  It allows a small gradient when the unit is not active:\n-\n-  ```\n-    f(x) = alpha * x if x < 0\n-    f(x) = x if x >= 0\n-  ```\n-\n-  Usage:\n-\n-  >>> layer = tf.keras.layers.LeakyReLU()\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [-0.9, -0.3, 0.0, 2.0]\n-  >>> layer = tf.keras.layers.LeakyReLU(alpha=0.1)\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [-0.3, -0.1, 0.0, 2.0]\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the batch axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    alpha: Float >= 0. Negative slope coefficient. Default to 0.3.\n-\n-  \"\"\"\n-\n-  def __init__(self, alpha=0.3, **kwargs):\n-    super(LeakyReLU, self).__init__(**kwargs)\n-    if alpha is None:\n-      raise ValueError(\n-          'The alpha value of a Leaky ReLU layer cannot be None, '\n-          f'Expecting a float. Received: {alpha}')\n-    self.supports_masking = True\n-    self.alpha = backend.cast_to_floatx(alpha)\n-\n-  def call(self, inputs):\n-    return backend.relu(inputs, alpha=self.alpha)\n-\n-  def get_config(self):\n-    config = {'alpha': float(self.alpha)}\n-    base_config = super(LeakyReLU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.PReLU')\n-class PReLU(Layer):\n-  \"\"\"Parametric Rectified Linear Unit.\n-\n-  It follows:\n-\n-  ```\n-    f(x) = alpha * x for x < 0\n-    f(x) = x for x >= 0\n-  ```\n-\n-  where `alpha` is a learned array with the same shape as x.\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    alpha_initializer: Initializer function for the weights.\n-    alpha_regularizer: Regularizer for the weights.\n-    alpha_constraint: Constraint for the weights.\n-    shared_axes: The axes along which to share learnable\n-      parameters for the activation function.\n-      For example, if the incoming feature maps\n-      are from a 2D convolution\n-      with output shape `(batch, height, width, channels)`,\n-      and you wish to share parameters across space\n-      so that each filter only has one set of parameters,\n-      set `shared_axes=[1, 2]`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               alpha_initializer='zeros',\n-               alpha_regularizer=None,\n-               alpha_constraint=None,\n-               shared_axes=None,\n-               **kwargs):\n-    super(PReLU, self).__init__(**kwargs)\n-    self.supports_masking = True\n-    self.alpha_initializer = initializers.get(alpha_initializer)\n-    self.alpha_regularizer = regularizers.get(alpha_regularizer)\n-    self.alpha_constraint = constraints.get(alpha_constraint)\n-    if shared_axes is None:\n-      self.shared_axes = None\n-    elif not isinstance(shared_axes, (list, tuple)):\n-      self.shared_axes = [shared_axes]\n-    else:\n-      self.shared_axes = list(shared_axes)\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    param_shape = list(input_shape[1:])\n-    if self.shared_axes is not None:\n-      for i in self.shared_axes:\n-        param_shape[i - 1] = 1\n-    self.alpha = self.add_weight(\n-        shape=param_shape,\n-        name='alpha',\n-        initializer=self.alpha_initializer,\n-        regularizer=self.alpha_regularizer,\n-        constraint=self.alpha_constraint)\n-    # Set input spec\n-    axes = {}\n-    if self.shared_axes:\n-      for i in range(1, len(input_shape)):\n-        if i not in self.shared_axes:\n-          axes[i] = input_shape[i]\n-    self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\n-    self.built = True\n-\n-  def call(self, inputs):\n-    pos = backend.relu(inputs)\n-    neg = -self.alpha * backend.relu(-inputs)\n-    return pos + neg\n-\n-  def get_config(self):\n-    config = {\n-        'alpha_initializer': initializers.serialize(self.alpha_initializer),\n-        'alpha_regularizer': regularizers.serialize(self.alpha_regularizer),\n-        'alpha_constraint': constraints.serialize(self.alpha_constraint),\n-        'shared_axes': self.shared_axes\n-    }\n-    base_config = super(PReLU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.ELU')\n-class ELU(Layer):\n-  \"\"\"Exponential Linear Unit.\n-\n-  It follows:\n-\n-  ```\n-    f(x) =  alpha * (exp(x) - 1.) for x < 0\n-    f(x) = x for x >= 0\n-  ```\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    alpha: Scale for the negative factor.\n-  \"\"\"\n-\n-  def __init__(self, alpha=1.0, **kwargs):\n-    super(ELU, self).__init__(**kwargs)\n-    if alpha is None:\n-      raise ValueError(\n-          'Alpha of an ELU layer cannot be None, expecting a float. '\n-          f'Received: {alpha}')\n-    self.supports_masking = True\n-    self.alpha = backend.cast_to_floatx(alpha)\n-\n-  def call(self, inputs):\n-    return backend.elu(inputs, self.alpha)\n-\n-  def get_config(self):\n-    config = {'alpha': float(self.alpha)}\n-    base_config = super(ELU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.ThresholdedReLU')\n-class ThresholdedReLU(Layer):\n-  \"\"\"Thresholded Rectified Linear Unit.\n-\n-  It follows:\n-\n-  ```\n-    f(x) = x for x > theta\n-    f(x) = 0 otherwise`\n-  ```\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    theta: Float >= 0. Threshold location of activation.\n-  \"\"\"\n-\n-  def __init__(self, theta=1.0, **kwargs):\n-    super(ThresholdedReLU, self).__init__(**kwargs)\n-    if theta is None:\n-      raise ValueError(\n-          'Theta of a Thresholded ReLU layer cannot be None, expecting a float.'\n-          f' Received: {theta}')\n-    if theta < 0:\n-      raise ValueError('The theta value of a Thresholded ReLU layer '\n-                       f'should be >=0. Received: {theta}')\n-    self.supports_masking = True\n-    self.theta = backend.cast_to_floatx(theta)\n-\n-  def call(self, inputs):\n-    theta = tf.cast(self.theta, inputs.dtype)\n-    return inputs * tf.cast(tf.greater(inputs, theta), inputs.dtype)\n-\n-  def get_config(self):\n-    config = {'theta': float(self.theta)}\n-    base_config = super(ThresholdedReLU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-def _large_compatible_negative(tensor_type):\n-  \"\"\"Large negative number as Tensor.\n-\n-  This function is necessary because the standard value for epsilon\n-  in this module (-1e9) cannot be represented using tf.float16\n-\n-  Args:\n-    tensor_type: a dtype to determine the type.\n-\n-  Returns:\n-    a large negative number.\n-  \"\"\"\n-  if tensor_type == tf.float16:\n-    return tf.float16.min\n-  return -1e9\n-\n-\n-@keras_export('keras.layers.Softmax')\n-class Softmax(Layer):\n-  \"\"\"Softmax activation function.\n-\n-  Example without mask:\n-\n-  >>> inp = np.asarray([1., 2., 1.])\n-  >>> layer = tf.keras.layers.Softmax()\n-  >>> layer(inp).numpy()\n-  array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)\n-  >>> mask = np.asarray([True, False, True], dtype=bool)\n-  >>> layer(inp, mask).numpy()\n-  array([0.5, 0. , 0.5], dtype=float32)\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    axis: Integer, or list of Integers, axis along which the softmax\n-      normalization is applied.\n-  Call arguments:\n-    inputs: The inputs, or logits to the softmax layer.\n-    mask: A boolean mask of the same shape as `inputs`. Defaults to `None`. The\n-      mask specifies 1 to keep and 0 to mask.\n-\n-  Returns:\n-    softmaxed output with the same shape as `inputs`.\n-  \"\"\"\n-\n-  def __init__(self, axis=-1, **kwargs):\n-    super(Softmax, self).__init__(**kwargs)\n-    self.supports_masking = True\n-    self.axis = axis\n-\n-  def call(self, inputs, mask=None):\n-    if mask is not None:\n-      # Since mask is 1.0 for positions we want to keep and 0.0 for\n-      # masked positions, this operation will create a tensor which is 0.0 for\n-      # positions we want to attend and -1e.9 for masked positions.\n-      adder = (1.0 - tf.cast(mask, inputs.dtype)) * (\n-          _large_compatible_negative(inputs.dtype))\n-\n-      # Since we are adding it to the raw scores before the softmax, this is\n-      # effectively the same as removing these entirely.\n-      inputs += adder\n-    if isinstance(self.axis, (tuple, list)):\n-      if len(self.axis) > 1:\n-        return tf.exp(inputs - tf.reduce_logsumexp(\n-            inputs, axis=self.axis, keepdims=True))\n-      else:\n-        return backend.softmax(inputs, axis=self.axis[0])\n-    return backend.softmax(inputs, axis=self.axis)\n-\n-  def get_config(self):\n-    config = {'axis': self.axis}\n-    base_config = super(Softmax, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.ReLU')\n-class ReLU(Layer):\n-  \"\"\"Rectified Linear Unit activation function.\n-\n-  With default values, it returns element-wise `max(x, 0)`.\n-\n-  Otherwise, it follows:\n-\n-  ```\n-    f(x) = max_value if x >= max_value\n-    f(x) = x if threshold <= x < max_value\n-    f(x) = negative_slope * (x - threshold) otherwise\n-  ```\n-\n-  Usage:\n-\n-  >>> layer = tf.keras.layers.ReLU()\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [0.0, 0.0, 0.0, 2.0]\n-  >>> layer = tf.keras.layers.ReLU(max_value=1.0)\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [0.0, 0.0, 0.0, 1.0]\n-  >>> layer = tf.keras.layers.ReLU(negative_slope=1.0)\n-  >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-  >>> list(output.numpy())\n-  [-3.0, -1.0, 0.0, 2.0]\n-  >>> layer = tf.keras.layers.ReLU(threshold=1.5)\n-  >>> output = layer([-3.0, -1.0, 1.0, 2.0])\n-  >>> list(output.numpy())\n-  [0.0, 0.0, 0.0, 2.0]\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the batch axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as the input.\n-\n-  Args:\n-    max_value: Float >= 0. Maximum activation value. Default to None, which\n-      means unlimited.\n-    negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n-    threshold: Float >= 0. Threshold value for thresholded activation. Default\n-      to 0.\n-  \"\"\"\n-\n-  def __init__(self, max_value=None, negative_slope=0., threshold=0., **kwargs):\n-    super(ReLU, self).__init__(**kwargs)\n-    if max_value is not None and max_value < 0.:\n-      raise ValueError('max_value of a ReLU layer cannot be a negative '\n-                       f'value. Received: {max_value}')\n-    if negative_slope is None or negative_slope < 0.:\n-      raise ValueError('negative_slope of a ReLU layer cannot be a negative '\n-                       f'value. Received: {negative_slope}')\n-    if threshold is None or threshold < 0.:\n-      raise ValueError('threshold of a ReLU layer cannot be a negative '\n-                       f'value. Received: {threshold}')\n-\n-    self.supports_masking = True\n-    if max_value is not None:\n-      max_value = backend.cast_to_floatx(max_value)\n-    self.max_value = max_value\n-    self.negative_slope = backend.cast_to_floatx(negative_slope)\n-    self.threshold = backend.cast_to_floatx(threshold)\n-\n-  def call(self, inputs):\n-    # alpha is used for leaky relu slope in activations instead of\n-    # negative_slope.\n-    return backend.relu(inputs,\n-                        alpha=self.negative_slope,\n-                        max_value=self.max_value,\n-                        threshold=self.threshold)\n-\n-  def get_config(self):\n-    config = {\n-        'max_value': self.max_value,\n-        'negative_slope': self.negative_slope,\n-        'threshold': self.threshold\n-    }\n-    base_config = super(ReLU, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n\n@@ -24,7 +24,7 @@ from keras import constraints\n from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n-from keras.layers import advanced_activations\n+from keras.layers import activation\n from keras.layers import einsum_dense\n from keras.layers import regularization\n from keras.utils import tf_utils\n@@ -422,7 +422,7 @@ class MultiHeadAttention(Layer):\n         _build_attention_equation(rank, attn_axes=self._attention_axes))\n     norm_axes = tuple(\n         range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n-    self._softmax = advanced_activations.Softmax(axis=norm_axes)\n+    self._softmax = activation.Softmax(axis=norm_axes)\n     self._dropout_layer = regularization.Dropout(rate=self._dropout)\n \n   def _masked_softmax(self, attention_scores, attention_mask=None):\n\n@@ -21,7 +21,7 @@ import threading\n from keras.engine import base_layer\n from keras.engine import input_layer\n from keras.engine import input_spec\n-from keras.layers import advanced_activations\n+from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n from keras.layers import convolutional_recurrent\n@@ -57,12 +57,12 @@ from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n from tensorflow.python.util.tf_export import keras_export\n \n-ALL_MODULES = (base_layer, input_layer, advanced_activations, attention,\n-               convolutional, convolutional_recurrent, core, cudnn_recurrent,\n-               embeddings, einsum_dense, locally_connected, merging,\n-               batch_normalization_v1, layer_normalization, unit_normalization,\n-               pooling, image_preprocessing, recurrent, regularization,\n-               reshaping, wrappers, hashing, hashed_crossing, category_encoding,\n+ALL_MODULES = (base_layer, input_layer, activation, attention, convolutional,\n+               convolutional_recurrent, core, cudnn_recurrent, embeddings,\n+               einsum_dense, locally_connected, merging, batch_normalization_v1,\n+               layer_normalization, unit_normalization, pooling,\n+               image_preprocessing, recurrent, regularization, reshaping,\n+               wrappers, hashing, hashed_crossing, category_encoding,\n                discretization, integer_lookup, preprocessing_normalization,\n                string_lookup, text_vectorization)\n ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n\n@@ -22,7 +22,7 @@ from keras import keras_parameterized\n from keras import layers\n from keras import models\n from keras import testing_utils\n-from keras.layers import advanced_activations\n+from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n from keras.layers import convolutional_recurrent\n@@ -84,12 +84,12 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n     return model\n \n   @parameterized.named_parameters(\n-      ('LeakyReLU', advanced_activations.LeakyReLU, (2, 2)),\n-      ('PReLU', advanced_activations.PReLU, (2, 2)),\n-      ('ELU', advanced_activations.ELU, (2, 2)),\n-      ('ThresholdedReLU', advanced_activations.ThresholdedReLU, (2, 2)),\n-      ('Softmax', advanced_activations.Softmax, (2, 2)),\n-      ('ReLU', advanced_activations.ReLU, (2, 2)),\n+      ('LeakyReLU', activation.LeakyReLU, (2, 2)),\n+      ('PReLU', activation.PReLU, (2, 2)),\n+      ('ELU', activation.ELU, (2, 2)),\n+      ('ThresholdedReLU', activation.ThresholdedReLU, (2, 2)),\n+      ('Softmax', activation.Softmax, (2, 2)),\n+      ('ReLU', activation.ReLU, (2, 2)),\n       ('Conv1D', lambda: convolutional.Conv1D(2, 2), (2, 2, 1)),\n       ('Conv2D', lambda: convolutional.Conv2D(2, 2), (2, 2, 2, 1)),\n       ('Conv3D', lambda: convolutional.Conv3D(2, 2), (2, 2, 2, 2, 1)),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
