{"custom_id": "keras#662f6c5bb82b54d90ec8e863ac7a44c3b8c1b938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 219 | Lines Deleted: 68 | Files Changed: 27 | Hunks: 51 | Methods Changed: 34 | Complexity Δ (Sum/Max): 17/7 | Churn Δ: 287 | Churn Cumulative: 26064 | Contributors (this commit): 20 | Commits (past 90d): 131 | Contributors (cumulative): 76 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -846,8 +846,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       input_masks = tf.nest.map_structure(\n           keras_tensor.keras_tensor_to_placeholder, input_masks)\n \n-      inputs = self._maybe_cast_inputs(inputs)\n-\n       with backend.name_scope(self._name_scope()):\n         with autocast_variable.enable_auto_cast_variables(\n             self._compute_dtype_object):\n@@ -855,6 +853,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           # overridden).\n           # TODO(kaftan): do we maybe_build here, or have we already done it?\n           self._maybe_build(inputs)\n+          inputs = self._maybe_cast_inputs(inputs)\n           outputs = call_fn(inputs, *args, **kwargs)\n \n         self._handle_activity_regularization(inputs, outputs)\n@@ -988,9 +987,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         build_graph=not eager,\n         training=training_mode):\n \n-      if self._autocast:\n-        inputs = self._maybe_cast_inputs(inputs, input_list)\n-\n       input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n       if eager:\n         call_fn = self.call\n@@ -1003,6 +999,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if not self.built:\n           self._maybe_build(inputs)\n \n+        if self._autocast:\n+          inputs = self._maybe_cast_inputs(inputs, input_list)\n+\n         with autocast_variable.enable_auto_cast_variables(\n             self._compute_dtype_object):\n           outputs = call_fn(inputs, *args, **kwargs)\n@@ -1111,8 +1110,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         layer=self, inputs=inputs, build_graph=True, training=training_value):\n       # Symbolic execution on symbolic tensors. We will attempt to build\n       # the corresponding TF subgraph inside `backend.get_graph()`\n-      # TODO(reedwm): We should assert input compatibility after the inputs\n-      # are casted, not before.\n       input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n       graph = backend.get_graph()\n       # Use `self._name_scope()` to avoid auto-incrementing the name.\n\n@@ -736,8 +736,6 @@ class Layer(base_layer.Layer):\n       if build_graph:\n         # Symbolic execution on symbolic tensors. We will attempt to build\n         # the corresponding TF subgraph inside `backend.get_graph()`\n-        # TODO(reedwm): We should assert input compatibility after the inputs\n-        # are casted, not before.\n         input_spec.assert_input_compatibility(self.input_spec, inputs,\n                                               self.name)\n         graph = backend.get_graph()\n\n@@ -368,7 +368,7 @@ class Functional(training_lib.Model):\n   @property\n   def _checkpoint_dependencies(self):\n     dependencies = [\n-        trackable.TrackableReference(name=name, ref=layer)\n+        tf.__internal__.tracking.TrackableReference(name=name, ref=layer)\n         for name, layer in self._layer_checkpoint_dependencies.items()]\n     dependencies.extend(super(Functional, self)._checkpoint_dependencies)\n     return dependencies\n\n@@ -128,6 +128,10 @@ class InputLayer(base_layer.Layer):\n         raise ValueError('Only provide the input_shape OR '\n                          'batch_input_shape argument to '\n                          'InputLayer, not both at the same time.')\n+      # Set the input shape and batch size from the batch_input_shape.\n+      # Note that batch_input_shape can be None (unknown rank) or [] (scalar),\n+      # in which case the batch size must be None.\n+      if batch_input_shape:\n         batch_size = batch_input_shape[0]\n         input_shape = batch_input_shape[1:]\n     if kwargs:\n\n@@ -20,6 +20,7 @@ from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n from tensorflow.python.framework import type_spec\n+from keras import backend as K\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n@@ -361,6 +362,12 @@ class InputLayerTest(keras_parameterized.TestCase):\n       model = model_config.model_from_json(model.to_json())\n       self.assertAllEqual(model(two_tensors), lambda_fn(two_tensors))\n \n+  def test_serialize_with_unknown_rank(self):\n+    inp = K.placeholder(shape=None, dtype=tf.string)\n+    x = input_layer_lib.InputLayer(input_tensor=inp, dtype=tf.string)\n+    loaded = input_layer_lib.InputLayer.from_config(x.get_config())\n+    self.assertIsNone(loaded._batch_input_shape)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -60,7 +60,7 @@ class _BaseFeaturesLayer(Layer):\n         name=name, trainable=trainable, **kwargs)\n     self._feature_columns = feature_column_v2._normalize_feature_columns(  # pylint: disable=protected-access\n         feature_columns)\n-    self._state_manager = feature_column_v2._StateManagerImpl(  # pylint: disable=protected-access\n+    self._state_manager = tf.__internal__.feature_column.StateManager(  # pylint: disable=protected-access\n         self, self.trainable)\n     self._partitioner = partitioner\n     for column in self._feature_columns:\n@@ -118,10 +118,8 @@ class _BaseFeaturesLayer(Layer):\n     return tf.concat(output_tensors, -1)\n \n   def get_config(self):\n-    # Import here to avoid circular imports.\n-    from tensorflow.python.feature_column import serialization  # pylint: disable=g-import-not-at-top\n-    column_configs = serialization.serialize_feature_columns(\n-        self._feature_columns)\n+    column_configs = [tf.__internal__.feature_column.serialize_feature_column(fc)\n+                      for fc in self._feature_columns]\n     config = {'feature_columns': column_configs}\n     config['partitioner'] = generic_utils.serialize_keras_object(\n         self._partitioner)\n@@ -132,11 +130,10 @@ class _BaseFeaturesLayer(Layer):\n \n   @classmethod\n   def from_config(cls, config, custom_objects=None):\n-    # Import here to avoid circular imports.\n-    from tensorflow.python.feature_column import serialization  # pylint: disable=g-import-not-at-top\n     config_cp = config.copy()\n-    config_cp['feature_columns'] = serialization.deserialize_feature_columns(\n-        config['feature_columns'], custom_objects=custom_objects)\n+    columns_by_name = {}\n+    config_cp['feature_columns'] = [tf.__internal__.feature_column.deserialize_feature_column(\n+        c, custom_objects, columns_by_name) for c in config['feature_columns']]\n     config_cp['partitioner'] = generic_utils.deserialize_keras_object(\n         config['partitioner'], custom_objects)\n \n\n@@ -18,6 +18,8 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow.compat.v2 as tf\n+\n import json\n \n from tensorflow.python.feature_column import feature_column_v2 as fc\n@@ -95,7 +97,7 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n         trainable=trainable,\n         name=name,\n         partitioner=partitioner,\n-        expected_column_type=fc.DenseColumn,\n+        expected_column_type=tf.__internal__.feature_column.DenseColumn,\n         **kwargs)\n \n   @property\n\n@@ -19,8 +19,6 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.feature_column import feature_column_v2 as fc\n from keras.feature_column import base_feature_layer as kfc\n from keras.feature_column import dense_features\n from keras.utils import tf_contextlib\n@@ -98,7 +96,7 @@ class DenseFeatures(dense_features.DenseFeatures):\n     super(kfc._BaseFeaturesLayer, self).build(None)  # pylint: disable=bad-super-call\n \n \n-class _StateManagerImplV2(fc._StateManagerImpl):  # pylint: disable=protected-access\n+class _StateManagerImplV2(tf.__internal__.feature_column.StateManager):  # pylint: disable=protected-access\n   \"\"\"Manages the state of DenseFeatures.\"\"\"\n \n   def create_variable(self,\n\n@@ -105,7 +105,7 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n         feature_columns=feature_columns,\n         trainable=trainable,\n         name=name,\n-        expected_column_type=fc.SequenceDenseColumn,\n+        expected_column_type=tf.__internal__.feature_column.SequenceDenseColumn,\n         **kwargs)\n \n   @property\n\n@@ -496,6 +496,36 @@ class CoreLayersTest(keras_parameterized.TestCase):\n     testing_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 5, 2))\n \n+  def test_dense_output(self):\n+    dense_inputs = tf.convert_to_tensor(\n+        np.random.uniform(size=(10, 10)).astype('f'))\n+    # Create some sparse data where multiple rows and columns are missing.\n+    sparse_inputs = tf.SparseTensor(\n+        indices=np.random.randint(low=0, high=10, size=(5, 2)),\n+        values=np.random.uniform(size=(5,)).astype('f'),\n+        dense_shape=[10, 10])\n+    sparse_inputs = tf.sparse.reorder(sparse_inputs)\n+\n+    layer = keras.layers.Dense(\n+        5,\n+        kernel_initializer=keras.initializers.RandomUniform(),\n+        bias_initializer=keras.initializers.RandomUniform(),\n+        dtype='float32')\n+    dense_outputs = layer(dense_inputs)\n+    sparse_outpus = layer(sparse_inputs)\n+\n+    expected_dense = tf.add(\n+        tf.matmul(dense_inputs, keras.backend.get_value(layer.kernel)),\n+        keras.backend.get_value(layer.bias))\n+    expected_sparse = tf.add(\n+        tf.matmul(\n+            tf.sparse.to_dense(sparse_inputs),\n+            keras.backend.get_value(layer.kernel)),\n+        keras.backend.get_value(layer.bias))\n+\n+    self.assertAllClose(dense_outputs, expected_dense)\n+    self.assertAllClose(sparse_outpus, expected_sparse)\n+\n   def test_dense_dtype(self):\n     inputs = tf.convert_to_tensor(\n         np.random.randint(low=0, high=7, size=(2, 2)))\n\n@@ -41,8 +41,28 @@ def dense(inputs, kernel, bias=None, activation=None, dtype=None):\n \n   rank = inputs.shape.rank\n   if rank == 2 or rank is None:\n+    # We use embedding_lookup_sparse as a more efficient matmul operation for\n+    # large sparse input tensors. The op will result in a sparse gradient, as\n+    # opposed to sparse_ops.sparse_tensor_dense_matmul which results in dense\n+    # gradients. This can lead to sigfinicant speedups, see b/171762937.\n     if isinstance(inputs, tf.SparseTensor):\n-      outputs = tf.sparse.sparse_dense_matmul(inputs, kernel)\n+      # We need to fill empty rows, as the op assumes at least one id per row.\n+      inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)\n+      # We need to do some munging of our input to use the embedding lookup as a\n+      # matrix multiply. We split our input matrix into separate ids and weights\n+      # tensors. The values of the ids tensor should be the column indices of\n+      # our input matrix and the values of the weights tensor can continue to\n+      # the actual matrix weights. The column arrangement of ids and weights\n+      # will be summed over and does not matter. See the documentation for\n+      # sparse_ops.sparse_tensor_dense_matmul a more detailed explanation of the\n+      # inputs to both ops.\n+      ids = tf.SparseTensor(\n+          indices=inputs.indices,\n+          values=inputs.indices[:, 1],\n+          dense_shape=inputs.dense_shape)\n+      weights = inputs\n+      outputs = tf.nn.embedding_lookup_sparse(\n+          kernel, ids, weights, combiner=\"sum\")\n     else:\n       outputs = tf.raw_ops.MatMul(a=inputs, b=kernel)\n   # Broadcast kernel to inputs.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.category_crossing.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.category_encoding.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.discretization.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Tests for keras.layers.preprocessing.hashing.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.image_preprocessing.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -129,6 +129,10 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         layer_name=self.__class__.__name__,\n         arg_name=\"output_mode\")\n \n+    if invert and output_mode != INT:\n+      raise ValueError(\"`output_mode` must be {} when `invert` is true. You \"\n+                       \"passed {}\".format(INT, output_mode))\n+\n     self.invert = invert\n     self.max_tokens = max_tokens\n     self.num_oov_indices = num_oov_indices\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.index_lookup.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -1422,6 +1422,17 @@ class IndexLookupInverseVocabularyTest(\n           dtype=tf.string,\n           invert=True)\n \n+  def test_non_int_output_fails(self):\n+    with self.assertRaisesRegex(ValueError, \"`output_mode` must be int\"):\n+      _ = get_layer_class()(\n+          max_tokens=None,\n+          num_oov_indices=1,\n+          mask_token=\"\",\n+          oov_token=\"[OOV]\",\n+          dtype=tf.string,\n+          output_mode=index_lookup.COUNT,\n+          invert=True)\n+\n   def test_vocab_with_repeated_element_fails(self):\n     vocab_data = [\"earth\", \"earth\", \"wind\", \"and\", \"fire\"]\n     layer = get_layer_class()(\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.normalization.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n+\"\"\"Distribution tests for keras.layers.preprocessing.text_vectorization.\"\"\"\n \n from __future__ import absolute_import\n from __future__ import division\n\n@@ -1525,7 +1525,7 @@ def huber(y_true, y_pred, delta=1.0):\n \n   ```\n   loss = 0.5 * x^2                  if |x| <= d\n-  loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d\n+  loss = d * |x| - 0.5 * d^2        if |x| > d\n   ```\n   where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n \n@@ -1545,9 +1545,8 @@ def huber(y_true, y_pred, delta=1.0):\n   abs_error = tf.abs(error)\n   half = tf.convert_to_tensor(0.5, dtype=abs_error.dtype)\n   return K.mean(\n-      tf.where(\n-          abs_error <= delta, half * tf.pow(error, 2),\n-          half * tf.pow(delta, 2) + delta * (abs_error - delta)),\n+      tf.where(abs_error <= delta, half * tf.square(error),\n+                         delta * abs_error - half * tf.square(delta)),\n       axis=-1)\n \n \n\n@@ -436,6 +436,34 @@ class KerasLayerTest(keras_parameterized.TestCase):\n     # Non-mixed policies are fine\n     mp_test_util.MultiplyLayer(dtype=policy.Policy('float64'))\n \n+  def test_input_spec_dtype(self):\n+    # Test the InputSpec's dtype is compared against the inputs before the layer\n+    # casts them, not after.\n+    layer = mp_test_util.MultiplyLayer(dtype='float64')\n+    layer.input_spec = input_spec.InputSpec(dtype='float16')\n+\n+    # Test passing Eager tensors\n+    x = tf.ones((2, 2), dtype='float16')\n+    layer(x)\n+    x = tf.ones((2, 2), dtype='float64')\n+    with self.assertRaisesRegex(\n+        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n+      layer(x)\n+\n+    # Test passing symbolic tensors\n+    x = layers.Input((2,), dtype='float16')\n+    y = layer(x)\n+    model = models.Model(x, y)\n+    model(tf.ones((2, 2)))\n+\n+    x = layers.Input((2,), dtype='float64')\n+    with self.assertRaisesRegex(\n+        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n+      # In TF2, the error is only raised when the model is run\n+      y = layer(x)\n+      model = models.Model(x, y)\n+      model(tf.ones((2, 2)))\n+\n \n class KerasModelTest(keras_parameterized.TestCase):\n   \"\"\"Test mixed precision with Keras models.\"\"\"\n\n@@ -23,6 +23,7 @@ from keras import backend\n from keras import optimizers\n from keras.mixed_precision import loss_scale as keras_loss_scale_module\n from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizer_v2 import utils as optimizer_utils\n from tensorflow.python.platform import tf_logging\n from tensorflow.python.training.experimental import mixed_precision\n from tensorflow.python.training.tracking import base as trackable\n@@ -268,7 +269,7 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n     weights = []\n     for (name, g), v in sorted(self._weights.items(), key=lambda i: i[0][0]):\n       if g == graph_key:\n-        weights.append(trackable.TrackableReference(name=name, ref=v))\n+        weights.append(tf.__internal__.tracking.TrackableReference(name=name, ref=v))\n     return (super(_DynamicLossScaleState, self)._checkpoint_dependencies +\n             weights)\n \n@@ -314,8 +315,8 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n     \"\"\"Updates the value of the loss scale.\n \n     Args:\n-      grads: A nested structure of unscaled gradients, each which is the\n-        gradient of the loss with respect to a weight.\n+      grads: A nested structure of unscaled gradients, each which is an\n+        all-reduced gradient of the loss with respect to a weight.\n \n     Returns:\n       update_op: In eager mode, None. In graph mode, an op to update the loss\n@@ -327,19 +328,13 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n     grads = tf.nest.flatten(grads)\n     if tf.distribute.has_strategy():\n       distribution = tf.distribute.get_strategy()\n-\n-      def get_is_finite(grads):\n-        is_finite = _is_all_finite(grads)\n-        # We cast to float, because we cannot reduce booleans with\n-        # DistributionStrategy.\n-        return tf.cast(is_finite, tf.float32)\n-\n-      is_finite_float = distribution.extended.call_for_each_replica(\n-          get_is_finite, args=(grads,))\n-      reduced_is_finite_float = distribution.reduce(tf.distribute.ReduceOp.SUM,\n-                                                    is_finite_float, axis=None)\n-      is_finite = tf.equal(reduced_is_finite_float,\n-                                 distribution.num_replicas_in_sync)\n+      is_finite_per_replica = distribution.extended.call_for_each_replica(\n+          _is_all_finite, args=(grads,))\n+      # Each replica computed the same `is_finite` value, since `grads` is\n+      # all-reduced across replicas. Arbitrarily take `is_finite` from the first\n+      # replica.\n+      is_finite = (\n+          distribution.experimental_local_results(is_finite_per_replica)[0])\n     else:\n       is_finite = _is_all_finite(grads)\n \n@@ -462,7 +457,7 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n   skipped is `1 / dynamic_growth_steps`).\n \n   `LossScaleOptimizer` delegates all public `Optimizer` methods to the inner\n-  optimizer. Additionally, in methods `minimize` and `get_gradients, it scales\n+  optimizer. Additionally, in methods `minimize` and `get_gradients`, it scales\n   the loss and unscales the gradients. In methods `minimize` and\n   `apply_gradients`, it additionally updates the loss scale and skips applying\n   gradients if any gradient has a nonfinite value.\n@@ -692,13 +687,24 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n     # as frequently the optimizer is created outside the strategy's scope.\n     self._raise_if_strategy_unsupported()\n \n+    grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n+    if experimental_aggregate_gradients:\n+      # We must aggregate the gradients here instead of in\n+      # self.optimizer.apply_gradients, so that any NaN or Inf gradients are\n+      # propogated to each replica. If any replica has a NaN or Inf gradient,\n+      # they must all have a NaN or Inf gradient so that they all skip the step.\n+      # pylint: disable=protected-access\n+      grads_and_vars = self._optimizer._transform_unaggregated_gradients(\n+          grads_and_vars)\n+      grads_and_vars = self._optimizer._aggregate_gradients(grads_and_vars)\n+      # pylint: enable=protected-access\n+\n     grads_and_vars = tuple(grads_and_vars)\n     return tf.distribute.get_replica_context().merge_call(\n         self._apply_gradients_cross_replica,\n-        args=(grads_and_vars, name, experimental_aggregate_gradients))\n+        args=(grads_and_vars, name))\n \n-  def _apply_gradients_cross_replica(self, distribution, grads_and_vars, name,\n-                                     experimental_aggregate_gradients):\n+  def _apply_gradients_cross_replica(self, distribution, grads_and_vars, name):\n     grads = [g for g, _ in grads_and_vars]\n     if isinstance(self._loss_scale, _DynamicLossScaleState):\n       loss_scale_update_op, should_apply_grads = self._loss_scale.update(grads)\n@@ -715,7 +721,7 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n       wrapped_vars = _UnwrapPreventer([v for _, v in grads_and_vars])\n       return distribution.extended.call_for_each_replica(\n           self._apply_gradients,\n-          args=(grads, wrapped_vars, name, experimental_aggregate_gradients))\n+          args=(grads, wrapped_vars, name))\n \n     def do_not_apply_fn():\n       # Normally self._optimizer.iterations is incremented in\n@@ -731,14 +737,15 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n                                            do_not_apply_fn)\n     return tf.group(maybe_apply_op, loss_scale_update_op)\n \n-  def _apply_gradients(self, grads, wrapped_vars, name,\n-                       experimental_aggregate_gradients):\n+  def _apply_gradients(self, grads, wrapped_vars, name):\n+    # Pass experimental_aggregate_gradients=False since LossScaleOptimizer\n+    # already aggregated the gradients.\n     # TODO(reedwm): This will raise a fairly cryptic error message if\n     # self._optimizer.apply_gradients does not take\n     # experimental_aggregate_gradients.\n     return self._optimizer.apply_gradients(\n         list(zip(grads, wrapped_vars.value)), name,\n-        experimental_aggregate_gradients=experimental_aggregate_gradients)\n+        experimental_aggregate_gradients=False)\n \n   def get_config(self):\n     serialized_optimizer = optimizers.serialize(self._optimizer)\n\n@@ -296,6 +296,58 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # and so the variable will be init_val - grad * lr == 5 - 1 * 2 == 3\n       self.assertAllClose([3.], self.evaluate(var))\n \n+  def testNanOnOneReplicaOnly(self):\n+    if not tf.test.is_gpu_available():\n+      self.skipTest('Test requires GPU')\n+    if (not tf.executing_eagerly() and\n+        not tf.compat.v1.control_flow_v2_enabled()):\n+      self.skipTest('b/181283011: GradientTape does not work properly with '\n+                    'V1 control flow, and opt.minimize uses GradientTape')\n+    with create_mirrored_strategy().scope() as strategy:\n+      var = tf.Variable([1.0, 2.0])\n+      opt = gradient_descent.SGD(1.0)\n+      opt = loss_scale_optimizer.LossScaleOptimizer(opt, initial_scale=2,\n+                                                    dynamic_growth_steps=2)\n+\n+      def loss():\n+        rep_id = (tf.distribute.get_replica_context()\n+                  .replica_id_in_sync_group)\n+        # The last element of last replica's gradient is NaN.\n+        return tf.compat.v1.cond(\n+            tf.constant(rep_id == 0), lambda: var * 2.,\n+            lambda: var * tf.constant([1., float('NaN')]))\n+      run_fn = lambda: opt.minimize(loss, var_list=[var])\n+      run_op = strategy.experimental_run(run_fn)\n+      self.evaluate(tf.compat.v1.global_variables_initializer())\n+      self._run_if_in_graph_mode(run_op)\n+      # Variable should not change from before, due to NaN gradients.\n+      self.assertAllClose(self.evaluate(var), [1.0, 2.0])\n+      # Loss scale should half due to NaN gradients.\n+      self.assertEqual(1., self.evaluate(opt.loss_scale))\n+\n+  def testCustomAggregater(self):\n+    def gradient_aggregator(grads_and_vars):\n+      # Simulate an all-reduce where a replica has a NaN gradient by setting\n+      # the last gradient to NaN\n+      grads_and_vars = list(grads_and_vars)\n+      last_grad, last_var = grads_and_vars[-1]\n+      grads_and_vars[-1] = (last_grad * float('NaN'), last_var)\n+      return grads_and_vars\n+\n+    var = tf.Variable([1.0, 2.0])\n+    opt = gradient_descent.SGD(1.0, gradient_aggregator=gradient_aggregator)\n+    opt = loss_scale_optimizer.LossScaleOptimizer(opt, initial_scale=2,\n+                                                  dynamic_growth_steps=2)\n+\n+    loss = lambda: var * 2\n+    run_op = opt.minimize(loss, var_list=[var])\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n+    self._run_if_in_graph_mode(run_op)\n+    # Variable should not change from before, due to NaN gradients.\n+    self.assertAllClose(self.evaluate(var), [1.0, 2.0])\n+    # Loss scale should half due to NaN gradients.\n+    self.assertEqual(1., self.evaluate(opt.loss_scale))\n+\n   @parameterized.named_parameters(*TESTCASES)\n   def testDynamicLossScaleWithSlots(self, strategy_fn):\n     strategy_obj = strategy_fn()\n\n@@ -802,6 +802,8 @@ def _finalize_saved_model_layers(layers):\n \n       if hasattr(_get_keras_attr(layer), 'call_and_return_conditional_losses'):\n         call_fn = _get_keras_attr(layer).call_and_return_conditional_losses\n+        if not call_fn.concrete_functions:\n+          continue\n         if call_fn.input_signature is None:\n           inputs = infer_inputs_from_restored_call_function(call_fn)\n         else:\n@@ -1099,8 +1101,8 @@ def infer_inputs_from_restored_call_function(fn):\n   \"\"\"Returns TensorSpec of inputs from a restored call function.\n \n   Args:\n-    fn: Restored layer call function. It is assumed that the inputs are entirely\n-      in the first argument.\n+    fn: Restored layer call function. It is assumed that `fn` has at least\n+        one concrete function and that the inputs are in the first argument.\n \n   Returns:\n     TensorSpec of call function inputs.\n\n@@ -22,7 +22,6 @@ import tensorflow.compat.v2 as tf\n import os\n \n from tensorflow.core.framework import versions_pb2\n-from tensorflow.python.distribute import distribution_strategy_context\n from keras import backend as K\n from keras.protobuf import saved_metadata_pb2\n from keras.saving import saving_utils\n@@ -90,10 +89,6 @@ def save(model, filepath, overwrite, include_optimizer, signatures=None,\n   # This is needed for compatibility reasons until learning phase setting\n   # is removed from the public apis.\n   with K.deprecated_internal_learning_phase_scope(0):\n-    # When saving a model involving batch norm layer within a strategy scope,\n-    # the replica context is not available when calling `add_update()`, and thus\n-    # we use the default replica context here.\n-    with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\n     with utils.keras_option_scope(save_traces):\n       saved_nodes, node_paths = save_lib.save_and_return_nodes(\n           model, filepath, signatures, options)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6c7ab366c5117b6195b6caa597fed0c537a8699", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 7212 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1010,7 +1010,8 @@ def name_scope(name):\n   return tf.name_scope(name)\n \n # Export V1 version.\n-keras_export(v1=['keras.backend.name_scope'], allow_multiple_exports=True)(tf.compat.v1.name_scope)\n+_v1_name_scope = tf.compat.v1.name_scope\n+keras_export(v1=['keras.backend.name_scope'], allow_multiple_exports=True)(_v1_name_scope)\n \n \n @keras_export('keras.backend.variable')\n\n@@ -21,20 +21,30 @@ from __future__ import print_function\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n+\n+_v1_zeros_initializer = tf.compat.v1.zeros_initializer\n+_v1_ones_initializer = tf.compat.v1.ones_initializer\n+_v1_constant_initializer = tf.compat.v1.constant_initializer\n+_v1_variance_scaling_initializer = tf.compat.v1.variance_scaling_initializer\n+_v1_orthogonal_initializer = tf.compat.v1.orthogonal_initializer\n+_v1_identity = tf.compat.v1.initializers.identity\n+_v1_glorot_uniform_initializer = tf.compat.v1.glorot_uniform_initializer\n+_v1_glorot_normal_initializer = tf.compat.v1.glorot_normal_initializer\n+\n keras_export(v1=['keras.initializers.Zeros', 'keras.initializers.zeros'], allow_multiple_exports=True)(\n-    tf.compat.v1.zeros_initializer)\n+    _v1_zeros_initializer)\n keras_export(v1=['keras.initializers.Ones', 'keras.initializers.ones'], allow_multiple_exports=True)(\n-    tf.compat.v1.ones_initializer)\n+    _v1_ones_initializer)\n keras_export(v1=['keras.initializers.Constant', 'keras.initializers.constant'], allow_multiple_exports=True)(\n-    tf.compat.v1.constant_initializer)\n+    _v1_constant_initializer)\n keras_export(v1=['keras.initializers.VarianceScaling'], allow_multiple_exports=True)(\n-    tf.compat.v1.variance_scaling_initializer)\n+    _v1_variance_scaling_initializer)\n keras_export(v1=['keras.initializers.Orthogonal',\n-                 'keras.initializers.orthogonal'], allow_multiple_exports=True)(tf.compat.v1.orthogonal_initializer)\n+                 'keras.initializers.orthogonal'], allow_multiple_exports=True)(_v1_orthogonal_initializer)\n keras_export(v1=['keras.initializers.Identity',\n-                 'keras.initializers.identity'], allow_multiple_exports=True)(tf.compat.v1.initializers.identity)\n-keras_export(v1=['keras.initializers.glorot_uniform'], allow_multiple_exports=True)(tf.compat.v1.glorot_uniform_initializer)\n-keras_export(v1=['keras.initializers.glorot_normal'], allow_multiple_exports=True)(tf.compat.v1.glorot_normal_initializer)\n+                 'keras.initializers.identity'], allow_multiple_exports=True)(_v1_identity)\n+keras_export(v1=['keras.initializers.glorot_uniform'], allow_multiple_exports=True)(_v1_glorot_uniform_initializer)\n+keras_export(v1=['keras.initializers.glorot_normal'], allow_multiple_exports=True)(_v1_glorot_normal_initializer)\n \n \n @keras_export(v1=['keras.initializers.RandomNormal',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3f6e4b5e0f808bd46c0a94990cb9e43098adcc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 446 | Lines Deleted: 662 | Files Changed: 27 | Hunks: 178 | Methods Changed: 74 | Complexity Δ (Sum/Max): -46/6 | Churn Δ: 1108 | Churn Cumulative: 37754 | Contributors (this commit): 44 | Commits (past 90d): 142 | Contributors (cumulative): 102 | DMM Complexity: 0.8657718120805369\n\nDIFF:\n@@ -37,7 +37,6 @@ import numpy as np\n from tensorflow.core.protobuf import config_pb2\n from tensorflow.python.distribute import distribute_coordinator as dc\n from tensorflow.python.distribute import distribute_coordinator_context as dc_context\n-from tensorflow.python.eager import function as eager_function\n from tensorflow.python.eager.context import get_config\n from tensorflow.python.framework import config\n from tensorflow.python.framework import ops\n@@ -1247,7 +1246,7 @@ def is_keras_tensor(x):\n                      keras_tensor.KerasTensor)):\n     raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) +\n                      '`. Expected a symbolic tensor instance.')\n-  if keras_tensor.keras_tensors_enabled():\n+  if tf.compat.v1.executing_eagerly_outside_functions():\n     return isinstance(x, keras_tensor.KerasTensor)\n   return hasattr(x, '_keras_history')\n \n@@ -1300,7 +1299,7 @@ def placeholder(shape=None,\n   if not shape:\n     if ndim:\n       shape = (None,) * ndim\n-  if keras_tensor.keras_tensors_enabled():\n+  if tf.compat.v1.executing_eagerly_outside_functions():\n     if sparse:\n       spec = tf.SparseTensorSpec(\n           shape=shape, dtype=dtype)\n@@ -1343,7 +1342,6 @@ def placeholder(shape=None,\n     # (intended to be used with keras.backend.function)\n     from keras.engine import input_layer  # pylint: disable=g-import-not-at-top\n     x = input_layer.Input(tensor=x)\n-    if keras_tensor.keras_tensors_enabled():\n     x._is_backend_placeholder = True\n \n   return x\n@@ -1359,7 +1357,7 @@ def is_placeholder(x):\n       Boolean.\n   \"\"\"\n   try:\n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       return hasattr(x, '_is_backend_placeholder')\n     from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n     if tf_utils.is_extension_type(x):\n@@ -3690,7 +3688,8 @@ def get_value(x):\n \n   if tf.compat.v1.executing_eagerly_outside_functions():\n     # This method of evaluating works inside the Keras FuncGraph.\n-    return eval_in_eager_or_function(x)\n+    with tf.init_scope():\n+      return x.numpy()\n \n   with x.graph.as_default():\n     return x.eval(session=get_session((x,)))\n@@ -4038,76 +4037,6 @@ class GraphExecutionFunction(object):\n     return tf.nest.map_structure(self._eval_if_composite, output_structure)\n \n \n-def eval_in_eager_or_function(outputs):\n-  \"\"\"Method to evaluate a tensor in eager or in a tf.function.\n-\n-  In the case of a tf.function, it will lift the tensor out of the function\n-  and try to evaluate that piece of the graph.\n-\n-  Warning: Do not add new usages of this function.\n-  TODO(b/150169018): delete this function once _keras_history_helper is no\n-  longer needed, after Keras switches to KerasTensors and op layers\n-  work via dispatch.\n-\n-  Args:\n-    outputs: tensors to fetch.\n-  Returns:\n-    The value of the tensors (as numpy arrays).\n-  \"\"\"\n-  outputs_structure = outputs\n-  outputs = tf.nest.flatten(outputs, expand_composites=True)\n-\n-  graphs = {\n-      i.graph\n-      for i in tf.nest.flatten([outputs])\n-      if hasattr(i, 'graph')\n-  }\n-  if len(graphs) > 1:\n-    raise ValueError('Cannot create an execution function which is comprised '\n-                     'of elements from multiple graphs.')\n-\n-  source_graph = graphs.pop()\n-\n-  with _scratch_graph() as exec_graph:\n-    global_graph = get_graph()\n-    if source_graph not in (exec_graph, global_graph):\n-      raise ValueError('Unknown graph. Aborting.')\n-\n-    if source_graph is global_graph and exec_graph is not global_graph:\n-      init_tensors = outputs\n-      lifted_map = tf.__internal__.lift_to_graph(\n-          tensors=init_tensors,\n-          graph=exec_graph,\n-          sources=[],\n-          add_sources=True,\n-          handle_captures=True,\n-          base_graph=source_graph)\n-\n-      outputs = [lifted_map[i] for i in outputs]\n-\n-  # Consolidate updates\n-  with exec_graph.as_default():\n-    outputs = cast_variables_to_tensor(outputs)\n-\n-    exec_graph.inputs = exec_graph.internal_captures\n-    exec_graph.outputs = outputs\n-    graph_fn = eager_function.ConcreteFunction(exec_graph)\n-\n-  graph_fn._num_positional_args = 0\n-  graph_fn._arg_keywords = []\n-\n-  outputs = graph_fn()\n-\n-  # EagerTensor.numpy() will often make a copy to ensure memory safety.\n-  # However in this case `outputs` is not directly returned, so it is always\n-  # safe to reuse the underlying buffer without checking. In such a case the\n-  # private numpy conversion method is preferred to guarantee performance.\n-  return tf.nest.pack_sequence_as(\n-      outputs_structure,\n-      [x._numpy() for x in outputs],  # pylint: disable=protected-access\n-      expand_composites=True)\n-\n-\n @keras_export('keras.backend.function')\n @doc_controls.do_not_generate_docs\n def function(inputs, outputs, updates=None, name=None, **kwargs):\n\n@@ -1078,7 +1078,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                                                 args, kwargs)\n         training_arg_passed_by_framework = True\n \n-    if keras_tensor.keras_tensors_enabled():\n     with call_context.enter(\n         layer=self, inputs=inputs, build_graph=True, training=training_value):\n       # Check input assumptions set after layer building, e.g. input shape.\n@@ -1099,83 +1098,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                                                 outputs)\n       return outputs\n \n-    # Only create Keras history if at least one tensor originates from a\n-    # `keras.Input`. Otherwise this Layer may be being used outside the Keras\n-    # framework.\n-    # TODO(kaftan): make this not special case inputs\n-    if base_layer_utils.needs_keras_history(inputs):\n-      base_layer_utils.create_keras_history(inputs)\n-\n-    with call_context.enter(\n-        layer=self, inputs=inputs, build_graph=True, training=training_value):\n-      # Symbolic execution on symbolic tensors. We will attempt to build\n-      # the corresponding TF subgraph inside `backend.get_graph()`\n-      input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n-      graph = backend.get_graph()\n-      # Use `self._name_scope()` to avoid auto-incrementing the name.\n-      with graph.as_default(), backend.name_scope(self._name_scope()):\n-        # Build layer if applicable (if the `build` method has been\n-        # overridden).\n-        self._maybe_build(inputs)\n-        cast_inputs = self._maybe_cast_inputs(inputs, input_list)\n-\n-        if not self.dynamic:\n-          # Wrapping `call` function in autograph to allow for dynamic control\n-          # flow and control dependencies in call. We are limiting this to\n-          # subclassed layers as autograph is strictly needed only for\n-          # subclassed layers and models.\n-          # tf_convert will respect the value of autograph setting in the\n-          # enclosing tf.function, if any.\n-          if (base_layer_utils.is_subclassed(self) and\n-              not base_layer_utils.from_saved_model(self)):\n-            call_fn = tf.__internal__.autograph.tf_convert(self.call,\n-                                           tf.__internal__.autograph.control_status_ctx())\n-          else:\n-            call_fn = self.call\n-\n-          try:\n-            with autocast_variable.enable_auto_cast_variables(\n-                self._compute_dtype_object):\n-              outputs = call_fn(cast_inputs, *args, **kwargs)\n-\n-          except tf.errors.OperatorNotAllowedInGraphError as e:\n-            raise TypeError('You are attempting to use Python control '\n-                            'flow in a layer that was not declared to be '\n-                            'dynamic. Pass `dynamic=True` to the class '\n-                            'constructor.\\nEncountered error:\\n\"\"\"\\n' + str(e) +\n-                            '\\n\"\"\"')\n-        else:\n-          # We will use static shape inference to return symbolic tensors\n-          # matching the specifications of the layer outputs.\n-          # Since `self.dynamic` is True, we will never attempt to\n-          # run the underlying TF graph (which is disconnected).\n-          # TODO(fchollet): consider py_func as an alternative, which\n-          # would enable us to run the underlying graph if needed.\n-          outputs = self._symbolic_call(inputs)\n-\n-        if outputs is None:\n-          raise ValueError('A layer\\'s `call` method should return a '\n-                           'Tensor or a list of Tensors, not None '\n-                           '(layer: ' + self.name + ').')\n-        # TODO(kaftan): This should be 'any' and check all args\n-        if base_layer_utils.have_all_keras_metadata(inputs):\n-          if training_arg_passed_by_framework:\n-            args, kwargs = self._set_call_arg_value(\n-                'training', None, args, kwargs, pop_kwarg_if_none=True)\n-          if mask_arg_passed_by_framework:\n-            kwargs.pop('mask')\n-          # Node connectivity does not special-case the first argument.\n-          outputs = self._set_connectivity_metadata((inputs,) + args, kwargs,\n-                                                    outputs)\n-        self._handle_activity_regularization(inputs, outputs)\n-        self._set_mask_metadata(inputs, outputs, input_masks, True)\n-        if hasattr(self, '_set_inputs') and not self.inputs:\n-          # Subclassed network: explicitly set metadata normally set by\n-          # a call to self._set_inputs().\n-          self._set_inputs(cast_inputs, outputs)\n-\n-    return outputs\n-\n   def _set_training_mode(self, args, kwargs, call_context):\n     training_mode = None\n     if self._expects_training_arg:\n@@ -1386,21 +1308,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     warnings.warn('`layer.updates` will be removed in a future version. '\n                   'This property should not be used in TensorFlow 2.0, '\n                   'as `updates` are applied automatically.')\n-    if keras_tensor.keras_tensors_enabled():\n     return []\n \n-    collected_updates = []\n-    all_layers = self._flatten_layers()\n-    with backend.get_graph().as_default():\n-      for layer in all_layers:\n-        if not layer.trainable and not layer.stateful:\n-          continue\n-        for u in layer._updates:\n-          if callable(u):\n-            u = u()\n-          collected_updates.append(u)\n-    return collected_updates\n-\n   @property\n   def losses(self):\n     \"\"\"List of losses added using the `add_loss()` API.\n@@ -1576,10 +1485,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     self._eager_losses.extend(eager_losses)\n \n-    if in_call_context and not keras_tensor.keras_tensors_enabled():\n-      for symbolic_loss in symbolic_losses:\n-        self._losses.append(symbolic_loss)\n-    else:\n     for symbolic_loss in symbolic_losses:\n       if getattr(self, '_is_graph_network', False):\n         self._graph_network_add_loss(symbolic_loss)\n@@ -1679,10 +1584,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       raise TypeError('Unknown keyword arguments: ', str(kwargs.keys()))\n \n     from_metric_obj = hasattr(value, '_metric_obj')\n-    if keras_tensor.keras_tensors_enabled():\n     is_symbolic = isinstance(value, keras_tensor.KerasTensor)\n-    else:\n-      is_symbolic = tf_utils.is_symbolic_tensor(value)\n     in_call_context = base_layer_utils.call_context().in_call\n \n     if name is None and not from_metric_obj:\n@@ -2705,13 +2607,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Optionally load weight values specified at layer instantiation.\n     if self._initial_weights is not None:\n-      if tf.compat.v1.executing_eagerly_outside_functions():\n       with tf.init_scope():\n         # Using `init_scope` since we want variable assignment in\n         # `set_weights` to be treated like variable initialization.\n         self.set_weights(self._initial_weights)\n-      else:\n-        self.set_weights(self._initial_weights)\n       self._initial_weights = None\n \n   def _symbolic_call(self, inputs):\n@@ -3294,31 +3193,11 @@ class AddMetric(Layer):\n \n def _in_functional_construction_mode(layer, inputs, args, kwargs, input_list):  # pylint: disable=unused-argument\n   \"\"\"Check the arguments to see if we are constructing a functional model.\"\"\"\n-  if keras_tensor.keras_tensors_enabled():\n   # We are constructing a functional model if any of the inputs\n   # are KerasTensors\n   return any(\n       isinstance(tensor, keras_tensor.KerasTensor)\n       for tensor in tf.nest.flatten([inputs, args, kwargs]))\n-  else:\n-    if tf.executing_eagerly():\n-      all_inputs_symbolic = all(\n-          tf_utils.is_symbolic_tensor(t) for t in input_list)\n-      if (base_layer_utils.is_subclassed(layer) and\n-          any(tf_utils.is_symbolic_tensor(t) for t in tf.nest.flatten(\n-              [inputs, args, kwargs])) and not all_inputs_symbolic):\n-        raise ValueError('It appears you are trying to construct a '\n-                         'functional model, but not all of the inputs in '\n-                         'the first positional argument of your layer call '\n-                         'are symbolic tensors. '\n-                         '(Input objects, or the output of another layer) '\n-                         'Functional models cannot correctly track custom '\n-                         'layers unless all values in the first call argument '\n-                         'are symbolic.')\n-      return all_inputs_symbolic\n-    else:\n-      return (base_layer_utils.is_in_keras_graph() or\n-              all(hasattr(t, '_keras_history') for t in input_list))\n \n \n def _convert_numpy_or_python_types(x):\n\n@@ -204,6 +204,9 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n     have been wrapped in `TensorFlowOpLayer` instances. Second element is\n     a list of the `TensorFlowOpLayer` instances created.\n   \"\"\"\n+  if tf.compat.v1.executing_eagerly_outside_functions():\n+    raise ValueError(\n+        '`create_keras_history` should only be called if eager is disabled!')\n   # Import of `base_layer` needed in order to create `TensorFlowOpLayer`.\n   # Cannot be imported at top because of circular dependencies.\n   # TODO(omalleyt): Resolve circular dependency.\n@@ -246,9 +249,6 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n             constants[i] = op_input\n           else:\n             with tf.init_scope():\n-              if tf.compat.v1.executing_eagerly_outside_functions():\n-                constants[i] = backend.eval_in_eager_or_function(op_input)\n-              else:\n               constants[i] = backend.function([], op_input)([])\n       layer_inputs = unnest_if_single_tensor(layer_inputs)\n       processed_ops, created_layers = _create_keras_history_helper(\n\n@@ -24,7 +24,6 @@ import keras\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n-from keras import testing_utils\n from keras.engine import base_layer_utils\n from tensorflow.python.ops import lookup_ops\n \n@@ -84,27 +83,6 @@ class OpLayerTest(keras_parameterized.TestCase):\n     output = model.predict(input_data)\n     self.assertAllClose(expected, output)\n \n-  def test_ragged_op_layer(self):\n-    with testing_utils.use_keras_tensors_scope(False):\n-      with self.assertRaisesRegex(\n-          ValueError, '(?ms)Keras automatic op wrapping'\n-          '.*Ragged tensors encountered: '\n-          r'\\[tf.RaggedTensor\\(values=Tensor\\(\"Cast:0\", shape=\\((\\?|None),\\), '\n-          r'dtype=float32\\), row_splits=Tensor\\(\"Placeholder_1:0\", '\n-          r'shape=\\((\\?|None),\\), dtype=int64\\)\\)\\]'):\n-        int_values = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n-        float_values = tf.cast(int_values, tf.float32)\n-        _ = keras.Model(int_values, float_values)\n-\n-  def test_sparse_op_layer(self):\n-    with testing_utils.use_keras_tensors_scope(False):\n-      with self.assertRaisesRegex(\n-          ValueError, \"(?ms)Keras automatic op wrapping\"\n-          r\".*Sparse ops encountered: \\[\\<tf\\.Operation 'Cast' type=Cast\\>\\]\"):\n-        int_values = keras.Input(shape=(None,), dtype=tf.int32, sparse=True)\n-        float_values = tf.cast(int_values, tf.float32)\n-        _ = keras.Model(int_values, float_values)\n-\n   def test_ragged_op_layer_keras_tensors(self):\n     int_values = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n     float_values = tf.cast(int_values, tf.float32)\n\n@@ -32,7 +32,6 @@ from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import input_layer as input_layer_module\n from keras.engine import input_spec\n-from keras.engine import keras_tensor\n from keras.engine import node as node_module\n from keras.engine import training as training_lib\n from keras.engine import training_utils\n@@ -147,7 +146,7 @@ class Functional(training_lib.Model):\n     else:\n       self._enable_dict_to_input_mapping = False\n \n-    if not keras_tensor.keras_tensors_enabled():\n+    if not tf.compat.v1.executing_eagerly_outside_functions():\n       if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\n         base_layer_utils.create_keras_history(self._nested_outputs)\n \n@@ -1077,7 +1076,7 @@ def _map_subgraph_network(inputs, outputs):\n   Returns:\n     A tuple of List{Node] and List[Layer].\n   \"\"\"\n-  if not keras_tensor.keras_tensors_enabled():\n+  if not tf.compat.v1.executing_eagerly_outside_functions():\n     base_layer_utils.create_keras_history(outputs)\n   # Keep only nodes and layers in the topology between inputs and outputs.\n   _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)\n\n@@ -176,7 +176,7 @@ class InputLayer(base_layer.Layer):\n       ]\n       for arg_name, arg in args_that_must_be_none:\n         _assert_other_arg_none(arg_name, arg)\n-      if not keras_tensor.keras_tensors_enabled():\n+      if not tf.compat.v1.executing_eagerly_outside_functions():\n         raise ValueError('Creating Keras inputs from a type_spec is only '\n                          'supported when eager execution is enabled.')\n       input_tensor = keras_tensor.keras_tensor_from_type_spec(type_spec)\n@@ -207,7 +207,7 @@ class InputLayer(base_layer.Layer):\n       self.is_placeholder = True\n       self._batch_input_shape = batch_input_shape\n     else:\n-      if keras_tensor.keras_tensors_enabled():\n+      if tf.compat.v1.executing_eagerly_outside_functions():\n         if not isinstance(input_tensor, keras_tensor.KerasTensor):\n           input_tensor = keras_tensor.keras_tensor_from_tensor(input_tensor)\n       else:\n\n@@ -23,7 +23,6 @@ from tensorflow.python.framework import type_spec\n from keras import backend as K\n from keras import combinations\n from keras import keras_parameterized\n-from keras import testing_utils\n from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.layers import core\n@@ -154,7 +153,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def testInputTensorArg(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # Create a Keras Input\n     x = input_layer_lib.Input(tensor=tf.zeros((7, 32)))\n     self.assertAllEqual(x.shape.as_list(), [7, 32])\n\n@@ -23,25 +23,6 @@ from keras.utils import object_identity\n \n # pylint: disable=g-classes-have-attributes\n \n-_KERAS_TENSORS_ENABLED = True\n-\n-\n-def enable_keras_tensors():\n-  \"\"\"Enable using KerasTensors in Keras's functional API.\"\"\"\n-  global _KERAS_TENSORS_ENABLED\n-  _KERAS_TENSORS_ENABLED = True\n-\n-\n-def disable_keras_tensors():\n-  \"\"\"Disable using KerasTensors in Keras's functional API.\"\"\"\n-  global _KERAS_TENSORS_ENABLED\n-  _KERAS_TENSORS_ENABLED = False\n-\n-\n-def keras_tensors_enabled():\n-  \"\"\"Return a bool specifying if KerasTensors are enabled.\"\"\"\n-  return _KERAS_TENSORS_ENABLED and tf.compat.v1.executing_eagerly_outside_functions()\n-\n \n # Tensorflow tensors have a maximum rank of 254\n # (See `MaxDimensions()` in //tensorflow/core/framework/tensor_shape.h )\n\n@@ -26,7 +26,6 @@ import json\n import numpy as np\n from keras import backend\n from keras.engine import base_layer_utils\n-from keras.engine import keras_tensor\n from keras.saving.saved_model import json_utils\n from keras.utils import tf_utils\n \n@@ -78,8 +77,8 @@ class Node(object):\n     self._single_positional_tensor_passed = (not self.call_kwargs and len(\n         self.call_args) == 1 and tf.is_tensor(self.call_args[0]))\n \n-    if not keras_tensor.keras_tensors_enabled():\n-      # Create TensorFlowOpLayers if needed.\n+    if not tf.compat.v1.executing_eagerly_outside_functions():\n+      # Create TensorFlowOpLayers if needed (in TF1)\n       for obj in self._flat_arguments:\n         if (isinstance(obj, tf.Tensor) and\n             base_layer_utils.needs_keras_history(\n\n@@ -1700,23 +1700,6 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n           },\n           run_eagerly=testing_utils.should_run_eagerly())\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n-  def test_sparse_op_with_op_layer(self):\n-    with testing_utils.use_keras_tensors_scope(False):\n-      # The meaningful error is only raised w/o KerasTensors.\n-      # It's tricky to raise the exact same error w/ KerasTensors enabled.\n-      # We may want to add dispatching to the sparse_ops and have dispatch\n-      # trigger on attributeerror so that these ops fully work w/ KerasTensors.\n-      # This may need to wait until dispatch v2\n-      inputs = layers_module.Input(\n-          shape=(2,), sparse=True, name='sparse_tensor')\n-      output = tf.sparse.minimum(inputs, inputs)\n-      with self.assertRaisesRegex(\n-          ValueError, 'not supported by Keras automatic '\n-          'op wrapping'):\n-        training_module.Model([inputs], output)\n-\n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_predict_error_with_empty_x(self):\n     inputs = layers_module.Input(shape=(2,))\n\n@@ -31,7 +31,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import keras_tensor\n from keras.engine import training_utils_v1\n-from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n \n \n@@ -51,17 +50,6 @@ class ModelInputsTest(tf.test.TestCase):\n   def test_single_thing_eager(self):\n     if not tf.executing_eagerly():\n       self.skipTest('Run in eager mode only.')\n-    with testing_utils.use_keras_tensors_scope(False):\n-      a = np.ones(10, dtype=np.int32)\n-      model_inputs = training_utils_v1.ModelInputs(a)\n-      self.assertEqual(['input_1'], model_inputs.get_input_names())\n-      val = model_inputs.get_symbolic_inputs()\n-      self.assertTrue(tf_utils.is_symbolic_tensor(val))\n-      vals = model_inputs.get_symbolic_inputs(return_single_as_list=True)\n-      self.assertEqual(1, len(vals))\n-      self.assertTrue(tf_utils.is_symbolic_tensor(vals[0]))\n-      self.assertEqual(tf.int32, vals[0].dtype)\n-    with testing_utils.use_keras_tensors_scope(True):\n     a = np.ones(10, dtype=np.int32)\n     model_inputs = training_utils_v1.ModelInputs(a)\n     self.assertEqual(['input_1'], model_inputs.get_input_names())\n@@ -83,14 +71,6 @@ class ModelInputsTest(tf.test.TestCase):\n   def test_list_eager(self):\n     if not tf.executing_eagerly():\n       self.skipTest('Run in eager mode only.')\n-    with testing_utils.use_keras_tensors_scope(False):\n-      a = [np.ones(10), np.ones(20)]\n-      model_inputs = training_utils_v1.ModelInputs(a)\n-      self.assertEqual(['input_1', 'input_2'], model_inputs.get_input_names())\n-      vals = model_inputs.get_symbolic_inputs()\n-      self.assertTrue(tf_utils.is_symbolic_tensor(vals[0]))\n-      self.assertTrue(tf_utils.is_symbolic_tensor(vals[1]))\n-    with testing_utils.use_keras_tensors_scope(True):\n     a = [np.ones(10), np.ones(20)]\n     model_inputs = training_utils_v1.ModelInputs(a)\n     self.assertEqual(['input_1', 'input_2'], model_inputs.get_input_names())\n@@ -109,14 +89,6 @@ class ModelInputsTest(tf.test.TestCase):\n   def test_dict_eager(self):\n     if not tf.executing_eagerly():\n       self.skipTest('Run in eager mode only.')\n-    with testing_utils.use_keras_tensors_scope(False):\n-      a = {'b': np.ones(10), 'a': np.ones(20)}\n-      model_inputs = training_utils_v1.ModelInputs(a)\n-      self.assertEqual(['a', 'b'], model_inputs.get_input_names())\n-      vals = model_inputs.get_symbolic_inputs()\n-      self.assertTrue(tf_utils.is_symbolic_tensor(vals['a']))\n-      self.assertTrue(tf_utils.is_symbolic_tensor(vals['b']))\n-    with testing_utils.use_keras_tensors_scope(True):\n     a = {'b': np.ones(10), 'a': np.ones(20)}\n     model_inputs = training_utils_v1.ModelInputs(a)\n     self.assertEqual(['a', 'b'], model_inputs.get_input_names())\n\n@@ -43,8 +43,10 @@ keras_export(v1=['keras.initializers.Orthogonal',\n                  'keras.initializers.orthogonal'], allow_multiple_exports=True)(_v1_orthogonal_initializer)\n keras_export(v1=['keras.initializers.Identity',\n                  'keras.initializers.identity'], allow_multiple_exports=True)(_v1_identity)\n-keras_export(v1=['keras.initializers.glorot_uniform'], allow_multiple_exports=True)(_v1_glorot_uniform_initializer)\n-keras_export(v1=['keras.initializers.glorot_normal'], allow_multiple_exports=True)(_v1_glorot_normal_initializer)\n+keras_export(v1=['keras.initializers.glorot_uniform'], allow_multiple_exports=True)(\n+    _v1_glorot_uniform_initializer)\n+keras_export(v1=['keras.initializers.glorot_normal'], allow_multiple_exports=True)(\n+    _v1_glorot_normal_initializer)\n \n \n @keras_export(v1=['keras.initializers.RandomNormal',\n\n@@ -24,10 +24,10 @@ import collections\n import json\n \n import numpy as np\n-from keras import backend as K\n from keras.engine import base_preprocessing_layer\n from keras.engine.base_preprocessing_layer import Combiner\n from keras.utils import tf_utils\n+from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -140,11 +140,12 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     Same as input shape.\n \n   Attributes:\n-    bins: Optional boundary specification or number of bins to compute if `int`.\n-      Bins exclude the left boundary and include the right boundary,\n-      so `bins=[0., 1., 2.]` generates bins\n-      `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`.\n-      This would correspond to bins = 4.\n+    bin_boundaries: A list of bin boundaries. The leftmost and rightmost bins\n+      will always extend to `-inf` and `inf`, so `bin_boundaries=[0., 1., 2.]`\n+      generates bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`. If\n+      this option is set, `adapt` should not be called.\n+    num_bins: The integer number of bins to compute. If this option is set,\n+      `adapt` should be called to learn the bin boundaries.\n     epsilon: Error tolerance, typically a small fraction close to zero (e.g.\n       0.01). Higher values of epsilon increase the quantile approximation, and\n       hence result in more unequal buckets, but could improve performance\n@@ -155,7 +156,7 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   Bucketize float values based on provided buckets.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n   >>> layer = tf.keras.layers.experimental.preprocessing.Discretization(\n-  ...          bins=[0., 1., 2.])\n+  ...          bin_boundaries=[0., 1., 2.])\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n   array([[0, 1, 3, 1],\n@@ -164,7 +165,7 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   Bucketize float values based on a number of buckets to compute.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n   >>> layer = tf.keras.layers.experimental.preprocessing.Discretization(\n-  ...          bins=4, epsilon=0.01)\n+  ...          num_bins=4, epsilon=0.01)\n   >>> layer.adapt(input)\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n@@ -173,35 +174,53 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   \"\"\"\n \n   def __init__(self,\n-               bins,\n+               bin_boundaries=None,\n+               num_bins=None,\n                epsilon=0.01,\n                **kwargs):\n+    # bins is a deprecated arg for setting bin_boundaries or num_bins that still\n+    # has some usage.\n+    if \"bins\" in kwargs:\n+      logging.warning(\n+          \"bins is deprecated, please use bin_boundaries or num_bins instead.\")\n+      if isinstance(kwargs[\"bins\"], int) and num_bins is None:\n+        num_bins = kwargs[\"bins\"]\n+      elif bin_boundaries is None:\n+        bin_boundaries = kwargs[\"bins\"]\n+      del kwargs[\"bins\"]\n     super(Discretization, self).__init__(\n         combiner=Discretization.DiscretizingCombiner(\n-            epsilon, bins if isinstance(bins, int) else 1),\n+            epsilon, num_bins if num_bins is not None else 1),\n         **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\n         \"Discretization\").set(True)\n-    if bins is not None and not isinstance(bins, int):\n-      self.bins = np.append(bins, [np.Inf])\n-    else:\n-      self.bins = np.zeros(bins)\n-    # Need this to return correct config\n-    self.input_bins = bins\n+    if num_bins is not None and num_bins < 0:\n+      raise ValueError(\"`num_bins` must be must be greater than or equal to 0. \"\n+                       \"You passed `num_bins={}`\".format(num_bins))\n+    if num_bins is not None and bin_boundaries is not None:\n+      raise ValueError(\"Both `num_bins` and `bin_boundaries` should not be \"\n+                       \"set. You passed `num_bins={}` and \"\n+                       \"`bin_boundaries={}`\".format(num_bins, bin_boundaries))\n+    self.bin_boundaries = bin_boundaries\n+    self.num_bins = num_bins\n     self.epsilon = epsilon\n \n   def build(self, input_shape):\n+    if self.bin_boundaries is not None:\n+      initial_bins = np.append(self.bin_boundaries, [np.Inf])\n+    else:\n+      initial_bins = np.zeros(self.num_bins)\n     self.bins = self._add_state_variable(\n         name=_BINS_NAME,\n-        shape=(self.bins.size,),\n+        shape=(initial_bins.size,),\n         dtype=tf.float32,\n-        initializer=tf.compat.v1.constant_initializer(self.bins))\n+        initializer=tf.compat.v1.constant_initializer(initial_bins))\n     super(Discretization, self).build(input_shape)\n \n   def get_config(self):\n     config = {\n-        \"bins\": None if self.input_bins is None else (\n-            K.get_value(self.input_bins)),\n+        \"bin_boundaries\": self.bin_boundaries,\n+        \"num_bins\": self.num_bins,\n         \"epsilon\": self.epsilon,\n     }\n     base_config = super(Discretization, self).get_config()\n\n@@ -47,7 +47,7 @@ class DiscretizationDistributionTest(\n \n     with distribution.scope():\n       input_data = keras.Input(shape=(4,))\n-      layer = discretization.Discretization(bins=[0., 1., 2.])\n+      layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])\n       bucket_data = layer(input_data)\n       self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n\n@@ -50,7 +50,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, 4]\n \n     input_data = keras.Input(shape=(4,))\n-    layer = discretization.Discretization(bins=[0., 1., 2.])\n+    layer = get_layer_class()(bin_boundaries=[0., 1., 2.])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -65,7 +65,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, 4]\n \n     input_data = keras.Input(shape=(4,), dtype=tf.int64)\n-    layer = discretization.Discretization(bins=[-.5, 0.5, 1.5])\n+    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -79,7 +79,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n         indices=indices, values=[-1.5, 1.0, 3.4], dense_shape=[2, 3])\n     expected_output = [0, 2, 3]\n     input_data = keras.Input(shape=(3,), dtype=tf.float32, sparse=True)\n-    layer = discretization.Discretization(bins=[-.5, 0.5, 1.5])\n+    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -95,7 +95,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, None]\n \n     input_data = keras.Input(shape=(None,), ragged=True)\n-    layer = discretization.Discretization(bins=[0., 1., 2.])\n+    layer = get_layer_class()(bin_boundaries=[0., 1., 2.])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -111,7 +111,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, None]\n \n     input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.int64)\n-    layer = discretization.Discretization(bins=[-.5, 0.5, 1.5])\n+    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -124,7 +124,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n         indices=indices, values=[-1, 1, 3], dense_shape=[2, 3])\n     expected_output = [0, 2, 3]\n     input_data = keras.Input(shape=(3,), dtype=tf.int32, sparse=True)\n-    layer = discretization.Discretization(bins=[-.5, 0.5, 1.5])\n+    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -132,6 +132,16 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     self.assertAllEqual(indices, output_dataset.indices)\n     self.assertAllEqual(expected_output, output_dataset.values)\n \n+  def test_num_bins_negative_fails(self):\n+    with self.assertRaisesRegex(ValueError, \"`num_bins` must be.*num_bins=-7\"):\n+      _ = get_layer_class()(num_bins=-7)\n+\n+  def test_num_bins_and_bins_set_fails(self):\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r\"`num_bins` and `bin_boundaries` should not be set.*5.*\\[1, 2\\]\"):\n+      _ = get_layer_class()(num_bins=5, bins=[1, 2])\n+\n   @parameterized.named_parameters([\n       {\n           \"testcase_name\": \"2d_single_element\",\n@@ -198,7 +208,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n           test_data.shape[0] // 2)\n \n     cls = get_layer_class()\n-    layer = cls(epsilon=epsilon, bins=num_bins)\n+    layer = cls(epsilon=epsilon, num_bins=num_bins)\n     layer.adapt(adapt_data)\n \n     input_data = keras.Input(shape=input_shape)\n\n@@ -49,55 +49,45 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   basis layer for both IntegerLookup and StringLookup; it holds the common\n   logic but is not intended to be exported as part of the Keras API.\n \n-  If desired, the user can call this layer's `adapt()` method on a data set,\n-  which will analyze the data set, determine the frequency of individual\n-  hashable values, and create a vocabulary from them. This vocabulary can have\n-  unlimited size or be capped, depending on the configuration options for this\n-  layer; if there are more unique values in the input than the maximum\n-  vocabulary size, the most frequent terms will be used to create the\n-  vocabulary.\n-\n   Args:\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this vocabulary\n-      includes the OOV and mask tokens, so the effective number of tokens is\n-      (max_tokens - num_oov_indices - (1 if mask_token else 0))\n+      there is no cap on the size of the vocabulary. Note that this size\n+      includes the OOV and mask tokens.\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n-      value is more than 1, OOV inputs are hashed to determine their OOV value;\n-      if this value is 0, passing an OOV input will result in a '-1' being\n-      returned for that value in the output tensor. (Note that, because the\n-      value is -1 and not 0, this will allow you to effectively drop OOV values\n-      from categorical encodings.)\n-    mask_token: A token that represents masked values, and which is mapped to\n-      index 0. If set to None, no mask term will be added and the OOV tokens, if\n-      any, will be indexed from (0...num_oov_indices) instead of\n-      (1...num_oov_indices+1).\n-    oov_token: The token representing an out-of-vocabulary value. This token is\n-      only used when performing an inverse lookup.\n+      value is more than 1, OOV inputs are hashed to determine their OOV value.\n+      If this value is 0, OOV inputs will map to -1 when `output_mode` is \"int\"\n+      and are dropped otherwise.\n+    mask_token: A token that represents masked inputs. When `output_mode` is\n+      \"int\", the token is included in vocabulary and mapped to index 0. In other\n+      output modes, the token will not appear in the vocabulary and instances\n+      of the mask token in the input will be dropped. If set to None, no mask\n+      term will be added.\n+    oov_token: Only used when `invert` is True. The token to return for OOV\n+      indices.\n     vocabulary: An optional list of vocabulary terms. If the list contains the\n       same token multiple times, an error will be thrown.\n-    invert: If true, this layer will map indices to vocabulary items instead\n-      of mapping vocabulary items to indices.\n-    output_mode: Specification for the output of the layer. Only applicable\n-      when `invert` is False.\n-      Defaults to \"int\". Values can\n-      be \"int\", \"binary\", or \"count\", configuring the layer as follows:\n-        \"int\": Return the raw integer indices of the input values.\n-        \"binary\": Outputs a single int array per batch, of either vocab_size or\n+    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n+      indices to vocabulary items instead of mapping vocabulary items to\n+      indices. Default to False.\n+    output_mode: Specification for the output of the layer. Defaults to \"int\".\n+      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n+      as follows:\n+        \"int\": Return the raw integer indices of the input tokens.\n+        \"binary\": Outputs a single int array per sample, of either vocab_size or\n           max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the batch item.\n+          to that index exists at least once in the sample.\n         \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n+          of times the token at that index appeared in the sample.\n         \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n           value in each token slot.\n-    pad_to_max_tokens: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes. If\n-      True, the output will have its feature axis padded to `max_tokens` even if\n-      the number of unique tokens in the vocabulary is less than max_tokens,\n-      resulting in a tensor of shape [batch_size, max_tokens] regardless of\n-      vocabulary size. Defaults to False.\n+    pad_to_max_tokens: Only valid when `output_mode` is \"binary\", \"count\", or\n+      \"tf-idf\". If True, the output will have its feature axis padded to\n+      `max_tokens` even if the number of unique tokens in the vocabulary is less\n+      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n+      regardless of vocabulary size. Defaults to False.\n     sparse: Boolean. Only applicable to \"binary\" and \"count\" output modes.\n-      If true, returns a `SparseTensor` instead of a dense `Tensor`.\n-      Defaults to `False`.\n+      If True, returns a `SparseTensor` instead of a dense `Tensor`.\n+      Defaults to False.\n   \"\"\"\n \n   def __init__(self,\n@@ -142,9 +132,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self.sparse = sparse\n     self.pad_to_max_tokens = pad_to_max_tokens\n     self._called = False\n-    self._num_special_tokens = self.num_oov_indices\n-    if self.mask_token is not None:\n-      self._num_special_tokens += 1\n     self._vocab_size = 0\n     # We need to keep track our current vocab size outside of our layer weights\n     # to support a static output shape when `output_mode != INT`. The bincount\n@@ -155,19 +142,8 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       self._vocab_size = kwargs[\"vocab_size\"]\n       del kwargs[\"vocab_size\"]\n \n-    # If there is only one OOV bucket, we can determine the OOV value (either 0\n-    # or 1 depending on whether 0 is reserved) and set that as the default\n-    # value of the index_lookup table. If we hav multiple OOV values, we need to\n-    # do a further hashing step; to make this easier, we set the OOV value to\n-    # -1. (This lets us do a vectorized add and cast to boolean to determine\n-    # locations where we need to do extra hashing.)\n-    if self.num_oov_indices == 1:\n-      self._oov_value = 0 if mask_token is None else 1\n-    else:\n-      self._oov_value = -1\n-\n     if max_tokens is not None:\n-      available_vocab_size = max_tokens - self._num_special_tokens\n+      available_vocab_size = max_tokens - self._token_start_index()\n     else:\n       available_vocab_size = None\n \n@@ -184,23 +160,42 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if invert:\n       self._key_dtype = tf.int64\n       self._value_dtype = self.dtype\n-      oov_value = self.oov_token\n+      self._mask_key = 0\n+      self._mask_value = mask_token\n+      default_value = self.oov_token\n       oov_indices = None\n     else:\n       self._key_dtype = self.dtype\n       self._value_dtype = tf.int64\n-      oov_value = self._oov_value\n-      if self.num_oov_indices <= 1:\n+      self._mask_key = mask_token\n+      # Masks should map to 0 for int output and be dropped otherwise. Max ints\n+      # will be dropped from the bincount op.\n+      self._mask_value = 0 if self.output_mode == INT else tf.int64.max\n+      oov_start = self._oov_start_index()\n+      token_start = self._token_start_index()\n+      if self.num_oov_indices == 0:\n+        # If there are no OOV indices, we map OOV tokens to -1 for int output\n+        # and drop them from bagged output. Max ints will be dropped from the\n+        # bincount op.\n+        default_value = -1 if self.output_mode == INT else tf.int64.max\n+        oov_indices = None\n+      elif self.num_oov_indices == 1:\n+        # If there is only one OOV index, we can set that index as the default\n+        # value of the index_lookup table.\n+        default_value = oov_start\n         oov_indices = None\n       else:\n-        oov_start = 1 if mask_token is not None else 0\n-        oov_end = oov_start + num_oov_indices\n-        oov_indices = list(range(oov_start, oov_end))\n+        # If we hav multiple OOV values, we need to do a further hashing step;\n+        # to make this easier, we set the OOV value to -1. (This lets us do a\n+        # vectorized add and cast to boolean to determine locations where we\n+        # need to do extra hashing.)\n+        default_value = -1\n+        oov_indices = list(range(oov_start, token_start))\n \n     if vocabulary is not None and isinstance(vocabulary,\n                                              tf.lookup.TextFileInitializer):\n       self._table = self._static_table_class()(\n-          vocabulary, default_value=oov_value)\n+          vocabulary, default_value=default_value)\n       self._table_handler = table_utils.TableHandler(\n           table=self._table,\n           mask_token=mask_token,\n@@ -213,7 +208,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       self._table = lookup_ops.MutableHashTable(\n           key_dtype=self._key_dtype,\n           value_dtype=self._value_dtype,\n-          default_value=oov_value,\n+          default_value=default_value,\n           name=(self._name + \"_index_table\"))\n       self._table_handler = table_utils.TableHandler(\n           table=self._table,\n@@ -282,23 +277,18 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     super(IndexLookup, self).adapt(data, reset_state)\n \n   def get_vocabulary(self):\n-    if self.vocab_size() == 0:\n+    if self._vocab_size == 0:\n       return []\n \n+    # The MutableHashTable data will not be sorted, so we will create a inverted\n+    # lookup here, and use that to lookup a range of indices [0, vocab_size).\n     keys, values = self._table_handler.data()\n-    # This is required because the MutableHashTable doesn't preserve insertion\n-    # order, but we rely on the order of the array to assign indices.\n     if self.invert:\n-      # If we are inverting, the vocabulary is in the values instead of keys.\n-      tokens = [x for _, x in sorted(zip(keys, values))]\n+      index_to_token = zip(keys, values)\n     else:\n-      tokens = [x for _, x in sorted(zip(values, keys))]\n-    # OOV values are not actually in the vocab, we need to add them in manually.\n-    if self.num_oov_indices > 0:\n-      oov_start_index = 1 if self.mask_token is not None else 0\n-      oov_values = [self.oov_token] * self.num_oov_indices\n-      tokens[oov_start_index:oov_start_index] = oov_values\n-    return tokens\n+      index_to_token = zip(values, keys)\n+    lookup = collections.defaultdict(lambda: self.oov_token, index_to_token)\n+    return [lookup[x] for x in range(self._vocab_size)]\n \n   def vocab_size(self):\n     return self._vocab_size\n@@ -356,21 +346,19 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                          \"False, the vocabulary cannot be changed after the \"\n                          \"layer is called.\".format(self.output_mode))\n \n-    should_have_mask = self.mask_token is not None\n-    has_mask = vocab[0] == self.mask_token\n-    oov_start = 1 if should_have_mask else 0\n+    oov_start = self._oov_start_index()\n+    token_start = self._token_start_index()\n+    should_have_mask = (oov_start > 0)\n+    has_mask = should_have_mask and vocab[0] == self.mask_token\n \n     should_have_oov = (self.num_oov_indices > 0)\n-    if should_have_oov:\n-      oov_end = oov_start + self.num_oov_indices\n     expected_oov = [self.oov_token] * self.num_oov_indices\n-      has_oov = vocab[oov_start:oov_end] == expected_oov\n+    found_oov = vocab[oov_start:token_start]\n+    has_oov = should_have_oov and found_oov == expected_oov\n     # If we get a numpy array, then has_oov may end up being a numpy array\n     # instead of a bool. Fix this by collapsing the variable if it's not bool.\n     if not isinstance(has_oov, bool):\n       has_oov = any(has_oov)\n-    else:\n-      has_oov = False\n \n     if all([should_have_mask, has_mask, should_have_oov]) and not has_oov:\n       raise ValueError(\n@@ -384,8 +372,8 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n               mask=self.mask_token,\n               oov=self.oov_token,\n               start=oov_start,\n-              end=oov_end,\n-              found=vocab[oov_start:oov_end]))\n+              end=token_start,\n+              found=found_oov))\n \n     if all([should_have_oov, has_oov, should_have_mask]) and not has_mask:\n       raise ValueError(\n@@ -399,13 +387,12 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n               mask=self.mask_token,\n               oov=self.oov_token,\n               start=oov_start,\n-              end=oov_end,\n+              end=token_start,\n               found=vocab[0]))\n \n     found_special_tokens = has_oov or has_mask\n-\n     if found_special_tokens:\n-      tokens = vocab[self._num_special_tokens:]\n+      tokens = vocab[token_start:]\n     else:\n       tokens = vocab\n \n@@ -428,7 +415,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                        \"OOV token for this layer.\".format(\n                            self.oov_token, tokens.index(self.oov_token)))\n \n-    self._vocab_size = len(tokens) + self._num_special_tokens\n+    self._vocab_size = token_start + len(tokens)\n     if self.max_tokens is not None and self._vocab_size > self.max_tokens:\n       raise ValueError(\n           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\n@@ -452,16 +439,13 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # hash table. OOV tokens are handled with the hash table default value and\n     # not added directly.\n     self._table_handler.clear()\n-    start_index = self._num_special_tokens\n-    indices = np.arange(start_index, len(tokens) + start_index, dtype=np.int64)\n+    indices = np.arange(token_start, len(tokens) + token_start, dtype=np.int64)\n     if self.invert:\n       self._table_handler.insert(indices, tokens)\n-      if self.mask_token is not None:\n-        self._table_handler.insert([0], [self.mask_token])\n     else:\n       self._table_handler.insert(tokens, indices)\n     if self.mask_token is not None:\n-        self._table_handler.insert([self.mask_token], [0])\n+      self._table_handler.insert([self._mask_key], [self._mask_value])\n \n     if self.output_mode == TFIDF:\n       # If the passed vocabulary has no special tokens, we need to pad the front\n@@ -472,7 +456,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         front_padding = 0\n         front_padding_value = 0\n       else:\n-        front_padding = self._num_special_tokens\n+        front_padding = token_start\n         front_padding_value = np.average(idf_weights)\n       # If pad_to_max_tokens is true, and max_tokens is greater than our total\n       # vocab size, we need to pad the back of idf_weights with zeros as well.\n@@ -532,6 +516,12 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _static_table_class(self):\n     return tf.lookup.StaticHashTable\n \n+  def _oov_start_index(self):\n+    return 1 if self.mask_token is not None and self.output_mode == INT else 0\n+\n+  def _token_start_index(self):\n+    return self._oov_start_index() + self.num_oov_indices\n+\n \n class _IndexLookupAccumulator(\n     collections.namedtuple(\"Accumulator\",\n\n@@ -247,21 +247,17 @@ def _get_end_to_end_test_cases():\n           \"input_data\":\n               np.array([[1138], [1729], [725], [42], [42], [725], [1138], [4]]),\n           \"kwargs\": {\n-              \"max_tokens\": 6,\n+              \"max_tokens\": 5,\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n               \"output_mode\": index_lookup.TFIDF,\n               \"dtype\": tf.int64,\n           },\n-          \"expected_output\": [[0, 0, 1.098612, 0, 0, 0],\n-                              [0, 0, 0, 1.252763, 0, 0],\n-                              [0, 0, 0, 0, 1.466337, 0],\n-                              [0, 0, 0, 0, 0, 1.7917595],\n-                              [0, 0, 0, 0, 0, 1.7917595],\n-                              [0, 0, 0, 0, 1.4663371, 0],\n-                              [0, 0, 1.098612, 0, 0, 0],\n-                              [0, 1.402368, 0, 0, 0, 0]],\n+          \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n+                              [0, 0, 0, 1.466337, 0], [0, 0, 0, 0, 1.7917595],\n+                              [0, 0, 0, 0, 1.7917595], [0, 0, 0, 1.4663371, 0],\n+                              [0, 1.098612, 0, 0, 0], [1.402368, 0, 0, 0, 0]],\n           \"input_dtype\":\n               tf.int64\n       },\n@@ -275,21 +271,17 @@ def _get_end_to_end_test_cases():\n               np.array([[\"earth\"], [\"wind\"], [\"and\"], [\"fire\"], [\"fire\"],\n                         [\"and\"], [\"earth\"], [\"michigan\"]]),\n           \"kwargs\": {\n-              \"max_tokens\": 6,\n+              \"max_tokens\": 5,\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n               \"output_mode\": index_lookup.TFIDF,\n               \"dtype\": tf.string,\n           },\n-          \"expected_output\": [[0, 0, 1.098612, 0, 0, 0],\n-                              [0, 0, 0, 1.252763, 0, 0],\n-                              [0, 0, 0, 0, 1.466337, 0],\n-                              [0, 0, 0, 0, 0, 1.7917595],\n-                              [0, 0, 0, 0, 0, 1.7917595],\n-                              [0, 0, 0, 0, 1.4663371, 0],\n-                              [0, 0, 1.098612, 0, 0, 0],\n-                              [0, 1.402368, 0, 0, 0, 0]],\n+          \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n+                              [0, 0, 0, 1.466337, 0], [0, 0, 0, 0, 1.7917595],\n+                              [0, 0, 0, 0, 1.7917595], [0, 0, 0, 1.4663371, 0],\n+                              [0, 1.098612, 0, 0, 0], [1.402368, 0, 0, 0, 0]],\n           \"input_dtype\":\n               tf.string\n       },\n@@ -754,6 +746,25 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_int_output_no_oov(self):\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"ohio\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[1, 2, 3, -1], [4, 3, 1, -1]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        max_tokens=None,\n+        num_oov_indices=0,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string)\n+    layer.set_vocabulary(vocab_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n   def test_int_output_explicit_vocab(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n@@ -779,13 +790,13 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\", \"\"],\n                             [\"fire\", \"fire\", \"and\", \"earth\", \"michigan\"]])\n     expected_output = [\n-        [1, 0, 1, 1, 1, 1, 0],\n-        [0, 1, 1, 0, 1, 1, 0],\n+        [0, 1, 1, 1, 1, 0],\n+        [1, 1, 0, 1, 1, 0],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = get_layer_class()(\n-        max_tokens=7,\n+        max_tokens=6,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n@@ -798,21 +809,46 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_binary_output_no_oov(self):\n+    \"\"\"Check binary output when pad_to_max_tokens=True.\"\"\"\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\", \"ohio\"],\n+                            [\"fire\", \"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [\n+        [1, 1, 1, 1, 0],\n+        [1, 0, 1, 1, 0],\n+    ]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        max_tokens=5,\n+        num_oov_indices=0,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        output_mode=index_lookup.BINARY,\n+        pad_to_max_tokens=True,\n+        dtype=tf.string)\n+    layer.set_vocabulary(vocab_data)\n+    binary_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=binary_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n   def test_binary_output_hard_maximum_multiple_adapts(self):\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n                             [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n     adapt_data = [\"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\"]\n     first_expected_output = [\n-        [0, 1, 1, 1, 0],\n-        [0, 1, 1, 0, 0],\n+        [1, 1, 1, 0, 0],\n+        [1, 1, 0, 0, 0],\n     ]\n     second_adapt_data = [\n         \"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\", \"and\",\n         \"and\", \"fire\"\n     ]\n     second_expected_output = [\n-        [0, 0, 1, 1, 1],\n-        [0, 1, 1, 0, 1],\n+        [0, 1, 1, 1, 0],\n+        [1, 1, 0, 1, 0],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -839,11 +875,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n   def test_binary_output_soft_maximum(self):\n     \"\"\"Check binary output when pad_to_max_tokens=False.\"\"\"\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\", \"\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\", \"\"]])\n     expected_output = [\n-        [0, 0, 1, 1, 1, 1],\n-        [0, 1, 1, 0, 1, 1],\n+        [0, 1, 1, 1, 1],\n+        [1, 1, 0, 1, 1],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -875,16 +911,16 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n   def test_count_output_hard_maxiumum(self):\n     \"\"\"Check count output when pad_to_max_tokens=True.\"\"\"\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"wind\"],\n-                            [\"fire\", \"fire\", \"fire\", \"michigan\"]])\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"wind\", \"\"],\n+                            [\"fire\", \"fire\", \"fire\", \"michigan\", \"\"]])\n     expected_output = [\n-        [0, 0, 1, 2, 1, 0, 0],\n-        [0, 1, 0, 0, 0, 3, 0],\n+        [0, 1, 2, 1, 0, 0],\n+        [1, 0, 0, 0, 3, 0],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = get_layer_class()(\n-        max_tokens=7,\n+        max_tokens=6,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n@@ -900,11 +936,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n   def test_count_output_soft_maximum(self):\n     \"\"\"Check count output when pad_to_max_tokens=False.\"\"\"\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"wind\"],\n-                            [\"fire\", \"fire\", \"fire\", \"michigan\"]])\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"wind\", \"\"],\n+                            [\"fire\", \"fire\", \"fire\", \"michigan\", \"\"]])\n     expected_output = [\n-        [0, 0, 1, 2, 1, 0],\n-        [0, 1, 0, 0, 0, 3],\n+        [0, 1, 2, 1, 0],\n+        [1, 0, 0, 0, 3],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -938,16 +974,16 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     # OOV idf weight (bucket 0) should 0.5, the average of passed weights.\n     idf_weights = [.4, .25, .75, .6]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n-                            [\"ohio\", \"fire\", \"earth\", \"michigan\"]])\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\", \"\"],\n+                            [\"ohio\", \"fire\", \"earth\", \"michigan\", \"\"]])\n     expected_output = [\n-        [0.00, 0.00, 0.80, 0.25, 0.75, 0.00, 0.00],\n-        [0.00, 1.00, 0.40, 0.00, 0.00, 0.60, 0.00],\n+        [0.00, 0.80, 0.25, 0.75, 0.00, 0.00],\n+        [1.00, 0.40, 0.00, 0.00, 0.60, 0.00],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = get_layer_class()(\n-        max_tokens=7,\n+        max_tokens=6,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n@@ -965,11 +1001,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     # OOV idf weight (bucket 0) should 0.5, the average of passed weights.\n     idf_weights = [.4, .25, .75, .6]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n-                            [\"ohio\", \"fire\", \"earth\", \"michigan\"]])\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\", \"\"],\n+                            [\"ohio\", \"fire\", \"earth\", \"michigan\", \"\"]])\n     expected_output = [\n-        [0.00, 0.00, 0.80, 0.25, 0.75, 0.00],\n-        [0.00, 1.00, 0.40, 0.00, 0.00, 0.60],\n+        [0.00, 0.80, 0.25, 0.75, 0.00],\n+        [1.00, 0.40, 0.00, 0.00, 0.60],\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n\n@@ -30,58 +30,72 @@ class IntegerLookup(index_lookup.IndexLookup):\n   \"\"\"Maps integers from a vocabulary to integer indices.\n \n   This layer translates a set of arbitrary integers into an integer output via a\n-  table-based lookup, with optional out-of-vocabulary handling.\n+  table-based vocabulary lookup.\n \n-  If desired, the user can call this layer's `adapt()` method on a data set,\n-  which will analyze the data set, determine the frequency of individual integer\n-  values, and create a vocabulary from them. This vocabulary can have\n-  unlimited size or be capped, depending on the configuration options for this\n-  layer; if there are more unique values in the input than the maximum\n-  vocabulary size, the most frequent values will be used to create the\n-  vocabulary (and the values that don't make the cut will be treated as OOV).\n+  The vocabulary for the layer can be supplied on construction or learned via\n+  `adapt()`. During `adapt()`, the layer will analyze a data set, determine the\n+  frequency of individual integer values, and create a vocabulary from them. If\n+  the vocabulary is capped in size, the most frequent values will be used to\n+  create the vocabulary and all others will be treated as out-of-vocabulary\n+  (OOV).\n+\n+  There are two possible output modes for the layer.\n+  When `output_mode` is \"int\", input values are converted to their index in the\n+  vocabulary (an integer).\n+  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input strings\n+  are encoded into an array where each dimension corresponds to an element in\n+  the vocabulary.\n+\n+  The vocabulary can optionally contain a mask value as well as an OOV value\n+  (which can optionally occupy multiple indices in the vocabulary, as set\n+  by `num_oov_indices`).\n+  The position of these values in the vocabulary is fixed. When `output_mode` is\n+  \"int\", the vocabulary will begin with the mask value at index 0, followed by\n+  OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n+  \"binary\", \"count\", or \"tf-idf\" the vocabulary will begin with OOV indices and\n+  instances of the mask value will be dropped.\n \n   Args:\n     max_values: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this vocabulary\n-      includes the OOV and mask values, so the effective number of values is\n-      `(max_values - num_oov_values - (1 if mask_token else 0))`.\n-    num_oov_indices: The number of out-of-vocabulary values to use; defaults to\n-      1. If this value is more than 1, OOV inputs are modulated to determine\n-      their OOV value; if this value is 0, passing an OOV input will result in\n-      a '-1' being returned for that value in the output tensor. (Note that,\n-      because the value is -1 and not 0, this will allow you to effectively drop\n-      OOV values from categorical encodings.)\n-    mask_value: A value that represents masked inputs, and which is mapped to\n-      index 0. Defaults to 0. If set to None, no mask term will be added and the\n-      OOV values, if any, will be indexed from `(0...num_oov_values)` instead of\n-      `(1...num_oov_values + 1)`.\n-    oov_value: The value representing an out-of-vocabulary value. Defaults to\n-      -1.\n+      there is no cap on the size of the vocabulary. Note that this size\n+      includes the OOV and mask values. Default to None.\n+    num_oov_indices: The number of out-of-vocabulary values to use. If this\n+      value is more than 1, OOV inputs are modulated to determine their OOV\n+      value. If this value is 0, OOV inputs will map to -1 when `output_mode` is\n+      \"int\" and are dropped otherwise. Defaults to 1.\n+    mask_value: A value that represents masked inputs. When `output_mode` is\n+      \"int\", the value is included in vocabulary and mapped to index 0. In other\n+      output modes, the value will not appear in the vocabulary and instances\n+      of the mask value in the input will be dropped. If set to None, no mask\n+      term will be added. Defaults to 0.\n+    oov_value: Only used when `invert` is True. The value to return for OOV\n+      indices. Defaults to -1.\n     vocabulary: An optional list of values, or a path to a text file containing\n       a vocabulary to load into this layer. The file should contain one value\n-      per line. If the list or file contains the same token multiple times, an\n+      per line. If the list or file contains the same value multiple times, an\n       error will be thrown.\n-    invert: If true, this layer will map indices to vocabulary items instead\n-      of mapping vocabulary items to indices.\n-    output_mode: Specification for the output of the layer. Only applicable\n-      when `invert` is False. Defaults to \"int\". Values can be \"int\", \"binary\",\n-      or \"count\", configuring the layer as follows:\n+    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n+      indices to vocabulary items instead of mapping vocabulary items to\n+      indices. Default to False.\n+    output_mode: Specification for the output of the layer. Defaults to \"int\".\n+      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n+      as follows:\n         \"int\": Return the raw integer indices of the input values.\n-        \"binary\": Outputs a single int array per batch, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the batch item.\n+        \"binary\": Outputs a single int array per sample, of either vocab_size or\n+          max_values size, containing 1s in all elements where the value mapped\n+          to that index exists at least once in the sample.\n         \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n+          of times the value at that index appeared in the sample.\n         \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n-    pad_to_max_values: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes. If\n-      True, the output will have its feature axis padded to `max_tokens` even if\n-      the number of unique tokens in the vocabulary is less than max_tokens,\n-      resulting in a tensor of shape [batch_size, max_tokens] regardless of\n-      vocabulary size. Defaults to True.\n-    sparse: Boolean. Only applicable to \"binary\" and \"count\" output modes.\n-      If true, returns a `SparseTensor` instead of a dense `Tensor`.\n-      Defaults to `False`.\n+          value in each value slot.\n+    pad_to_max_values: Only applicable when `output_mode` is \"binary\", \"count\",\n+      or \"tf-idf\". If True, the output will have its feature axis padded to\n+      `max_values` even if the number of unique values in the vocabulary is less\n+      than max_values, resulting in a tensor of shape [batch_size, max_values]\n+      regardless of vocabulary size. Defaults to False.\n+    sparse: Boolean. Only applicable when `output_mode` is \"binary\", \"count\",\n+      or \"tf-idf\". If True, returns a `SparseTensor` instead of a dense\n+      `Tensor`. Defaults to False.\n \n   Examples:\n \n@@ -143,42 +157,39 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   **Multi-hot output**\n \n-  Configure the layer with `output_mode='binary'`. Note that the first two\n-  dimensions in the binary encoding represent the mask value and the OOV value\n-  (if any).\n+  Configure the layer with `output_mode='binary'`. Note that the first\n+  `num_oov_indices` dimensions in the binary encoding represent OOV values\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n   >>> layer = IntegerLookup(vocabulary=vocab, output_mode='binary')\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0., 0., 1., 0., 1., 1.],\n-           [0., 1., 0., 1., 0., 1.]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0., 1., 0., 1., 1.],\n+           [1., 0., 1., 0., 1.]], dtype=float32)>\n \n   **Value count output**\n \n   Configure the layer with `output_mode='count'`. As with binary output, the\n-  first two dimensions in the output represent the mask value and the OOV value\n-  (if any).\n+  first `num_oov_indices` dimensions in the output represent OOV values.\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n   >>> layer = IntegerLookup(vocabulary=vocab, output_mode='count')\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0., 0., 1., 0., 1., 2.],\n-           [0., 2., 0., 1., 0., 1.]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0., 1., 0., 1., 2.],\n+           [2., 0., 1., 0., 1.]], dtype=float32)>\n \n   **TF-IDF output**\n \n   Configure the layer with `output_mode='tf-idf'`. As with binary output, the\n-  first two dimensions in the output represent the mask value and the OOV value,\n-  respectively.\n+  first `num_oov_indices` dimensions in the output represent OOV values.\n \n   Each value bin will output `value_count * idf_weight`, where the idf weights\n   are the inverse document frequency weights per value. These should be provided\n-  along with the vocabulary. Note that the `idf_weight` for mask values and OOV\n-  values will default to the average of all idf weights passed in.\n+  along with the vocabulary. Note that the `idf_weight` for OOV values will\n+  default to the average of all idf weights passed in.\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n@@ -186,22 +197,22 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> layer = IntegerLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0.  , 0.  , 0.25, 0.  , 0.6 , 0.8 ],\n-           [0.  , 1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n+           [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  To specify the idf weights for mask and oov values, you will need to pass the\n-  entire vocabularly including these values.\n+  To specify the idf weights for oov values, you will need to pass the entire\n+  vocabularly including the leading oov value.\n \n-  >>> vocab = [0, -1, 12, 36, 1138, 42]\n-  >>> idf_weights = [0.0, 0.9, 0.25, 0.75, 0.6, 0.4]\n+  >>> vocab = [-1, 12, 36, 1138, 42]\n+  >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n   >>> layer = IntegerLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0.  , 0.  , 0.25, 0.  , 0.6 , 0.8 ],\n-           [0.  , 1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n+           [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n   When adapting the layer in tf-idf mode, each input sample will be considered a\n   document, and idf weight per value will be calculated as\n\n@@ -434,7 +434,7 @@ class IntegerLookupVocabularyTest(\n   def test_binary_output(self):\n     vocab_data = [2, 3, 4, 5]\n     input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 2]])\n-    expected_output = [[0, 0, 1, 1, 1, 0], [1, 1, 1, 0, 0, 1]]\n+    expected_output = [[0, 1, 1, 1, 0], [1, 1, 0, 0, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"binary\")\n@@ -446,7 +446,7 @@ class IntegerLookupVocabularyTest(\n   def test_count_output(self):\n     vocab_data = [2, 3, 4, 5]\n     input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 6]])\n-    expected_output = [[0, 0, 2, 1, 1, 0], [1, 2, 0, 0, 0, 1]]\n+    expected_output = [[0, 2, 1, 1, 0], [2, 0, 0, 0, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"count\")\n\n@@ -30,59 +30,72 @@ class StringLookup(index_lookup.IndexLookup):\n   \"\"\"Maps strings from a vocabulary to integer indices.\n \n   This layer translates a set of arbitrary strings into an integer output via a\n-  table-based lookup, with optional out-of-vocabulary handling.\n+  table-based vocabulary lookup.\n \n-  If desired, the user can call this layer's `adapt()` method on a data set,\n-  which will analyze the data set, determine the frequency of individual string\n-  values, and create a vocabulary from them. This vocabulary can have\n-  unlimited size or be capped, depending on the configuration options for this\n-  layer; if there are more unique values in the input than the maximum\n-  vocabulary size, the most frequent terms will be used to create the\n-  vocabulary (and the terms that don't make the cut will be treated as OOV).\n+  The vocabulary for the layer can be supplied on construction or learned via\n+  `adapt()`. During `adapt()`, the layer will analyze a data set, determine the\n+  frequency of individual strings tokens, and create a vocabulary from them. If\n+  the vocabulary is capped in size, the most frequent tokens will be used to\n+  create the vocabulary and all others will be treated as out-of-vocabulary\n+  (OOV).\n+\n+  There are two possible output modes for the layer.\n+  When `output_mode` is \"int\",\n+  input strings are converted to their index in the vocabulary (an integer).\n+  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input strings\n+  are encoded into an array where each dimension corresponds to an element in\n+  the vocabulary.\n+\n+  The vocabulary can optionally contain a mask token as well as an OOV token\n+  (which can optionally occupy multiple indices in the vocabulary, as set\n+  by `num_oov_indices`).\n+  The position of these tokens in the vocabulary is fixed. When `output_mode` is\n+  \"int\", the vocabulary will begin with the mask token at index 0, followed by\n+  OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n+  \"binary\", \"count\", or \"tf-idf\" the vocabulary will begin with OOV indices and\n+  instances of the mask token will be dropped.\n \n   Args:\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this vocabulary\n-      includes the OOV and mask tokens, so the effective number of tokens is\n-      `(max_tokens - num_oov_indices - (1 if mask_token else 0))`.\n-    num_oov_indices: The number of out-of-vocabulary tokens to use; defaults to\n-      1. If this value is more than 1, OOV inputs are hashed to determine their\n-      OOV value; if this value is 0, passing an OOV input will result in a '-1'\n-      being returned for that value in the output tensor. (Note that, because\n-      the value is -1 and not 0, this will allow you to effectively drop OOV\n-      values from categorical encodings.)\n-    mask_token: A token that represents masked values, and which is mapped to\n-      index 0. Defaults to the empty string `\"\"`. If set to None, no mask term\n-      will be added and the OOV tokens, if any, will be indexed from\n-      `(0...num_oov_indices)` instead of `(1...num_oov_indices+1)`.\n-    oov_token: The token representing an out-of-vocabulary value. Defaults to\n-      `\"[UNK]\"`.\n-    vocabulary: An optional list of vocabulary terms, or a path to a text file\n-      containing a vocabulary to load into this layer. The file should contain\n-      one token per line. If the list or file contains the same token multiple\n-      times, an error will be thrown.\n-    encoding: The Python string encoding to use. Defaults to `\"utf-8\"`.\n-    invert: If true, this layer will map indices to vocabulary items instead\n-      of mapping vocabulary items to indices.\n-    output_mode: Specification for the output of the layer. Only applicable\n-      when `invert` is False. Defaults to \"int\". Values can be \"int\", \"binary\",\n-      or \"count\", configuring the layer as follows:\n-        \"int\": Return the raw integer indices of the input values.\n-        \"binary\": Outputs a single int array per batch, of either vocab_size or\n+      there is no cap on the size of the vocabulary. Note that this size\n+      includes the OOV and mask tokens. Default to None.\n+    num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n+      value is more than 1, OOV inputs are hashed to determine their OOV value.\n+      If this value is 0, OOV inputs will map to -1 when `output_mode` is \"int\"\n+      and are dropped otherwise. Defaults to 1.\n+    mask_token: A token that represents masked inputs. When `output_mode` is\n+      \"int\", the token is included in vocabulary and mapped to index 0. In other\n+      output modes, the token will not appear in the vocabulary and instances\n+      of the mask token in the input will be dropped. If set to None, no mask\n+      term will be added. Defaults to `\"\"`.\n+    oov_token: Only used when `invert` is True. The token to return for OOV\n+      indices. Defaults to `\"[UNK]\"`.\n+    vocabulary: An optional list of tokens, or a path to a text file containing\n+      a vocabulary to load into this layer. The file should contain one token\n+      per line. If the list or file contains the same token multiple times, an\n+      error will be thrown.\n+    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n+      indices to vocabulary items instead of mapping vocabulary items to\n+      indices. Default to False.\n+    output_mode: Specification for the output of the layer. Defaults to \"int\".\n+      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n+      as follows:\n+        \"int\": Return the raw integer indices of the input tokens.\n+        \"binary\": Outputs a single int array per sample, of either vocab_size or\n           max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the batch item.\n+          to that index exists at least once in the sample.\n         \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n+          of times the token at that index appeared in the sample.\n         \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n           value in each token slot.\n-    pad_to_max_tokens: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes. If\n-      True, the output will have its feature axis padded to `max_tokens` even if\n-      the number of unique tokens in the vocabulary is less than max_tokens,\n-      resulting in a tensor of shape [batch_size, max_tokens] regardless of\n-      vocabulary size. Defaults to True.\n-    sparse: Boolean. Only applicable to \"binary\" and \"count\" output modes.\n-      If true, returns a `SparseTensor` instead of a dense `Tensor`.\n-      Defaults to `False`.\n+    pad_to_max_tokens: Only applicable when `output_mode` is \"binary\", \"count\",\n+      or \"tf-idf\". If True, the output will have its feature axis padded to\n+      `max_tokens` even if the number of unique tokens in the vocabulary is less\n+      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n+      regardless of vocabulary size. Defaults to False.\n+    sparse: Boolean. Only applicable when `output_mode` is \"binary\", \"count\",\n+      or \"tf-idf\". If True, returns a `SparseTensor` instead of a dense\n+      `Tensor`. Defaults to False.\n \n   Examples:\n \n@@ -143,42 +156,39 @@ class StringLookup(index_lookup.IndexLookup):\n \n   **Multi-hot output**\n \n-  Configure the layer with `output_mode='binary'`. Note that the first two\n-  dimensions in the binary encoding represent the mask token and the OOV token,\n-  respectively.\n+  Configure the layer with `output_mode='binary'`. Note that the first\n+  `num_oov_indices` dimensions in the binary encoding represent OOV values.\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n   >>> layer = StringLookup(vocabulary=vocab, output_mode='binary')\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0., 0., 1., 0., 1., 1.],\n-           [0., 1., 0., 1., 0., 1.]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0., 1., 0., 1., 1.],\n+           [1., 0., 1., 0., 1.]], dtype=float32)>\n \n   **Token count output**\n \n   Configure the layer with `output_mode='count'`. As with binary output, the\n-  first two dimensions in the output represent the mask token and the OOV token,\n-  respectively.\n+  first `num_oov_indices` dimensions in the output represent OOV values.\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n   >>> layer = StringLookup(vocabulary=vocab, output_mode='count')\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0., 0., 1., 0., 1., 2.],\n-           [0., 2., 0., 1., 0., 1.]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0., 1., 0., 1., 2.],\n+           [2., 0., 1., 0., 1.]], dtype=float32)>\n \n   **TF-IDF output**\n \n   Configure the layer with `output_mode='tf-idf'`. As with binary output, the\n-  first two dimensions in the output represent the mask token and the OOV token,\n-  respectively.\n+  first `num_oov_indices` dimensions in the output represent OOV values.\n \n   Each token bin will output `token_count * idf_weight`, where the idf weights\n   are the inverse document frequency weights per token. These should be provided\n-  along with the vocabulary. Note that the `idf_weight` for mask tokens and OOV\n-  tokens will default to the average of all idf weights passed in.\n+  along with the vocabulary. Note that the `idf_weight` for OOV values will\n+  default to the average of all idf weights passed in.\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n@@ -186,22 +196,22 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> layer = StringLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0.  , 0.  , 0.25, 0.  , 0.6 , 0.8 ],\n-           [0.  , 1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n+           [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  To specify the idf weights for mask and oov values, you will need to pass the\n-  entire vocabularly including these values.\n+  To specify the idf weights for oov values, you will need to pass the entire\n+  vocabularly including the leading oov token.\n \n-  >>> vocab = [\"\", \"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n-  >>> idf_weights = [0.0, 0.9, 0.25, 0.75, 0.6, 0.4]\n+  >>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n+  >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n   >>> layer = StringLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n-  <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n-    array([[0.  , 0.  , 0.25, 0.  , 0.6 , 0.8 ],\n-           [0.  , 1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n+  <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n+    array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n+           [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n   When adapting the layer in tf-idf mode, each input sample will be considered a\n   document, and idf weight per token will be calculated as\n\n@@ -171,7 +171,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[0, 0, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1]]\n+    expected_output = [[0, 1, 1, 1, 1], [1, 1, 0, 1, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"binary\")\n@@ -184,7 +184,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"earth\", \"fire\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[0, 0, 2, 0, 0, 2], [0, 1, 1, 0, 1, 1]]\n+    expected_output = [[0, 2, 0, 0, 2], [1, 1, 0, 1, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"count\")\n\n@@ -145,7 +145,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       True, the output will have its feature axis padded to `max_tokens` even if\n       the number of unique tokens in the vocabulary is less than max_tokens,\n       resulting in a tensor of shape [batch_size, max_tokens] regardless of\n-      vocabulary size. Defaults to True.\n+      vocabulary size. Defaults to False.\n     vocabulary: An optional list of vocabulary terms, or a path to a text file\n       containing a vocabulary to load into this layer. The file should contain\n       one token per line. If the list or file contains the same token multiple\n@@ -240,7 +240,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n                ngrams=None,\n                output_mode=INT,\n                output_sequence_length=None,\n-               pad_to_max_tokens=True,\n+               pad_to_max_tokens=False,\n                vocabulary=None,\n                **kwargs):\n \n@@ -297,18 +297,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       raise ValueError(\"`output_sequence_length` must not be set if \"\n                        \"`output_mode` is not 'int'.\")\n \n-    # If max_tokens is set, the value must be greater than 1 - otherwise we\n-    # are creating a 0-element vocab, which doesn't make sense.\n-    if max_tokens is not None and max_tokens < 1:\n-      raise ValueError(\"max_tokens must be > 1.\")\n-\n     self._max_tokens = max_tokens\n-\n-    # In INT mode, the zero value is reserved for padding (per Keras standard\n-    # padding approaches). In non-INT modes, there is no padding so we can set\n-    # the OOV value to zero instead of one.\n-    self._oov_value = 1 if output_mode == INT else 0\n-\n     self._standardize = standardize\n     self._split = split\n     self._ngrams_arg = ngrams\n@@ -319,7 +308,6 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     self._output_mode = output_mode\n     self._output_sequence_length = output_sequence_length\n-    self._pad_to_max = pad_to_max_tokens\n     vocab_size = 0\n     # IndexLookup needs to keep track the current vocab size outside of its\n     # layer weights. We persist it as a hidden part of the config during\n@@ -334,10 +322,8 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\n         \"TextVectorization\").set(True)\n \n-    mask_token = \"\" if output_mode in [None, INT] else None\n     self._index_lookup_layer = self._get_index_lookup_class()(\n         max_tokens=max_tokens,\n-        mask_token=mask_token,\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n         output_mode=output_mode if output_mode is not None else INT,\n@@ -435,7 +421,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"ngrams\": self._ngrams_arg,\n         \"output_mode\": self._output_mode,\n         \"output_sequence_length\": self._output_sequence_length,\n-        \"pad_to_max_tokens\": self._pad_to_max,\n+        \"pad_to_max_tokens\": self._index_lookup_layer.pad_to_max_tokens,\n         \"vocab_size\": self._index_lookup_layer.vocab_size(),\n     }\n     base_config = super(TextVectorization, self).get_config()\n\n@@ -1185,7 +1185,8 @@ class TextVectorizationOutputTest(\n         max_tokens=6,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.COUNT)\n+        output_mode=text_vectorization.COUNT,\n+        pad_to_max_tokens=True)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n\n@@ -83,7 +83,7 @@ class TextVectorization(text_vectorization.TextVectorization,\n                ngrams=None,\n                output_mode=text_vectorization.INT,\n                output_sequence_length=None,\n-               pad_to_max_tokens=True,\n+               pad_to_max_tokens=False,\n                **kwargs):\n     super(TextVectorization,\n           self).__init__(max_tokens, standardize, split, ngrams, output_mode,\n\n@@ -157,7 +157,7 @@ def _int32_manipulation_at_max_shape_dims_limit():\n   # Verify that a value was actually inferred for a tensor that *might*\n   # represent the shape, bying checking that a value in\n   # the range appears in the printed inferred value\n-  if keras_tensor.keras_tensors_enabled():\n+  if tf.compat.v1.executing_eagerly_outside_functions():\n     assert str(keras_tensor._MAX_TENSOR_RANK - 1) in str(x)\n \n   x = tf.reshape(x, (batch_size, num_features))\n@@ -413,7 +413,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     expected = tf.stack([\n         tf.range(8)[::step] for _ in range(batch_size)])\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       self.assertNotIn('tf.strided_slice', (\n@@ -447,7 +447,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     args = tf.constant(stop, shape=(batch_size,))\n     expected = x[:stop]\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       # TODO(b/161925288): Fix the dispatch triggering then uncomment:\n@@ -481,7 +481,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     args = tf.constant(index, shape=(batch_size,))\n     expected = x[index]\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       # TODO(b/161925288): Fix the bug then uncomment:\n@@ -518,7 +518,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     args = [x, tf.constant(stop, shape=(batch_size,))]\n     expected = x[:stop]\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       self.assertNotIn('tf.strided_slice', (\n@@ -555,7 +555,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     expected = tf.stack([\n         tf.range(8)[:stop] for _ in range(batch_size)])\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       self.assertNotIn('tf.strided_slice', (\n@@ -605,7 +605,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n         tf.range(8)[start:stop:step]\n         for _ in range(4)]) for _ in range(batch_size)])\n \n-    if keras_tensor.keras_tensors_enabled():\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertIn('tf.__operators__.getitem', (\n           x.name for x in model.layers))\n       self.assertNotIn('tf.strided_slice', (\n\n@@ -32,7 +32,6 @@ from keras import backend\n from keras import layers\n from keras import models\n from keras.engine import base_layer_utils\n-from keras.engine import keras_tensor\n from keras.optimizer_v2 import adadelta as adadelta_v2\n from keras.optimizer_v2 import adagrad as adagrad_v2\n from keras.optimizer_v2 import adam as adam_v2\n@@ -353,29 +352,6 @@ def run_eagerly_scope(value):\n     _thread_local_data.run_eagerly = previous_value\n \n \n-@tf_contextlib.contextmanager\n-def use_keras_tensors_scope(value):\n-  \"\"\"Provides a scope within which we use KerasTensors in the func. API or not.\n-\n-  The boolean gets restored to its original value upon exiting the scope.\n-\n-  Args:\n-     value: Bool specifying if we should build functional models\n-      using KerasTensors in the active test.\n-     Should be True or False.\n-\n-  Yields:\n-    The provided value.\n-  \"\"\"\n-  previous_value = keras_tensor._KERAS_TENSORS_ENABLED  # pylint: disable=protected-access\n-  try:\n-    keras_tensor._KERAS_TENSORS_ENABLED = value  # pylint: disable=protected-access\n-    yield value\n-  finally:\n-    # Restore KerasTensor usage to initial value.\n-    keras_tensor._KERAS_TENSORS_ENABLED = previous_value  # pylint: disable=protected-access\n-\n-\n def should_run_eagerly():\n   \"\"\"Returns whether the models we are testing should be run eagerly.\"\"\"\n   if _thread_local_data.run_eagerly is None:\n\n@@ -606,21 +606,46 @@ def deserialize_keras_object(identifier,\n                              printable_module_name='object'):\n   \"\"\"Turns the serialized form of a Keras object back into an actual object.\n \n-  Calls to `deserialize_keras_object` while underneath the\n+  This function is for mid-level library implementers rather than end users.\n+\n+  Importantly, this utility requires you to provide the dict of `module_objects`\n+  to use for looking up the object config; this is not populated by default.\n+  If you need a deserialization utility that has preexisting knowledge of\n+  built-in Keras objects, use e.g. `keras.layers.deserialize(config)`,\n+  `keras.metrics.deserialize(config)`, etc.\n+\n+  Calling `deserialize_keras_object` while underneath the\n   `SharedObjectLoadingScope` context manager will cause any already-seen shared\n   objects to be returned as-is rather than creating a new object.\n \n   Args:\n     identifier: the serialized form of the object.\n-    module_objects: A dictionary of custom objects to look the name up in.\n-      Generally, module_objects is provided by midlevel library implementers.\n+    module_objects: A dictionary of built-in objects to look the name up in.\n+      Generally, `module_objects` is provided by midlevel library implementers.\n     custom_objects: A dictionary of custom objects to look the name up in.\n-      Generally, custom_objects is provided by the user.\n+      Generally, `custom_objects` is provided by the end user.\n     printable_module_name: A human-readable string representing the type of the\n       object. Printed in case of exception.\n \n   Returns:\n     The deserialized object.\n+\n+  Example:\n+\n+  A mid-level library implementer might want to implement a utility for\n+  retrieving an object from its config, as such:\n+\n+  ```python\n+  def deserialize(config, custom_objects=None):\n+     return deserialize_keras_object(\n+       identifier,\n+       module_objects=globals(),\n+       custom_objects=custom_objects,\n+       name=\"MyObjectType\",\n+     )\n+  ```\n+\n+  This is how e.g. `keras.layers.deserialize()` is implemented.\n   \"\"\"\n   if identifier is None:\n     return None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
