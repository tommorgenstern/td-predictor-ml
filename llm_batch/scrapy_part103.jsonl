{"custom_id": "scrapy#9fccc113631fee4a3148fe688164d7b750ace45f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 14 | Files Changed: 5 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 344 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,12 @@\n from scrapy.webservice import JsonRpcResource\n-from scrapy.extension import extensions\n+from scrapy.project import crawler\n \n class ExtensionsResource(JsonRpcResource):\n \n     ws_name = 'extensions'\n \n-    def __init__(self, _extensions=extensions):\n+    def __init__(self, _extensions=None):\n+        if _extensions is None:\n+            _extensions = crawler.extensions\n         JsonRpcResource.__init__(self)\n         self._target = _extensions\n\n@@ -4,16 +4,16 @@ from twisted.internet import reactor, defer\n \n from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n-from scrapy.extension import extensions\n from scrapy import log\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n class Crawler(object):\n \n-    def __init__(self, spiders):\n+    def __init__(self, spiders, extensions):\n         self.configured = False\n         self.control_reactor = True\n+        self.extensions = extensions\n         self.spiders = spiders\n         self.engine = ExecutionEngine(self)\n \n@@ -24,11 +24,11 @@ class Crawler(object):\n \n         if not log.started:\n             log.start()\n-        if not extensions.loaded:\n-            extensions.load()\n+        if not self.extensions.loaded:\n+            self.extensions.load()\n         if not self.spiders.loaded:\n             self.spiders.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n+        log.msg(\"Enabled extensions: %s\" % \", \".join(self.extensions.enabled.iterkeys()),\n             level=log.DEBUG)\n \n         self.queue = queue or ExecutionQueue()\n\n@@ -38,5 +38,3 @@ class ExtensionManager(object):\n \n     def reload(self):\n         self.load()\n-\n-extensions = ExtensionManager()\n\n@@ -1,6 +1,8 @@\n-from scrapy.conf import settings\n-from scrapy.utils.misc import load_object\n from scrapy.crawler import Crawler\n+from scrapy.extension import ExtensionManager\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n \n _spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n-crawler = Crawler(_spiders)\n+_extensions = ExtensionManager()\n+crawler = Crawler(_spiders, _extensions)\n\n@@ -10,7 +10,6 @@ from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n from twisted.internet import reactor, protocol\n \n-from scrapy.extension import extensions\n from scrapy.exceptions import NotConfigured\n from scrapy.project import crawler\n from scrapy.stats import stats\n@@ -49,7 +48,7 @@ class TelnetConsole(protocol.ServerFactory):\n         telnet_vars = {\n             'engine': crawler.engine,\n             'manager': crawler,\n-            'extensions': extensions,\n+            'extensions': crawler.extensions,\n             'stats': stats,\n             'spiders': crawler.spiders,\n             'settings': settings,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1225e0f451fd9261fbaba8b737ed6a9603aa69d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 115 | Files Changed: 2 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 184 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,6 @@ from scrapy.command import ScrapyCommand\n from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.item import BaseItem\n-from scrapy.project import crawler\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.url import is_url\n@@ -21,115 +20,90 @@ class Command(ScrapyCommand):\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n-            help=\"always use this spider\")\n+            help=\"use this spider without looking for one\")\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n-            help=\"don't show extracted links\")\n+            help=\"don't show links to follow (extracted requests)\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n             help=\"don't show scraped items\")\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", \\\n             help=\"avoid using pygments to colorize the output\")\n         parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", \\\n-            help=\"try to match and parse the url with the defined rules (if any)\")\n-        parser.add_option(\"-c\", \"--callbacks\", dest=\"callbacks\", action=\"store\", \\\n-            help=\"use the provided callback(s) for parsing the url (separated with commas)\")\n-\n-    def process_options(self, args, opts):\n-        super(Command, self).process_options(args, opts)\n-        self.callbacks = opts.callbacks.split(',') if opts.callbacks else []\n+            help=\"use CrawlSpider rules to discover the callback\")\n+        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", \\\n+            help=\"use this callback for parsing, instead looking for a callback\")\n \n     def pipeline_process(self, item, spider, opts):\n         return item\n \n-    def run_callback(self, spider, response, callback, args, opts):\n-        if callback:\n-            callback_fcn = callback if callable(callback) else getattr(spider, callback, None)\n-            if not callback_fcn:\n-                log.msg('Cannot find callback %s in %s spider' % (callback, spider.name))\n+    def run_callback(self, spider, response, callback, opts):\n+        cb = callback if callable(callback) else getattr(spider, callback, None)\n+        if not cb:\n+            log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))\n             return (), ()\n \n-            result = iterate_spider_output(callback_fcn(response))\n-            links = [i for i in result if isinstance(i, Request)]\n-            items = [self.pipeline_process(i, spider, opts) for i in result if \\\n-                     isinstance(i, BaseItem)]\n-            return items, links\n+        items, requests = [], []\n+        for x in iterate_spider_output(cb(response)):\n+            if isinstance(x, BaseItem):\n+                items.append(x)\n+            elif isinstance(x, Request):\n+                requests.append(x)\n+        return items, requests\n \n-        return (), ()\n+    def get_callback_from_rules(self, spider, response):\n+        if getattr(spider, 'rules', None):\n+            for rule in spider.rules:\n+                if rule.link_extractor.matches(response.url) and rule.callback:\n+                    return rule.callback\n+        else:\n+            log.msg(\"No CrawlSpider rules found in spider %r, please specify \"\n+                \"a callback to use for parsing\" % spider.name, log.ERROR)\n \n-    def print_results(self, items, links, cb_name, opts):\n-        display.nocolour = opts.nocolour\n+    def print_results(self, items, requests, cb_name, opts):\n         if not opts.noitems:\n-            for item in items:\n-                for key in item.__dict__.keys():\n-                    if key.startswith('_'):\n-                        item.__dict__.pop(key, None)\n             print \"# Scraped Items - callback: %s\" % cb_name, \"-\"*60\n-            display.pprint(list(items))\n-\n+            display.pprint([dict(x) for x in items], colorize=not opts.nocolour)\n         if not opts.nolinks:\n-            print \"# Links - callback: %s\" % cb_name, \"-\"*68\n-            display.pprint(list(links))\n+            print \"# Requests - callback: %s\" % cb_name, \"-\"*68\n+            display.pprint(requests, colorize=not opts.nocolour)\n+\n+    def get_spider(self, request, opts):\n+        if opts.spider:\n+            try:\n+                return crawler.spiders.create(opts.spider)\n+            except KeyError:\n+                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n+        else:\n+            spider = crawler.spiders.create_for_request(request)\n+            if spider:\n+                return spider\n+            log.msg('Unable to find spider for: %s' % request, log.ERROR)\n+\n+    def get_response_and_spider(self, url, opts):\n+        responses = [] # to collect downloaded responses\n+        request = Request(url, callback=responses.append)\n+        spider = self.get_spider(request, opts)\n+        if not spider:\n+            return None, None\n+        crawler.queue.append_request(request, spider)\n+        crawler.start()\n+        if not responses:\n+            log.msg('No response downloaded for: %s' % request, log.ERROR, \\\n+                spider=spider)\n+            return None, None\n+        return responses[0], spider\n \n     def run(self, args, opts):\n         if not len(args) == 1 or not is_url(args[0]):\n             return False\n-\n-        responses = [] # to collect downloaded responses\n-        request = Request(args[0], callback=responses.append)\n-\n-        if opts.spider:\n-            try:\n-                spider = crawler.spiders.create(opts.spider)\n-            except KeyError:\n-                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n+        response, spider = self.get_response_and_spider(args[0], opts)\n+        if not response:\n             return\n-        else:\n-            spider = crawler.spiders.create_for_request(request)\n-            if spider is None:\n-                log.msg('Unable to find spider for URL: %s' % args[0], log.ERROR)\n-                return\n-\n-        crawler.configure()\n-        crawler.queue.append_request(request, spider)\n-        crawler.start()\n-\n-        if not responses:\n-            log.msg('No response returned', log.ERROR, spider=spider)\n-            return\n-\n-        # now process response\n-        #   - if callbacks defined then call each one print results\n-        #   - if --rules option given search for matching spider's rule\n-        #   - default print result using default 'parse' spider's callback\n-        response = responses[0]\n-\n-        if self.callbacks:\n-            # apply each callback\n-            for callback in self.callbacks:\n-                items, links = self.run_callback(spider, response,\n-                                                    callback, args, opts)\n-                self.print_results(items, links, callback, opts)\n+        callback = None\n+        if opts.callback:\n+            callback = opts.callback\n         elif opts.rules:\n-            # search for matching spider's rule\n-            if hasattr(spider, 'rules') and spider.rules:\n-                items, links = [], []\n-                for rule in spider.rules:\n-                    if rule.link_extractor.matches(response.url) \\\n-                        and rule.callback:\n-\n-                        items, links = self.run_callback(spider,\n-                                            response, rule.callback,\n-                                            args, opts)\n-                        self.print_results(items, links,\n-                                            rule.callback, opts)\n-                        # first-match rule breaks rules loop\n-                        break\n-            else:\n-                log.msg('No rules found for spider \"%s\", ' \\\n-                        'please specify a callback for parsing' \\\n-                        % spider.name, log.ERROR)\n-        else:\n-            # default callback 'parse'\n-            items, links = self.run_callback(spider, response,\n-                                                'parse', args, opts)\n-            self.print_results(items, links, 'parse', opts)\n+            callback = self.get_callback_from_rules(spider, response)\n+        items, requests = self.run_callback(spider, response, callback or 'parse', \\\n+            opts)\n+        self.print_results(items, requests, callback, opts)\n \n\n@@ -1,15 +1,12 @@\n \"\"\"\n-Helper functions for formatting and pretty printing some objects\n+pprint and pformat wrappers with colorization support\n \"\"\"\n+\n import sys\n-import pprint as pypprint\n+from pprint import pformat as pformat_\n \n-from scrapy.item import BaseItem\n-\n-nocolour = False\n-\n-def colorize(text):\n-    if nocolour or not sys.stdout.isatty():\n+def _colorize(text, colorize=True):\n+    if not colorize or not sys.stdout.isatty():\n         return text\n     try:\n         from pygments import highlight\n@@ -19,25 +16,8 @@ def colorize(text):\n     except ImportError:\n         return text\n \n-def _pformat_dictobj(obj):\n-    clsname = obj.__class__.__name__\n-    return \"%s(%s)\\n\" % (clsname, colorize(pypprint.pformat(obj.__dict__)))\n-\n def pformat(obj, *args, **kwargs):\n-    \"\"\"\n-    Wrapper which autodetects the object type and uses the proper formatting\n-    function\n-    \"\"\"\n-    if isinstance(obj, BaseItem):\n-        return _pformat_dictobj(obj)\n-    elif hasattr(obj, '__iter__'):\n-        return \"\".join(map(pformat, obj))\n-    else:\n-        return colorize(pypprint.pformat(repr(obj)))\n+    return _colorize(pformat_(obj), kwargs.pop('colorize', True))\n \n def pprint(obj, *args, **kwargs):\n-    \"\"\"\n-    Wrapper which autodetects the object type and uses the proper printing\n-    function\n-    \"\"\"\n     print pformat(obj, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5474d8cc63ed06f750750cd18154bad6e97055a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 43 | Files Changed: 4 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 58 | Churn Cumulative: 396 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -4,16 +4,17 @@ from twisted.internet import reactor, defer\n \n from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n+from scrapy.extension import ExtensionManager\n from scrapy import log\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n class Crawler(object):\n \n-    def __init__(self, spiders, extensions):\n+    def __init__(self, settings, spiders):\n         self.configured = False\n         self.control_reactor = True\n-        self.extensions = extensions\n+        self.settings = settings\n         self.spiders = spiders\n         self.engine = ExecutionEngine(self)\n \n@@ -24,12 +25,9 @@ class Crawler(object):\n \n         if not log.started:\n             log.start()\n-        if not self.extensions.loaded:\n-            self.extensions.load()\n+        self.extensions = ExtensionManager.from_settings(self.settings)\n         if not self.spiders.loaded:\n             self.spiders.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(self.extensions.enabled.iterkeys()),\n-            level=log.DEBUG)\n \n         self.queue = queue or ExecutionQueue()\n         self.engine.configure(self._spider_closed)\n\n@@ -1,40 +1,16 @@\n \"\"\"\n-This module contains the ExtensionManager  which takes care of loading and\n-keeping track of all enabled extensions. It also contains an instantiated\n-ExtensionManager (extensions) to be used as singleton.\n+The Extension Manager\n+\n+See documentation in docs/topics/extensions.rst\n \"\"\"\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n-from scrapy import log\n-from scrapy.conf import settings\n \n-class ExtensionManager(object):\n+class ExtensionManager(MiddlewareManager):\n \n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n+    component_name = 'extension'\n \n-    def load(self):\n-        \"\"\"\n-        Load enabled extensions in settings module\n-        \"\"\"\n-\n-        self.loaded = False\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        extlist = build_component_list(settings['EXTENSIONS_BASE'], \\\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return build_component_list(settings['EXTENSIONS_BASE'], \\\n             settings['EXTENSIONS'])\n-        for extension_path in extlist:\n-            try:\n-                cls = load_object(extension_path)\n-                self.enabled[cls.__name__] = cls()\n-            except NotConfigured, e:\n-                self.disabled[cls.__name__] = extension_path\n-                if e.args:\n-                    log.msg(e)\n-        self.loaded = True\n-\n-    def reload(self):\n-        self.load()\n\n@@ -35,7 +35,7 @@ class MiddlewareManager(object):\n             except NotConfigured, e:\n                 if e.args:\n                     log.msg(e)\n-        enabled = [type(x).__name__ for x in middlewares]\n+        enabled = [x.__class__.__name__ for x in middlewares]\n         log.msg(\"Enabled %ss: %s\" % (cls.component_name, \", \".join(enabled)), \\\n             level=log.DEBUG)\n         return cls(*middlewares)\n\n@@ -1,8 +1,6 @@\n from scrapy.crawler import Crawler\n-from scrapy.extension import ExtensionManager\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n _spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n-_extensions = ExtensionManager()\n-crawler = Crawler(_spiders, _extensions)\n+crawler = Crawler(settings, _spiders)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7546a0805c51a11ba3aeada612a020a23a0a6e7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 26 | Churn Cumulative: 442 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -259,7 +259,5 @@ WEBSERVICE_RESOURCES = {}\n WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.manager.ManagerResource': 1,\n     'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n-    'scrapy.contrib.webservice.extensions.ExtensionsResource': 1,\n-    'scrapy.contrib.webservice.spiders.SpidersResource': 1,\n     'scrapy.contrib.webservice.stats.StatsResource': 1,\n }\n\n@@ -1,12 +0,0 @@\n-from scrapy.webservice import JsonRpcResource\n-from scrapy.project import crawler\n-\n-class ExtensionsResource(JsonRpcResource):\n-\n-    ws_name = 'extensions'\n-\n-    def __init__(self, _extensions=None):\n-        if _extensions is None:\n-            _extensions = crawler.extensions\n-        JsonRpcResource.__init__(self)\n-        self._target = _extensions\n\n@@ -1,12 +0,0 @@\n-from scrapy.webservice import JsonRpcResource\n-from scrapy.project import crawler\n-\n-class SpidersResource(JsonRpcResource):\n-\n-    ws_name = 'spiders'\n-\n-    def __init__(self, _spiders=None):\n-        if _spiders is None:\n-            _spiders = crawler.spiders\n-        JsonRpcResource.__init__(self)\n-        self._target = _spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cbfec4bb0e96cce60197580adf1650ca067c2b98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 22 | Churn Cumulative: 422 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -257,7 +257,7 @@ WEBSERVICE_LOGFILE = None\n WEBSERVICE_PORT = 6080\n WEBSERVICE_RESOURCES = {}\n WEBSERVICE_RESOURCES_BASE = {\n-    'scrapy.contrib.webservice.manager.ManagerResource': 1,\n+    'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n     'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n     'scrapy.contrib.webservice.stats.StatsResource': 1,\n }\n\n@@ -0,0 +1,10 @@\n+from scrapy.webservice import JsonRpcResource\n+from scrapy.project import crawler\n+\n+class CrawlerResource(JsonRpcResource):\n+\n+    ws_name = 'crawler'\n+\n+    def __init__(self, _crawler=crawler):\n+        JsonRpcResource.__init__(self)\n+        self._target = _crawler\n\n@@ -1,10 +0,0 @@\n-from scrapy.webservice import JsonRpcResource\n-from scrapy.project import crawler\n-\n-class ManagerResource(JsonRpcResource):\n-\n-    ws_name = 'manager'\n-\n-    def __init__(self, _manager=crawler):\n-        JsonRpcResource.__init__(self)\n-        self._target = _manager\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33686fa563ea9a1529ed28636b234c0d68e296c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 122 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -96,7 +96,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n     try:\n         import lxml\n     except ImportError:\n-        skip = True\n+        skip = \"lxml not available\"\n \n \n class UtilsCsvTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd784cd131a7e48ce7261ab54662f0affac0d4ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 766 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n+from scrapy.utils.url import file_uri_to_path\n from scrapy.conf import settings\n \n class BlockingFeedStorage(object):\n@@ -40,8 +41,7 @@ class StdoutFeedStorage(object):\n class FileFeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n-        u = urlparse(uri)\n-        self.path = u.path\n+        self.path = file_uri_to_path(uri)\n \n     def _store_in_thread(self, file, spider):\n         dirname = os.path.dirname(self.path)\n\n@@ -6,6 +6,7 @@ from cStringIO import StringIO\n \n from scrapy.spider import BaseSpider\n from scrapy.contrib.feedexport import FileFeedStorage, FTPFeedStorage, S3FeedStorage\n+from scrapy.utils.url import path_to_file_uri\n \n class FeedStorageTest(unittest.TestCase):\n \n@@ -23,13 +24,13 @@ class FileFeedStorageTest(FeedStorageTest):\n \n     def test_store_file_uri(self):\n         path = os.path.abspath(self.mktemp())\n-        uri = \"file://%s\" % path\n+        uri = path_to_file_uri(path)\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n     def test_store_file_uri_makedirs(self):\n         path = os.path.abspath(self.mktemp())\n         path = os.path.join(path, 'more', 'paths', 'file.txt')\n-        uri = \"file://%s\" % path\n+        uri = path_to_file_uri(path)\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n     def test_store_direct_path(self):\n\n@@ -274,7 +274,7 @@ class TestPageParsing(TestCase):\n         \"\"\"\n         Tests from real pages. More reliable and easy to build for more complicated structures\n         \"\"\"\n-        samples_file = open(os.path.join(path, \"samples_pageparsing.json.gz\"), \"r\")\n+        samples_file = open(os.path.join(path, \"samples_pageparsing.json.gz\"), \"rb\")\n         samples = []\n         for line in GzipFile(fileobj=StringIO(samples_file.read())).readlines():\n             samples.append(json.loads(line))\n\n@@ -173,5 +173,7 @@ def any_to_uri(uri_or_path):\n     \"\"\"If given a path name, return its File URI, otherwise return it\n     unmodified\n     \"\"\"\n+    if os.path.splitdrive(uri_or_path)[0]:\n+        return path_to_file_uri(uri_or_path)\n     u = urlparse.urlparse(uri_or_path)\n     return uri_or_path if u.scheme else path_to_file_uri(uri_or_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#254d517da131dcf654bb13935feb571b17a0d391", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 517 | Contributors (this commit): 5 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -212,7 +212,7 @@ SCHEDULER_ORDER = 'DFO'\n SERVICE_QUEUE = 'scrapy.contrib.queue.SqliteExecutionQueue'\n SERVICE_QUEUE_FILE = 'scrapy.db'\n \n-SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.SpiderManager'\n+SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n \n\n\n@@ -8,7 +8,7 @@ from twisted.trial import unittest\n # ugly hack to avoid cyclic imports of scrapy.spider when running this test\n # alone\n import scrapy.spider\n-from scrapy.contrib.spidermanager import SpiderManager\n+from scrapy.spidermanager import SpiderManager\n from scrapy.http import Request\n \n module_dir = os.path.dirname(os.path.abspath(__file__))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e189861b46ae5ebd2ef7f1d900046322887e2d72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 726 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,7 @@ class ItemLoader(object):\n \n     def add_value(self, field_name, value, *processors, **kw):\n         value = self.get_value(value, *processors, **kw)\n-        if not value:\n+        if value is None:\n             return\n         if not field_name:\n             for k,v in value.iteritems():\n@@ -39,7 +39,7 @@ class ItemLoader(object):\n \n     def replace_value(self, field_name, value, *processors, **kw):\n         value = self.get_value(value, *processors, **kw)\n-        if not value:\n+        if value is None:\n             return\n         if not field_name:\n             for k,v in value.iteritems():\n\n@@ -68,6 +68,11 @@ class ItemLoaderTest(unittest.TestCase):\n         il.add_value(None, u'Jim', lambda x: {'name': x})\n         self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe', u'Jim'])\n \n+    def test_add_zero(self):\n+        il = NameItemLoader()\n+        il.add_value('name', 0)\n+        self.assertEqual(il.get_collected_values('name'), [0])\n+\n     def test_replace_value(self):\n         il = TestItemLoader()\n         il.replace_value('name', u'marta')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6585c1a28f6f3acf60f92cec16fd81fab5035da8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 167 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,12 +28,13 @@ mail_sent = object()\n class MailSender(object):\n \n     def __init__(self, smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, \\\n-            smtpport=None):\n+            smtpport=None, debug=False):\n         self.smtphost = smtphost or settings['MAIL_HOST']\n         self.smtpport = smtpport or settings.getint('MAIL_PORT')\n         self.smtpuser = smtpuser or settings['MAIL_USER']\n         self.smtppass = smtppass or settings['MAIL_PASS']\n         self.mailfrom = mailfrom or settings['MAIL_FROM']\n+        self.debug = debug\n \n         if not self.smtphost or not self.mailfrom:\n             raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n@@ -67,7 +68,7 @@ class MailSender(object):\n         send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,\n                        cc=cc, attach=attachs, msg=msg)\n \n-        if settings.getbool('MAIL_DEBUG'):\n+        if self.debug:\n             log.msg('Debug mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n                 (to, cc, subject, len(attachs)), level=log.DEBUG)\n             return\n\n@@ -3,22 +3,20 @@ import unittest\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.conf import settings\n from scrapy.mail import MailSender, mail_sent\n \n \n class MailSenderTest(unittest.TestCase):\n \n     def setUp(self):\n-        settings.disabled = False\n-        settings.overrides['MAIL_DEBUG'] = True\n-\n         self.catched_msg = None\n-\n         dispatcher.connect(self._catch_mail_sent, signal=mail_sent)\n \n+    def tearDown(self):\n+        dispatcher.disconnect(self._catch_mail_sent, signal=mail_sent)\n+\n     def test_send(self):\n-        mailsender = MailSender()\n+        mailsender = MailSender(debug=True)\n         mailsender.send(to=['test@scrapy.org'], subject='subject', body='body')\n \n         assert self.catched_msg\n@@ -38,7 +36,7 @@ class MailSenderTest(unittest.TestCase):\n         attach.seek(0)\n         attachs = [('attachment', 'text/plain', attach)]\n \n-        mailsender = MailSender()\n+        mailsender = MailSender(debug=True)\n         mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n                        attachs=attachs)\n \n@@ -59,10 +57,6 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(text.get_payload(decode=True), 'body')\n         self.assertEqual(attach.get_payload(decode=True), 'content')\n \n-    def tearDown(self):\n-        del settings.overrides['MAIL_DEBUG']\n-        settings.disabled = True\n-\n     def _catch_mail_sent(self, **kwargs):\n         self.catched_msg = dict(**kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ee92686c71edaf77725c05dfd8aba9c22bc93e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 6 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -57,9 +57,9 @@ class SpiderManagerTest(unittest.TestCase):\n             ['spider3'])\n \n     def test_load_spider_module(self):\n-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider1'])\n+        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider1'])\n         assert len(self.spiderman._spiders) == 1\n \n     def test_load_base_spider(self):\n-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider0'])\n+        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider0'])\n         assert len(self.spiderman._spiders) == 0\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e83f527b375f33c1989e6ab4301353e1ba0db0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 179 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,30 +0,0 @@\n-#!/usr/bin/env python\n-\n-import sys\n-\n-from boto.sqs.connection import SQSConnection\n-from boto.sqs.message import Message\n-\n-from scrapy.utils.py26 import json\n-from scrapy.conf import settings\n-\n-qname = settings['SQS_QUEUE']\n-\n-if len(sys.argv) <= 1:\n-    print \"usage: %s <command> [args]\" % sys.argv[0]\n-    print\n-    print \"available commands:\"\n-    print \"  put <spider_name> - append spider to queue\"\n-    print\n-    print \"SQS queue: %s\" % qname\n-    print\n-    sys.exit()\n-\n-cmd, args = sys.argv[1], sys.argv[2:]\n-\n-if cmd == 'put':\n-    conn = SQSConnection(settings['AWS_ACCESS_KEY_ID'], \\\n-        settings['AWS_SECRET_ACCESS_KEY'])\n-    q = conn.create_queue(qname)\n-    msg = Message(body=json.dumps({'name': args[0]}))\n-    q.write(msg)\n\n@@ -74,7 +74,7 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n     for file_info in data_files:\n         file_info[0] = '\\\\PURELIB\\\\%s' % file_info[0]\n \n-scripts = ['bin/scrapy', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py']\n+scripts = ['bin/scrapy', 'bin/scrapy-ws.py']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54024d1d3fdc9a82385abbca149ef724feff5190", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,7 +48,6 @@ class Command(ScrapyCommand):\n             except KeyError:\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n \n-        crawler.configure()\n         crawler.queue.append_request(request, spider, \\\n             default_spider=BaseSpider('default'))\n         crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d24175c8159cf5bd209f53bdcfc133b78c5a70f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 53 | Churn Cumulative: 566 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 3 | DMM Complexity: 0.75\n\nDIFF:\n@@ -114,7 +114,8 @@ def execute(argv=None):\n     settings.defaults.update(cmd.default_settings)\n     del args[0]  # remove command name from args\n     from scrapy.project import crawler\n-    crawler.configure(control_reactor=True)\n+    log.start()\n+    crawler.configure()\n     ret = _run_command(cmd, args, opts)\n     if ret is False:\n         parser.print_help()\n\n@@ -2,29 +2,23 @@ import signal\n \n from twisted.internet import reactor, defer\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import ExtensionManager\n-from scrapy import log\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n+from scrapy import log, signals\n \n \n class Crawler(object):\n \n     def __init__(self, settings, spiders):\n         self.configured = False\n-        self.control_reactor = True\n         self.settings = settings\n         self.spiders = spiders\n         self.engine = ExecutionEngine(self)\n \n-    def configure(self, control_reactor=True, queue=None):\n-        self.control_reactor = control_reactor\n-        if control_reactor:\n-            install_shutdown_handlers(self._signal_shutdown)\n-\n-        if not log.started:\n-            log.start()\n+    def configure(self, queue=None):\n         self.extensions = ExtensionManager.from_settings(self.settings)\n         if not self.spiders.loaded:\n             self.spiders.load()\n@@ -63,9 +57,6 @@ class Crawler(object):\n     def start(self):\n         yield defer.maybeDeferred(self.engine.start)\n         self._nextcall = reactor.callLater(0, self._start_next_spider)\n-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n-        if self.control_reactor:\n-            reactor.run(installSignalHandlers=False)\n \n     @defer.inlineCallbacks\n     def stop(self):\n@@ -73,22 +64,45 @@ class Crawler(object):\n             self._nextcall.cancel()\n         if self.engine.running:\n             yield defer.maybeDeferred(self.engine.stop)\n+\n+\n+class CrawlerProcess(Crawler):\n+    \"\"\"A class to run a single Scrapy crawler in a process. It provides\n+    automatic control of the Twisted reactor and installs some convenient\n+    signals for shutting down the crawl.\n+    \"\"\"\n+\n+    def __init__(self, *a, **kw):\n+        super(CrawlerProcess, self).__init__(*a, **kw)\n+        dispatcher.connect(self.stop, signals.engine_stopped)\n+        install_shutdown_handlers(self._signal_shutdown)\n+\n+    def start(self):\n+        super(CrawlerProcess, self).start()\n+        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n+        reactor.run(installSignalHandlers=False) # blocking call\n+\n+    def stop(self):\n+        d = super(CrawlerProcess, self).stop()\n+        d.addBoth(self._stop_reactor)\n+        return d\n+\n+    def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n         except RuntimeError: # raised if already stopped or in shutdown stage\n             pass\n \n     def _signal_shutdown(self, signum, _):\n+        install_shutdown_handlers(self._signal_kill)\n         signame = signal_names[signum]\n         log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n             \"unclean shutdown\" % signame, level=log.INFO)\n         reactor.callFromThread(self.stop)\n-        install_shutdown_handlers(self._signal_kill)\n \n     def _signal_kill(self, signum, _):\n+        install_shutdown_handlers(signal.SIG_IGN)\n         signame = signal_names[signum]\n         log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n             level=log.INFO)\n-        log.log_level = log.SILENT # disable logging of confusing tracebacks\n-        reactor.callFromThread(self.engine.kill)\n-        install_shutdown_handlers(signal.SIG_IGN)\n+        reactor.callFromThread(self._stop_reactor)\n\n@@ -1,6 +1,6 @@\n-from scrapy.crawler import Crawler\n+from scrapy.crawler import CrawlerProcess\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n _spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n-crawler = Crawler(settings, _spiders)\n+crawler = CrawlerProcess(settings, _spiders)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bea2f94357fb7aa545bbb28bf1ca2dd4267928df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 337 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,15 +7,16 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import ExtensionManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n+from scrapy.utils.misc import load_object\n from scrapy import log, signals\n \n \n class Crawler(object):\n \n-    def __init__(self, settings, spiders):\n+    def __init__(self, settings):\n         self.configured = False\n         self.settings = settings\n-        self.spiders = spiders\n+        self.spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n         self.engine = ExecutionEngine(self)\n \n     def configure(self, queue=None):\n\n@@ -1,6 +1,4 @@\n from scrapy.crawler import CrawlerProcess\n-from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n-_spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n-crawler = CrawlerProcess(settings, _spiders)\n+crawler = CrawlerProcess(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb51b9f7853003d6380766f5363368eca8f20037", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 392 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -173,8 +173,6 @@ MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_REPORT = False\n MEMUSAGE_WARNING_MB = 0\n \n-MYSQL_CONNECTION_SETTINGS = {}\n-\n NEWSPIDER_MODULE = ''\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fc78c4d0ad512dd2c23c23a7ad5c1c24be5e00b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 227 | Lines Deleted: 235 | Files Changed: 30 | Hunks: 130 | Methods Changed: 77 | Complexity Δ (Sum/Max): 25/18 | Churn Δ: 462 | Churn Cumulative: 4947 | Contributors (this commit): 6 | Commits (past 90d): 153 | Contributors (cumulative): 45 | DMM Complexity: 0.9444444444444444\n\nDIFF:\n@@ -7,6 +7,7 @@ import cProfile\n \n import scrapy\n from scrapy import log\n+from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n from scrapy.command import ScrapyCommand\n@@ -79,6 +80,8 @@ def check_deprecated_scrapy_ctl(argv):\n def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n+    crawler = CrawlerProcess(settings)\n+    crawler.install()\n     check_deprecated_scrapy_ctl(argv) # TODO: remove for Scrapy 0.11\n     cmds = _get_commands_dict()\n     cmdname = _get_command_name(argv)\n@@ -113,9 +116,8 @@ def execute(argv=None):\n \n     settings.defaults.update(cmd.default_settings)\n     del args[0]  # remove command name from args\n-    from scrapy.project import crawler\n     log.start()\n-    crawler.configure()\n+    cmd.set_crawler(crawler)\n     ret = _run_command(cmd, args, opts)\n     if ret is False:\n         parser.print_help()\n\n@@ -18,6 +18,14 @@ class ScrapyCommand(object):\n     # default settings to be used for this command instead of global defaults\n     default_settings = {}\n \n+    def set_crawler(self, crawler):\n+        self._crawler = crawler\n+\n+    @property\n+    def crawler(self):\n+        self._crawler.configure()\n+        return self._crawler\n+\n     def syntax(self):\n         \"\"\"\n         Command syntax (preferably one-line). Do not include command name.\n\n@@ -1,10 +1,7 @@\n from scrapy import log\n from scrapy.command import ScrapyCommand\n-from scrapy.core.queue import ExecutionQueue\n-from scrapy.project import crawler\n from scrapy.conf import settings\n from scrapy.http import Request\n-from scrapy.project import crawler\n from scrapy.utils.url import is_url\n \n from collections import defaultdict\n@@ -32,31 +29,29 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        q = ExecutionQueue()\n+        q = self.crawler.queue\n         urls, names = self._split_urls_and_names(args)\n         for name in names:\n             q.append_spider_name(name)\n \n         if opts.spider:\n             try:\n-                spider = crawler.spiders.create(opts.spider)\n+                spider = self.crawler.spiders.create(opts.spider)\n                 for url in urls:\n                     q.append_url(url, spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n             for name, urls in self._group_urls_by_spider(urls):\n-                spider = crawler.spiders.create(name)\n+                spider = self.crawler.spiders.create(name)\n                 for url in urls:\n                     q.append_url(url, spider)\n-\n-        crawler.queue = q\n-        crawler.start()\n+        self.crawler.start()\n \n     def _group_urls_by_spider(self, urls):\n         spider_urls = defaultdict(list)\n         for url in urls:\n-            spider_names = crawler.spiders.find_by_request(Request(url))\n+            spider_names = self.crawler.spiders.find_by_request(Request(url))\n             if not spider_names:\n                 log.msg('Could not find spider that handles url: %s' % url,\n                         log.ERROR)\n\n@@ -2,10 +2,8 @@ import pprint\n \n from scrapy import log\n from scrapy.command import ScrapyCommand\n-from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n-from scrapy.project import crawler\n from scrapy.utils.url import is_url\n \n class Command(ScrapyCommand):\n@@ -44,11 +42,11 @@ class Command(ScrapyCommand):\n         spider = None\n         if opts.spider:\n             try:\n-                spider = crawler.spiders.create(opts.spider)\n+                spider = self.crawler.spiders.create(opts.spider)\n             except KeyError:\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n \n-        crawler.queue.append_request(request, spider, \\\n+        self.crawler.queue.append_request(request, spider, \\\n             default_spider=BaseSpider('default'))\n-        crawler.start()\n+        self.crawler.start()\n \n\n@@ -4,7 +4,6 @@ from os import listdir\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n-from scrapy.project import crawler\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n@@ -60,7 +59,7 @@ class Command(ScrapyCommand):\n         name, domain = args[0:2]\n         module = sanitize_module_name(name)\n         try:\n-            spider = crawler.spiders.create(name)\n+            spider = self.crawler.spiders.create(name)\n         except KeyError:\n             pass\n         else:\n\n@@ -1,5 +1,4 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.project import crawler\n \n class Command(ScrapyCommand):\n \n@@ -10,4 +9,4 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        print \"\\n\".join(crawler.spiders.list())\n+        print \"\\n\".join(self.crawler.spiders.list())\n\n@@ -1,5 +1,4 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n@@ -69,11 +68,11 @@ class Command(ScrapyCommand):\n     def get_spider(self, request, opts):\n         if opts.spider:\n             try:\n-                return crawler.spiders.create(opts.spider)\n+                return self.crawler.spiders.create(opts.spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n-            spider = crawler.spiders.create_for_request(request)\n+            spider = self.crawler.spiders.create_for_request(request)\n             if spider:\n                 return spider\n             log.msg('Unable to find spider for: %s' % request, log.ERROR)\n@@ -84,8 +83,8 @@ class Command(ScrapyCommand):\n         spider = self.get_spider(request, opts)\n         if not spider:\n             return None, None\n-        crawler.queue.append_request(request, spider)\n-        crawler.start()\n+        self.crawler.queue.append_request(request, spider)\n+        self.crawler.start()\n         if not responses:\n             log.msg('No response downloaded for: %s' % request, log.ERROR, \\\n                 spider=spider)\n\n@@ -1,8 +1,9 @@\n from scrapy.command import ScrapyCommand\n+from scrapy.commands import runserver\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n-class Command(ScrapyCommand):\n+class Command(runserver.Command):\n \n     requires_project = True\n     default_settings = {'LOG_ENABLED': False}\n@@ -26,7 +27,7 @@ class Command(ScrapyCommand):\n         cmd = args[0]\n \n         botname = settings['BOT_NAME']\n-        queue = load_object(settings['SERVICE_QUEUE'])().queue\n+        queue = self.crawler.queue.queue\n \n         if cmd == 'add':\n             if len(args) < 2:\n\n@@ -1,6 +1,4 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.project import crawler\n-from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n class Command(ScrapyCommand):\n@@ -10,7 +8,9 @@ class Command(ScrapyCommand):\n     def short_desc(self):\n         return \"Start Scrapy in server mode\"\n \n+    def process_options(self, args, opts):\n+        super(Command, self).process_options(args, opts)\n+        settings.overrides['QUEUE_CLASS'] = settings['SERVER_QUEUE_CLASS']\n+\n     def run(self, args, opts):\n-        queue_class = load_object(settings['SERVICE_QUEUE'])\n-        crawler.queue = queue_class()\n-        crawler.start()\n+        self.crawler.start()\n\n@@ -4,7 +4,6 @@ import os\n from scrapy import log\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n-from scrapy.project import crawler\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n@@ -52,5 +51,5 @@ class Command(ScrapyCommand):\n             return\n         spider = spclasses.pop()()\n         # schedule spider and start engine\n-        crawler.queue.append_spider(spider)\n-        crawler.start()\n+        self.crawler.queue.append_spider(spider)\n+        self.crawler.start()\n\n@@ -1,5 +1,4 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings as settings_\n \n class Command(ScrapyCommand):\n \n@@ -24,26 +23,16 @@ class Command(ScrapyCommand):\n             help=\"print setting value, intepreted as an float\")\n         parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\", \\\n             help=\"print setting value, intepreted as an float\")\n-        parser.add_option(\"--init\", dest=\"init\", action=\"store_true\", \\\n-            help=\"print initial setting value (before loading extensions and spiders)\")\n-\n-    def process_options(self, args, opts):\n-        super(Command, self).process_options(args, opts)\n-        if opts.init:\n-            self._print_setting(opts)\n \n     def run(self, args, opts):\n-        if not opts.init:\n-            self._print_setting(opts)\n-\n-    def _print_setting(self, opts):\n+        settings = self.crawler.settings\n         if opts.get:\n-            print settings_.get(opts.get)\n+            print settings.get(opts.get)\n         elif opts.getbool:\n-            print settings_.getbool(opts.getbool)\n+            print settings.getbool(opts.getbool)\n         elif opts.getint:\n-            print settings_.getint(opts.getint)\n+            print settings.getint(opts.getint)\n         elif opts.getfloat:\n-            print settings_.getfloat(opts.getfloat)\n+            print settings.getfloat(opts.getfloat)\n         elif opts.getlist:\n-            print settings_.getlist(opts.getlist)\n+            print settings.getlist(opts.getlist)\n\n@@ -4,14 +4,13 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-from scrapy.project import crawler\n-from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n \n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'QUEUE_CLASS': 'scrapy.core.queue.KeepAliveExecutionQueue'}\n \n     def syntax(self):\n         return \"[url|file]\"\n@@ -30,7 +29,6 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        shell = Shell(crawler, update_vars=self.update_vars, inthread=True)\n-        shell.start(url=url).addBoth(lambda _: crawler.stop())\n-        crawler.queue = KeepAliveExecutionQueue()\n-        crawler.start()\n+        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True)\n+        shell.start(url=url).addBoth(lambda _: self.crawler.stop())\n+        self.crawler.start()\n\n@@ -175,6 +175,8 @@ MEMUSAGE_WARNING_MB = 0\n \n NEWSPIDER_MODULE = ''\n \n+QUEUE_CLASS = 'scrapy.core.queue.ExecutionQueue'\n+\n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REDIRECT_MAX_METAREFRESH_DELAY = 100\n@@ -207,8 +209,7 @@ SCHEDULER_MIDDLEWARES_BASE = {\n \n SCHEDULER_ORDER = 'DFO'\n \n-SERVICE_QUEUE = 'scrapy.contrib.queue.SqliteExecutionQueue'\n-SERVICE_QUEUE_FILE = 'scrapy.db'\n+SERVER_QUEUE_CLASS = 'scrapy.contrib.queue.SqliteExecutionQueue'\n \n SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'\n \n@@ -227,6 +228,8 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n+SQLITE_DB = 'scrapy.db'\n+\n SQS_QUEUE = 'scrapy'\n SQS_VISIBILITY_TIMEOUT = 7200\n SQS_POLLING_DELAY = 30\n\n@@ -2,7 +2,6 @@ from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n-from scrapy.project import crawler\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n@@ -21,6 +20,8 @@ class MediaPipeline(object):\n \n     def __init__(self):\n         self.spiderinfo = {}\n+        from scrapy.project import crawler\n+        self.crawler = crawler\n \n     def open_spider(self, spider):\n         self.spiderinfo[spider] = self.SpiderInfo(spider)\n\n@@ -8,7 +8,7 @@ class SqliteExecutionQueue(ExecutionQueue):\n \n     def __init__(self, *a, **kw):\n         super(SqliteExecutionQueue, self).__init__(*a, **kw)\n-        self.queue = JsonSqlitePriorityQueue(settings['SERVICE_QUEUE_FILE'])\n+        self.queue = JsonSqlitePriorityQueue(settings['SQLITE_DB'])\n \n     def _append_next(self):\n         msg = self.queue.pop()\n\n@@ -40,7 +40,7 @@ class ExecutionEngine(object):\n         \"\"\"\n         self.scheduler = load_object(settings['SCHEDULER'])()\n         self.downloader = Downloader()\n-        self.scraper = Scraper(self)\n+        self.scraper = Scraper(self, self.crawler.settings)\n         self.configured = True\n         self._spider_closed_callback = spider_closed_callback\n \n\n@@ -9,10 +9,7 @@ class ExecutionQueue(object):\n \n     polling_delay = 5\n \n-    def __init__(self, _spiders=None):\n-        if _spiders is None:\n-            from scrapy.project import crawler\n-            _spiders = crawler.spiders\n+    def __init__(self, _spiders):\n         self.spider_requests = []\n         self._spiders = _spiders\n \n\n@@ -15,7 +15,6 @@ from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy import log\n from scrapy.stats import stats\n-from scrapy.conf import settings\n \n \n class SpiderInfo(object):\n@@ -60,7 +59,7 @@ class SpiderInfo(object):\n \n class Scraper(object):\n \n-    def __init__(self, engine):\n+    def __init__(self, engine, settings):\n         self.sites = {}\n         self.spidermw = SpiderMiddlewareManager()\n         itemproc_cls = load_object(settings['ITEM_PROCESSOR'])\n\n@@ -4,7 +4,6 @@ from twisted.internet import reactor, defer\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import ExtensionManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n@@ -16,17 +15,28 @@ class Crawler(object):\n     def __init__(self, settings):\n         self.configured = False\n         self.settings = settings\n-        self.spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n-        self.engine = ExecutionEngine(self)\n \n-    def configure(self, queue=None):\n-        self.extensions = ExtensionManager.from_settings(self.settings)\n-        if not self.spiders.loaded:\n-            self.spiders.load()\n+    def install(self):\n+        import scrapy.project\n+        assert not hasattr(scrapy.project, 'crawler'), \"crawler already installed\"\n+        scrapy.project.crawler = self\n \n-        self.queue = queue or ExecutionQueue()\n-        self.engine.configure(self._spider_closed)\n+    def uninstall(self):\n+        import scrapy.project\n+        assert hasattr(scrapy.project, 'crawler'), \"crawler not installed\"\n+        del scrapy.project.crawler\n+\n+    def configure(self):\n+        if self.configured:\n+            return\n         self.configured = True\n+        self.engine = ExecutionEngine(self)\n+        self.extensions = ExtensionManager.from_settings(self.settings)\n+        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n+        self.spiders = spman_cls.from_settings(self.settings)\n+        queue_cls = load_object(self.settings['QUEUE_CLASS'])\n+        self.queue = queue_cls(self.spiders)\n+        self.engine.configure(self._spider_closed)\n \n     @defer.inlineCallbacks\n     def _start_next_spider(self):\n@@ -56,6 +66,7 @@ class Crawler(object):\n \n     @defer.inlineCallbacks\n     def start(self):\n+        yield defer.maybeDeferred(self.configure)\n         yield defer.maybeDeferred(self.engine.start)\n         self._nextcall = reactor.callLater(0, self._start_next_spider)\n \n\n@@ -1,4 +1,11 @@\n-from scrapy.crawler import CrawlerProcess\n-from scrapy.conf import settings\n+\"\"\"\n+This module contains the Scrapy Crawler once installed by calling the crawler\n+``install`` method, like this::\n \n-crawler = CrawlerProcess(settings)\n+    crawler.install()\n+\n+After that, you can import the (singleton) crawler like this::\n+\n+    from scrapy.project import crawler\n+\n+\"\"\"\n\n@@ -4,16 +4,26 @@ spiders\n \"\"\"\n \n from scrapy import log\n-from scrapy.conf import settings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n \n \n class SpiderManager(object):\n \n-    def __init__(self):\n-        self.loaded = False\n+    def __init__(self, spider_modules):\n+        self.spider_modules = spider_modules\n         self._spiders = {}\n+        for name in self.spider_modules:\n+            for module in walk_modules(name):\n+                self._load_spiders(module)\n+\n+    def _load_spiders(self, module):\n+        for spcls in iter_spider_classes(module):\n+            self._spiders[spcls.name] = spcls\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(settings.getlist('SPIDER_MODULES'))\n \n     def create(self, spider_name, **spider_kwargs):\n         \"\"\"Returns a Spider instance for the given spider name, using the given\n@@ -53,21 +63,5 @@ class SpiderManager(object):\n         \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n \n-    def load(self, spider_modules=None):\n-        \"\"\"Load spiders from spider_modules or SPIDER_MODULES setting.\"\"\"\n-        if spider_modules is None:\n-            spider_modules = settings.getlist('SPIDER_MODULES')\n-        self.spider_modules = spider_modules\n-\n-        self._spiders = {}\n-        for name in self.spider_modules:\n-            for module in walk_modules(name):\n-                self._load_spiders(module)\n-        self.loaded = True\n-\n-    def _load_spiders(self, module):\n-        for spcls in iter_spider_classes(module):\n-            self._spiders[spcls.name] = spcls\n-\n     def close_spider(self, spider):\n         pass\n\n@@ -10,9 +10,11 @@ from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n from twisted.internet import reactor, protocol\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy.project import crawler\n from scrapy.stats import stats\n+from scrapy import signals\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n@@ -35,8 +37,15 @@ class TelnetConsole(protocol.ServerFactory):\n         if not settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n         self.noisy = False\n-        port = settings.getint('TELNETCONSOLE_PORT')\n-        reactor.callWhenRunning(reactor.listenTCP, port, self)\n+        self.portnum = settings.getint('TELNETCONSOLE_PORT')\n+        dispatcher.connect(self.start_listening, signals.engine_started)\n+        dispatcher.connect(self.stop_listening, signals.engine_stopped)\n+\n+    def start_listening(self):\n+        self.port = reactor.listenTCP(self.portnum, self)\n+\n+    def stop_listening(self):\n+        self.port.stopListening()\n \n     def protocol(self):\n         telnet_vars = self._get_telnet_vars()\n\n@@ -20,29 +20,20 @@ class CmdlineTest(unittest.TestCase):\n         return comm[0].strip()\n \n     def test_default_settings(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init'), \\\n-            'default')\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n             'default + loaded + started')\n \n     def test_override_settings_using_settings_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init', \\\n-            '--settings', 'scrapy.tests.test_cmdline.settings2'), \\\n-            'override')\n         self.assertEqual(self._execute('settings', '--get', 'TEST1', \\\n             '--settings', 'scrapy.tests.test_cmdline.settings2'), \\\n             'override + loaded + started')\n \n     def test_override_settings_using_set_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init', '--set', 'TEST1=override'), \\\n-            'override')\n         self.assertEqual(self._execute('settings', '--get', 'TEST1', '--set', 'TEST1=override'), \\\n             'override + loaded + started')\n \n     def test_override_settings_using_envvar(self):\n         self.env['SCRAPY_TEST1'] = 'override'\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init'), \\\n-            'override')\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n             'override + loaded + started')\n \n\n@@ -1,20 +1,19 @@\n-\"\"\"\n-Scrapy engine tests\n-\"\"\"\n+import sys, os, re, urlparse\n \n-import sys, os, re, urlparse, unittest\n-\n-from twisted.internet import reactor\n-from twisted.web import server, resource, static, util\n+from twisted.internet import reactor, defer\n+from twisted.web import server, static, util\n+from twisted.trial import unittest\n \n from scrapy import signals\n-from scrapy.project import crawler\n+from scrapy.conf import Settings\n+from scrapy.crawler import Crawler\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.tests import tests_datadir\n from scrapy.spider import BaseSpider\n from scrapy.item import Item, Field\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.http import Request\n+from scrapy.utils.signal import disconnect_all\n \n class TestItem(Item):\n     name = Field()\n@@ -24,7 +23,6 @@ class TestItem(Item):\n class TestSpider(BaseSpider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n-    start_urls = ['http://localhost']\n \n     itemurl_re = re.compile(\"item\\d+.html\")\n     name_re = re.compile(\"<h1>(.*?)</h1>\", re.M)\n@@ -48,66 +46,61 @@ class TestSpider(BaseSpider):\n             item['price'] = m.group(1)\n         return item\n \n-#class TestResource(resource.Resource):\n-#    isLeaf = True\n-#\n-#    def render_GET(self, request):\n-#        return \"hello world!\"\n-\n-def start_test_site():\n+def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n-#    r.putChild(\"test\", TestResource())\n     r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n     r.putChild(\"redirected\", static.Data(\"Redirected here\", \"text/plain\"))\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n+    if debug:\n+        print \"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" \\\n+            % port.getHost().port\n     return port\n \n \n-class CrawlingSession(object):\n+class CrawlerRun(object):\n+    \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n \n     def __init__(self):\n-        self.name = 'scrapytest.org'\n         self.spider = None\n         self.respplug = []\n         self.reqplug = []\n         self.itemresp = []\n         self.signals_catched = {}\n-        self.wasrun = False\n \n     def run(self):\n         self.port = start_test_site()\n         self.portno = self.port.getHost().port\n \n-        self.spider = TestSpider()\n-        if self.spider:\n-            self.spider.start_urls = [\n-                self.geturl(\"/\"),\n-                self.geturl(\"/redirect\"),\n-                ]\n+        start_urls = [self.geturl(\"/\"), self.geturl(\"/redirect\")]\n+        self.spider = TestSpider(start_urls=start_urls)\n \n-            dispatcher.connect(self.record_signal, signals.engine_started)\n-            dispatcher.connect(self.record_signal, signals.engine_stopped)\n-            dispatcher.connect(self.record_signal, signals.spider_opened)\n-            dispatcher.connect(self.record_signal, signals.spider_idle)\n-            dispatcher.connect(self.record_signal, signals.spider_closed)\n+        for name, signal in vars(signals).items():\n+            if not name.startswith('_'):\n+                dispatcher.connect(self.record_signal, signal)\n         dispatcher.connect(self.item_scraped, signals.item_scraped)\n         dispatcher.connect(self.request_received, signals.request_received)\n         dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n-            crawler.configure()\n-            crawler.queue.append_spider(self.spider)\n-            crawler.start()\n+        settings = Settings()\n+        self.crawler = Crawler(settings)\n+        self.crawler.install()\n+        self.crawler.configure()\n+        self.crawler.queue.append_spider(self.spider)\n+        self.crawler.start()\n+\n+        self.deferred = defer.Deferred()\n+        dispatcher.connect(self.stop, signals.engine_stopped)\n+        return self.deferred\n+\n+    def stop(self):\n         self.port.stopListening()\n-            self.wasrun = True\n-            # FIXME: extremly ugly hack to avoid propagating errors to other\n-            # stats because of living signals. This whole test_engine.py should\n-            # be rewritten from scratch actually.\n-            from scrapy.utils.signal import disconnect_all\n-            disconnect_all(signals.stats_spider_opened)\n-            disconnect_all(signals.stats_spider_closing)\n-            disconnect_all(signals.stats_spider_closed)\n+        for name, signal in vars(signals).items():\n+            if not name.startswith('_'):\n+                disconnect_all(signal)\n+        self.crawler.uninstall()\n+        self.deferred.callback(None)\n \n     def geturl(self, path):\n         return \"http://localhost:%s%s\" % (self.portno, path)\n@@ -132,67 +125,49 @@ class CrawlingSession(object):\n         signalargs.pop('sender', None)\n         self.signals_catched[sig] = signalargs\n \n-session = CrawlingSession()\n-\n \n class EngineTest(unittest.TestCase):\n \n-    def setUp(self):\n-        if not session.wasrun:\n-            session.run()\n+    @defer.inlineCallbacks\n+    def test_crawler(self):\n+        self.run = CrawlerRun()\n+        yield self.run.run()\n+        self._assert_visited_urls()\n+        self._assert_received_requests()\n+        self._assert_downloaded_responses()\n+        self._assert_scraped_items()\n+        self._assert_signals_catched()\n \n-    def test_spider_locator(self):\n-        \"\"\"\n-        Check the spider is loaded and located properly via the SpiderLocator\n-        \"\"\"\n-        assert session.spider is not None\n-        self.assertEqual(session.spider.name, session.name)\n-\n-    def test_visited_urls(self):\n-        \"\"\"\n-        Make sure certain URls were actually visited\n-        \"\"\"\n-        # expected urls that should be visited\n+    def _assert_visited_urls(self):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\", \n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n-\n-        urls_visited = set([rp[0].url for rp in session.respplug])\n-        urls_expected = set([session.geturl(p) for p in must_be_visited])\n+        urls_visited = set([rp[0].url for rp in self.run.respplug])\n+        urls_expected = set([self.run.geturl(p) for p in must_be_visited])\n         assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n \n-    def test_requests_received(self):\n-        \"\"\"\n-        Check requests received\n-        \"\"\"\n+    def _assert_received_requests(self):\n         # 3 requests should be received from the spider. start_urls and redirects don't count\n-        self.assertEqual(3, len(session.reqplug))\n+        self.assertEqual(3, len(self.run.reqplug))\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n-        urls_requested = set([rq[0].url for rq in session.reqplug])\n-        urls_expected = set([session.geturl(p) for p in paths_expected])\n+        urls_requested = set([rq[0].url for rq in self.run.reqplug])\n+        urls_expected = set([self.run.geturl(p) for p in paths_expected])\n         assert urls_expected <= urls_requested\n \n-    def test_responses_downloaded(self):\n-        \"\"\"\n-        Check responses downloaded\n-        \"\"\"\n+    def _assert_downloaded_responses(self):\n         # response tests\n-        self.assertEqual(6, len(session.respplug))\n+        self.assertEqual(6, len(self.run.respplug))\n \n-        for response, spider in session.respplug:\n-            if session.getpath(response.url) == '/item999.html':\n+        for response, _ in self.run.respplug:\n+            if self.run.getpath(response.url) == '/item999.html':\n                 self.assertEqual(404, response.status)\n-            if session.getpath(response.url) == '/redirect':\n+            if self.run.getpath(response.url) == '/redirect':\n                 self.assertEqual(302, response.status)\n \n-    def test_item_data(self):\n-        \"\"\"\n-        Check item data\n-        \"\"\"\n-        # item tests\n-        self.assertEqual(2, len(session.itemresp))\n-        for item, response in session.itemresp:\n+    def _assert_scraped_items(self):\n+        self.assertEqual(2, len(self.run.itemresp))\n+        for item, response in self.run.itemresp:\n             self.assertEqual(item['url'], response.url)\n             if 'item1.html' in item['url']:\n                 self.assertEqual('Item 1 name', item['name'])\n@@ -201,29 +176,22 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n-    def test_signals(self):\n-        \"\"\"\n-        Check signals were sent properly\n-        \"\"\"\n-        from scrapy import signals\n+    def _assert_signals_catched(self):\n+        assert signals.engine_started in self.run.signals_catched\n+        assert signals.engine_stopped in self.run.signals_catched\n+        assert signals.spider_opened in self.run.signals_catched\n+        assert signals.spider_idle in self.run.signals_catched\n+        assert signals.spider_closed in self.run.signals_catched\n \n-        assert signals.engine_started in session.signals_catched\n-        assert signals.engine_stopped in session.signals_catched\n-        assert signals.spider_opened in session.signals_catched\n-        assert signals.spider_idle in session.signals_catched\n-        assert signals.spider_closed in session.signals_catched\n+        self.assertEqual({'spider': self.run.spider},\n+                         self.run.signals_catched[signals.spider_opened])\n+        self.assertEqual({'spider': self.run.spider},\n+                         self.run.signals_catched[signals.spider_idle])\n+        self.assertEqual({'spider': self.run.spider, 'reason': 'finished'},\n+                         self.run.signals_catched[signals.spider_closed])\n \n-        self.assertEqual({'spider': session.spider},\n-                         session.signals_catched[signals.spider_opened])\n-        self.assertEqual({'spider': session.spider},\n-                         session.signals_catched[signals.spider_idle])\n-        self.assertEqual({'spider': session.spider, 'reason': 'finished'},\n-                         session.signals_catched[signals.spider_closed])\n \n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n-        port = start_test_site()\n-        print \"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" % port.getHost().port\n+        start_test_site(debug=True)\n         reactor.run()\n-    else:\n-        unittest.main()\n\n@@ -1,5 +1,6 @@\n import os\n from twisted.trial import unittest\n+from scrapy.crawler import Crawler\n from scrapy.conf import settings\n from tempfile import mkdtemp\n from shutil import rmtree\n@@ -7,6 +8,8 @@ from shutil import rmtree\n \n class ImagesPipelineTestCase(unittest.TestCase):\n     def setUp(self):\n+        self.crawler = Crawler(settings)\n+        self.crawler.install()\n         try:\n             import Image\n         except ImportError, e:\n@@ -23,6 +26,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         del self.pipeline\n         rmtree(self.tempdir)\n         settings.disabled = self.settings_disabled_before\n+        self.crawler.uninstall()\n \n     def test_image_path(self):\n         image_path = self.pipeline.image_key\n\n@@ -2,6 +2,8 @@ from twisted.trial import unittest\n from twisted.python import failure\n from twisted.internet import defer, reactor\n \n+from scrapy.conf import Settings\n+from scrapy.crawler import Crawler\n from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n from scrapy.utils.request import request_fingerprint\n@@ -29,12 +31,15 @@ class MediaPipelineTestCase(unittest.TestCase):\n     pipeline_class = _MockedMediaPipeline\n \n     def setUp(self):\n+        self.crawler = Crawler(Settings())\n+        self.crawler.install()\n         self.spider = BaseSpider('media.com')\n         self.pipe = self.pipeline_class()\n         self.pipe.open_spider(self.spider)\n \n     def tearDown(self):\n         self.pipe.close_spider(self.spider)\n+        self.crawler.uninstall()\n \n     @defer.inlineCallbacks\n     def test_return_item_by_default(self):\n\n@@ -22,10 +22,7 @@ class SpiderManagerTest(unittest.TestCase):\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n-        self.spiderman = SpiderManager()\n-        assert not self.spiderman.loaded\n-        self.spiderman.load(['test_spiders_xxx'])\n-        assert self.spiderman.loaded\n+        self.spiderman = SpiderManager(['test_spiders_xxx'])\n \n     def tearDown(self):\n         del self.spiderman\n@@ -57,9 +54,9 @@ class SpiderManagerTest(unittest.TestCase):\n             ['spider3'])\n \n     def test_load_spider_module(self):\n-        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider1'])\n+        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider1'])\n         assert len(self.spiderman._spiders) == 1\n \n     def test_load_base_spider(self):\n-        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider0'])\n+        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider0'])\n         assert len(self.spiderman._spiders) == 0\n\n@@ -3,6 +3,8 @@ from cStringIO import StringIO\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n+from scrapy.utils.serialize import ScrapyJSONDecoder\n+from scrapy.tests.test_utils_serialize import ExecutionMangerStub\n from scrapy.utils.py26 import json\n \n class urllib_stub(object):\n@@ -30,6 +32,10 @@ class TestTarget(object):\n \n class JsonRpcUtilsTestCase(unittest.TestCase):\n \n+    def setUp(self):\n+        crawler = ExecutionMangerStub([])\n+        self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n+\n     def test_jsonrpc_client_call_request(self):\n         ul = urllib_stub(1)\n         jsonrpc_client_call('url', 'test', 'one', 2, _urllib=ul)\n@@ -62,26 +68,26 @@ class JsonRpcUtilsTestCase(unittest.TestCase):\n \n     def test_jsonrpc_server_call(self):\n         t = TestTarget()\n-        r = jsonrpc_server_call(t, 'invalid json data')\n+        r = jsonrpc_server_call(t, 'invalid json data', self.json_decoder)\n         assert 'error' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] is None\n         self.assertEqual(r['error']['code'], jsonrpc_errors.PARSE_ERROR)\n         assert 'Traceback' in r['error']['data']\n \n-        r = jsonrpc_server_call(t, '{\"test\": \"test\"}')\n+        r = jsonrpc_server_call(t, '{\"test\": \"test\"}', self.json_decoder)\n         assert 'error' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] is None\n         self.assertEqual(r['error']['code'], jsonrpc_errors.INVALID_REQUEST)\n \n-        r = jsonrpc_server_call(t, '{\"method\": \"notfound\", \"id\": 1}')\n+        r = jsonrpc_server_call(t, '{\"method\": \"notfound\", \"id\": 1}', self.json_decoder)\n         assert 'error' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] == 1\n         self.assertEqual(r['error']['code'], jsonrpc_errors.METHOD_NOT_FOUND)\n \n-        r = jsonrpc_server_call(t, '{\"method\": \"exception\", \"id\": 1}')\n+        r = jsonrpc_server_call(t, '{\"method\": \"exception\", \"id\": 1}', self.json_decoder)\n         assert 'error' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] == 1\n@@ -89,19 +95,21 @@ class JsonRpcUtilsTestCase(unittest.TestCase):\n         assert 'testing-errors' in r['error']['message']\n         assert 'Traceback' in r['error']['data']\n \n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"id\": 2}')\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"id\": 2}', self.json_decoder)\n         assert 'result' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] == 2\n         self.assertEqual(r['result'], ([], {}))\n \n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": [456, 123], \"id\": 3}')\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": [456, 123], \"id\": 3}', \\\n+            self.json_decoder)\n         assert 'result' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] == 3\n         self.assertEqual(r['result'], ([456, 123], {}))\n \n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": {\"data\": 789}, \"id\": 3}')\n+        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": {\"data\": 789}, \"id\": 3}', \\\n+            self.json_decoder)\n         assert 'result' in r\n         assert r['jsonrpc'] == '2.0'\n         assert r['id'] == 3\n\n@@ -4,7 +4,6 @@ import decimal\n \n from twisted.internet import defer\n \n-from scrapy.project import crawler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n from scrapy.utils.py26 import json\n@@ -20,8 +19,10 @@ class SpiderReferencer(object):\n \n     spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')\n \n-    def __init__(self, manager=None):\n-        self.manager = manager or crawler\n+    def __init__(self, crawler=None):\n+        if crawler is None:\n+            from scrapy.project import crawler\n+        self.manager = crawler\n \n     def get_reference_from_spider(self, spider):\n         return 'spider:%x:%s' % (id(spider), spider.name)\n@@ -109,7 +110,8 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n class ScrapyJSONDecoder(json.JSONDecoder):\n \n     def __init__(self, *a, **kw):\n-        self.spref = kw.pop('spref', None) or SpiderReferencer()\n+        crawler = kw.pop('crawler', None)\n+        self.spref = kw.pop('spref', None) or SpiderReferencer(crawler)\n         super(ScrapyJSONDecoder, self).__init__(*a, **kw)\n \n     def decode(self, s):\n\n@@ -7,7 +7,9 @@ See docs/topics/ws.rst\n from twisted.internet import reactor\n from twisted.web import server, resource, error\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n+from scrapy import signals\n from scrapy.utils.jsonrpc import jsonrpc_server_call\n from scrapy.utils.serialize import ScrapyJSONEncoder, ScrapyJSONDecoder\n from scrapy.utils.misc import load_object\n@@ -73,7 +75,7 @@ class WebService(server.Site):\n         if not settings.getbool('WEBSERVICE_ENABLED'):\n             raise NotConfigured\n         logfile = settings['WEBSERVICE_LOGFILE']\n-        port = settings.getint('WEBSERVICE_PORT')\n+        self.portnum = settings.getint('WEBSERVICE_PORT')\n         root = RootResource()\n         reslist = build_component_list(settings['WEBSERVICE_RESOURCES_BASE'], \\\n             settings['WEBSERVICE_RESOURCES'])\n@@ -82,5 +84,12 @@ class WebService(server.Site):\n             root.putChild(res.ws_name, res)\n         server.Site.__init__(self, root, logPath=logfile)\n         self.noisy = False\n-        reactor.callWhenRunning(reactor.listenTCP, port, self)\n+        dispatcher.connect(self.start_listening, signals.engine_started)\n+        dispatcher.connect(self.stop_listening, signals.engine_stopped)\n+\n+    def start_listening(self):\n+        self.port = reactor.listenTCP(self.portnum, self)\n+\n+    def stop_listening(self):\n+        self.port.stopListening()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
