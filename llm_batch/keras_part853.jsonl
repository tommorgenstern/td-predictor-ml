{"custom_id": "keras#f0869a9cbe29d0f9fff7ed0e086240cea25a133a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 989 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -723,7 +723,7 @@ class BaseOptimizer(KerasSaveable):\n             if filtered_grads[i] is None:\n                 filtered_grads.pop(i)\n                 v = filtered_vars.pop(i)\n-                missing_grad_vars.append(v.name)\n+                missing_grad_vars.append(v.path)\n \n         if not filtered_grads:\n             raise ValueError(\"No gradients provided for any variable.\")\n\n@@ -665,9 +665,20 @@ class CompileLoss(losses_module.Loss):\n             tree.assert_same_structure(y_pred, y_true, check_types=False)\n         except ValueError:\n             # y_true is either flat or leaf\n-            if not tree.is_nested(y_true):\n+            if (\n+                not tree.is_nested(y_true)\n+                and isinstance(y_pred, (tuple, list))\n+                and len(y_pred) == 1\n+            ):\n                 y_true = [y_true]\n+            try:\n                 y_true = tree.pack_sequence_as(y_pred, y_true)\n+            except:\n+                raise ValueError(\n+                    \"y_true and y_pred have different structures.\\n\"\n+                    f\"y_true: {y_true}\\n\"\n+                    f\"y_pred: {y_pred}\\n\"\n+                )\n \n         if not self.built:\n             self.build(y_true, y_pred)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8dc19d2d1234366b7e3f0f2f425c58cbbccfed42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 815 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -667,7 +667,7 @@ class CompileLoss(losses_module.Loss):\n             # y_true is either flat or leaf\n             if (\n                 not tree.is_nested(y_true)\n-                and isinstance(y_pred, (tuple, list))\n+                and hasattr(y_pred, \"__len__\")\n                 and len(y_pred) == 1\n             ):\n                 y_true = [y_true]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb5c5ae1cb13084469b73b1a470301449d565598", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 5 | Files Changed: 6 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 33 | Churn Cumulative: 1022 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,6 +35,29 @@ def add(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n+\n+    # Special case of `tf.add`: `tf.nn.bias_add`\n+    # `BiasAdd` can be fused with `MatMul` and `Conv*` kernels\n+    # Expecting `x1` to be `inputs` and `x2` to be `bias` (no swapping)\n+    x2_squeeze_shape = [d for d in x2.shape if d is None or d > 1]\n+    if (\n+        # `x2` looks like bias (can be squeezed to vector)\n+        1 == len(x2_squeeze_shape)\n+        # `x1` looks like input tensor (rank >= 2)\n+        and len(x1.shape) > 1\n+        # `x2` non-squeezable dimension defined\n+        and x2_squeeze_shape[0] is not None\n+        # `x2` non-squeezable dimension match `x1` channel dimension\n+        and x2_squeeze_shape[0] in {x1.shape[1], x1.shape[-1]}\n+    ):\n+        if x1.shape[-1] == x2_squeeze_shape[0]:\n+            data_format = \"NHWC\"\n+        else:\n+            data_format = \"NCHW\"\n+        if len(x2.shape) > 1:\n+            x2 = tf.squeeze(x2)\n+        return tf.nn.bias_add(x1, x2, data_format=data_format)\n+\n     return tf.add(x1, x2)\n \n \n\n@@ -250,7 +250,7 @@ class BaseConv(Layer):\n             else:\n                 bias_shape = (1, self.filters) + (1,) * self.rank\n             bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n+            outputs = ops.add(outputs, bias)\n \n         if self.activation is not None:\n             return self.activation(outputs)\n\n@@ -205,7 +205,7 @@ class BaseConvTranspose(Layer):\n             else:\n                 bias_shape = (1, self.filters) + (1,) * self.rank\n             bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n+            outputs = ops.add(outputs, bias)\n \n         if self.activation is not None:\n             return self.activation(outputs)\n\n@@ -220,7 +220,7 @@ class BaseDepthwiseConv(Layer):\n                     1,\n                 ) * self.rank\n             bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n+            outputs = ops.add(outputs, bias)\n \n         if self.activation is not None:\n             return self.activation(outputs)\n\n@@ -232,7 +232,7 @@ class BaseSeparableConv(Layer):\n             else:\n                 bias_shape = (1, self.filters) + (1,) * self.rank\n             bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n+            outputs = ops.add(outputs, bias)\n \n         if self.activation is not None:\n             return self.activation(outputs)\n\n@@ -163,7 +163,7 @@ class Conv1D(BaseConv):\n             else:\n                 bias_shape = (1, self.filters) + (1,) * self.rank\n             bias = ops.reshape(self.bias, bias_shape)\n-            outputs += bias\n+            outputs = ops.add(outputs, bias)\n \n         if self.activation is not None:\n             return self.activation(outputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b771bb6e5b0a820cf7e1276609ede945f8e5874", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 15 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class Softmax(Layer):\n     ```\n \n     Example:\n-    >>> softmax_layer = keras.layers.activations.Softmax()\n+    >>> softmax_layer = keras.layers.Softmax()\n     >>> input = np.array([1.0, 2.0, 1.0])\n     >>> result = softmax_layer(input)\n     >>> result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1de47276f654d25508a6303800016745bc3cf95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 18/5 | Churn Δ: 149 | Churn Cumulative: 4103 | Contributors (this commit): 20 | Commits (past 90d): 65 | Contributors (cumulative): 98 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from keras.src.activations.activations import celu\n from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n+from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n\n@@ -71,6 +71,7 @@ from keras.src.ops.nn import depthwise_conv\n from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n+from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -17,6 +17,7 @@ from keras.src.ops.nn import depthwise_conv\n from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n+from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -11,6 +11,7 @@ from keras.src.activations.activations import celu\n from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n+from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n\n@@ -71,6 +71,7 @@ from keras.src.ops.nn import depthwise_conv\n from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n+from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -17,6 +17,7 @@ from keras.src.ops.nn import depthwise_conv\n from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n+from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -4,6 +4,7 @@ from keras.src.activations.activations import celu\n from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n+from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import leaky_relu\n@@ -35,6 +36,7 @@ ALL_OBJECTS = {\n     softsign,\n     silu,\n     gelu,\n+    glu,\n     tanh,\n     sigmoid,\n     exponential,\n\n@@ -323,6 +323,27 @@ def celu(x, alpha=1.0):\n     return ops.celu(x, alpha=alpha)\n \n \n+@keras_export(\"keras.activations.glu\")\n+def glu(x, axis=-1):\n+    \"\"\"Gated Linear Unit (GLU) activation function.\n+\n+    The GLU activation function is defined as:\n+\n+    `glu(x) = a * sigmoid(b)`,\n+\n+    where `x` is split into two equal parts `a` and `b` along the given axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis along which to split the input tensor. Defaults to `-1`.\n+\n+    Reference:\n+\n+    - [Dauphin et al., 2017](https://arxiv.org/abs/1612.08083)\n+    \"\"\"\n+    return ops.glu(x, axis=axis)\n+\n+\n @keras_export(\"keras.activations.tanh\")\n def tanh(x):\n     \"\"\"Hyperbolic tangent activation function.\n\n@@ -598,6 +598,21 @@ class ActivationsTest(testing.TestCase):\n         expected = celu(x, True)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_glu(self):\n+        def glu(x, axis=-1):\n+            x1, x2 = np.split(x, 2, axis)\n+            return x1 * (1 / (1 + np.exp(-x2)))\n+\n+        x = np.random.random((2, 4))\n+        result = activations.glu(x[np.newaxis, :])[0]\n+        expected = glu(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        x = np.random.random((2, 4))\n+        result = activations.glu(x[np.newaxis, :], axis=-2)[0]\n+        expected = glu(x, axis=-2)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_elu(self):\n         x = np.random.random((2, 5))\n         result = activations.elu(x[np.newaxis, :])[0]\n\n@@ -90,6 +90,11 @@ def celu(x, alpha=1.0):\n     return jnn.celu(x, alpha=alpha)\n \n \n+def glu(x, axis=-1):\n+    x = convert_to_tensor(x)\n+    return jnn.glu(x, axis=axis)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n\n@@ -121,6 +121,17 @@ def celu(x, alpha=1.0):\n     )\n \n \n+def glu(x, axis=-1):\n+    x = convert_to_tensor(x)\n+    if x.shape[axis] % 2 != 0:\n+        raise ValueError(\n+            \"axis size must be divisible by 2. \"\n+            f\"Received: x.shape={x.shape} with axis={axis}\"\n+        )\n+    x1, x2 = np.split(x, 2, axis)\n+    return x1 * (1 / (1 + np.exp(-x2)))\n+\n+\n def softmax(x, axis=None):\n     exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n     return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n@@ -82,6 +82,16 @@ def celu(x, alpha=1.0):\n     )\n \n \n+def glu(x, axis=-1):\n+    if x.shape[axis] % 2 != 0:\n+        raise ValueError(\n+            \"axis size must be divisible by 2. \"\n+            f\"Received: x.shape={x.shape} with axis={axis}\"\n+        )\n+    x1, x2 = tf.split(x, num_or_size_splits=2, axis=axis)\n+    return x1 * tf.sigmoid(x2)\n+\n+\n def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n\n@@ -93,6 +93,11 @@ def celu(x, alpha=1.0):\n     return tnn.celu(x, alpha=alpha)\n \n \n+def glu(x, axis=-1):\n+    x = convert_to_tensor(x)\n+    return tnn.glu(x, dim=axis)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     dtype = backend.standardize_dtype(x.dtype)\n\n@@ -538,6 +538,47 @@ def celu(x, alpha=1.0):\n     return backend.nn.celu(x, alpha)\n \n \n+class Glu(Operation):\n+    def __init__(self, axis=-1):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.nn.glu(x, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.glu\", \"keras.ops.nn.glu\"])\n+def glu(x, axis=-1):\n+    \"\"\"Gated Linear Unit (GLU) activation function.\n+\n+    It is defined as:\n+\n+    `f(x) = a * sigmoid(b)`\n+    where `x` is split into `a` and `b` along the given axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: The axis along which to split the input tensor. Defaults to `-1`.\n+\n+    Returns:\n+        A tensor with the same shape as half of the input.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1. , 1.])\n+    >>> x_glu = keras.ops.glu(x)\n+    >>> print(x_glu)\n+    array([-0.73105858, 0. ], shape=(2,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Glu(axis).symbolic_call(x)\n+    return backend.nn.glu(x, axis=axis)\n+\n+\n class Softmax(Operation):\n     def __init__(self, axis=-1):\n         super().__init__()\n\n@@ -145,6 +145,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.celu(x).shape, (None, 2, 3))\n \n+    def test_glu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.glu(x).shape, (None, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n@@ -794,6 +798,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.celu(x).shape, (1, 2, 3))\n \n+    def test_glu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.glu(x).shape, (1, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n@@ -1307,6 +1315,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [-0.63212055, 0.0, 1.0, 2.0, 3.0],\n         )\n \n+    def test_glu(self):\n+        x = np.array([-1, 0, 1, 2, 3, 4], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.glu(x),\n+            [-0.8807971, 0.0, 0.98201376],\n+        )\n+\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n@@ -2396,6 +2411,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_glu(self, dtype):\n+        import jax.nn as jnn\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((2), dtype=dtype)\n+        x_jax = jnp.ones((2), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnn.glu(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.glu(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.Glu().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_hard_sigmoid(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4bb8e3b75fcf42e0b324bd62878a90022aa68d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 11 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class ReLU(Layer):\n \n     Example:\n     ``` python\n-    relu_layer = keras.layers.activations.ReLU(\n+    relu_layer = keras.layers.ReLU(\n         max_value=10,\n         negative_slope=0.5,\n         threshold=0,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56eaab34aa45fa9a5d80a9de900492670b5c5120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 777 | Lines Deleted: 0 | Files Changed: 12 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 77/22 | Churn Δ: 777 | Churn Cumulative: 1599 | Contributors (this commit): 15 | Commits (past 90d): 14 | Contributors (cumulative): 34 | DMM Complexity: 0.040998217468805706\n\nDIFF:\n@@ -50,6 +50,7 @@ from keras.api import saving\n from keras.api import tree\n from keras.api import utils\n from keras.api import version\n+from keras.api import visualization\n \n # END DO NOT EDIT.\n \n\n@@ -31,6 +31,7 @@ from keras.api import regularizers\n from keras.api import saving\n from keras.api import tree\n from keras.api import utils\n+from keras.api import visualization\n from keras.src.backend import Variable\n from keras.src.backend import device\n from keras.src.backend import name_scope\n\n@@ -24,6 +24,7 @@ from keras.api import random\n from keras.api import regularizers\n from keras.api import tree\n from keras.api import utils\n+from keras.api import visualization\n from keras.api._tf_keras.keras import backend\n from keras.api._tf_keras.keras import layers\n from keras.api._tf_keras.keras import losses\n\n@@ -0,0 +1,17 @@\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+from keras.src.visualization.draw_bounding_boxes import draw_bounding_boxes\n+from keras.src.visualization.draw_segmentation_masks import (\n+    draw_segmentation_masks,\n+)\n+from keras.src.visualization.plot_bounding_box_gallery import (\n+    plot_bounding_box_gallery,\n+)\n+from keras.src.visualization.plot_image_gallery import plot_image_gallery\n+from keras.src.visualization.plot_segmentation_mask_gallery import (\n+    plot_segmentation_mask_gallery,\n+)\n\n@@ -0,0 +1,17 @@\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+from keras.src.visualization.draw_bounding_boxes import draw_bounding_boxes\n+from keras.src.visualization.draw_segmentation_masks import (\n+    draw_segmentation_masks,\n+)\n+from keras.src.visualization.plot_bounding_box_gallery import (\n+    plot_bounding_box_gallery,\n+)\n+from keras.src.visualization.plot_image_gallery import plot_image_gallery\n+from keras.src.visualization.plot_segmentation_mask_gallery import (\n+    plot_segmentation_mask_gallery,\n+)\n\n@@ -10,6 +10,7 @@ from keras.src import ops\n from keras.src import optimizers\n from keras.src import regularizers\n from keras.src import utils\n+from keras.src import visualization\n from keras.src.backend import KerasTensor\n from keras.src.layers import Input\n from keras.src.layers import Layer\n\n@@ -0,0 +1,2 @@\n+from keras.src.visualization import draw_bounding_boxes\n+from keras.src.visualization import plot_image_gallery\n\n@@ -0,0 +1,177 @@\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n+\n+try:\n+    import cv2\n+except ImportError:\n+    cv2 = None\n+\n+\n+@keras_export(\"keras.visualization.draw_bounding_boxes\")\n+def draw_bounding_boxes(\n+    images,\n+    bounding_boxes,\n+    bounding_box_format,\n+    class_mapping=None,\n+    color=(128, 128, 128),\n+    line_thickness=2,\n+    text_thickness=1,\n+    font_scale=1.0,\n+    data_format=None,\n+):\n+    \"\"\"Draws bounding boxes on images.\n+\n+    This function draws bounding boxes on a batch of images.  It supports\n+    different bounding box formats and can optionally display class labels\n+    and confidences.\n+\n+    Args:\n+        images: A batch of images as a 4D tensor or NumPy array. Shape should be\n+            `(batch_size, height, width, channels)`.\n+        bounding_boxes: A dictionary containing bounding box data.  Should have\n+            the following keys:\n+            - `boxes`: A tensor or array of shape `(batch_size, num_boxes, 4)`\n+               containing the bounding box coordinates in the specified format.\n+            - `labels`: A tensor or array of shape `(batch_size, num_boxes)`\n+              containing the class labels for each bounding box.\n+            - `confidences` (Optional): A tensor or array of shape\n+               `(batch_size, num_boxes)` containing the confidence scores for\n+               each bounding box.\n+        bounding_box_format: A string specifying the format of the bounding\n+            boxes. Refer [keras-io](TODO)\n+        class_mapping: A dictionary mapping class IDs (integers) to class labels\n+            (strings).  Used to display class labels next to the bounding boxes.\n+            Defaults to None (no labels displayed).\n+        color: A tuple or list representing the RGB color of the bounding boxes.\n+            For example, `(255, 0, 0)` for red. Defaults to `(128, 128, 128)`.\n+        line_thickness: An integer specifying the thickness of the bounding box\n+            lines. Defaults to `2`.\n+        text_thickness: An integer specifying the thickness of the text labels.\n+            Defaults to `1`.\n+        font_scale: A float specifying the scale of the font used for text\n+            labels. Defaults to `1.0`.\n+        data_format: A string, either `\"channels_last\"` or `\"channels_first\"`,\n+            specifying the order of dimensions in the input images. Defaults to\n+            the `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            \"channels_last\".\n+\n+    Returns:\n+        A NumPy array of the annotated images with the bounding boxes drawn.\n+        The array will have the same shape as the input `images`.\n+\n+    Raises:\n+        ValueError: If `images` is not a 4D tensor/array, if `bounding_boxes` is\n+            not a dictionary, or if `bounding_boxes` does not contain `\"boxes\"`\n+            and `\"labels\"` keys.\n+        TypeError: If `bounding_boxes` is not a dictionary.\n+        ImportError: If `cv2` (OpenCV) is not installed.\n+    \"\"\"\n+\n+    if cv2 is None:\n+        raise ImportError(\n+            \"The `draw_bounding_boxes` function requires the `cv2` package \"\n+            \" (OpenCV). Please install it with `pip install opencv-python`.\"\n+        )\n+\n+    class_mapping = class_mapping or {}\n+    text_thickness = (\n+        text_thickness or line_thickness\n+    )  # Default text_thickness if not provided.\n+    data_format = data_format or backend.image_data_format()\n+    images_shape = ops.shape(images)\n+    if len(images_shape) != 4:\n+        raise ValueError(\n+            \"`images` must be batched 4D tensor. \"\n+            f\"Received: images.shape={images_shape}\"\n+        )\n+    if not isinstance(bounding_boxes, dict):\n+        raise TypeError(\n+            \"`bounding_boxes` should be a dict. \"\n+            f\"Received: bounding_boxes={bounding_boxes} of type \"\n+            f\"{type(bounding_boxes)}\"\n+        )\n+    if \"boxes\" not in bounding_boxes or \"labels\" not in bounding_boxes:\n+        raise ValueError(\n+            \"`bounding_boxes` should be a dict containing 'boxes' and \"\n+            f\"'labels' keys. Received: bounding_boxes={bounding_boxes}\"\n+        )\n+    if data_format == \"channels_last\":\n+        h_axis = -3\n+        w_axis = -2\n+    else:\n+        h_axis = -2\n+        w_axis = -1\n+    height = images_shape[h_axis]\n+    width = images_shape[w_axis]\n+    bounding_boxes = bounding_boxes.copy()\n+    bounding_boxes = convert_format(\n+        bounding_boxes, bounding_box_format, \"xyxy\", height, width\n+    )\n+\n+    # To numpy array\n+    images = ops.convert_to_numpy(images).astype(\"uint8\")\n+    boxes = ops.convert_to_numpy(bounding_boxes[\"boxes\"])\n+    labels = ops.convert_to_numpy(bounding_boxes[\"labels\"])\n+    if \"confidences\" in bounding_boxes:\n+        confidences = ops.convert_to_numpy(bounding_boxes[\"confidences\"])\n+    else:\n+        confidences = None\n+\n+    result = []\n+    batch_size = images.shape[0]\n+    for i in range(batch_size):\n+        _image = images[i]\n+        _box = boxes[i]\n+        _class = labels[i]\n+        for box_i in range(_box.shape[0]):\n+            x1, y1, x2, y2 = _box[box_i].astype(\"int32\")\n+            c = _class[box_i].astype(\"int32\")\n+            if c == -1:\n+                continue\n+            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n+            c = int(c)\n+            # Draw bounding box\n+            cv2.rectangle(_image, (x1, y1), (x2, y2), color, line_thickness)\n+\n+            if c in class_mapping:\n+                label = class_mapping[c]\n+                if confidences is not None:\n+                    conf = confidences[i][box_i]\n+                    label = f\"{label} | {conf:.2f}\"\n+\n+                font_x1, font_y1 = _find_text_location(\n+                    x1, y1, font_scale, text_thickness\n+                )\n+                cv2.putText(\n+                    img=_image,\n+                    text=label,\n+                    org=(font_x1, font_y1),\n+                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n+                    fontScale=font_scale,\n+                    color=color,\n+                    thickness=text_thickness,\n+                )\n+        result.append(_image)\n+    return np.stack(result, axis=0)\n+\n+\n+def _find_text_location(x, y, font_scale, thickness):\n+    font_height = int(font_scale * 12)\n+    target_y = y - 8\n+    if target_y - (2 * font_height) > 0:\n+        return x, y - 8\n+\n+    line_offset = thickness\n+    static_offset = 3\n+\n+    return (\n+        x + static_offset,\n+        y + (2 * font_height) + line_offset + static_offset,\n+    )\n\n@@ -0,0 +1,109 @@\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+\n+\n+@keras_export(\"keras.visualization.draw_segmentation_masks\")\n+def draw_segmentation_masks(\n+    images,\n+    segmentation_masks,\n+    num_classes=None,\n+    color_mapping=None,\n+    alpha=0.8,\n+    blend=True,\n+    ignore_index=-1,\n+    data_format=None,\n+):\n+    \"\"\"Draws segmentation masks on images.\n+\n+    The function overlays segmentation masks on the input images.\n+    The masks are blended with the images using the specified alpha value.\n+\n+    Args:\n+        images: A batch of images as a 4D tensor or NumPy array. Shape\n+            should be (batch_size, height, width, channels).\n+        segmentation_masks: A batch of segmentation masks as a 3D or 4D tensor\n+            or NumPy array.  Shape should be (batch_size, height, width) or\n+            (batch_size, height, width, 1). The values represent class indices\n+            starting from 1 up to `num_classes`. Class 0 is reserved for\n+            the background and will be ignored if `ignore_index` is not 0.\n+        num_classes: The number of segmentation classes. If `None`, it is\n+            inferred from the maximum value in `segmentation_masks`.\n+        color_mapping: A dictionary mapping class indices to RGB colors.\n+            If `None`, a default color palette is generated. The keys should be\n+            integers starting from 1 up to `num_classes`.\n+        alpha: The opacity of the segmentation masks. Must be in the range\n+            `[0, 1]`.\n+        blend: Whether to blend the masks with the input image using the\n+            `alpha` value. If `False`, the masks are drawn directly on the\n+            images without blending. Defaults to `True`.\n+        ignore_index: The class index to ignore. Mask pixels with this value\n+            will not be drawn.  Defaults to -1.\n+        data_format: Image data format, either `\"channels_last\"` or\n+            `\"channels_first\"`. Defaults to the `image_data_format` value found\n+            in your Keras config file at `~/.keras/keras.json`. If you never\n+            set it, then it will be `\"channels_last\"`.\n+\n+    Returns:\n+        A NumPy array of the images with the segmentation masks overlaid.\n+\n+    Raises:\n+        ValueError: If the input `images` is not a 4D tensor or NumPy array.\n+        TypeError: If the input `segmentation_masks` is not an integer type.\n+    \"\"\"\n+    data_format = data_format or backend.image_data_format()\n+    images_shape = ops.shape(images)\n+    if len(images_shape) != 4:\n+        raise ValueError(\n+            \"`images` must be batched 4D tensor. \"\n+            f\"Received: images.shape={images_shape}\"\n+        )\n+    if data_format == \"channels_first\":\n+        images = ops.transpose(images, (0, 2, 3, 1))\n+        segmentation_masks = ops.transpose(segmentation_masks, (0, 2, 3, 1))\n+    images = ops.convert_to_tensor(images, dtype=\"float32\")\n+    segmentation_masks = ops.convert_to_tensor(segmentation_masks)\n+\n+    if not backend.is_int_dtype(segmentation_masks.dtype):\n+        dtype = backend.standardize_dtype(segmentation_masks.dtype)\n+        raise TypeError(\n+            \"`segmentation_masks` must be in integer dtype. \"\n+            f\"Received: segmentation_masks.dtype={dtype}\"\n+        )\n+\n+    # Infer num_classes\n+    if num_classes is None:\n+        num_classes = int(ops.convert_to_numpy(ops.max(segmentation_masks)))\n+    if color_mapping is None:\n+        colors = _generate_color_palette(num_classes)\n+    else:\n+        colors = [color_mapping[i] for i in range(num_classes)]\n+    valid_masks = ops.not_equal(segmentation_masks, ignore_index)\n+    valid_masks = ops.squeeze(valid_masks, axis=-1)\n+    segmentation_masks = ops.one_hot(segmentation_masks, num_classes)\n+    segmentation_masks = segmentation_masks[..., 0, :]\n+    segmentation_masks = ops.convert_to_numpy(segmentation_masks)\n+\n+    # Replace class with color\n+    masks = segmentation_masks\n+    masks = np.transpose(masks, axes=(3, 0, 1, 2)).astype(\"bool\")\n+    images_to_draw = ops.convert_to_numpy(images).copy()\n+    for mask, color in zip(masks, colors):\n+        color = np.array(color, dtype=images_to_draw.dtype)\n+        images_to_draw[mask, ...] = color[None, :]\n+    images_to_draw = ops.convert_to_tensor(images_to_draw)\n+    outputs = ops.cast(images_to_draw, dtype=\"float32\")\n+\n+    if blend:\n+        outputs = images * (1 - alpha) + outputs * alpha\n+        outputs = ops.where(valid_masks[..., None], outputs, images)\n+        outputs = ops.cast(outputs, dtype=\"uint8\")\n+        outputs = ops.convert_to_numpy(outputs)\n+    return outputs\n+\n+\n+def _generate_color_palette(num_classes: int):\n+    palette = np.array([2**25 - 1, 2**15 - 1, 2**21 - 1])\n+    return [((i * palette) % 255).tolist() for i in range(num_classes)]\n\n@@ -0,0 +1,165 @@\n+import functools\n+\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.visualization.draw_bounding_boxes import draw_bounding_boxes\n+from keras.src.visualization.plot_image_gallery import plot_image_gallery\n+\n+try:\n+    from matplotlib import patches  # For legend patches\n+except ImportError:\n+    patches = None\n+\n+\n+@keras_export(\"keras.visualization.plot_bounding_box_gallery\")\n+def plot_bounding_box_gallery(\n+    images,\n+    bounding_box_format,\n+    y_true=None,\n+    y_pred=None,\n+    value_range=(0, 255),\n+    true_color=(0, 188, 212),\n+    pred_color=(255, 235, 59),\n+    line_thickness=2,\n+    font_scale=1.0,\n+    text_thickness=None,\n+    class_mapping=None,\n+    ground_truth_mapping=None,\n+    prediction_mapping=None,\n+    legend=False,\n+    legend_handles=None,\n+    rows=None,\n+    cols=None,\n+    data_format=None,\n+    **kwargs,\n+):\n+    \"\"\"Plots a gallery of images with bounding boxes.\n+\n+    This function can display both ground truth and predicted bounding boxes on\n+    a set of images.  It supports various bounding box formats and can include\n+    class labels and a legend.\n+\n+    Args:\n+        images: A 4D tensor or NumPy array of images. Shape should be\n+            `(batch_size, height, width, channels)`.\n+        bounding_box_format: The format of the bounding boxes.\n+            Refer [keras-io](TODO)\n+        y_true: A dictionary containing the ground truth bounding boxes and\n+            labels. Should have the same structure as the `bounding_boxes`\n+            argument in `keras.visualization.draw_bounding_boxes`.\n+            Defaults to `None`.\n+        y_pred: A dictionary containing the predicted bounding boxes and labels.\n+            Should have the same structure as `y_true`. Defaults to `None`.\n+        value_range: A tuple specifying the value range of the images\n+            (e.g., `(0, 255)` or `(0, 1)`). Defaults to `(0, 255)`.\n+        true_color: A tuple of three integers representing the RGB color for the\n+            ground truth bounding boxes. Defaults to `(0, 188, 212)`.\n+        pred_color: A tuple of three integers representing the RGB color for the\n+            predicted bounding boxes. Defaults to `(255, 235, 59)`.\n+        line_thickness: The thickness of the bounding box lines. Defaults to 2.\n+        font_scale: The scale of the font used for labels. Defaults to 1.0.\n+        text_thickness: The thickness of the bounding box text. Defaults to\n+            `line_thickness`.\n+        class_mapping: A dictionary mapping class IDs to class names.  Used f\n+            or both ground truth and predicted boxes if `ground_truth_mapping`\n+            and `prediction_mapping` are not provided. Defaults to `None`.\n+        ground_truth_mapping:  A dictionary mapping class IDs to class names\n+            specifically for ground truth boxes. Overrides `class_mapping`\n+            for ground truth. Defaults to `None`.\n+        prediction_mapping: A dictionary mapping class IDs to class names\n+            specifically for predicted boxes. Overrides `class_mapping` for\n+            predictions. Defaults to `None`.\n+        legend: A boolean indicating whether to show a legend.\n+            Defaults to `False`.\n+        legend_handles: A list of matplotlib `Patch` objects to use for the\n+            legend. If this is provided, the `legend` argument will be ignored.\n+            Defaults to `None`.\n+        rows: The number of rows in the image gallery. Required if the images\n+            are not batched. Defaults to `None`.\n+        cols: The number of columns in the image gallery. Required if the images\n+            are not batched. Defaults to `None`.\n+        data_format: The image data format `\"channels_last\"` or\n+            `\"channels_first\"`. Defaults to the Keras backend data format.\n+        kwargs: Additional keyword arguments to be passed to\n+            `keras.visualization.plot_image_gallery`.\n+\n+    Returns:\n+       The output of `keras.visualization.plot_image_gallery`.\n+\n+    Raises:\n+        ValueError: If `images` is not a 4D tensor/array or if both `legend` a\n+        nd `legend_handles` are specified.\n+        ImportError: if matplotlib is not installed\n+    \"\"\"\n+    if patches is None:\n+        raise ImportError(\n+            \"The `plot_bounding_box_gallery` function requires the \"\n+            \" `matplotlib` package. Please install it with \"\n+            \" `pip install matplotlib`.\"\n+        )\n+\n+    prediction_mapping = prediction_mapping or class_mapping\n+    ground_truth_mapping = ground_truth_mapping or class_mapping\n+    data_format = data_format or backend.image_data_format()\n+    images_shape = ops.shape(images)\n+    if len(images_shape) != 4:\n+        raise ValueError(\n+            \"`images` must be batched 4D tensor. \"\n+            f\"Received: images.shape={images_shape}\"\n+        )\n+    if data_format == \"channels_first\":  # Ensure correct data format\n+        images = ops.transpose(images, (0, 2, 3, 1))\n+    plotted_images = ops.convert_to_numpy(images)\n+\n+    draw_fn = functools.partial(\n+        draw_bounding_boxes,\n+        bounding_box_format=bounding_box_format,\n+        line_thickness=line_thickness,\n+        text_thickness=text_thickness,\n+        font_scale=font_scale,\n+    )\n+\n+    if y_true is not None:\n+        plotted_images = draw_fn(\n+            plotted_images,\n+            y_true,\n+            color=true_color,\n+            class_mapping=ground_truth_mapping,\n+        )\n+\n+    if y_pred is not None:\n+        plotted_images = draw_fn(\n+            plotted_images,\n+            y_pred,\n+            color=pred_color,\n+            class_mapping=prediction_mapping,\n+        )\n+\n+    if legend:\n+        if legend_handles:\n+            raise ValueError(\n+                \"Only pass `legend` OR `legend_handles` to \"\n+                \"`keras.visualization.plot_bounding_box_gallery()`.\"\n+            )\n+        legend_handles = [\n+            patches.Patch(\n+                color=np.array(true_color) / 255.0,  # Normalize color\n+                label=\"Ground Truth\",\n+            ),\n+            patches.Patch(\n+                color=np.array(pred_color) / 255.0,  # Normalize color\n+                label=\"Prediction\",\n+            ),\n+        ]\n+\n+    return plot_image_gallery(\n+        plotted_images,\n+        value_range=value_range,\n+        legend_handles=legend_handles,\n+        rows=rows,\n+        cols=cols,\n+        **kwargs,\n+    )\n\n@@ -0,0 +1,165 @@\n+import math\n+\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+\n+try:\n+    import matplotlib.pyplot as plt\n+except ImportError:\n+    plt = None\n+\n+\n+def _extract_image_batch(images, num_images, batch_size):\n+    \"\"\"Extracts a batch of images for plotting.\n+\n+    Args:\n+        images: The 4D tensor or NumPy array of images.\n+        num_images: The number of images to extract.\n+        batch_size: The original batch size of the images.\n+\n+    Returns:\n+        A 4D tensor or NumPy array containing the extracted images.\n+\n+    Raises:\n+        ValueError: If `images` is not a 4D tensor/array.\n+    \"\"\"\n+\n+    if len(ops.shape(images)) != 4:\n+        raise ValueError(\n+            \"`plot_images_gallery()` requires you to \"\n+            \"batch your `np.array` samples together.\"\n+        )\n+    num_samples = min(num_images, batch_size)\n+    sample = images[:num_samples, ...]\n+\n+    return sample\n+\n+\n+@keras_export(\"keras.visualization.plot_image_gallery\")\n+def plot_image_gallery(\n+    images,\n+    rows=None,\n+    cols=None,\n+    value_range=(0, 255),\n+    scale=2,\n+    path=None,\n+    show=None,\n+    transparent=True,\n+    dpi=60,\n+    legend_handles=None,\n+    data_format=None,\n+):\n+    \"\"\"Displays a gallery of images.\n+\n+    Args:\n+        images: A 4D tensor or NumPy array of images. Shape should be\n+           `(batch_size, height, width, channels)`.\n+        value_range: A tuple specifying the value range of the images\n+            (e.g., `(0, 255)` or `(0, 1)`). Defaults to `(0, 255)`.\n+        rows: The number of rows in the gallery. If `None`, it's calculated\n+            based on the number of images and `cols`. Defaults to `None`.\n+        cols: The number of columns in the gallery. If `None`, it's calculated\n+            based on the number of images and `rows`. Defaults to `None`.\n+        scale: A float controlling the size of the displayed images. The images\n+            are scaled by this factor. Defaults to `2`.\n+        path: The path to save the generated gallery image. If `None`, the\n+            image is displayed using `plt.show()`. Defaults to `None`.\n+        show: Whether to display the image using `plt.show()`. If `True`, the\n+            image is displayed. If `False`, the image is not displayed.\n+            Ignored if `path` is not `None`. Defaults to `True` if `path`\n+            is `None`, `False` otherwise.\n+        transparent:  A boolean, whether to save the figure with a transparent\n+            background. Defaults to `True`.\n+        dpi: The DPI (dots per inch) for saving the figure. Defaults to 60.\n+        legend_handles: A list of matplotlib `Patch` objects to use as legend\n+            handles. Defaults to `None`.\n+        data_format: The image data format `\"channels_last\"` or\n+            `\"channels_first\"`. Defaults to the Keras backend data format.\n+\n+    Raises:\n+        ValueError: If both `path` and `show` are set to non-`None` values or if\n+            `images` is not a 4D tensor or array.\n+        ImportError: if matplotlib is not installed.\n+    \"\"\"\n+    if plt is None:\n+        raise ImportError(\n+            \"The `plot_image_gallery` function requires the `matplotlib` \"\n+            \"package. Please install it with `pip install matplotlib`.\"\n+        )\n+\n+    if path is not None and show:\n+        raise ValueError(\n+            \"plot_gallery() expects either `path` to be set, or `show` \"\n+            \"to be true.\"\n+        )\n+\n+    show = show if show is not None else (path is None)\n+    data_format = data_format or backend.image_data_format()\n+\n+    batch_size = ops.shape(images)[0] if len(ops.shape(images)) == 4 else 1\n+\n+    rows = rows or int(math.ceil(math.sqrt(batch_size)))\n+    cols = cols or int(math.ceil(batch_size // rows))\n+    num_images = rows * cols\n+\n+    images = _extract_image_batch(images, num_images, batch_size)\n+    if (\n+        data_format == \"channels_first\"\n+    ):  # Ensure correct data format for plotting\n+        images = ops.transpose(images, (0, 2, 3, 1))\n+    # Generate subplots\n+    fig, axes = plt.subplots(\n+        nrows=rows,\n+        ncols=cols,\n+        figsize=(cols * scale, rows * scale),\n+        frameon=False,\n+        layout=\"tight\",\n+        squeeze=True,\n+        sharex=\"row\",\n+        sharey=\"col\",\n+    )\n+    fig.subplots_adjust(wspace=0, hspace=0)\n+\n+    if isinstance(axes, np.ndarray) and len(axes.shape) == 1:\n+        expand_axis = 0 if rows == 1 else -1\n+        axes = np.expand_dims(axes, expand_axis)\n+\n+    if legend_handles is not None:\n+        fig.legend(handles=legend_handles, loc=\"lower center\")\n+\n+    images = BaseImagePreprocessingLayer()._transform_value_range(\n+        images=images, original_range=value_range, target_range=(0, 255)\n+    )\n+\n+    images = ops.convert_to_numpy(images)\n+    if data_format == \"channels_first\":\n+        images = images.transpose(0, 2, 3, 1)\n+\n+    for row in range(rows):\n+        for col in range(cols):\n+            index = row * cols + col\n+            current_axis = (\n+                axes[row, col] if isinstance(axes, np.ndarray) else axes\n+            )\n+            current_axis.imshow(images[index].astype(\"uint8\"))\n+            current_axis.margins(x=0, y=0)\n+            current_axis.axis(\"off\")\n+\n+    if path is not None:\n+        plt.savefig(\n+            fname=path,\n+            pad_inches=0,\n+            bbox_inches=\"tight\",\n+            transparent=transparent,\n+            dpi=dpi,\n+        )\n+        plt.close()\n+    elif show:\n+        plt.show()\n+        plt.close()\n\n@@ -0,0 +1,121 @@\n+import functools\n+\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.visualization.draw_segmentation_masks import (\n+    draw_segmentation_masks,\n+)\n+from keras.src.visualization.plot_image_gallery import plot_image_gallery\n+\n+\n+@keras_export(\"keras.visualization.plot_segmentation_mask_gallery\")\n+def plot_segmentation_mask_gallery(\n+    images,\n+    num_classes,\n+    value_range=(0, 255),\n+    y_true=None,\n+    y_pred=None,\n+    color_mapping=None,\n+    blend=True,\n+    alpha=0.8,\n+    ignore_index=-1,\n+    data_format=None,\n+    **kwargs,\n+):\n+    \"\"\"Plots a gallery of images with corresponding segmentation masks.\n+\n+    Args:\n+        images: A 4D tensor or NumPy array of images. Shape should be\n+            `(batch_size, height, width, channels)`.\n+        num_classes: The number of segmentation classes.  Class indices should\n+            start from `1`.  Class `0` will be treated as background and\n+            ignored if `ignore_index` is not 0.\n+        value_range: A tuple specifying the value range of the images\n+            (e.g., `(0, 255)` or `(0, 1)`). Defaults to `(0, 255)`.\n+        y_true: A 3D/4D tensor or NumPy array representing the ground truth\n+            segmentation masks. Shape should be `(batch_size, height, width)` or\n+            `(batch_size, height, width, 1)`. Defaults to `None`.\n+        y_pred: A 3D/4D tensor or NumPy array representing the predicted\n+            segmentation masks.  Shape should be the same as `y_true`.\n+            Defaults to `None`.\n+        color_mapping: A dictionary mapping class indices to RGB colors.\n+            If `None`, a default color palette is used. Class indices start\n+            from `1`. Defaults to `None`.\n+        blend: Whether to blend the masks with the input image using the\n+            `alpha` value. If `False`, the masks are drawn directly on the\n+            images without blending. Defaults to `True`.\n+        alpha: The opacity of the segmentation masks (a float between 0 and 1).\n+            Defaults to `0.8`.\n+        ignore_index: The class index to ignore when drawing masks.\n+            Defaults to `-1`.\n+        data_format: The image data format `\"channels_last\"` or\n+            `\"channels_first\"`. Defaults to the Keras backend data format.\n+        kwargs: Additional keyword arguments to be passed to\n+            `keras.visualization.plot_image_gallery`.\n+\n+    Returns:\n+        The output of `keras.visualization.plot_image_gallery`.\n+\n+    Raises:\n+        ValueError: If `images` is not a 4D tensor/array.\n+    \"\"\"\n+    data_format = data_format or backend.image_data_format()\n+    image_shape = ops.shape(images)\n+    if len(image_shape) != 4:\n+        raise ValueError(\n+            \"`images` must be batched 4D tensor. \"\n+            f\"Received: images.shape={image_shape}\"\n+        )\n+    if data_format == \"channels_first\":\n+        images = ops.transpose(images, (0, 2, 3, 1))\n+\n+    batch_size = image_shape[0] if len(image_shape) == 4 else 1\n+\n+    rows = batch_size\n+    cols = 1\n+\n+    if y_true is not None:\n+        cols += 1\n+\n+    if y_pred is not None:\n+        cols += 1\n+\n+    images_np = ops.convert_to_numpy(images)\n+\n+    draw_masks_fn = functools.partial(\n+        draw_segmentation_masks,\n+        num_classes=num_classes,\n+        color_mapping=color_mapping,\n+        alpha=alpha,\n+        ignore_index=ignore_index,\n+        blend=blend,\n+    )\n+\n+    if y_true is not None:\n+        if data_format == \"channels_first\":\n+            y_true = ops.transpose(y_true, (0, 2, 3, 1))\n+        y_true = ops.cast(y_true, \"int32\")\n+        true_masks_drawn = draw_masks_fn(images_np, y_true)\n+\n+    if y_pred is not None:\n+        if data_format == \"channels_first\":\n+            y_pred = ops.transpose(y_pred, (0, 2, 3, 1))\n+        y_pred = ops.cast(y_pred, \"int32\")\n+        predicted_masks_drawn = draw_masks_fn(images_np, y_pred)\n+\n+    images_with_masks = []\n+    for i in range(batch_size):\n+        images_with_masks.append(images_np[i])\n+        if y_true is not None:\n+            images_with_masks.append(true_masks_drawn[i])\n+        if y_pred is not None:\n+            images_with_masks.append(predicted_masks_drawn[i])\n+\n+    gallery_images = np.stack(images_with_masks, axis=0)\n+\n+    return plot_image_gallery(\n+        gallery_images, value_range=value_range, rows=rows, cols=cols, **kwargs\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
