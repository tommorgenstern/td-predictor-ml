{"custom_id": "keras#788c56b9c60120e42f5d6cbd269499b878ba4859", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 42 | Files Changed: 5 | Hunks: 43 | Methods Changed: 46 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 102 | Churn Cumulative: 18729 | Contributors (this commit): 108 | Commits (past 90d): 74 | Contributors (cumulative): 181 | DMM Complexity: 0.631578947368421\n\nDIFF:\n@@ -1,7 +1,7 @@\n from __future__ import print_function\n import cntk as C\n import numpy as np\n-from .common import _FLOATX, _EPSILON, image_dim_ordering, image_data_format\n+from .common import floatx, epsilon, image_dim_ordering, image_data_format\n from collections import defaultdict\n from contextlib import contextmanager\n import warnings\n@@ -120,7 +120,10 @@ def _convert_dtype_string(dtype):\n                          'float64.' % dtype)\n \n \n-def variable(value, dtype=_FLOATX, name=None):\n+def variable(value, dtype=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n+\n     if name is None:\n         name = ''\n \n@@ -217,10 +220,12 @@ def eval(x):\n def placeholder(\n         shape=None,\n         ndim=None,\n-        dtype=_FLOATX,\n+        dtype=None,\n         sparse=False,\n         name=None,\n         dynamic_axis_num=1):\n+    if dtype is None:\n+        dtype = floatx()\n     if not shape:\n         if ndim:\n             shape = tuple([None for _ in range(ndim)])\n@@ -294,7 +299,7 @@ def _prepare_name(name, default):\n \n def constant(value, dtype=None, shape=None, name=None):\n     if dtype is None:\n-        dtype = _FLOATX\n+        dtype = floatx()\n     if shape is None:\n         shape = ()\n     np_value = value * np.ones(shape)\n@@ -341,8 +346,10 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     return random_uniform_variable(shape, minval, maxval, dtype, seed)\n \n \n-def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n-                            name=None, seed=None):\n+def random_uniform_variable(shape, low, high,\n+                            dtype=None, name=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e3)\n@@ -370,9 +377,11 @@ def random_normal_variable(\n         shape,\n         mean,\n         scale,\n-        dtype=_FLOATX,\n+        dtype=None,\n         name=None,\n         seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     if seed is None:\n         # ensure that randomness is conditioned by the Numpy RNG\n         seed = np.random.randint(10e7)\n@@ -393,7 +402,9 @@ def random_normal_variable(\n         name=name)\n \n \n-def random_normal(shape, mean=0.0, stddev=1.0, dtype=_FLOATX, seed=None):\n+def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    if dtype is None:\n+        dtype = floatx()\n     for _ in shape:\n         if _ is None:\n             raise ValueError('CNTK Backend: randomness op with '\n@@ -425,17 +436,23 @@ def dtype(x):\n     return _convert_dtype_string(x.dtype)\n \n \n-def zeros(shape, dtype=_FLOATX, name=None):\n+def zeros(shape, dtype=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n     ctype = _convert_string_dtype(dtype)\n     return variable(value=np.zeros(shape, ctype), dtype=dtype, name=name)\n \n \n-def ones(shape, dtype=_FLOATX, name=None):\n+def ones(shape, dtype=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n     ctype = _convert_string_dtype(dtype)\n     return variable(value=np.ones(shape, ctype), dtype=dtype, name=name)\n \n \n-def eye(size, dtype=_FLOATX, name=None):\n+def eye(size, dtype=None, name=None):\n+    if dtype is None:\n+        dtype = floatx()\n     return variable(np.eye(size), dtype, name)\n \n \n@@ -719,7 +736,7 @@ def all(x, axis=None, keepdims=False):\n         return all_matrix\n \n \n-def classification_error(output, target, axis=-1):\n+def classification_error(target, output, axis=-1):\n     return C.ops.reduce_mean(\n         C.equal(\n             argmax(\n@@ -791,10 +808,10 @@ def clip(x, min_value, max_value):\n     return C.clip(x, min_value, max_value)\n \n \n-def binary_crossentropy(output, target, from_logits=False):\n+def binary_crossentropy(target, output, from_logits=False):\n     if from_logits:\n         output = C.sigmoid(output)\n-    output = C.clip(output, _EPSILON, 1.0 - _EPSILON)\n+    output = C.clip(output, epsilon(), 1.0 - epsilon())\n     output = -target * C.log(output) - (1.0 - target) * C.log(1.0 - output)\n     return output\n \n@@ -1533,7 +1550,7 @@ def softsign(x):\n     return x / (1 + C.abs(x))\n \n \n-def categorical_crossentropy(output, target, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False):\n     if from_logits:\n         result = C.cross_entropy_with_softmax(output, target)\n         # cntk's result shape is (batch, 1), while keras expect (batch, )\n@@ -1541,18 +1558,19 @@ def categorical_crossentropy(output, target, from_logits=False):\n     else:\n         # scale preds so that the class probas of each sample sum to 1\n         output /= C.reduce_sum(output, axis=-1)\n-        # avoid numerical instability with _EPSILON clipping\n-        output = C.clip(output, _EPSILON, 1.0 - _EPSILON)\n+        # avoid numerical instability with epsilon clipping\n+        output = C.clip(output, epsilon(), 1.0 - epsilon())\n         return -sum(target * C.log(output), axis=-1)\n \n \n-def sparse_categorical_crossentropy(output, target, from_logits=False):\n+def sparse_categorical_crossentropy(target, output, from_logits=False):\n     target = C.one_hot(target, output.shape[-1])\n     target = C.reshape(target, output.shape)\n     return categorical_crossentropy(output, target, from_logits)\n \n \n class Function(object):\n+\n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n         self.placeholders = inputs\n         self.trainer = None\n@@ -1589,7 +1607,8 @@ class Function(object):\n                     p_list.append(grad_parameter_dict[g])\n                     u_list.append(g)\n                 else:\n-                    raise ValueError('CNTK backend: when constructing trainer, '\n+                    raise ValueError(\n+                        'CNTK backend: when constructing trainer, '\n                         'found gradient node `%s` which is not '\n                         'related to any parameters in the model. '\n                         'Please double check how the gradient node '\n@@ -1644,6 +1663,7 @@ class Function(object):\n                value.dtype != np.float32 and\n                value.dtype != np.float64):\n                 value = value.astype(np.float32)\n+\n             if tensor == _LEARNING_PHASE:\n                 _LEARNING_PHASE.value = np.asarray(value)\n             else:\n@@ -1664,7 +1684,8 @@ class Function(object):\n                 if argument in feed_dict:\n                     input_dict[argument] = feed_dict[argument]\n                 else:\n-                    raise ValueError('CNTK backend: argument %s is not found in inputs. '\n+                    raise ValueError(\n+                        'CNTK backend: argument %s is not found in inputs. '\n                         'Please double check the model and inputs in '\n                         '`train_function`.' % argument.name)\n \n@@ -1714,7 +1735,8 @@ class Function(object):\n                 if argument in feed_dict:\n                     input_dict[argument] = feed_dict[argument]\n                 else:\n-                    raise ValueError('CNTK backend: assign ops argument %s '\n+                    raise ValueError(\n+                        'CNTK backend: assign ops argument %s '\n                         'is not found in inputs. Please double '\n                         'check the model and inputs.' % argument.name)\n             self.unrelated_updates.eval(input_dict, as_numpy=False)\n\n@@ -2688,14 +2688,14 @@ def softsign(x):\n     return tf.nn.softsign(x)\n \n \n-def categorical_crossentropy(output, target, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False):\n     \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n \n     # Arguments\n+        target: A tensor of the same shape as `output`.\n         output: A tensor resulting from a softmax\n             (unless `from_logits` is True, in which\n             case `output` is expected to be the logits).\n-        target: A tensor of the same shape as `output`.\n         from_logits: Boolean, whether `output` is the\n             result of a softmax, or is a tensor of logits.\n \n@@ -2719,14 +2719,14 @@ def categorical_crossentropy(output, target, from_logits=False):\n                                                        logits=output)\n \n \n-def sparse_categorical_crossentropy(output, target, from_logits=False):\n+def sparse_categorical_crossentropy(target, output, from_logits=False):\n     \"\"\"Categorical crossentropy with integer targets.\n \n     # Arguments\n+        target: An integer tensor.\n         output: A tensor resulting from a softmax\n             (unless `from_logits` is True, in which\n             case `output` is expected to be the logits).\n-        target: An integer tensor.\n         from_logits: Boolean, whether `output` is the\n             result of a softmax, or is a tensor of logits.\n \n@@ -2753,12 +2753,12 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n         return res\n \n \n-def binary_crossentropy(output, target, from_logits=False):\n+def binary_crossentropy(target, output, from_logits=False):\n     \"\"\"Binary crossentropy between an output tensor and a target tensor.\n \n     # Arguments\n-        output: A tensor.\n         target: A tensor with the same shape as `output`.\n+        output: A tensor.\n         from_logits: Whether `output` is expected to be a logits tensor.\n             By default, we consider that `output`\n             encodes a probability distribution.\n\n@@ -1,7 +1,6 @@\n from collections import defaultdict\n from contextlib import contextmanager\n import theano\n-from theano import ifelse\n from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from theano.tensor.signal import pool\n@@ -1512,7 +1511,7 @@ def softsign(x):\n     return T_softsign(x)\n \n \n-def categorical_crossentropy(output, target, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False):\n     if from_logits:\n         output = T.nnet.softmax(output)\n     else:\n@@ -1523,14 +1522,14 @@ def categorical_crossentropy(output, target, from_logits=False):\n     return T.nnet.categorical_crossentropy(output, target)\n \n \n-def sparse_categorical_crossentropy(output, target, from_logits=False):\n+def sparse_categorical_crossentropy(target, output, from_logits=False):\n     target = T.cast(T.flatten(target), 'int32')\n     target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n     target = reshape(target, shape(output))\n     return categorical_crossentropy(output, target, from_logits)\n \n \n-def binary_crossentropy(output, target, from_logits=False):\n+def binary_crossentropy(target, output, from_logits=False):\n     if from_logits:\n         output = T.nnet.sigmoid(output)\n     # avoid numerical instability with _EPSILON clipping\n@@ -2365,9 +2364,8 @@ def foldl(fn, elems, initializer=None, name=None):\n \n     # We need to change the order of the arguments because theano accepts x as\n     # first parameter and accumulator as second\n-    fn2 = lambda x, acc: fn(acc, x)\n-\n-    return theano.foldl(fn2, elems, initializer, name=name)[0]\n+    return theano.foldl(lambda x, acc: fn(acc, x),\n+                        elems, initializer, name=name)[0]\n \n \n def foldr(fn, elems, initializer=None, name=None):\n@@ -2389,9 +2387,8 @@ def foldr(fn, elems, initializer=None, name=None):\n \n     # We need to change the order of the arguments because theano accepts x as\n     # first parameter and accumulator as second\n-    fn2 = lambda x, acc: fn(acc, x)\n-\n-    return theano.foldr(fn2, elems, initializer, name=name)[0]\n+    return theano.foldr(lambda x, acc: fn(acc, x),\n+                        elems, initializer, name=name)[0]\n \n \n def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n@@ -2459,5 +2456,4 @@ def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format\n         output = reshape(output,\n                          (output_row, output_col, -1, filters))\n         output = permute_dimensions(output, (2, 0, 1, 3))\n-\n     return output\n\n@@ -46,15 +46,15 @@ def logcosh(y_true, y_pred):\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    return K.categorical_crossentropy(y_pred, y_true)\n+    return K.categorical_crossentropy(y_true, y_pred)\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    return K.sparse_categorical_crossentropy(y_pred, y_true)\n+    return K.sparse_categorical_crossentropy(y_true, y_pred)\n \n \n def binary_crossentropy(y_true, y_pred):\n-    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n+    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n \n \n def kullback_leibler_divergence(y_true, y_pred):\n\n@@ -756,7 +756,7 @@ class TestBackend(object):\n                            [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n         yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016],\n                            [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n-        check_two_tensor_operation('categorical_crossentropy', xval, yval,\n+        check_two_tensor_operation('categorical_crossentropy', yval, xval,\n                                    BACKENDS, cntk_two_dynamicity=True, from_logits=True)\n         check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), BACKENDS, from_logits=False)\n         check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), BACKENDS, from_logits=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e88f9034eb031f7eff58b9288b060ff748d49a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 18 | Churn Cumulative: 5124 | Contributors (this commit): 15 | Commits (past 90d): 26 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -255,6 +255,12 @@ def placeholder(\n \n \n def is_keras_tensor(x):\n+    if not isinstance(x, (C.variables.Constant,\n+                          C.variables.Variable,\n+                          C.variables.Parameter,\n+                          C.ops.functions.Function)):\n+        raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '\n+                         'Expected a symbolic tensor instance.')\n     return hasattr(x, '_keras_history')\n \n \n\n@@ -150,19 +150,15 @@ def check_composed_tensor_operations(first_function_name, first_function_args,\n class TestBackend(object):\n \n     def test_is_keras_tensor(self):\n-        for k in [KTH, KTF]:\n+        for k in BACKENDS:\n             np_var = np.array([1, 2])\n-            try:\n+            with pytest.raises(ValueError):\n                 k.is_keras_tensor(np_var)\n-                assert True is False\n-            except ValueError:\n-                # This is the expected behavior\n-                continue\n \n             keras_var = k.variable(np_var)\n-            assert k.is_keras_tensor(keras_var) is True\n+            assert k.is_keras_tensor(keras_var) is False\n             keras_placeholder = k.placeholder(shape=(2, 4, 5))\n-            assert k.is_keras_tensor(keras_placeholder) is True\n+            assert k.is_keras_tensor(keras_placeholder) is False\n \n     def test_set_learning_phase(self):\n         # not supported learning_phase\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01e2148732e4083b4850345e5ce4dd499cb5999e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 14118 | Contributors (this commit): 109 | Commits (past 90d): 52 | Contributors (cumulative): 173 | DMM Complexity: None\n\nDIFF:\n@@ -7,8 +7,7 @@ _IMAGE_DATA_FORMAT = 'channels_last'\n \n \n def epsilon():\n-    \"\"\"Returns the value of the fuzz\n-    factor used in numeric expressions.\n+    \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n \n     # Returns\n         A float.\n@@ -23,8 +22,7 @@ def epsilon():\n \n \n def set_epsilon(e):\n-    \"\"\"Sets the value of the fuzz\n-    factor used in numeric expressions.\n+    \"\"\"Sets the value of the fuzz factor used in numeric expressions.\n \n     # Arguments\n         e: float. New value of epsilon.\n\n@@ -11,8 +11,7 @@ from collections import defaultdict\n import numpy as np\n import os\n \n-from .common import floatx\n-from .common import _EPSILON\n+from .common import floatx, epsilon\n from .common import image_data_format\n from ..utils.generic_utils import has_arg\n \n@@ -2729,8 +2728,8 @@ def categorical_crossentropy(target, output, from_logits=False):\n                                 axis=len(output.get_shape()) - 1,\n                                 keep_dims=True)\n         # manual computation of crossentropy\n-        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n-        output = tf.clip_by_value(output, epsilon, 1. - epsilon)\n+        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n         return - tf.reduce_sum(target * tf.log(output),\n                                axis=len(output.get_shape()) - 1)\n     else:\n@@ -2755,8 +2754,8 @@ def sparse_categorical_crossentropy(target, output, from_logits=False):\n     # Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n-        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n-        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n+        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n         output = tf.log(output)\n \n     output_shape = output.get_shape()\n@@ -2789,8 +2788,8 @@ def binary_crossentropy(target, output, from_logits=False):\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n         # transform back to logits\n-        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n-        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n+        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n+        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n         output = tf.log(output / (1 - output))\n \n     return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n\n@@ -15,7 +15,7 @@ except ImportError:\n     from theano.sandbox.softsign import softsign as T_softsign\n \n import numpy as np\n-from .common import _FLOATX, floatx, _EPSILON, image_data_format\n+from .common import floatx, epsilon, image_data_format\n from ..utils.generic_utils import has_arg\n # Legacy functions\n from .common import set_image_dim_ordering, image_dim_ordering\n@@ -25,7 +25,7 @@ py_sum = sum\n \n \n # INTERNAL UTILS\n-theano.config.floatX = _FLOATX\n+theano.config.floatX = floatx()\n _LEARNING_PHASE = T.scalar(dtype='uint8', name='keras_learning_phase')  # 0 = test, 1 = train\n _UID_PREFIXES = defaultdict(int)\n \n@@ -1518,7 +1518,7 @@ def categorical_crossentropy(target, output, from_logits=False):\n         # scale preds so that the class probas of each sample sum to 1\n         output /= output.sum(axis=-1, keepdims=True)\n     # avoid numerical instability with _EPSILON clipping\n-    output = T.clip(output, _EPSILON, 1.0 - _EPSILON)\n+    output = T.clip(output, epsilon(), 1.0 - epsilon())\n     return T.nnet.categorical_crossentropy(output, target)\n \n \n@@ -1533,7 +1533,7 @@ def binary_crossentropy(target, output, from_logits=False):\n     if from_logits:\n         output = T.nnet.sigmoid(output)\n     # avoid numerical instability with _EPSILON clipping\n-    output = T.clip(output, _EPSILON, 1.0 - _EPSILON)\n+    output = T.clip(output, epsilon(), 1.0 - epsilon())\n     return T.nnet.binary_crossentropy(output, target)\n \n \n@@ -1584,7 +1584,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n \n def l2_normalize(x, axis=None):\n     square_sum = T.sum(T.square(x), axis=axis, keepdims=True)\n-    norm = T.sqrt(T.maximum(square_sum, _EPSILON))\n+    norm = T.sqrt(T.maximum(square_sum, epsilon()))\n     return x / norm\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaa54e41534d7ad7db5669738b20f5d9152da51a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 57 | Churn Cumulative: 3929 | Contributors (this commit): 57 | Commits (past 90d): 24 | Contributors (cumulative): 70 | DMM Complexity: 0.6382978723404256\n\nDIFF:\n@@ -955,6 +955,10 @@ class CSVLogger(Callback):\n             else:\n                 return k\n \n+        if self.model.stop_training:\n+            # We set NA so that csv parsers do not fail for this last epoch.\n+            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n+\n         if not self.writer:\n             self.keys = sorted(logs.keys())\n \n\n@@ -3,6 +3,7 @@ import multiprocessing\n \n import numpy as np\n import pytest\n+from csv import reader\n from csv import Sniffer\n import shutil\n from keras import optimizers\n@@ -72,6 +73,58 @@ def test_TerminateOnNaN():\n     assert loss[0] == np.inf or np.isnan(loss[0])\n \n \n+@keras_test\n+def test_stop_training_csv(tmpdir):\n+    np.random.seed(1337)\n+    fp = str(tmpdir / 'test.csv')\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         num_classes=num_class)\n+\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+    cbks = [callbacks.TerminateOnNaN(), callbacks.CSVLogger(fp)]\n+    model = Sequential()\n+    for _ in range(5):\n+        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='linear'))\n+    model.compile(loss='mean_squared_error',\n+                  optimizer='rmsprop')\n+\n+    def data_generator():\n+        i = 0\n+        max_batch_index = len(X_train) // batch_size\n+        tot = 0\n+        while 1:\n+            if tot > 3 * len(X_train):\n+                yield np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_class]) * np.nan\n+            else:\n+                yield (X_train[i * batch_size: (i + 1) * batch_size],\n+                       y_train[i * batch_size: (i + 1) * batch_size])\n+            i += 1\n+            tot += 1\n+            i = i % max_batch_index\n+\n+    history = model.fit_generator(data_generator(),\n+                                  len(X_train) // batch_size,\n+                                  validation_data=(X_test, y_test),\n+                                  callbacks=cbks,\n+                                  epochs=20)\n+    loss = history.history['loss']\n+    assert len(loss) > 1\n+    assert loss[-1] == np.inf or np.isnan(loss[-1])\n+\n+    values = []\n+    with open(fp) as f:\n+        for x in reader(f):\n+            values.append(x)\n+\n+    assert 'nan' in values[-1], 'The last epoch was not logged.'\n+    os.remove(fp)\n+\n+\n @keras_test\n def test_ModelCheckpoint(tmpdir):\n     np.random.seed(1337)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b347eed98780f64cca45db93e304d3ee7fda399f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 16394 | Contributors (this commit): 146 | Commits (past 90d): 65 | Contributors (cumulative): 216 | DMM Complexity: None\n\nDIFF:\n@@ -444,7 +444,7 @@ def shape(x):\n         A symbolic shape (which is itself a tensor).\n \n     # Examples\n-    ```\n+    ```python\n         # TensorFlow example\n         >>> from keras import backend as K\n         >>> tf_session = K.get_session()\n\n@@ -53,11 +53,11 @@ def get_uid(prefix=''):\n         An integer.\n \n     # Example\n-    ```\n+    ```python\n         >>> keras.backend.get_uid('dense')\n-        >>> 1\n+        1\n         >>> keras.backend.get_uid('dense')\n-        >>> 2\n+        2\n     ```\n \n     \"\"\"\n\n@@ -591,7 +591,7 @@ class TensorBoard(Callback):\n \n     If you have installed TensorFlow with pip, you should be able\n     to launch TensorBoard from the command line:\n-    ```\n+    ```sh\n     tensorboard --logdir=/full_path_to_your_logs\n     ```\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbbd585f25c25ee6c0ab95af474c40a3bc427ddd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 27 | Files Changed: 3 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 65 | Churn Cumulative: 10585 | Contributors (this commit): 77 | Commits (past 90d): 53 | Contributors (cumulative): 95 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1083,17 +1083,32 @@ def permute_dimensions(x, pattern):\n     return C.transpose(x, axis)\n \n \n-def resize_images(X, height_factor, width_factor, data_format):\n+def resize_images(x, height_factor, width_factor, data_format):\n     if data_format == 'channels_first':\n-        output = repeat_elements(X, height_factor, axis=2)\n+        output = repeat_elements(x, height_factor, axis=2)\n         output = repeat_elements(output, width_factor, axis=3)\n         return output\n     elif data_format == 'channels_last':\n-        output = repeat_elements(X, height_factor, axis=1)\n+        output = repeat_elements(x, height_factor, axis=1)\n         output = repeat_elements(output, width_factor, axis=2)\n         return output\n     else:\n-        raise ValueError('CNTK Backend: Invalid dim_ordering:', data_format)\n+        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n+\n+\n+def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n+    if data_format == 'channels_first':\n+        output = repeat_elements(x, depth_factor, axis=2)\n+        output = repeat_elements(output, height_factor, axis=3)\n+        output = repeat_elements(output, width_factor, axis=4)\n+        return output\n+    elif data_format == 'channels_last':\n+        output = repeat_elements(x, depth_factor, axis=1)\n+        output = repeat_elements(output, height_factor, axis=2)\n+        output = repeat_elements(output, width_factor, axis=3)\n+        return output\n+    else:\n+        raise ValueError('CNTK Backend: Invalid data_format:', data_format)\n \n \n def repeat_elements(x, rep, axis):\n\n@@ -853,7 +853,7 @@ def repeat_elements(x, rep, axis):\n     return y\n \n \n-def resize_images(X, height_factor, width_factor, data_format):\n+def resize_images(x, height_factor, width_factor, data_format):\n     \"\"\"Resize the images contained in a 4D tensor of shape\n     - [batch, channels, height, width] (for 'channels_first' data_format)\n     - [batch, height, width, channels] (for 'channels_last' data_format)\n@@ -861,18 +861,18 @@ def resize_images(X, height_factor, width_factor, data_format):\n     positive integers.\n     \"\"\"\n     if data_format == 'channels_first':\n-        output = repeat_elements(X, height_factor, axis=2)\n+        output = repeat_elements(x, height_factor, axis=2)\n         output = repeat_elements(output, width_factor, axis=3)\n         return output\n     elif data_format == 'channels_last':\n-        output = repeat_elements(X, height_factor, axis=1)\n+        output = repeat_elements(x, height_factor, axis=1)\n         output = repeat_elements(output, width_factor, axis=2)\n         return output\n     else:\n         raise ValueError('Invalid data_format:', data_format)\n \n \n-def resize_volumes(X, depth_factor, height_factor, width_factor, data_format):\n+def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     \"\"\"Resize the volume contained in a 5D tensor of shape\n     - [batch, channels, depth, height, width] (for 'channels_first' data_format)\n     - [batch, depth, height, width, channels] (for 'channels_last' data_format)\n@@ -880,12 +880,12 @@ def resize_volumes(X, depth_factor, height_factor, width_factor, data_format):\n     Both factors should be positive integers.\n     \"\"\"\n     if data_format == 'channels_first':\n-        output = repeat_elements(X, depth_factor, axis=2)\n+        output = repeat_elements(x, depth_factor, axis=2)\n         output = repeat_elements(output, height_factor, axis=3)\n         output = repeat_elements(output, width_factor, axis=4)\n         return output\n     elif data_format == 'channels_last':\n-        output = repeat_elements(X, depth_factor, axis=1)\n+        output = repeat_elements(x, depth_factor, axis=1)\n         output = repeat_elements(output, height_factor, axis=2)\n         output = repeat_elements(output, width_factor, axis=3)\n         return output\n\n@@ -675,23 +675,23 @@ class TestBackend(object):\n         (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), 1, False),\n         (np.array([[1.1, 1.2, 1.3], [0.9, 0.7, 1.4]]), -1, False),\n     ])\n-    @pytest.mark.parametrize('K', [KTH, KTF], ids=[\"KTH\", \"KTF\"])\n-    def test_logsumexp(self, x_np, axis, keepdims, K):\n+    def test_logsumexp(self, x_np, axis, keepdims):\n         '''\n         Check if K.logsumexp works properly for values close to one.\n         '''\n-        x = K.variable(x_np)\n-        assert_allclose(K.eval(K.logsumexp(x, axis=axis, keepdims=keepdims)),\n+        for k in BACKENDS:\n+            x = k.variable(x_np)\n+            assert_allclose(k.eval(k.logsumexp(x, axis=axis, keepdims=keepdims)),\n                             np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)),\n                             rtol=1e-5)\n \n-    @pytest.mark.parametrize('K', [KTF], ids=[\"KTF\"])\n-    def test_logsumexp_optim(self, K):\n+    def test_logsumexp_optim(self):\n         '''\n         Check if optimization works.\n         '''\n+        for k in [KTF]:\n             x_np = np.array([1e+4, 1e-4])\n-        assert_allclose(K.eval(K.logsumexp(K.variable(x_np), axis=0)),\n+            assert_allclose(k.eval(k.logsumexp(k.variable(x_np), axis=0)),\n                             1e4,\n                             rtol=1e-5)\n \n@@ -716,10 +716,11 @@ class TestBackend(object):\n \n         z_list = [k.eval(k.dropout(k.variable(val), level=0.2,\n                                    noise_shape=list(val.shape)))\n-                  for k in [KTF, KTH]]\n+                  for k in BACKENDS]\n         assert_list_pairwise(z_list, allclose=False)\n         # dropout patterns are different, only check mean\n-        assert np.abs(z_list[0].mean() - z_list[1].mean()) < 0.05\n+        for i in range(len(z_list) - 1):\n+            assert np.abs(z_list[i].mean() - z_list[i + 1].mean()) < 0.05\n \n         # Test invalid use cases\n         for k in BACKENDS:\n@@ -955,7 +956,7 @@ class TestBackend(object):\n         assert ztf.shape == (6, 2, 5, 4, 3)\n \n     def test_pooling_invalid_use(self):\n-        for (input_shape, pool_size) in ([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n+        for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n             for k in BACKENDS:\n                 x = k.variable(np.random.random(input_shape))\n                 if len(pool_size) == 2:\n@@ -971,11 +972,6 @@ class TestBackend(object):\n                     with pytest.raises(ValueError):\n                         k.pool3d(x, pool_size=pool_size, padding='twice')\n                     with pytest.raises(ValueError):\n-                        # In the current CNTK backend,\n-                        # `_preprocess_conv3d_input` is misimplemented.\n-                        if k == KC:\n-                            raise ValueError\n-                        else:\n                         k.pool3d(x, pool_size=pool_size, pool_mode='median')\n \n     def test_resize_images(self):\n@@ -1006,7 +1002,7 @@ class TestBackend(object):\n             elif data_format == 'channels_last':\n                 x_shape = (2,) + shape + (3,)\n             check_single_tensor_operation('resize_volumes', x_shape,\n-                                          [KTH, KTF],\n+                                          BACKENDS, cntk_dynamicity=True,\n                                           depth_factor=2,\n                                           height_factor=2,\n                                           width_factor=2,\n@@ -1014,7 +1010,7 @@ class TestBackend(object):\n \n         # Test invalid use cases\n         xval = np.random.random(x_shape)\n-        for k in (KTH, KTF):\n+        for k in BACKENDS:\n             with pytest.raises(ValueError):\n                 k.resize_volumes(k.variable(xval), 2, 2, 2,\n                                  data_format='channels_middle')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b6bd3172c5e40f61473b5a32fbdc1809ee071b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 8355 | Contributors (this commit): 86 | Commits (past 90d): 30 | Contributors (cumulative): 86 | DMM Complexity: None\n\nDIFF:\n@@ -1800,7 +1800,7 @@ def repeat_elements(x, rep, axis):\n         x_rep = [s for s in splits for _ in range(rep)]\n         return concatenate(x_rep, axis)\n \n-    # Here we use tf.tile to mimic behaviour of np.repeat so that\n+    # Here we use tf.tile to mimic behavior of np.repeat so that\n     # we can handle dynamic shapes (that include None).\n     # To do that, we need an auxiliary axis to repeat elements along\n     # it and then merge them along the desired axis.\n@@ -2574,7 +2574,7 @@ def in_train_phase(x, alt, training=None):\n             (tensor or callable that returns a tensor).\n         training: Optional scalar tensor\n             (or Python boolean, or Python integer)\n-            specifing the learning phase.\n+            specifying the learning phase.\n \n     # Returns\n         Either `x` or `alt` based on the `training` flag.\n@@ -2617,7 +2617,7 @@ def in_test_phase(x, alt, training=None):\n             (tensor or callable that returns a tensor).\n         training: Optional scalar tensor\n             (or Python boolean, or Python integer)\n-            specifing the learning phase.\n+            specifying the learning phase.\n \n     # Returns\n         Either `x` or `alt` based on `K.learning_phase`.\n@@ -3746,7 +3746,7 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n         data_format: the data format, channels_first or channels_last\n \n     # Returns\n-        the tensor after 1d conv with un-shared weights, with shape (batch_size, output_lenght, filters)\n+        the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters)\n \n     # Raises\n         ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45f8f11104e938f4c7da9feae07abffdb4706276", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 13691 | Contributors (this commit): 106 | Commits (past 90d): 54 | Contributors (cumulative): 161 | DMM Complexity: None\n\nDIFF:\n@@ -372,6 +372,7 @@ def is_keras_tensor(x):\n     # Examples\n     ```python\n         >>> from keras import backend as K\n+        >>> from keras.layers import Input, Dense\n         >>> np_var = numpy.array([1, 2])\n         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n         ValueError\n@@ -379,10 +380,16 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n         False\n         >>> keras_var = K.variable(np_var)\n-        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is a Keras tensor.\n-        True\n+        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n+        False\n         >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n-        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n+        False\n+        >>> keras_input = Input([10])\n+        >>> K.is_keras_tensor(keras_input) # An Input layer is a Keras tensor.\n+        True\n+        >>> keras_layer = Dense(10)(keras_input)\n+        >>> K.is_keras_tensor(keras_layer) # Any Keras layer is a Keras tensor.\n         True\n     ```\n     \"\"\"\n\n@@ -179,6 +179,7 @@ def is_keras_tensor(x):\n     # Examples\n     ```python\n         >>> from keras import backend as K\n+        >>> from keras.layers import Input, Dense\n         >>> np_var = numpy.array([1, 2])\n         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n         ValueError\n@@ -186,10 +187,16 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(k_var) # A variable created directly from tensorflow/theano is not a Keras tensor.\n         False\n         >>> keras_var = K.variable(np_var)\n-        >>> K.is_keras_tensor(keras_var) # A variable created with the keras backend is a Keras tensor.\n-        True\n+        >>> K.is_keras_tensor(keras_var) # A variable created with the keras backend is not a Keras tensor.\n+        False\n         >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n-        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n+        False\n+        >>> keras_input = Input([10])\n+        >>> K.is_keras_tensor(keras_input) # An Input layer is a Keras tensor.\n+        True\n+        >>> keras_layer = Dense(10)(keras_input)\n+        >>> K.is_keras_tensor(keras_layer) # Any Keras layer is a Keras tensor.\n         True\n     ```\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36b862f259fd1f062e815a5212381a968a544709", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 13 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 13725 | Contributors (this commit): 106 | Commits (past 90d): 56 | Contributors (cumulative): 161 | DMM Complexity: None\n\nDIFF:\n@@ -360,14 +360,17 @@ def constant(value, dtype=None, shape=None, name=None):\n def is_keras_tensor(x):\n     \"\"\"Returns whether `x` is a Keras tensor.\n \n+    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n+    (`Layer` class) or by `Input`.\n+\n     # Arguments\n-        x: a potential tensor.\n+        x: A candidate tensor.\n \n     # Returns\n-        A boolean: whether the argument is a Keras tensor.\n+        A boolean: Whether the argument is a Keras tensor.\n \n     # Raises\n-        ValueError: in case `x` is not a symbolic tensor.\n+        ValueError: In case `x` is not a symbolic tensor.\n \n     # Examples\n     ```python\n@@ -386,10 +389,10 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n         False\n         >>> keras_input = Input([10])\n-        >>> K.is_keras_tensor(keras_input) # An Input layer is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n         True\n-        >>> keras_layer = Dense(10)(keras_input)\n-        >>> K.is_keras_tensor(keras_layer) # Any Keras layer is a Keras tensor.\n+        >>> keras_layer_output = Dense(10)(keras_input)\n+        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n         True\n     ```\n     \"\"\"\n\n@@ -167,14 +167,17 @@ def constant(value, dtype=None, shape=None, name=None):\n def is_keras_tensor(x):\n     \"\"\"Returns whether `x` is a Keras tensor.\n \n+    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n+    (`Layer` class) or by `Input`.\n+\n     # Arguments\n-        x: a potential tensor.\n+        x: A candidate tensor.\n \n     # Returns\n-        A boolean: whether the argument is a Keras tensor.\n+        A boolean: Whether the argument is a Keras tensor.\n \n     # Raises\n-        ValueError: in case `x` is not a symbolic tensor.\n+        ValueError: In case `x` is not a symbolic tensor.\n \n     # Examples\n     ```python\n@@ -183,8 +186,8 @@ def is_keras_tensor(x):\n         >>> np_var = numpy.array([1, 2])\n         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n         ValueError\n-        >>> k_var = theano.shared(value=np.array([1,2,3]))\n-        >>> K.is_keras_tensor(k_var) # A variable created directly from tensorflow/theano is not a Keras tensor.\n+        >>> k_var = tf.placeholder('float32', shape=(1,1))\n+        >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n         False\n         >>> keras_var = K.variable(np_var)\n         >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n@@ -193,10 +196,10 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n         False\n         >>> keras_input = Input([10])\n-        >>> K.is_keras_tensor(keras_input) # An Input layer is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n         True\n-        >>> keras_layer = Dense(10)(keras_input)\n-        >>> K.is_keras_tensor(keras_layer) # Any Keras layer is a Keras tensor.\n+        >>> keras_layer_output = Dense(10)(keras_input)\n+        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n         True\n     ```\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc52d9a084699a549ec1aadc7aed839bde75ef7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 18966 | Contributors (this commit): 83 | Commits (past 90d): 52 | Contributors (cumulative): 98 | DMM Complexity: None\n\nDIFF:\n@@ -1718,7 +1718,7 @@ class Function(object):\n \n     def __call__(self, inputs):\n         global _LEARNING_PHASE\n-        assert type(inputs) in {list, tuple}\n+        assert isinstance(inputs, (list, tuple))\n         feed_dict = {}\n         for tensor, value in zip(self.placeholders, inputs):\n             # cntk only support calculate on float, do auto cast here\n\n@@ -639,7 +639,7 @@ class Lambda(Layer):\n             if not isinstance(shape, (list, tuple)):\n                 raise ValueError('`output_shape` function must return a tuple or a list of tuples.')\n             if isinstance(shape, list):\n-                if type(shape[0]) == int or shape[0] is None:\n+                if isinstance(shape[0], int) or shape[0] is None:\n                     shape = tuple(shape)\n             return shape\n \n\n@@ -26,7 +26,7 @@ def cntk_func_single_tensor(function_name, x_shape, **kwargs):\n \n \n def cntk_func_two_tensor(function_name, x_shape, y, **kwargs):\n-    if type(y).__name__ == 'ndarray':\n+    if isinstance(y, (np.generic, np.ndarray)):\n         xc = KC.placeholder(x_shape)\n         output_cntk = getattr(KC, function_name)(xc, KC.variable(y), **kwargs)\n         return KC.function([xc], [output_cntk])\n\n@@ -151,20 +151,20 @@ def test_node_construction():\n     node = a_layer.inbound_nodes[a_node_index]\n     assert node.outbound_layer == a_layer\n \n-    assert type(node.inbound_layers) is list\n+    assert isinstance(node.inbound_layers, list)\n     assert node.inbound_layers == []\n-    assert type(node.input_tensors) is list\n+    assert isinstance(node.input_tensors, list)\n     assert node.input_tensors == [a]\n-    assert type(node.input_masks) is list\n+    assert isinstance(node.input_masks, list)\n     assert node.input_masks == [None]\n-    assert type(node.input_shapes) is list\n+    assert isinstance(node.input_shapes, list)\n     assert node.input_shapes == [(None, 32)]\n \n-    assert type(node.output_tensors) is list\n+    assert isinstance(node.output_tensors, list)\n     assert node.output_tensors == [a]\n-    assert type(node.output_shapes) is list\n+    assert isinstance(node.output_shapes, list)\n     assert node.output_shapes == [(None, 32)]\n-    assert type(node.output_masks) is list\n+    assert isinstance(node.output_masks, list)\n     assert node.output_masks == [None]\n \n     dense = Dense(16, name='dense_1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1cb81f2f9fe20074a3df13716ee3c8eb7bdec109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5344 | Contributors (this commit): 76 | Commits (past 90d): 24 | Contributors (cumulative): 76 | DMM Complexity: None\n\nDIFF:\n@@ -1536,7 +1536,7 @@ def sparse_categorical_crossentropy(target, output, from_logits=False):\n     target = T.cast(T.flatten(target), 'int32')\n     target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n     target = reshape(target, shape(output))\n-    return categorical_crossentropy(output, target, from_logits)\n+    return categorical_crossentropy(target, output, from_logits)\n \n \n def binary_crossentropy(target, output, from_logits=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c7d73530c1ab1b47f9fb5f0612ec13fef1a26c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 1047 | Contributors (this commit): 18 | Commits (past 90d): 12 | Contributors (cumulative): 22 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -4,6 +4,7 @@ from __future__ import absolute_import\n import copy\n from ..engine import Layer\n from ..engine import InputSpec\n+from ..engine.topology import _object_list_uid\n from ..utils.generic_utils import has_arg\n from .. import backend as K\n \n@@ -21,6 +22,10 @@ class Wrapper(Layer):\n \n     def __init__(self, layer, **kwargs):\n         self.layer = layer\n+        # Tracks mapping of Wrapper inputs to inner layer inputs. Useful when\n+        # the inner layer has update ops that depend on it's inputs (as opposed\n+        # to the inputs to the Wrapper layer).\n+        self._input_map = {}\n         super(Wrapper, self).__init__(**kwargs)\n \n     def build(self, input_shape=None):\n@@ -48,10 +53,17 @@ class Wrapper(Layer):\n         return []\n \n     def get_updates_for(self, inputs=None):\n-        if inputs is None:\n-            updates = self.layer.get_updates_for(None)\n-            return updates + super(Wrapper, self).get_updates_for(None)\n-        return super(Wrapper, self).get_updates_for(inputs)\n+        # If the wrapper modifies the inputs, use the modified inputs to\n+        # get the updates from the inner layer.\n+        inner_inputs = inputs\n+        if inputs is not None:\n+            uid = _object_list_uid(inputs)\n+            if uid in self._input_map:\n+                inner_inputs = self._input_map[uid]\n+\n+        updates = self.layer.get_updates_for(inner_inputs)\n+        updates += super(Wrapper, self).get_updates_for(inputs)\n+        return updates\n \n     @property\n     def losses(self):\n@@ -182,8 +194,11 @@ class TimeDistributed(Wrapper):\n             input_length = input_shape[1]\n             if not input_length:\n                 input_length = K.shape(inputs)[1]\n-            # Shape: (num_samples * timesteps, ...)\n+            # Shape: (num_samples * timesteps, ...). And track the\n+            # transformation in self._input_map.\n+            input_uid = _object_list_uid(inputs)\n             inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n+            self._input_map[input_uid] = inputs\n             # (num_samples * timesteps, ...)\n             y = self.layer.call(inputs, **kwargs)\n             if hasattr(y, '_uses_learning_phase'):\n\n@@ -3,9 +3,10 @@ import numpy as np\n from numpy.testing import assert_allclose\n from keras.utils.test_utils import keras_test\n from keras.layers import wrappers, Input\n-from keras.layers import core, convolutional, recurrent, embeddings\n+from keras.layers import core, convolutional, recurrent, embeddings, normalization\n from keras.models import Sequential, Model, model_from_json\n from keras import backend as K\n+from keras.engine.topology import _object_list_uid\n \n \n @keras_test\n@@ -87,6 +88,27 @@ def test_TimeDistributed():\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n     outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n \n+    # test with BatchNormalization\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(normalization.BatchNormalization(center=True, scale=True),\n+              name='bn', input_shape=(10, 2)))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    # Assert that mean and variance are 0 and 1.\n+    td = model.layers[0]\n+    assert np.array_equal(td.get_weights()[2], np.array([0, 0]))\n+    assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n+    # Train\n+    model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n+                         np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n+    # Assert that mean and variance changed.\n+    assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n+    assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n+    # Verify input_map has one mapping from inputs to reshaped inputs.\n+    uid = _object_list_uid(model.inputs)\n+    assert len(td._input_map.keys()) == 1\n+    assert uid in td._input_map\n+    assert K.int_shape(td._input_map[uid]) == (None, 2)\n+\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
