{"custom_id": "keras#a31c82f179c72e9bec621c651883037d0fb11064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1706 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1348,7 +1348,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n   def _create_or_restore_slot_variable(\n       self, slot_variable_position, slot_name, variable):\n-    \"\"\"Restore a slot variable's value, possibly creating it.\n+    \"\"\"Returns the slot variable that should have a value restored into it.\n+\n+    It is up to the caller to retore the value into the slot variable if a valid\n+    slot variable is returned.\n \n     Called when a variable which has an associated slot variable is created or\n     restored. When executing eagerly, we create the slot variable with a\n@@ -1366,6 +1369,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         indicating the slot variable `Trackable` object to be restored.\n       slot_name: The name of this `Optimizer`'s slot to restore into.\n       variable: The variable object this slot is being created for.\n+\n+    Returns:\n+      A slot variable that should have a value restored into it, or None if a\n+      slot variable should not be restored at this time.\n     \"\"\"\n     variable_key = _var_key(variable)\n     slot_dict = self._slots.get(variable_key, {})\n@@ -1404,7 +1411,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if slot_variable is not None:\n       # If we've either made this slot variable, or if we've pulled out an\n       # existing slot variable, we should restore it.\n-      slot_variable_position.restore(slot_variable)\n+      return slot_variable\n     else:\n       # We didn't make the slot variable. Defer restoring until it gets created\n       # normally. We keep a list rather than the one with the highest restore\n@@ -1413,6 +1420,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       self._deferred_slot_restorations.setdefault(\n           slot_name, {}).setdefault(variable_key, []).append(\n               slot_variable_position)\n+    return None\n \n   @contextlib.contextmanager\n   def _distribution_strategy_scope(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#355e7af2376541ca272457edb796c6de07c7e7c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 105 | Files Changed: 9 | Hunks: 63 | Methods Changed: 26 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 220 | Churn Cumulative: 38513 | Contributors (this commit): 100 | Commits (past 90d): 68 | Contributors (cumulative): 151 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1606,8 +1606,7 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     # Test noise shape\n     outputs = backend.dropout(inputs, 0.2, noise_shape=(200, 1))\n     outputs_val = backend.eval(outputs)\n-    # Make sure the whole column gets the same dropout\n-    self.assertEqual(np.min(outputs_val[0, :]), np.max(outputs_val[0, :]))\n+    self.assertAllClose(outputs_val[2, :], outputs_val[3, :], atol=1e-5)\n \n \n class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -3414,42 +3414,6 @@ def _apply_name_scope_on_model_declaration(enable):\n   _is_name_scope_on_model_declaration_enabled = enable\n \n \n-class BaseRandomLayer(Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, seed=None, force_generator=False, **kwargs):\n-    \"\"\"Initialize the BaseRandomLayer.\n-\n-    Note that the constructor is annotated with\n-    @no_automatic_dependency_tracking. This is to skip the auto\n-    tracking of self._random_generator instance, which is an AutoTrackable.\n-    The backend.RandomGenerator could contain a tf.random.Generator instance\n-    which will have tf.Variable as the internal state. We want to avoid saving\n-    that state into model.weights and checkpoints for backward compatibility\n-    reason. In the meantime, we still need to make them visible to SavedModel\n-    when it is tracing the tf.function for the `call()`.\n-    See _list_extra_dependencies_for_serialization below for more details.\n-\n-    Args:\n-      seed: optional integer, used to create RandomGenerator.\n-      force_generator: boolean, default to False, whether to force the\n-        RandomGenerator to use the code branch of tf.random.Generator.\n-      **kwargs: other keyward arguements that will be passed to the parent class\n-    \"\"\"\n-    super().__init__(**kwargs)\n-    self._random_generator = backend.RandomGenerator(\n-        seed, force_generator=force_generator)\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel only\n-    # (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n-\n-\n # Avoid breaking users who directly import this symbol from this file.\n # TODO(fchollet): remove this.\n InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -14,24 +14,24 @@\n # ==============================================================================\n # pylint: disable=protected-access\n # pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-direct-tensorflow-import\n \"\"\"Convolutional-recurrent layers.\"\"\"\n \n+import tensorflow.compat.v2 as tf\n+\n+import numpy as np\n+\n from keras import activations\n from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine import base_layer\n+from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n from keras.layers.recurrent import DropoutRNNCellMixin\n from keras.layers.recurrent import RNN\n from keras.utils import conv_utils\n from keras.utils import generic_utils\n from keras.utils import tf_utils\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -209,7 +209,7 @@ class ConvRNN(RNN):\n         shape=(batch_size, None) + input_shape[2:self.rank + 3])\n \n     # allow cell (if layer) to build before we set or validate state_spec\n-    if isinstance(self.cell, base_layer.Layer):\n+    if isinstance(self.cell, Layer):\n       step_input_shape = (input_shape[0],) + input_shape[2:]\n       if constants_shape is not None:\n         self.cell.build([step_input_shape] + constants_shape)\n@@ -397,7 +397,7 @@ class ConvRNN(RNN):\n         backend.set_value(state, value)\n \n \n-class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n   \"\"\"Cell class for the ConvLSTM layer.\n \n   Args:\n\n@@ -16,14 +16,14 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n-from keras.engine import base_layer\n+from keras.engine.base_layer import Layer\n from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.layers.Dropout')\n-class Dropout(base_layer.BaseRandomLayer):\n+class Dropout(Layer):\n   \"\"\"Applies Dropout to the input.\n \n   The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n@@ -74,6 +74,7 @@ class Dropout(base_layer.BaseRandomLayer):\n       training mode (adding dropout) or in inference mode (doing nothing).\n   \"\"\"\n \n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n     # Note that the constructor is annotated with\n     # @no_automatic_dependency_tracking. This is to skip the auto\n@@ -84,7 +85,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     # reason. In the meantime, we still need to make them visible to SavedModel\n     # when it is tracing the tf.function for the `call()`.\n     # See _list_extra_dependencies_for_serialization below for more details.\n-    super(Dropout, self).__init__(seed=seed, **kwargs)\n+    super(Dropout, self).__init__(**kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n                        f'`rate`, expected a value between 0 and 1.')\n@@ -92,6 +93,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n+    self._random_generator = backend.RandomGenerator(seed)\n \n   def build(self, input_shape):\n     self._random_generator._maybe_init()  # pylint: disable=protected-access\n@@ -132,3 +134,11 @@ class Dropout(base_layer.BaseRandomLayer):\n     }\n     base_config = super(Dropout, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n\n@@ -17,16 +17,15 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n-from keras import backend\n-from keras.engine import base_layer\n-from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import control_flow_util\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseDenseAttention(base_layer.BaseRandomLayer):\n+class BaseDenseAttention(Layer):\n   \"\"\"Base Attention class for Dense networks.\n \n   This class is suitable for Dense or CNN networks, and not for RNN networks.\n@@ -68,12 +67,12 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       `[batch_size, Tq, Tv]`.\n   \"\"\"\n \n-  def __init__(self, causal=False, dropout=0.0, **kwargs):\n+  def __init__(self, causal=False, dropout=0.0,\n+               **kwargs):\n     super(BaseDenseAttention, self).__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n     self.supports_masking = True\n-    self._random_generator = backend.RandomGenerator()\n \n   def _calculate_scores(self, query, key):\n     \"\"\"Calculates attention scores.\n@@ -127,7 +126,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n     weights = tf.nn.softmax(scores)\n \n     def dropped_weights():\n-      return self._random_generator.dropout(weights, rate=self.dropout)\n+      return tf.nn.dropout(weights, rate=self.dropout)\n \n     weights = control_flow_util.smart_cond(training, dropped_weights,\n                                            lambda: tf.identity(weights))\n\n@@ -25,8 +25,34 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+class BaseRandomLayer(base_layer.Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, **kwargs):\n+    # Note that the constructor is annotated with\n+    # @no_automatic_dependency_tracking. This is to skip the auto\n+    # tracking of self._random_generator instance, which is an AutoTrackable.\n+    # The backend.RandomGenerator could contain a tf.random.Generator instance\n+    # which will have tf.Variable as the internal state. We want to avoid saving\n+    # that state into model.weights and checkpoints for backward compatibility\n+    # reason. In the meantime, we still need to make them visible to SavedModel\n+    # when it is tracing the tf.function for the `call()`.\n+    # See _list_extra_dependencies_for_serialization below for more details.\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(seed)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel\n+    # only (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n @keras_export('keras.layers.GaussianNoise')\n-class GaussianNoise(base_layer.BaseRandomLayer):\n+class GaussianNoise(BaseRandomLayer):\n   \"\"\"Apply additive zero-centered Gaussian noise.\n \n   This is useful to mitigate overfitting\n@@ -82,7 +108,7 @@ class GaussianNoise(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.GaussianDropout')\n-class GaussianDropout(base_layer.BaseRandomLayer):\n+class GaussianDropout(BaseRandomLayer):\n   \"\"\"Apply multiplicative 1-centered Gaussian noise.\n \n   As it is a regularization layer, it is only active at training time.\n@@ -138,7 +164,7 @@ class GaussianDropout(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.AlphaDropout')\n-class AlphaDropout(base_layer.BaseRandomLayer):\n+class AlphaDropout(BaseRandomLayer):\n   \"\"\"Applies Alpha Dropout to the input.\n \n   Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n\n@@ -189,9 +189,35 @@ class CenterCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n+class BaseRandomLayer(base_layer.Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, **kwargs):\n+    # Note that the constructor is annotated with\n+    # @no_automatic_dependency_tracking. This is to skip the auto\n+    # tracking of self._random_generator instance, which is an AutoTrackable.\n+    # The backend.RandomGenerator could contain a tf.random.Generator instance\n+    # which will have tf.Variable as the internal state. We want to avoid saving\n+    # that state into model.weights and checkpoints for backward compatibility\n+    # reason. In the meantime, we still need to make them visible to SavedModel\n+    # when it is tracing the tf.function for the `call()`.\n+    # See _list_extra_dependencies_for_serialization below for more details.\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n @keras_export('keras.layers.RandomCrop',\n               'keras.layers.experimental.preprocessing.RandomCrop')\n-class RandomCrop(base_layer.BaseRandomLayer):\n+class RandomCrop(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n   During training, this layer will randomly choose a location to crop images\n@@ -223,8 +249,7 @@ class RandomCrop(base_layer.BaseRandomLayer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed,\n-                                     force_generator=True)\n+    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed)\n     self.height = height\n     self.width = width\n     self.seed = seed\n@@ -329,7 +354,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n @keras_export('keras.layers.RandomFlip',\n               'keras.layers.experimental.preprocessing.RandomFlip')\n-class RandomFlip(base_layer.BaseRandomLayer):\n+class RandomFlip(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n   This layer will flip the images horizontally and or vertically based on the\n@@ -359,7 +384,7 @@ class RandomFlip(base_layer.BaseRandomLayer):\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super(RandomFlip, self).__init__(seed=seed, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -420,7 +445,7 @@ class RandomFlip(base_layer.BaseRandomLayer):\n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.RandomTranslation',\n               'keras.layers.experimental.preprocessing.RandomTranslation')\n-class RandomTranslation(base_layer.BaseRandomLayer):\n+class RandomTranslation(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly translates images during training.\n \n   This layer will apply random translations to each image during training,\n@@ -482,8 +507,7 @@ class RandomTranslation(base_layer.BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n-    super(RandomTranslation, self).__init__(seed=seed, force_generator=True,\n-                                            **kwargs)\n+    super(RandomTranslation, self).__init__(seed=seed, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -747,7 +771,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomRotation',\n               'keras.layers.experimental.preprocessing.RandomRotation')\n-class RandomRotation(base_layer.BaseRandomLayer):\n+class RandomRotation(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n   This layer will apply random rotations to each image, filling empty space\n@@ -803,8 +827,7 @@ class RandomRotation(base_layer.BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n-    super(RandomRotation, self).__init__(seed=seed, force_generator=True,\n-                                         **kwargs)\n+    super(RandomRotation, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -874,7 +897,7 @@ class RandomRotation(base_layer.BaseRandomLayer):\n \n @keras_export('keras.layers.RandomZoom',\n               'keras.layers.experimental.preprocessing.RandomZoom')\n-class RandomZoom(base_layer.BaseRandomLayer):\n+class RandomZoom(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n   This layer will randomly zoom in or out on each axis of an image\n@@ -941,7 +964,7 @@ class RandomZoom(base_layer.BaseRandomLayer):\n                fill_value=0.0,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n-    super(RandomZoom, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super(RandomZoom, self).__init__(seed=seed, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -1080,7 +1103,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomContrast',\n               'keras.layers.experimental.preprocessing.RandomContrast')\n-class RandomContrast(base_layer.BaseRandomLayer):\n+class RandomContrast(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n   This layer will randomly adjust the contrast of an image or images by a random\n@@ -1113,8 +1136,7 @@ class RandomContrast(base_layer.BaseRandomLayer):\n   def __init__(self, factor, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n-    super(RandomContrast, self).__init__(seed=seed, force_generator=True,\n-                                         **kwargs)\n+    super(RandomContrast, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -1159,7 +1181,7 @@ class RandomContrast(base_layer.BaseRandomLayer):\n \n @keras_export('keras.layers.RandomHeight',\n               'keras.layers.experimental.preprocessing.RandomHeight')\n-class RandomHeight(base_layer.BaseRandomLayer):\n+class RandomHeight(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image height during training.\n \n   This layer adjusts the height of a batch of images by a random factor.\n@@ -1200,8 +1222,7 @@ class RandomHeight(base_layer.BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n-    super(RandomHeight, self).__init__(seed=seed, force_generator=True,\n-                                       **kwargs)\n+    super(RandomHeight, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.height_lower = factor[0]\n@@ -1262,7 +1283,7 @@ class RandomHeight(base_layer.BaseRandomLayer):\n \n @keras_export('keras.layers.RandomWidth',\n               'keras.layers.experimental.preprocessing.RandomWidth')\n-class RandomWidth(base_layer.BaseRandomLayer):\n+class RandomWidth(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image width during training.\n \n   This layer will randomly adjusts the width of a batch of images of a\n@@ -1303,7 +1324,7 @@ class RandomWidth(base_layer.BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n-    super(RandomWidth, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super(RandomWidth, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.width_lower = factor[0]\n\n@@ -28,7 +28,7 @@ from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine import base_layer\n+from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n from keras.saving.saved_model import layer_serialization\n from keras.utils import control_flow_util\n@@ -45,7 +45,7 @@ RECURRENT_DROPOUT_WARNING_MSG = (\n \n \n @keras_export('keras.layers.StackedRNNCells')\n-class StackedRNNCells(base_layer.Layer):\n+class StackedRNNCells(Layer):\n   \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n \n   Used to implement efficient stacked RNNs.\n@@ -158,7 +158,7 @@ class StackedRNNCells(base_layer.Layer):\n       return tuple([batch_size] + shape)\n \n     for cell in self.cells:\n-      if isinstance(cell, base_layer.Layer) and not cell.built:\n+      if isinstance(cell, Layer) and not cell.built:\n         with backend.name_scope(cell.name):\n           cell.build(input_shape)\n           cell.built = True\n@@ -196,7 +196,7 @@ class StackedRNNCells(base_layer.Layer):\n \n \n @keras_export('keras.layers.RNN')\n-class RNN(base_layer.Layer):\n+class RNN(Layer):\n   \"\"\"Base class for recurrent layers.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -587,7 +587,7 @@ class RNN(base_layer.Layer):\n       step_input_shape = tf.nest.map_structure(get_step_input_shape, input_shape)\n \n     # allow cell (if layer) to build before we set or validate state_spec.\n-    if isinstance(self.cell, base_layer.Layer) and not self.cell.built:\n+    if isinstance(self.cell, Layer) and not self.cell.built:\n       with backend.name_scope(self.cell.name):\n         self.cell.build(step_input_shape)\n         self.cell.built = True\n@@ -1025,7 +1025,7 @@ class RNN(base_layer.Layer):\n \n \n @keras_export('keras.layers.AbstractRNNCell')\n-class AbstractRNNCell(base_layer.Layer):\n+class AbstractRNNCell(Layer):\n   \"\"\"Abstract object representing an RNN cell.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1126,8 +1126,6 @@ class DropoutRNNCellMixin:\n       tensor need to dropout.\n     recurrent_dropout: a float number within range [0, 1). The ratio that the\n       recurrent state weights need to dropout.\n-    _random_generator: A backend.RandomGenerator instance, which will be used\n-      to produce outputs based on the inputs and dropout rate.\n   This object will create and cache created dropout masks, and reuse them for\n   the incoming data, so that the same mask is used for every batch input.\n   \"\"\"\n@@ -1183,7 +1181,6 @@ class DropoutRNNCellMixin:\n \n   def _create_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n-        self._random_generator,\n         tf.ones_like(inputs),\n         self.dropout,\n         training=training,\n@@ -1191,7 +1188,6 @@ class DropoutRNNCellMixin:\n \n   def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n-        self._random_generator,\n         tf.ones_like(inputs),\n         self.recurrent_dropout,\n         training=training,\n@@ -1256,7 +1252,7 @@ class DropoutRNNCellMixin:\n \n \n @keras_export('keras.layers.SimpleRNNCell')\n-class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n   \"\"\"Cell class for SimpleRNN.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1715,7 +1711,7 @@ class SimpleRNN(RNN):\n \n \n @keras_export(v1=['keras.layers.GRUCell'])\n-class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class GRUCell(DropoutRNNCellMixin, Layer):\n   \"\"\"Cell class for the GRU layer.\n \n   Args:\n@@ -2275,7 +2271,7 @@ class GRU(RNN):\n \n \n @keras_export(v1=['keras.layers.LSTMCell'])\n-class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n+class LSTMCell(DropoutRNNCellMixin, Layer):\n   \"\"\"Cell class for the LSTM layer.\n \n   Args:\n@@ -2949,9 +2945,9 @@ class LSTM(RNN):\n     return cls(**config)\n \n \n-def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n+def _generate_dropout_mask(ones, rate, training=None, count=1):\n   def dropped_inputs():\n-    return generator.dropout(ones, rate)\n+    return backend.dropout(ones, rate)\n \n   if count > 1:\n     return [\n\n@@ -12,20 +12,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+# pylint: disable=g-classes-have-attributes\n \"\"\"Recurrent layers for TF 2.\"\"\"\n \n+import tensorflow.compat.v2 as tf\n \n import uuid\n-\n+from tensorflow.python.eager.context import get_device_name\n from keras import activations\n from keras import backend\n-from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers import recurrent\n-\n-import tensorflow.compat.v2 as tf\n-from tensorflow.python.eager.context import get_device_name\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -204,8 +201,7 @@ class GRUCell(recurrent.GRUCell):\n \n \n @keras_export('keras.layers.GRU', v1=[])\n-class GRU(recurrent.DropoutRNNCellMixin,\n-          recurrent.GRU, base_layer.BaseRandomLayer):\n+class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n   \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -945,8 +941,7 @@ class LSTMCell(recurrent.LSTMCell):\n \n \n @keras_export('keras.layers.LSTM', v1=[])\n-class LSTM(recurrent.DropoutRNNCellMixin,\n-           recurrent.LSTM, base_layer.BaseRandomLayer):\n+class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n   \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13ebd9d78f77f2fec7a64d29d570e2983fcb9711", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 55 | Churn Cumulative: 790 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -21,6 +21,7 @@ from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-d\n \n _PRINT_EVAL_STEP_EVERY_SEC = 60.0\n _ITERATIONS_UNINITIALIZED = -1\n+_CHECKPOINT_TIMEOUT_SEC = 30\n \n \n def list_checkpoint_attributes(ckpt_dir_or_file):\n@@ -173,13 +174,27 @@ class SidecarEvaluator:\n     self.steps = steps\n     self.callbacks = callbacks or []\n \n+  def _timeout_fn(self):\n+    logging.info(\n+        f'No checkpoints appear to be found after {_CHECKPOINT_TIMEOUT_SEC} '\n+        'seconds. Please check if you are properly using a '\n+        '`tf.train.Checkpoint/CheckpointManager` or '\n+        '`tf.keras.callbacks.ModelCheckpoint(save_weights_only=True)` to save '\n+        'checkpoints by the training. See '\n+        '`tf.keras.experimental.SidecarEvaluator` doc for recommended flows '\n+        'of saving checkpoints.')\n+    return False\n+\n   def start(self):\n     \"\"\"Starts the evaluation loop.\"\"\"\n     optimizer_checkpoint = tf.train.Checkpoint(iter=self._iterations)\n     checkpoint = tf.train.Checkpoint(\n         model=self.model, optimizer=optimizer_checkpoint)\n \n-    for latest_checkpoint in tf.train.checkpoints_iterator(self.checkpoint_dir):\n+    for latest_checkpoint in tf.train.checkpoints_iterator(\n+        self.checkpoint_dir,\n+        timeout=_CHECKPOINT_TIMEOUT_SEC,\n+        timeout_fn=self._timeout_fn):\n       try:\n         # `expect_partial` because the checkpoint can have other `Trackable`s\n         # such as `optimizer`.\n\n@@ -15,18 +15,19 @@\n # ==============================================================================\n \"\"\"Test covering sidecar_evaluator.py.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n import enum\n import os\n+import threading\n+import time\n \n from absl import logging\n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras.distribute import sidecar_evaluator as sidecar_evaluator_lib\n from keras.optimizer_v2 import gradient_descent\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n _BATCH_SIZE = 32\n \n@@ -254,6 +255,37 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n \n     self.assertSummaryEventsWritten(os.path.join(log_dir, 'validation'))\n \n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(\n+          mode=['eager'],\n+          model_type=[ModelType.SEQUENTIAL, ModelType.SUBCLASS],\n+          build_model=[True, False]))\n+  def testTimeoutFunction(self, model_type, build_model):\n+    checkpoint_dir = os.path.join(self.get_temp_dir(), 'checkpoints')\n+    # Create a model with synthetic data, and fit for one epoch.\n+    data = np.random.random((1000, 32))\n+    labels = np.random.random((1000, 10))\n+    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n+    dataset = dataset.batch(_BATCH_SIZE)\n+\n+    # Create a new model used for evaluation.\n+    eval_model = _test_model_builder(\n+        model_type=model_type, compile_model=True, build_model=build_model)\n+    # Have an sidecar_evaluator evaluate once.\n+    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(\n+        eval_model,\n+        data=dataset,\n+        checkpoint_dir=checkpoint_dir,\n+        max_evaluations=1)\n+    with self.assertLogs() as cm:\n+      threading.Thread(target=sidecar_evaluator.start, daemon=True).start()\n+      time.sleep(50)\n+\n+    metrics_logging = [\n+        l for l in cm.output if 'No checkpoints appear to be found' in l\n+    ]\n+    self.assertGreaterEqual(len(metrics_logging), 1)\n+\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#def69775ed579e22ed05f4bf5f367d5f0de5c572", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1718 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1348,10 +1348,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n   def _create_or_restore_slot_variable(\n       self, slot_variable_position, slot_name, variable):\n-    \"\"\"Returns the slot variable that should have a value restored into it.\n-\n-    It is up to the caller to retore the value into the slot variable if a valid\n-    slot variable is returned.\n+    \"\"\"Restore a slot variable's value, possibly creating it.\n \n     Called when a variable which has an associated slot variable is created or\n     restored. When executing eagerly, we create the slot variable with a\n@@ -1369,10 +1366,6 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         indicating the slot variable `Trackable` object to be restored.\n       slot_name: The name of this `Optimizer`'s slot to restore into.\n       variable: The variable object this slot is being created for.\n-\n-    Returns:\n-      A slot variable that should have a value restored into it, or None if a\n-      slot variable should not be restored at this time.\n     \"\"\"\n     variable_key = _var_key(variable)\n     slot_dict = self._slots.get(variable_key, {})\n@@ -1411,7 +1404,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if slot_variable is not None:\n       # If we've either made this slot variable, or if we've pulled out an\n       # existing slot variable, we should restore it.\n-      return slot_variable\n+      slot_variable_position.restore(slot_variable)\n     else:\n       # We didn't make the slot variable. Defer restoring until it gets created\n       # normally. We keep a list rather than the one with the highest restore\n@@ -1420,7 +1413,6 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       self._deferred_slot_restorations.setdefault(\n           slot_name, {}).setdefault(variable_key, []).append(\n               slot_variable_position)\n-    return None\n \n   @contextlib.contextmanager\n   def _distribution_strategy_scope(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3c888399d7f8f124ca4b51bece554c10dafbd42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 564 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,15 +14,15 @@\n # ==============================================================================\n \"\"\"Tests for running legacy optimizer code with DistributionStrategy.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-import numpy\n from keras.distribute import optimizer_combinations\n from keras.distribute.test_example import batchnorm_example\n from keras.distribute.test_example import minimize_loss_example\n from keras.layers import core\n from keras.optimizer_v2 import optimizer_v2\n+import numpy\n+import tensorflow.compat.v2 as tf\n \n \n VAR_MAP_V1 = {\n@@ -490,16 +490,12 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n           run_step = sess.make_callable(run_step())\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n \n-      weights, biases, losses = [], [], []\n+      weights, biases = [], []\n       for _ in range(5):\n-        _, loss = run_step()\n-        losses.append(loss)\n+        run_step()\n         weights.append(self.evaluate(layer.kernel))\n         biases.append(self.evaluate(layer.bias))\n \n-      loss_is_not_increasing = all(y <= x for x, y in zip(losses, losses[1:]))\n-      self.assertTrue(loss_is_not_increasing)\n-\n       error = abs(\n           numpy.add(numpy.squeeze(weights), numpy.squeeze(biases)) - 1)\n       error_is_not_increasing = all(y <= x for x, y in zip(error, error[1:]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8960c6711fbd811b80bcea26d3768ac7ce66f5d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 20 | Churn Cumulative: 9996 | Contributors (this commit): 22 | Commits (past 90d): 29 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2727,7 +2727,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           self._set_dtype_policy(policy.Policy(dtype))\n       input_shapes = None\n       # Converts Tensors / CompositeTensors to TensorShapes.\n-      if all(hasattr(x, 'shape') for x in input_list):\n+      if any(hasattr(x, 'shape') for x in input_list):\n         input_shapes = tf_utils.get_shapes(inputs)\n       else:\n         # Converts input shape to TensorShapes.\n\n@@ -715,6 +715,21 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertTrue(layer.built)\n     self.assertEqual([None, 3], layer._build_input_shape.as_list())\n \n+  def test_build_input_shape_list_with_none(self):\n+\n+    class CustomLayer(base_layer.Layer):\n+\n+      def build(self, input_shape):\n+        super().build(input_shape)\n+        self.build_shape = input_shape\n+\n+      def call(self, inputs):\n+        return inputs[0]\n+\n+    layer = CustomLayer()\n+    layer([tf.constant([1.0]), None, tf.constant([2.0])])\n+    self.assertEqual(layer.build_shape, [[1], None, [1]])\n+\n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_layer_input_shape_raises_error(self):\n     layer = layers.Dense(3)\n\n@@ -164,7 +164,8 @@ def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n \n def get_shapes(tensors):\n   \"\"\"Gets shapes from tensors.\"\"\"\n-  return tf.nest.map_structure(lambda x: x.shape, tensors)\n+  return tf.nest.map_structure(\n+      lambda x: x.shape if hasattr(x, 'shape') else None, tensors)\n \n \n #  pylint: enable=protected-access\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8933ff823bbd491ec0dfbb8633ee6bccfcacaa3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 292 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 43/43 | Churn Δ: 292 | Churn Cumulative: 444 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,291 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+r\"\"\"Utils for EfficientNet V2 models for Keras.\n+\n+Write weights from  ckpt file as in original repo\n+(https://github.com/google/automl/tree/master/efficientnetv2)\n+to h5 file for keras implementation of the models.\n+\n+Usage:\n+\n+# use checkpoint efficientnet-b0/ (can be downloaded from\n+# https://storage.googleapis.com/cloud-tpu-checkpoints/\n+# efficientnet/v2/efficientnetv2-s.tgz\n+\n+# to update weight without top layers, saving to efficientnetv2_b0_notop.h5\n+python efficientnet_v2_weight_update_util.py --model b0 --notop \\\n+    --ckpt efficientnet-b0/ --o efficientnet_v2_b0_notop.h5\n+\n+# use ImageNet21K pretrained and finetuned models for s/m/l variants:\n+# (can be downloaded from\n+# https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/\n+# v2/efficientnetv2-s-21k-ft1k.tgz)\n+# to update weight with top layers, saving to efficientnet_v2_s_new.h5\n+python efficientnet_v2_weight_update_util.py --model s \\\n+    --ckpt efficientnetv2-s-21k-ft1k/ --o efficientnet_v2_s_new.h5\n+\"\"\"\n+import tensorflow.compat.v2 as tf\n+\n+import argparse\n+import warnings\n+\n+from keras.applications import efficientnet_v2\n+from efficientnet_weight_update_util import (\n+    keras_name_to_tf_name_stem_top,\n+    get_keras_blocks,\n+    get_tf_blocks,\n+    get_variable_names_from_ckpt,\n+)\n+\n+\n+def write_ckpt_to_h5(path_h5, path_ckpt, keras_model, use_ema=True):\n+  \"\"\"Map the weights in checkpoint file (tf) to h5 file (keras).\n+\n+  Args:\n+    path_h5: str, path to output hdf5 file to write weights loaded from ckpt\n+      files.\n+    path_ckpt: str, path to the ckpt files (e.g. 'efficientnetv2-b0/model')\n+      that records efficientnet v2 weights from original repo\n+      https://github.com/google/automl/tree/master/efficientnetv2\n+    keras_model: keras model, built from keras.applications efficientnet_v2\n+      functions (e.g. EfficientNetV2_B0)\n+    use_ema: Bool, whether to use ExponentialMovingAverage result or not\n+  \"\"\"\n+  model_name_keras = keras_model.name\n+  model_name_tf = model_name_keras\n+\n+  keras_weight_names = [w.name for w in keras_model.weights]\n+  tf_weight_names = get_variable_names_from_ckpt(path_ckpt)\n+\n+  keras_blocks = get_keras_blocks(keras_weight_names)\n+  tf_blocks = get_tf_blocks(tf_weight_names)\n+\n+  print('check variables match in each block')\n+  for keras_block, tf_block in zip(keras_blocks, tf_blocks):\n+    check_match(keras_block, tf_block, keras_weight_names, tf_weight_names,\n+                model_name_tf)\n+    print('{} and {} match.'.format(tf_block, keras_block))\n+\n+  block_mapping = {x[0]: x[1] for x in zip(keras_blocks, tf_blocks)}\n+\n+  changed_weights = 0\n+  for w in keras_model.weights:\n+    if 'block' in w.name:\n+      # example: 'block1a_dwconv/depthwise_kernel:0' -> 'block1a'\n+      keras_block = w.name.split('/')[0].split('_')[0]\n+      tf_block = block_mapping[keras_block]\n+      tf_name = keras_name_to_tf_name_block(\n+          w.name,\n+          keras_block=keras_block,\n+          tf_block=tf_block,\n+          use_ema=use_ema,\n+          model_name_tf=model_name_tf)\n+    elif any([x in w.name for x in ['stem', 'top', 'predictions', 'probs']]):\n+      tf_name = keras_name_to_tf_name_stem_top(\n+          w.name, use_ema=use_ema, model_name_tf=model_name_tf)\n+    elif 'normalization' in w.name:\n+      print('skipping variable {}: normalization is a layer'\n+            'in keras implementation, but preprocessing in '\n+            'TF implementation.'.format(w.name))\n+      continue\n+    else:\n+      raise ValueError('{} failed to parse.'.format(w.name))\n+\n+    try:\n+      w_tf = tf.train.load_variable(path_ckpt, tf_name)\n+      if (w.value().numpy() != w_tf).any():\n+        w.assign(w_tf)\n+        changed_weights += 1\n+    except ValueError as e:\n+      if any([x in w.name for x in ['top', 'predictions', 'probs']]):\n+        warnings.warn('Fail to load top layer variable {}'\n+                      'from {} because of {}.'.format(w.name, tf_name, e))\n+      else:\n+        raise ValueError('Fail to load {} from {}'.format(w.name, tf_name))\n+\n+  total_weights = len(keras_model.weights)\n+  print('{}/{} weights updated'.format(changed_weights, total_weights))\n+  keras_model.save_weights(path_h5)\n+\n+\n+def check_match(keras_block, tf_block, keras_weight_names, tf_weight_names,\n+                model_name_tf):\n+  \"\"\"Check if the weights in h5 and ckpt match.\n+\n+  we match each name from keras_weight_names that is in keras_block\n+  and check if there is 1-1 correspondence to names from tf_weight_names\n+  that is in tf_block\n+\n+  Args:\n+    keras_block: str, the block name for keras implementation (e.g. 'block1a')\n+    tf_block: str, the block name for tf implementation (e.g. 'blocks_0')\n+    keras_weight_names: list of str, weight names in keras implementation\n+    tf_weight_names: list of str, weight names in tf implementation\n+    model_name_tf: str, the name of model in ckpt.\n+  \"\"\"\n+  names_from_keras = set()\n+  for x in keras_weight_names:\n+    if keras_block in x:\n+      y = keras_name_to_tf_name_block(\n+          x,\n+          keras_block=keras_block,\n+          tf_block=tf_block,\n+          model_name_tf=model_name_tf)\n+      names_from_keras.add(y)\n+\n+  names_from_tf = set()\n+  for x in tf_weight_names:\n+    if tf_block in x and x.split('/')[1].endswith(tf_block):\n+      names_from_tf.add(x)\n+\n+  names_missing = names_from_keras - names_from_tf\n+  if names_missing:\n+    raise ValueError('{} variables not found in checkpoint file: {}'.format(\n+        len(names_missing), names_missing))\n+\n+  names_unused = names_from_tf - names_from_keras\n+  if names_unused:\n+    warnings.warn('{} variables from checkpoint file are not used: {}'.format(\n+        len(names_unused), names_unused))\n+\n+\n+def keras_name_to_tf_name_block(keras_name,\n+                                keras_block='block1a',\n+                                tf_block='blocks_0',\n+                                use_ema=True,\n+                                model_name_tf='efficientnetv2-b0'):\n+  \"\"\"Mapping name in h5 to ckpt that belongs to a block.\n+\n+  we map name keras_name that points to a weight in h5 file\n+  to a name of weight in ckpt file.\n+\n+  Args:\n+    keras_name: str, the name of weight in the h5 file of keras implementation\n+    keras_block: str, the block name for keras implementation (e.g. 'block1a')\n+    tf_block: str, the block name for tf implementation (e.g. 'blocks_0')\n+    use_ema: Bool, use the ExponentialMovingAverage resuolt in ckpt or not\n+    model_name_tf: str, the name of model in ckpt.\n+\n+  Returns:\n+    String for the name of weight as in ckpt file.\n+\n+  Raises:\n+    ValueError if keras_block does not show up in keras_name\n+  \"\"\"\n+\n+  if keras_block not in keras_name:\n+    raise ValueError('block name {} not found in {}'.format(\n+        keras_block, keras_name))\n+\n+  # all blocks in the first group will not have expand conv and bn\n+  is_first_blocks = (keras_block[5] == '1')\n+\n+  # All blocks after 4th are fused mb conv type\n+  is_fused_mb_block = int(keras_block[5]) >= 4\n+\n+  tf_name = [model_name_tf, tf_block]\n+\n+  # depthwide conv\n+  if 'dwconv' in keras_name:\n+    tf_name.append('depthwise_conv2d')\n+    tf_name.append('depthwise_kernel')\n+\n+  # conv layers\n+  if is_first_blocks:\n+    # first blocks only have one conv2d\n+    if 'project_conv' in keras_name:\n+      tf_name.append('conv2d')\n+      tf_name.append('kernel')\n+  else:\n+    if 'project_conv' in keras_name:\n+      tf_name.append('conv2d_1')\n+      tf_name.append('kernel')\n+    elif 'expand_conv' in keras_name:\n+      tf_name.append('conv2d')\n+      tf_name.append('kernel')\n+\n+  # squeeze expansion layers\n+  if '_se_' in keras_name:\n+    if 'reduce' in keras_name:\n+      tf_name.append('se/conv2d')\n+    elif 'expand' in keras_name:\n+      tf_name.append('se/conv2d_1')\n+\n+    if 'kernel' in keras_name:\n+      tf_name.append('kernel')\n+    elif 'bias' in keras_name:\n+      tf_name.append('bias')\n+\n+  # batch normalization layers\n+  if 'bn' in keras_name:\n+    if is_first_blocks:\n+      if 'project' in keras_name:\n+        tf_name.append('tpu_batch_normalization')\n+      else:\n+        tf_name.append('tpu_batch_normalization_1')\n+    else:\n+      if is_fused_mb_block:\n+        if \"project\" in keras_name:\n+          tf_name.append(\"tpu_batch_normalization_2\")\n+        elif \"expand\" in keras_name:\n+          tf_name.append(\"tpu_batch_normalization\")\n+        else:\n+          tf_name.append(\"tpu_batch_normalization_1\")\n+    \n+      else:\n+        if 'project' in keras_name:\n+          tf_name.append('tpu_batch_normalization_1')\n+        else:\n+          tf_name.append('tpu_batch_normalization')\n+\n+    for x in ['moving_mean', 'moving_variance', 'beta', 'gamma']:\n+      if x in keras_name:\n+        tf_name.append(x)\n+  if use_ema:\n+    tf_name.append('ExponentialMovingAverage')\n+  return '/'.join(tf_name)\n+\n+if __name__ == \"__main__\":\n+  arg_to_model = {\n+    'b0': efficientnet_v2.EfficientNetV2_B0,\n+    'b1': efficientnet_v2.EfficientNetV2_B1,\n+    'b2': efficientnet_v2.EfficientNetV2_B2,\n+    'b3': efficientnet_v2.EfficientNetV2_B3,\n+    's': efficientnet_v2.EfficientNetV2_S,\n+    'm': efficientnet_v2.EfficientNetV2_M,\n+    'l': efficientnet_v2.EfficientNetV2_L,\n+  }\n+\n+  p = argparse.ArgumentParser(description='write weights from checkpoint to h5')\n+  p.add_argument(\n+      '--model',\n+      required=True,\n+      type=str,\n+      help='name of efficient model',\n+      choices=arg_to_model.keys())\n+  p.add_argument(\n+      '--notop',\n+      action='store_true',\n+      help='do not include top layers',\n+      default=False)\n+  p.add_argument('--ckpt', required=True, type=str, help='checkpoint path')\n+  p.add_argument(\n+      '--output', '-o', required=True, type=str, help='output (h5) file path')\n+  args = p.parse_args()\n+\n+  include_top = not args.notop\n+\n+  model = arg_to_model[args.model](include_top=include_top, weights=None)\n+  write_ckpt_to_h5(args.output, args.ckpt, keras_model=model)\n\n@@ -25,6 +25,7 @@ import os\n PIP_EXCLUDED_FILES = frozenset([\n     'keras/api/create_python_api_wrapper.py',\n     'keras/applications/efficientnet_weight_update_util.py',\n+    'keras/applications/efficientnet_v2_weight_update_util.py',\n     'keras/distribute/tpu_strategy_test_utils.py',\n     'keras/saving/saved_model/create_test_saved_model.py',\n     'keras/tools/pip_package/setup.py',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
