{"custom_id": "scrapy#d9e69bfb51d5ed5a08d57878b29a6e7db8ef9d15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 400 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -289,8 +289,8 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n     )\n     if cipher_string:\n         ctx = factory.getContext()\n-        # disabling TLS1.2+ because it unconditionally enables some strong ciphers\n-        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_2 | SSL_OP_NO_TLSv1_3)\n+        # disabling TLS1.3 because it unconditionally enables some strong ciphers\n+        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL_OP_NO_TLSv1_3)\n         ctx.set_cipher_list(to_bytes(cipher_string))\n     return factory\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a87ab71d1061585e41864d7283557bbe9823a91b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 40 | Files Changed: 4 | Hunks: 24 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 100 | Churn Cumulative: 4691 | Contributors (this commit): 23 | Commits (past 90d): 76 | Contributors (cumulative): 27 | DMM Complexity: 0.625\n\nDIFF:\n@@ -289,7 +289,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             self._conn_lost_deferred.callback(self._conn_lost_errors)\n \n         for stream in self.streams.values():\n-            if stream.request_sent:\n+            if stream.metadata['request_sent']:\n                 close_reason = StreamCloseReason.CONNECTION_LOST\n             else:\n                 close_reason = StreamCloseReason.INACTIVE\n\n@@ -5,14 +5,14 @@ from typing import List, Optional, Tuple, TYPE_CHECKING\n from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n-from h2.exceptions import H2Error, StreamClosedError\n+from h2.exceptions import H2Error, StreamClosedError, ProtocolError\n from hpack import HeaderTuple\n from twisted.internet.defer import Deferred, CancelledError\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n \n-from scrapy.core.http2.types import H2ResponseDict\n+from scrapy.core.http2.types import H2ResponseDict, H2StreamMetadataDict\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n@@ -100,24 +100,14 @@ class Stream:\n         self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n         self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n \n-        self.request_start_time = None\n-\n-        self.content_length = 0 if self._request.body is None else len(self._request.body)\n-\n-        # Flag to keep track whether this stream has initiated the request\n-        self.request_sent = False\n-\n-        # Flag to track whether we have logged about exceeding download warnsize\n-        self._reached_warnsize = False\n-\n-        # Each time we send a data frame, we will decrease value by the amount send.\n-        self.remaining_content_length = self.content_length\n-\n-        # Flag to keep track whether we have closed this stream\n-        self.stream_closed_local = False\n-\n-        # Flag to keep track whether the server has closed the stream\n-        self.stream_closed_server = False\n+        self.metadata: H2StreamMetadataDict = {\n+            'request_content_length': 0 if self._request.body is None else len(self._request.body),\n+            'request_sent': False,\n+            'reached_warnsize': False,\n+            'remaining_content_length': 0 if self._request.body is None else len(self._request.body),\n+            'stream_closed_local': False,\n+            'stream_closed_server': False,\n+        }\n \n         # Private variable used to build the response\n         # this response is then converted to appropriate Response class\n@@ -132,7 +122,7 @@ class Stream:\n             # Close this stream as gracefully as possible\n             # If the associated request is initiated we reset this stream\n             # else we directly call close() method\n-            if self.request_sent:\n+            if self.metadata['request_sent']:\n                 self.reset_stream(StreamCloseReason.CANCELLED)\n             else:\n                 self.close(StreamCloseReason.CANCELLED)\n@@ -160,7 +150,7 @@ class Stream:\n                 self._response['flow_controlled_size'] > self._download_warnsize\n                 or content_length_header > self._download_warnsize\n             )\n-            and not self._reached_warnsize\n+            and not self.metadata['reached_warnsize']\n         )\n \n     def get_response(self) -> Deferred:\n@@ -220,7 +210,7 @@ class Stream:\n         if self.check_request_url():\n             headers = self._get_request_headers()\n             self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)\n-            self.request_sent = True\n+            self.metadata['request_sent'] = True\n             self.send_data()\n         else:\n             # Close this stream calling the response errback\n@@ -238,7 +228,7 @@ class Stream:\n             and has initiated request already by sending HEADER frame. If not then\n             stream will raise ProtocolError (raise by h2 state machine).\n          \"\"\"\n-        if self.stream_closed_local:\n+        if self.metadata['stream_closed_local']:\n             raise StreamClosedError(self.stream_id)\n \n         # Firstly, check what the flow control window is for current stream.\n@@ -249,24 +239,24 @@ class Stream:\n \n         # We will send no more than the window size or the remaining file size\n         # of data in this call, whichever is smaller.\n-        bytes_to_send_size = min(window_size, self.remaining_content_length)\n+        bytes_to_send_size = min(window_size, self.metadata['remaining_content_length'])\n \n         # We now need to send a number of data frames.\n         while bytes_to_send_size > 0:\n             chunk_size = min(bytes_to_send_size, max_frame_size)\n \n-            data_chunk_start_id = self.content_length - self.remaining_content_length\n+            data_chunk_start_id = self.metadata['request_content_length'] - self.metadata['remaining_content_length']\n             data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]\n \n             self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n             bytes_to_send_size = bytes_to_send_size - chunk_size\n-            self.remaining_content_length = self.remaining_content_length - chunk_size\n+            self.metadata['remaining_content_length'] = self.metadata['remaining_content_length'] - chunk_size\n \n-        self.remaining_content_length = max(0, self.remaining_content_length)\n+        self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])\n \n         # End the stream if no more data needs to be send\n-        if self.remaining_content_length == 0:\n+        if self.metadata['remaining_content_length'] == 0:\n             self._protocol.conn.end_stream(self.stream_id)\n \n         # Q. What about the rest of the data?\n@@ -277,7 +267,11 @@ class Stream:\n         Send data that earlier could not be sent as we were\n         blocked behind the flow control.\n         \"\"\"\n-        if self.remaining_content_length and not self.stream_closed_server and self.request_sent:\n+        if (\n+            self.metadata['remaining_content_length']\n+            and not self.metadata['stream_closed_server']\n+            and self.metadata['request_sent']\n+        ):\n             self.send_data()\n \n     def receive_data(self, data: bytes, flow_controlled_length: int) -> None:\n@@ -290,7 +284,7 @@ class Stream:\n             return\n \n         if self._log_warnsize:\n-            self._reached_warnsize = True\n+            self.metadata['reached_warnsize'] = True\n             warning_msg = (\n                 f'Received more ({self._response[\"flow_controlled_size\"]}) bytes than download '\n                 f'warn size ({self._download_warnsize}) in request {self._request}'\n@@ -314,7 +308,7 @@ class Stream:\n             return\n \n         if self._log_warnsize:\n-            self._reached_warnsize = True\n+            self.metadata['reached_warnsize'] = True\n             warning_msg = (\n                 f'Expected response size ({expected_size}) larger than '\n                 f'download warn size ({self._download_warnsize}) in request {self._request}'\n@@ -323,18 +317,18 @@ class Stream:\n \n     def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:\n         \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n-        if self.stream_closed_local:\n+        if self.metadata['stream_closed_local']:\n             raise StreamClosedError(self.stream_id)\n \n         # Clear buffer earlier to avoid keeping data in memory for a long time\n         self._response['body'].truncate(0)\n \n-        self.stream_closed_local = True\n+        self.metadata['stream_closed_local'] = True\n         self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n         self.close(reason)\n \n     def _is_data_lost(self) -> bool:\n-        assert self.stream_closed_server\n+        assert self.metadata['stream_closed_server']\n \n         expected_size = self._response['flow_controlled_size']\n         received_body_size = int(self._response['headers'][b'Content-Length'])\n@@ -349,7 +343,7 @@ class Stream:\n     ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\n         \"\"\"\n-        if self.stream_closed_server:\n+        if self.metadata['stream_closed_server']:\n             raise StreamClosedError(self.stream_id)\n \n         if not isinstance(reason, StreamCloseReason):\n@@ -362,7 +356,7 @@ class Stream:\n         if not from_protocol:\n             self._protocol.pop_stream(self.stream_id)\n \n-        self.stream_closed_server = True\n+        self.metadata['stream_closed_server'] = True\n \n         # We do not check for Content-Length or Transfer-Encoding in response headers\n         # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n@@ -402,7 +396,10 @@ class Stream:\n \n         elif reason is StreamCloseReason.RESET:\n             self._deferred_response.errback(ResponseFailed([\n-                Failure(f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM')\n+                Failure(\n+                    f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM',\n+                    ProtocolError\n+                )\n             ]))\n \n         elif reason is StreamCloseReason.CONNECTION_LOST:\n\n@@ -31,6 +31,29 @@ class H2ConnectionMetadataDict(TypedDict):\n     default_download_warnsize: int\n \n \n+class H2StreamMetadataDict(TypedDict):\n+    \"\"\"Metadata of an HTTP/2 connection stream\n+    initialized when stream is instantiated\n+    \"\"\"\n+\n+    request_content_length: int\n+\n+    # Flag to keep track whether the stream has initiated the request\n+    request_sent: bool\n+\n+    # Flag to track whether we have logged about exceeding download warnsize\n+    reached_warnsize: bool\n+\n+    # Each time we send a data frame, we will decrease value by the amount send.\n+    remaining_content_length: int\n+\n+    # Flag to keep track whether we have closed this stream\n+    stream_closed_local: bool\n+\n+    # Flag to keep track whether the server has closed the stream\n+    stream_closed_server: bool\n+\n+\n class H2ResponseDict(TypedDict):\n     # Data received frame by frame from the server is appended\n     # and passed to the response Deferred when completely received.\n\n@@ -232,7 +232,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n-        self.host = u'localhost'\n+        self.host = 'localhost'\n         if self.scheme == 'https':\n             # Using WrappingFactory do not enable HTTP/2 failing all the\n             # tests with H2DownloadHandler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3233b79deee6ad283ed4316135e30ab774c1078", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1715 | Contributors (this commit): 2 | Commits (past 90d): 42 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class H2DownloadHandler:\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings, crawler)\n \n-    def download_request(self, request: Request, spider: Spider):\n+    def download_request(self, request: Request, spider: Spider) -> Deferred:\n         agent = ScrapyH2Agent(\n             context_factory=self._context_factory,\n             pool=self._pool,\n\n@@ -5,7 +5,7 @@ from typing import List, Optional, Tuple, TYPE_CHECKING\n from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n-from h2.exceptions import H2Error, StreamClosedError, ProtocolError\n+from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n from hpack import HeaderTuple\n from twisted.internet.defer import Deferred, CancelledError\n from twisted.internet.error import ConnectionClosed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30eb005639b77ea94e8859e1d041dfe53048cf77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 2926 | Contributors (this commit): 2 | Commits (past 90d): 71 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,7 +68,7 @@ class H2ConnectionPool:\n \n         # Now as we have established a proper HTTP/2 connection\n         # we fire all the deferred's with the connection instance\n-        pending_requests = self._pending_requests.pop(key)\n+        pending_requests = self._pending_requests.pop(key, None)\n         while pending_requests:\n             d = pending_requests.popleft()\n             d.callback(conn)\n\n@@ -39,7 +39,7 @@ class InvalidNegotiatedProtocol(H2Error):\n         self.negotiated_protocol = negotiated_protocol\n \n     def __str__(self) -> str:\n-        return f'InvalidHostname: Expected h2 as negotiated protocol, received {self.negotiated_protocol!r}'\n+        return f'InvalidNegotiatedProtocol: Expected h2 as negotiated protocol, received {self.negotiated_protocol!r}'\n \n \n class RemoteTerminatedConnection(H2Error):\n\n@@ -31,6 +31,9 @@ class InactiveStreamClosed(ConnectionClosed):\n     def __init__(self, request: Request):\n         self.request = request\n \n+    def __str__(self) -> str:\n+        return f'InactiveStreamClosed: Connection was closed without sending the request {self.request!r}'\n+\n \n class InvalidHostname(H2Error):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f00666d749b4a736536bafc786e63be06113676", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 46 | Files Changed: 3 | Hunks: 15 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 95 | Churn Cumulative: 1014 | Contributors (this commit): 9 | Commits (past 90d): 23 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,12 @@\n import warnings\n \n from OpenSSL import SSL\n+from twisted.internet._sslverify import _setAcceptableProtocols\n from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n from zope.interface.declarations import implementer\n+from zope.interface.verify import verifyObject\n \n from scrapy.core.downloader.tls import DEFAULT_CIPHERS, openssl_methods, ScrapyClientTLSOptions\n from scrapy.utils.misc import create_instance, load_object\n@@ -84,8 +86,8 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n     The default OpenSSL method is ``TLS_METHOD`` (also called\n     ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n     \"\"\"\n-    def creatorForNetloc(self, hostname, port):\n \n+    def creatorForNetloc(self, hostname, port):\n         # trustRoot set to platformTrust() will use the platform's root CAs.\n         #\n         # This means that a website like https://www.cacert.org will be rejected\n@@ -97,6 +99,24 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n         )\n \n \n+@implementer(IPolicyForHTTPS)\n+class AcceptableProtocolsContextFactory:\n+    \"\"\"Context factory to used to override the acceptable protocols\n+    to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN\n+    negotiation.\n+    \"\"\"\n+\n+    def __init__(self, context_factory, acceptable_protocols):\n+        verifyObject(IPolicyForHTTPS, context_factory)\n+        self._wrapped_context_factory = context_factory\n+        self._acceptable_protocols = acceptable_protocols\n+\n+    def creatorForNetloc(self, hostname, port):\n+        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n+        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n+        return options\n+\n+\n def load_context_factory_from_settings(settings, crawler):\n     ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n     context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n\n@@ -1,16 +1,15 @@\n import warnings\n from time import time\n-from typing import Optional, Tuple\n+from typing import Optional\n from urllib.parse import urldefrag\n \n-from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n-from twisted.web.client import BrowserLikePolicyForHTTPS, URI\n+from twisted.web.client import URI\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n-from scrapy.core.http2.agent import H2Agent, H2ConnectionPool\n+from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -41,30 +40,6 @@ class H2DownloadHandler:\n         self._pool.close_connections()\n \n \n-class ScrapyProxyH2Agent(H2Agent):\n-    def __init__(\n-        self, reactor: ReactorBase,\n-        proxy_uri: URI, pool: H2ConnectionPool,\n-        context_factory=BrowserLikePolicyForHTTPS(),\n-        connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n-    ) -> None:\n-        super(ScrapyProxyH2Agent, self).__init__(\n-            reactor=reactor,\n-            pool=pool,\n-            context_factory=context_factory,\n-            connect_timeout=connect_timeout,\n-            bind_address=bind_address\n-        )\n-        self._proxy_uri = proxy_uri\n-\n-    def get_endpoint(self, uri: URI):\n-        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n-\n-    def get_key(self, uri: URI) -> Tuple:\n-        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n-        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n-\n-\n class ScrapyH2Agent:\n     _Agent = H2Agent\n     _ProxyAgent = ScrapyProxyH2Agent\n\n@@ -2,17 +2,14 @@ from collections import deque\n from typing import Deque, Dict, List, Tuple, Optional\n \n from twisted.internet import defer\n-from twisted.internet._sslverify import _setAcceptableProtocols, ClientTLSOptions\n from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n from twisted.internet.endpoints import HostnameEndpoint\n from twisted.python.failure import Failure\n from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _StandardEndpointFactory\n from twisted.web.error import SchemeNotSupported\n-from twisted.web.iweb import IPolicyForHTTPS\n-from zope.interface import implementer\n-from zope.interface.verify import verifyObject\n \n+from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\n from scrapy.core.http2.protocol import H2ClientProtocol, H2ClientFactory\n from scrapy.http.request import Request\n from scrapy.settings import Settings\n@@ -96,19 +93,6 @@ class H2ConnectionPool:\n             conn.transport.abortConnection()\n \n \n-@implementer(IPolicyForHTTPS)\n-class AcceptableProtocolsContextFactory:\n-    def __init__(self, context_factory, acceptable_protocols: List[bytes]) -> None:\n-        verifyObject(IPolicyForHTTPS, context_factory)\n-        self._wrapped_context_factory = context_factory\n-        self._acceptable_protocols = acceptable_protocols\n-\n-    def creatorForNetloc(self, hostname, port) -> ClientTLSOptions:\n-        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n-        _setAcceptableProtocols(options._ctx, self._acceptable_protocols)\n-        return options\n-\n-\n class H2Agent:\n     def __init__(\n         self, reactor: ReactorBase, pool: H2ConnectionPool,\n@@ -144,3 +128,27 @@ class H2Agent:\n         d = self._pool.get_connection(key, uri, endpoint)\n         d.addCallback(lambda conn: conn.request(request, spider))\n         return d\n+\n+\n+class ScrapyProxyH2Agent(H2Agent):\n+    def __init__(\n+        self, reactor: ReactorBase,\n+        proxy_uri: URI, pool: H2ConnectionPool,\n+        context_factory=BrowserLikePolicyForHTTPS(),\n+        connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n+    ) -> None:\n+        super(ScrapyProxyH2Agent, self).__init__(\n+            reactor=reactor,\n+            pool=pool,\n+            context_factory=context_factory,\n+            connect_timeout=connect_timeout,\n+            bind_address=bind_address\n+        )\n+        self._proxy_uri = proxy_uri\n+\n+    def get_endpoint(self, uri: URI):\n+        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n+\n+    def get_key(self, uri: URI) -> Tuple:\n+        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n+        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42383cc267393c3c5fb89c9108759125939ab3e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 4 | Files Changed: 7 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 71 | Churn Cumulative: 5075 | Contributors (this commit): 71 | Commits (past 90d): 27 | Contributors (cumulative): 136 | DMM Complexity: 1.0\n\nDIFF:\n@@ -340,5 +340,5 @@ class CrawlerProcess(CrawlerRunner):\n \n     def _handle_twisted_reactor(self):\n         if self.settings.get(\"TWISTED_REACTOR\"):\n-            install_reactor(self.settings[\"TWISTED_REACTOR\"])\n+            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n         super()._handle_twisted_reactor()\n\n@@ -19,6 +19,8 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n+ASYNCIO_EVENT_LOOP = None\n+\n AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n AUTOTHROTTLE_MAX_DELAY = 60.0\n\n@@ -150,6 +150,13 @@ def log_scrapy_info(settings):\n     logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n     from twisted.internet import reactor\n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n+    from twisted.internet import asyncioreactor\n+    if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):\n+        logger.debug(\n+            \"Using asyncio event loop: %s.%s\",\n+            reactor._asyncioEventloop.__module__,\n+            reactor._asyncioEventloop.__class__.__name__,\n+        )\n \n \n class StreamLogger:\n\n@@ -50,13 +50,19 @@ class CallLaterOnce:\n         return self._func(*self._a, **self._kw)\n \n \n-def install_reactor(reactor_path):\n+def install_reactor(reactor_path, event_loop_path=None):\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n-    import path.\"\"\"\n+    import path. Also installs the asyncio event loop with the specified import\n+    path if the asyncio reactor is enabled\"\"\"\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n-            asyncioreactor.install(asyncio.get_event_loop())\n+            if event_loop_path is not None:\n+                event_loop_class = load_object(event_loop_path)\n+                event_loop = event_loop_class()\n+            else:\n+                event_loop = asyncio.new_event_loop()\n+            asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n         installer_path = module + [\"install\"]\n\n@@ -0,0 +1,17 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\"\n+})\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -16,6 +16,7 @@ from tempfile import mkdtemp\n from threading import Timer\n from unittest import skipIf\n \n+from pytest import mark\n from twisted.trial import unittest\n \n import scrapy\n@@ -570,6 +571,28 @@ class BadSpider(scrapy.Spider):\n         log = self.get_log(self.debug_log_spider, args=[])\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n+    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    def test_custom_asyncio_loop_enabled_true(self):\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s',\n+            'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor',\n+            '-s',\n+            'ASYNCIO_EVENT_LOOP=uvloop.Loop',\n+        ])\n+        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+\n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n+    def test_custom_asyncio_loop_enabled_false(self):\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n+        ])\n+        import asyncio\n+        loop = asyncio.new_event_loop()\n+        self.assertIn(\"Using asyncio event loop: %s.%s\" % (loop.__module__, loop.__class__.__name__), log)\n+\n     def test_output(self):\n         spider_code = \"\"\"\n import scrapy\n\n@@ -345,6 +345,14 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n+    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    def test_custom_loop_asyncio(self):\n+        log = self.run_script(\"asyncio_custom_loop.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerRunner')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a57db9e3024fe1426021b8b692a48f4c8db82a77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 583 | Contributors (this commit): 26 | Commits (past 90d): 13 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ if has_environment_marker_platform_impl_support():\n     ]\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n         # Earlier lxml versions are affected by\n-        # https://bitbucket.org/pypy/pypy/issues/2498/cython-on-pypy-3-dict-object-has-no,\n+        # https://foss.heptapod.net/pypy/pypy/-/issues/2498,\n         # which was fixed in Cython 0.26, released on 2017-06-19, and used to\n         # generate the C headers of lxml release tarballs published since then, the\n         # first of which was:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d68aab992e435aa1c88061e83d03e57e7d31533d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 38 | Churn Cumulative: 635 | Contributors (this commit): 26 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import os\n import re\n import hashlib\n import warnings\n+from collections import deque\n from contextlib import contextmanager\n from importlib import import_module\n from pkgutil import iter_modules\n@@ -184,6 +185,22 @@ def set_environ(**kwargs):\n                 os.environ[k] = v\n \n \n+def walk_callable(node):\n+    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n+    functions defined within the node.\n+    \"\"\"\n+    todo = deque([node])\n+    walked_func_def = False\n+    while todo:\n+        node = todo.popleft()\n+        if isinstance(node, ast.FunctionDef):\n+            if walked_func_def:\n+                continue\n+            walked_func_def = True\n+        todo.extend(ast.iter_child_nodes(node))\n+        yield node\n+\n+\n _generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n \n \n@@ -201,7 +218,7 @@ def is_generator_with_return_value(callable):\n \n     if inspect.isgeneratorfunction(callable):\n         tree = ast.parse(dedent(inspect.getsource(callable)))\n-        for node in ast.walk(tree):\n+        for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n                 _generator_callbacks_cache[callable] = True\n                 return _generator_callbacks_cache[callable]\n\n@@ -29,9 +29,28 @@ class UtilsMiscPy3TestCase(unittest.TestCase):\n             yield 1\n             yield from g()\n \n+        def m():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+\n+        def n():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+            return 2\n+\n         assert is_generator_with_return_value(f)\n         assert is_generator_with_return_value(g)\n         assert not is_generator_with_return_value(h)\n         assert not is_generator_with_return_value(i)\n         assert not is_generator_with_return_value(j)\n         assert not is_generator_with_return_value(k)  # not recursive\n+        assert not is_generator_with_return_value(m)\n+        assert is_generator_with_return_value(n)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e90be0d8a5e3c20ab6aced22ccac59a876db8c99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 25 | Churn Cumulative: 503 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import os\n \n+from pytest import mark\n from twisted.trial import unittest\n \n from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml\n@@ -149,6 +150,26 @@ class XmliterTestCase(unittest.TestCase):\n         self.assertEqual(node.xpath('id/text()').getall(), [])\n         self.assertEqual(node.xpath('price/text()').getall(), [])\n \n+    def test_xmliter_namespaced_nodename(self):\n+        body = b\"\"\"\n+            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+            <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n+                <channel>\n+                <title>My Dummy Company</title>\n+                <link>http://www.mydummycompany.com</link>\n+                <description>This is a dummy company. We do nothing.</description>\n+                <item>\n+                    <title>Item 1</title>\n+                    <description>This is item 1</description>\n+                    <link>http://www.mydummycompany.com/items/1</link>\n+                    <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>\n+                    <g:id>ITEM_1</g:id>\n+                    <g:price>400</g:price>\n+                </item>\n+                </channel>\n+            </rss>\n+        \"\"\"\n+        response = XmlResponse(url='http://mydummycompany.com', body=body)\n         my_iter = self.xmliter(response, 'g:image_link')\n         node = next(my_iter)\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n@@ -187,6 +208,10 @@ class XmliterTestCase(unittest.TestCase):\n class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n+    @mark.xfail(reason='known bug of the current implementation')\n+    def test_xmliter_namespaced_nodename(self):\n+        super().test_xmliter_namespaced_nodename()\n+\n     def test_xmliter_iterate_namespace(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#afd3a4d116809ecf4c334657c5150c157aa9465b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 10 | Churn Cumulative: 392 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,7 +41,15 @@ def xmliter(obj, nodename):\n \n     r = re.compile(r'<%(np)s[\\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)\n     for match in r.finditer(text):\n-        nodetext = document_header + match.group().replace(nodename, '%s %s' % (nodename, ' '.join(namespaces.values())), 1) + header_end\n+        nodetext = (\n+            document_header\n+            + match.group().replace(\n+                nodename,\n+                '%s %s' % (nodename, ' '.join(namespaces.values())),\n+                1\n+            )\n+            + header_end\n+        )\n         yield Selector(text=nodetext, type='xml')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1250177dc1c486517b9ca8ed136162533014da7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 463 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,8 +3,8 @@ import gc\n import operator\n import platform\n import unittest\n+from datetime import datetime\n from itertools import count\n-from sys import version_info\n from warnings import catch_warnings\n \n from scrapy.utils.python import (\n@@ -216,15 +216,15 @@ class UtilsPythonTestCase(unittest.TestCase):\n             self.assertEqual(get_func_args(str.split), [])\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n-        else:\n-            self.assertEqual(\n-                get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n-            self.assertEqual(\n-                get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n-            if version_info < (3, 6):\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n-            else:\n+        elif platform.python_implementation() == 'PyPy':\n+            self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n+            self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n+\n+            build_date = datetime.strptime(platform.python_build()[1], '%b %d %Y')\n+            if build_date >= datetime(2020, 4, 7):  # PyPy 3.6-v7.3.1\n                 self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n+            else:\n+                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1432161477673152f4d2f95cd32829a81ffe3f70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 585 | Contributors (this commit): 26 | Commits (past 90d): 14 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'h2>=3.2.0',\n-    'typing-extensions>=3.7',\n+    'typing-extensions>=3.7.4',\n ]\n extras_require = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d8ec9d44fa2201986da98c3f5f7c3c7f1ecfb56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 4127 | Contributors (this commit): 33 | Commits (past 90d): 81 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -394,13 +394,14 @@ class ScrapyAgent:\n         fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n \n         if maxsize and expected_size > maxsize:\n-            error_msg = (\"Cancelling download of %(url)s: expected response \"\n-                         \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n-            error_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n+            warning_msg = (\"Expected response size (%(size)s) larger than \"\n+                           \"download max size (%(maxsize)s) in request %(request)s.\")\n+            warning_args = {'request': request, 'size': expected_size, 'maxsize': maxsize}\n+\n+            logger.warning(warning_msg, warning_args)\n \n-            logger.error(error_msg, error_args)\n             txresponse._transport._producer.loseConnection()\n-            raise defer.CancelledError(error_msg % error_args)\n+            raise defer.CancelledError(warning_msg % warning_args)\n \n         if warnsize and expected_size > warnsize:\n             logger.warning(\"Expected response size (%(size)s) larger than \"\n@@ -523,7 +524,7 @@ class _ResponseReader(protocol.Protocol):\n                 self._finish_response(flags=[\"download_stopped\"], failure=failure)\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n-            logger.error(\"Received (%(bytes)s) bytes larger than download \"\n+            logger.warning(\"Received (%(bytes)s) bytes larger than download \"\n                            \"max size (%(maxsize)s) in request %(request)s.\",\n                            {'bytes': self._bytes_received,\n                             'maxsize': self._maxsize,\n\n@@ -410,7 +410,7 @@ class Http11TestCase(HttpTestCase):\n             request = Request(self.getURL('largechunkedfile'))\n \n             def check(logger):\n-                logger.error.assert_called_once_with(mock.ANY, mock.ANY)\n+                logger.warning.assert_called_once_with(mock.ANY, mock.ANY)\n \n             d = self.download_request(request, Spider('foo', download_maxsize=1500))\n             yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b3881d65e12e7e0ac5f70c0f33e45dbab546c5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2198 | Contributors (this commit): 27 | Commits (past 90d): 6 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -394,9 +394,9 @@ class ScrapyAgent:\n         fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n \n         if maxsize and expected_size > maxsize:\n-            warning_msg = (\"Expected response size (%(size)s) larger than \"\n-                           \"download max size (%(maxsize)s) in request %(request)s.\")\n-            warning_args = {'request': request, 'size': expected_size, 'maxsize': maxsize}\n+            warning_msg = (\"Cancelling download of %(url)s: expected response \"\n+                           \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n+            warning_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n \n             logger.warning(warning_msg, warning_args)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b84a65a6b2d391fdd9c49426d748751a03351f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 30/19 | Churn Δ: 86 | Churn Cumulative: 1385 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 32 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in topics/media-pipeline.rst\n import functools\n import hashlib\n import six\n+import warnings\n \n try:\n     from cStringIO import StringIO as BytesIO\n@@ -19,6 +20,7 @@ from scrapy.utils.python import to_bytes, get_func_args\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy.exceptions import DropItem\n+from scrapy.exceptions import ScrapyDeprecationWarning\n #TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.pipelines.files import FileException, FilesPipeline\n \n@@ -132,8 +134,6 @@ class ImagesPipeline(FilesPipeline):\n         if self._deprecated_convert_image is None:\n             self._deprecated_convert_image = 'response_body' not in get_func_args(self.convert_image)\n             if self._deprecated_convert_image:\n-                from scrapy.exceptions import ScrapyDeprecationWarning\n-                import warnings\n                 warnings.warn('ImagesPipeline.convert_image() method overriden in a deprecated way, '\n                               'overriden method does not accept response_body argument.',\n                               category=ScrapyDeprecationWarning, stacklevel=1)\n@@ -153,9 +153,7 @@ class ImagesPipeline(FilesPipeline):\n             yield thumb_path, thumb_image, thumb_buf\n \n     def convert_image(self, image, size=None, response_body=None):\n-        if not response_body:\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n+        if response_body is None:\n             warnings.warn('ImagesPipeline.convert_image() method called in a deprecated way, '\n                           'method called without response_body argument.',\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n@@ -175,7 +173,7 @@ class ImagesPipeline(FilesPipeline):\n         if size:\n             image = image.copy()\n             image.thumbnail(size, Image.ANTIALIAS)\n-        elif response_body and image.format == 'JPEG':\n+        elif response_body is not None and image.format == 'JPEG':\n             return image, response_body\n                 \n         buf = BytesIO()\n@@ -193,8 +191,6 @@ class ImagesPipeline(FilesPipeline):\n     def file_path(self, request, response=None, info=None):\n         ## start of deprecation warning block (can be removed in the future)\n         def _warn():\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n             warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n                           'please use file_path(request, response=None, info=None) instead',\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n@@ -221,8 +217,6 @@ class ImagesPipeline(FilesPipeline):\n     def thumb_path(self, request, thumb_id, response=None, info=None):\n         ## start of deprecation warning block (can be removed in the future)\n         def _warn():\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n             warnings.warn('ImagesPipeline.thumb_key(url) method is deprecated, please use '\n                           'thumb_path(request, thumb_id, response=None, info=None) instead',\n                           category=ScrapyDeprecationWarning, stacklevel=1)\n\n@@ -75,41 +75,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n-    def test_convert_image(self):\n-        # tests for old API\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            SIZE = (100, 100)\n-            # straigh forward case: RGB and JPEG\n-            COLOUR = (0, 127, 255)\n-            im, _ = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n-            self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n-\n-            # check that thumbnail keep image ratio\n-            thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n-            self.assertEqual(thumbnail.mode, 'RGB')\n-            self.assertEqual(thumbnail.size, (10, 10))\n-\n-            # transparency case: RGBA and PNG\n-            COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n-            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n-\n-            # transparency case with palette: P and PNG\n-            COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n-            im = im.convert('P')\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, 'RGB')\n-            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n-\n-            # ensure that we recieved deprecation warnings\n-            self.assertTrue(len([warning for warning in w if 'ImagesPipeline.convert_image() method called in a deprecated way' in str(warning.message)]) == 4)\n-\n+    def test_convert_image_new(self):\n         # tests for new API\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\n@@ -207,6 +173,42 @@ class DeprecatedImagesPipelineTestCase(unittest.TestCase):\n             self.assertEqual(len(w), 1)\n             self.assertTrue('thumb_key(url) method is deprecated' in str(w[-1].message))\n \n+    def test_overriden_convert_image_method(self):\n+        self.init_pipeline(ImagesPipeline)\n+        # tests for old API\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter('always')\n+            SIZE = (100, 100)\n+            # straigh forward case: RGB and JPEG\n+            COLOUR = (0, 127, 255)\n+            im, _ = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n+            converted, _ = self.pipeline.convert_image(im)\n+            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n+\n+            # check that thumbnail keep image ratio\n+            thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n+            self.assertEqual(thumbnail.mode, 'RGB')\n+            self.assertEqual(thumbnail.size, (10, 10))\n+\n+            # transparency case: RGBA and PNG\n+            COLOUR = (0, 127, 255, 50)\n+            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+            converted, _ = self.pipeline.convert_image(im)\n+            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n+\n+            # transparency case with palette: P and PNG\n+            COLOUR = (0, 127, 255, 50)\n+            im, _ = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+            im = im.convert('P')\n+            converted, _ = self.pipeline.convert_image(im)\n+            self.assertEqual(converted.mode, 'RGB')\n+            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n+\n+            # ensure that we recieved deprecation warnings\n+            self.assertTrue(len([warning for warning in w if 'ImagesPipeline.convert_image() method called in a deprecated way' in str(warning.message)]) == 4)\n+\n     def tearDown(self):\n         rmtree(self.tempdir)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
