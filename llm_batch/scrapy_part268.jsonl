{"custom_id": "scrapy#df00389c16fc8e74c5ab7737e5bffcbde6cc011b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 65 | Files Changed: 36 | Hunks: 64 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 133 | Churn Cumulative: 10680 | Contributors (this commit): 111 | Commits (past 90d): 106 | Contributors (cumulative): 342 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from six.moves import copyreg\n+import copyreg\n \n \n # Undo what Twisted's perspective broker adds to pickle register\n\n@@ -1,8 +1,7 @@\n import sys\n import time\n import subprocess\n-\n-from six.moves.urllib.parse import urlencode\n+from urllib.parse import urlencode\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n\n@@ -30,7 +30,7 @@ In case of status 200 request, response.headers will come with two keys:\n \n import re\n from io import BytesIO\n-from six.moves.urllib.parse import unquote\n+from urllib.parse import unquote\n \n from twisted.internet import reactor\n from twisted.protocols.ftp import FTPClient, CommandFailed\n\n@@ -2,10 +2,10 @@\n \n import re\n import logging\n+import warnings\n from io import BytesIO\n from time import time\n-import warnings\n-from six.moves.urllib.parse import urldefrag\n+from urllib.parse import urldefrag\n \n from zope.interface import implementer\n from twisted.internet import defer, reactor, protocol\n\n@@ -1,4 +1,4 @@\n-from six.moves.urllib.parse import unquote\n+from urllib.parse import unquote\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -1,5 +1,5 @@\n from time import time\n-from six.moves.urllib.parse import urlparse, urlunparse, urldefrag\n+from urllib.parse import urlparse, urlunparse, urldefrag\n \n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n\n@@ -1,7 +1,6 @@\n import base64\n-from six.moves.urllib.parse import unquote, urlunparse\n-from six.moves.urllib.request import getproxies, proxy_bypass\n-from urllib.request import _parse_proxy\n+from urllib.parse import unquote, urlunparse\n+from urllib.request import getproxies, proxy_bypass, _parse_proxy\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -1,5 +1,5 @@\n import logging\n-from six.moves.urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin, urlparse\n \n from w3lib.url import safe_url_string\n \n@@ -7,6 +7,7 @@ from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -7,15 +7,16 @@ import io\n import sys\n import pprint\n import marshal\n-import six\n-from six.moves import cPickle as pickle\n+import warnings\n+from import pickle\n from xml.sax.saxutils import XMLGenerator\n \n+import six\n+\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n from scrapy.item import BaseItem\n from scrapy.exceptions import ScrapyDeprecationWarning\n-import warnings\n \n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n\n@@ -10,7 +10,7 @@ import logging\n import posixpath\n from tempfile import NamedTemporaryFile\n from datetime import datetime\n-from six.moves.urllib.parse import urlparse, unquote\n+from urllib.parse import urlparse, unquote\n from ftplib import FTP\n \n from zope.interface import Interface, implementer\n\n@@ -1,13 +1,13 @@\n import gzip\n import logging\n import os\n+import pickle\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from time import time\n from warnings import warn\n from weakref import WeakKeyDictionary\n \n-from six.moves import cPickle as pickle\n from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n\n@@ -1,10 +1,11 @@\n import os\n-from six.moves import cPickle as pickle\n+import pickle\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n+\n class SpiderState(object):\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n\n@@ -1,7 +1,6 @@\n import time\n-from six.moves.http_cookiejar import (\n-    CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n-)\n+from http.cookiejar import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n+\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n\n@@ -5,10 +5,10 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import six\n-from six.moves.urllib.parse import urljoin, urlencode\n+from urllib.parse import urljoin, urlencode\n \n import lxml.html\n+import six\n from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -4,7 +4,7 @@ This module implements the XmlRpcRequest class which is a more convenient class\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-from six.moves import xmlrpc_client as xmlrpclib\n+import xmlrpc.client as xmlrpclib\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n\n@@ -4,7 +4,7 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-from six.moves.urllib.parse import urljoin\n+from urllib.parse import urljoin\n \n from scrapy.http.request import Request\n from scrapy.http.headers import Headers\n\n@@ -5,10 +5,10 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import six\n-from six.moves.urllib.parse import urljoin\n+from urllib.parse import urljoin\n \n import parsel\n+import six\n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from w3lib.html import strip_html5_whitespace\n\n@@ -6,8 +6,8 @@ This package contains a collection of Link Extractors.\n For more info see docs/topics/link-extractors.rst\n \"\"\"\n import re\n+from urllib.parse import urlparse\n \n-from six.moves.urllib.parse import urlparse\n from parsel.csstranslator import HTMLTranslator\n from w3lib.url import canonicalize_url\n \n\n@@ -2,10 +2,10 @@\n HTMLParser-based link extractor\n \"\"\"\n import warnings\n-import six\n-from six.moves.html_parser import HTMLParser\n-from six.moves.urllib.parse import urljoin\n+from html.parser import HTMLParser\n+from urllib.parse import urljoin\n \n+import six\n from w3lib.url import safe_url_string\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -1,9 +1,9 @@\n \"\"\"\n Link extractor based on lxml.html\n \"\"\"\n-import six\n-from six.moves.urllib.parse import urljoin\n+from urllib.parse import urljoin\n \n+import six\n import lxml.etree as etree\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url\n\n@@ -1,11 +1,12 @@\n import re\n-from six.moves.urllib.parse import urljoin\n+from urllib.parse import urljoin\n \n from w3lib.html import remove_tags, replace_entities, replace_escape_chars, get_base_url\n \n from scrapy.link import Link\n from .sgml import SgmlLinkExtractor\n \n+\n linkre = re.compile(\n         \"<a\\s.*?href=(\\\"[.#]+?\\\"|\\'[.#]+?\\'|[^\\s]+?)(>|\\s.*?>)(.*?)<[/ ]?a>\",\n         re.DOTALL | re.IGNORECASE)\n\n@@ -1,11 +1,11 @@\n \"\"\"\n SGMLParser-based Link extractors\n \"\"\"\n-import six\n-from six.moves.urllib.parse import urljoin\n import warnings\n+from urllib.parse import urljoin\n from sgmllib import SGMLParser\n \n+import six\n from w3lib.url import safe_url_string, canonicalize_url\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -3,21 +3,21 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n-from io import BytesIO\n import logging\n-\n-from email.utils import COMMASPACE, formatdate\n-from six.moves.email_mime_multipart import MIMEMultipart\n-from six.moves.email_mime_text import MIMEText\n-from six.moves.email_mime_base import MIMEBase\n-from email.mime.nonmultipart import MIMENonMultipart\n from email import encoders as Encoders\n+from email.mime.base import MIMEBase\n+from email.mime.multipart import MIMEMultipart\n+from email.mime.nonmultipart import MIMENonMultipart\n+from email.mime.text import MIMEText\n+from email.utils import COMMASPACE, formatdate\n+from io import BytesIO\n \n from twisted.internet import defer, reactor, ssl\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -5,17 +5,17 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n-from io import BytesIO\n+import logging\n import mimetypes\n import os\n import os.path\n import time\n-import logging\n-from email.utils import parsedate_tz, mktime_tz\n-from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n-import six\n+from email.utils import parsedate_tz, mktime_tz\n+from io import BytesIO\n+from urllib.parse import urlparse\n \n+import six\n from twisted.internet import defer, threads\n \n from scrapy.pipelines.media import MediaPipeline\n\n@@ -53,7 +53,7 @@ class RobotParser(with_metaclass(ABCMeta)):\n \n class PythonRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n-        from six.moves.urllib_robotparser import RobotFileParser\n+        from urllib.robotparser import RobotFileParser\n         self.spider = spider\n         robotstxt_body = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n         self.rp = RobotFileParser()\n\n@@ -2,8 +2,8 @@\n RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n-from six.moves.urllib.parse import urlparse\n import warnings\n+from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n \n\n@@ -3,7 +3,7 @@ Scheduler queues\n \"\"\"\n \n import marshal\n-from six.moves import cPickle as pickle\n+import pickle\n \n from queuelib import queue\n \n\n@@ -1,5 +1,6 @@\n import random\n-from six.moves.urllib.parse import urlencode\n+from urllib.parse import urlencode\n+\n from twisted.web.server import Site\n from twisted.web.resource import Resource\n from twisted.internet import reactor\n\n@@ -1,9 +1,9 @@\n import argparse\n import warnings\n from shlex import split\n+from http.cookies import SimpleCookie\n+from urllib.parse import urlparse\n \n-from six.moves.http_cookies import SimpleCookie\n-from six.moves.urllib.parse import urlparse\n from six import string_types, iteritems\n from w3lib.http import basic_auth_header\n \n\n@@ -1,8 +1,8 @@\n \"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n \n import weakref\n+from urllib.parse import urlparse\n \n-from six.moves.urllib.parse import urlparse\n \n _urlparse_cache = weakref.WeakKeyDictionary()\n def urlparse_cached(request_or_response):\n\n@@ -1,5 +1,5 @@\n import os\n-from six.moves import cPickle as pickle\n+import pickle\n import warnings\n \n from importlib import import_module\n@@ -7,8 +7,8 @@ from os.path import join, dirname, abspath, isabs, exists\n \n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import Settings\n-from scrapy.exceptions import NotConfigured\n-from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+\n \n ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n DATADIR_CFG_SECTION = 'datadir'\n\n@@ -65,7 +65,7 @@ def is_listlike(x):\n     True\n     >>> is_listlike((x for x in range(3)))\n     True\n-    >>> is_listlike(six.moves.xrange(5))\n+    >>> is_listlike(range(5))\n     True\n     \"\"\"\n     return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))\n\n@@ -5,13 +5,13 @@ scrapy.http.Request objects\n \n import hashlib\n import weakref\n-from six.moves.urllib.parse import urlunparse\n+from urllib.parse import urlunparse\n \n from w3lib.http import basic_auth_header\n-from scrapy.utils.python import to_bytes, to_unicode\n-\n from w3lib.url import canonicalize_url\n+\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_bytes, to_unicode\n \n \n _fingerprint_cache = weakref.WeakKeyDictionary()\n\n@@ -5,8 +5,9 @@ Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n \n+from urllib.parse import urljoin\n+\n import lxml.etree\n-from six.moves.urllib.parse import urljoin\n \n \n class Sitemap(object):\n\n@@ -1,4 +1,4 @@\n-from six.moves.urllib.parse import urljoin\n+from urllib.parse import urljoin\n \n from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n\n@@ -7,7 +7,7 @@ to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n import re\n-from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse, urlunparse)\n+from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ab0f189ce33f235b847f96b66a243d808b5ce1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 43 | Files Changed: 18 | Hunks: 35 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 82 | Churn Cumulative: 7302 | Contributors (this commit): 50 | Commits (past 90d): 49 | Contributors (cumulative): 149 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,10 @@\n-import sys, time, random, os, json\n-from six.moves.urllib.parse import urlencode\n+import json\n+import os\n+import random\n+import sys\n+import time\n from subprocess import Popen, PIPE\n+from urllib.parse import urlencode\n \n from OpenSSL import SSL\n from twisted.web.server import Site, NOT_DONE_YET\n\n@@ -3,7 +3,7 @@ Some spiders used for testing and benchmarking\n \"\"\"\n \n import time\n-from six.moves.urllib.parse import urlencode\n+from urllib.parse import urlencode\n \n from scrapy.spiders import Spider\n from scrapy.http import Request\n\n@@ -140,7 +140,7 @@ class CrawlTestCase(TestCase):\n     def test_unbounded_response(self):\n         # Completeness of responses without Content-Length or Transfer-Encoding\n         # can not be determined, we treat them as valid but flagged as \"partial\"\n-        from six.moves.urllib.parse import urlencode\n+        from urllib.parse import urlencode\n         query = urlencode({'raw': '''\\\n HTTP/1.1 200 OK\n Server: Apache-Coyote/1.1\n\n@@ -10,8 +10,10 @@ module with the ``runserver`` argument::\n     python test_engine.py runserver\n \"\"\"\n \n-import sys, os, re\n-from six.moves.urllib.parse import urlparse\n+import os\n+import re\n+import sys\n+from urllib.parse import urlparse\n \n from twisted.internet import reactor, defer\n from twisted.web import server, static, util\n\n@@ -1,11 +1,11 @@\n import re\n import json\n import marshal\n+import pickle\n import tempfile\n import unittest\n from io import BytesIO\n from datetime import datetime\n-from six.moves import cPickle as pickle\n \n import lxml.etree\n import six\n\n@@ -2,13 +2,13 @@ import os\n import csv\n import json\n import warnings\n-from io import BytesIO\n import tempfile\n import shutil\n import string\n+from io import BytesIO\n from unittest import mock\n-from six.moves.urllib.parse import urljoin, urlparse, quote\n-from six.moves.urllib.request import pathname2url\n+from urllib.parse import urljoin, urlparse, quote\n+from urllib.request import pathname2url\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n\n@@ -1,4 +1,4 @@\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n from unittest import TestCase\n \n from scrapy.http import Request, Response\n\n@@ -3,12 +3,10 @@ import cgi\n import unittest\n import re\n import json\n-from unittest import mock\n-from urllib.parse import unquote_to_bytes\n+import xmlrpc.client as xmlrpclib\n import warnings\n-\n-from six.moves import xmlrpc_client as xmlrpclib\n-from six.moves.urllib.parse import urlparse, parse_qs, unquote\n+from unittest import mock\n+from urllib.parse import parse_qs, unquote, unquote_to_bytes, urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_unicode\n\n@@ -4,9 +4,9 @@ import time\n from tempfile import mkdtemp\n from shutil import rmtree\n from unittest import mock\n-from six.moves.urllib.parse import urlparse\n-from six import BytesIO\n+from urllib.parse import urlparse\n \n+from six import BytesIO\n from twisted.trial import unittest\n from twisted.internet import defer\n \n\n@@ -1,15 +1,15 @@\n import json\n import os\n import time\n-\n-from six.moves.urllib.parse import urlsplit, urlunsplit\n+from urllib.parse import urlsplit, urlunsplit\n from threading import Thread\n+\n from libmproxy import controller, proxy\n from netlib import http_auth\n from testfixtures import LogCapture\n-\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n+\n from scrapy.utils.test import get_crawler\n from scrapy.http import Request\n from tests.spiders import SimpleSpider, SingleRequestSpider\n\n@@ -1,13 +1,12 @@\n from unittest import TestCase\n-\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n+import warnings\n \n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n-from scrapy.spidermiddlewares.offsite import OffsiteMiddleware\n-from scrapy.spidermiddlewares.offsite import URLWarning\n+from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, URLWarning\n from scrapy.utils.test import get_crawler\n-import warnings\n+\n \n class TestOffsiteMiddleware(TestCase):\n \n\n@@ -1,4 +1,4 @@\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n from unittest import TestCase\n import warnings\n \n\n@@ -1,4 +1,4 @@\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n import unittest\n \n \n\n@@ -192,14 +192,6 @@ class SequenceExcludeTest(unittest.TestCase):\n         self.assertIn(20, d)\n         self.assertNotIn(15, d)\n \n-    def test_six_range(self):\n-        import six.moves\n-        seq = six.moves.range(10**3, 10**6)\n-        d = SequenceExclude(seq)\n-        self.assertIn(10**2, d)\n-        self.assertIn(10**7, d)\n-        self.assertNotIn(10**4, d)\n-\n     def test_range_step(self):\n         seq = range(10, 20, 3)\n         d = SequenceExclude(seq)\n\n@@ -5,8 +5,6 @@ from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, process_chain, \\\n     process_chain_both, process_parallel, iter_errback\n \n-from six.moves import xrange\n-\n \n class MustbeDeferredTest(unittest.TestCase):\n     def test_success_function(self):\n@@ -83,7 +81,7 @@ class IterErrbackTest(unittest.TestCase):\n \n     def test_iter_errback_good(self):\n         def itergood():\n-            for x in xrange(10):\n+            for x in range(10):\n                 yield x\n \n         errors = []\n@@ -93,7 +91,7 @@ class IterErrbackTest(unittest.TestCase):\n \n     def test_iter_errback_bad(self):\n         def iterbad():\n-            for x in xrange(10):\n+            for x in range(10):\n                 if x == 5:\n                     a = 1/0\n                 yield x\n\n@@ -1,9 +1,10 @@\n import unittest\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n \n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n+\n class HttpobjUtilsTest(unittest.TestCase):\n \n     def test_urlparse_cached(self):\n\n@@ -1,12 +1,13 @@\n import os\n import unittest\n-from six.moves.urllib.parse import urlparse\n+from urllib.parse import urlparse\n \n from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (response_httprepr, open_in_browser,\n                                    get_meta_refresh, get_base_url, response_status_message)\n \n+\n __doctests__ = ['scrapy.utils.response']\n \n \n\n@@ -1,14 +1,15 @@\n # -*- coding: utf-8 -*-\n import unittest\n+from urllib.parse import urlparse\n \n import six\n-from six.moves.urllib.parse import urlparse\n \n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n                               add_http_if_no_scheme, guess_scheme,\n                               parse_url, strip_url)\n \n+\n __doctests__ = ['scrapy.utils.url']\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1aba5136939ff26503a59ca9fb9320a030aa77de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 67 | Files Changed: 20 | Hunks: 67 | Methods Changed: 34 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 121 | Churn Cumulative: 7197 | Contributors (this commit): 75 | Commits (past 90d): 57 | Contributors (cumulative): 201 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ from time import time\n from datetime import datetime\n from collections import deque\n \n-import six\n from twisted.internet import reactor, defer, task\n \n from scrapy.utils.defer import mustbe_deferred\n@@ -189,7 +188,7 @@ class Downloader(object):\n \n     def close(self):\n         self._slot_gc_loop.stop()\n-        for slot in six.itervalues(self.slots):\n+        for slot in self.slots.values():\n             slot.close()\n \n     def _slot_gc(self, age=60):\n\n@@ -1,8 +1,9 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n import logging\n+\n from twisted.internet import defer\n-import six\n+\n from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n@@ -22,7 +23,7 @@ class DownloadHandlers(object):\n         self._notconfigured = {}  # remembers failed handlers\n         handlers = without_none_values(\n             crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n-        for scheme, clspath in six.iteritems(handlers):\n+        for scheme, clspath in handlers.items():\n             self._schemes[scheme] = clspath\n             self._load_handler(scheme, skip_lazy=True)\n \n\n@@ -1,5 +1,4 @@\n import os\n-import six\n import logging\n from collections import defaultdict\n \n@@ -8,6 +7,7 @@ from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n from scrapy.utils.python import to_unicode\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -82,8 +82,10 @@ class CookiesMiddleware(object):\n \n     def _get_request_cookies(self, jar, request):\n         if isinstance(request.cookies, dict):\n-            cookie_list = [{'name': k, 'value': v} for k, v in \\\n-                    six.iteritems(request.cookies)]\n+            cookie_list = [\n+                {'name': k, 'value': v}\n+                for k, v in request.cookies.items()\n+            ]\n         else:\n             cookie_list = request.cookies\n \n\n@@ -4,16 +4,15 @@ and extract the potentially compressed responses that may arrive.\n \n import bz2\n import gzip\n-from io import BytesIO\n import zipfile\n import tarfile\n import logging\n+from io import BytesIO\n from tempfile import mktemp\n \n-import six\n-\n from scrapy.responsetypes import responsetypes\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -75,7 +74,7 @@ class DecompressionMiddleware(object):\n         if not response.body:\n             return response\n \n-        for fmt, func in six.iteritems(self._formats):\n+        for fmt, func in self._formats.items():\n             new_response = func(response)\n             if new_response:\n                 logger.debug('Decompressed response with format: %(responsefmt)s',\n\n@@ -62,9 +62,9 @@ class BaseItemExporter(object):\n             include_empty = self.export_empty_fields\n         if self.fields_to_export is None:\n             if include_empty and not isinstance(item, dict):\n-                field_iter = six.iterkeys(item.fields)\n+                field_iter = item.fields.keys()\n             else:\n-                field_iter = six.iterkeys(item)\n+                field_iter = item.keys()\n         else:\n             if include_empty:\n                 field_iter = self.fields_to_export\n@@ -326,7 +326,7 @@ class PythonItemExporter(BaseItemExporter):\n         return value\n \n     def _serialize_dict(self, value):\n-        for key, val in six.iteritems(value):\n+        for key, val in value.items():\n             key = to_bytes(key) if self.binary else key\n             yield key, self._serialize_value(val)\n \n\n@@ -5,7 +5,6 @@ See documentation in docs/topics/extensions.rst\n \"\"\"\n \n import gc\n-import six\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n@@ -28,7 +27,7 @@ class MemoryDebugger(object):\n     def spider_closed(self, spider, reason):\n         gc.collect()\n         self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n-        for cls, wdict in six.iteritems(live_refs):\n+        for cls, wdict in live_refs.items():\n             if not wdict:\n                 continue\n             self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)\n\n@@ -8,7 +8,6 @@ See documentation in docs/topics/request-response.rst\n from urllib.parse import urljoin, urlencode\n \n import lxml.html\n-import six\n from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n@@ -208,7 +207,7 @@ def _get_clickable(clickdata, form):\n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n     xpath = u'.//*' + \\\n-            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n+            u''.join(u'[@%s=\"%s\"]' % c for c in clickdata.items())\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].get('name'), el[0].get('value') or '')\n\n@@ -78,7 +78,7 @@ class DictItem(MutableMapping, BaseItem):\n     def __init__(self, *args, **kwargs):\n         self._values = {}\n         if args or kwargs:  # avoid creating dict for most common case\n-            for k, v in six.iteritems(dict(*args, **kwargs)):\n+            for k, v in dict(*args, **kwargs).items():\n                 self[k] = v\n \n     def __getitem__(self, key):\n\n@@ -5,8 +5,6 @@ See documentation in docs/topics/loaders.rst\n \"\"\"\n from collections import defaultdict\n \n-import six\n-\n from scrapy.item import Item\n from scrapy.loader.common import wrap_loader_context\n from scrapy.loader.processors import Identity\n@@ -72,7 +70,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k, v in six.iteritems(value):\n+            for k, v in value.items():\n                 self._add_value(k, v)\n         else:\n             self._add_value(field_name, value)\n@@ -82,7 +80,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k, v in six.iteritems(value):\n+            for k, v in value.items():\n                 self._replace_value(k, v)\n         else:\n             self._replace_value(field_name, value)\n\n@@ -8,14 +8,12 @@ import hashlib\n import logging\n import mimetypes\n import os\n-import os.path\n import time\n from collections import defaultdict\n from email.utils import parsedate_tz, mktime_tz\n from io import BytesIO\n from urllib.parse import urlparse\n \n-import six\n from twisted.internet import defer, threads\n \n from scrapy.pipelines.media import MediaPipeline\n@@ -29,6 +27,7 @@ from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.datatypes import CaselessDict\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -153,14 +152,14 @@ class S3FilesStore(object):\n                 Bucket=self.bucket,\n                 Key=key_name,\n                 Body=buf,\n-                Metadata={k: str(v) for k, v in six.iteritems(meta or {})},\n+                Metadata={k: str(v) for k, v in (meta or {}).items()},\n                 ACL=self.POLICY,\n                 **extra)\n         else:\n             b = self._get_boto_bucket()\n             k = b.new_key(key_name)\n             if meta:\n-                for metakey, metavalue in six.iteritems(meta):\n+                for metakey, metavalue in meta.items():\n                     k.set_metadata(metakey, str(metavalue))\n             h = self.HEADERS.copy()\n             if headers:\n@@ -201,7 +200,7 @@ class S3FilesStore(object):\n             'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',\n         })\n         extra = {}\n-        for key, value in six.iteritems(headers):\n+        for key, value in headers.items():\n             try:\n                 kwarg = mapping[key]\n             except KeyError:\n@@ -249,7 +248,7 @@ class GCSFilesStore(object):\n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         blob = self.bucket.blob(self.prefix + path)\n         blob.cache_control = self.CACHE_CONTROL\n-        blob.metadata = {k: str(v) for k, v in six.iteritems(meta or {})}\n+        blob.metadata = {k: str(v) for k, v in (meta or {}).items()}\n         return threads.deferToThread(\n             blob.upload_from_string,\n             data=buf.getvalue(),\n\n@@ -6,7 +6,6 @@ See documentation in topics/media-pipeline.rst\n import functools\n import hashlib\n from io import BytesIO\n-import six\n \n from PIL import Image\n \n@@ -126,7 +125,7 @@ class ImagesPipeline(FilesPipeline):\n         image, buf = self.convert_image(orig_image)\n         yield path, image, buf\n \n-        for thumb_id, size in six.iteritems(self.thumbs):\n+        for thumb_id, size in self.thumbs.items():\n             thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n\n@@ -5,7 +5,6 @@ based on different criteria.\n from mimetypes import MimeTypes\n from pkgutil import get_data\n from io import StringIO\n-import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n@@ -36,7 +35,7 @@ class ResponseTypes(object):\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types').decode('utf8')\n         self.mimetypes.readfp(StringIO(mimedata))\n-        for mimetype, cls in six.iteritems(self.CLASSES):\n+        for mimetype, cls in self.CLASSES.items():\n             self.classes[mimetype] = load_object(cls)\n \n     def from_mimetype(self, mimetype):\n\n@@ -317,10 +317,10 @@ class BaseSettings(MutableMapping):\n             values = json.loads(values)\n         if values is not None:\n             if isinstance(values, BaseSettings):\n-                for name, value in six.iteritems(values):\n+                for name, value in values.items():\n                     self.set(name, value, values.getpriority(name))\n             else:\n-                for name, value in six.iteritems(values):\n+                for name, value in values.items():\n                     self.set(name, value, priority)\n \n     def delete(self, name, priority='project'):\n@@ -377,7 +377,7 @@ class BaseSettings(MutableMapping):\n \n     def _to_dict(self):\n         return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n-                for k, v in six.iteritems(self)}\n+                for k, v in self.items()}\n \n     def copy_to_dict(self):\n         \"\"\"\n@@ -445,7 +445,7 @@ class Settings(BaseSettings):\n         self.setmodule(default_settings, 'default')\n         # Promote default dictionaries to BaseSettings instances for per-key\n         # priorities\n-        for name, val in six.iteritems(self):\n+        for name, val in self.items():\n             if isinstance(val, dict):\n                 self.set(name, BaseSettings(val, 'default'), 'default')\n         self.update(values, priority)\n\n@@ -1,11 +1,9 @@\n-from configparser import ConfigParser\n import os\n import sys\n import numbers\n+from configparser import ConfigParser\n from operator import itemgetter\n \n-import six\n-\n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n@@ -22,7 +20,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n     def _map_keys(compdict):\n         if isinstance(compdict, BaseSettings):\n             compbs = BaseSettings()\n-            for k, v in six.iteritems(compdict):\n+            for k, v in compdict.items():\n                 prio = compdict.getpriority(k)\n                 if compbs.getpriority(convert(k)) == prio:\n                     raise ValueError('Some paths in {!r} convert to the same '\n@@ -33,11 +31,11 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n             return compbs\n         else:\n             _check_components(compdict)\n-            return {convert(k): v for k, v in six.iteritems(compdict)}\n+            return {convert(k): v for k, v in compdict.items()}\n \n     def _validate_values(compdict):\n         \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n-        for name, value in six.iteritems(compdict):\n+        for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n                 raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                  'a real number or None instead'.format(value, name))\n@@ -53,7 +51,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n \n     _validate_values(compdict)\n     compdict = without_none_values(_map_keys(compdict))\n-    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n+    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n \n \n def arglist_to_dict(arglist):\n\n@@ -7,11 +7,8 @@ This module must not depend on any module outside the Standard Library.\n \n import copy\n import collections\n-from collections.abc import Mapping\n import warnings\n \n-import six\n-\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -151,7 +148,7 @@ class MultiValueDict(dict):\n                         self.setlistdefault(key, []).append(value)\n                 except TypeError:\n                     raise ValueError(\"MultiValueDict.update() takes either a MultiValueDict or dictionary\")\n-        for key, value in six.iteritems(kwargs):\n+        for key, value in kwargs.items():\n             self.setlistdefault(key, []).append(value)\n \n \n@@ -226,7 +223,7 @@ class CaselessDict(dict):\n         return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\n \n     def update(self, seq):\n-        seq = seq.items() if isinstance(seq, Mapping) else seq\n+        seq = seq.items() if isinstance(seq, collections.abc.Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super(CaselessDict, self).update(iseq)\n \n\n@@ -300,7 +300,7 @@ def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     dict or a list of tuples, like any dict constructor supports.\n     \"\"\"\n     d = {}\n-    for k, v in six.iteritems(dict(dct_or_tuples)):\n+    for k, v in dict(dct_or_tuples).items():\n         k = k.encode(encoding) if isinstance(k, six.text_type) else k\n         if not keys_only:\n             v = v.encode(encoding) if isinstance(v, six.text_type) else v\n@@ -345,7 +345,7 @@ def without_none_values(iterable):\n     value ``None`` have been removed.\n     \"\"\"\n     try:\n-        return {k: v for k, v in six.iteritems(iterable) if v is not None}\n+        return {k: v for k, v in iterable.items() if v is not None}\n     except AttributeError:\n         return type(iterable)((v for v in iterable if v is not None))\n \n\n@@ -1,11 +1,10 @@\n import logging\n import inspect\n \n-import six\n-\n from scrapy.spiders import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -21,7 +20,7 @@ def iter_spider_classes(module):\n     # singleton in scrapy.spider.spiders\n     from scrapy.spiders import Spider\n \n-    for obj in six.itervalues(vars(module)):\n+    for obj in vars(module).values():\n         if inspect.isclass(obj) and \\\n            issubclass(obj, Spider) and \\\n            obj.__module__ == module.__name__ and \\\n\n@@ -13,7 +13,6 @@ import weakref\n from time import time\n from operator import itemgetter\n from collections import defaultdict\n-import six\n \n \n NoneType = type(None)\n@@ -36,13 +35,13 @@ def format_live_refs(ignore=NoneType):\n     \"\"\"Return a tabular representation of tracked objects\"\"\"\n     s = \"Live References\\n\\n\"\n     now = time()\n-    for cls, wdict in sorted(six.iteritems(live_refs),\n+    for cls, wdict in sorted(live_refs.items(),\n                              key=lambda x: x[0].__name__):\n         if not wdict:\n             continue\n         if issubclass(cls, ignore):\n             continue\n-        oldest = min(six.itervalues(wdict))\n+        oldest = min(wdict.values())\n         s += \"%-30s %6d   oldest: %ds ago\\n\" % (\n             cls.__name__, len(wdict), now - oldest\n         )\n@@ -56,15 +55,15 @@ def print_live_refs(*a, **kw):\n \n def get_oldest(class_name):\n     \"\"\"Get the oldest object for a specific class name\"\"\"\n-    for cls, wdict in six.iteritems(live_refs):\n+    for cls, wdict in live_refs.items():\n         if cls.__name__ == class_name:\n             if not wdict:\n                 break\n-            return min(six.iteritems(wdict), key=itemgetter(1))[0]\n+            return min(wdict.items(), key=itemgetter(1))[0]\n \n \n def iter_all(class_name):\n     \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n-    for cls, wdict in six.iteritems(live_refs):\n+    for cls, wdict in live_refs.items():\n         if cls.__name__ == class_name:\n-            return six.iterkeys(wdict)\n+            return wdict.keys()\n\n@@ -10,7 +10,7 @@ from . import default_settings\n class SettingsGlobalFuncsTest(unittest.TestCase):\n \n     def test_get_settings_priority(self):\n-        for prio_str, prio_num in six.iteritems(SETTINGS_PRIORITIES):\n+        for prio_str, prio_num in SETTINGS_PRIORITIES.items():\n             self.assertEqual(get_settings_priority(prio_str), prio_num)\n         self.assertEqual(get_settings_priority(99), 99)\n \n@@ -148,10 +148,10 @@ class BaseSettingsTest(unittest.TestCase):\n         self.settings.setmodule(\n             'tests.test_settings.default_settings', 10)\n \n-        self.assertCountEqual(six.iterkeys(self.settings.attributes),\n-                              six.iterkeys(ctrl_attributes))\n+        self.assertCountEqual(self.settings.attributes.keys(),\n+                              ctrl_attributes.keys())\n \n-        for key in six.iterkeys(ctrl_attributes):\n+        for key in ctrl_attributes.keys():\n             attr = self.settings.attributes[key]\n             ctrl_attr = ctrl_attributes[key]\n             self.assertEqual(attr.value, ctrl_attr.value)\n@@ -227,7 +227,7 @@ class BaseSettingsTest(unittest.TestCase):\n         }\n         settings = self.settings\n         settings.attributes = {key: SettingsAttribute(value, 0) for key, value\n-                               in six.iteritems(test_configuration)}\n+                               in test_configuration.items()}\n \n         self.assertTrue(settings.getbool('TEST_ENABLED1'))\n         self.assertTrue(settings.getbool('TEST_ENABLED2'))\n\n@@ -318,7 +318,7 @@ class WebClientTestCase(unittest.TestCase):\n         def cleanup(passthrough):\n             # Clean up the server which is hanging around not doing\n             # anything.\n-            connected = list(six.iterkeys(self.wrapper.protocols))\n+            connected = list(self.wrapper.protocols.keys())\n             # There might be nothing here if the server managed to already see\n             # that the connection was lost.\n             if connected:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68bf192172e548da49613064086a6e02cf0bae54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 316 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ import sys\n import pprint\n import marshal\n import warnings\n-from import pickle\n+import pickle\n from xml.sax.saxutils import XMLGenerator\n \n import six\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce8e515fa8960d0229132ac33fc5f8a6e43bbd6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 34 | Files Changed: 14 | Hunks: 34 | Methods Changed: 28 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 68 | Churn Cumulative: 5856 | Contributors (this commit): 57 | Commits (past 90d): 55 | Contributors (cumulative): 158 | DMM Complexity: None\n\nDIFF:\n@@ -205,7 +205,7 @@ class CrawlerRunner(object):\n         return self._create_crawler(crawler_or_spidercls)\n \n     def _create_crawler(self, spidercls):\n-        if isinstance(spidercls, six.string_types):\n+        if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n\n@@ -182,7 +182,7 @@ class XmlItemExporter(BaseItemExporter):\n             for value in serialized_value:\n                 self._export_xml_field('value', value, depth=depth+1)\n             self._beautify_indent(depth=depth)\n-        elif isinstance(serialized_value, six.text_type):\n+        elif isinstance(serialized_value, str):\n             self.xg.characters(serialized_value)\n         else:\n             self.xg.characters(str(serialized_value))\n@@ -321,7 +321,7 @@ class PythonItemExporter(BaseItemExporter):\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n         encode_func = to_bytes if self.binary else to_unicode\n-        if isinstance(value, (six.text_type, bytes)):\n+        if isinstance(value, (str, bytes)):\n             return encode_func(value, encoding=self.encoding)\n         return value\n \n\n@@ -19,7 +19,7 @@ class Headers(CaselessDict):\n         \"\"\"Normalize values to bytes\"\"\"\n         if value is None:\n             value = []\n-        elif isinstance(value, (six.text_type, bytes)):\n+        elif isinstance(value, (str, bytes)):\n             value = [value]\n         elif not hasattr(value, '__iter__'):\n             value = [value]\n@@ -29,10 +29,10 @@ class Headers(CaselessDict):\n     def _tobytes(self, x):\n         if isinstance(x, bytes):\n             return x\n-        elif isinstance(x, six.text_type):\n+        elif isinstance(x, str):\n             return x.encode(self.encoding)\n         elif isinstance(x, int):\n-            return six.text_type(x).encode(self.encoding)\n+            return str(x).encode(self.encoding)\n         else:\n             raise TypeError('Unsupported value type: {}'.format(type(x)))\n \n\n@@ -60,7 +60,7 @@ class Request(object_ref):\n         return self._url\n \n     def _set_url(self, url):\n-        if not isinstance(url, six.string_types):\n+        if not isinstance(url, str):\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n \n         s = safe_url_string(url, self.encoding)\n\n@@ -31,14 +31,14 @@ class TextResponse(Response):\n         super(TextResponse, self).__init__(*args, **kwargs)\n \n     def _set_url(self, url):\n-        if isinstance(url, six.text_type):\n+        if isinstance(url, str):\n             self._url = to_unicode(url, self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n \n     def _set_body(self, body):\n         self._body = b''  # used by encoding detection\n-        if isinstance(body, six.text_type):\n+        if isinstance(body, str):\n             if self._encoding is None:\n                 raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                     type(self).__name__)\n@@ -158,7 +158,7 @@ class TextResponse(Response):\n \n def _url_from_selector(sel):\n     # type: (parsel.Selector) -> str\n-    if isinstance(sel.root, six.string_types):\n+    if isinstance(sel.root, str):\n         # e.g. ::attr(href) result\n         return strip_html5_whitespace(sel.root)\n     if not hasattr(sel.root, 'tag'):\n\n@@ -42,7 +42,7 @@ class HtmlParserLinkExtractor(HTMLParser):\n         ret = []\n         base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n-            if isinstance(link.url, six.text_type):\n+            if isinstance(link.url, str):\n                 link.url = link.url.encode(response_encoding)\n             try:\n                 link.url = urljoin(base_url, link.url)\n\n@@ -22,7 +22,7 @@ _collect_string_content = etree.XPath(\"string()\")\n \n \n def _nons(tag):\n-    if isinstance(tag, six.string_types):\n+    if isinstance(tag, str):\n         if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE)+1] == XHTML_NAMESPACE:\n             return tag.split('}')[-1]\n     return tag\n\n@@ -49,7 +49,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         if base_url is None:\n             base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in self.links:\n-            if isinstance(link.url, six.text_type):\n+            if isinstance(link.url, str):\n                 link.url = link.url.encode(response_encoding)\n             try:\n                 link.url = urljoin(base_url, link.url)\n\n@@ -23,7 +23,7 @@ def get_settings_priority(priority):\n     :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n     numerical value, or directly returns a given numerical priority.\n     \"\"\"\n-    if isinstance(priority, six.string_types):\n+    if isinstance(priority, str):\n         return SETTINGS_PRIORITIES[priority]\n     else:\n         return priority\n@@ -173,7 +173,7 @@ class BaseSettings(MutableMapping):\n         :type default: any\n         \"\"\"\n         value = self.get(name, default or [])\n-        if isinstance(value, six.string_types):\n+        if isinstance(value, str):\n             value = value.split(',')\n         return list(value)\n \n@@ -194,7 +194,7 @@ class BaseSettings(MutableMapping):\n         :type default: any\n         \"\"\"\n         value = self.get(name, default or {})\n-        if isinstance(value, six.string_types):\n+        if isinstance(value, str):\n             value = json.loads(value)\n         return dict(value)\n \n@@ -284,7 +284,7 @@ class BaseSettings(MutableMapping):\n         :type priority: string or int\n         \"\"\"\n         self._assert_mutability()\n-        if isinstance(module, six.string_types):\n+        if isinstance(module, str):\n             module = import_module(module)\n         for key in dir(module):\n             if key.isupper():\n@@ -313,7 +313,7 @@ class BaseSettings(MutableMapping):\n         :type priority: string or int\n         \"\"\"\n         self._assert_mutability()\n-        if isinstance(values, six.string_types):\n+        if isinstance(values, str):\n             values = json.loads(values)\n         if values is not None:\n             if isinstance(values, BaseSettings):\n\n@@ -25,7 +25,7 @@ def _identity(request, response):\n def _get_method(method, spider):\n     if callable(method):\n         return method\n-    elif isinstance(method, six.string_types):\n+    elif isinstance(method, str):\n         return getattr(spider, method, None)\n \n \n\n@@ -22,7 +22,7 @@ class SitemapSpider(Spider):\n         super(SitemapSpider, self).__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n-            if isinstance(c, six.string_types):\n+            if isinstance(c, str):\n                 c = getattr(self, c)\n             self._cbs.append((regex(r), c))\n         self._follow = [regex(x) for x in self.sitemap_follow]\n@@ -86,7 +86,7 @@ class SitemapSpider(Spider):\n \n \n def regex(x):\n-    if isinstance(x, six.string_types):\n+    if isinstance(x, str):\n         return re.compile(x)\n     return x\n \n\n@@ -60,7 +60,7 @@ class _StreamReader(object):\n             self._text, self.encoding = obj.body, obj.encoding\n         else:\n             self._text, self.encoding = obj, 'utf-8'\n-        self._is_unicode = isinstance(self._text, six.text_type)\n+        self._is_unicode = isinstance(self._text, str)\n \n     def read(self, n=65535):\n         self.read = self._read_unicode if self._is_unicode else self._read_string\n@@ -125,7 +125,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n \n \n def _body_or_str(obj, unicode=True):\n-    expected_types = (Response, six.text_type, six.binary_type)\n+    expected_types = (Response, str, bytes)\n     assert isinstance(obj, expected_types), \\\n         \"obj must be %s, not %s\" % (\n             \" or \".join(t.__name__ for t in expected_types),\n@@ -137,7 +137,7 @@ def _body_or_str(obj, unicode=True):\n             return obj.text\n         else:\n             return obj.body.decode('utf-8')\n-    elif isinstance(obj, six.text_type):\n+    elif isinstance(obj, str):\n         return obj if unicode else obj.encode('utf-8')\n     else:\n         return obj.decode('utf-8') if unicode else obj\n\n@@ -13,7 +13,7 @@ from scrapy.utils.python import flatten, to_unicode\n from scrapy.item import BaseItem\n \n \n-_ITERABLE_SINGLE_VALUES = dict, BaseItem, six.text_type, bytes\n+_ITERABLE_SINGLE_VALUES = dict, BaseItem, str, bytes\n \n \n def arg_to_iter(arg):\n@@ -83,7 +83,7 @@ def extract_regex(regex, text, encoding='utf-8'):\n     * if the regex doesn't contain any group the entire regex matching is returned\n     \"\"\"\n \n-    if isinstance(regex, six.string_types):\n+    if isinstance(regex, str):\n         regex = re.compile(regex, re.UNICODE)\n \n     try:\n@@ -92,7 +92,7 @@ def extract_regex(regex, text, encoding='utf-8'):\n         strings = regex.findall(text)    # full regex or numbered groups\n     strings = flatten(strings)\n \n-    if isinstance(text, six.text_type):\n+    if isinstance(text, str):\n         return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n     else:\n         return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n\n@@ -68,7 +68,7 @@ def is_listlike(x):\n     >>> is_listlike(range(5))\n     True\n     \"\"\"\n-    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))\n+    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n \n \n def unique(list_, key=lambda x: x):\n@@ -87,9 +87,9 @@ def unique(list_, key=lambda x: x):\n def to_unicode(text, encoding=None, errors='strict'):\n     \"\"\"Return the unicode representation of a bytes object ``text``. If\n     ``text`` is already an unicode object, return it as-is.\"\"\"\n-    if isinstance(text, six.text_type):\n+    if isinstance(text, str):\n         return text\n-    if not isinstance(text, (bytes, six.text_type)):\n+    if not isinstance(text, (bytes, str)):\n         raise TypeError('to_unicode must receive a bytes or str '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n@@ -102,7 +102,7 @@ def to_bytes(text, encoding=None, errors='strict'):\n     is already a bytes object, return it as-is.\"\"\"\n     if isinstance(text, bytes):\n         return text\n-    if not isinstance(text, six.string_types):\n+    if not isinstance(text, str):\n         raise TypeError('to_bytes must receive a str or bytes '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n@@ -138,7 +138,7 @@ def re_rsearch(pattern, text, chunk_size=1024):\n             yield (text[offset:], offset)\n         yield (text, 0)\n \n-    if isinstance(pattern, six.string_types):\n+    if isinstance(pattern, str):\n         pattern = re.compile(pattern)\n \n     for chunk, offset in _chunk_iter():\n@@ -301,9 +301,9 @@ def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     \"\"\"\n     d = {}\n     for k, v in dict(dct_or_tuples).items():\n-        k = k.encode(encoding) if isinstance(k, six.text_type) else k\n+        k = k.encode(encoding) if isinstance(k, str) else k\n         if not keys_only:\n-            v = v.encode(encoding) if isinstance(v, six.text_type) else v\n+            v = v.encode(encoding) if isinstance(v, str) else v\n         d[k] = v\n     return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
