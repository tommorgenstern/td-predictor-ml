{"custom_id": "scrapy#de0bf22010f6236614a9ab03b00b124ba6b14bd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 1749 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -108,13 +108,15 @@ class ExecutionEngine(object):\n             if not self._next_request(spider):\n                 break\n \n-        if self.spider_is_idle(spider):\n         slot = self.slots[spider]\n+        if slot.requests and not self._needs_backout(spider):\n             try:\n                 request = slot.requests.next()\n                 self.crawl(request, spider)\n             except StopIteration:\n-                if slot.close_if_idle:\n+                slot.requests = None\n+\n+        if self.spider_is_idle(spider) and slot.close_if_idle:\n             self._spider_idle(spider)\n \n     def _needs_backout(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc6ef3beb2e4c3136bd0be12c47e9efe4ac5fe4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1757 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class Slot(object):\n     def __init__(self, start_requests, close_if_idle):\n         self.closing = False\n         self.inprogress = set() # requests in progress\n-        self.requests = iter(start_requests)\n+        self.start_requests = iter(start_requests)\n         self.close_if_idle = close_if_idle\n \n     def add_request(self, request):\n@@ -109,12 +109,12 @@ class ExecutionEngine(object):\n                 break\n \n         slot = self.slots[spider]\n-        if slot.requests and not self._needs_backout(spider):\n+        if slot.start_requests and not self._needs_backout(spider):\n             try:\n-                request = slot.requests.next()\n+                request = slot.start_requests.next()\n                 self.crawl(request, spider)\n             except StopIteration:\n-                slot.requests = None\n+                slot.start_requests = None\n \n         if self.spider_is_idle(spider) and slot.close_if_idle:\n             self._spider_idle(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e008268e1ca41817d5b8e26d0832924d66a652c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 100 | Files Changed: 4 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 100 | Churn Cumulative: 207 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: 0.22033898305084745\n\nDIFF:\n@@ -1,57 +0,0 @@\n-\"\"\"\n-A Stats collector for persisting stats to Amazon SimpleDB.\n-\n-Requires the boto library: http://code.google.com/p/boto/\n-\"\"\"\n-\n-from datetime import datetime\n-\n-from twisted.internet import threads\n-\n-from scrapy.utils.simpledb import to_sdb_value\n-from scrapy.statscol import StatsCollector\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-class SimpledbStatsCollector(StatsCollector):\n-\n-    def __init__(self):\n-        super(SimpledbStatsCollector, self).__init__()\n-        self._sdbdomain = settings['STATS_SDB_DOMAIN']\n-        self._access_key = settings['AWS_ACCESS_KEY_ID']\n-        self._secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-\n-        self._async = settings.getbool('STATS_SDB_ASYNC')\n-        import boto\n-        self.connect_sdb = boto.connect_sdb\n-        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).create_domain(self._sdbdomain)\n-\n-    def _persist_stats(self, stats, spider=None):\n-        if spider is None: # only store spider-specific stats\n-            return\n-        if not self._sdbdomain:\n-            return\n-        if self._async:\n-            dfd = threads.deferToThread(self._persist_to_sdb, spider, stats.copy())\n-            dfd.addErrback(log.err, 'Error uploading stats to SimpleDB', \\\n-                spider=spider)\n-        else:\n-            self._persist_to_sdb(spider, stats)\n-\n-    def _persist_to_sdb(self, spider, stats):\n-        ts = self._get_timestamp(spider).isoformat()\n-        sdb_item_id = \"%s_%s\" % (spider.name, ts)\n-        sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())\n-        sdb_item['spider'] = spider.name\n-        sdb_item['timestamp'] = self._to_sdb_value(ts)\n-        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n-\n-    def _get_timestamp(self, spider):\n-        return datetime.utcnow()\n-\n-    def _to_sdb_value(self, obj, key=None):\n-        try:\n-            return to_sdb_value(obj)\n-        except TypeError:\n-            raise TypeError(\"%s unsupported type %r used in key %r\" % \\\n-                (type(self).__name__, type(obj).__name__, key))\n\n@@ -247,9 +247,6 @@ STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n STATS_ENABLED = True\n STATS_DUMP = True\n \n-STATS_SDB_DOMAIN = 'scrapy_stats'\n-STATS_SDB_ASYNC = False\n-\n STATSMAILER_RCPTS = []\n \n TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n\n@@ -1,21 +0,0 @@\n-import unittest\n-from datetime import datetime\n-\n-from scrapy.utils.simpledb import to_sdb_value\n-\n-class SimpleddbUtilsTest(unittest.TestCase):\n-\n-    def test_to_sdb_value(self):\n-        self.assertEqual(to_sdb_value(123), u'0000000000000123')\n-        self.assertEqual(to_sdb_value(123L), u'0000000000000123')\n-        self.assertEqual(to_sdb_value(True), u'1')\n-        self.assertEqual(to_sdb_value(False), u'0')\n-        self.assertEqual(to_sdb_value(None), u'')\n-        self.assertEqual(to_sdb_value(datetime(2009, 01, 01, 10, 10, 10)), \\\n-            u'2009-01-01T10:10:10')\n-        self.assertEqual(to_sdb_value('test'), 'test')\n-        self.assertEqual(to_sdb_value(u'test'), u'test')\n-        self.assertRaises(TypeError, to_sdb_value, object())\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,19 +0,0 @@\n-\"\"\"Helper functions for Amazon SimpleDB\"\"\"\n-\n-from datetime import datetime\n-\n-def to_sdb_value(obj):\n-    \"\"\"Convert the given object to proper value to store in Amazon SimpleDB\"\"\"\n-    if isinstance(obj, bool):\n-        return u'%d' % obj\n-    elif isinstance(obj, (int, long)):\n-        return \"%016d\" % obj\n-    elif isinstance(obj, datetime):\n-        return obj.isoformat()\n-    elif isinstance(obj, basestring):\n-        return obj\n-    elif obj is None:\n-        return u''\n-    else:\n-        raise TypeError(\"Unsupported Type: %s\" % type(obj).__name__)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d18fe18e2d46d34720b0864ff53e7f81fb9fc01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 95 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ from w3lib.url import urljoin_rfc\n from scrapy import log\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n-from scrapy.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.conf import settings\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f19442425a25fe794d31201c7ee3eb72674cf9c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 276 | Lines Deleted: 38 | Files Changed: 5 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 64/67 | Churn Δ: 314 | Churn Cumulative: 1328 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.136986301369863\n\nDIFF:\n@@ -0,0 +1,269 @@\n+\"\"\"\n+This module contains a fork of the UnicodeDammit class from BeautifulSoup, that\n+expliclty disabled any usage of chardet library.\n+\n+The UnicodeDammit class is used as a last resource for detecting the encoding\n+of a response.\n+\"\"\"\n+\n+import re\n+import codecs\n+\n+chardet = None # we don't want to use chardet since it's very slow,\n+\n+class UnicodeDammit:\n+    \"\"\"A class for detecting the encoding of a *ML document and\n+    converting it to a Unicode string. If the source encoding is\n+    windows-1252, can replace MS smart quotes with their HTML or XML\n+    equivalents.\"\"\"\n+\n+    # This dictionary maps commonly seen values for \"charset\" in HTML\n+    # meta tags to the corresponding Python codec names. It only covers\n+    # values that aren't in Python's aliases and can't be determined\n+    # by the heuristics in find_codec.\n+    CHARSET_ALIASES = { \"macintosh\" : \"mac-roman\",\n+                        \"x-sjis\" : \"shift-jis\" }\n+\n+    def __init__(self, markup, overrideEncodings=[],\n+                 smartQuotesTo='xml', isHTML=False):\n+        self.declaredHTMLEncoding = None\n+        self.markup, documentEncoding, sniffedEncoding = \\\n+                     self._detectEncoding(markup, isHTML)\n+        self.smartQuotesTo = smartQuotesTo\n+        self.triedEncodings = []\n+        if markup == '' or isinstance(markup, unicode):\n+            self.originalEncoding = None\n+            self.unicode = unicode(markup)\n+            return\n+\n+        u = None\n+        for proposedEncoding in overrideEncodings:\n+            u = self._convertFrom(proposedEncoding)\n+            if u: break\n+        if not u:\n+            for proposedEncoding in (documentEncoding, sniffedEncoding):\n+                u = self._convertFrom(proposedEncoding)\n+                if u: break\n+\n+        # If no luck and we have auto-detection library, try that:\n+        if not u and chardet and not isinstance(self.markup, unicode):\n+            u = self._convertFrom(chardet.detect(self.markup)['encoding'])\n+\n+        # As a last resort, try utf-8 and windows-1252:\n+        if not u:\n+            for proposed_encoding in (\"utf-8\", \"windows-1252\"):\n+                u = self._convertFrom(proposed_encoding)\n+                if u: break\n+\n+        self.unicode = u\n+        if not u: self.originalEncoding = None\n+\n+    def _subMSChar(self, orig):\n+        \"\"\"Changes a MS smart quote character to an XML or HTML\n+        entity.\"\"\"\n+        sub = self.MS_CHARS.get(orig)\n+        if isinstance(sub, tuple):\n+            if self.smartQuotesTo == 'xml':\n+                sub = '&#x%s;' % sub[1]\n+            else:\n+                sub = '&%s;' % sub[0]\n+        return sub\n+\n+    def _convertFrom(self, proposed):\n+        proposed = self.find_codec(proposed)\n+        if not proposed or proposed in self.triedEncodings:\n+            return None\n+        self.triedEncodings.append(proposed)\n+        markup = self.markup\n+\n+        # Convert smart quotes to HTML if coming from an encoding\n+        # that might have them.\n+        if self.smartQuotesTo and proposed.lower() in(\"windows-1252\",\n+                                                      \"iso-8859-1\",\n+                                                      \"iso-8859-2\"):\n+            markup = re.compile(\"([\\x80-\\x9f])\").sub \\\n+                     (lambda(x): self._subMSChar(x.group(1)),\n+                      markup)\n+\n+        try:\n+            # print \"Trying to convert document to %s\" % proposed\n+            u = self._toUnicode(markup, proposed)\n+            self.markup = u\n+            self.originalEncoding = proposed\n+        except Exception, e:\n+            # print \"That didn't work!\"\n+            # print e\n+            return None\n+        #print \"Correct encoding: %s\" % proposed\n+        return self.markup\n+\n+    def _toUnicode(self, data, encoding):\n+        '''Given a string and its encoding, decodes the string into Unicode.\n+        %encoding is a string recognized by encodings.aliases'''\n+\n+        # strip Byte Order Mark (if present)\n+        if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n+               and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16be'\n+            data = data[2:]\n+        elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n+                 and (data[2:4] != '\\x00\\x00'):\n+            encoding = 'utf-16le'\n+            data = data[2:]\n+        elif data[:3] == '\\xef\\xbb\\xbf':\n+            encoding = 'utf-8'\n+            data = data[3:]\n+        elif data[:4] == '\\x00\\x00\\xfe\\xff':\n+            encoding = 'utf-32be'\n+            data = data[4:]\n+        elif data[:4] == '\\xff\\xfe\\x00\\x00':\n+            encoding = 'utf-32le'\n+            data = data[4:]\n+        newdata = unicode(data, encoding)\n+        return newdata\n+\n+    def _detectEncoding(self, xml_data, isHTML=False):\n+        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n+        xml_encoding = sniffed_xml_encoding = None\n+        try:\n+            if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n+                # EBCDIC\n+                xml_data = self._ebcdic_to_ascii(xml_data)\n+            elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n+                # UTF-16BE\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n+                     and (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16BE with BOM\n+                sniffed_xml_encoding = 'utf-16be'\n+                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n+                # UTF-16LE\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n+                     (xml_data[2:4] != '\\x00\\x00'):\n+                # UTF-16LE with BOM\n+                sniffed_xml_encoding = 'utf-16le'\n+                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n+                # UTF-32BE\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n+                # UTF-32LE\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n+            elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n+                # UTF-32BE with BOM\n+                sniffed_xml_encoding = 'utf-32be'\n+                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n+            elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n+                # UTF-32LE with BOM\n+                sniffed_xml_encoding = 'utf-32le'\n+                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n+            elif xml_data[:3] == '\\xef\\xbb\\xbf':\n+                # UTF-8 with BOM\n+                sniffed_xml_encoding = 'utf-8'\n+                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n+            else:\n+                sniffed_xml_encoding = 'ascii'\n+                pass\n+        except:\n+            xml_encoding_match = None\n+        xml_encoding_match = re.compile(\n+            '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\n+        if not xml_encoding_match and isHTML:\n+            regexp = re.compile('<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]', re.I)\n+            xml_encoding_match = regexp.search(xml_data)\n+        if xml_encoding_match is not None:\n+            xml_encoding = xml_encoding_match.groups()[0].lower()\n+            if isHTML:\n+                self.declaredHTMLEncoding = xml_encoding\n+            if sniffed_xml_encoding and \\\n+               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n+                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n+                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',\n+                                 'utf16', 'u16')):\n+                xml_encoding = sniffed_xml_encoding\n+        return xml_data, xml_encoding, sniffed_xml_encoding\n+\n+\n+    def find_codec(self, charset):\n+        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"\"))) \\\n+               or (charset and self._codec(charset.replace(\"-\", \"_\"))) \\\n+               or charset\n+\n+    def _codec(self, charset):\n+        if not charset: return charset\n+        codec = None\n+        try:\n+            codecs.lookup(charset)\n+            codec = charset\n+        except (LookupError, ValueError):\n+            pass\n+        return codec\n+\n+    EBCDIC_TO_ASCII_MAP = None\n+    def _ebcdic_to_ascii(self, s):\n+        c = self.__class__\n+        if not c.EBCDIC_TO_ASCII_MAP:\n+            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,\n+                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,\n+                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,\n+                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,\n+                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,\n+                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,\n+                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,\n+                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,\n+                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,\n+                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,\n+                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,\n+                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,\n+                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,\n+                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,\n+                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,\n+                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,\n+                    250,251,252,253,254,255)\n+            import string\n+            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \\\n+            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))\n+        return s.translate(c.EBCDIC_TO_ASCII_MAP)\n+\n+    MS_CHARS = { '\\x80' : ('euro', '20AC'),\n+                 '\\x81' : ' ',\n+                 '\\x82' : ('sbquo', '201A'),\n+                 '\\x83' : ('fnof', '192'),\n+                 '\\x84' : ('bdquo', '201E'),\n+                 '\\x85' : ('hellip', '2026'),\n+                 '\\x86' : ('dagger', '2020'),\n+                 '\\x87' : ('Dagger', '2021'),\n+                 '\\x88' : ('circ', '2C6'),\n+                 '\\x89' : ('permil', '2030'),\n+                 '\\x8A' : ('Scaron', '160'),\n+                 '\\x8B' : ('lsaquo', '2039'),\n+                 '\\x8C' : ('OElig', '152'),\n+                 '\\x8D' : '?',\n+                 '\\x8E' : ('#x17D', '17D'),\n+                 '\\x8F' : '?',\n+                 '\\x90' : '?',\n+                 '\\x91' : ('lsquo', '2018'),\n+                 '\\x92' : ('rsquo', '2019'),\n+                 '\\x93' : ('ldquo', '201C'),\n+                 '\\x94' : ('rdquo', '201D'),\n+                 '\\x95' : ('bull', '2022'),\n+                 '\\x96' : ('ndash', '2013'),\n+                 '\\x97' : ('mdash', '2014'),\n+                 '\\x98' : ('tilde', '2DC'),\n+                 '\\x99' : ('trade', '2122'),\n+                 '\\x9a' : ('scaron', '161'),\n+                 '\\x9b' : ('rsaquo', '203A'),\n+                 '\\x9c' : ('oelig', '153'),\n+                 '\\x9d' : '?',\n+                 '\\x9e' : ('#x17E', '17E'),\n+                 '\\x9f' : ('Yuml', ''),}\n+\n+#######################################################################\n+\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n import re\n import codecs\n-from scrapy.xlib.BeautifulSoup import UnicodeDammit\n+from scrapy.http.response.dammit import UnicodeDammit\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n from scrapy.utils.encoding import encoding_exists, resolve_encoding\n\n@@ -2,10 +2,8 @@ import os\n import unittest\n import urlparse\n \n-from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, TextResponse, HtmlResponse\n-from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, \\\n-    response_httprepr, get_cached_beautifulsoup, open_in_browser\n+from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser\n \n __doctests__ = ['scrapy.utils.response']\n \n@@ -41,31 +39,6 @@ class ResponseUtilsTest(unittest.TestCase):\n         r1 = Response(\"http://www.example.com\", status=6666, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n-    def test_get_cached_beautifulsoup(self):\n-        r1 = Response('http://www.example.com', body='')\n-\n-        soup1 = get_cached_beautifulsoup(r1)\n-        soup2 = get_cached_beautifulsoup(r1)\n-\n-        assert isinstance(soup1, BeautifulSoup)\n-        assert isinstance(soup2, BeautifulSoup)\n-        # make sure it's cached\n-        assert soup1 is soup2\n-\n-        # when body is None, an empty soup should be returned\n-        r1 = Response('http://www.example.com')\n-        assert r1.body == \"\"\n-        assert isinstance(get_cached_beautifulsoup(r1), BeautifulSoup)\n-\n-        r1 = Response('http://www.example.com', body='')\n-        soup1 = get_cached_beautifulsoup(r1)\n-        r2 = r1.copy()\n-        soup2 = get_cached_beautifulsoup(r1)\n-        soup3 = get_cached_beautifulsoup(r2)\n-\n-        assert soup1 is soup2\n-        assert soup1 is not soup3\n-\n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n         body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n\n@@ -43,14 +43,6 @@ def get_meta_refresh(response):\n             response.encoding)\n     return _metaref_cache[response]\n \n-_beautifulsoup_cache = weakref.WeakKeyDictionary()\n-def get_cached_beautifulsoup(response):\n-    \"\"\"Return BeautifulSoup object of the given response, with caching\n-    support\"\"\"\n-    if response not in _beautifulsoup_cache:\n-        _beautifulsoup_cache[response] = BeautifulSoup(response.body)\n-    return _beautifulsoup_cache[response]\n-\n def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n \n\n@@ -1,4 +1,8 @@\n-\"\"\"Beautiful Soup\n+\"\"\"\n+FIXME: this module is only needed beacuse scrapy.xlib.ClientForm uses it. We\n+should remove it after we remove the scrapy.xlib.ClientForm module.\n+\n+Beautiful Soup\n Elixir and Tonic\n \"The Screen-Scraper's Friend\"\n http://www.crummy.com/software/BeautifulSoup/\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6b83fee3e8d6f13f162265b5b5850ae73f86334", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 18 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 76 | Churn Cumulative: 2664 | Contributors (this commit): 6 | Commits (past 90d): 24 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,7 @@ class ExecutionEngine(object):\n             or slot.closing \\\n             or self.spider_is_closed(spider) \\\n             or self.downloader.sites[spider].needs_backout() \\\n-            or self.scraper.sites[spider].needs_backout()\n+            or self.scraper.slots[spider].needs_backout()\n \n     def _next_request(self, spider):\n         request = self.scheduler.next_request(spider)\n@@ -157,8 +157,8 @@ class ExecutionEngine(object):\n         return d\n \n     def spider_is_idle(self, spider):\n-        scraper_idle = spider in self.scraper.sites \\\n-            and self.scraper.sites[spider].is_idle()\n+        scraper_idle = spider in self.scraper.slots \\\n+            and self.scraper.slots[spider].is_idle()\n         pending = self.scheduler.spider_has_pending_requests(spider)\n         downloading = spider in self.downloader.sites \\\n             and self.downloader.sites[spider].active\n\n@@ -19,8 +19,8 @@ from scrapy import log\n from scrapy.stats import stats\n \n \n-class SpiderInfo(object):\n-    \"\"\"Object for holding data of the responses being scraped\"\"\"\n+class Slot(object):\n+    \"\"\"Scraper slot (one per running spider)\"\"\"\n \n     MIN_RESPONSE_SIZE = 1024\n \n@@ -62,7 +62,7 @@ class SpiderInfo(object):\n class Scraper(object):\n \n     def __init__(self, engine, settings):\n-        self.sites = {}\n+        self.slots = {}\n         self.spidermw = SpiderMiddlewareManager.from_settings(settings)\n         itemproc_cls = load_object(settings['ITEM_PROCESSOR'])\n         self.itemproc = itemproc_cls.from_settings(settings)\n@@ -72,47 +72,47 @@ class Scraper(object):\n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n-        self.sites[spider] = SpiderInfo()\n+        assert spider not in self.slots, \"Spider already opened: %s\" % spider\n+        self.slots[spider] = Slot()\n         yield self.itemproc.open_spider(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n-        assert spider in self.sites, \"Spider not opened: %s\" % spider\n-        site = self.sites[spider]\n-        site.closing = defer.Deferred()\n-        site.closing.addCallback(self.itemproc.close_spider)\n-        self._check_if_closing(spider, site)\n-        return site.closing\n+        assert spider in self.slots, \"Spider not opened: %s\" % spider\n+        slot = self.slots[spider]\n+        slot.closing = defer.Deferred()\n+        slot.closing.addCallback(self.itemproc.close_spider)\n+        self._check_if_closing(spider, slot)\n+        return slot.closing\n \n     def is_idle(self):\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n-        return not self.sites\n+        return not self.slots\n \n-    def _check_if_closing(self, spider, site):\n-        if site.closing and site.is_idle():\n-            del self.sites[spider]\n-            site.closing.callback(spider)\n+    def _check_if_closing(self, spider, slot):\n+        if slot.closing and slot.is_idle():\n+            del self.slots[spider]\n+            slot.closing.callback(spider)\n \n     def enqueue_scrape(self, response, request, spider):\n-        site = self.sites.get(spider, None)\n-        if site is None:\n+        slot = self.slots.get(spider, None)\n+        if slot is None:\n             return\n-        dfd = site.add_response_request(response, request)\n+        dfd = slot.add_response_request(response, request)\n         def finish_scraping(_):\n-            site.finish_response(response)\n-            self._check_if_closing(spider, site)\n-            self._scrape_next(spider, site)\n+            slot.finish_response(response)\n+            self._check_if_closing(spider, slot)\n+            self._scrape_next(spider, slot)\n             return _\n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(log.err, 'Scraper bug processing %s' % request, \\\n             spider=spider)\n-        self._scrape_next(spider, site)\n+        self._scrape_next(spider, slot)\n         return dfd\n \n-    def _scrape_next(self, spider, site):\n-        while site.queue:\n-            response, request, deferred = site.next_response_request_deferred()\n+    def _scrape_next(self, spider, slot):\n+        while slot.queue:\n+            response, request, deferred = slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n     def _scrape(self, response, request, spider):\n@@ -170,7 +170,7 @@ class Scraper(object):\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n-            self.sites[spider].itemproc_size += 1\n+            self.slots[spider].itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, response, spider)\n             return dfd\n@@ -193,7 +193,7 @@ class Scraper(object):\n     def _itemproc_finished(self, output, item, response, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n-        self.sites[spider].itemproc_size -= 1\n+        self.slots[spider].itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n\n@@ -18,7 +18,7 @@ def get_engine_status(engine=None):\n         \"engine.downloader.is_idle()\",\n         \"len(engine.downloader.sites)\",\n         \"engine.scraper.is_idle()\",\n-        \"len(engine.scraper.sites)\",\n+        \"len(engine.scraper.slots)\",\n     ]\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n@@ -28,11 +28,11 @@ def get_engine_status(engine=None):\n         \"len(engine.downloader.sites[spider].active)\",\n         \"len(engine.downloader.sites[spider].transferring)\",\n         \"engine.downloader.sites[spider].lastseen\",\n-        \"len(engine.scraper.sites[spider].queue)\",\n-        \"len(engine.scraper.sites[spider].active)\",\n-        \"engine.scraper.sites[spider].active_size\",\n-        \"engine.scraper.sites[spider].itemproc_size\",\n-        \"engine.scraper.sites[spider].needs_backout()\",\n+        \"len(engine.scraper.slots[spider].queue)\",\n+        \"len(engine.scraper.slots[spider].active)\",\n+        \"engine.scraper.slots[spider].active_size\",\n+        \"engine.scraper.slots[spider].itemproc_size\",\n+        \"engine.scraper.slots[spider].needs_backout()\",\n     ]\n \n     status = {'global': [], 'spiders': {}}\n@@ -41,7 +41,7 @@ def get_engine_status(engine=None):\n             status['global'] += [(test, eval(test))]\n         except Exception, e:\n             status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n-    for spider in set(engine.downloader.sites.keys() + engine.scraper.sites.keys()):\n+    for spider in set(engine.downloader.sites.keys() + engine.scraper.slots.keys()):\n         x = []\n         for test in spider_tests:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ac08a713d7f8100de28612e5576306643121bd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 42 | Files Changed: 3 | Hunks: 27 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 2329 | Contributors (this commit): 5 | Commits (past 90d): 25 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -18,8 +18,8 @@ from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n \n-class SpiderInfo(object):\n-    \"\"\"Simple class to keep information and state for each open spider\"\"\"\n+class Slot(object):\n+    \"\"\"Downloader spider slot\"\"\"\n \n     def __init__(self, spider):\n         setattr_default(spider, 'download_delay', spider.settings.getfloat('DOWNLOAD_DELAY'))\n@@ -62,7 +62,7 @@ class Downloader(object):\n     \"\"\"\n \n     def __init__(self):\n-        self.sites = {}\n+        self.slots = {}\n         self.handlers = DownloadHandlers()\n         self.middleware = DownloaderMiddlewareManager.from_settings(settings)\n         self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n@@ -74,11 +74,11 @@ class Downloader(object):\n         Response object, then request never reach downloader queue, and it will\n         not be downloaded from site.\n         \"\"\"\n-        site = self.sites[spider]\n+        slot = self.slots[spider]\n \n-        site.active.add(request)\n+        slot.active.add(request)\n         def _deactivate(response):\n-            site.active.remove(request)\n+            slot.active.remove(request)\n             return response\n \n         dfd = self.middleware.download(self.enqueue, request, spider)\n@@ -86,7 +86,7 @@ class Downloader(object):\n \n     def enqueue(self, request, spider):\n         \"\"\"Enqueue a Request for a effective download from site\"\"\"\n-        site = self.sites[spider]\n+        slot = self.slots[spider]\n \n         def _downloaded(response):\n             send_catch_log(signal=signals.response_downloaded, \\\n@@ -94,37 +94,37 @@ class Downloader(object):\n             return response\n \n         deferred = defer.Deferred().addCallback(_downloaded)\n-        site.queue.append((request, deferred))\n+        slot.queue.append((request, deferred))\n         self._process_queue(spider)\n         return deferred\n \n     def _process_queue(self, spider):\n-        \"\"\"Effective download requests from site queue\"\"\"\n-        site = self.sites.get(spider)\n-        if not site:\n+        \"\"\"Effective download requests from slot queue\"\"\"\n+        slot = self.slots.get(spider)\n+        if not slot:\n             return\n \n         # Delay queue processing if a download_delay is configured\n         now = time()\n-        delay = site.download_delay()\n+        delay = slot.download_delay()\n         if delay:\n-            penalty = delay - now + site.lastseen\n-            if penalty > 0 and site.free_transfer_slots():\n+            penalty = delay - now + slot.lastseen\n+            if penalty > 0 and slot.free_transfer_slots():\n                 d = defer.Deferred()\n                 d.addCallback(self._process_queue)\n                 call = reactor.callLater(penalty, d.callback, spider)\n-                site.next_request_calls.add(call)\n-                d.addBoth(lambda x: site.next_request_calls.remove(call))\n+                slot.next_request_calls.add(call)\n+                d.addBoth(lambda x: slot.next_request_calls.remove(call))\n                 return\n-        site.lastseen = now\n+        slot.lastseen = now\n \n-        # Process enqueued requests if there are free slots to transfer for this site\n-        while site.queue and site.free_transfer_slots() > 0:\n-            request, deferred = site.queue.popleft()\n-            dfd = self._download(site, request, spider)\n+        # Process enqueued requests if there are free slots to transfer for this slot\n+        while slot.queue and slot.free_transfer_slots() > 0:\n+            request, deferred = slot.queue.popleft()\n+            dfd = self._download(slot, request, spider)\n             dfd.chainDeferred(deferred)\n \n-    def _download(self, site, request, spider):\n+    def _download(self, slot, request, spider):\n         # The order is very important for the following deferreds. Do not change!\n \n         # 1. Create the download deferred\n@@ -134,24 +134,24 @@ class Downloader(object):\n         # state to free up the transferring slot so it can be used by the\n         # following requests (perhaps those which came from the downloader\n         # middleware itself)\n-        site.transferring.add(request)\n+        slot.transferring.add(request)\n         def finish_transferring(_):\n-            site.transferring.remove(request)\n+            slot.transferring.remove(request)\n             self._process_queue(spider)\n             return _\n         return dfd.addBoth(finish_transferring)\n \n     def open_spider(self, spider):\n         \"\"\"Allocate resources to begin processing a spider\"\"\"\n-        assert spider not in self.sites, \"Spider already opened: %s\" % spider\n-        self.sites[spider] = SpiderInfo(spider)\n+        assert spider not in self.slots, \"Spider already opened: %s\" % spider\n+        self.slots[spider] = Slot(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n-        assert spider in self.sites, \"Spider not opened: %s\" % spider\n-        site = self.sites.pop(spider)\n-        site.cancel_request_calls()\n+        assert spider in self.slots, \"Spider not opened: %s\" % spider\n+        slot = self.slots.pop(spider)\n+        slot.cancel_request_calls()\n \n     def is_idle(self):\n-        return not self.sites\n+        return not self.slots\n \n\n@@ -124,7 +124,7 @@ class ExecutionEngine(object):\n         return not self.running \\\n             or slot.closing \\\n             or self.spider_is_closed(spider) \\\n-            or self.downloader.sites[spider].needs_backout() \\\n+            or self.downloader.slots[spider].needs_backout() \\\n             or self.scraper.slots[spider].needs_backout()\n \n     def _next_request(self, spider):\n@@ -160,22 +160,22 @@ class ExecutionEngine(object):\n         scraper_idle = spider in self.scraper.slots \\\n             and self.scraper.slots[spider].is_idle()\n         pending = self.scheduler.spider_has_pending_requests(spider)\n-        downloading = spider in self.downloader.sites \\\n-            and self.downloader.sites[spider].active\n+        downloading = spider in self.downloader.slots \\\n+            and self.downloader.slots[spider].active\n         return scraper_idle and not (pending or downloading)\n \n     def spider_is_closed(self, spider):\n         \"\"\"Return True if the spider is fully closed (ie. not even in the\n         closing stage)\"\"\"\n-        return spider not in self.downloader.sites\n+        return spider not in self.downloader.slots\n \n     @property\n     def open_spiders(self):\n-        return self.downloader.sites.keys()\n+        return self.downloader.slots.keys()\n \n     def has_capacity(self):\n         \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n-        return len(self.downloader.sites) < self.downloader.concurrent_spiders\n+        return len(self.downloader.slots) < self.downloader.concurrent_spiders\n \n     def crawl(self, request, spider):\n         assert spider in self.open_spiders, \\\n\n@@ -16,7 +16,7 @@ def get_engine_status(engine=None):\n         \"engine.scheduler.is_idle()\",\n         \"len(engine.scheduler.pending_requests)\",\n         \"engine.downloader.is_idle()\",\n-        \"len(engine.downloader.sites)\",\n+        \"len(engine.downloader.slots)\",\n         \"engine.scraper.is_idle()\",\n         \"len(engine.scraper.slots)\",\n     ]\n@@ -24,10 +24,10 @@ def get_engine_status(engine=None):\n         \"engine.spider_is_idle(spider)\",\n         \"engine.slots[spider].closing\",\n         \"len(engine.scheduler.pending_requests[spider])\",\n-        \"len(engine.downloader.sites[spider].queue)\",\n-        \"len(engine.downloader.sites[spider].active)\",\n-        \"len(engine.downloader.sites[spider].transferring)\",\n-        \"engine.downloader.sites[spider].lastseen\",\n+        \"len(engine.downloader.slots[spider].queue)\",\n+        \"len(engine.downloader.slots[spider].active)\",\n+        \"len(engine.downloader.slots[spider].transferring)\",\n+        \"engine.downloader.slots[spider].lastseen\",\n         \"len(engine.scraper.slots[spider].queue)\",\n         \"len(engine.scraper.slots[spider].active)\",\n         \"engine.scraper.slots[spider].active_size\",\n@@ -41,7 +41,7 @@ def get_engine_status(engine=None):\n             status['global'] += [(test, eval(test))]\n         except Exception, e:\n             status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n-    for spider in set(engine.downloader.sites.keys() + engine.scraper.slots.keys()):\n+    for spider in set(engine.downloader.slots.keys() + engine.scraper.slots.keys()):\n         x = []\n         for test in spider_tests:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#209ecdf471fad43d84a45b275d117e0a584067b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,6 @@\n-DATABASE_ENGINE = 'sqlite3'\n-DATABASE_NAME = ':memory:'\n+DATABASES = {\n+    'default': {\n+        'ENGINE': 'django.db.backends.sqlite3',\n+        'NAME': ':memory:',\n+    }\n+}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea3bf6d95d180232d7fd26b6160ec7ec7deb8472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 49 | Files Changed: 4 | Hunks: 27 | Methods Changed: 23 | Complexity Δ (Sum/Max): 1/6 | Churn Δ: 101 | Churn Cumulative: 2818 | Contributors (this commit): 7 | Commits (past 90d): 30 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ For more information see docs/topics/architecture.rst\n \"\"\"\n from time import time\n \n-from twisted.internet import reactor, defer\n+from twisted.internet import defer\n from twisted.python.failure import Failure\n \n from scrapy import log, signals\n@@ -18,14 +18,17 @@ from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.reactor import CallLaterOnce\n+\n \n class Slot(object):\n \n-    def __init__(self, start_requests, close_if_idle):\n+    def __init__(self, start_requests, close_if_idle, nextcall):\n         self.closing = False\n         self.inprogress = set() # requests in progress\n         self.start_requests = iter(start_requests)\n         self.close_if_idle = close_if_idle\n+        self.nextcall = nextcall\n \n     def add_request(self, request):\n         self.inprogress.add(request)\n@@ -41,6 +44,8 @@ class Slot(object):\n \n     def _maybe_fire_closing(self):\n         if self.closing and not self.inprogress:\n+            if self.nextcall:\n+                self.nextcall.cancel()\n             self.closing.callback(None)\n \n \n@@ -51,7 +56,6 @@ class ExecutionEngine(object):\n         self.slots = {}\n         self.running = False\n         self.paused = False\n-        self._next_request_calls = {}\n         self.scheduler = load_object(settings['SCHEDULER'])()\n         self.downloader = Downloader()\n         self.scraper = Scraper(self, self.settings)\n@@ -84,31 +88,19 @@ class ExecutionEngine(object):\n         return self.scheduler.is_idle() and self.downloader.is_idle() and \\\n             self.scraper.is_idle()\n \n-    def next_request(self, spider, now=False):\n-        \"\"\"Scrape the next request for the spider passed.\n-\n-        The next request to be scraped is retrieved from the scheduler and\n-        requested from the downloader.\n-\n-        The spider is closed if there are no more pages to scrape.\n-        \"\"\"\n-        if now:\n-            self._next_request_calls.pop(spider, None)\n-        elif spider not in self._next_request_calls:\n-            call = reactor.callLater(0, self.next_request, spider, now=True)\n-            self._next_request_calls[spider] = call\n-            return call\n-        else:\n+    def _next_request(self, spider):\n+        try:\n+            slot = self.slots[spider]\n+        except KeyError:\n             return\n \n         if self.paused:\n-            return reactor.callLater(5, self.next_request, spider)\n+            return slot.nextcall.schedule(5)\n \n         while not self._needs_backout(spider):\n-            if not self._next_request(spider):\n+            if not self._next_request_from_scheduler(spider):\n                 break\n \n-        slot = self.slots[spider]\n         if slot.start_requests and not self._needs_backout(spider):\n             try:\n                 request = slot.start_requests.next()\n@@ -127,7 +119,7 @@ class ExecutionEngine(object):\n             or self.downloader.slots[spider].needs_backout() \\\n             or self.scraper.slots[spider].needs_backout()\n \n-    def _next_request(self, spider):\n+    def _next_request_from_scheduler(self, spider):\n         request = self.scheduler.next_request(spider)\n         if not request:\n             return\n@@ -137,7 +129,8 @@ class ExecutionEngine(object):\n         slot = self.slots[spider]\n         d.addBoth(lambda _: slot.remove_request(request))\n         d.addErrback(log.msg, spider=spider)\n-        d.addBoth(lambda _: self.next_request(spider))\n+        d.addBoth(lambda _: slot.nextcall.schedule())\n+        d.addErrback(log.msg, spider=spider)\n         return d\n \n     def _handle_downloader_output(self, response, request, spider):\n@@ -147,13 +140,8 @@ class ExecutionEngine(object):\n             self.crawl(response, spider)\n             return\n         # response is a Response or Failure\n-        d = defer.Deferred()\n-        d.addBoth(self.scraper.enqueue_scrape, request, spider)\n+        d = self.scraper.enqueue_scrape(response, request, spider)\n         d.addErrback(log.err, spider=spider)\n-        if isinstance(response, Failure):\n-            d.errback(response)\n-        else:\n-            d.callback(response)\n         return d\n \n     def spider_is_idle(self, spider):\n@@ -171,7 +159,7 @@ class ExecutionEngine(object):\n \n     @property\n     def open_spiders(self):\n-        return self.downloader.slots.keys()\n+        return self.slots.keys()\n \n     def has_capacity(self):\n         \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n@@ -181,7 +169,7 @@ class ExecutionEngine(object):\n         assert spider in self.open_spiders, \\\n             \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n         self.schedule(request, spider)\n-        self.next_request(spider)\n+        self.slots[spider].nextcall.schedule()\n \n     def schedule(self, request, spider):\n         return self.scheduler.enqueue_request(spider, request)\n@@ -202,7 +190,6 @@ class ExecutionEngine(object):\n         slot = self.slots[spider]\n         slot.add_request(request)\n         def _on_success(response):\n-            \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n@@ -213,7 +200,7 @@ class ExecutionEngine(object):\n             return response\n \n         def _on_complete(_):\n-            self.next_request(spider)\n+            slot.nextcall.schedule()\n             return _\n \n         dwld = mustbe_deferred(self.downloader.fetch, request, spider)\n@@ -226,13 +213,15 @@ class ExecutionEngine(object):\n         assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n-        self.slots[spider] = Slot(start_requests or (), close_if_idle)\n+        nextcall = CallLaterOnce(self._next_request, spider)\n+        slot = Slot(start_requests or (), close_if_idle, nextcall)\n+        self.slots[spider] = slot\n         yield self.scheduler.open_spider(spider)\n         self.downloader.open_spider(spider)\n         yield self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n         yield send_catch_log_deferred(signals.spider_opened, spider=spider)\n-        self.next_request(spider)\n+        slot.nextcall.schedule()\n \n     def _spider_idle(self, spider):\n         \"\"\"Called when a spider gets idle. This function is called when there\n@@ -246,8 +235,7 @@ class ExecutionEngine(object):\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                 for _, x in res):\n-            reactor.callLater(5, self.next_request, spider)\n-            return\n+            return self.slots[spider].nextcall.schedule(5)\n \n         if self.spider_is_idle(spider):\n             self.close_spider(spider, reason='finished')\n@@ -273,9 +261,6 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n-        dfd.addBoth(lambda _: self._cancel_next_call(spider))\n-        dfd.addErrback(log.err, spider=spider)\n-\n         dfd.addBoth(lambda _: send_catch_log_deferred(signal=signals.spider_closed, \\\n             spider=spider, reason=reason))\n         dfd.addErrback(log.err, spider=spider)\n@@ -292,11 +277,6 @@ class ExecutionEngine(object):\n \n         return dfd\n \n-    def _cancel_next_call(self, spider):\n-        call = self._next_request_calls.pop(spider, None)\n-        if call and call.active:\n-            call.cancel()\n-\n     def _close_all_spiders(self):\n         dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n         dlist = defer.DeferredList(dfds)\n\n@@ -95,9 +95,7 @@ class Scraper(object):\n             slot.closing.callback(spider)\n \n     def enqueue_scrape(self, response, request, spider):\n-        slot = self.slots.get(spider, None)\n-        if slot is None:\n-            return\n+        slot = self.slots[spider]\n         dfd = slot.add_response_request(response, request)\n         def finish_scraping(_):\n             slot.finish_response(response)\n\n@@ -23,6 +23,7 @@ def get_engine_status(engine=None):\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n         \"engine.slots[spider].closing\",\n+        \"len(engine.slots[spider].inprogress)\",\n         \"len(engine.scheduler.pending_requests[spider])\",\n         \"len(engine.downloader.slots[spider].queue)\",\n         \"len(engine.downloader.slots[spider].active)\",\n@@ -41,7 +42,7 @@ def get_engine_status(engine=None):\n             status['global'] += [(test, eval(test))]\n         except Exception, e:\n             status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n-    for spider in set(engine.downloader.slots.keys() + engine.scraper.slots.keys()):\n+    for spider in engine.slots.keys():\n         x = []\n         for test in spider_tests:\n             try:\n\n@@ -15,3 +15,27 @@ def listen_tcp(portrange, host, factory):\n         except error.CannotListenError:\n             if x == portrange[1]:\n                 raise\n+\n+\n+class CallLaterOnce(object):\n+    \"\"\"Schedule a function to be called in the next reactor loop, but only if\n+    it hasn't been already scheduled since the last time it run.\n+    \"\"\"\n+\n+    def __init__(self, func, *a, **kw):\n+        self._func = func\n+        self._a = a\n+        self._kw = kw\n+        self._call = None\n+\n+    def schedule(self, delay=0):\n+        if self._call is None:\n+            self._call = reactor.callLater(delay, self)\n+\n+    def cancel(self):\n+        if self._call:\n+            self._call.cancel()\n+\n+    def __call__(self):\n+        self._call = None\n+        return self._func(*self._a, **self._kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
