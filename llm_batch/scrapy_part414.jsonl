{"custom_id": "scrapy#f98ffc71d25ae74351df70ae03ef5299a2ab3813", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 19 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 92 | Churn Cumulative: 3671 | Contributors (this commit): 26 | Commits (past 90d): 13 | Contributors (cumulative): 42 | DMM Complexity: 0.8918918918918919\n\nDIFF:\n@@ -173,8 +173,8 @@ class Scheduler(BaseScheduler):\n \n     :ref:`Start requests <start-requests>` are sent in the order they are\n     yielded from :meth:`~scrapy.Spider.start`, and given the same\n-    :attr:`~scrapy.http.Request.priority`, start requests take precedence over\n-    other requests.\n+    :attr:`~scrapy.http.Request.priority`, other requests take precedence over\n+    start requests.\n \n     You can set :setting:`SCHEDULER_START_MEMORY_QUEUE` and\n     :setting:`SCHEDULER_START_DISK_QUEUE` to ``None`` to handle start requests\n\n@@ -160,28 +160,32 @@ class ScrapyPriorityQueue:\n \n     def pop(self) -> Request | None:\n         while self.curprio is not None:\n-            if self._start_queues:\n             try:\n-                    q = self._start_queues[self.curprio]\n+                q = self.queues[self.curprio]\n             except KeyError:\n                 pass\n             else:\n                 m = q.pop()\n                 if not q:\n-                        del self._start_queues[self.curprio]\n+                    del self.queues[self.curprio]\n                     q.close()\n+                    if not self._start_queues:\n+                        self._update_curprio()\n                 return m\n+            if self._start_queues:\n                 try:\n-                q = self.queues[self.curprio]\n+                    q = self._start_queues[self.curprio]\n                 except KeyError:\n                     self._update_curprio()\n                 else:\n                     m = q.pop()\n                     if not q:\n-                    del self.queues[self.curprio]\n+                        del self._start_queues[self.curprio]\n                         q.close()\n                         self._update_curprio()\n                     return m\n+            else:\n+                self._update_curprio()\n         return None\n \n     def _update_curprio(self) -> None:\n\n@@ -189,9 +189,9 @@ class RequestSendOrderTestCase(TestCase):\n \n     @deferred_f_from_coro_f\n     async def test_default(self):\n-        \"\"\"By default, start requests take priority over callback requests and\n+        \"\"\"By default, callback requests take priority over start requests and\n         are sent in order. Priority matters, but given the same priority, a\n-        start request takes precedence.\"\"\"\n+        callback request takes precedence.\"\"\"\n         nums = [1, 2, 3, 4, 5, 6]\n         response_seconds = 0\n         download_slots = 1\n@@ -207,13 +207,13 @@ class RequestSendOrderTestCase(TestCase):\n             yield _request(1)\n \n             for request in (\n-                _request(4, priority=1),\n-                _request(6),\n+                _request(2, priority=1),\n+                _request(5),\n             ):\n                 spider.crawler.engine._slot.scheduler.enqueue_request(request)\n-            yield _request(5)\n-            yield _request(2, priority=1)\n+            yield _request(6)\n             yield _request(3, priority=1)\n+            yield _request(4, priority=1)\n \n         def parse(spider, response):\n             return\n@@ -249,13 +249,13 @@ class RequestSendOrderTestCase(TestCase):\n             yield _request(1)\n \n             for request in (\n-                _request(4, priority=1),\n-                _request(6),\n+                _request(2, priority=1),\n+                _request(5),\n             ):\n                 spider.crawler.engine._slot.scheduler.enqueue_request(request)\n-            yield _request(5)\n+            yield _request(6)\n+            yield _request(4, priority=1)\n             yield _request(3, priority=1)\n-            yield _request(2, priority=1)\n \n         def parse(spider, response):\n             return\n\n@@ -7,6 +7,7 @@ from scrapy.http.request import Request\n from scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue\n from scrapy.spiders import Spider\n from scrapy.squeues import FifoMemoryQueue\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.test import get_crawler\n from tests.test_scheduler import MockDownloader, MockEngine\n \n@@ -155,3 +156,56 @@ class TestDownloaderAwarePriorityQueue:\n         assert self.queue.peek().url == req3.url\n         assert self.queue.pop().url == req3.url\n         assert self.queue.peek() is None\n+\n+\n+@pytest.mark.parametrize(\n+    (\"input\", \"output\"),\n+    [\n+        # By default, start requests are FIFO, other requests are LIFO.\n+        ([{}, {}], [2, 1]),\n+        ([{\"start\": True}, {\"start\": True}], [1, 2]),\n+        # Priority matters.\n+        ([{\"priority\": 1}, {\"start\": True}], [1, 2]),\n+        ([{}, {\"start\": True, \"priority\": 1}], [2, 1]),\n+        # For the same priority, start requests pop last.\n+        ([{}, {\"start\": True}], [1, 2]),\n+        ([{\"start\": True}, {}], [2, 1]),\n+    ],\n+)\n+def test_pop_order(input, output):\n+    def make_url(index):\n+        return f\"https://toscrape.com/{index}\"\n+\n+    def make_request(index, data):\n+        meta = {}\n+        if data.get(\"start\", False):\n+            meta[\"is_start_request\"] = True\n+        return Request(\n+            url=make_url(index),\n+            priority=data.get(\"priority\", 0),\n+            meta=meta,\n+        )\n+\n+    input_requests = [\n+        make_request(index, data) for index, data in enumerate(input, start=1)\n+    ]\n+    expected_output_urls = [make_url(index) for index in output]\n+\n+    crawler = get_crawler(Spider)\n+    settings = crawler.settings\n+    queue = build_from_crawler(\n+        ScrapyPriorityQueue,\n+        crawler,\n+        downstream_queue_cls=load_object(settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n+        key=\"\",\n+        start_queue_cls=load_object(settings[\"SCHEDULER_START_MEMORY_QUEUE\"]),\n+    )\n+\n+    for request in input_requests:\n+        queue.push(request)\n+\n+    actual_output_urls = []\n+    while request := queue.pop():\n+        actual_output_urls.append(request.url)\n+\n+    assert actual_output_urls == expected_output_urls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dceb85bf3e41d06dfac81c037c69fd0f1ab61156", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 35 | Files Changed: 8 | Hunks: 18 | Methods Changed: 14 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 135 | Churn Cumulative: 3063 | Contributors (this commit): 42 | Commits (past 90d): 22 | Contributors (cumulative): 83 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,38 @@\n+\"\"\"Utilities related to asyncio and its support in Scrapy.\"\"\"\n+\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n+\n+\n+def is_asyncio_available() -> bool:\n+    \"\"\"Check if it's possible to call asyncio code that relies on the asyncio event loop.\n+\n+    .. versionadded:: VERSION\n+\n+    Currently this function is identical to\n+    :func:`scrapy.utils.reactor.is_asyncio_reactor_installed`: it returns\n+    ``True`` if the Twisted reactor that is installed is\n+    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`, returns\n+    ``False`` if a different reactor is installed, and raises a\n+    :exc:`RuntimeError` if no reactor is installed. In a future Scrapy version,\n+    when Scrapy supports running without a Twisted reactor, this function will\n+    also return ``True`` when running in that mode, so code that doesn't\n+    directly require a Twisted reactor should use this function instead of\n+    :func:`~scrapy.utils.reactor.is_asyncio_reactor_installed`.\n+\n+    When this returns ``True``, an asyncio loop is installed and used by\n+    Scrapy. It's possible to call functions that require it, such as\n+    :func:`asyncio.sleep`, and await on :class:`asyncio.Future` objects in\n+    Scrapy-related code.\n+\n+    When this returns ``False``, a non-asyncio Twisted reactor is installed.\n+    It's not possible to use asyncio features that require an asyncio event\n+    loop or await on :class:`asyncio.Future` objects in Scrapy-related code,\n+    but it's possible to await on :class:`~twisted.internet.defer.Deferred`\n+    objects.\n+    \"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\n+            \"is_asyncio_available() called without an installed reactor.\"\n+        )\n+\n+    return is_asyncio_reactor_installed()\n\n@@ -22,7 +22,8 @@ from twisted.internet.task import Cooperator\n from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n-from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n+from scrapy.utils.asyncio import is_asyncio_available\n+from scrapy.utils.reactor import _get_asyncio_event_loop\n \n if TYPE_CHECKING:\n     from collections.abc import AsyncIterator, Callable\n@@ -379,7 +380,7 @@ def deferred_from_coro(o: Awaitable[_T] | _T2) -> Deferred[_T] | _T2:\n     if isinstance(o, Deferred):\n         return o\n     if inspect.isawaitable(o):\n-        if not is_asyncio_reactor_installed():\n+        if not is_asyncio_available():\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n             return Deferred.fromCoroutine(cast(Coroutine[Deferred[Any], Any, _T], o))\n@@ -471,6 +472,6 @@ def maybe_deferred_to_future(d: Deferred[_T]) -> Deferred[_T] | Future[_T]:\n                 deferred = self.crawler.engine.download(additional_request)\n                 additional_response = await maybe_deferred_to_future(deferred)\n     \"\"\"\n-    if not is_asyncio_reactor_installed():\n+    if not is_asyncio_available():\n         return d\n     return deferred_to_future(d)\n\n@@ -202,6 +202,12 @@ def is_asyncio_reactor_installed() -> bool:\n     \"\"\"Check whether the installed reactor is :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n \n     Raise a :exc:`RuntimeError` if no reactor is installed.\n+\n+    In a future Scrapy version, when Scrapy supports running without a Twisted\n+    reactor, this function won't be useful for checking if it's possible to use\n+    asyncio features, so the code that that doesn't directly require a Twisted\n+    reactor should use :func:`scrapy.utils.asyncio.is_asyncio_available`\n+    instead of this function.\n     \"\"\"\n     if not is_reactor_installed():\n         raise RuntimeError(\n\n@@ -1,5 +1,6 @@\n import scrapy\n from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.asyncio import is_asyncio_available\n from scrapy.utils.reactor import (\n     install_reactor,\n     is_asyncio_reactor_installed,\n@@ -18,6 +19,13 @@ except RuntimeError:\n else:\n     raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n \n+try:\n+    is_asyncio_available()\n+except RuntimeError:\n+    pass\n+else:\n+    raise RuntimeError(\"is_asyncio_available() did not raise RuntimeError.\")\n+\n if is_reactor_installed():\n     raise RuntimeError(\n         \"Reactor already installed after is_asyncio_reactor_installed().\"\n@@ -33,6 +41,8 @@ class ReactorCheckExtension:\n     def __init__(self):\n         if not is_asyncio_reactor_installed():\n             raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n+        if not is_asyncio_available():\n+            raise RuntimeError(\"ReactorCheckExtension requires asyncio support.\")\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -1,35 +1,10 @@\n-import asyncio\n-import warnings\n-\n import pytest\n-from twisted.trial.unittest import TestCase\n \n-from scrapy.utils.defer import deferred_f_from_coro_f\n-from scrapy.utils.reactor import (\n-    install_reactor,\n-    is_asyncio_reactor_installed,\n-    set_asyncio_event_loop,\n-)\n+from scrapy.utils.asyncio import is_asyncio_available\n \n \n @pytest.mark.usefixtures(\"reactor_pytest\")\n-class TestAsyncio(TestCase):\n-    def test_is_asyncio_reactor_installed(self):\n+class TestAsyncio:\n+    def test_is_asyncio_available(self):\n         # the result should depend only on the pytest --reactor argument\n-        assert is_asyncio_reactor_installed() == (self.reactor_pytest != \"default\")\n-\n-    def test_install_asyncio_reactor(self):\n-        from twisted.internet import reactor as original_reactor\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n-            assert len(w) == 0\n-        from twisted.internet import reactor  # pylint: disable=reimported\n-\n-        assert original_reactor == reactor\n-\n-    @pytest.mark.only_asyncio\n-    @deferred_f_from_coro_f\n-    async def test_set_asyncio_event_loop(self):\n-        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n-        assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n+        assert is_asyncio_available() == (self.reactor_pytest != \"default\")\n\n@@ -311,6 +311,7 @@ class TestDeferredFFromCoroF(unittest.TestCase):\n         yield self._assert_result(c_f)\n \n \n+@pytest.mark.only_asyncio\n class TestDeferredToFuture(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_deferred(self):\n@@ -332,7 +333,6 @@ class TestDeferredToFuture(unittest.TestCase):\n         future_result = await result\n         assert future_result == 42\n \n-    @pytest.mark.only_asyncio\n     @deferred_f_from_coro_f\n     async def test_wrapped_coroutine_asyncio(self):\n         async def c_f() -> int:\n@@ -340,7 +340,7 @@ class TestDeferredToFuture(unittest.TestCase):\n             return 42\n \n         d = deferred_from_coro(c_f())\n-        result = maybe_deferred_to_future(d)\n+        result = deferred_to_future(d)\n         assert isinstance(result, Future)\n         future_result = await result\n         assert future_result == 42\n\n@@ -243,7 +243,7 @@ class TestWarnWhenSubclassed:\n             )\n \n         w = self._mywarnings(w)\n-        assert len(w) == 0, str(map(str, w))\n+        assert len(w) == 0, [str(warning) for warning in w]\n \n         with warnings.catch_warnings(record=True) as w:\n             AlsoDeprecated()\n\n@@ -0,0 +1,35 @@\n+import asyncio\n+import warnings\n+\n+import pytest\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.reactor import (\n+    install_reactor,\n+    is_asyncio_reactor_installed,\n+    set_asyncio_event_loop,\n+)\n+\n+\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n+class TestAsyncio(TestCase):\n+    def test_is_asyncio_reactor_installed(self):\n+        # the result should depend only on the pytest --reactor argument\n+        assert is_asyncio_reactor_installed() == (self.reactor_pytest != \"default\")\n+\n+    def test_install_asyncio_reactor(self):\n+        from twisted.internet import reactor as original_reactor\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+            assert len(w) == 0, [str(warning) for warning in w]\n+        from twisted.internet import reactor  # pylint: disable=reimported\n+\n+        assert original_reactor == reactor\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_set_asyncio_event_loop(self):\n+        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05b3b205ce296c72063e05d15f5cae8047476ca2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1130 | Lines Deleted: 211 | Files Changed: 33 | Hunks: 96 | Methods Changed: 85 | Complexity Δ (Sum/Max): 72/19 | Churn Δ: 1341 | Churn Cumulative: 13166 | Contributors (this commit): 73 | Commits (past 90d): 63 | Contributors (cumulative): 186 | DMM Complexity: 0.8772563176895307\n\nDIFF:\n@@ -19,6 +19,8 @@ collect_ignore = [\n     \"tests/mockserver.py\",\n     \"tests/pipelines.py\",\n     \"tests/spiders.py\",\n+    # contains scripts to be run by tests/test_crawler.py::AsyncCrawlerProcessSubprocess\n+    *_py_files(\"tests/AsyncCrawlerProcess\"),\n     # contains scripts to be run by tests/test_crawler.py::AsyncCrawlerRunnerSubprocess\n     *_py_files(\"tests/AsyncCrawlerRunner\"),\n     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n\n@@ -22,6 +22,7 @@ from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.utils.defer import (\n     deferred_f_from_coro_f,\n+    deferred_from_coro,\n     maybe_deferred_to_future,\n )\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n@@ -122,8 +123,10 @@ class ExecutionEngine:\n             )\n         return scheduler_cls\n \n-    @deferred_f_from_coro_f\n-    async def start(self, _start_request_processing=True) -> None:\n+    def start(self, _start_request_processing=True) -> Deferred[None]:\n+        return deferred_from_coro(self.start_async(_start_request_processing))\n+\n+    async def start_async(self, _start_request_processing=True) -> None:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n@@ -392,10 +395,15 @@ class ExecutionEngine:\n         finally:\n             self._slot.nextcall.schedule()\n \n-    @deferred_f_from_coro_f\n-    async def open_spider(\n+    def open_spider(self, spider: Spider, close_if_idle: bool = True) -> Deferred[None]:\n+        return deferred_from_coro(\n+            self.open_spider_async(spider, close_if_idle=close_if_idle)\n+        )\n+\n+    async def open_spider_async(\n         self,\n         spider: Spider,\n+        *,\n         close_if_idle: bool = True,\n     ) -> None:\n         if self._slot is not None:\n\n@@ -21,7 +21,8 @@ from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n from scrapy.settings import BaseSettings, Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.utils.defer import deferred_to_future\n+from scrapy.utils.asyncio import is_asyncio_available\n+from scrapy.utils.defer import deferred_from_coro, deferred_to_future\n from scrapy.utils.log import (\n     LogCounterHandler,\n     configure_logging,\n@@ -33,8 +34,10 @@ from scrapy.utils.log import (\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.reactor import (\n+    _asyncio_reactor_path,\n     install_reactor,\n     is_asyncio_reactor_installed,\n+    is_reactor_installed,\n     verify_installed_asyncio_event_loop,\n     verify_installed_reactor,\n )\n@@ -142,6 +145,12 @@ class Crawler:\n     # this method.\n     @inlineCallbacks\n     def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred[Any], Any, None]:\n+        \"\"\"Start the crawler by instantiating its spider class with the given\n+        *args* and *kwargs* arguments, while setting the execution engine in\n+        motion. Should be called only once.\n+\n+        Return a deferred that is fired when the crawl is finished.\n+        \"\"\"\n         if self.crawling:\n             raise RuntimeError(\"Crawling already taking place\")\n         if self._started:\n@@ -163,6 +172,42 @@ class Crawler:\n                 yield self.engine.close()\n             raise\n \n+    async def crawl_async(self, *args: Any, **kwargs: Any) -> None:\n+        \"\"\"Start the crawler by instantiating its spider class with the given\n+        *args* and *kwargs* arguments, while setting the execution engine in\n+        motion. Should be called only once.\n+\n+        .. versionadded:: VERSION\n+\n+        Complete when the crawl is finished.\n+\n+        This function requires\n+        :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` to be\n+        installed.\n+        \"\"\"\n+        if not is_asyncio_available():\n+            raise RuntimeError(\"Crawler.crawl_async() requires AsyncioSelectorReactor.\")\n+        if self.crawling:\n+            raise RuntimeError(\"Crawling already taking place\")\n+        if self._started:\n+            raise RuntimeError(\n+                \"Cannot run Crawler.crawl_async() more than once on the same instance.\"\n+            )\n+        self.crawling = self._started = True\n+\n+        try:\n+            self.spider = self._create_spider(*args, **kwargs)\n+            self._apply_settings()\n+            self._update_root_log_handler()\n+            self.engine = self._create_engine()\n+            await self.engine.open_spider_async(self.spider)\n+            await self.engine.start_async()\n+        except Exception:\n+            self.crawling = False\n+            if self.engine is not None:\n+                await deferred_to_future(self.engine.close())\n+            raise\n+\n     def _create_spider(self, *args: Any, **kwargs: Any) -> Spider:\n         return self.spidercls.from_crawler(self, *args, **kwargs)\n \n@@ -171,13 +216,26 @@ class Crawler:\n \n     @inlineCallbacks\n     def stop(self) -> Generator[Deferred[Any], Any, None]:\n-        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n+        \"\"\"Start a graceful stop of the crawler and return a deferred that is\n         fired when the crawler is stopped.\"\"\"\n         if self.crawling:\n             self.crawling = False\n             assert self.engine\n             yield self.engine.stop()\n \n+    async def stop_async(self) -> None:\n+        \"\"\"Start a graceful stop of the crawler and complete when the crawler is stopped.\n+\n+        .. versionadded:: VERSION\n+\n+        This function requires\n+        :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` to be\n+        installed.\n+        \"\"\"\n+        if not is_asyncio_available():\n+            raise RuntimeError(\"Crawler.stop_async() requires AsyncioSelectorReactor.\")\n+        await deferred_to_future(self.stop())\n+\n     @staticmethod\n     def _get_component(\n         component_class: type[_T], components: Iterable[Any]\n@@ -318,9 +376,6 @@ class CrawlerRunnerBase:\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n-    def _stop(self) -> Deferred[Any]:\n-        return DeferredList([c.stop() for c in list(self.crawlers)])\n-\n \n class CrawlerRunner(CrawlerRunnerBase):\n     \"\"\"\n@@ -397,7 +452,7 @@ class CrawlerRunner(CrawlerRunnerBase):\n \n         Returns a deferred that is fired when they all have ended.\n         \"\"\"\n-        return self._stop()\n+        return DeferredList(c.stop() for c in self.crawlers)\n \n     @inlineCallbacks\n     def join(self) -> Generator[Deferred[Any], Any, None]:\n@@ -429,14 +484,14 @@ class AsyncCrawlerRunner(CrawlerRunnerBase):\n \n     def __init__(self, settings: dict[str, Any] | Settings | None = None):\n         super().__init__(settings)\n-        self._active: set[asyncio.Future[None]] = set()\n+        self._active: set[asyncio.Task[None]] = set()\n \n     def crawl(\n         self,\n         crawler_or_spidercls: type[Spider] | str | Crawler,\n         *args: Any,\n         **kwargs: Any,\n-    ) -> asyncio.Future[None]:\n+    ) -> asyncio.Task[None]:\n         \"\"\"\n         Run a crawler with the provided arguments.\n \n@@ -447,7 +502,7 @@ class AsyncCrawlerRunner(CrawlerRunnerBase):\n         instance, this method will try to create one using this parameter as\n         the spider class given to it.\n \n-        Returns a :class:`~asyncio.Future` object which completes when the\n+        Returns a :class:`~asyncio.Task` object which completes when the\n         crawling is finished.\n \n         :param crawler_or_spidercls: already created crawler, or a spider class\n@@ -465,24 +520,27 @@ class AsyncCrawlerRunner(CrawlerRunnerBase):\n                 \"it must be a spider class (or a Crawler object)\"\n             )\n         if not is_asyncio_reactor_installed():\n-            raise RuntimeError(\"AsyncCrawlerRunner requires AsyncioSelectorReactor.\")\n+            raise RuntimeError(\n+                f\"{type(self).__name__} requires AsyncioSelectorReactor.\"\n+            )\n         crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n-    def _crawl(\n-        self, crawler: Crawler, *args: Any, **kwargs: Any\n-    ) -> asyncio.Future[None]:\n+    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> asyncio.Task[None]:\n+        # At this point the asyncio loop has been installed either by the user\n+        # or by AsyncCrawlerProcess (but it isn't running yet, so no asyncio.create_task()).\n+        loop = asyncio.get_event_loop()\n         self.crawlers.add(crawler)\n-        future = deferred_to_future(crawler.crawl(*args, **kwargs))\n-        self._active.add(future)\n+        task = loop.create_task(crawler.crawl_async(*args, **kwargs))\n+        self._active.add(task)\n \n-        def _done(_: asyncio.Future[None]) -> None:\n+        def _done(_: asyncio.Task[None]) -> None:\n             self.crawlers.discard(crawler)\n-            self._active.discard(future)\n+            self._active.discard(task)\n             self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n \n-        future.add_done_callback(_done)\n-        return future\n+        task.add_done_callback(_done)\n+        return task\n \n     async def stop(self) -> None:\n         \"\"\"\n@@ -490,7 +548,10 @@ class AsyncCrawlerRunner(CrawlerRunnerBase):\n \n         Completes when they all have ended.\n         \"\"\"\n-        await deferred_to_future(self._stop())\n+        if self.crawlers:\n+            await asyncio.wait(\n+                [asyncio.create_task(c.stop_async()) for c in self.crawlers]\n+            )\n \n     async def join(self) -> None:\n         \"\"\"\n@@ -498,33 +559,10 @@ class AsyncCrawlerRunner(CrawlerRunnerBase):\n         executions.\n         \"\"\"\n         while self._active:\n-            await asyncio.gather(*self._active)\n+            await asyncio.wait(self._active)\n \n \n-class CrawlerProcess(CrawlerRunner):\n-    \"\"\"\n-    A class to run multiple scrapy crawlers in a process simultaneously.\n-\n-    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n-    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n-    signals, like the keyboard interrupt command Ctrl-C. It also configures\n-    top-level logging.\n-\n-    This utility should be a better fit than\n-    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n-    :mod:`~twisted.internet.reactor` within your application.\n-\n-    The CrawlerProcess object must be instantiated with a\n-    :class:`~scrapy.settings.Settings` object.\n-\n-    :param install_root_handler: whether to install root logging handler\n-        (default: True)\n-\n-    This class shouldn't be needed (since Scrapy is responsible of using it\n-    accordingly) unless writing scripts that manually handle the crawling\n-    process. See :ref:`run-from-script` for an example.\n-    \"\"\"\n-\n+class CrawlerProcessBase(CrawlerRunnerBase):\n     def __init__(\n         self,\n         settings: dict[str, Any] | Settings | None = None,\n@@ -533,7 +571,6 @@ class CrawlerProcess(CrawlerRunner):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n-        self._initialized_reactor: bool = False\n \n     def _signal_shutdown(self, signum: int, _: Any) -> None:\n         from twisted.internet import reactor\n@@ -556,6 +593,75 @@ class CrawlerProcess(CrawlerRunner):\n         )\n         reactor.callFromThread(self._stop_reactor)\n \n+    def _setup_reactor(self, install_signal_handlers: bool) -> None:\n+        from twisted.internet import reactor\n+\n+        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n+        # We pass self, which is CrawlerProcess, instead of Crawler here,\n+        # which works because the default resolvers only use crawler.settings.\n+        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n+        resolver.install_on_reactor()\n+        tp = reactor.getThreadPool()\n+        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n+        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self._stop_dfd)\n+        if install_signal_handlers:\n+            reactor.addSystemEventTrigger(\n+                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n+            )\n+\n+    def _stop_dfd(self) -> Deferred[Any]:\n+        raise NotImplementedError\n+\n+    @inlineCallbacks\n+    def _graceful_stop_reactor(self) -> Generator[Deferred[Any], Any, None]:\n+        try:\n+            yield self._stop_dfd()\n+        finally:\n+            self._stop_reactor()\n+\n+    def _stop_reactor(self, _: Any = None) -> None:\n+        from twisted.internet import reactor\n+\n+        # raised if already stopped or in shutdown stage\n+        with contextlib.suppress(RuntimeError):\n+            reactor.stop()\n+\n+\n+class CrawlerProcess(CrawlerProcessBase, CrawlerRunner):\n+    \"\"\"\n+    A class to run multiple scrapy crawlers in a process simultaneously.\n+\n+    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n+    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n+    signals, like the keyboard interrupt command Ctrl-C. It also configures\n+    top-level logging.\n+\n+    This utility should be a better fit than\n+    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n+    :mod:`~twisted.internet.reactor` within your application.\n+\n+    The CrawlerProcess object must be instantiated with a\n+    :class:`~scrapy.settings.Settings` object.\n+\n+    :param install_root_handler: whether to install root logging handler\n+        (default: True)\n+\n+    This class shouldn't be needed (since Scrapy is responsible of using it\n+    accordingly) unless writing scripts that manually handle the crawling\n+    process. See :ref:`run-from-script` for an example.\n+\n+    This class provides Deferred-based APIs. Use :class:`AsyncCrawlerProcess`\n+    for modern coroutine APIs.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        settings: dict[str, Any] | Settings | None = None,\n+        install_root_handler: bool = True,\n+    ):\n+        super().__init__(settings, install_root_handler)\n+        self._initialized_reactor: bool = False\n+\n     def _create_crawler(self, spidercls: type[Spider] | str) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n@@ -563,6 +669,9 @@ class CrawlerProcess(CrawlerRunner):\n         self._initialized_reactor = True\n         return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n+    def _stop_dfd(self) -> Deferred[Any]:\n+        return self.stop()\n+\n     def start(\n         self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n     ) -> None:\n@@ -589,30 +698,86 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n-        # We pass self, which is CrawlerProcess, instead of Crawler here,\n-        # which works because the default resolvers only use crawler.settings.\n-        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n-        resolver.install_on_reactor()\n-        tp = reactor.getThreadPool()\n-        tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n-        reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n-        if install_signal_handlers:\n-            reactor.addSystemEventTrigger(\n-                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n-            )\n+        self._setup_reactor(install_signal_handlers)\n         reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n-    @inlineCallbacks\n-    def _graceful_stop_reactor(self) -> Generator[Deferred[Any], Any, None]:\n-        try:\n-            yield self.stop()\n-        finally:\n-            self._stop_reactor()\n \n-    def _stop_reactor(self, _: Any = None) -> None:\n+class AsyncCrawlerProcess(CrawlerProcessBase, AsyncCrawlerRunner):\n+    \"\"\"\n+    A class to run multiple scrapy crawlers in a process simultaneously.\n+\n+    This class extends :class:`~scrapy.crawler.AsyncCrawlerRunner` by adding support\n+    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n+    signals, like the keyboard interrupt command Ctrl-C. It also configures\n+    top-level logging.\n+\n+    This utility should be a better fit than\n+    :class:`~scrapy.crawler.AsyncCrawlerRunner` if you aren't running another\n+    :mod:`~twisted.internet.reactor` within your application.\n+\n+    The AsyncCrawlerProcess object must be instantiated with a\n+    :class:`~scrapy.settings.Settings` object.\n+\n+    :param install_root_handler: whether to install root logging handler\n+        (default: True)\n+\n+    This class shouldn't be needed (since Scrapy is responsible of using it\n+    accordingly) unless writing scripts that manually handle the crawling\n+    process. See :ref:`run-from-script` for an example.\n+\n+    This class provides coroutine APIs. It requires\n+    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        settings: dict[str, Any] | Settings | None = None,\n+        install_root_handler: bool = True,\n+    ):\n+        super().__init__(settings, install_root_handler)\n+        # We want the asyncio event loop to be installed early, so that it's\n+        # always the correct one. And as we do that, we can also install the\n+        # reactor here.\n+        # The ASYNCIO_EVENT_LOOP setting cannot be overridden by add-ons and\n+        # spiders when using AsyncCrawlerProcess.\n+        loop_path = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n+        if is_reactor_installed():\n+            # The user could install a reactor before this class is instantiated.\n+            # We need to make sure the reactor is the correct one and the loop\n+            # type matches the setting.\n+            verify_installed_reactor(_asyncio_reactor_path)\n+            if loop_path:\n+                verify_installed_asyncio_event_loop(loop_path)\n+        else:\n+            install_reactor(_asyncio_reactor_path, loop_path)\n+        self._initialized_reactor = True\n+\n+    def _stop_dfd(self) -> Deferred[Any]:\n+        return deferred_from_coro(self.stop())\n+\n+    def start(\n+        self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n+    ) -> None:\n+        \"\"\"\n+        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n+        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n+        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n+\n+        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n+        crawlers have finished, using :meth:`join`.\n+\n+        :param bool stop_after_crawl: stop or not the reactor when all\n+            crawlers have finished\n+\n+        :param bool install_signal_handlers: whether to install the OS signal\n+            handlers from Twisted and Scrapy (default: True)\n+        \"\"\"\n         from twisted.internet import reactor\n \n-        # raised if already stopped or in shutdown stage\n-        with contextlib.suppress(RuntimeError):\n-            reactor.stop()\n+        if stop_after_crawl:\n+            loop = asyncio.get_event_loop()\n+            join_task = loop.create_task(self.join())\n+            join_task.add_done_callback(self._stop_reactor)\n+\n+        self._setup_reactor(install_signal_handlers)\n+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n\n@@ -23,7 +23,6 @@ from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.utils.asyncio import is_asyncio_available\n-from scrapy.utils.reactor import _get_asyncio_event_loop\n \n if TYPE_CHECKING:\n     from collections.abc import AsyncIterator, Callable\n@@ -385,8 +384,7 @@ def deferred_from_coro(o: Awaitable[_T] | _T2) -> Deferred[_T] | _T2:\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n             return Deferred.fromCoroutine(cast(Coroutine[Deferred[Any], Any, _T], o))\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-        event_loop = _get_asyncio_event_loop()\n-        return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n+        return Deferred.fromFuture(asyncio.ensure_future(o))\n     return o\n \n \n@@ -430,6 +428,10 @@ def deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n \n     Return an :class:`asyncio.Future` object that wraps *d*.\n \n+    This function requires\n+    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` to be\n+    installed.\n+\n     When :ref:`using the asyncio reactor <install-asyncio>`, you cannot await\n     on :class:`~twisted.internet.defer.Deferred` objects from :ref:`Scrapy\n     callables defined as coroutines <coroutine-support>`, you can only await on\n@@ -442,8 +444,15 @@ def deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n                 additional_request = scrapy.Request('https://example.org/price')\n                 deferred = self.crawler.engine.download(additional_request)\n                 additional_response = await deferred_to_future(deferred)\n+\n+    .. versionchanged:: VERSION\n+        This function no longer installs an asyncio loop if called before the\n+        Twisted asyncio reactor is installed. A :exc:`RuntimeError` is raised\n+        in this case.\n     \"\"\"\n-    return d.asFuture(_get_asyncio_event_loop())\n+    if not is_asyncio_available():\n+        raise RuntimeError(\"deferred_to_future() requires AsyncioSelectorReactor.\")\n+    return d.asFuture(asyncio.get_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred[_T]) -> Deferred[_T] | Future[_T]:\n\n@@ -10,6 +10,7 @@ from twisted.internet import asyncioreactor, error\n from twisted.internet.defer import Deferred\n \n from scrapy.utils.misc import load_object\n+from scrapy.utils.python import global_object_name\n \n if TYPE_CHECKING:\n     from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n@@ -87,6 +88,9 @@ class CallLaterOnce(Generic[_T]):\n         await maybe_deferred_to_future(d)\n \n \n+_asyncio_reactor_path = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+\n+\n def set_asyncio_event_loop_policy() -> None:\n     \"\"\"The policy functions from asyncio often behave unexpectedly,\n     so we restrict their use to the absolutely essential case.\n@@ -161,21 +165,34 @@ def set_asyncio_event_loop(event_loop_path: str | None) -> AbstractEventLoop:\n \n \n def verify_installed_reactor(reactor_path: str) -> None:\n-    \"\"\"Raises :exc:`Exception` if the installed\n+    \"\"\"Raise :exc:`RuntimeError` if the installed\n     :mod:`~twisted.internet.reactor` does not match the specified import\n-    path.\"\"\"\n+    path or if no reactor is installed.\"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\n+            \"verify_installed_reactor() called without an installed reactor.\"\n+        )\n+\n     from twisted.internet import reactor\n \n-    reactor_class = load_object(reactor_path)\n-    if not reactor.__class__ == reactor_class:\n+    expected_reactor_type = load_object(reactor_path)\n+    reactor_type = type(reactor)\n+    if not reactor_type == expected_reactor_type:\n         raise RuntimeError(\n-            \"The installed reactor \"\n-            f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n-            f\"match the requested one ({reactor_path})\"\n+            f\"The installed reactor ({global_object_name(reactor_type)}) \"\n+            f\"does not match the requested one ({reactor_path})\"\n         )\n \n \n def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n+    \"\"\"Raise :exc:`RuntimeError` if the even loop of the installed\n+    :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`\n+    does not match the specified import path or if no reactor is installed.\"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\n+            \"verify_installed_asyncio_event_loop() called without an installed reactor.\"\n+        )\n+\n     from twisted.internet import reactor\n \n     loop_class = load_object(loop_path)\n@@ -185,16 +202,16 @@ def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n         f\"{reactor._asyncioEventloop.__class__.__module__}\"\n         f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n     )\n-    specified = f\"{loop_class.__module__}.{loop_class.__qualname__}\"\n     raise RuntimeError(\n         \"Scrapy found an asyncio Twisted reactor already \"\n         f\"installed, and its event loop class ({installed}) does \"\n         \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n-        f\"setting ({specified})\"\n+        f\"setting ({global_object_name(loop_class)})\"\n     )\n \n \n def is_reactor_installed() -> bool:\n+    \"\"\"Check whether a :mod:`~twisted.internet.reactor` is installed.\"\"\"\n     return \"twisted.internet.reactor\" in sys.modules\n \n \n\n@@ -0,0 +1,25 @@\n+from typing import Any\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess, Crawler\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n+        return spider\n+\n+    async def start(self):\n+        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider, foo=42)\n+process.start()\n\n@@ -0,0 +1,20 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,23 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+    custom_settings = {\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": None,\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,23 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+    custom_settings = {\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,48 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import sys\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerProcess\n+from scrapy.utils.defer import deferred_from_coro\n+\n+\n+class UppercasePipeline:\n+    async def _open_spider(self, spider):\n+        spider.logger.info(\"async pipeline opened!\")\n+        await asyncio.sleep(0.1)\n+\n+    def open_spider(self, spider):\n+        return deferred_from_coro(self._open_spider(spider))\n+\n+    def process_item(self, item, spider):\n+        return {\"url\": item[\"url\"].upper()}\n+\n+\n+class UrlSpider(Spider):\n+    name = \"url_spider\"\n+    start_urls = [\"data:,\"]\n+    custom_settings = {\n+        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n+    }\n+\n+    def parse(self, response):\n+        yield {\"url\": response.url}\n+\n+\n+if __name__ == \"__main__\":\n+    ASYNCIO_EVENT_LOOP: str | None\n+    try:\n+        ASYNCIO_EVENT_LOOP = sys.argv[1]\n+    except IndexError:\n+        ASYNCIO_EVENT_LOOP = None\n+\n+    process = AsyncCrawlerProcess(\n+        settings={\n+            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n+        }\n+    )\n+    process.crawl(UrlSpider)\n+    process.start()\n\n@@ -0,0 +1,27 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+from scrapy.utils.reactor import is_asyncio_reactor_installed\n+\n+\n+class ReactorCheckExtension:\n+    def __init__(self):\n+        if not is_asyncio_reactor_installed():\n+            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,53 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+from scrapy.utils.reactor import (\n+    install_reactor,\n+    is_asyncio_reactor_installed,\n+    is_reactor_installed,\n+)\n+\n+if is_reactor_installed():\n+    raise RuntimeError(\n+        \"Reactor already installed before is_asyncio_reactor_installed().\"\n+    )\n+\n+try:\n+    is_asyncio_reactor_installed()\n+except RuntimeError:\n+    pass\n+else:\n+    raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n+\n+if is_reactor_installed():\n+    raise RuntimeError(\n+        \"Reactor already installed after is_asyncio_reactor_installed().\"\n+    )\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+if not is_asyncio_reactor_installed():\n+    raise RuntimeError(\"Wrong reactor installed after install_reactor().\")\n+\n+\n+class ReactorCheckExtension:\n+    def __init__(self):\n+        if not is_asyncio_reactor_installed():\n+            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,29 @@\n+import asyncio\n+import sys\n+\n+from twisted.internet import asyncioreactor\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+if sys.platform == \"win32\":\n+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+asyncioreactor.install(asyncio.get_event_loop())\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,31 @@\n+import asyncio\n+import sys\n+\n+from twisted.internet import asyncioreactor\n+from uvloop import Loop\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+if sys.platform == \"win32\":\n+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+asyncio.set_event_loop(Loop())\n+asyncioreactor.install(asyncio.get_event_loop())\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,35 @@\n+import sys\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class CachingHostnameResolverSpider(scrapy.Spider):\n+    \"\"\"\n+    Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)\n+    \"\"\"\n+\n+    name = \"caching_hostname_resolver_spider\"\n+\n+    async def start(self):\n+        yield scrapy.Request(self.url)\n+\n+    def parse(self, response):\n+        for _ in range(10):\n+            yield scrapy.Request(\n+                response.url, dont_filter=True, callback=self.ignore_response\n+            )\n+\n+    def ignore_response(self, response):\n+        self.logger.info(repr(response.ip_address))\n+\n+\n+if __name__ == \"__main__\":\n+    process = AsyncCrawlerProcess(\n+        settings={\n+            \"RETRY_ENABLED\": False,\n+            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n+        }\n+    )\n+    process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])\n+    process.start()\n\n@@ -0,0 +1,22 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class CachingHostnameResolverSpider(scrapy.Spider):\n+    \"\"\"\n+    Finishes without a twisted.internet.error.DNSLookupError exception\n+    \"\"\"\n+\n+    name = \"caching_hostname_resolver_spider\"\n+    start_urls = [\"http://[::1]\"]\n+\n+\n+if __name__ == \"__main__\":\n+    process = AsyncCrawlerProcess(\n+        settings={\n+            \"RETRY_ENABLED\": False,\n+            \"DNS_RESOLVER\": \"scrapy.resolver.CachingHostnameResolver\",\n+        }\n+    )\n+    process.crawl(CachingHostnameResolverSpider)\n+    process.start()\n\n@@ -0,0 +1,18 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class IPv6Spider(scrapy.Spider):\n+    \"\"\"\n+    Raises a twisted.internet.error.DNSLookupError:\n+    the default name resolver does not handle IPv6 addresses.\n+    \"\"\"\n+\n+    name = \"ipv6_spider\"\n+    start_urls = [\"http://[::1]\"]\n+\n+\n+if __name__ == \"__main__\":\n+    process = AsyncCrawlerProcess(settings={\"RETRY_ENABLED\": False})\n+    process.crawl(IPv6Spider)\n+    process.start()\n\n@@ -0,0 +1,17 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,18 @@\n+from twisted.internet import reactor  # noqa: F401,TID253\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(settings={})\n+\n+d = process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,16 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+process = AsyncCrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,20 @@\n+import asyncio\n+import sys\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class SleepingSpider(scrapy.Spider):\n+    name = \"sleeping\"\n+\n+    start_urls = [\"data:,;\"]\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(int(sys.argv[1]))\n+\n+\n+process = AsyncCrawlerProcess(settings={})\n+\n+process.crawl(SleepingSpider)\n+process.start()\n\n@@ -0,0 +1,15 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = \"asyncio_reactor\"\n+\n+\n+process = AsyncCrawlerProcess(\n+    settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+)\n+process.crawl(AsyncioReactorSpider)\n+process.start()\n\n@@ -0,0 +1,14 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = \"asyncio_reactor\"\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+process = AsyncCrawlerProcess()\n+process.crawl(AsyncioReactorSpider)\n+process.start()\n\n@@ -0,0 +1,22 @@\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+\n+class AsyncioReactorSpider1(scrapy.Spider):\n+    name = \"asyncio_reactor1\"\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+class AsyncioReactorSpider2(scrapy.Spider):\n+    name = \"asyncio_reactor2\"\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+\n+process = AsyncCrawlerProcess()\n+process.crawl(AsyncioReactorSpider1)\n+process.crawl(AsyncioReactorSpider2)\n+process.start()\n\n@@ -0,0 +1,30 @@\n+from __future__ import annotations\n+\n+import logging\n+from typing import TYPE_CHECKING\n+\n+import scrapy\n+from scrapy.crawler import AsyncCrawlerProcess\n+\n+if TYPE_CHECKING:\n+    from asyncio import Task\n+\n+\n+class AsyncioReactorSpider(scrapy.Spider):\n+    name = \"asyncio_reactor\"\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n+    }\n+\n+\n+def log_task_exception(task: Task) -> None:\n+    try:\n+        task.result()\n+    except Exception:\n+        logging.exception(\"Crawl task failed\")\n+\n+\n+process = AsyncCrawlerProcess()\n+task = process.crawl(AsyncioReactorSpider)\n+task.add_done_callback(log_task_exception)\n+process.start()\n\n@@ -0,0 +1,31 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    await runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,31 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import AsyncCrawlerRunner\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+@deferred_f_from_coro_f\n+async def main(reactor):\n+    configure_logging()\n+    runner = AsyncCrawlerRunner()\n+    await runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\", \"uvloop.Loop\")\n+react(main)\n\n@@ -4,12 +4,12 @@ import sys\n from twisted.internet import asyncioreactor\n from twisted.python import log\n \n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n if sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n-asyncioreactor.install(asyncio.get_event_loop())\n-\n-import scrapy  # noqa: E402\n-from scrapy.crawler import CrawlerProcess  # noqa: E402\n+asyncioreactor.install()\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -4,13 +4,13 @@ import sys\n from twisted.internet import asyncioreactor\n from uvloop import Loop\n \n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n if sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncio.set_event_loop(Loop())\n-asyncioreactor.install(asyncio.get_event_loop())\n-\n-import scrapy  # noqa: E402\n-from scrapy.crawler import CrawlerProcess  # noqa: E402\n+asyncioreactor.install()\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -0,0 +1,29 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+def main(reactor):\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    return runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+react(main)\n\n@@ -0,0 +1,29 @@\n+from twisted.internet.task import react\n+\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+from scrapy.utils.reactor import install_reactor\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+    }\n+\n+    async def start(self):\n+        return\n+        yield\n+\n+\n+def main(reactor):\n+    configure_logging()\n+    runner = CrawlerRunner()\n+    return runner.crawl(NoRequestsSpider)\n+\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\", \"uvloop.Loop\")\n+react(main)\n\n@@ -1,3 +1,4 @@\n+import asyncio\n import logging\n import platform\n import re\n@@ -5,6 +6,7 @@ import signal\n import subprocess\n import sys\n import warnings\n+from abc import ABC, abstractmethod\n from pathlib import Path\n from typing import Any\n \n@@ -18,12 +20,18 @@ from zope.interface.exceptions import MultipleInvalid\n \n import scrapy\n from scrapy import Spider\n-from scrapy.crawler import AsyncCrawlerRunner, Crawler, CrawlerProcess, CrawlerRunner\n+from scrapy.crawler import (\n+    AsyncCrawlerProcess,\n+    AsyncCrawlerRunner,\n+    Crawler,\n+    CrawlerProcess,\n+    CrawlerRunner,\n+)\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n-from scrapy.utils.defer import deferred_from_coro\n+from scrapy.utils.defer import deferred_f_from_coro_f, deferred_from_coro\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler, get_reactor_settings\n@@ -88,12 +96,39 @@ class TestCrawler(TestBaseCrawler):\n             Crawler(DefaultSpider())\n \n     @inlineCallbacks\n-    def test_crawler_crawl_twice_unsupported(self):\n+    def test_crawler_crawl_twice_seq_unsupported(self):\n         crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n         yield crawler.crawl()\n         with pytest.raises(RuntimeError, match=\"more than once on the same instance\"):\n             yield crawler.crawl()\n \n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_crawler_crawl_async_twice_seq_unsupported(self):\n+        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n+        await crawler.crawl_async()\n+        with pytest.raises(RuntimeError, match=\"more than once on the same instance\"):\n+            await crawler.crawl_async()\n+\n+    @inlineCallbacks\n+    def test_crawler_crawl_twice_parallel_unsupported(self):\n+        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n+        d1 = crawler.crawl()\n+        d2 = crawler.crawl()\n+        yield d1\n+        with pytest.raises(RuntimeError, match=\"Crawling already taking place\"):\n+            yield d2\n+\n+    @pytest.mark.only_asyncio\n+    @deferred_f_from_coro_f\n+    async def test_crawler_crawl_async_twice_parallel_unsupported(self):\n+        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n+        t1 = asyncio.create_task(crawler.crawl_async())\n+        t2 = asyncio.create_task(crawler.crawl_async())\n+        await t1\n+        with pytest.raises(RuntimeError, match=\"Crawling already taking place\"):\n+            await t2\n+\n     def test_get_addon(self):\n         class ParentAddon:\n             pass\n@@ -590,6 +625,18 @@ class TestCrawlerProcess(TestBaseCrawler):\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n \n+@pytest.mark.only_asyncio\n+class TestAsyncCrawlerProcess(TestBaseCrawler):\n+    def test_crawler_process_accepts_dict(self):\n+        runner = AsyncCrawlerProcess({\"foo\": \"bar\"})\n+        assert runner.settings[\"foo\"] == \"bar\"\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n+\n+    def test_crawler_process_accepts_None(self):\n+        runner = AsyncCrawlerProcess()\n+        self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n+\n+\n class ExceptionSpider(scrapy.Spider):\n     name = \"exception\"\n \n@@ -692,8 +739,15 @@ class TestAsyncCrawlerRunnerHasSpider(TestCrawlerRunnerHasSpider):\n         pytest.skip(\"This test is only for CrawlerRunner\")\n \n \n-class ScriptRunnerMixin:\n-    script_dir: Path\n+class ScriptRunnerMixin(ABC):\n+    @property\n+    @abstractmethod\n+    def script_dir(self) -> Path:\n+        raise NotImplementedError\n+\n+    @staticmethod\n+    def get_script_dir(name: str) -> Path:\n+        return Path(__file__).parent.resolve() / name\n \n     def get_script_args(self, script_name: str, *script_args: str) -> list[str]:\n         script_path = self.script_dir / script_name\n@@ -711,8 +765,10 @@ class ScriptRunnerMixin:\n         return stderr.decode(\"utf-8\")\n \n \n-class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n-    script_dir = Path(__file__).parent.resolve() / \"CrawlerProcess\"\n+class TestCrawlerProcessSubprocessBase(ScriptRunnerMixin, unittest.TestCase):\n+    \"\"\"Common tests between CrawlerProcess and AsyncCrawlerProcess,\n+    with the same file names and expectations.\n+    \"\"\"\n \n     def test_simple(self):\n         log = self.run_script(\"simple.py\")\n@@ -739,48 +795,6 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n         ) in log\n \n-    def test_reactor_default_twisted_reactor_select(self):\n-        log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n-        if platform.system() in [\"Windows\", \"Darwin\"]:\n-            # The goal of this test function is to test that, when a reactor is\n-            # installed (the default one here) and a different reactor is\n-            # configured (select here), an error raises.\n-            #\n-            # In Windows the default reactor is the select reactor, so that\n-            # error does not raise.\n-            #\n-            # If that ever becomes the case on more platforms (i.e. if Linux\n-            # also starts using the select reactor by default in a future\n-            # version of Twisted), then we will need to rethink this test.\n-            assert \"Spider closed (finished)\" in log\n-        else:\n-            assert \"Spider closed (finished)\" not in log\n-            assert (\n-                \"does not match the requested one \"\n-                \"(twisted.internet.selectreactor.SelectReactor)\"\n-            ) in log\n-\n-    def test_reactor_select(self):\n-        log = self.run_script(\"reactor_select.py\")\n-        assert \"Spider closed (finished)\" not in log\n-        assert (\n-            \"does not match the requested one \"\n-            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n-        ) in log\n-\n-    def test_reactor_select_twisted_reactor_select(self):\n-        log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert \"ReactorAlreadyInstalledError\" not in log\n-\n-    def test_reactor_select_subclass_twisted_reactor_select(self):\n-        log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n-        assert \"Spider closed (finished)\" not in log\n-        assert (\n-            \"does not match the requested one \"\n-            \"(twisted.internet.selectreactor.SelectReactor)\"\n-        ) in log\n-\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script(\"asyncio_enabled_no_reactor.py\")\n         assert \"Spider closed (finished)\" in log\n@@ -829,19 +843,6 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             assert \"TimeoutError\" not in log\n             assert \"twisted.internet.error.DNSLookupError\" not in log\n \n-    def test_twisted_reactor_select(self):\n-        log = self.run_script(\"twisted_reactor_select.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n-\n-    @pytest.mark.skipif(\n-        platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n-    )\n-    def test_twisted_reactor_poll(self):\n-        log = self.run_script(\"twisted_reactor_poll.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert \"Using reactor: twisted.internet.pollreactor.PollReactor\" in log\n-\n     def test_twisted_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n         assert \"Spider closed (finished)\" in log\n@@ -866,14 +867,6 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             in log\n         )\n \n-    def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n-        log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n-        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n-        assert (\n-            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\"\n-            in log\n-        )\n-\n     @pytest.mark.requires_uvloop\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n@@ -960,8 +953,113 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.wait()\n \n \n-class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n-    script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n+class TestCrawlerProcessSubprocess(TestCrawlerProcessSubprocessBase):\n+    @property\n+    def script_dir(self) -> Path:\n+        return self.get_script_dir(\"CrawlerProcess\")\n+\n+    def test_reactor_default_twisted_reactor_select(self):\n+        log = self.run_script(\"reactor_default_twisted_reactor_select.py\")\n+        if platform.system() in [\"Windows\", \"Darwin\"]:\n+            # The goal of this test function is to test that, when a reactor is\n+            # installed (the default one here) and a different reactor is\n+            # configured (select here), an error raises.\n+            #\n+            # In Windows the default reactor is the select reactor, so that\n+            # error does not raise.\n+            #\n+            # If that ever becomes the case on more platforms (i.e. if Linux\n+            # also starts using the select reactor by default in a future\n+            # version of Twisted), then we will need to rethink this test.\n+            assert \"Spider closed (finished)\" in log\n+        else:\n+            assert \"Spider closed (finished)\" not in log\n+            assert (\n+                \"does not match the requested one \"\n+                \"(twisted.internet.selectreactor.SelectReactor)\"\n+            ) in log\n+\n+    def test_reactor_select(self):\n+        log = self.run_script(\"reactor_select.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"does not match the requested one \"\n+            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor)\"\n+        ) in log\n+\n+    def test_reactor_select_twisted_reactor_select(self):\n+        log = self.run_script(\"reactor_select_twisted_reactor_select.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert \"ReactorAlreadyInstalledError\" not in log\n+\n+    def test_reactor_select_subclass_twisted_reactor_select(self):\n+        log = self.run_script(\"reactor_select_subclass_twisted_reactor_select.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"does not match the requested one \"\n+            \"(twisted.internet.selectreactor.SelectReactor)\"\n+        ) in log\n+\n+    def test_twisted_reactor_select(self):\n+        log = self.run_script(\"twisted_reactor_select.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n+\n+    @pytest.mark.skipif(\n+        platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n+    )\n+    def test_twisted_reactor_poll(self):\n+        log = self.run_script(\"twisted_reactor_poll.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert \"Using reactor: twisted.internet.pollreactor.PollReactor\" in log\n+\n+    def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n+        log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n+        assert \"Using reactor: twisted.internet.selectreactor.SelectReactor\" in log\n+        assert (\n+            \"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\"\n+            in log\n+        )\n+\n+\n+class TestAsyncCrawlerProcessSubprocess(TestCrawlerProcessSubprocessBase):\n+    @property\n+    def script_dir(self) -> Path:\n+        return self.get_script_dir(\"AsyncCrawlerProcess\")\n+\n+    def test_twisted_reactor_custom_settings_select(self):\n+        log = self.run_script(\"twisted_reactor_custom_settings_select.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"(twisted.internet.asyncioreactor.AsyncioSelectorReactor) \"\n+            \"does not match the requested one \"\n+            \"(twisted.internet.selectreactor.SelectReactor)\"\n+        ) in log\n+\n+    @pytest.mark.requires_uvloop\n+    def test_asyncio_enabled_reactor_same_loop(self):\n+        log = self.run_script(\"asyncio_custom_loop_custom_settings_same.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n+\n+    @pytest.mark.requires_uvloop\n+    def test_asyncio_enabled_reactor_different_loop(self):\n+        log = self.run_script(\"asyncio_custom_loop_custom_settings_different.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n+            \"setting (uvloop.Loop)\"\n+        ) in log\n+\n+\n+class TestCrawlerRunnerSubprocessBase(ScriptRunnerMixin):\n+    \"\"\"Common tests between CrawlerRunner and AsyncCrawlerRunner,\n+    with the same file names and expectations.\n+    \"\"\"\n \n     def test_simple(self):\n         log = self.run_script(\"simple.py\")\n@@ -971,14 +1069,6 @@ class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n             in log\n         )\n \n-    def test_explicit_default_reactor(self):\n-        log = self.run_script(\"explicit_default_reactor.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert (\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            not in log\n-        )\n-\n     def test_multi_parallel(self):\n         log = self.run_script(\"multi_parallel.py\")\n         assert \"Spider closed (finished)\" in log\n@@ -1005,6 +1095,39 @@ class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n             re.DOTALL,\n         )\n \n+    @pytest.mark.requires_uvloop\n+    def test_custom_loop_same(self):\n+        log = self.run_script(\"custom_loop_same.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            in log\n+        )\n+        assert \"Using asyncio event loop: uvloop.Loop\" in log\n+\n+    @pytest.mark.requires_uvloop\n+    def test_custom_loop_different(self):\n+        log = self.run_script(\"custom_loop_different.py\")\n+        assert \"Spider closed (finished)\" not in log\n+        assert (\n+            \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n+            \"setting (uvloop.Loop)\"\n+        ) in log\n+\n+\n+class TestCrawlerRunnerSubprocess(TestCrawlerRunnerSubprocessBase):\n+    @property\n+    def script_dir(self) -> Path:\n+        return self.get_script_dir(\"CrawlerRunner\")\n+\n+    def test_explicit_default_reactor(self):\n+        log = self.run_script(\"explicit_default_reactor.py\")\n+        assert \"Spider closed (finished)\" in log\n+        assert (\n+            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+            not in log\n+        )\n+\n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n         assert \"INFO: Spider closed (finished)\" in log\n@@ -1021,48 +1144,16 @@ class TestCrawlerRunnerSubprocess(ScriptRunnerMixin):\n         assert \"DEBUG: Using asyncio event loop\" in log\n \n \n-class TestAsyncCrawlerRunnerSubprocess(ScriptRunnerMixin):\n-    script_dir = Path(__file__).parent.resolve() / \"AsyncCrawlerRunner\"\n-\n-    def test_simple(self):\n-        log = self.run_script(\"simple.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert (\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            in log\n-        )\n+class TestAsyncCrawlerRunnerSubprocess(TestCrawlerRunnerSubprocessBase):\n+    @property\n+    def script_dir(self) -> Path:\n+        return self.get_script_dir(\"AsyncCrawlerRunner\")\n \n     def test_simple_default_reactor(self):\n         log = self.run_script(\"simple_default_reactor.py\")\n         assert \"Spider closed (finished)\" not in log\n         assert \"RuntimeError: AsyncCrawlerRunner requires AsyncioSelectorReactor\" in log\n \n-    def test_multi_parallel(self):\n-        log = self.run_script(\"multi_parallel.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert (\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            in log\n-        )\n-        assert re.search(\n-            r\"Spider opened.+Spider opened.+Closing spider.+Closing spider\",\n-            log,\n-            re.DOTALL,\n-        )\n-\n-    def test_multi_seq(self):\n-        log = self.run_script(\"multi_seq.py\")\n-        assert \"Spider closed (finished)\" in log\n-        assert (\n-            \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n-            in log\n-        )\n-        assert re.search(\n-            r\"Spider opened.+Closing spider.+Spider opened.+Closing spider\",\n-            log,\n-            re.DOTALL,\n-        )\n-\n \n @pytest.mark.parametrize(\n     (\"settings\", \"items\"),\n\n@@ -6,6 +6,7 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.utils.defer import deferred_f_from_coro_f\n from scrapy.utils.reactor import (\n+    _asyncio_reactor_path,\n     install_reactor,\n     is_asyncio_reactor_installed,\n     set_asyncio_event_loop,\n@@ -22,7 +23,7 @@ class TestAsyncio(TestCase):\n         from twisted.internet import reactor as original_reactor\n \n         with warnings.catch_warnings(record=True) as w:\n-            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+            install_reactor(_asyncio_reactor_path)\n             assert len(w) == 0, [str(warning) for warning in w]\n         from twisted.internet import reactor  # pylint: disable=reimported\n \n@@ -31,5 +32,5 @@ class TestAsyncio(TestCase):\n     @pytest.mark.only_asyncio\n     @deferred_f_from_coro_f\n     async def test_set_asyncio_event_loop(self):\n-        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        install_reactor(_asyncio_reactor_path)\n         assert set_asyncio_event_loop(None) is asyncio.get_running_loop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
