{"custom_id": "keras#d290db4b0cd055b84aa17b7264615d468921280a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 6618 | Contributors (this commit): 26 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -1744,6 +1744,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         shutil.rmtree(savepath)\n \n     @test_combinations.run_with_all_model_types\n+    @test_utils.run_v2_only\n     def test_fit_with_ModelCheckpoint_with_steps_per_execution(self):\n         layers = [\n             keras.layers.Dense(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bebf726ae4294c4049a8fccd02a78a06dcfdfef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 9563 | Contributors (this commit): 117 | Commits (past 90d): 6 | Contributors (cumulative): 117 | DMM Complexity: None\n\nDIFF:\n@@ -2327,10 +2327,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     * Weight histograms\n     * Sampled profiling\n \n-    When used in `Model.evaluate`, in addition to epoch summaries, there will be\n-    a summary that records evaluation metrics vs `Model.optimizer.iterations`\n-    written. The metric names will be prepended with `evaluation`, with\n-    `Model.optimizer.iterations` being the step in the visualized TensorBoard.\n+    When used in `Model.evaluate` or regular validation\n+    ([on_test_end](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_test_end)),\n+    in addition to epoch summaries, there will be a summary that records\n+    evaluation metrics vs `Model.optimizer.iterations` written. The metric names\n+    will be prepended with `evaluation`, with `Model.optimizer.iterations` being\n+    the step in the visualized TensorBoard.\n \n     If you have installed TensorFlow with pip, you should be able\n     to launch TensorBoard from the command line:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ecbabf27273af8ece5028d2c4aaa814883980c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 50 | Files Changed: 4 | Hunks: 21 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 72 | Churn Cumulative: 1512 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import integration_test_utils\n-from keras.dtensor import optimizers as optimizer_lib\n from keras.dtensor import test_util\n+from keras.optimizers import adam\n from keras.utils import tf_utils\n \n \n@@ -47,7 +47,7 @@ class MnistTest(test_util.DTensorBaseTest):\n             integration_test_utils.get_all_replicated_layout_map(mesh)\n         )\n \n-        optimizer = optimizer_lib.Adam(learning_rate=0.001, mesh=mesh)\n+        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n         optimizer.build(model.trainable_variables)\n \n         train_losses = integration_test_utils.train_mnist_model_batch_sharded(\n@@ -76,7 +76,7 @@ class MnistTest(test_util.DTensorBaseTest):\n             integration_test_utils.get_all_replicated_layout_map(mesh)\n         )\n \n-        optimizer = optimizer_lib.Adam(learning_rate=0.001, mesh=mesh)\n+        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n         optimizer.build(model.trainable_variables)\n \n         train_losses = integration_test_utils.train_mnist_model_batch_sharded(\n\n@@ -1,29 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"DTensor specific Keras optimizers.\"\"\"\n-\n-from keras.optimizers import adadelta\n-from keras.optimizers import adagrad\n-from keras.optimizers import adam\n-from keras.optimizers import adamw\n-from keras.optimizers import rmsprop\n-from keras.optimizers import sgd\n-\n-Adadelta = adadelta.Adadelta\n-Adagrad = adagrad.Adagrad\n-Adam = adam.Adam\n-AdamW = adamw.AdamW\n-RMSprop = rmsprop.RMSprop\n-SGD = sgd.SGD\n\n@@ -26,9 +26,13 @@ from keras import losses\n from keras import models\n from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import layout_map\n-from keras.dtensor import optimizers as dtensor_optimizers\n from keras.dtensor import test_util\n+from keras.optimizers import adadelta\n+from keras.optimizers import adagrad\n from keras.optimizers import adam\n+from keras.optimizers import adamw\n+from keras.optimizers import rmsprop\n+from keras.optimizers import sgd\n \n \n class OptimizersTest(test_util.DTensorBaseTest):\n@@ -47,9 +51,8 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         }\n         self.mesh = self.configTestMesh(mesh_dict)\n \n-    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n-    def test_add_variable_from_reference(self, optimizer_cls):\n-        optimizer = optimizer_cls(mesh=self.mesh)\n+    def test_add_variable_from_reference(self):\n+        optimizer = adam.Adam(mesh=self.mesh)\n         variable_init_value = tf.ones([4, 4], dtype=tf.float32)\n         variable_init_value = dtensor.copy_to_mesh(\n             variable_init_value,\n@@ -66,9 +69,8 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         # Make sure the variable contains the correct layout info\n         self.assertEqual(state_variable.layout, model_variable.layout)\n \n-    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n-    def test_build_index_dict(self, optimizer_cls):\n-        optimizer = optimizer_cls(mesh=self.mesh)\n+    def test_build_index_dict(self):\n+        optimizer = adam.Adam(mesh=self.mesh)\n         variable_init_value = tf.ones(shape=(), dtype=tf.float32)\n         variable_init_value = dtensor.copy_to_mesh(\n             variable_init_value,\n@@ -86,7 +88,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n     @parameterized.named_parameters(\n         (\n             \"Adadelta\",\n-            dtensor_optimizers.Adadelta,\n+            adadelta.Adadelta,\n             {},\n             [\n                 \"Adadelta/accumulated_grad/Variable\",\n@@ -96,7 +98,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         ),\n         (\n             \"Adam\",\n-            dtensor_optimizers.Adam,\n+            adam.Adam,\n             {\"amsgrad\": True},\n             [\n                 \"Adam/m/Variable\",\n@@ -107,7 +109,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         ),\n         (\n             \"AdamW\",\n-            dtensor_optimizers.AdamW,\n+            adamw.AdamW,\n             {\"amsgrad\": True},\n             [\n                 \"AdamW/m/Variable\",\n@@ -118,13 +120,13 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         ),\n         (\n             \"Adagrad\",\n-            dtensor_optimizers.Adagrad,\n+            adagrad.Adagrad,\n             {},\n             [\"Adagrad/accumulator/Variable\", \"iteration\"],\n         ),\n         (\n             \"RMSprop\",\n-            dtensor_optimizers.RMSprop,\n+            rmsprop.RMSprop,\n             {\"momentum\": 0.1, \"centered\": True},\n             [\n                 \"RMSprop/velocity/Variable\",\n@@ -135,7 +137,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         ),\n         (\n             \"SGD\",\n-            dtensor_optimizers.SGD,\n+            sgd.SGD,\n             {\"momentum\": 0.1},\n             [\"SGD/m/Variable\", \"iteration\"],\n         ),\n@@ -167,8 +169,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         all_names = [var._shared_name for var in optimizer_variables]\n         self.assertCountEqual(all_names, expect_variable_names)\n \n-    @parameterized.parameters([adam.Adam, dtensor_optimizers.Adam])\n-    def test_embedding_lookup_backward_path(self, optimizer_cls):\n+    def test_embedding_lookup_backward_path(self):\n         # See b/265441685 for more context.\n         backend.enable_tf_random_generator()\n         os.environ[\n@@ -212,7 +213,7 @@ class OptimizersTest(test_util.DTensorBaseTest):\n             preds = layers.Dense(output_size, activation=\"softmax\")(x)\n             model = models.Model(inputs, preds)\n \n-        optimizer = optimizer_cls(mesh=self.mesh)\n+        optimizer = adam.Adam(mesh=self.mesh)\n \n         @tf.function\n         def train_func(model, inputs, label, optimizer):\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras.dtensor import integration_test_utils\n-from keras.dtensor import optimizers\n from keras.dtensor import test_util\n+from keras.optimizers import adam\n from keras.utils import tf_utils\n \n # isort: off\n@@ -52,7 +52,7 @@ class TrainingTest(test_util.DTensorBaseTest):\n     @parameterized.product(\n         run_eagerly=[True, False],\n         jit_compile=[True, False],\n-        optimizer_creator=[lambda: optimizers.Adam(), lambda: \"adam\"],\n+        optimizer_creator=[lambda: adam.Adam(), lambda: \"adam\"],\n     )\n     def test_model_fit(self, run_eagerly, jit_compile, optimizer_creator):\n         if run_eagerly and jit_compile:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c72e310ab5452e5a8bbcc2f18ab092f21d8846b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2dfd7fcf7e0299c12cce2da72601e367fc3b0d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3865 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,12 +49,14 @@ class _UnwrapPreventer:\n def _is_all_finite(grads):\n     \"\"\"Returns a scalar boolean tensor indicating if all gradients are\n     finite.\"\"\"\n+\n     def raw_values(g):\n         return g.values if isinstance(g, tf.IndexedSlices) else g\n \n     is_finite_per_grad = [\n         tf.reduce_all(tf.math.is_finite(raw_values(g)))\n-        for g in grads if g is not None\n+        for g in grads\n+        if g is not None\n     ]\n     return tf.reduce_all(is_finite_per_grad)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f23b6e1d4f038ba1a3c559215f094f93246491c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 24554 | Contributors (this commit): 163 | Commits (past 90d): 17 | Contributors (cumulative): 183 | DMM Complexity: None\n\nDIFF:\n@@ -3362,7 +3362,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 terminal window sizes).\n             positions: Relative or absolute positions of log elements\n                 in each line. If not provided,\n-                defaults to `[.33, .55, .67, 1.]`.\n+                defaults to `[0.3, 0.6, 0.70, 1.]`\n             print_fn: Print function to use. By default, prints to `stdout`.\n                 If `stdout` doesn't work in your environment, change to `print`.\n                 It will be called on each line of the summary.\n\n@@ -330,7 +330,7 @@ def print_summary(\n             (e.g. set this to adapt the display to different\n             terminal window sizes).\n         positions: Relative or absolute positions of log elements in each line.\n-            If not provided, defaults to `[.33, .55, .67, 1.]`.\n+            If not provided, defaults to `[0.3, 0.6, 0.70, 1.]`.\n         print_fn: Print function to use.\n             It will be called on each line of the summary.\n             You can set it to a custom function\n@@ -395,7 +395,7 @@ def print_summary(\n         to_display = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n     else:\n         line_length = line_length or 98\n-        positions = positions or [0.33, 0.55, 0.67, 1.0]\n+        positions = positions or [0.3, 0.6, 0.70, 1.0]\n         if positions[-1] <= 1:\n             positions = [int(line_length * p) for p in positions]\n         # header names for the different log elements\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00b6a12f78dc621a54d2ca4fcac7c24ce4b22325", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1130 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -167,7 +167,7 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n                         x.synchronization, x._variable.synchronization\n                     )\n                     self.assertEqual(x.aggregation, x._variable.aggregation)\n-                    self.assertEqual(self.evaluate(x.initialized_value()), 7)\n+                    self.assertEqual(self.evaluate(x.read_value()), 7)\n                     if not tf.executing_eagerly():\n                         if not tf.distribute.has_strategy():\n                             # These functions are not supported for\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff1fd84ae21f4c0e2fe6ba28739760f927c66d9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 109 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -46,20 +46,20 @@ class MemoryTest(tf.test.TestCase):\n         if not memory_test_util.memory_profiler_is_available():\n             self.skipTest(\"memory_profiler required to run this test\")\n \n-        inputs = tf.zeros([32, 100], tf.float32)\n+        inputs = tf.zeros([1000, 1000], tf.float32)\n         net = SingleLayerNet()\n \n         def f():\n             with tf.GradientTape():\n                 net(inputs)\n \n-        memory_test_util.assert_no_leak(f)\n+        memory_test_util.assert_no_leak(f, num_iters=1000)\n \n     def testMemoryLeakInSimpleModelForwardAndBackward(self):\n         if not memory_test_util.memory_profiler_is_available():\n             self.skipTest(\"memory_profiler required to run this test\")\n \n-        inputs = tf.zeros([32, 100], tf.float32)\n+        inputs = tf.zeros([1000, 1000], tf.float32)\n         net = SingleLayerNet()\n \n         def f():\n@@ -70,7 +70,7 @@ class MemoryTest(tf.test.TestCase):\n \n             del tape\n \n-        memory_test_util.assert_no_leak(f)\n+        memory_test_util.assert_no_leak(f, num_iters=1000)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1dc1d0af7b8257ab5016e8ac4d0c37601a5b9e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 107 | Churn Cumulative: 14603 | Contributors (this commit): 32 | Commits (past 90d): 9 | Contributors (cumulative): 56 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1715,21 +1715,20 @@ def _make_class_weight_map_fn(class_weight):\n                 \"output.\"\n             )\n \n-        if y.shape.rank > 2:\n-            raise ValueError(\n-                \"`class_weight` not supported for 3+ dimensional targets.\"\n-            )\n-\n         y_classes = tf.__internal__.smart_cond.smart_cond(\n-            y.shape.rank == 2 and backend.shape(y)[1] > 1,\n-            lambda: backend.argmax(y, axis=1),\n-            lambda: tf.cast(backend.reshape(y, (-1,)), tf.int64),\n+            backend.shape(y)[-1] > 1,\n+            lambda: backend.argmax(y, axis=-1),\n+            lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int64),\n         )\n \n         cw = tf.gather(class_weight_tensor, y_classes)\n         if sw is not None:\n             cw = tf.cast(cw, sw.dtype)\n             # `class_weight` and `sample_weight` are multiplicative.\n+            # If class_weight has more than 2 dimensions, we need to reshape\n+            # sample_weight to make broadcasting possible for multiplication.\n+            rank_delta = cw.shape.rank - sw.shape.rank\n+            sw = tf.reshape(sw, sw.shape + [1] * rank_delta)\n             sw = sw * cw\n         else:\n             sw = cw\n\n@@ -1374,6 +1374,58 @@ class DataHandlerTest(test_combinations.TestCase):\n                 class_weight={0: 0.5, 1: 1.0, 2: 1.5},\n             )\n \n+    @parameterized.named_parameters((\"one_hot\", True), (\"sparse\", False))\n+    def test_class_weights_applied(self, one_hot):\n+        num_channels = 3\n+        num_classes = 5\n+        batch_size = 2\n+        image_width = 8\n+\n+        input_shape = (batch_size, image_width, image_width, num_channels)\n+        output_shape = (batch_size, image_width, image_width)\n+\n+        x = tf.random.uniform(input_shape)\n+        sparse_y = tf.random.uniform(\n+            output_shape, maxval=num_classes, dtype=tf.int32\n+        )\n+\n+        if one_hot:\n+            y = tf.one_hot(sparse_y, num_classes)\n+        else:\n+            y = tf.expand_dims(sparse_y, axis=-1)\n+\n+        # Class weight is equal to class number + 1\n+        class_weight = dict([(x, x + 1) for x in range(num_classes)])\n+\n+        sample_weight = np.array([1, 2])\n+\n+        data_handler = data_adapter.DataHandler(\n+            x=x,\n+            y=y,\n+            class_weight=class_weight,\n+            sample_weight=sample_weight,\n+            batch_size=batch_size,\n+            epochs=1,\n+        )\n+        returned_data = []\n+        for _, iterator in data_handler.enumerate_epochs():\n+            epoch_data = []\n+            for _ in data_handler.steps():\n+                epoch_data.append(next(iterator))\n+            returned_data.append(epoch_data)\n+        returned_data = self.evaluate(returned_data)\n+\n+        # We had only 1 batch and 1 epoch, so we extract x, y, sample_weight\n+        result_x, result_y, result_sample_weight = returned_data[0][0]\n+        self.assertAllEqual(x, result_x)\n+        self.assertAllEqual(y, result_y)\n+\n+        # Because class weight = class + 1, resulting class weight = y + 1\n+        # Sample weight is 1 for the first sample, 2 for the second,\n+        # so we double the expected sample weight for the second sample.\n+        self.assertAllEqual(sparse_y[0] + 1, result_sample_weight[0])\n+        self.assertAllEqual(2 * (sparse_y[1] + 1), result_sample_weight[1])\n+\n     @parameterized.named_parameters((\"numpy\", True), (\"dataset\", False))\n     def test_single_x_input_no_tuple_wrapping(self, use_numpy):\n         x = np.ones((10, 1))\n\n@@ -2585,6 +2585,46 @@ class LossWeightingTest(test_combinations.TestCase):\n         # TODO(b/152990697): Fix the class weights test here.\n         # self.assertLess(score[0], ref_score[0])\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_segmentation_class_weights(self):\n+        num_channels = 3\n+        num_classes = 5\n+        batch_size = 2\n+        image_width = 8\n+\n+        input_shape = (batch_size, image_width, image_width, num_channels)\n+        output_shape = (batch_size, image_width, image_width, num_classes)\n+\n+        model = sequential.Sequential([layers_module.Conv2D(num_classes, 1)])\n+\n+        model.compile(\n+            loss=\"categorical_crossentropy\",\n+            metrics=[\"acc\", metrics_module.CategoricalAccuracy()],\n+            weighted_metrics=[\"mae\", metrics_module.CategoricalAccuracy()],\n+            optimizer=\"adam\",\n+            run_eagerly=test_utils.should_run_eagerly(),\n+        )\n+\n+        x = tf.random.uniform(input_shape)\n+        y = tf.random.uniform(output_shape, dtype=tf.int32, maxval=num_classes)\n+\n+        # Class weights are just the class value + 1\n+        class_weight = dict([(i, i + 1) for i in range(num_classes)])\n+\n+        # This test simply asserts that the model can be compiled and fit\n+        # can run without error. Verification that the class weights are\n+        # applied correctly is performed in data_adapter_test.\n+        model.fit(x, y, class_weight=class_weight, steps_per_epoch=1)\n+\n+        sample_weight = np.array([x + 1 for x in range(batch_size)])\n+        model.fit(\n+            x,\n+            y,\n+            class_weight=class_weight,\n+            sample_weight=sample_weight,\n+            steps_per_epoch=1,\n+        )\n+\n     @test_combinations.run_all_keras_modes\n     def test_temporal_sample_weights(self):\n         num_classes = 5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46269ea5fc5d404051c47a7eadcd42acd2a62ad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 223 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -116,7 +116,6 @@ class SpectralNormalizationTest(test_combinations.TestCase):\n                 keras.layers.MaxPooling2D(2, 2)\n             ).build((2, 2))\n \n-    @test_combinations.run_all_keras_modes\n     @parameterized.parameters(\n         [\n             (lambda: keras.layers.Dense(2), [3, 2]),\n@@ -127,15 +126,15 @@ class SpectralNormalizationTest(test_combinations.TestCase):\n             (lambda: keras.layers.Embedding(2, 10), [2]),\n         ],\n     )\n+    @test_combinations.run_all_keras_modes\n     def test_model_build(self, base_layer_fn, input_shape):\n         inputs = keras.layers.Input(shape=input_shape)\n         base_layer = base_layer_fn()\n         sn_layer = keras.layers.SpectralNormalization(base_layer)\n         model = keras.models.Sequential(layers=[inputs, sn_layer])\n         model.build()\n-        self.assertTrue(hasattr(model.layers[0], \"u\"))\n+        self.assertTrue(hasattr(model.layers[0], \"vector_u\"))\n \n-    @test_combinations.run_all_keras_modes\n     @parameterized.parameters(\n         [\n             (lambda: keras.layers.Dense(2), [3, 2], [3, 2]),\n@@ -147,6 +146,7 @@ class SpectralNormalizationTest(test_combinations.TestCase):\n             (lambda: keras.layers.Embedding(2, 10), [2], [2, 10]),\n         ],\n     )\n+    @test_combinations.run_all_keras_modes\n     def test_model_fit(self, base_layer_fn, input_shape, output_shape):\n         inputs = keras.layers.Input(shape=input_shape)\n         base_layer = base_layer_fn()\n@@ -166,4 +166,4 @@ class SpectralNormalizationTest(test_combinations.TestCase):\n             batch_size=10,\n             verbose=0,\n         )\n-        self.assertTrue(hasattr(model.layers[0], \"u\"))\n+        self.assertTrue(hasattr(model.layers[0], \"vector_u\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6080ffbd3a145b2fd4cf4266a691221d5f5f1ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 99 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -6/2 | Churn Δ: 107 | Churn Cumulative: 14710 | Contributors (this commit): 32 | Commits (past 90d): 12 | Contributors (cumulative): 56 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1715,20 +1715,21 @@ def _make_class_weight_map_fn(class_weight):\n                 \"output.\"\n             )\n \n+        if y.shape.rank > 2:\n+            raise ValueError(\n+                \"`class_weight` not supported for 3+ dimensional targets.\"\n+            )\n+\n         y_classes = tf.__internal__.smart_cond.smart_cond(\n-            backend.shape(y)[-1] > 1,\n-            lambda: backend.argmax(y, axis=-1),\n-            lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int64),\n+            y.shape.rank == 2 and backend.shape(y)[1] > 1,\n+            lambda: backend.argmax(y, axis=1),\n+            lambda: tf.cast(backend.reshape(y, (-1,)), tf.int64),\n         )\n \n         cw = tf.gather(class_weight_tensor, y_classes)\n         if sw is not None:\n             cw = tf.cast(cw, sw.dtype)\n             # `class_weight` and `sample_weight` are multiplicative.\n-            # If class_weight has more than 2 dimensions, we need to reshape\n-            # sample_weight to make broadcasting possible for multiplication.\n-            rank_delta = cw.shape.rank - sw.shape.rank\n-            sw = tf.reshape(sw, sw.shape + [1] * rank_delta)\n             sw = sw * cw\n         else:\n             sw = cw\n\n@@ -1374,58 +1374,6 @@ class DataHandlerTest(test_combinations.TestCase):\n                 class_weight={0: 0.5, 1: 1.0, 2: 1.5},\n             )\n \n-    @parameterized.named_parameters((\"one_hot\", True), (\"sparse\", False))\n-    def test_class_weights_applied(self, one_hot):\n-        num_channels = 3\n-        num_classes = 5\n-        batch_size = 2\n-        image_width = 8\n-\n-        input_shape = (batch_size, image_width, image_width, num_channels)\n-        output_shape = (batch_size, image_width, image_width)\n-\n-        x = tf.random.uniform(input_shape)\n-        sparse_y = tf.random.uniform(\n-            output_shape, maxval=num_classes, dtype=tf.int32\n-        )\n-\n-        if one_hot:\n-            y = tf.one_hot(sparse_y, num_classes)\n-        else:\n-            y = tf.expand_dims(sparse_y, axis=-1)\n-\n-        # Class weight is equal to class number + 1\n-        class_weight = dict([(x, x + 1) for x in range(num_classes)])\n-\n-        sample_weight = np.array([1, 2])\n-\n-        data_handler = data_adapter.DataHandler(\n-            x=x,\n-            y=y,\n-            class_weight=class_weight,\n-            sample_weight=sample_weight,\n-            batch_size=batch_size,\n-            epochs=1,\n-        )\n-        returned_data = []\n-        for _, iterator in data_handler.enumerate_epochs():\n-            epoch_data = []\n-            for _ in data_handler.steps():\n-                epoch_data.append(next(iterator))\n-            returned_data.append(epoch_data)\n-        returned_data = self.evaluate(returned_data)\n-\n-        # We had only 1 batch and 1 epoch, so we extract x, y, sample_weight\n-        result_x, result_y, result_sample_weight = returned_data[0][0]\n-        self.assertAllEqual(x, result_x)\n-        self.assertAllEqual(y, result_y)\n-\n-        # Because class weight = class + 1, resulting class weight = y + 1\n-        # Sample weight is 1 for the first sample, 2 for the second,\n-        # so we double the expected sample weight for the second sample.\n-        self.assertAllEqual(sparse_y[0] + 1, result_sample_weight[0])\n-        self.assertAllEqual(2 * (sparse_y[1] + 1), result_sample_weight[1])\n-\n     @parameterized.named_parameters((\"numpy\", True), (\"dataset\", False))\n     def test_single_x_input_no_tuple_wrapping(self, use_numpy):\n         x = np.ones((10, 1))\n\n@@ -2585,46 +2585,6 @@ class LossWeightingTest(test_combinations.TestCase):\n         # TODO(b/152990697): Fix the class weights test here.\n         # self.assertLess(score[0], ref_score[0])\n \n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_segmentation_class_weights(self):\n-        num_channels = 3\n-        num_classes = 5\n-        batch_size = 2\n-        image_width = 8\n-\n-        input_shape = (batch_size, image_width, image_width, num_channels)\n-        output_shape = (batch_size, image_width, image_width, num_classes)\n-\n-        model = sequential.Sequential([layers_module.Conv2D(num_classes, 1)])\n-\n-        model.compile(\n-            loss=\"categorical_crossentropy\",\n-            metrics=[\"acc\", metrics_module.CategoricalAccuracy()],\n-            weighted_metrics=[\"mae\", metrics_module.CategoricalAccuracy()],\n-            optimizer=\"adam\",\n-            run_eagerly=test_utils.should_run_eagerly(),\n-        )\n-\n-        x = tf.random.uniform(input_shape)\n-        y = tf.random.uniform(output_shape, dtype=tf.int32, maxval=num_classes)\n-\n-        # Class weights are just the class value + 1\n-        class_weight = dict([(i, i + 1) for i in range(num_classes)])\n-\n-        # This test simply asserts that the model can be compiled and fit\n-        # can run without error. Verification that the class weights are\n-        # applied correctly is performed in data_adapter_test.\n-        model.fit(x, y, class_weight=class_weight, steps_per_epoch=1)\n-\n-        sample_weight = np.array([x + 1 for x in range(batch_size)])\n-        model.fit(\n-            x,\n-            y,\n-            class_weight=class_weight,\n-            sample_weight=sample_weight,\n-            steps_per_epoch=1,\n-        )\n-\n     @test_combinations.run_all_keras_modes\n     def test_temporal_sample_weights(self):\n         num_classes = 5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b661c9f73d8860cda6425e3c5b0b1ec145329dc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 513 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -441,7 +441,9 @@ def test_wheel(wheel_path, expected_version, requirements_path):\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n-        \"--nightly\", default=False, help=\"Whether this is for keras-nightly\"\n+        \"--nightly\",\n+        action=argparse.BooleanOptionalAction,\n+        help=\"Whether this is for the `keras-nightly` package.\",\n     )\n     args = parser.parse_args()\n     is_nightly = args.nightly\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#098832a01aeb43dcfe494a1cc59ea24c66e4b929", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 515 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -442,7 +442,7 @@ if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--nightly\",\n-        action=argparse.BooleanOptionalAction,\n+        action=\"store_true\",\n         help=\"Whether this is for the `keras-nightly` package.\",\n     )\n     args = parser.parse_args()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5aa5fb60f34788be6d0a3e0cd3da093c5a70877b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 46 | Churn Cumulative: 16227 | Contributors (this commit): 124 | Commits (past 90d): 11 | Contributors (cumulative): 145 | DMM Complexity: 0.8214285714285714\n\nDIFF:\n@@ -3167,12 +3167,15 @@ class CSVLogger(Callback):\n \n         if self.keys is None:\n             self.keys = sorted(logs.keys())\n-\n-        if self.model.stop_training:\n-            # We set NA so that csv parsers do not fail for this last epoch.\n-            logs = dict(\n-                (k, logs[k]) if k in logs else (k, \"NA\") for k in self.keys\n-            )\n+            # When validation_freq > 1, `val_` keys are not in first epoch logs\n+            # Add the `val_` keys so that its part of the fieldnames of writer.\n+            val_keys_found = False\n+            for key in self.keys:\n+                if key.startswith(\"val_\"):\n+                    val_keys_found = True\n+                    break\n+            if not val_keys_found:\n+                self.keys.extend([\"val_\" + k for k in self.keys])\n \n         if not self.writer:\n \n@@ -3188,7 +3191,9 @@ class CSVLogger(Callback):\n                 self.writer.writeheader()\n \n         row_dict = collections.OrderedDict({\"epoch\": epoch})\n-        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n+        row_dict.update(\n+            (key, handle_value(logs.get(key, \"NA\"))) for key in self.keys\n+        )\n         self.writer.writerow(row_dict)\n         self.csv_file.flush()\n \n\n@@ -1415,7 +1415,6 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         return model, train_ds, callback, filepath\n \n     def _run_load_weights_on_restart_test_common_iterations(self):\n-\n         (\n             model,\n             train_ds,\n@@ -2285,6 +2284,32 @@ class KerasCallbacksTest(test_combinations.TestCase):\n \n             os.remove(filepath)\n \n+            # case 3, Verify Val. loss also registered when Validation Freq > 1\n+            model = make_model()\n+            cbks = [keras.callbacks.CSVLogger(filepath, separator=sep)]\n+            hist = model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=BATCH_SIZE,\n+                validation_data=(x_test, y_test),\n+                validation_freq=3,\n+                callbacks=cbks,\n+                epochs=5,\n+                verbose=0,\n+            )\n+            assert os.path.exists(filepath)\n+            # Verify that validation loss is registered at val. freq\n+            with open(filepath) as csvfile:\n+                rows = csv.DictReader(csvfile, delimiter=sep)\n+                for idx, row in enumerate(rows, 1):\n+                    self.assertIn(\"val_loss\", row)\n+                    if idx == 3:\n+                        self.assertEqual(\n+                            row[\"val_loss\"], str(hist.history[\"val_loss\"][0])\n+                        )\n+                    else:\n+                        self.assertEqual(row[\"val_loss\"], \"NA\")\n+\n     def test_stop_training_csv(self):\n         # Test that using the CSVLogger callback with the TerminateOnNaN\n         # callback does not result in invalid CSVs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dad9fd2cee7413ff39c0fa9ec708f60f90b9a405", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 218 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -41,8 +41,8 @@ class Embedding(Layer):\n     and `tf.keras.layers.IntegerLookup` preprocessing layers can help prepare\n     inputs for an `Embedding` layer.\n \n-    This layer accepts `tf.Tensor` and `tf.RaggedTensor` inputs. It cannot be\n-    called with `tf.SparseTensor` input.\n+    This layer accepts `tf.Tensor`, `tf.RaggedTensor` and `tf.SparseTensor`\n+    input.\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6aa5b6dfefa29c4952cdc100642a79205d0f3d94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 279 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ class Lion(optimizer.Optimizer):\n \n     The Lion optimizer is a stochastic-gradient-descent method that uses the\n     sign operator to control the magnitude of the update, unlike other adaptive\n-    optimizers such as Adam that also rely on second-order moments. This make\n+    optimizers such as Adam that rely on second-order moments. This make\n     Lion more memory-efficient as it only keeps track of the momentum. According\n     to the authors (see reference), its performance gain over Adam grows with\n     the batch size. Because the update of Lion is produced through the sign\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f56033645287df77f6fd0c0741bb16b74090f0eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 42 | Files Changed: 2 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 111 | Churn Cumulative: 17555 | Contributors (this commit): 46 | Commits (past 90d): 22 | Contributors (cumulative): 58 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5589,26 +5589,38 @@ def categorical_focal_crossentropy(\n \n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n     helps to apply a focal factor to down-weight easy examples and focus more on\n-    hard examples. By default, the focal tensor is computed as follows:\n+    hard examples. The general formula for the focal loss (FL)\n+    is as follows:\n \n-    It has pt defined as:\n-    pt = p, if y = 1 else 1 - p\n+    `FL(p_t) = (1 − p_t)^gamma * log(p_t)`\n \n-    The authors use alpha-balanced variant of focal loss in the paper:\n-    FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n+    where `p_t` is defined as follows:\n+    `p_t = output if y_true == 1, else 1 - output`\n+\n+    `(1 − p_t)^gamma` is the `modulating_factor`, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.\n+    `gamma` reduces the importance given to simple examples in a smooth manner.\n+\n+    The authors use alpha-balanced variant of focal loss (FL) in the paper:\n+    `FL(p_t) = −alpha * (1 − p_t)^gamma * log(p_t)`\n+\n+    where `alpha` is the weight factor for the classes. If `alpha` = 1, the\n+    loss won't be able to handle class imbalance properly as all\n+    classes will have the same weight. This can be a constant or a list of\n+    constants. If alpha is a list, it must have the same length as the number\n+    of classes.\n+\n+    The formula above can be generalized to:\n+    `FL(p_t) = alpha * (1 − p_t)^gamma * CrossEntropy(target, output)`\n+\n+    where minus comes from `CrossEntropy(target, output)` (CE).\n \n     Extending this to multi-class case is straightforward:\n-    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from\n-    negative log-likelihood and included in CE.\n-\n-    `modulating_factor` is (1 − pt)^gamma, where `gamma` is a focusing\n-    parameter. When `gamma` = 0, there is no focal effect on the categorical\n-    crossentropy. And if alpha = 1, at the same time the loss is equivalent\n-    to the categorical crossentropy.\n+    `FL(p_t) = alpha * (1 − p_t)^gamma * CategoricalCE(target, output)`\n \n     Args:\n-        target: A tensor with the same shape as `output`.\n-        output: A tensor.\n+        target: Ground truth values from the dataset.\n+        output: Predictions of the model.\n         alpha: A weight balancing factor for all classes, default is `0.25` as\n             mentioned in the reference. It can be a list of floats or a scalar.\n             In the multi-class case, alpha may be set by inverse class\n@@ -5619,6 +5631,9 @@ def categorical_focal_crossentropy(\n         from_logits: Whether `output` is expected to be a logits tensor. By\n             default, we consider that `output` encodes a probability\n             distribution.\n+        axis: Int specifying the channels axis. `axis=-1` corresponds to data\n+             format `channels_last`, and `axis=1` corresponds to data format\n+             `channels_first`.\n \n     Returns:\n         A tensor.\n@@ -5631,13 +5646,13 @@ def categorical_focal_crossentropy(\n         output, from_logits, \"Softmax\", \"categorical_focal_crossentropy\"\n     )\n \n-    output = tf.__internal__.smart_cond.smart_cond(\n-        from_logits,\n-        lambda: softmax(output),\n-        lambda: output,\n-    )\n+    if from_logits:\n+        output = tf.nn.softmax(output, axis=axis)\n \n-    # scale preds so that the class probas of each sample sum to 1\n+    # Adjust the predictions so that the probability of\n+    # each class for every sample adds up to 1\n+    # This is needed to ensure that the cross entropy is\n+    # computed correctly.\n     output = output / tf.reduce_sum(output, axis=axis, keepdims=True)\n \n     epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n\n@@ -926,29 +926,41 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the alpha balanced focal crossentropy loss.\n \n-    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-    helps to apply a focal factor to down-weight easy examples and focus more on\n-    hard examples. By default, the focal tensor is computed as follows:\n-\n-    It has pt defined as:\n-    pt = p, if y = 1 else 1 - p\n-\n-    The authors use alpha-balanced variant of focal loss in the paper:\n-    FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n-\n-    Extending this to multi-class case is straightforward:\n-    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from\n-    negative log-likelihood and included in CE.\n-\n-    `modulating_factor` is (1 − pt)^gamma, where `gamma` is a focusing\n-    parameter. When `gamma` = 0, there is no focal effect on the categorical\n-    crossentropy. And if alpha = 1, at the same time the loss is equivalent to\n-    the categorical crossentropy.\n-\n     Use this crossentropy loss function when there are two or more label\n     classes and if you want to handle class imbalance without using\n-    `class_weights`.\n-    We expect labels to be provided in a `one_hot` representation.\n+    `class_weights`. We expect labels to be provided in a `one_hot`\n+    representation.\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a focal factor to down-weight easy examples and focus more on\n+    hard examples. The general formula for the focal loss (FL)\n+    is as follows:\n+\n+    `FL(p_t) = (1 − p_t)^gamma * log(p_t)`\n+\n+    where `p_t` is defined as follows:\n+    `p_t = output if y_true == 1, else 1 - output`\n+\n+    `(1 − p_t)^gamma` is the `modulating_factor`, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.\n+    `gamma` reduces the importance given to simple examples in a smooth manner.\n+\n+    The authors use alpha-balanced variant of focal loss (FL) in the paper:\n+    `FL(p_t) = −alpha * (1 − p_t)^gamma * log(p_t)`\n+\n+    where `alpha` is the weight factor for the classes. If `alpha` = 1, the\n+    loss won't be able to handle class imbalance properly as all\n+    classes will have the same weight. This can be a constant or a list of\n+    constants. If alpha is a list, it must have the same length as the number\n+    of classes.\n+\n+    The formula above can be generalized to:\n+    `FL(p_t) = alpha * (1 − p_t)^gamma * CrossEntropy(y_true, y_pred)`\n+\n+    where minus comes from `CrossEntropy(y_true, y_pred)` (CE).\n+\n+    Extending this to multi-class case is straightforward:\n+    `FL(p_t) = alpha * (1 − p_t)^gamma * CategoricalCE(y_true, y_pred)`\n \n     In the snippet below, there is `# classes` floating pointing values per\n     example. The shape of both `y_pred` and `y_true` are\n@@ -981,7 +993,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n \n     Usage with the `compile()` API:\n     ```python\n-    model.compile(optimizer='sgd',\n+    model.compile(optimizer='adam',\n                   loss=tf.keras.losses.CategoricalFocalCrossentropy())\n     ```\n     Args:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82130f73e19298bf66652a6ae6e30f23b407dc09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 2660 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,8 +73,9 @@ def handle_partial_sample_weights(\n       describing the raw sample weights.\n     \"\"\"\n     if not isinstance(sample_weights, (list, tuple)):\n-        sample_weights = (sample_weights,)\n-\n+        any_sample_weight = (sample_weights,) is not None and sample_weights is not None\n+        partial_sample_weight = any_sample_weight and sample_weights is None\n+    else:\n         any_sample_weight = sample_weights is not None and any(\n             w is not None for w in sample_weights\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89d2b51ba2fc429293afe689e1e7f942618363ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 120 | Churn Cumulative: 36138 | Contributors (this commit): 154 | Commits (past 90d): 29 | Contributors (cumulative): 202 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1715,21 +1715,24 @@ def _make_class_weight_map_fn(class_weight):\n                 \"output.\"\n             )\n \n-        if y.shape.rank > 2:\n-            raise ValueError(\n-                \"`class_weight` not supported for 3+ dimensional targets.\"\n-            )\n-\n+        if y.shape.rank >= 2:\n             y_classes = tf.__internal__.smart_cond.smart_cond(\n-            y.shape.rank == 2 and backend.shape(y)[1] > 1,\n-            lambda: backend.argmax(y, axis=1),\n-            lambda: tf.cast(backend.reshape(y, (-1,)), tf.int64),\n+                backend.shape(y)[-1] > 1,\n+                lambda: backend.argmax(y, axis=-1),\n+                lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int64),\n             )\n+        else:\n+            # Special casing for rank 1, where we can guarantee sparse encoding.\n+            y_classes = tf.cast(tf.round(y), tf.int64)\n \n         cw = tf.gather(class_weight_tensor, y_classes)\n         if sw is not None:\n             cw = tf.cast(cw, sw.dtype)\n             # `class_weight` and `sample_weight` are multiplicative.\n+            # If class_weight has more than 2 dimensions, we need to reshape\n+            # sample_weight to make broadcasting possible for multiplication.\n+            rank_delta = cw.shape.rank - sw.shape.rank\n+            sw = tf.reshape(sw, sw.shape + [1] * rank_delta)\n             sw = sw * cw\n         else:\n             sw = cw\n\n@@ -1374,6 +1374,58 @@ class DataHandlerTest(test_combinations.TestCase):\n                 class_weight={0: 0.5, 1: 1.0, 2: 1.5},\n             )\n \n+    @parameterized.named_parameters((\"one_hot\", True), (\"sparse\", False))\n+    def test_class_weights_applied(self, one_hot):\n+        num_channels = 3\n+        num_classes = 5\n+        batch_size = 2\n+        image_width = 8\n+\n+        input_shape = (batch_size, image_width, image_width, num_channels)\n+        output_shape = (batch_size, image_width, image_width)\n+\n+        x = tf.random.uniform(input_shape)\n+        sparse_y = tf.random.uniform(\n+            output_shape, maxval=num_classes, dtype=tf.int32\n+        )\n+\n+        if one_hot:\n+            y = tf.one_hot(sparse_y, num_classes)\n+        else:\n+            y = tf.expand_dims(sparse_y, axis=-1)\n+\n+        # Class weight is equal to class number + 1\n+        class_weight = dict([(x, x + 1) for x in range(num_classes)])\n+\n+        sample_weight = np.array([1, 2])\n+\n+        data_handler = data_adapter.DataHandler(\n+            x=x,\n+            y=y,\n+            class_weight=class_weight,\n+            sample_weight=sample_weight,\n+            batch_size=batch_size,\n+            epochs=1,\n+        )\n+        returned_data = []\n+        for _, iterator in data_handler.enumerate_epochs():\n+            epoch_data = []\n+            for _ in data_handler.steps():\n+                epoch_data.append(next(iterator))\n+            returned_data.append(epoch_data)\n+        returned_data = self.evaluate(returned_data)\n+\n+        # We had only 1 batch and 1 epoch, so we extract x, y, sample_weight\n+        result_x, result_y, result_sample_weight = returned_data[0][0]\n+        self.assertAllEqual(x, result_x)\n+        self.assertAllEqual(y, result_y)\n+\n+        # Because class weight = class + 1, resulting class weight = y + 1\n+        # Sample weight is 1 for the first sample, 2 for the second,\n+        # so we double the expected sample weight for the second sample.\n+        self.assertAllEqual(sparse_y[0] + 1, result_sample_weight[0])\n+        self.assertAllEqual(2 * (sparse_y[1] + 1), result_sample_weight[1])\n+\n     @parameterized.named_parameters((\"numpy\", True), (\"dataset\", False))\n     def test_single_x_input_no_tuple_wrapping(self, use_numpy):\n         x = np.ones((10, 1))\n\n@@ -1515,7 +1515,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 (during training only).\n                 This can be useful to tell the model to\n                 \"pay more attention\" to samples from\n-                an under-represented class.\n+                an under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must be\n+                one-hot encoded, or an explicit final dimension of `1` must\n+                be included for sparse class labels.\n             sample_weight: Optional Numpy array of weights for\n                 the training samples, used for weighting the loss function\n                 (during training only). You can either pass a flat (1D)\n@@ -2636,7 +2639,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               to a weight (float) to apply to the model's loss for the samples\n               from this class during training. This can be useful to tell the\n               model to \"pay more attention\" to samples from an under-represented\n-              class.\n+              class. When `class_weight` is specified and targets have a rank of\n+              2 or greater, either `y` must be one-hot encoded, or an explicit\n+              final dimension of `1` must be included for sparse class labels.\n             reset_metrics: If `True`, the metrics returned will be only for this\n               batch. If `False`, the metrics will be statefully accumulated\n               across batches.\n\n@@ -2585,6 +2585,46 @@ class LossWeightingTest(test_combinations.TestCase):\n         # TODO(b/152990697): Fix the class weights test here.\n         # self.assertLess(score[0], ref_score[0])\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_segmentation_class_weights(self):\n+        num_channels = 3\n+        num_classes = 5\n+        batch_size = 2\n+        image_width = 8\n+\n+        input_shape = (batch_size, image_width, image_width, num_channels)\n+        output_shape = (batch_size, image_width, image_width, num_classes)\n+\n+        model = sequential.Sequential([layers_module.Conv2D(num_classes, 1)])\n+\n+        model.compile(\n+            loss=\"categorical_crossentropy\",\n+            metrics=[\"acc\", metrics_module.CategoricalAccuracy()],\n+            weighted_metrics=[\"mae\", metrics_module.CategoricalAccuracy()],\n+            optimizer=\"adam\",\n+            run_eagerly=test_utils.should_run_eagerly(),\n+        )\n+\n+        x = tf.random.uniform(input_shape)\n+        y = tf.random.uniform(output_shape, dtype=tf.int32, maxval=num_classes)\n+\n+        # Class weights are just the class value + 1\n+        class_weight = dict([(i, i + 1) for i in range(num_classes)])\n+\n+        # This test simply asserts that the model can be compiled and fit\n+        # can run without error. Verification that the class weights are\n+        # applied correctly is performed in data_adapter_test.\n+        model.fit(x, y, class_weight=class_weight, steps_per_epoch=1)\n+\n+        sample_weight = np.array([x + 1 for x in range(batch_size)])\n+        model.fit(\n+            x,\n+            y,\n+            class_weight=class_weight,\n+            sample_weight=sample_weight,\n+            steps_per_epoch=1,\n+        )\n+\n     @test_combinations.run_all_keras_modes\n     def test_temporal_sample_weights(self):\n         num_classes = 5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
